<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-28T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14412" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14417" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14426" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14434" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14440" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14447" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14504" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14511" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14523" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14524" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14530" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14571" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14616" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14636" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14665" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14696" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14707" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14749" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14777" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14876" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14933" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14953" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14968" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.15006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.15042" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.15043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.15075" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.08452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.03314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.16468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.04582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08560" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03714" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.01747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19956" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05412" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05879" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10219" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12060" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15560" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02601" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08565" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08602" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18564" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01339" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07603" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08115" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11174" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13324" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13758" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14011" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13544" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.14412">
<title>Harnessing Neuron Stability to Improve DNN Verification. (arXiv:2401.14412v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14412</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks (DNN) have emerged as an effective approach to tackling
real-world problems. However, like human-written software, DNNs are susceptible
to bugs and attacks. This has generated significant interests in developing
effective and scalable DNN verification techniques and tools. In this paper, we
present VeriStable, a novel extension of recently proposed DPLL-based
constraint DNN verification approach. VeriStable leverages the insight that
while neuron behavior may be non-linear across the entire DNN input space, at
intermediate states computed during verification many neurons may be
constrained to have linear behavior - these neurons are stable. Efficiently
detecting stable neurons reduces combinatorial complexity without compromising
the precision of abstractions. Moreover, the structure of clauses arising in
DNN verification problems shares important characteristics with industrial SAT
benchmarks. We adapt and incorporate multi-threading and restart optimizations
targeting those characteristics to further optimize DPLL-based DNN
verification. We evaluate the effectiveness of VeriStable across a range of
challenging benchmarks including fully-connected feedforward networks (FNNs),
convolutional neural networks (CNNs) and residual networks (ResNets) applied to
the standard MNIST and CIFAR datasets. Preliminary results show that VeriStable
is competitive and outperforms state-of-the-art DNN verification tools,
including $\alpha$-$\beta$-CROWN and MN-BaB, the first and second performers of
the VNN-COMP, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duong_H/0/1/0/all/0/1&quot;&gt;Hai Duong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;ThanhVu Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dwyer_M/0/1/0/all/0/1&quot;&gt;Matthew B. Dwyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14413">
<title>Aprendizado de m\&apos;aquina aplicado na eletroqu\&apos;imica. (arXiv:2401.14413v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14413</link>
<description rdf:parseType="Literal">&lt;p&gt;This systematic review focuses on analyzing the use of machine learning
techniques for identifying and quantifying analytes in various electrochemical
applications, presenting the available applications in the literature. Machine
learning is a tool that can facilitate the analysis and enhance the
understanding of processes involving various analytes. In electrochemical
biosensors, it increases the precision of medical diagnostics, improving the
identification of biomarkers and pathogens with high reliability. It can be
effectively used for the classification of complex chemical products; in
environmental monitoring, using low-cost sensors; in portable devices and
wearable systems; among others. Currently, the analysis of some analytes is
still performed manually, requiring the expertise of a specialist in the field
and thus hindering the generalization of results. In light of the advancements
in artificial intelligence today, this work proposes to carry out a systematic
review of the literature on the applications of artificial intelligence
techniques. A set of articles has been identified that address electrochemical
problems using machine learning techniques, more specifically, supervised
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araujo_C/0/1/0/all/0/1&quot;&gt;Carlos Eduardo do Egito Ara&amp;#xfa;jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sgobbi_L/0/1/0/all/0/1&quot;&gt;L&amp;#xed;via F. Sgobbi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sene_I/0/1/0/all/0/1&quot;&gt;Iwens Gervasio Sene Jr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carvalho_S/0/1/0/all/0/1&quot;&gt;Sergio Teixeira de Carvalho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14417">
<title>Fuzzy Logic Function as a Post-hoc Explanator of the Nonlinear Classifier. (arXiv:2401.14417v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14417</link>
<description rdf:parseType="Literal">&lt;p&gt;Pattern recognition systems implemented using deep neural networks achieve
better results than linear models. However, their drawback is the black box
property. This property means that one with no experience utilising nonlinear
systems may need help understanding the outcome of the decision. Such a
solution is unacceptable to the user responsible for the final decision. He
must not only believe in the decision but also understand it. Therefore,
recognisers must have an architecture that allows interpreters to interpret the
findings. The idea of post-hoc explainable classifiers is to design an
interpretable classifier parallel to the black box classifier, giving the same
decisions as the black box classifier. This paper shows that the explainable
classifier completes matching classification decisions with the black box
classifier on the MNIST and FashionMNIST databases if Zadeh`s fuzzy logic
function forms the classifier and DeconvNet importance gives the truth values.
Since the other tested significance measures achieved lower performance than
DeconvNet, it is the optimal transformation of the feature values to their
truth values as inputs to the fuzzy logic function for the databases and
recogniser architecture used.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klimo_M/0/1/0/all/0/1&quot;&gt;Martin Klimo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kralik_L/0/1/0/all/0/1&quot;&gt;Lubomir Kralik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14424">
<title>Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search. (arXiv:2401.14424v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14424</link>
<description rdf:parseType="Literal">&lt;p&gt;Finding a concise and interpretable mathematical formula that accurately
describes the relationship between each variable and the predicted value in the
data is a crucial task in scientific research, as well as a significant
challenge in artificial intelligence. This problem is referred to as symbolic
regression, which is an NP-hard problem. Last year, a symbolic regression
method based on Monte Carlo Tree Search (MCTS) was proposed and sota was
obtained on multiple datasets. While this algorithm has shown considerable
improvement in recovering target expressions compared to previous methods, the
lack of guidance during the MCTS process severely hampers its search
efficiency. Recently, some algorithms have added a pre-trained policy network
to guide the search of MCTS, but the pre-trained policy network generalizes
poorly. To balance efficiency and generality, we propose SR-GPT combining ideas
from AlphaZero. SR-GPT is a new symbolic regression algorithm that combines
MCTS with a Generative Pre-Trained Transformer (GPT). By using GPT to guide the
MCTS process, the search efficiency of MCTS is significantly improved. Next, we
utilize the MCTS results to further refine the GPT, enhancing its capabilities
and providing more accurate guidance for the MCTS process. MCTS and GPT are
coupled together and optimize each other until the target expression is
successfully determined. We conducted extensive evaluations of SR-GPT using 222
expressions sourced from over 10 different symbolic regression datasets. The
experimental results demonstrate that SR-GPT outperforms existing
state-of-the-art algorithms in accurately recovering symbolic expressions both
with and without added noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weijun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lina Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Min Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_M/0/1/0/all/0/1&quot;&gt;Meilan Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1&quot;&gt;Shu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yusong Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14425">
<title>No Longer Trending on Artstation: Prompt Analysis of Generative AI Art. (arXiv:2401.14425v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.14425</link>
<description rdf:parseType="Literal">&lt;p&gt;Image generation using generative AI is rapidly becoming a major new source
of visual media, with billions of AI generated images created using diffusion
models such as Stable Diffusion and Midjourney over the last few years. In this
paper we collect and analyse over 3 million prompts and the images they
generate. Using natural language processing, topic analysis and visualisation
methods we aim to understand collectively how people are using text prompts,
the impact of these systems on artists, and more broadly on the visual cultures
they promote. Our study shows that prompting focuses largely on surface
aesthetics, reinforcing cultural norms, popular conventional representations
and imagery. We also find that many users focus on popular topics (such as
making colouring books, fantasy art, or Christmas cards), suggesting that the
dominant use for the systems analysed is recreational rather than artistic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCormack_J/0/1/0/all/0/1&quot;&gt;Jon McCormack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Llano_M/0/1/0/all/0/1&quot;&gt;Maria Teresa Llano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krol_S/0/1/0/all/0/1&quot;&gt;Stephen James Krol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajcic_N/0/1/0/all/0/1&quot;&gt;Nina Rajcic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14426">
<title>M$^3$TN: Multi-gate Mixture-of-Experts based Multi-valued Treatment Network for Uplift Modeling. (arXiv:2401.14426v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14426</link>
<description rdf:parseType="Literal">&lt;p&gt;Uplift modeling is a technique used to predict the effect of a treatment
(e.g., discounts) on an individual&apos;s response. Although several methods have
been proposed for multi-valued treatment, they are extended from binary
treatment methods. There are still some limitations. Firstly, existing methods
calculate uplift based on predicted responses, which may not guarantee a
consistent uplift distribution between treatment and control groups. Moreover,
this may cause cumulative errors for multi-valued treatment. Secondly, the
model parameters become numerous with many prediction heads, leading to reduced
efficiency. To address these issues, we propose a novel \underline{M}ulti-gate
\underline{M}ixture-of-Experts based \underline{M}ulti-valued
\underline{T}reatment \underline{N}etwork (M$^3$TN). M$^3$TN consists of two
components: 1) a feature representation module with Multi-gate
Mixture-of-Experts to improve the efficiency; 2) a reparameterization module by
modeling uplift explicitly to improve the effectiveness. We also conduct
extensive experiments to demonstrate the effectiveness and efficiency of our
M$^3$TN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zexu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14434">
<title>Transforming gradient-based techniques into interpretable methods. (arXiv:2401.14434v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14434</link>
<description rdf:parseType="Literal">&lt;p&gt;The explication of Convolutional Neural Networks (CNN) through xAI techniques
often poses challenges in interpretation. The inherent complexity of input
features, notably pixels extracted from images, engenders complex correlations.
Gradient-based methodologies, exemplified by Integrated Gradients (IG),
effectively demonstrate the significance of these features. Nevertheless, the
conversion of these explanations into images frequently yields considerable
noise. Presently, we introduce GAD (Gradient Artificial Distancing) as a
supportive framework for gradient-based techniques. Its primary objective is to
accentuate influential regions by establishing distinctions between classes.
The essence of GAD is to limit the scope of analysis during visualization and,
consequently reduce image noise. Empirical investigations involving occluded
images have demonstrated that the identified regions through this methodology
indeed play a pivotal role in facilitating class differentiation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodrigues_C/0/1/0/all/0/1&quot;&gt;Caroline Mazini Rodrigues&lt;/a&gt; (LRDE, LIGM), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boutry_N/0/1/0/all/0/1&quot;&gt;Nicolas Boutry&lt;/a&gt; (LRDE), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najman_L/0/1/0/all/0/1&quot;&gt;Laurent Najman&lt;/a&gt; (LIGM)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14436">
<title>Trust model of privacy-concerned, emotionally-aware agents in a cooperative logistics problem. (arXiv:2401.14436v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2401.14436</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose a trust model to be used into a hypothetical mixed
environment where humans and unmanned vehicles cooperate. We address the
inclusion of emotions inside a trust model in a coherent way to the practical
approaches to the current psychology theories. The most innovative contribution
is how privacy issues play a role in the cooperation decisions of the emotional
trust model. Both, emotions and trust have been cognitively modeled and managed
with the Beliefs, Desires and Intentions (BDI) paradigm into autonomous agents
implemented in GAML (the programming language of GAMA agent platform) that
communicates using the IEEE FIPA standard. The trusting behaviour of these
emotional agents is tested in a cooperative logistics problem where: agents
have to move objects to destinations and some of the objects and places have
privacy issues. The execution of simulations of this logistic problem shows how
emotions and trust contribute to improve the performance of agents in terms of
both, time savings and privacy protection
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carbo_J/0/1/0/all/0/1&quot;&gt;J. Carbo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molina_J/0/1/0/all/0/1&quot;&gt;J.M. Molina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14440">
<title>Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models. (arXiv:2401.14440v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14440</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies of the emergent capabilities of transformer-based Natural
Language Understanding (NLU) models have indicated that they have an
understanding of lexical and compositional semantics. We provide evidence that
suggests these claims should be taken with a grain of salt: we find that
state-of-the-art Natural Language Inference (NLI) models are sensitive towards
minor semantics preserving surface-form variations, which lead to sizable
inconsistent model decisions during inference. Notably, this behaviour differs
from valid and in-depth comprehension of compositional semantics, however does
neither emerge when evaluating model accuracy on standard benchmarks nor when
probing for syntactic, monotonic, and logically robust reasoning. We propose a
novel framework to measure the extent of semantic sensitivity. To this end, we
evaluate NLI models on adversarially generated examples containing minor
semantics-preserving surface-form input noise. This is achieved using
conditional text generation, with the explicit condition that the NLI model
predicts the relationship between the original and adversarial inputs as a
symmetric equivalence entailment. We systematically study the effects of the
phenomenon across NLI models for \emph{in-} and \emph{out-of} domain settings.
Our experiments show that semantic sensitivity causes performance degradations
of $12.92\%$ and $23.71\%$ average over \emph{in-} and \emph{out-of-} domain
settings, respectively. We further perform ablation studies, analysing this
phenomenon across models, datasets, and variations in inference and show that
semantic sensitivity can lead to major inconsistency within model predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arakelyan_E/0/1/0/all/0/1&quot;&gt;Erik Arakelyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhaoqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1&quot;&gt;Isabelle Augenstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14444">
<title>ICASSP 2024 Speech Signal Improvement Challenge. (arXiv:2401.14444v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.14444</link>
<description rdf:parseType="Literal">&lt;p&gt;The ICASSP 2024 Speech Signal Improvement Grand Challenge is intended to
stimulate research in the area of improving the speech signal quality in
communication systems. This marks our second challenge, building upon the
success from the previous ICASSP 2023 Grand Challenge. We enhance the
competition by introducing a dataset synthesizer, enabling all participating
teams to start at a higher baseline, an objective metric for our extended P.804
tests, transcripts for the 2023 test set, and we add Word Accuracy (WAcc) as a
metric. We evaluate a total of 13 systems in the real-time track and 11 systems
in the non-real-time track using both subjective P.804 and objective Word
Accuracy metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1&quot;&gt;Nicolae Catalin Ristea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saabas_A/0/1/0/all/0/1&quot;&gt;Ando Saabas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cutler_R/0/1/0/all/0/1&quot;&gt;Ross Cutler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naderi_B/0/1/0/all/0/1&quot;&gt;Babak Naderi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braun_S/0/1/0/all/0/1&quot;&gt;Sebastian Braun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Branets_S/0/1/0/all/0/1&quot;&gt;Solomiya Branets&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14446">
<title>Black-Box Access is Insufficient for Rigorous AI Audits. (arXiv:2401.14446v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.14446</link>
<description rdf:parseType="Literal">&lt;p&gt;External audits of AI systems are increasingly recognized as a key mechanism
for AI governance. The effectiveness of an audit, however, depends on the
degree of system access granted to auditors. Recent audits of state-of-the-art
AI systems have primarily relied on black-box access, in which auditors can
only query the system and observe its outputs. However, white-box access to the
system&apos;s inner workings (e.g., weights, activations, gradients) allows an
auditor to perform stronger attacks, more thoroughly interpret models, and
conduct fine-tuning. Meanwhile, outside-the-box access to its training and
deployment information (e.g., methodology, code, documentation,
hyperparameters, data, deployment details, findings from internal evaluations)
allows for auditors to scrutinize the development process and design more
targeted evaluations. In this paper, we examine the limitations of black-box
audits and the advantages of white- and outside-the-box audits. We also discuss
technical, physical, and legal safeguards for performing these audits with
minimal security risks. Given that different forms of access can lead to very
different levels of evaluation, we conclude that (1) transparency regarding the
access and methods used by auditors is necessary to properly interpret audit
results, and (2) white- and outside-the-box access allow for substantially more
scrutiny than black-box access alone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1&quot;&gt;Stephen Casper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ezell_C/0/1/0/all/0/1&quot;&gt;Carson Ezell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siegmann_C/0/1/0/all/0/1&quot;&gt;Charlotte Siegmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolt_N/0/1/0/all/0/1&quot;&gt;Noam Kolt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Curtis_T/0/1/0/all/0/1&quot;&gt;Taylor Lynn Curtis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bucknall_B/0/1/0/all/0/1&quot;&gt;Benjamin Bucknall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haupt_A/0/1/0/all/0/1&quot;&gt;Andreas Haupt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1&quot;&gt;Kevin Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scheurer_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xe9;my Scheurer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hobbhahn_M/0/1/0/all/0/1&quot;&gt;Marius Hobbhahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharkey_L/0/1/0/all/0/1&quot;&gt;Lee Sharkey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_S/0/1/0/all/0/1&quot;&gt;Satyapriya Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hagen_M/0/1/0/all/0/1&quot;&gt;Marvin Von Hagen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alberti_S/0/1/0/all/0/1&quot;&gt;Silas Alberti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1&quot;&gt;Alan Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Qinyi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerovitch_M/0/1/0/all/0/1&quot;&gt;Michael Gerovitch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1&quot;&gt;David Bau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tegmark_M/0/1/0/all/0/1&quot;&gt;Max Tegmark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1&quot;&gt;David Krueger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1&quot;&gt;Dylan Hadfield-Menell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14447">
<title>Wordflow: Social Prompt Engineering for Large Language Models. (arXiv:2401.14447v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.14447</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) require well-crafted prompts for effective use.
Prompt engineering, the process of designing prompts, is challenging,
particularly for non-experts who are less familiar with AI technologies. While
researchers have proposed techniques and tools to assist LLM users in prompt
design, these works primarily target AI application developers rather than
non-experts. To address this research gap, we propose social prompt
engineering, a novel paradigm that leverages social computing techniques to
facilitate collaborative prompt design. To investigate social prompt
engineering, we introduce Wordflow, an open-source and social text editor that
enables everyday users to easily create, run, share, and discover LLM prompts.
Additionally, by leveraging modern web technologies, Wordflow allows users to
run LLMs locally and privately in their browsers. Two usage scenarios highlight
how social prompt engineering and our tool can enhance laypeople&apos;s interaction
with LLMs. Wordflow is publicly accessible at
https://poloclub.github.io/wordflow.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zijie J. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakravarthy_A/0/1/0/all/0/1&quot;&gt;Aishwarya Chakravarthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munechika_D/0/1/0/all/0/1&quot;&gt;David Munechika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1&quot;&gt;Duen Horng Chau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14461">
<title>Marabou 2.0: A Versatile Formal Analyzer of Neural Networks. (arXiv:2401.14461v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.14461</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper serves as a comprehensive system description of version 2.0 of the
Marabou framework for formal analysis of neural networks. We discuss the tool&apos;s
architectural design and highlight the major features and components introduced
since its initial release.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haoze Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isac_O/0/1/0/all/0/1&quot;&gt;Omri Isac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeljic_A/0/1/0/all/0/1&quot;&gt;Aleksandar Zelji&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tagomori_T/0/1/0/all/0/1&quot;&gt;Teruhiro Tagomori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daggitt_M/0/1/0/all/0/1&quot;&gt;Matthew Daggitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kokke_W/0/1/0/all/0/1&quot;&gt;Wen Kokke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Refaeli_I/0/1/0/all/0/1&quot;&gt;Idan Refaeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amir_G/0/1/0/all/0/1&quot;&gt;Guy Amir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Julian_K/0/1/0/all/0/1&quot;&gt;Kyle Julian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bassan_S/0/1/0/all/0/1&quot;&gt;Shahaf Bassan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Pei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lahav_O/0/1/0/all/0/1&quot;&gt;Ori Lahav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Min Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komendantskaya_E/0/1/0/all/0/1&quot;&gt;Ekaterina Komendantskaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1&quot;&gt;Guy Katz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrett_C/0/1/0/all/0/1&quot;&gt;Clark Barrett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14469">
<title>Unveiling the Unseen: Identifiable Clusters in Trained Depthwise Convolutional Kernels. (arXiv:2401.14469v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14469</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in depthwise-separable convolutional neural networks
(DS-CNNs) have led to novel architectures, that surpass the performance of
classical CNNs, by a considerable scalability and accuracy margin. This paper
reveals another striking property of DS-CNN architectures: discernible and
explainable patterns emerge in their trained depthwise convolutional kernels in
all layers. Through an extensive analysis of millions of trained filters, with
different sizes and from various models, we employed unsupervised clustering
with autoencoders, to categorize these filters. Astonishingly, the patterns
converged into a few main clusters, each resembling the difference of Gaussian
(DoG) functions, and their first and second-order derivatives. Notably, we were
able to classify over 95\% and 90\% of the filters from state-of-the-art
ConvNextV2 and ConvNeXt models, respectively. This finding is not merely a
technological curiosity; it echoes the foundational models neuroscientists have
long proposed for the vision systems of mammals. Our results thus deepen our
understanding of the emergent properties of trained DS-CNNs and provide a
bridge between artificial and biological visual processing systems. More
broadly, they pave the way for more interpretable and biologically-inspired
neural network designs in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babaiee_Z/0/1/0/all/0/1&quot;&gt;Zahra Babaiee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiasari_P/0/1/0/all/0/1&quot;&gt;Peyman M. Kiasari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1&quot;&gt;Daniela Rus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosu_R/0/1/0/all/0/1&quot;&gt;Radu Grosu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14484">
<title>Design Principles for Generative AI Applications. (arXiv:2401.14484v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.14484</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative AI applications present unique design challenges. As generative AI
technologies are increasingly being incorporated into mainstream applications,
there is an urgent need for guidance on how to design user experiences that
foster effective and safe use. We present six principles for the design of
generative AI applications that address unique characteristics of generative AI
UX and offer new interpretations and extensions of known issues in the design
of AI applications. Each principle is coupled with a set of design strategies
for implementing that principle via UX capabilities or through the design
process. The principles and strategies were developed through an iterative
process involving literature review, feedback from design practitioners,
validation against real-world generative AI applications, and incorporation
into the design process of two generative AI applications. We anticipate the
principles to usefully inform the design of generative AI applications by
driving actionable design recommendations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weisz_J/0/1/0/all/0/1&quot;&gt;Justin D. Weisz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jessica He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1&quot;&gt;Michael Muller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoefer_G/0/1/0/all/0/1&quot;&gt;Gabriela Hoefer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miles_R/0/1/0/all/0/1&quot;&gt;Rachel Miles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geyer_W/0/1/0/all/0/1&quot;&gt;Werner Geyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14488">
<title>Scilab-RL: A software framework for efficient reinforcement learning and cognitive modeling research. (arXiv:2401.14488v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14488</link>
<description rdf:parseType="Literal">&lt;p&gt;One problem with researching cognitive modeling and reinforcement learning
(RL) is that researchers spend too much time on setting up an appropriate
computational framework for their experiments. Many open source implementations
of current RL algorithms exist, but there is a lack of a modular suite of tools
combining different robotic simulators and platforms, data visualization,
hyperparameter optimization, and baseline experiments. To address this problem,
we present Scilab-RL, a software framework for efficient research in cognitive
modeling and reinforcement learning for robotic agents. The framework focuses
on goal-conditioned reinforcement learning using Stable Baselines 3 and the
OpenAI gym interface. It enables native possibilities for experiment
visualizations and hyperparameter optimization. We describe how these features
enable researchers to conduct experiments with minimal time effort, thus
maximizing research output.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dohmen_J/0/1/0/all/0/1&quot;&gt;Jan Dohmen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roder_F/0/1/0/all/0/1&quot;&gt;Frank R&amp;#xf6;der&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eppe_M/0/1/0/all/0/1&quot;&gt;Manfred Eppe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14489">
<title>The Case for Co-Designing Model Architectures with Hardware. (arXiv:2401.14489v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2401.14489</link>
<description rdf:parseType="Literal">&lt;p&gt;While GPUs are responsible for training the vast majority of state-of-the-art
deep learning models, the implications of their architecture are often
overlooked when designing new deep learning (DL) models. As a consequence,
modifying a DL model to be more amenable to the target hardware can
significantly improve the runtime performance of DL training and inference. In
this paper, we provide a set of guidelines for users to maximize the runtime
performance of their transformer models. These guidelines have been created by
carefully considering the impact of various model hyperparameters controlling
model shape on the efficiency of the underlying computation kernels executed on
the GPU. We find the throughput of models with efficient model shapes is up to
39\% higher while preserving accuracy compared to models with a similar number
of parameters but with unoptimized shapes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anthony_Q/0/1/0/all/0/1&quot;&gt;Quentin Anthony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatef_J/0/1/0/all/0/1&quot;&gt;Jacob Hatef&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanan_D/0/1/0/all/0/1&quot;&gt;Deepak Narayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1&quot;&gt;Stella Biderman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bekman_S/0/1/0/all/0/1&quot;&gt;Stas Bekman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Junqi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafi_A/0/1/0/all/0/1&quot;&gt;Aamir Shafi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramoni_H/0/1/0/all/0/1&quot;&gt;Hari Subramoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panda_D/0/1/0/all/0/1&quot;&gt;Dhabaleswar Panda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14504">
<title>Learning When to See for Long-term Traffic Data Collection on Power-constrained Devices. (arXiv:2401.14504v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2401.14504</link>
<description rdf:parseType="Literal">&lt;p&gt;Collecting traffic data is crucial for transportation systems and urban
planning, and is often more desirable through easy-to-deploy but
power-constrained devices, due to the unavailability or high cost of power and
network infrastructure. The limited power means an inevitable trade-off between
data collection duration and accuracy/resolution. We introduce a novel
learning-based framework that strategically decides observation timings for
battery-powered devices and reconstructs the full data stream from sparsely
sampled observations, resulting in minimal performance loss and a significantly
prolonged system lifetime. Our framework comprises a predictor, a controller,
and an estimator. The predictor utilizes historical data to forecast future
trends within a fixed time horizon. The controller uses the forecasts to
determine the next optimal timing for data collection. Finally, the estimator
reconstructs the complete data profile from the sampled observations. We
evaluate the performance of the proposed method on PeMS data by an RNN
(Recurrent Neural Network) predictor and estimator, and a DRQN (Deep Recurrent
Q-Network) controller, and compare it against the baseline that uses Kalman
filter and uniform sampling. The results indicate that our method outperforms
the baseline, primarily due to the inclusion of more representative data points
in the profile, resulting in an overall 10\% improvement in estimation
accuracy. Source code will be publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruixuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_W/0/1/0/all/0/1&quot;&gt;Wenyu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bian_Z/0/1/0/all/0/1&quot;&gt;Zilin Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ozbay_K/0/1/0/all/0/1&quot;&gt;Kaan Ozbay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_C/0/1/0/all/0/1&quot;&gt;Chen Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14511">
<title>Automated legal reasoning with discretion to act using s(LAW). (arXiv:2401.14511v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.14511</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated legal reasoning and its application in smart contracts and
automated decisions are increasingly attracting interest. In this context,
ethical and legal concerns make it necessary for automated reasoners to justify
in human-understandable terms the advice given. Logic Programming, specially
Answer Set Programming, has a rich semantics and has been used to very
concisely express complex knowledge. However, modelling discretionality to act
and other vague concepts such as ambiguity cannot be expressed in top-down
execution models based on Prolog, and in bottom-up execution models based on
ASP the justifications are incomplete and/or not scalable. We propose to use
s(CASP), a top-down execution model for predicate ASP, to model vague concepts
following a set of patterns. We have implemented a framework, called s(LAW), to
model, reason, and justify the applicable legislation and validate it by
translating (and benchmarking) a representative use case, the criteria for the
admission of students in the &quot;Comunidad de Madrid&quot;.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arias_J/0/1/0/all/0/1&quot;&gt;Joaqu&amp;#xed;n Arias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreno_Rebato_M/0/1/0/all/0/1&quot;&gt;Mar Moreno-Rebato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_Garcia_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; A. Rodr&amp;#xed;guez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ossowski_S/0/1/0/all/0/1&quot;&gt;Sascha Ossowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14521">
<title>Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron. (arXiv:2401.14521v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14521</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the applicability of machine learning technologies to the
development of parsimonious, interpretable, catchment-scale hydrologic models
using directed-graph architectures based on the mass-conserving perceptron
(MCP) as the fundamental computational unit. Here, we focus on architectural
complexity (depth) at a single location, rather than universal applicability
(breadth) across large samples of catchments. The goal is to discover a minimal
representation (numbers of cell-states and flow paths) that represents the
dominant processes that can explain the input-state-output behaviors of a given
catchment, with particular emphasis given to simulating the full range (high,
medium, and low) of flow dynamics. We find that a HyMod-like architecture with
three cell-states and two major flow pathways achieves such a representation at
our study location, but that the additional incorporation of an input-bypass
mechanism significantly improves the timing and shape of the hydrograph, while
the inclusion of bi-directional groundwater mass exchanges significantly
enhances the simulation of baseflow. Overall, our results demonstrate the
importance of using multiple diagnostic metrics for model evaluation, while
highlighting the need for designing training metrics that are better suited to
extracting information across the full range of flow dynamics. Further, they
set the stage for interpretable regional-scale MCP-based hydrological modeling
(using large sample data) by using neural architecture search to determine
appropriate minimal representations for catchments in different hydroclimatic
regimes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuan-Heng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1&quot;&gt;Hoshin V. Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14523">
<title>Empathy and the Right to Be an Exception: What LLMs Can and Cannot Do. (arXiv:2401.14523v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.14523</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in the performance of large language models (LLMs) have led some
researchers to propose the emergence of theory of mind (ToM) in artificial
intelligence (AI). LLMs can attribute beliefs, desires, intentions, and
emotions, and they will improve in their accuracy. Rather than employing the
characteristically human method of empathy, they learn to attribute mental
states by recognizing linguistic patterns in a dataset that typically do not
include that individual. We ask whether LLMs&apos; inability to empathize precludes
them from honoring an individual&apos;s right to be an exception, that is, from
making assessments of character and predictions of behavior that reflect
appropriate sensitivity to a person&apos;s individuality. Can LLMs seriously
consider an individual&apos;s claim that their case is different based on internal
mental states like beliefs, desires, and intentions, or are they limited to
judging that case based on its similarities to others? We propose that the
method of empathy has special significance for honoring the right to be an
exception that is distinct from the value of predictive accuracy, at which LLMs
excel. We conclude by considering whether using empathy to consider exceptional
cases has intrinsic or merely practical value and we introduce conceptual and
empirical avenues for advancing this investigation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kidder_W/0/1/0/all/0/1&quot;&gt;William Kidder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DCruz_J/0/1/0/all/0/1&quot;&gt;Jason D&amp;#x27;Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varshney_K/0/1/0/all/0/1&quot;&gt;Kush R. Varshney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14524">
<title>Evaluating GPT-3.5&apos;s Awareness and Summarization Abilities for European Constitutional Texts with Shared Topics. (arXiv:2401.14524v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14524</link>
<description rdf:parseType="Literal">&lt;p&gt;Constitutions are foundational legal documents that underpin the governmental
and societal structures. As such, they are a reflection of a nation&apos;s cultural
and social uniqueness, but also contribute to establish topics of universal
importance, like citizens&apos; rights and duties (RD). In this work, using the
renowned GPT-3.5, we leverage generative large language models to understand
constitutional passages that transcend national boundaries. A key contribution
of our study is the introduction of a novel application of abstractive
summarization on a multi-source collection of constitutional texts, with a
focus on European countries&apos; constitution passages related to RD topics. Our
results show the meaningfulness of GPT-3.5 to produce informative, coherent and
faithful summaries capturing RD topics across European countries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greco_C/0/1/0/all/0/1&quot;&gt;Candida M. Greco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tagarelli_A/0/1/0/all/0/1&quot;&gt;A. Tagarelli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14530">
<title>Relative Value Biases in Large Language Models. (arXiv:2401.14530v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14530</link>
<description rdf:parseType="Literal">&lt;p&gt;Studies of reinforcement learning in humans and animals have demonstrated a
preference for options that yielded relatively better outcomes in the past,
even when those options are associated with lower absolute reward. The present
study tested whether large language models would exhibit a similar bias. We had
gpt-4-1106-preview (GPT-4 Turbo) and Llama-2-70B make repeated choices between
pairs of options with the goal of maximizing payoffs. A complete record of
previous outcomes was included in each prompt. Both models exhibited relative
value decision biases similar to those observed in humans and animals. Making
relative comparisons among outcomes more explicit magnified the bias, whereas
prompting the models to estimate expected outcomes caused the bias to
disappear. These results have implications for the potential mechanisms that
contribute to context-dependent choice in human agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayes_W/0/1/0/all/0/1&quot;&gt;William M. Hayes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yax_N/0/1/0/all/0/1&quot;&gt;Nicolas Yax&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palminteri_S/0/1/0/all/0/1&quot;&gt;Stefano Palminteri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14542">
<title>Exploring Musical Roots: Applying Audio Embeddings to Empower Influence Attribution for a Generative Music Model. (arXiv:2401.14542v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.14542</link>
<description rdf:parseType="Literal">&lt;p&gt;Every artist has a creative process that draws inspiration from previous
artists and their works. Today, &quot;inspiration&quot; has been automated by generative
music models. The black box nature of these models obscures the identity of the
works that influence their creative output. As a result, users may
inadvertently appropriate, misuse, or copy existing artists&apos; works. We
establish a replicable methodology to systematically identify similar pieces of
music audio in a manner that is useful for understanding training data
attribution. A key aspect of our approach is to harness an effective music
audio similarity measure. We compare the effect of applying CLMR and CLAP
embeddings to similarity measurement in a set of 5 million audio clips used to
train VampNet, a recent open source generative music model. We validate this
approach with a human listening study. We also explore the effect that
modifications of an audio example (e.g., pitch shifting, time stretching,
background noise) have on similarity measurements. This work is foundational to
incorporating automated influence attribution into generative modeling, which
promises to let model creators and users move from ignorant appropriation to
informed creation. Audio samples that accompany this paper are available at
https://tinyurl.com/exploring-musical-roots.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnett_J/0/1/0/all/0/1&quot;&gt;Julia Barnett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_H/0/1/0/all/0/1&quot;&gt;Hugo Flores Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pardo_B/0/1/0/all/0/1&quot;&gt;Bryan Pardo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14559">
<title>Language Modelling Approaches to Adaptive Machine Translation. (arXiv:2401.14559v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14559</link>
<description rdf:parseType="Literal">&lt;p&gt;Consistency is a key requirement of high-quality translation. It is
especially important to adhere to pre-approved terminology and adapt to
corrected translations in domain-specific projects. Machine translation (MT)
has achieved significant progress in the area of domain adaptation. However,
in-domain data scarcity is common in translation settings, due to the lack of
specialised datasets and terminology, or inconsistency and inaccuracy of
available in-domain translations. In such scenarios where there is insufficient
in-domain data to fine-tune MT models, producing translations that are
consistent with the relevant context is challenging. While real-time adaptation
can make use of smaller amounts of in-domain data to improve the translation on
the fly, it remains challenging due to supported context limitations and
efficiency constraints. Large language models (LLMs) have recently shown
interesting capabilities of in-context learning, where they learn to replicate
certain input-output text generation patterns, without further fine-tuning.
Such capabilities have opened new horizons for domain-specific data
augmentation and real-time adaptive MT. This work attempts to address two main
relevant questions: 1) in scenarios involving human interaction and continuous
feedback, can we employ language models to improve the quality of adaptive MT
at inference time? and 2) in the absence of sufficient in-domain data, can we
use pre-trained large-scale language models to improve the process of MT domain
adaptation?
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moslem_Y/0/1/0/all/0/1&quot;&gt;Yasmin Moslem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14571">
<title>Driving Towards Inclusion: Revisiting In-Vehicle Interaction in Autonomous Vehicles. (arXiv:2401.14571v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.14571</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a comprehensive literature review of the current state of
in-vehicle human-computer interaction (HCI) in the context of self-driving
vehicles, with a specific focus on inclusion and accessibility. This study&apos;s
aim is to examine the user-centered design principles for inclusive HCI in
self-driving vehicles, evaluate existing HCI systems, and identify emerging
technologies that have the potential to enhance the passenger experience. The
paper begins by providing an overview of the current state of self-driving
vehicle technology, followed by an examination of the importance of HCI in this
context. Next, the paper reviews the existing literature on inclusive HCI
design principles and evaluates the effectiveness of current HCI systems in
self-driving vehicles. The paper also identifies emerging technologies that
have the potential to enhance the passenger experience, such as voice-activated
interfaces, haptic feedback systems, and augmented reality displays. Finally,
the paper proposes an end-to-end design framework for the development of an
inclusive in-vehicle experience, which takes into consideration the needs of
all passengers, including those with disabilities, or other accessibility
requirements. This literature review highlights the importance of user-centered
design principles in the development of HCI systems for self-driving vehicles
and emphasizes the need for inclusive design to ensure that all passengers can
safely and comfortably use these vehicles. The proposed end-to-end design
framework provides a practical approach to achieving this goal and can serve as
a valuable resource for designers, researchers, and policymakers in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bastola_A/0/1/0/all/0/1&quot;&gt;Ashish Bastola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brinkley_J/0/1/0/all/0/1&quot;&gt;Julian Brinkley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razi_A/0/1/0/all/0/1&quot;&gt;Abolfazl Razi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14589">
<title>Enhancing Diagnostic Accuracy through Multi-Agent Conversations: Using Large Language Models to Mitigate Cognitive Bias. (arXiv:2401.14589v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14589</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: Cognitive biases in clinical decision-making significantly
contribute to errors in diagnosis and suboptimal patient outcomes. Addressing
these biases presents a formidable challenge in the medical field. This study
explores the role of large language models (LLMs) in mitigating these biases
through the utilization of a multi-agent framework. We simulate the clinical
decision-making processes through multi-agent conversation and evaluate its
efficacy in improving diagnostic accuracy. Methods: A total of 16 published and
unpublished case reports where cognitive biases have resulted in misdiagnoses
were identified from the literature. In the multi-agent system, we leveraged
GPT-4 Turbo to facilitate interactions among four simulated agents to replicate
clinical team dynamics. Each agent has a distinct role: 1) To make the initial
and final diagnosis after considering the discussions, 2) The devil&apos;s advocate
and correct confirmation and anchoring bias, 3) The tutor and facilitator of
the discussion to reduce premature closure bias, and 4) To record and summarize
the findings. A total of 80 simulations were evaluated for the accuracy of
initial diagnosis, top differential diagnosis and final two differential
diagnoses. Findings: In a total of 80 responses evaluating both initial and
final diagnoses, the initial diagnosis had an accuracy of 0% (0/80), but
following multi-agent discussions, the accuracy for the top differential
diagnosis increased to 71.3% (57/80), and for the final two differential
diagnoses, to 80.0% (64/80). The system demonstrated an ability to reevaluate
and correct misconceptions, even in scenarios with misleading initial
investigations. Interpretation: The LLM-driven multi-agent conversation system
shows promise in enhancing diagnostic accuracy in diagnostically challenging
medical scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_Y/0/1/0/all/0/1&quot;&gt;Yu He Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Rui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lie_S/0/1/0/all/0/1&quot;&gt;Sui An Lie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_T/0/1/0/all/0/1&quot;&gt;Taylor Xin Yi Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdullah_H/0/1/0/all/0/1&quot;&gt;Hairil Rizal Abdullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ting_D/0/1/0/all/0/1&quot;&gt;Daniel Shu Wei Ting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Nan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14616">
<title>Alternative Speech: Complementary Method to Counter-Narrative for Better Discourse. (arXiv:2401.14616v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14616</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the concept of &quot;Alternative Speech&quot; as a new way to directly
combat hate speech and complement the limitations of counter-narrative. An
alternative speech provides practical alternatives to hate speech in real-world
scenarios by offering speech-level corrections to speakers while considering
the surrounding context and promoting speakers to reform. Further, an
alternative speech can combat hate speech alongside counter-narratives,
offering a useful tool to address social issues such as racial discrimination
and gender inequality. We propose the new concept and provide detailed
guidelines for constructing the necessary dataset. Through discussion, we
demonstrate that combining alternative speech and counter-narrative can be a
more effective strategy for combating hate speech by complementing specificity
and guiding capacity of counter-narrative. This paper presents another
perspective for dealing with hate speech, offering viable remedies to
complement the constraints of current approaches to mitigating harmful bias.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seungyoon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1&quot;&gt;Dahyun Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Chanjun Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seolhwa Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1&quot;&gt;Heuiseok Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14617">
<title>A Systematic Literature Review on Explainability for Machine/Deep Learning-based Software Engineering Research. (arXiv:2401.14617v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.14617</link>
<description rdf:parseType="Literal">&lt;p&gt;The remarkable achievements of Artificial Intelligence (AI) algorithms,
particularly in Machine Learning (ML) and Deep Learning (DL), have fueled their
extensive deployment across multiple sectors, including Software Engineering
(SE). However, due to their black-box nature, these promising AI-driven SE
models are still far from being deployed in practice. This lack of
explainability poses unwanted risks for their applications in critical tasks,
such as vulnerability detection, where decision-making transparency is of
paramount importance. This paper endeavors to elucidate this interdisciplinary
domain by presenting a systematic literature review of approaches that aim to
improve the explainability of AI models within the context of SE. The review
canvasses work appearing in the most prominent SE &amp;amp; AI conferences and
journals, and spans 63 papers across 21 unique SE tasks. Based on three key
Research Questions (RQs), we aim to (1) summarize the SE tasks where XAI
techniques have shown success to date; (2) classify and analyze different XAI
techniques; and (3) investigate existing evaluation approaches. Based on our
findings, we identified a set of challenges remaining to be addressed in
existing studies, together with a roadmap highlighting potential opportunities
we deemed appropriate and important for future work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1&quot;&gt;Sicong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiaobing Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Widyasari_R/0/1/0/all/0/1&quot;&gt;Ratnadira Widyasari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_D/0/1/0/all/0/1&quot;&gt;David Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoxue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1&quot;&gt;Lili Bo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiale Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Di Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yixin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14630">
<title>An Empirical Investigation of Domain Adaptation Ability for Chinese Spelling Check Models. (arXiv:2401.14630v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14630</link>
<description rdf:parseType="Literal">&lt;p&gt;Chinese Spelling Check (CSC) is a meaningful task in the area of Natural
Language Processing (NLP) which aims at detecting spelling errors in Chinese
texts and then correcting these errors. However, CSC models are based on
pretrained language models, which are trained on a general corpus.
Consequently, their performance may drop when confronted with downstream tasks
involving domain-specific terms. In this paper, we conduct a thorough
evaluation about the domain adaption ability of various typical CSC models by
building three new datasets encompassing rich domain-specific terms from the
financial, medical, and legal domains. Then we conduct empirical investigations
in the corresponding domain-specific test datasets to ascertain the
cross-domain adaptation ability of several typical CSC models. We also test the
performance of the popular large language model ChatGPT. As shown in our
experiments, the performances of the CSC models drop significantly in the new
domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Ruoqing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Hongliang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Piji Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14636">
<title>Efficient Constraint Generation for Stochastic Shortest Path Problems. (arXiv:2401.14636v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.14636</link>
<description rdf:parseType="Literal">&lt;p&gt;Current methods for solving Stochastic Shortest Path Problems (SSPs) find
states&apos; costs-to-go by applying Bellman backups, where state-of-the-art methods
employ heuristics to select states to back up and prune. A fundamental
limitation of these algorithms is their need to compute the cost-to-go for
every applicable action during each state backup, leading to unnecessary
computation for actions identified as sub-optimal. We present new connections
between planning and operations research and, using this framework, we address
this issue of unnecessary computation by introducing an efficient version of
constraint generation for SSPs. This technique allows algorithms to ignore
sub-optimal actions and avoid computing their costs-to-go. We also apply our
novel technique to iLAO* resulting in a new algorithm, CG-iLAO*. Our
experiments show that CG-iLAO* ignores up to 57% of iLAO*&apos;s actions and it
solves problems up to 8x and 3x faster than LRTDP and iLAO*.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmalz_J/0/1/0/all/0/1&quot;&gt;Johannes Schmalz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trevizan_F/0/1/0/all/0/1&quot;&gt;Felipe Trevizan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14665">
<title>PepGB: Facilitating peptide drug discovery via graph neural networks. (arXiv:2401.14665v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2401.14665</link>
<description rdf:parseType="Literal">&lt;p&gt;Peptides offer great biomedical potential and serve as promising drug
candidates. Currently, the majority of approved peptide drugs are directly
derived from well-explored natural human peptides. It is quite necessary to
utilize advanced deep learning techniques to identify novel peptide drugs in
the vast, unexplored biochemical space. Despite various in silico methods
having been developed to accelerate peptide early drug discovery, existing
models face challenges of overfitting and lacking generalizability due to the
limited size, imbalanced distribution and inconsistent quality of experimental
data. In this study, we propose PepGB, a deep learning framework to facilitate
peptide early drug discovery by predicting peptide-protein interactions
(PepPIs). Employing graph neural networks, PepGB incorporates a fine-grained
perturbation module and a dual-view objective with contrastive learning-based
peptide pre-trained representation to predict PepPIs. Through rigorous
evaluations, we demonstrated that PepGB greatly outperforms baselines and can
accurately identify PepPIs for novel targets and peptide hits, thereby
contributing to the target identification and hit discovery processes. Next, we
derive an extended version, diPepGB, to tackle the bottleneck of modeling
highly imbalanced data prevalent in lead generation and optimization processes.
Utilizing directed edges to represent relative binding strength between two
peptide nodes, diPepGB achieves superior performance in real-world assays. In
summary, our proposed frameworks can serve as potent tools to facilitate
peptide early drug discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lei_Y/0/1/0/all/0/1&quot;&gt;Yipin Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Fang_M/0/1/0/all/0/1&quot;&gt;Meng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Han Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zeng_J/0/1/0/all/0/1&quot;&gt;Jianyang Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14694">
<title>TA-RNN: an Attention-based Time-aware Recurrent Neural Network Architecture for Electronic Health Records. (arXiv:2401.14694v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14694</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivation: Electronic Health Records (EHR) represent a comprehensive
resource of a patient&apos;s medical history. EHR are essential for utilizing
advanced technologies such as deep learning (DL), enabling healthcare providers
to analyze extensive data, extract valuable insights, and make precise and
data-driven clinical decisions. DL methods such as Recurrent Neural Networks
(RNN) have been utilized to analyze EHR to model disease progression and
predict diagnosis. However, these methods do not address some inherent
irregularities in EHR data such as irregular time intervals between clinical
visits. Furthermore, most DL models are not interpretable. In this study, we
propose two interpretable DL architectures based on RNN, namely Time-Aware RNN
(TA-RNN) and TA-RNN-Autoencoder (TA-RNN-AE) to predict patient&apos;s clinical
outcome in EHR at next visit and multiple visits ahead, respectively. To
mitigate the impact of irregular time intervals, we propose incorporating time
embedding of the elapsed times between visits. For interpretability, we propose
employing a dual-level attention mechanism that operates between visits and
features within each visit.
&lt;/p&gt;
&lt;p&gt;Results: The results of the experiments conducted on Alzheimer&apos;s Disease
Neuroimaging Initiative (ADNI) and National Alzheimer&apos;s Coordinating Center
(NACC) datasets indicated superior performance of proposed models for
predicting Alzheimer&apos;s Disease (AD) compared to state-of-the-art and baseline
approaches based on F2 and sensitivity. Additionally, TA-RNN showed superior
performance on Medical Information Mart for Intensive Care (MIMIC-III) dataset
for mortality prediction. In our ablation study, we observed enhanced
predictive performance by incorporating time embedding and attention
mechanisms. Finally, investigating attention weights helped identify
influential visits and features in predictions.
&lt;/p&gt;
&lt;p&gt;Availability: https://github.com/bozdaglab/TA-RNN
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olaimat_M/0/1/0/all/0/1&quot;&gt;Mohammad Al Olaimat&lt;/a&gt; (1, 3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bozdag_S/0/1/0/all/0/1&quot;&gt;Serdar Bozdag&lt;/a&gt; (1, 2 and 3), the &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Initiative_A/0/1/0/all/0/1&quot;&gt;Alzheimer&amp;#x27;s Disease Neuroimaging Initiative&lt;/a&gt; ((1) Dept. of Computer Science and Engineering, University of North Texas, Denton, USA, (2) Dept. of Mathematics, University of North Texas, Denton, USA, (3) BioDiscovery Institute, University of North Texas, Denton, USA)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14696">
<title>Asymptotic Midpoint Mixup for Margin Balancing and Moderate Broadening. (arXiv:2401.14696v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14696</link>
<description rdf:parseType="Literal">&lt;p&gt;In the feature space, the collapse between features invokes critical problems
in representation learning by remaining the features undistinguished.
Interpolation-based augmentation methods such as mixup have shown their
effectiveness in relieving the collapse problem between different classes,
called inter-class collapse. However, intra-class collapse raised in
coarse-to-fine transfer learning has not been discussed in the augmentation
approach. To address them, we propose a better feature augmentation method,
asymptotic midpoint mixup. The method generates augmented features by
interpolation but gradually moves them toward the midpoint of inter-class
feature pairs. As a result, the method induces two effects: 1) balancing the
margin for all classes and 2) only moderately broadening the margin until it
holds maximal confidence. We empirically analyze the collapse effects by
measuring alignment and uniformity with visualizing representations. Then, we
validate the intra-class collapse effects in coarse-to-fine transfer learning
and the inter-class collapse effects in imbalanced learning on long-tailed
datasets. In both tasks, our method shows better performance than other
augmentation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hoyong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Semi Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kangil Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14698">
<title>Under the Surface: Tracking the Artifactuality of LLM-Generated Data. (arXiv:2401.14698v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14698</link>
<description rdf:parseType="Literal">&lt;p&gt;This work delves into the expanding role of large language models (LLMs) in
generating artificial data. LLMs are increasingly employed to create a variety
of outputs, including annotations, preferences, instruction prompts, simulated
dialogues, and free text. As these forms of LLM-generated data often intersect
in their application, they exert mutual influence on each other and raise
significant concerns about the quality and diversity of the artificial data
incorporated into training cycles, leading to an artificial data ecosystem. To
the best of our knowledge, this is the first study to aggregate various types
of LLM-generated text data, from more tightly constrained data like &quot;task
labels&quot; to more lightly constrained &quot;free-form text&quot;. We then stress test the
quality and implications of LLM-generated artificial data, comparing it with
human data across various existing benchmarks. Despite artificial data&apos;s
capability to match human performance, this paper reveals significant hidden
disparities, especially in complex tasks where LLMs often miss the nuanced
understanding of intrinsic human-generated content. This study critically
examines diverse LLM-generated data and emphasizes the need for ethical
practices in data creation and when using LLMs. It highlights the LLMs&apos;
shortcomings in replicating human traits and behaviors, underscoring the
importance of addressing biases and artifacts produced in LLM-generated content
for future research and development. All data and code are available on our
project page.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1&quot;&gt;Debarati Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langis_K/0/1/0/all/0/1&quot;&gt;Karin De Langis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1&quot;&gt;Anna Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jaehyung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Minhwa Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Z/0/1/0/all/0/1&quot;&gt;Zae Myung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayati_S/0/1/0/all/0/1&quot;&gt;Shirley Hayati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Owan_R/0/1/0/all/0/1&quot;&gt;Risako Owan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Bin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parkar_R/0/1/0/all/0/1&quot;&gt;Ritik Parkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koo_R/0/1/0/all/0/1&quot;&gt;Ryan Koo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jonginn Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tyagi_A/0/1/0/all/0/1&quot;&gt;Aahan Tyagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferland_L/0/1/0/all/0/1&quot;&gt;Libby Ferland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Sanjali Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_V/0/1/0/all/0/1&quot;&gt;Vincent Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1&quot;&gt;Dongyeop Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14702">
<title>FairSample: Training Fair and Accurate Graph Convolutional Neural Networks Efficiently. (arXiv:2401.14702v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14702</link>
<description rdf:parseType="Literal">&lt;p&gt;Fairness in Graph Convolutional Neural Networks (GCNs) becomes a more and
more important concern as GCNs are adopted in many crucial applications.
Societal biases against sensitive groups may exist in many real world graphs.
GCNs trained on those graphs may be vulnerable to being affected by such
biases. In this paper, we adopt the well-known fairness notion of demographic
parity and tackle the challenge of training fair and accurate GCNs efficiently.
We present an in-depth analysis on how graph structure bias, node attribute
bias, and model parameters may affect the demographic parity of GCNs. Our
insights lead to FairSample, a framework that jointly mitigates the three types
of biases. We employ two intuitive strategies to rectify graph structures.
First, we inject edges across nodes that are in different sensitive groups but
similar in node features. Second, to enhance model fairness and retain model
quality, we develop a learnable neighbor sampling policy using reinforcement
learning. To address the bias in node features and model parameters, FairSample
is complemented by a regularization objective to optimize fairness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_Z/0/1/0/all/0/1&quot;&gt;Zicun Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baoxu_S/0/1/0/all/0/1&quot;&gt;Shi Baoxu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jaewon Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1&quot;&gt;Jian Pei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14707">
<title>Mitigating Feature Gap for Adversarial Robustness by Feature Disentanglement. (arXiv:2401.14707v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14707</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are vulnerable to adversarial samples. Adversarial
fine-tuning methods aim to enhance adversarial robustness through fine-tuning
the naturally pre-trained model in an adversarial training manner. However, we
identify that some latent features of adversarial samples are confused by
adversarial perturbation and lead to an unexpectedly increasing gap between
features in the last hidden layer of natural and adversarial samples. To
address this issue, we propose a disentanglement-based approach to explicitly
model and further remove the latent features that cause the feature gap.
Specifically, we introduce a feature disentangler to separate out the latent
features from the features of the adversarial samples, thereby boosting
robustness by eliminating the latent features. Besides, we align features in
the pre-trained model with features of adversarial samples in the fine-tuned
model, to further benefit from the features from natural samples without
confusion. Empirical evaluations on three benchmark datasets demonstrate that
our approach surpasses existing adversarial fine-tuning methods and adversarial
training baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1&quot;&gt;Nuoyan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Dawei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Decheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nannan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14717">
<title>Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion. (arXiv:2401.14717v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14717</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an approach for continuous prediction of turn-taking and
backchanneling locations in spoken dialogue by fusing a neural acoustic model
with a large language model (LLM). Experiments on the Switchboard human-human
conversation dataset demonstrate that our approach consistently outperforms the
baseline models with single modality. We also develop a novel multi-task
instruction fine-tuning strategy to further benefit from LLM-encoded knowledge
for understanding the tasks and conversational contexts, leading to additional
improvements. Our approach demonstrates the potential of combined LLMs and
acoustic models for a more natural and conversational interaction between
humans and speech-enabled AI agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinhan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Long Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khare_A/0/1/0/all/0/1&quot;&gt;Aparna Khare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1&quot;&gt;Anirudh Raju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dheram_P/0/1/0/all/0/1&quot;&gt;Pranav Dheram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1&quot;&gt;Di He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Minhua Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1&quot;&gt;Andreas Stolcke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravichandran_V/0/1/0/all/0/1&quot;&gt;Venkatesh Ravichandran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14743">
<title>Synthetic Multimodal Dataset for Empowering Safety and Well-being in Home Environments. (arXiv:2401.14743v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.14743</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a synthetic multimodal dataset of daily activities that
fuses video data from a 3D virtual space simulator with knowledge graphs
depicting the spatiotemporal context of the activities. The dataset is
developed for the Knowledge Graph Reasoning Challenge for Social Issues
(KGRC4SI), which focuses on identifying and addressing hazardous situations in
the home environment. The dataset is available to the public as a valuable
resource for researchers and practitioners developing innovative solutions
recognizing human behaviors to enhance safety and well-being in
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ugai_T/0/1/0/all/0/1&quot;&gt;Takanori Ugai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Egami_S/0/1/0/all/0/1&quot;&gt;Shusaku Egami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Htun_S/0/1/0/all/0/1&quot;&gt;Swe Nwe Nwe Htun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kozaki_K/0/1/0/all/0/1&quot;&gt;Kouji Kozaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawamura_T/0/1/0/all/0/1&quot;&gt;Takahiro Kawamura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fukuda_K/0/1/0/all/0/1&quot;&gt;Ken Fukuda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14749">
<title>Topology-Aware Exploration of Energy-Based Models Equilibrium: Toric QC-LDPC Codes and Hyperbolic MET QC-LDPC Codes. (arXiv:2401.14749v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2401.14749</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a method for achieving equilibrium in the ISING
Hamiltonian when confronted with unevenly distributed charges on an irregular
grid. Employing (Multi-Edge) QC-LDPC codes and the Boltzmann machine, our
approach involves dimensionally expanding the system, substituting charges with
circulants, and representing distances through circulant shifts. This results
in a systematic mapping of the charge system onto a space, transforming the
irregular grid into a uniform configuration, applicable to Torical and Circular
Hyperboloid Topologies. The paper covers fundamental definitions and notations
related to QC-LDPC Codes, Multi-Edge QC-LDPC codes, and the Boltzmann machine.
It explores the marginalization problem in code on the graph probabilistic
models for evaluating the partition function, encompassing exact and
approximate estimation techniques. Rigorous proof is provided for the
attainability of equilibrium states for the Boltzmann machine under Torical and
Circular Hyperboloid, paving the way for the application of our methodology.
Practical applications of our approach are investigated in Finite Geometry
QC-LDPC Codes, specifically in Material Science. The paper further explores its
effectiveness in the realm of Natural Language Processing Transformer Deep
Neural Networks, examining Generalized Repeat Accumulate Codes,
Spatially-Coupled and Cage-Graph QC-LDPC Codes. The versatile and impactful
nature of our topology-aware hardware-efficient quasi-cycle codes equilibrium
method is showcased across diverse scientific domains without the use of
specific section delineations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Usatyuk_V/0/1/0/all/0/1&quot;&gt;Vasiliy Usatyuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sapozhnikov_D/0/1/0/all/0/1&quot;&gt;Denis Sapozhnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Egorov_S/0/1/0/all/0/1&quot;&gt;Sergey Egorov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14777">
<title>Large Language Model Adaptation for Financial Sentiment Analysis. (arXiv:2401.14777v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14777</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural language processing (NLP) has recently gained relevance within
financial institutions by providing highly valuable insights into companies and
markets&apos; financial documents. However, the landscape of the financial domain
presents extra challenges for NLP, due to the complexity of the texts and the
use of specific terminology. Generalist language models tend to fall short in
tasks specifically tailored for finance, even when using large language models
(LLMs) with great natural language understanding and generative capabilities.
This paper presents a study on LLM adaptation methods targeted at the financial
domain and with high emphasis on financial sentiment analysis. To this purpose,
two foundation models with less than 1.5B parameters have been adapted using a
wide range of strategies. We show that through careful fine-tuning on both
financial documents and instructions, these foundation models can be adapted to
the target domain. Moreover, we observe that small LLMs have comparable
performance to larger scale models, while being more efficient in terms of
parameters and data. In addition to the models, we show how to generate
artificial instructions through LLMs to augment the number of samples of the
instruction dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inserte_P/0/1/0/all/0/1&quot;&gt;Pau Rodriguez Inserte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakhle_M/0/1/0/all/0/1&quot;&gt;Mariam Nakhl&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qader_R/0/1/0/all/0/1&quot;&gt;Raheel Qader&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caillaut_G/0/1/0/all/0/1&quot;&gt;Gaetan Caillaut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingshu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14811">
<title>On the Limitations of Markovian Rewards to Express Multi-Objective, Risk-Sensitive, and Modal Tasks. (arXiv:2401.14811v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.14811</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the expressivity of scalar, Markovian reward
functions in Reinforcement Learning (RL), and identify several limitations to
what they can express. Specifically, we look at three classes of RL tasks;
multi-objective RL, risk-sensitive RL, and modal RL. For each class, we derive
necessary and sufficient conditions that describe when a problem in this class
can be expressed using a scalar, Markovian reward. Moreover, we find that
scalar, Markovian rewards are unable to express most of the instances in each
of these three classes. We thereby contribute to a more complete understanding
of what standard reward functions can and cannot express. In addition to this,
we also call attention to modal problems as a new class of problems, since they
have so far not been given any systematic treatment in the RL literature. We
also briefly outline some approaches for solving some of the problems we
discuss, by means of bespoke RL algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skalse_J/0/1/0/all/0/1&quot;&gt;Joar Skalse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1&quot;&gt;Alessandro Abate&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14831">
<title>The Machine Vision Iceberg Explained: Advancing Dynamic Testing by Considering Holistic Environmental Circumstances. (arXiv:2401.14831v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.14831</link>
<description rdf:parseType="Literal">&lt;p&gt;Are we heading for an iceberg with the current testing of machine vision?
This work delves into the landscape of Machine Vision (MV) testing, which is
heavily required in Highly Automated Driving (HAD) systems. Utilizing the
metaphorical notion of navigating towards an iceberg, we discuss the potential
shortcomings concealed within current testing strategies. We emphasize the
urgent need for a deeper understanding of how to deal with the opaque functions
of MV in development processes. As overlooked considerations can cost lives.
Our main contribution is the hierarchical level model, which we call
Granularity Grades. The model encourages a refined exploration of the
multi-scaled depths of understanding about the circumstances of environments in
which MV is intended to operate. This model aims to provide a holistic overview
of all entities that may impact MV functions, ranging from relations of
individual entities like object attributes to entire environmental scenes. The
application of our model delivers a structured exploration of entities in a
specific domain, their relationships and assigning results of a MV-under-test
to construct an entity-relationship graph. Through clustering patterns of
relations in the graph general MV deficits are arguable. In Summary, our work
contributes to a more nuanced and systematized identification of deficits of a
MV test object in correlation to holistic circumstances in HAD operating
domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padusinski_H/0/1/0/all/0/1&quot;&gt;Hubert Padusinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braun_T/0/1/0/all/0/1&quot;&gt;Thilo Braun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinhauser_C/0/1/0/all/0/1&quot;&gt;Christian Steinhauser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ries_L/0/1/0/all/0/1&quot;&gt;Lennart Ries&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sax_E/0/1/0/all/0/1&quot;&gt;Eric Sax&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14856">
<title>Memory-Inspired Temporal Prompt Interaction for Text-Image Classification. (arXiv:2401.14856v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14856</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, large-scale pre-trained multimodal models (LMM) generally
emerge to integrate the vision and language modalities, achieving considerable
success in various natural language processing and computer vision tasks. The
growing size of LMMs, however, results in a significant computational cost for
fine-tuning these models for downstream tasks. Hence, prompt-based interaction
strategy is studied to align modalities more efficiently. In this contex, we
propose a novel prompt-based multimodal interaction strategy inspired by human
memory strategy, namely Memory-Inspired Temporal Prompt Interaction (MITP). Our
proposed method involves in two stages as in human memory strategy: the
acquiring stage, and the consolidation and activation stage. We utilize
temporal prompts on intermediate layers to imitate the acquiring stage,
leverage similarity-based prompt interaction to imitate memory consolidation,
and employ prompt generation strategy to imitate memory activation. The main
strength of our paper is that we interact the prompt vectors on intermediate
layers to leverage sufficient information exchange between modalities, with
compressed trainable parameters and memory usage. We achieve competitive
results on several datasets with relatively small memory usage and 2.0M of
trainable parameters (about 1% of the pre-trained foundation model).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xinyao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Hao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1&quot;&gt;Rui Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1&quot;&gt;Zhenjia Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yen-Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Lanfen Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14876">
<title>Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes for Alleviating the Over-smoothing Problem. (arXiv:2401.14876v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14876</link>
<description rdf:parseType="Literal">&lt;p&gt;The vanilla Graph Convolutional Network (GCN) uses a low-pass filter to
extract low-frequency signals from graph topology, which may lead to the
over-smoothing problem when GCN goes deep. To this end, various methods have
been proposed to create an adaptive filter by incorporating an extra filter
(e.g., a high-pass filter) extracted from the graph topology. However, these
methods heavily rely on topological information and ignore the node attribute
space, which severely sacrifices the expressive power of the deep GCNs,
especially when dealing with disassortative graphs. In this paper, we propose a
cross-space adaptive filter, called CSF, to produce the adaptive-frequency
information extracted from both the topology and attribute spaces.
Specifically, we first derive a tailored attribute-based high-pass filter that
can be interpreted theoretically as a minimizer for semi-supervised kernel
ridge regression. Then, we cast the topology-based low-pass filter as a
Mercer&apos;s kernel within the context of GCNs. This serves as a foundation for
combining it with the attribute-based filter to capture the adaptive-frequency
information. Finally, we derive the cross-space filter via an effective
multiple-kernel learning strategy, which unifies the attribute-based high-pass
filter and the topology-based low-pass filter. This helps to address the
over-smoothing problem while maintaining effectiveness. Extensive experiments
demonstrate that CSF not only successfully alleviates the over-smoothing
problem but also promotes the effectiveness of the node classification task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1&quot;&gt;Wenqiang Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1&quot;&gt;Jiancheng Lv&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14915">
<title>Charting the Future of AI in Project-Based Learning: A Co-Design Exploration with Students. (arXiv:2401.14915v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.14915</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing use of Artificial Intelligence (AI) by students in learning
presents new challenges for assessing their learning outcomes in project-based
learning (PBL). This paper introduces a co-design study to explore the
potential of students&apos; AI usage data as a novel material for PBL assessment. We
conducted workshops with 18 college students, encouraging them to speculate an
alternative world where they could freely employ AI in PBL while needing to
report this process to assess their skills and contributions. Our workshops
yielded various scenarios of students&apos; use of AI in PBL and ways of analyzing
these uses grounded by students&apos; vision of education goal transformation. We
also found students with different attitudes toward AI exhibited distinct
preferences in how to analyze and understand the use of AI. Based on these
findings, we discuss future research opportunities on student-AI interactions
and understanding AI-enhanced learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chengbo Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1&quot;&gt;Kangyu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1&quot;&gt;Bingcan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mogavi_R/0/1/0/all/0/1&quot;&gt;Reza Hadi Mogavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1&quot;&gt;Zhenhui Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shuai Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaojuan Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14923">
<title>Reinforcement Learning Interventions on Boundedly Rational Human Agents in Frictionful Tasks. (arXiv:2401.14923v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.14923</link>
<description rdf:parseType="Literal">&lt;p&gt;Many important behavior changes are frictionful; they require individuals to
expend effort over a long period with little immediate gratification. Here, an
artificial intelligence (AI) agent can provide personalized interventions to
help individuals stick to their goals. In these settings, the AI agent must
personalize rapidly (before the individual disengages) and interpretably, to
help us understand the behavioral interventions. In this paper, we introduce
Behavior Model Reinforcement Learning (BMRL), a framework in which an AI agent
intervenes on the parameters of a Markov Decision Process (MDP) belonging to a
boundedly rational human agent. Our formulation of the human decision-maker as
a planning agent allows us to attribute undesirable human policies (ones that
do not lead to the goal) to their maladapted MDP parameters, such as an
extremely low discount factor. Furthermore, we propose a class of tractable
human models that captures fundamental behaviors in frictionful tasks.
Introducing a notion of MDP equivalence specific to BMRL, we theoretically and
empirically show that AI planning with our human models can lead to helpful
policies on a wide range of more complex, ground-truth humans.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nofshin_E/0/1/0/all/0/1&quot;&gt;Eura Nofshin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swaroop_S/0/1/0/all/0/1&quot;&gt;Siddharth Swaroop&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1&quot;&gt;Weiwei Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphy_S/0/1/0/all/0/1&quot;&gt;Susan Murphy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1&quot;&gt;Finale Doshi-Velez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14931">
<title>Do LLMs Dream of Ontologies?. (arXiv:2401.14931v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14931</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have recently revolutionized automated text
understanding and generation. The performance of these models relies on the
high number of parameters of the underlying neural architectures, which allows
LLMs to memorize part of the vast quantity of data seen during the training.
This paper investigates whether and to what extent general-purpose pre-trained
LLMs have memorized information from known ontologies. Our results show that
LLMs partially know ontologies: they can, and do indeed, memorize concepts from
ontologies mentioned in the text, but the level of memorization of their
concepts seems to vary proportionally to their popularity on the Web, the
primary source of their training material. We additionally propose new metrics
to estimate the degree of memorization of ontological information in LLMs by
measuring the consistency of the output produced across different prompt
repetitions, query languages, and degrees of determinism.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bombieri_M/0/1/0/all/0/1&quot;&gt;Marco Bombieri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiorini_P/0/1/0/all/0/1&quot;&gt;Paolo Fiorini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ponzetto_S/0/1/0/all/0/1&quot;&gt;Simone Paolo Ponzetto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rospocher_M/0/1/0/all/0/1&quot;&gt;Marco Rospocher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14933">
<title>SSDOnt: an Ontology for representing Single-Subject Design Studies. (arXiv:2401.14933v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.14933</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: Single-Subject Design is used in several areas such as education
and biomedicine. However, no suited formal vocabulary exists for annotating the
detailed configuration and the results of this type of research studies with
the appropriate granularity for looking for information about them. Therefore,
the search for those study designs relies heavily on a syntactical search on
the abstract, keywords or full text of the publications about the study, which
entails some limitations. Objective: To present SSDOnt, a specific purpose
ontology for describing and annotating single-subject design studies, so that
complex questions can be asked about them afterwards. Methods: The ontology was
developed following the NeOn methodology. Once the requirements of the ontology
were defined, a formal model was described in a Description Logic and later
implemented in the ontology language OWL 2 DL. Results: We show how the
ontology provides a reference model with a suitable terminology for the
annotation and searching of single-subject design studies and their main
components, such as the phases, the intervention types, the outcomes and the
results. Some mappings with terms of related ontologies have been established.
We show as proof-of-concept that classes in the ontology can be easily extended
to annotate more precise information about specific interventions and outcomes
such as those related to autism. Moreover, we provide examples of some types of
queries that can be posed to the ontology. Conclusions: SSDOnt has achieved the
purpose of covering the descriptions of the domain of single-subject research
studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berges_I/0/1/0/all/0/1&quot;&gt;Idoia Berges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bermudez_J/0/1/0/all/0/1&quot;&gt;Jes&amp;#xfa;s Berm&amp;#xfa;dez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Illarramendi_A/0/1/0/all/0/1&quot;&gt;Arantza Illarramendi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14948">
<title>Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training. (arXiv:2401.14948v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14948</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial training improves the robustness of neural networks against
adversarial attacks, albeit at the expense of the trade-off between standard
and robust generalization. To unveil the underlying factors driving this
phenomenon, we examine the layer-wise learning capabilities of neural networks
during the transition from a standard to an adversarial setting. Our empirical
findings demonstrate that selectively updating specific layers while preserving
others can substantially enhance the network&apos;s learning capacity. We therefore
propose CURE, a novel training framework that leverages a gradient prominence
criterion to perform selective conservation, updating, and revision of weights.
Importantly, CURE is designed to be dataset- and architecture-agnostic,
ensuring its applicability across various scenarios. It effectively tackles
both memorization and overfitting issues, thus enhancing the trade-off between
robustness and generalization and additionally, this training approach also
aids in mitigating &quot;robust overfitting&quot;. Furthermore, our study provides
valuable insights into the mechanisms of selective adversarial training and
offers a promising avenue for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1&quot;&gt;Shruthi Gowda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1&quot;&gt;Bahram Zonooz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1&quot;&gt;Elahe Arani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14953">
<title>Learning Universal Predictors. (arXiv:2401.14953v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14953</link>
<description rdf:parseType="Literal">&lt;p&gt;Meta-learning has emerged as a powerful approach to train neural networks to
learn new tasks quickly from limited data. Broad exposure to different tasks
leads to versatile representations enabling general problem solving. But, what
are the limits of meta-learning? In this work, we explore the potential of
amortizing the most powerful universal predictor, namely Solomonoff Induction
(SI), into neural networks via leveraging meta-learning to its limits. We use
Universal Turing Machines (UTMs) to generate training data used to expose
networks to a broad range of patterns. We provide theoretical analysis of the
UTM data generation processes and meta-training protocols. We conduct
comprehensive experiments with neural architectures (e.g. LSTMs, Transformers)
and algorithmic data generators of varying complexity and universality. Our
results suggest that UTM data is a valuable resource for meta-learning, and
that it can be used to train neural networks capable of learning universal
prediction strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grau_Moya_J/0/1/0/all/0/1&quot;&gt;Jordi Grau-Moya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Genewein_T/0/1/0/all/0/1&quot;&gt;Tim Genewein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1&quot;&gt;Marcus Hutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orseau_L/0/1/0/all/0/1&quot;&gt;Laurent Orseau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deletang_G/0/1/0/all/0/1&quot;&gt;Gr&amp;#xe9;goire Del&amp;#xe9;tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Catt_E/0/1/0/all/0/1&quot;&gt;Elliot Catt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruoss_A/0/1/0/all/0/1&quot;&gt;Anian Ruoss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wenliang_L/0/1/0/all/0/1&quot;&gt;Li Kevin Wenliang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mattern_C/0/1/0/all/0/1&quot;&gt;Christopher Mattern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aitchison_M/0/1/0/all/0/1&quot;&gt;Matthew Aitchison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veness_J/0/1/0/all/0/1&quot;&gt;Joel Veness&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14968">
<title>Atmosphere: Context and situational-aware collaborative IoT architecture for edge-fog-cloud computing. (arXiv:2401.14968v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2401.14968</link>
<description rdf:parseType="Literal">&lt;p&gt;The Internet of Things (IoT) has grown significantly in popularity,
accompanied by increased capacity and lower cost of communications, and
overwhelming development of technologies. At the same time, big data and
real-time data analysis have taken on great importance and have been
accompanied by unprecedented interest in sharing data among citizens, public
administrations and other organisms, giving rise to what is known as the
Collaborative Internet of Things. This growth in data and infrastructure must
be accompanied by a software architecture that allows its exploitation.
Although there are various proposals focused on the exploitation of the IoT at
edge, fog and/or cloud levels, it is not easy to find a software solution that
exploits the three tiers together, taking maximum advantage not only of the
analysis of contextual and situational data at each tier, but also of two-way
communications between adjacent ones. In this paper, we propose an architecture
that solves these deficiencies by proposing novel technologies which are
appropriate for managing the resources of each tier: edge, fog and cloud. In
addition, the fact that two-way communications along the three tiers of the
architecture is allowed considerably enriches the contextual and situational
information in each layer, and substantially assists decision making in real
time. The paper illustrates the proposed software architecture through a case
study of respiratory disease surveillance in hospitals. As a result, the
proposed architecture permits efficient communications between the different
tiers responding to the needs of these types of IoT scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortiz_G/0/1/0/all/0/1&quot;&gt;Guadalupe Ortiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zouai_M/0/1/0/all/0/1&quot;&gt;Meftah Zouai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazar_O/0/1/0/all/0/1&quot;&gt;Okba Kazar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_de_Prado_A/0/1/0/all/0/1&quot;&gt;Alfonso Garcia-de-Prado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boubeta_Puig_J/0/1/0/all/0/1&quot;&gt;Juan Boubeta-Puig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.15006">
<title>Airavata: Introducing Hindi Instruction-tuned LLM. (arXiv:2401.15006v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.15006</link>
<description rdf:parseType="Literal">&lt;p&gt;We announce the initial release of &quot;Airavata,&quot; an instruction-tuned LLM for
Hindi. Airavata was created by fine-tuning OpenHathi with diverse,
instruction-tuning Hindi datasets to make it better suited for assistive tasks.
Along with the model, we also share the IndicInstruct dataset, which is a
collection of diverse instruction-tuning datasets to enable further research
for Indic LLMs. Additionally, we present evaluation benchmarks and a framework
for assessing LLM performance across tasks in Hindi. Currently, Airavata
supports Hindi, but we plan to expand this to all 22 scheduled Indic languages.
You can access all artifacts at https://ai4bharat.github.io/airavata.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gala_J/0/1/0/all/0/1&quot;&gt;Jay Gala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayakumar_T/0/1/0/all/0/1&quot;&gt;Thanmay Jayakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Husain_J/0/1/0/all/0/1&quot;&gt;Jaavid Aktar Husain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+M_A/0/1/0/all/0/1&quot;&gt;Aswanth Kumar M&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Mohammed Safi Ur Rahman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1&quot;&gt;Diptesh Kanojia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puduppully_R/0/1/0/all/0/1&quot;&gt;Ratish Puduppully&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1&quot;&gt;Mitesh M. Khapra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1&quot;&gt;Raj Dabre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murthy_R/0/1/0/all/0/1&quot;&gt;Rudra Murthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1&quot;&gt;Anoop Kunchukuttan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.15042">
<title>PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models. (arXiv:2401.15042v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.15042</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have exhibited remarkable success in long-form
context comprehension tasks. However, their capacity to generate long contents,
such as reports and articles, remains insufficiently explored. Current
benchmarks do not adequately assess LLMs&apos; ability to produce informative and
comprehensive content, necessitating a more rigorous evaluation approach. In
this study, we introduce \textsc{ProxyQA}, a framework for evaluating long-form
text generation, comprising in-depth human-curated \textit{meta-questions}
spanning various domains. Each meta-question contains corresponding
\textit{proxy-questions} with annotated answers. LLMs are prompted to generate
extensive content in response to these meta-questions. Utilizing an evaluator
and incorporating generated content as background context, \textsc{ProxyQA}
evaluates the quality of generated content based on the evaluator&apos;s performance
in answering the \textit{proxy-questions}. We examine multiple LLMs,
emphasizing \textsc{ProxyQA}&apos;s demanding nature as a high-quality assessment
tool. Human evaluation demonstrates that evaluating through
\textit{proxy-questions} is a highly self-consistent and
human-criteria-correlated validation method. The dataset and leaderboard will
be available at \url{https://github.com/Namco0816/ProxyQA}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1&quot;&gt;Haochen Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhijiang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhili Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoguang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yasheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1&quot;&gt;Lifeng Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Linqi Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.15043">
<title>Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning. (arXiv:2401.15043v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.15043</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: The reading level of health educational materials significantly
influences information understandability and accessibility, particularly for
minoritized populations. Many patient educational resources surpass the reading
level and complexity of widely accepted standards. There is a critical need for
high-performing text simplification models in health information to enhance
dissemination and literacy. This need is particularly acute in cancer
education, where effective prevention and screening education can substantially
reduce morbidity and mortality.
&lt;/p&gt;
&lt;p&gt;Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel
corpus of cancer education materials tailored for health text simplification
research. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore
Large Language Model (LLM)-based simplification methods, including fine-tuning,
reinforcement learning (RL), reinforcement learning with human feedback (RLHF),
domain adaptation, and prompt-based approaches. Our experimentation encompasses
Llama 2 and GPT-4. A novel RLHF reward function is introduced, featuring a
lightweight model adept at distinguishing between original and simplified
texts, thereby enhancing the model&apos;s effectiveness with unlabeled data.
&lt;/p&gt;
&lt;p&gt;Results: Fine-tuned Llama 2 models demonstrated high performance across
various metrics. Our innovative RLHF reward function surpassed existing RL text
simplification reward functions in effectiveness. The results underscore that
RL/RLHF can augment fine-tuning, facilitating model training on unlabeled text
and improving performance. Additionally, these methods effectively adapt
out-of-domain text simplification models to targeted domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Md Mushfiqur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1&quot;&gt;Mohammad Sabik Irbaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+North_K/0/1/0/all/0/1&quot;&gt;Kai North&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_M/0/1/0/all/0/1&quot;&gt;Michelle S. Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1&quot;&gt;Marcos Zampieri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lybarger_K/0/1/0/all/0/1&quot;&gt;Kevin Lybarger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.15075">
<title>Annotated Hands for Generative Models. (arXiv:2401.15075v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.15075</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models such as GANs and diffusion models have demonstrated
impressive image generation capabilities. Despite these successes, these
systems are surprisingly poor at creating images with hands. We propose a novel
training framework for generative models that substantially improves the
ability of such systems to create hand images. Our approach is to augment the
training images with three additional channels that provide annotations to
hands in the image. These annotations provide additional structure that coax
the generative model to produce higher quality hand images. We demonstrate this
approach on two different generative models: a generative adversarial network
and a diffusion model. We demonstrate our method both on a new synthetic
dataset of hand images and also on real photographs that contain hands. We
measure the improved quality of the generated hands through higher confidence
in finger joint identification using an off-the-shelf hand detector.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yue Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandhi_A/0/1/0/all/0/1&quot;&gt;Atith N Gandhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turk_G/0/1/0/all/0/1&quot;&gt;Greg Turk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.08452">
<title>On minimizers and convolutional filters: theoretical connections and applications to genome analysis. (arXiv:2111.08452v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2111.08452</link>
<description rdf:parseType="Literal">&lt;p&gt;Minimizers and convolutional neural networks (CNNs) are two quite distinct
popular techniques that have both been employed to analyze categorical
biological sequences. At face value, the methods seem entirely dissimilar.
Minimizers use min-wise hashing on a rolling window to extract a single
important k-mer feature per window. CNNs start with a wide array of randomly
initialized convolutional filters, paired with a pooling operation, and then
multiple additional neural layers to learn both the filters themselves and how
they can be used to classify the sequence.
&lt;/p&gt;
&lt;p&gt;Here, our main result is a careful mathematical analysis of hash function
properties showing that for sequences over a categorical alphabet, random
Gaussian initialization of convolutional filters with max-pooling is equivalent
to choosing a minimizer ordering such that selected k-mers are (in Hamming
distance) far from the k-mers within the sequence but close to other
minimizers. In empirical experiments, we find that this property manifests as
decreased density in repetitive regions, both in simulation and on real human
telomeres. We additionally train from scratch a CNN embedding of synthetic
short-reads from the SARS-CoV-2 genome into 3D Euclidean space that locally
recapitulates the linear sequence distance of the read origins, a modest step
towards building a deep learning assembler, though it is at present too slow to
be practical. In total, this manuscript provides a partial explanation for the
effectiveness of CNNs in categorical sequence analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yun William Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.03314">
<title>A Robot Web for Distributed Many-Device Localisation. (arXiv:2202.03314v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2202.03314</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that a distributed network of robots or other devices which make
measurements of each other can collaborate to globally localise via efficient
ad-hoc peer to peer communication. Our Robot Web solution is based on Gaussian
Belief Propagation on the fundamental non-linear factor graph describing the
probabilistic structure of all of the observations robots make internally or of
each other, and is flexible for any type of robot, motion or sensor. We define
a simple and efficient communication protocol which can be implemented by the
publishing and reading of web pages or other asynchronous communication
technologies. We show in simulations with up to 1000 robots interacting in
arbitrary patterns that our solution convergently achieves global accuracy as
accurate as a centralised non-linear factor graph solver while operating with
high distributed efficiency of computation and communication. Via the use of
robust factors in GBP, our method is tolerant to a high percentage of faults in
sensor measurements or dropped communication packets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murai_R/0/1/0/all/0/1&quot;&gt;Riku Murai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortiz_J/0/1/0/all/0/1&quot;&gt;Joseph Ortiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saeedi_S/0/1/0/all/0/1&quot;&gt;Sajad Saeedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kelly_P/0/1/0/all/0/1&quot;&gt;Paul H.J. Kelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1&quot;&gt;Andrew J. Davison&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.16468">
<title>Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs. (arXiv:2211.16468v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2211.16468</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal effect estimation from observational data is a fundamental task in
empirical sciences. It becomes particularly challenging when unobserved
confounders are involved in a system. This paper focuses on front-door
adjustment -- a classic technique which, using observed mediators allows to
identify causal effects even in the presence of unobserved confounding. While
the statistical properties of the front-door estimation are quite well
understood, its algorithmic aspects remained unexplored for a long time. In
2022, Jeong, Tian, and Bareinboim presented the first polynomial-time algorithm
for finding sets satisfying the front-door criterion in a given directed
acyclic graph (DAG), with an $O(n^3(n+m))$ run time, where $n$ denotes the
number of variables and $m$ the number of edges of the causal graph. In our
work, we give the first linear-time, i.e., $O(n+m)$, algorithm for this task,
which thus reaches the asymptotically optimal time complexity. This result
implies an $O(n(n+m))$ delay enumeration algorithm of all front-door adjustment
sets, again improving previous work by a factor of $n^3$. Moreover, we provide
the first linear-time algorithm for finding a minimal front-door adjustment
set. We offer implementations of our algorithms in multiple programming
languages to facilitate practical usage and empirically validate their
feasibility, even for large graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wienobst_M/0/1/0/all/0/1&quot;&gt;Marcel Wien&amp;#xf6;bst&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zander_B/0/1/0/all/0/1&quot;&gt;Benito van der Zander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liskiewicz_M/0/1/0/all/0/1&quot;&gt;Maciej Li&amp;#x15b;kiewicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.04582">
<title>Towards Holistic Surgical Scene Understanding. (arXiv:2212.04582v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.04582</link>
<description rdf:parseType="Literal">&lt;p&gt;Most benchmarks for studying surgical interventions focus on a specific
challenge instead of leveraging the intrinsic complementarity among different
tasks. In this work, we present a new experimental framework towards holistic
surgical scene understanding. First, we introduce the Phase, Step, Instrument,
and Atomic Visual Action recognition (PSI-AVA) Dataset. PSI-AVA includes
annotations for both long-term (Phase and Step recognition) and short-term
reasoning (Instrument detection and novel Atomic Action recognition) in
robot-assisted radical prostatectomy videos. Second, we present Transformers
for Action, Phase, Instrument, and steps Recognition (TAPIR) as a strong
baseline for surgical scene understanding. TAPIR leverages our dataset&apos;s
multi-level annotations as it benefits from the learned representation on the
instrument detection task to improve its classification capacity. Our
experimental results in both PSI-AVA and other publicly available databases
demonstrate the adequacy of our framework to spur future research on holistic
surgical scene understanding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valderrama_N/0/1/0/all/0/1&quot;&gt;Natalia Valderrama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puentes_P/0/1/0/all/0/1&quot;&gt;Paola Ruiz Puentes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_I/0/1/0/all/0/1&quot;&gt;Isabela Hern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayobi_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xe1;s Ayobi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verlyk_M/0/1/0/all/0/1&quot;&gt;Mathilde Verlyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santander_J/0/1/0/all/0/1&quot;&gt;Jessica Santander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caicedo_J/0/1/0/all/0/1&quot;&gt;Juan Caicedo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xe1;s Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1&quot;&gt;Pablo Arbel&amp;#xe1;ez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08560">
<title>Dual RL: Unification and New Methods for Reinforcement and Imitation Learning. (arXiv:2302.08560v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08560</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of reinforcement learning (RL) is to find a policy that maximizes
the expected cumulative return. It has been shown that this objective can be
represented as an optimization problem of state-action visitation distribution
under linear constraints. The dual problem of this formulation, which we refer
to as dual RL, is unconstrained and easier to optimize. In this work, we first
cast several state-of-the-art offline RL and offline imitation learning (IL)
algorithms as instances of dual RL approaches with shared structures. Such
unification allows us to identify the root cause of the shortcomings of prior
methods. For offline IL, our analysis shows that prior methods are based on a
restrictive coverage assumption that greatly limits their performance in
practice. To fix this limitation, we propose a new discriminator-free method
ReCOIL that learns to imitate from arbitrary off-policy data to obtain
near-expert performance. For offline RL, our analysis frames a recent offline
RL method XQL in the dual framework, and we further propose a new method f-DVL
that provides alternative choices to the Gumbel regression loss that fixes the
known training instability issue of XQL. The performance improvements by both
of our proposed methods, ReCOIL and f-DVL, in IL and RL are validated on an
extensive suite of simulated robot locomotion and manipulation tasks. Project
code and details can be found at this https://hari-sikchi.github.io/dual-rl.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sikchi_H/0/1/0/all/0/1&quot;&gt;Harshit Sikchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qinqing Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Amy Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1&quot;&gt;Scott Niekum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03714">
<title>Generative Modeling with Flow-Guided Density Ratio Learning. (arXiv:2303.03714v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03714</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Flow-Guided Density Ratio Learning (FDRL), a simple and scalable
approach to generative modeling which builds on the stale (time-independent)
approximation of the gradient flow of entropy-regularized f-divergences
introduced in DGflow. In DGflow, the intractable time-dependent density ratio
is approximated by a stale estimator given by a GAN discriminator. This is
sufficient in the case of sample refinement, where the source and target
distributions of the flow are close to each other. However, this assumption is
invalid for generation and a naive application of the stale estimator fails due
to the large chasm between the two distributions. FDRL proposes to train a
density ratio estimator such that it learns from progressively improving
samples during the training process. We show that this simple method alleviates
the density chasm problem, allowing FDRL to generate images of dimensions as
high as $128\times128$, as well as outperform existing gradient flow baselines
on quantitative benchmarks. We also show the flexibility of FDRL with two use
cases. First, unconditional FDRL can be easily composed with external
classifiers to perform class-conditional generation. Second, FDRL can be
directly applied to unpaired image-to-image translation with no modifications
needed to the framework. Code is publicly available at
https://github.com/ajrheng/FDRL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heng_A/0/1/0/all/0/1&quot;&gt;Alvin Heng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ansari_A/0/1/0/all/0/1&quot;&gt;Abdul Fatir Ansari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soh_H/0/1/0/all/0/1&quot;&gt;Harold Soh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07250">
<title>Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments. (arXiv:2304.07250v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07250</link>
<description rdf:parseType="Literal">&lt;p&gt;The localization of objects is a crucial task in various applications such as
robotics, virtual and augmented reality, and the transportation of goods in
warehouses. Recent advances in deep learning have enabled the localization
using monocular visual cameras. While structure from motion (SfM) predicts the
absolute pose from a point cloud, absolute pose regression (APR) methods learn
a semantic understanding of the environment through neural networks. However,
both fields face challenges caused by the environment such as motion blur,
lighting changes, repetitive patterns, and feature-less structures. This study
aims to address these challenges by incorporating additional information and
regularizing the absolute pose using relative pose regression (RPR) methods.
RPR methods suffer under different challenges, i.e., motion blur. The optical
flow between consecutive images is computed using the Lucas-Kanade algorithm,
and the relative pose is predicted using an auxiliary small recurrent
convolutional network. The fusion of absolute and relative poses is a complex
task due to the mismatch between the global and local coordinate systems.
State-of-the-art methods fusing absolute and relative poses use pose graph
optimization (PGO) to regularize the absolute pose predictions using relative
poses. In this work, we propose recurrent fusion networks to optimally align
absolute and relative pose predictions to improve the absolute pose prediction.
We evaluate eight different recurrent units and construct a simulation
environment to pre-train the APR and RPR networks for better generalized
training. Additionally, we record a large database of different scenarios in a
challenging large-scale indoor environment that mimics a warehouse with
transportation robots. We conduct hyperparameter searches and experiments to
show the effectiveness of our recurrent fusion method compared to PGO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ott_F/0/1/0/all/0/1&quot;&gt;Felix Ott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heublein_L/0/1/0/all/0/1&quot;&gt;Lucas Heublein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1&quot;&gt;David R&amp;#xfc;gamer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1&quot;&gt;Bernd Bischl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mutschler_C/0/1/0/all/0/1&quot;&gt;Christopher Mutschler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.01747">
<title>Expectation Maximization Pseudo Labels. (arXiv:2305.01747v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.01747</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study pseudo-labelling. Pseudo-labelling employs raw
inferences on unlabelled data as pseudo-labels for self-training. We elucidate
the empirical successes of pseudo-labelling by establishing a link between this
technique and the Expectation Maximisation algorithm. Through this, we realise
that the original pseudo-labelling serves as an empirical estimation of its
more comprehensive underlying formulation. Following this insight, we present a
full generalisation of pseudo-labels under Bayes&apos; theorem, termed Bayesian
Pseudo Labels. Subsequently, we introduce a variational approach to generate
these Bayesian Pseudo Labels, involving the learning of a threshold to
automatically select high-quality pseudo labels. In the remainder of the paper,
we showcase the applications of pseudo-labelling and its generalised form,
Bayesian Pseudo-Labelling, in the semi-supervised segmentation of medical
images. Specifically, we focus on: 1) 3D binary segmentation of lung vessels
from CT volumes; 2) 2D multi-class segmentation of brain tumours from MRI
volumes; 3) 3D binary segmentation of whole brain tumours from MRI volumes; and
4) 3D binary segmentation of prostate from MRI volumes. We further demonstrate
that pseudo-labels can enhance the robustness of the learned representations.
The code is released in the following GitHub repository:
https://github.com/moucheng2017/EMSSL
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Moucheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yukun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Chen Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groot_M/0/1/0/all/0/1&quot;&gt;Marius de Groot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexander_D/0/1/0/all/0/1&quot;&gt;Daniel C. Alexander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oxtoby_N/0/1/0/all/0/1&quot;&gt;Neil P. Oxtoby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yipeng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacob_J/0/1/0/all/0/1&quot;&gt;Joseph Jacob&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09802">
<title>Sasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models. (arXiv:2305.09802v3 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09802</link>
<description rdf:parseType="Literal">&lt;p&gt;Smart home assistants function best when user commands are direct and
well-specified (e.g., &quot;turn on the kitchen light&quot;), or when a hard-coded
routine specifies the response. In more natural communication, however, human
speech is unconstrained, often describing goals (e.g., &quot;make it cozy in here&quot;
or &quot;help me save energy&quot;) rather than indicating specific target devices and
actions to take on those devices. Current systems fail to understand these
under-specified commands since they cannot reason about devices and settings as
they relate to human situations. We introduce large language models (LLMs) to
this problem space, exploring their use for controlling devices and creating
automation routines in response to under-specified user commands in smart
homes. We empirically study the baseline quality and failure modes of
LLM-created action plans with a survey of age-diverse users. We find that LLMs
can reason creatively to achieve challenging goals, but they experience
patterns of failure that diminish their usefulness. We address these gaps with
Sasha, a smarter smart home assistant. Sasha responds to loosely-constrained
commands like &quot;make it cozy&quot; or &quot;help me sleep better&quot; by executing plans to
achieve user goals, e.g., setting a mood with available devices, or devising
automation routines. We implement and evaluate Sasha in a hands-on user study,
showing the capabilities and limitations of LLM-driven smart homes when faced
with unconstrained user-generated scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+King_E/0/1/0/all/0/1&quot;&gt;Evan King&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haoxiang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sangsu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Julien_C/0/1/0/all/0/1&quot;&gt;Christine Julien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19956">
<title>MicroSegNet: A Deep Learning Approach for Prostate Segmentation on Micro-Ultrasound Images. (arXiv:2305.19956v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19956</link>
<description rdf:parseType="Literal">&lt;p&gt;Micro-ultrasound (micro-US) is a novel 29-MHz ultrasound technique that
provides 3-4 times higher resolution than traditional ultrasound, potentially
enabling low-cost, accurate diagnosis of prostate cancer. Accurate prostate
segmentation is crucial for prostate volume measurement, cancer diagnosis,
prostate biopsy, and treatment planning. However, prostate segmentation on
micro-US is challenging due to artifacts and indistinct borders between the
prostate, bladder, and urethra in the midline. This paper presents MicroSegNet,
a multi-scale annotation-guided transformer UNet model designed specifically to
tackle these challenges. During the training process, MicroSegNet focuses more
on regions that are hard to segment (hard regions), characterized by
discrepancies between expert and non-expert annotations. We achieve this by
proposing an annotation-guided binary cross entropy (AG-BCE) loss that assigns
a larger weight to prediction errors in hard regions and a lower weight to
prediction errors in easy regions. The AG-BCE loss was seamlessly integrated
into the training process through the utilization of multi-scale deep
supervision, enabling MicroSegNet to capture global contextual dependencies and
local information at various scales. We trained our model using micro-US images
from 55 patients, followed by evaluation on 20 patients. Our MicroSegNet model
achieved a Dice coefficient of 0.939 and a Hausdorff distance of 2.02 mm,
outperforming several state-of-the-art segmentation methods, as well as three
human annotators with different experience levels. Our code is publicly
available at https://github.com/mirthAI/MicroSegNet and our dataset is publicly
available at https://zenodo.org/records/10475293.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hongxu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1&quot;&gt;Muhammad Imran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muralidharan_P/0/1/0/all/0/1&quot;&gt;Preethika Muralidharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1&quot;&gt;Anjali Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pensa_J/0/1/0/all/0/1&quot;&gt;Jake Pensa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1&quot;&gt;Muxuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benidir_T/0/1/0/all/0/1&quot;&gt;Tarik Benidir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grajo_J/0/1/0/all/0/1&quot;&gt;Joseph R. Grajo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joseph_J/0/1/0/all/0/1&quot;&gt;Jason P. Joseph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terry_R/0/1/0/all/0/1&quot;&gt;Russell Terry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DiBianco_J/0/1/0/all/0/1&quot;&gt;John Michael DiBianco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1&quot;&gt;Li-Ming Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuyin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brisbane_W/0/1/0/all/0/1&quot;&gt;Wayne G. Brisbane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1&quot;&gt;Wei Shao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02766">
<title>Networked Communication for Decentralised Agents in Mean-Field Games. (arXiv:2306.02766v2 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02766</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce networked communication to the mean-field game framework, in
particular to oracle-free settings where $N$ decentralised agents learn along a
single, non-episodic evolution path of the empirical system. We prove that our
architecture, with only a few reasonable assumptions about network structure,
has sample guarantees bounded between those of the centralised- and
independent-learning cases. We discuss how the sample guarantees of the three
theoretical algorithms do not actually result in practical convergence.
Accordingly, we show that in practical settings where the theoretical
parameters are not observed (leading to poor estimation of the Q-function), our
communication scheme significantly accelerates convergence over the independent
case, without relying on the undesirable assumption of a centralised
controller. We contribute several further practical enhancements to all three
theoretical algorithms, allowing us to showcase their first empirical
demonstrations. Our experiments confirm that we can remove several of the key
theoretical assumptions of the algorithms, and display the empirical
convergence benefits brought by our new networked communication. We
additionally show that the networked approach has significant advantages, over
both the centralised and independent alternatives, in terms of robustness to
unexpected learning failures and to changes in population size.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benjamin_P/0/1/0/all/0/1&quot;&gt;Patrick Benjamin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1&quot;&gt;Alessandro Abate&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05412">
<title>Decoupled Prioritized Resampling for Offline RL. (arXiv:2306.05412v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05412</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline reinforcement learning (RL) is challenged by the distributional shift
problem. To address this problem, existing works mainly focus on designing
sophisticated policy constraints between the learned policy and the behavior
policy. However, these constraints are applied equally to well-performing and
inferior actions through uniform sampling, which might negatively affect the
learned policy. To alleviate this issue, we propose Offline Prioritized
Experience Replay (OPER), featuring a class of priority functions designed to
prioritize highly-rewarding transitions, making them more frequently visited
during training. Through theoretical analysis, we show that this class of
priority functions induce an improved behavior policy, and when constrained to
this improved policy, a policy-constrained offline RL algorithm is likely to
yield a better solution. We develop two practical strategies to obtain priority
weights by estimating advantages based on a fitted value network (OPER-A) or
utilizing trajectory returns (OPER-R) for quick computation. OPER is a
plug-and-play component for offline RL algorithms. As case studies, we evaluate
OPER on five different algorithms, including BC, TD3+BC, Onestep RL, CQL, and
IQL. Extensive experiments demonstrate that both OPER-A and OPER-R
significantly improve the performance for all baseline methods. Codes and
priority weights are availiable at https://github.com/sail-sg/OPER.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yang Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1&quot;&gt;Bingyi Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qisen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Gao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shiji Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shuicheng Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05879">
<title>FedWon: Triumphing Multi-domain Federated Learning Without Normalization. (arXiv:2306.05879v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05879</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) enhances data privacy with collaborative in-situ
training on decentralized clients. Nevertheless, FL encounters challenges due
to non-independent and identically distributed (non-i.i.d) data, leading to
potential performance degradation and hindered convergence. While prior studies
predominantly addressed the issue of skewed label distribution, our research
addresses a crucial yet frequently overlooked problem known as multi-domain FL.
In this scenario, clients&apos; data originate from diverse domains with distinct
feature distributions, instead of label distributions. To address the
multi-domain problem in FL, we propose a novel method called Federated learning
Without normalizations (FedWon). FedWon draws inspiration from the observation
that batch normalization (BN) faces challenges in effectively modeling the
statistics of multiple domains, while existing normalization techniques possess
their own limitations. In order to address these issues, FedWon eliminates the
normalization layers in FL and reparameterizes convolution layers with scaled
weight standardization. Through extensive experimentation on five datasets and
five models, our comprehensive experimental results demonstrate that FedWon
surpasses both FedAvg and the current state-of-the-art method (FedBN) across
all experimental setups, achieving notable accuracy improvements of more than
10% in certain domains. Furthermore, FedWon is versatile for both cross-silo
and cross-device FL, exhibiting robust domain generalization capability,
showcasing strong performance even with a batch size as small as 1, thereby
catering to resource-constrained devices. Additionally, FedWon can also
effectively tackle the challenge of skewed label distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1&quot;&gt;Weiming Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1&quot;&gt;Lingjuan Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11305">
<title>Progressive Fourier Neural Representation for Sequential Video Compilation. (arXiv:2306.11305v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11305</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Implicit Representation (NIR) has recently gained significant
attention due to its remarkable ability to encode complex and high-dimensional
data into representation space and easily reconstruct it through a trainable
mapping function. However, NIR methods assume a one-to-one mapping between the
target data and representation models regardless of data relevancy or
similarity. This results in poor generalization over multiple complex data and
limits their efficiency and scalability. Motivated by continual learning, this
work investigates how to accumulate and transfer neural implicit
representations for multiple complex video data over sequential encoding
sessions. To overcome the limitation of NIR, we propose a novel method,
Progressive Fourier Neural Representation (PFNR), that aims to find an adaptive
and compact sub-module in Fourier space to encode videos in each training
session. This sparsified neural encoding allows the neural network to hold free
weights, enabling an improved adaptation for future videos. In addition, when
learning a representation for a new video, PFNR transfers the representation of
previous videos with frozen weights. This design allows the model to
continuously accumulate high-quality neural representations for multiple videos
while ensuring lossless decoding that perfectly preserves the learned
representations for previous videos. We validate our PFNR method on the UVG8/17
and DAVIS50 video sequence benchmarks and achieve impressive performance gains
over strong continual learning baselines. The PFNR code is available at
https://github.com/ihaeyong/PFNR.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1&quot;&gt;Haeyong Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jaehong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;DaHyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sung Ju Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1&quot;&gt;Chang D Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10219">
<title>Exploring Link Prediction over Hyper-Relational Temporal Knowledge Graphs Enhanced with Time-Invariant Relational Knowledge. (arXiv:2307.10219v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10219</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been an increasing interest in studying graph reasoning over
hyper-relational KGs (HKGs). Compared with traditional knowledge graphs (KGs),
HKGs introduce additional factual information in the form of qualifiers
(key-value pairs) for each KG fact that helps to better restrict the fact
validity. Meanwhile, due to the ever-evolving nature of world knowledge,
extensive parallel works have been studying temporal KG (TKG) reasoning. Each
TKG fact can be viewed as a KG fact coupled with a timestamp (or time period)
specifying its time validity. The existing HKG reasoning approaches do not
consider temporal information because it is not explicitly specified in
previous benchmark datasets. Besides, traditional TKG reasoning methods only
focus on temporal reasoning and have no way to learn from qualifiers. To this
end, we aim to fill the gap between TKG and HKG reasoning. We develop two new
benchmark hyper-relational TKG (HTKG) datasets, i.e., Wiki-hy and YAGO-hy, and
propose an HTKG reasoning model that efficiently models both temporal facts and
qualifiers. We further exploit additional time-invariant relational knowledge
from the Wikidata knowledge base to improve HTKG reasoning. Time-invariant
relational knowledge serves as the knowledge that remains unchanged in time
(e.g., Sasha Obama is the child of Barack Obama). Experimental results show
that our model achieves strong performance on HTKG link prediction and can be
enhanced by jointly leveraging both temporal and time-invariant relational
knowledge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Zifeng Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jingcheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jingpei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1&quot;&gt;Volker Tresp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12044">
<title>A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12044</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparsity is a highly desired feature in deep neural networks (DNNs) since it
ensures numerical efficiency, improves the interpretability of models (due to
the smaller number of relevant features), and robustness. In machine learning
approaches based on linear models, it is well known that there exists a
connecting path between the sparsest solution in terms of the $\ell^1$
norm,i.e., zero weights and the non-regularized solution, which is called the
regularization path. Very recently, there was a first attempt to extend the
concept of regularization paths to DNNs by means of treating the empirical loss
and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the
resulting multiobjective optimization problem. However, due to the
non-smoothness of the $\ell^1$ norm and the high number of parameters, this
approach is not very efficient from a computational perspective. To overcome
this limitation, we present an algorithm that allows for the approximation of
the entire Pareto front for the above-mentioned objectives in a very efficient
manner. We present numerical examples using both deterministic and stochastic
gradients. We furthermore demonstrate that knowledge of the regularization path
allows for a well-generalizing network parametrization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amakor_A/0/1/0/all/0/1&quot;&gt;Augustina C. Amakor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonntag_K/0/1/0/all/0/1&quot;&gt;Konstantin Sonntag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peitz_S/0/1/0/all/0/1&quot;&gt;Sebastian Peitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12060">
<title>FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering. (arXiv:2308.12060v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12060</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge base question answering (KBQA) is a critical yet challenging task
due to the vast number of entities within knowledge bases and the diversity of
natural language questions posed by users. Unfortunately, the performance of
most KBQA models tends to decline significantly in real-world scenarios where
high-quality annotated data is insufficient. To mitigate the burden associated
with manual annotation, we introduce FlexKBQA by utilizing Large Language
Models (LLMs) as program translators for addressing the challenges inherent in
the few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms
to sample diverse programs, such as SPARQL queries, from the knowledge base,
which are subsequently converted into natural language questions via LLMs. This
synthetic dataset facilitates training a specialized lightweight model for the
KB. Additionally, to reduce the barriers of distribution shift between
synthetic data and real user questions, FlexKBQA introduces an executionguided
self-training method to iterative leverage unlabeled user questions.
Furthermore, we explore harnessing the inherent reasoning capability of LLMs to
enhance the entire framework. Consequently, FlexKBQA delivers substantial
flexibility, encompassing data annotation, deployment, and being domain
agnostic. Through extensive experiments on GrailQA, WebQSP, and KQA Pro, we
observe that under the few-shot even the more challenging zero-shot scenarios,
FlexKBQA achieves impressive results with a few annotations, surpassing all
previous baselines and even approaching the performance of supervised models,
achieving a remarkable 93% performance relative to the fully-supervised models.
We posit that FlexKBQA represents a significant advancement towards exploring
better integration of large and lightweight models. The code is open-sourced.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1&quot;&gt;Sunqi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yu Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiuxing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1&quot;&gt;Zhichao Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1&quot;&gt;Bowen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ning Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianyong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15560">
<title>WeatherBench 2: A benchmark for the next generation of data-driven global weather models. (arXiv:2308.15560v2 [physics.ao-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15560</link>
<description rdf:parseType="Literal">&lt;p&gt;WeatherBench 2 is an update to the global, medium-range (1-14 day) weather
forecasting benchmark proposed by Rasp et al. (2020), designed with the aim to
accelerate progress in data-driven weather modeling. WeatherBench 2 consists of
an open-source evaluation framework, publicly available training, ground truth
and baseline data as well as a continuously updated website with the latest
metrics and state-of-the-art models:
https://sites.research.google/weatherbench. This paper describes the design
principles of the evaluation framework and presents results for current
state-of-the-art physical and data-driven weather models. The metrics are based
on established practices for evaluating weather forecasts at leading
operational weather centers. We define a set of headline scores to provide an
overview of model performance. In addition, we also discuss caveats in the
current evaluation setup and challenges for the future of data-driven weather
forecasting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Rasp_S/0/1/0/all/0/1&quot;&gt;Stephan Rasp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hoyer_S/0/1/0/all/0/1&quot;&gt;Stephan Hoyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Merose_A/0/1/0/all/0/1&quot;&gt;Alexander Merose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Langmore_I/0/1/0/all/0/1&quot;&gt;Ian Langmore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Battaglia_P/0/1/0/all/0/1&quot;&gt;Peter Battaglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Russel_T/0/1/0/all/0/1&quot;&gt;Tyler Russel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Sanchez_Gonzalez_A/0/1/0/all/0/1&quot;&gt;Alvaro Sanchez-Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yang_V/0/1/0/all/0/1&quot;&gt;Vivian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Carver_R/0/1/0/all/0/1&quot;&gt;Rob Carver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Agrawal_S/0/1/0/all/0/1&quot;&gt;Shreya Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chantry_M/0/1/0/all/0/1&quot;&gt;Matthew Chantry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bouallegue_Z/0/1/0/all/0/1&quot;&gt;Zied Ben Bouallegue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Dueben_P/0/1/0/all/0/1&quot;&gt;Peter Dueben&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bromberg_C/0/1/0/all/0/1&quot;&gt;Carla Bromberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Sisk_J/0/1/0/all/0/1&quot;&gt;Jared Sisk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Barrington_L/0/1/0/all/0/1&quot;&gt;Luke Barrington&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bell_A/0/1/0/all/0/1&quot;&gt;Aaron Bell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Sha_F/0/1/0/all/0/1&quot;&gt;Fei Sha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02731">
<title>HC3 Plus: A Semantic-Invariant Human ChatGPT Comparison Corpus. (arXiv:2309.02731v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02731</link>
<description rdf:parseType="Literal">&lt;p&gt;ChatGPT has gained significant interest due to its impressive performance,
but people are increasingly concerned about its potential risks, particularly
around the detection of AI-generated content (AIGC), which is often difficult
for untrained humans to identify. Current datasets utilized for detecting
ChatGPT-generated text primarily center around question-answering, yet they
tend to disregard tasks that possess semantic-invariant properties, such as
summarization, translation, and paraphrasing. Our primary studies demonstrate
that detecting model-generated text on semantic-invariant tasks is more
difficult. To fill this gap, we introduce a more extensive and comprehensive
dataset that considers more types of tasks than previous work, including
semantic-invariant tasks. In addition, the model after a large number of task
instruction fine-tuning shows a strong powerful performance. Owing to its
previous success, we further instruct fine-tuning T\textit{k}-instruct and
build a more powerful detection system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1&quot;&gt;Zhenpeng Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xing Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1&quot;&gt;Guangyuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Songlin Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07200">
<title>Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck. (arXiv:2309.07200v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07200</link>
<description rdf:parseType="Literal">&lt;p&gt;Markov processes are widely used mathematical models for describing dynamic
systems in various fields. However, accurately simulating large-scale systems
at long time scales is computationally expensive due to the short time steps
required for accurate integration. In this paper, we introduce an inference
process that maps complex systems into a simplified representational space and
models large jumps in time. To achieve this, we propose Time-lagged Information
Bottleneck (T-IB), a principled objective rooted in information theory, which
aims to capture relevant temporal features while discarding high-frequency
information to simplify the simulation task and minimize the inference error.
Our experiments demonstrate that T-IB learns information-optimal
representations for accurately modeling the statistical properties and dynamics
of the original process at a selected time lag, outperforming existing
time-lagged dimensionality reduction methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Federici_M/0/1/0/all/0/1&quot;&gt;Marco Federici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forre_P/0/1/0/all/0/1&quot;&gt;Patrick Forr&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomioka_R/0/1/0/all/0/1&quot;&gt;Ryota Tomioka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veeling_B/0/1/0/all/0/1&quot;&gt;Bastiaan S. Veeling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12244">
<title>ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events. (arXiv:2309.12244v3 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12244</link>
<description rdf:parseType="Literal">&lt;p&gt;Children typically learn to identify and express emotions through sharing
their stories and feelings with others, particularly their family. However, it
is challenging for parents or siblings to have emotional communication with
children since children are still developing their communication skills. We
present ChaCha, a chatbot that encourages and guides children to share personal
events and associated emotions. ChaCha combines a state machine and large
language models (LLMs) to keep the dialogue on track while carrying on
free-form conversations. Through an exploratory study with 20 children (aged
8-12), we examine how ChaCha prompts children to share personal events and
guides them to describe associated emotions. Participants perceived ChaCha as a
close friend and shared their stories on various topics, such as family trips
and personal achievements. Based on the findings, we discuss opportunities for
leveraging LLMs to design child-friendly chatbots to support children in
sharing emotions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_W/0/1/0/all/0/1&quot;&gt;Woosuk Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chanmo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Young-Ho Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02601">
<title>MagicDrive: Street View Generation with Diverse 3D Geometry Control. (arXiv:2310.02601v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02601</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in diffusion models have significantly enhanced the data
synthesis with 2D control. Yet, precise 3D control in street view generation,
crucial for 3D perception tasks, remains elusive. Specifically, utilizing
Bird&apos;s-Eye View (BEV) as the primary condition often leads to challenges in
geometry control (e.g., height), affecting the representation of object shapes,
occlusion patterns, and road surface elevations, all of which are essential to
perception data synthesis, especially for 3D object detection tasks. In this
paper, we introduce MagicDrive, a novel street view generation framework
offering diverse 3D geometry controls, including camera poses, road maps, and
3D bounding boxes, together with textual descriptions, achieved through
tailored encoding strategies. Besides, our design incorporates a cross-view
attention module, ensuring consistency across multiple camera views. With
MagicDrive, we achieve high-fidelity street-view synthesis that captures
nuanced 3D geometry and various scene descriptions, enhancing tasks like BEV
segmentation and 3D object detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Ruiyuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1&quot;&gt;Lanqing Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1&quot;&gt;Dit-Yan Yeung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02998">
<title>ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models. (arXiv:2310.02998v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02998</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Vision-Language Models (LVLMs) can understand the world comprehensively
by integrating rich information from different modalities, achieving remarkable
advancements on various multimodal downstream tasks. However, deploying LVLMs
is often problematic due to their massive computational/energy costs and carbon
consumption. Such issues make it infeasible to adopt conventional iterative
global pruning, which is costly due to computing the Hessian matrix of the
entire large model for sparsification. Alternatively, several studies have
recently proposed layer-wise pruning approaches to avoid the expensive
computation of global pruning and efficiently compress model weights according
to their importance within a layer. However, they often suffer from suboptimal
model compression due to their lack of a global perspective. To address this
limitation in recent efficient pruning methods for large models, we propose
Efficient Coarse-to-Fine LayerWise Pruning (ECoFLaP), a two-stage
coarse-to-fine weight pruning approach for LVLMs. We first determine the
sparsity ratios of different layers or blocks by leveraging the global
importance score, which is efficiently computed based on the zeroth-order
approximation of the global model gradients. Then, the model performs local
layer-wise unstructured weight pruning based on globally-informed sparsity
ratios. We validate our proposed method across various multimodal and unimodal
models and datasets, demonstrating significant performance improvements over
prevalent pruning techniques in the high-sparsity regime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1&quot;&gt;Yi-Lin Sung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jaehong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08565">
<title>Security Considerations in AI-Robotics: A Survey of Current Methods, Challenges, and Opportunities. (arXiv:2310.08565v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08565</link>
<description rdf:parseType="Literal">&lt;p&gt;Robotics and Artificial Intelligence (AI) have been inextricably intertwined
since their inception. Today, AI-Robotics systems have become an integral part
of our daily lives, from robotic vacuum cleaners to semi-autonomous cars. These
systems are built upon three fundamental architectural elements: perception,
navigation and planning, and control. However, while the integration of
AI-Robotics systems has enhanced the quality our lives, it has also presented a
serious problem - these systems are vulnerable to security attacks. The
physical components, algorithms, and data that make up AI-Robotics systems can
be exploited by malicious actors, potentially leading to dire consequences.
Motivated by the need to address the security concerns in AI-Robotics systems,
this paper presents a comprehensive survey and taxonomy across three
dimensions: attack surfaces, ethical and legal concerns, and Human-Robot
Interaction (HRI) security. Our goal is to provide users, developers and other
stakeholders with a holistic understanding of these areas to enhance the
overall AI-Robotics system security. We begin by surveying potential attack
surfaces and provide mitigating defensive strategies. We then delve into
ethical issues, such as dependency and psychological impact, as well as the
legal concerns regarding accountability for these systems. Besides, emerging
trends such as HRI are discussed, considering privacy, integrity, safety,
trustworthiness, and explainability concerns. Finally, we present our vision
for future research directions in this dynamic and promising field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neupane_S/0/1/0/all/0/1&quot;&gt;Subash Neupane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1&quot;&gt;Shaswata Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_I/0/1/0/all/0/1&quot;&gt;Ivan A. Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1&quot;&gt;Swayamjit Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1&quot;&gt;Sudip Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jingdao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pillai_N/0/1/0/all/0/1&quot;&gt;Nisha Pillai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahimi_S/0/1/0/all/0/1&quot;&gt;Shahram Rahimi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08602">
<title>Safe Deep Policy Adaptation. (arXiv:2310.08602v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08602</link>
<description rdf:parseType="Literal">&lt;p&gt;A critical goal of autonomy and artificial intelligence is enabling
autonomous robots to rapidly adapt in dynamic and uncertain environments.
Classic adaptive control and safe control provide stability and safety
guarantees but are limited to specific system classes. In contrast, policy
adaptation based on reinforcement learning (RL) offers versatility and
generalizability but presents safety and robustness challenges. We propose
SafeDPA, a novel RL and control framework that simultaneously tackles the
problems of policy adaptation and safe reinforcement learning. SafeDPA jointly
learns adaptive policy and dynamics models in simulation, predicts environment
configurations, and fine-tunes dynamics models with few-shot real-world data. A
safety filter based on the Control Barrier Function (CBF) on top of the RL
policy is introduced to ensure safety during real-world deployment. We provide
theoretical safety guarantees of SafeDPA and show the robustness of SafeDPA
against learning errors and extra perturbations. Comprehensive experiments on
(1) classic control problems (Inverted Pendulum), (2) simulation benchmarks
(Safety Gym), and (3) a real-world agile robotics platform (RC Car) demonstrate
great superiority of SafeDPA in both safety and task performance, over
state-of-the-art baselines. Particularly, SafeDPA demonstrates notable
generalizability, achieving a 300% increase in safety rate compared to the
baselines, under unseen disturbances in real-world experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1&quot;&gt;Wenli Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tairan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolan_J/0/1/0/all/0/1&quot;&gt;John Dolan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1&quot;&gt;Guanya Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15950">
<title>Representation Learning with Large Language Models for Recommendation. (arXiv:2310.15950v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15950</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender systems have seen significant advancements with the influence of
deep learning and graph neural networks, particularly in capturing complex
user-item relationships. However, these graph-based recommenders heavily depend
on ID-based data, potentially disregarding valuable textual information
associated with users and items, resulting in less informative learned
representations. Moreover, the utilization of implicit feedback data introduces
potential noise and bias, posing challenges for the effectiveness of user
preference learning. While the integration of large language models (LLMs) into
traditional ID-based recommenders has gained attention, challenges such as
scalability issues, limitations in text-only reliance, and prompt input
constraints need to be addressed for effective implementation in practical
recommender systems. To address these challenges, we propose a model-agnostic
framework RLMRec that aims to enhance existing recommenders with LLM-empowered
representation learning. It proposes a recommendation paradigm that integrates
representation learning with LLMs to capture intricate semantic aspects of user
behaviors and preferences. RLMRec incorporates auxiliary textual signals,
develops a user/item profiling paradigm empowered by LLMs, and aligns the
semantic space of LLMs with the representation space of collaborative
relational signals through a cross-view alignment framework. This work further
establish a theoretical foundation demonstrating that incorporating textual
signals through mutual information maximization enhances the quality of
representations. In our evaluation, we integrate RLMRec with state-of-the-art
recommender models, while also analyzing its efficiency and robustness to noise
data. Our implementation codes are available at
https://github.com/HKUDS/RLMRec.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xubin Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1&quot;&gt;Lianghao Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1&quot;&gt;Lixin Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Suqi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dawei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18564">
<title>A General Framework for Robust G-Invariance in G-Equivariant Networks. (arXiv:2310.18564v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18564</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a general method for achieving robust group-invariance in
group-equivariant convolutional neural networks ($G$-CNNs), which we call the
$G$-triple-correlation ($G$-TC) layer. The approach leverages the theory of the
triple-correlation on groups, which is the unique, lowest-degree polynomial
invariant map that is also complete. Many commonly used invariant maps--such as
the max--are incomplete: they remove both group and signal structure. A
complete invariant, by contrast, removes only the variation due to the actions
of the group, while preserving all information about the structure of the
signal. The completeness of the triple correlation endows the $G$-TC layer with
strong robustness, which can be observed in its resistance to invariance-based
adversarial attacks. In addition, we observe that it yields measurable
improvements in classification accuracy over standard Max $G$-Pooling in
$G$-CNN architectures. We provide a general and efficient implementation of the
method for any discretized group, which requires only a table defining the
group&apos;s product structure. We demonstrate the benefits of this method for
$G$-CNNs defined on both commutative and non-commutative groups--$SO(2)$,
$O(2)$, $SO(3)$, and $O(3)$ (discretized as the cyclic $C8$, dihedral $D16$,
chiral octahedral $O$ and full octahedral $O_h$ groups)--acting on
$\mathbb{R}^2$ and $\mathbb{R}^3$ on both $G$-MNIST and $G$-ModelNet10
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanborn_S/0/1/0/all/0/1&quot;&gt;Sophia Sanborn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miolane_N/0/1/0/all/0/1&quot;&gt;Nina Miolane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19731">
<title>ViR: Towards Efficient Vision Retention Backbones. (arXiv:2310.19731v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19731</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) have attracted a lot of popularity in recent
years, due to their exceptional capabilities in modeling long-range spatial
dependencies and scalability for large scale training. Although the training
parallelism of self-attention mechanism plays an important role in retaining
great performance, its quadratic complexity baffles the application of ViTs in
many scenarios which demand fast inference. This effect is even more pronounced
in applications in which autoregressive modeling of input features is required.
In Natural Language Processing (NLP), a new stream of efforts has proposed
parallelizable models with recurrent formulation that allows for efficient
inference in generative applications. Inspired by this trend, we propose a new
class of computer vision models, dubbed Vision Retention Networks (ViR), with
dual parallel and recurrent formulations, which strike an optimal balance
between fast inference and parallel training with competitive performance. In
particular, ViR scales favorably for image throughput and memory consumption in
tasks that require higher-resolution images due to its flexible formulation in
processing large sequence lengths. The ViR is the first attempt to realize dual
parallel and recurrent equivalency in a general vision backbone for recognition
tasks. We have validated the effectiveness of ViR through extensive experiments
with different dataset sizes and various image resolutions and achieved
competitive performance. Code: https://github.com/NVlabs/ViR
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatamizadeh_A/0/1/0/all/0/1&quot;&gt;Ali Hatamizadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranzinger_M/0/1/0/all/0/1&quot;&gt;Michael Ranzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_S/0/1/0/all/0/1&quot;&gt;Shiyi Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1&quot;&gt;Jose M. Alvarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1&quot;&gt;Sanja Fidler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1&quot;&gt;Jan Kautz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11211">
<title>Leveraging Generative AI for Clinical Evidence Summarization Needs to Ensure Trustworthiness. (arXiv:2311.11211v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11211</link>
<description rdf:parseType="Literal">&lt;p&gt;Evidence-based medicine promises to improve the quality of healthcare by
empowering medical decisions and practices with the best available evidence.
The rapid growth of medical evidence, which can be obtained from various
sources, poses a challenge in collecting, appraising, and synthesizing the
evidential information. Recent advancements in generative AI, exemplified by
large language models, hold promise in facilitating the arduous task. However,
developing accountable, fair, and inclusive models remains a complicated
undertaking. In this perspective, we discuss the trustworthiness of generative
AI in the context of automated summarization of medical evidence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Gongbo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1&quot;&gt;Qiao Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McInerney_D/0/1/0/all/0/1&quot;&gt;Denis Jered McInerney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cole_C/0/1/0/all/0/1&quot;&gt;Curtis L. Cole&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanshan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malin_B/0/1/0/all/0/1&quot;&gt;Bradley A. Malin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peleg_M/0/1/0/all/0/1&quot;&gt;Mor Peleg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1&quot;&gt;Byron C. Wallace&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1&quot;&gt;Chunhua Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yifan Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13691">
<title>Next-Generation Earth System Models: Towards Reliable Hybrid Models for Weather and Climate Applications. (arXiv:2311.13691v2 [physics.ao-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13691</link>
<description rdf:parseType="Literal">&lt;p&gt;We review how machine learning has transformed our ability to model the Earth
system, and how we expect recent breakthroughs to benefit end-users in
Switzerland in the near future. Drawing from our review, we identify three
recommendations.
&lt;/p&gt;
&lt;p&gt;Recommendation 1: Develop Hybrid AI-Physical Models: Emphasize the
integration of AI and physical modeling for improved reliability, especially
for longer prediction horizons, acknowledging the delicate balance between
knowledge-based and data-driven components required for optimal performance.
Recommendation 2: Emphasize Robustness in AI Downscaling Approaches, favoring
techniques that respect physical laws, preserve inter-variable dependencies and
spatial structures, and accurately represent extremes at the local scale.
Recommendation 3: Promote Inclusive Model Development: Ensure Earth System
Model development is open and accessible to diverse stakeholders, enabling
forecasters, the public, and AI/statistics experts to use, develop, and engage
with the model and its predictions/projections.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Beucler_T/0/1/0/all/0/1&quot;&gt;Tom Beucler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Koch_E/0/1/0/all/0/1&quot;&gt;Erwan Koch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kotlarski_S/0/1/0/all/0/1&quot;&gt;Sven Kotlarski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Leutwyler_D/0/1/0/all/0/1&quot;&gt;David Leutwyler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Michel_A/0/1/0/all/0/1&quot;&gt;Adrien Michel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Koh_J/0/1/0/all/0/1&quot;&gt;Jonathan Koh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14786">
<title>GPT-4V Takes the Wheel: Promises and Challenges for Pedestrian Behavior Prediction. (arXiv:2311.14786v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14786</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting pedestrian behavior is the key to ensure safety and reliability of
autonomous vehicles. While deep learning methods have been promising by
learning from annotated video frame sequences, they often fail to fully grasp
the dynamic interactions between pedestrians and traffic, crucial for accurate
predictions. These models also lack nuanced common sense reasoning. Moreover,
the manual annotation of datasets for these models is expensive and challenging
to adapt to new situations. The advent of Vision Language Models (VLMs)
introduces promising alternatives to these issues, thanks to their advanced
visual and causal reasoning skills. To our knowledge, this research is the
first to conduct both quantitative and qualitative evaluations of VLMs in the
context of pedestrian behavior prediction for autonomous driving. We evaluate
GPT-4V(ision) on publicly available pedestrian datasets: JAAD and WiDEVIEW. Our
quantitative analysis focuses on GPT-4V&apos;s ability to predict pedestrian
behavior in current and future frames. The model achieves a 57% accuracy in a
zero-shot manner, which, while impressive, is still behind the state-of-the-art
domain-specific models (70%) in predicting pedestrian crossing actions.
Qualitatively, GPT-4V shows an impressive ability to process and interpret
complex traffic scenarios, differentiate between various pedestrian behaviors,
and detect and analyze groups. However, it faces challenges, such as difficulty
in detecting smaller pedestrians and assessing the relative motion between
pedestrians and the ego vehicle.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jia Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1&quot;&gt;Peng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gautam_A/0/1/0/all/0/1&quot;&gt;Alvika Gautam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saripalli_S/0/1/0/all/0/1&quot;&gt;Srikanth Saripalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01339">
<title>ArabIcros: AI-Powered Arabic Crossword Puzzle Generation for Educational Applications. (arXiv:2312.01339v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01339</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents the first Arabic crossword puzzle generator driven by
advanced AI technology. Leveraging cutting-edge large language models including
GPT4, GPT3-Davinci, GPT3-Curie, GPT3-Babbage, GPT3-Ada, and BERT, the system
generates distinctive and challenging clues. Based on a dataset comprising over
50,000 clue-answer pairs, the generator employs fine-tuning, few/zero-shot
learning strategies, and rigorous quality-checking protocols to enforce the
generation of high-quality clue-answer pairs. Importantly, educational
crosswords contribute to enhancing memory, expanding vocabulary, and promoting
problem-solving skills, thereby augmenting the learning experience through a
fun and engaging approach, reshaping the landscape of traditional learning
methods. The overall system can be exploited as a powerful educational tool
that amalgamates AI and innovative learning techniques, heralding a
transformative era for Arabic crossword puzzles and the intersection of
technology and education.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeinalipour_K/0/1/0/all/0/1&quot;&gt;Kamyar Zeinalipour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saad_M/0/1/0/all/0/1&quot;&gt;Mohamed Zaky Saad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggini_M/0/1/0/all/0/1&quot;&gt;Marco Maggini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1&quot;&gt;Marco Gori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02003">
<title>A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly. (arXiv:2312.02003v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02003</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized
natural language understanding and generation. They possess deep language
comprehension, human-like text generation capabilities, contextual awareness,
and robust problem-solving skills, making them invaluable in various domains
(e.g., search engines, customer support, translation). In the meantime, LLMs
have also gained traction in the security community, revealing security
vulnerabilities and showcasing their potential in security-related tasks. This
paper explores the intersection of LLMs with security and privacy.
Specifically, we investigate how LLMs positively impact security and privacy,
potential risks and threats associated with their use, and inherent
vulnerabilities within LLMs. Through a comprehensive literature review, the
paper categorizes the papers into &quot;The Good&quot; (beneficial LLM applications),
&quot;The Bad&quot; (offensive applications), and &quot;The Ugly&quot; (vulnerabilities of LLMs and
their defenses). We have some interesting findings. For example, LLMs have
proven to enhance code security (code vulnerability detection) and data privacy
(data confidentiality protection), outperforming traditional methods. However,
they can also be harnessed for various attacks (particularly user-level
attacks) due to their human-like reasoning abilities. We have identified areas
that require further research efforts. For example, Research on model and
parameter extraction attacks is limited and often theoretical, hindered by LLM
parameter scale and confidentiality. Safe instruction tuning, a recent
development, requires more exploration. We hope that our work can shed light on
the LLMs&apos; potential to both bolster and jeopardize cybersecurity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yifan Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Jinhao Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kaidi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;Yuanfang Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhibo Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yue Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08463">
<title>How much can change in a year? Revisiting Evaluation in Multi-Agent Reinforcement Learning. (arXiv:2312.08463v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08463</link>
<description rdf:parseType="Literal">&lt;p&gt;Establishing sound experimental standards and rigour is important in any
growing field of research. Deep Multi-Agent Reinforcement Learning (MARL) is
one such nascent field. Although exciting progress has been made, MARL has
recently come under scrutiny for replicability issues and a lack of
standardised evaluation methodology, specifically in the cooperative setting.
Although protocols have been proposed to help alleviate the issue, it remains
important to actively monitor the health of the field. In this work, we extend
the database of evaluation methodology previously published by containing
meta-data on MARL publications from top-rated conferences and compare the
findings extracted from this updated database to the trends identified in their
work. Our analysis shows that many of the worrying trends in performance
reporting remain. This includes the omission of uncertainty quantification, not
reporting all relevant evaluation details and a narrowing of algorithmic
development classes. Promisingly, we do observe a trend towards more difficult
scenarios in SMAC-v1, which if continued into SMAC-v2 will encourage novel
algorithmic development. Our data indicate that replicability needs to be
approached more proactively by the MARL community to ensure trust in the field
as we move towards exciting new frontiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Siddarth Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahjoub_O/0/1/0/all/0/1&quot;&gt;Omayma Mahjoub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kock_R/0/1/0/all/0/1&quot;&gt;Ruan de Kock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khlifi_W/0/1/0/all/0/1&quot;&gt;Wiem Khlifi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vall_A/0/1/0/all/0/1&quot;&gt;Abidine Vall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tessera_K/0/1/0/all/0/1&quot;&gt;Kale-ab Tessera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pretorius_A/0/1/0/all/0/1&quot;&gt;Arnu Pretorius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08466">
<title>Efficiently Quantifying Individual Agent Importance in Cooperative MARL. (arXiv:2312.08466v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08466</link>
<description rdf:parseType="Literal">&lt;p&gt;Measuring the contribution of individual agents is challenging in cooperative
multi-agent reinforcement learning (MARL). In cooperative MARL, team
performance is typically inferred from a single shared global reward. Arguably,
among the best current approaches to effectively measure individual agent
contributions is to use Shapley values. However, calculating these values is
expensive as the computational complexity grows exponentially with respect to
the number of agents. In this paper, we adapt difference rewards into an
efficient method for quantifying the contribution of individual agents,
referred to as Agent Importance, offering a linear computational complexity
relative to the number of agents. We show empirically that the computed values
are strongly correlated with the true Shapley values, as well as the true
underlying individual agent rewards, used as the ground truth in environments
where these are available. We demonstrate how Agent Importance can be used to
help study MARL systems by diagnosing algorithmic failures discovered in prior
MARL benchmarking work. Our analysis illustrates Agent Importance as a valuable
explainability component for future MARL benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahjoub_O/0/1/0/all/0/1&quot;&gt;Omayma Mahjoub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kock_R/0/1/0/all/0/1&quot;&gt;Ruan de Kock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Siddarth Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khlifi_W/0/1/0/all/0/1&quot;&gt;Wiem Khlifi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vall_A/0/1/0/all/0/1&quot;&gt;Abidine Vall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tessera_K/0/1/0/all/0/1&quot;&gt;Kale-ab Tessera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pretorius_A/0/1/0/all/0/1&quot;&gt;Arnu Pretorius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03568">
<title>Agent AI: Surveying the Horizons of Multimodal Interaction. (arXiv:2401.03568v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03568</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal AI systems will likely become a ubiquitous presence in our
everyday lives. A promising approach to making these systems more interactive
is to embody them as agents within physical and virtual environments. At
present, systems leverage existing foundation models as the basic building
blocks for the creation of embodied agents. Embedding agents within such
environments facilitates the ability of models to process and interpret visual
and contextual data, which is critical for the creation of more sophisticated
and context-aware AI systems. For example, a system that can perceive user
actions, human behavior, environmental objects, audio expressions, and the
collective sentiment of a scene can be used to inform and direct agent
responses within the given environment. To accelerate research on agent-based
multimodal intelligence, we define &quot;Agent AI&quot; as a class of interactive systems
that can perceive visual stimuli, language inputs, and other
environmentally-grounded data, and can produce meaningful embodied actions. In
particular, we explore systems that aim to improve agents based on
next-embodied action prediction by incorporating external knowledge,
multi-sensory inputs, and human feedback. We argue that by developing agentic
AI systems in grounded environments, one can also mitigate the hallucinations
of large foundation models and their tendency to generate environmentally
incorrect outputs. The emerging field of Agent AI subsumes the broader embodied
and agentic aspects of multimodal interactions. Beyond agents acting and
interacting in the physical world, we envision a future where people can easily
create any virtual reality or simulated scene and interact with agents embodied
within the virtual environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durante_Z/0/1/0/all/0/1&quot;&gt;Zane Durante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qiuyuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wake_N/0/1/0/all/0/1&quot;&gt;Naoki Wake&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1&quot;&gt;Ran Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jae Sung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_B/0/1/0/all/0/1&quot;&gt;Bidipta Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1&quot;&gt;Rohan Taori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noda_Y/0/1/0/all/0/1&quot;&gt;Yusuke Noda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terzopoulos_D/0/1/0/all/0/1&quot;&gt;Demetri Terzopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yejin Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ikeuchi_K/0/1/0/all/0/1&quot;&gt;Katsushi Ikeuchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1&quot;&gt;Hoi Vo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1&quot;&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06401">
<title>DevEval: Evaluating Code Generation in Practical Software Projects. (arXiv:2401.06401v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2401.06401</link>
<description rdf:parseType="Literal">&lt;p&gt;How to evaluate Large Language Models (LLMs) in code generation is an open
question. Many benchmarks have been proposed but are inconsistent with
practical software projects, e.g., unreal program distributions, insufficient
dependencies, and small-scale project contexts. Thus, the capabilities of LLMs
in practical projects are still unclear. In this paper, we propose a new
benchmark named DevEval, aligned with Developers&apos; experiences in practical
projects. DevEval is collected through a rigorous pipeline, containing 2,690
samples from 119 practical projects and covering 10 domains. Compared to
previous benchmarks, DevEval aligns to practical projects in multiple
dimensions, e.g., real program distributions, sufficient dependencies, and
enough-scale project contexts. We assess five popular LLMs on DevEval (e.g.,
gpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual
abilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo
only is 42 in our experiments. We also discuss the challenges and future
directions of code generation in practical projects. We open-source DevEval and
hope it can facilitate the development of code generation in practical
projects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Ge Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yunfei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yongmin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zhi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huanyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kaibo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lecheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zheng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lanshen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1&quot;&gt;Jiazheng Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuanming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yihong Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuqi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1&quot;&gt;Bin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mengfei Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07603">
<title>Multi-task robot data for dual-arm fine manipulation. (arXiv:2401.07603v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07603</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of robotic manipulation, deep imitation learning is recognized
as a promising approach for acquiring manipulation skills. Additionally,
learning from diverse robot datasets is considered a viable method to achieve
versatility and adaptability. In such research, by learning various tasks,
robots achieved generality across multiple objects. However, such multi-task
robot datasets have mainly focused on single-arm tasks that are relatively
imprecise, not addressing the fine-grained object manipulation that robots are
expected to perform in the real world. This paper introduces a dataset of
diverse object manipulations that includes dual-arm tasks and/or tasks
requiring fine manipulation. To this end, we have generated dataset with 224k
episodes (150 hours, 1,104 language instructions) which includes dual-arm fine
tasks such as bowl-moving, pencil-case opening or banana-peeling, and this data
is publicly available. Additionally, this dataset includes visual attention
signals as well as dual-action labels, a signal that separates actions into a
robust reaching trajectory and precise interaction with objects, and language
instructions to achieve robust and precise object manipulation. We applied the
dataset to our Dual-Action and Attention (DAA), a model designed for
fine-grained dual arm manipulation tasks and robust against covariate shifts.
The model was tested with over 7k total trials in real robot manipulation
tasks, demonstrating its capability in fine manipulation. The dataset is
available at https://sites.google.com/view/multi-task-fine.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Heecheol Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohmura_Y/0/1/0/all/0/1&quot;&gt;Yoshiyuki Ohmura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuniyoshi_Y/0/1/0/all/0/1&quot;&gt;Yasuo Kuniyoshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08115">
<title>No-Clean-Reference Image Super-Resolution: Application to Electron Microscopy. (arXiv:2401.08115v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08115</link>
<description rdf:parseType="Literal">&lt;p&gt;The inability to acquire clean high-resolution (HR) electron microscopy (EM)
images over a large brain tissue volume hampers many neuroscience studies. To
address this challenge, we propose a deep-learning-based image super-resolution
(SR) approach to computationally reconstruct clean HR 3D-EM with a large field
of view (FoV) from noisy low-resolution (LR) acquisition. Our contributions are
I) Investigating training with no-clean references for $\ell_2$ and $\ell_1$
loss functions; II) Introducing a novel network architecture, named EMSR, for
enhancing the resolution of LR EM images while reducing inherent noise; and,
III) Comparing different training strategies including using acquired LR and HR
image pairs, i.e., real pairs with no-clean references contaminated with real
corruptions, the pairs of synthetic LR and acquired HR, as well as acquired LR
and denoised HR pairs. Experiments with nine brain datasets showed that
training with real pairs can produce high-quality super-resolved results,
demonstrating the feasibility of training with non-clean references for both
loss functions. Additionally, comparable results were observed, both visually
and numerically, when employing denoised and noisy references for training.
Moreover, utilizing the network trained with synthetically generated LR images
from HR counterparts proved effective in yielding satisfactory SR results, even
in certain cases, outperforming training with real pairs. The proposed SR
network was compared quantitatively and qualitatively with several established
SR techniques, showcasing either the superiority or competitiveness of the
proposed method in mitigating noise while recovering fine details.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khateri_M/0/1/0/all/0/1&quot;&gt;Mohammad Khateri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghahremani_M/0/1/0/all/0/1&quot;&gt;Morteza Ghahremani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sierra_A/0/1/0/all/0/1&quot;&gt;Alejandra Sierra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tohka_J/0/1/0/all/0/1&quot;&gt;Jussi Tohka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08534">
<title>DiConStruct: Causal Concept-based Explanations through Black-Box Distillation. (arXiv:2401.08534v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08534</link>
<description rdf:parseType="Literal">&lt;p&gt;Model interpretability plays a central role in human-AI decision-making
systems. Ideally, explanations should be expressed using human-interpretable
semantic concepts. Moreover, the causal relations between these concepts should
be captured by the explainer to allow for reasoning about the explanations.
Lastly, explanation methods should be efficient and not compromise the
performance of the predictive task. Despite the rapid advances in AI
explainability in recent years, as far as we know to date, no method fulfills
these three properties. Indeed, mainstream methods for local concept
explainability do not produce causal explanations and incur a trade-off between
explainability and prediction performance. We present DiConStruct, an
explanation method that is both concept-based and causal, with the goal of
creating more interpretable local explanations in the form of structural causal
models and concept attributions. Our explainer works as a distillation model to
any black-box machine learning model by approximating its predictions while
producing the respective explanations. Because of this, DiConStruct generates
explanations efficiently while not impacting the black-box prediction task. We
validate our method on an image dataset and a tabular dataset, showing that
DiConStruct approximates the black-box models with higher fidelity than other
concept explainability baselines, while providing explanations that include the
causal relations between the concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreira_R/0/1/0/all/0/1&quot;&gt;Ricardo Moreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bono_J/0/1/0/all/0/1&quot;&gt;Jacopo Bono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardoso_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe1;rio Cardoso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saleiro_P/0/1/0/all/0/1&quot;&gt;Pedro Saleiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Figueiredo_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe1;rio A. T. Figueiredo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bizarro_P/0/1/0/all/0/1&quot;&gt;Pedro Bizarro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10189">
<title>Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10189</link>
<description rdf:parseType="Literal">&lt;p&gt;Fine-grained few-shot entity extraction in the chemical domain faces two
unique challenges. First, compared with entity extraction tasks in the general
domain, sentences from chemical papers usually contain more entities. Moreover,
entity extraction models usually have difficulty extracting entities of
long-tailed types. In this paper, we propose Chem-FINESE, a novel
sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to
address these two challenges. Our Chem-FINESE has two components: a seq2seq
entity extractor to extract named entities from the input sentence and a
seq2seq self-validation module to reconstruct the original input sentence from
extracted entities. Inspired by the fact that a good entity extraction system
needs to extract entities faithfully, our new self-validation module leverages
entity extraction results to reconstruct the original input sentence. Besides,
we design a new contrastive loss to reduce excessive copying during the
extraction process. Finally, we release ChemNER+, a new fine-grained chemical
entity extraction dataset that is annotated by domain experts with the ChemNER
schema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasets
show that our newly proposed framework has contributed up to 8.26% and 6.84%
absolute F1-score gains respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qingyun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zixuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongxiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiawei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Huimin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11143">
<title>Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.11143</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a
novel probabilistic attention framework, and the Gaussian Adaptive Transformer
(GAT), designed to enhance information aggregation across multiple modalities,
including Speech, Text and Vision. GAAM integrates learnable mean and variance
into its attention mechanism, implemented in a Multi-Headed framework enabling
it to collectively model any Probability Distribution for dynamic recalibration
of feature significance. This method demonstrates significant improvements,
especially with highly non-stationary data, surpassing the state-of-the-art
attention techniques in model performance (up to approximately +20% in
accuracy) by identifying key elements within the feature space. GAAM&apos;s
compatibility with dot-product-based attention models and relatively low number
of parameters showcases its adaptability and potential to boost existing
attention frameworks. Empirically, GAAM exhibits superior adaptability and
efficacy across a diverse range of tasks, including emotion recognition in
speech, image classification, and text classification, thereby establishing its
robustness and versatility in handling multi-modal data. Furthermore, we
introduce the Importance Factor (IF), a new learning-based metric that enhances
the explainability of models trained with GAAM-based methods. Overall, GAAM
represents an advancement towards development of better performing and more
explainable attention models across multiple modalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ioannides_G/0/1/0/all/0/1&quot;&gt;Georgios Ioannides&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1&quot;&gt;Aman Chadha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elkins_A/0/1/0/all/0/1&quot;&gt;Aaron Elkins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11174">
<title>Pixel-Wise Recognition for Holistic Surgical Scene Understanding. (arXiv:2401.11174v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.11174</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents the Holistic and Multi-Granular Surgical Scene
Understanding of Prostatectomies (GraSP) dataset, a curated benchmark that
models surgical scene understanding as a hierarchy of complementary tasks with
varying levels of granularity. Our approach enables a multi-level comprehension
of surgical activities, encompassing long-term tasks such as surgical phases
and steps recognition and short-term tasks including surgical instrument
segmentation and atomic visual actions detection. To exploit our proposed
benchmark, we introduce the Transformers for Actions, Phases, Steps, and
Instrument Segmentation (TAPIS) model, a general architecture that combines a
global video feature extractor with localized region proposals from an
instrument segmentation model to tackle the multi-granularity of our benchmark.
Through extensive experimentation, we demonstrate the impact of including
segmentation annotations in short-term recognition tasks, highlight the varying
granularity requirements of each task, and establish TAPIS&apos;s superiority over
previously proposed baselines and conventional CNN-based models. Additionally,
we validate the robustness of our method across multiple public benchmarks,
confirming the reliability and applicability of our dataset. This work
represents a significant step forward in Endoscopic Vision, offering a novel
and comprehensive framework for future research towards a holistic
understanding of surgical procedures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayobi_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xe1;s Ayobi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_S/0/1/0/all/0/1&quot;&gt;Santiago Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_A/0/1/0/all/0/1&quot;&gt;Alejandra P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_I/0/1/0/all/0/1&quot;&gt;Isabela Hern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aparicio_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xe1;s Aparicio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dessevres_E/0/1/0/all/0/1&quot;&gt;Eug&amp;#xe9;nie Dessevres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pena_S/0/1/0/all/0/1&quot;&gt;Sebasti&amp;#xe1;n Pe&amp;#xf1;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santander_J/0/1/0/all/0/1&quot;&gt;Jessica Santander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caicedo_J/0/1/0/all/0/1&quot;&gt;Juan Ignacio Caicedo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xe1;s Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1&quot;&gt;Pablo Arbel&amp;#xe1;ez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13324">
<title>Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions. (arXiv:2401.13324v3 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2401.13324</link>
<description rdf:parseType="Literal">&lt;p&gt;Explanations of AI systems rarely address the information needs of people
affected by algorithmic decision-making (ADM). This gap between conveyed
information and information that matters to affected stakeholders can impede
understanding and adherence to regulatory frameworks such as the AI Act. To
address this gap, we present the &quot;XAI Novice Question Bank&quot;: A catalog of
affected stakeholders&apos; information needs in two ADM use cases (employment
prediction and health monitoring), covering the categories data, system
context, system usage, and system specifications. Information needs were
gathered in an interview study where participants received explanations in
response to their inquiries. Participants further reported their understanding
and decision confidence, showing that while confidence tended to increase after
receiving explanations, participants also met understanding challenges, such as
being unable to tell why their understanding felt incomplete. Explanations
further influenced participants&apos; perceptions of the systems&apos; risks and
benefits, which they confirmed or changed depending on the use case. When risks
were perceived as high, participants expressed particular interest in
explanations about intention, such as why and to what end a system was put in
place. With this work, we aim to support the inclusion of affected stakeholders
into explainability by contributing an overview of information and challenges
relevant to them when deciding on the adoption of ADM systems. We close by
summarizing our findings in a list of six key implications that inform the
design of future explanations for affected stakeholder audiences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmude_T/0/1/0/all/0/1&quot;&gt;Timoth&amp;#xe9;e Schmude&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koesten_L/0/1/0/all/0/1&quot;&gt;Laura Koesten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moller_T/0/1/0/all/0/1&quot;&gt;Torsten M&amp;#xf6;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tschiatschek_S/0/1/0/all/0/1&quot;&gt;Sebastian Tschiatschek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13758">
<title>Assumptions and Bounds in the Instrumental Variable Model. (arXiv:2401.13758v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2401.13758</link>
<description rdf:parseType="Literal">&lt;p&gt;In this note we give proofs for results relating to the Instrumental Variable
(IV) model with binary response $Y$ and binary treatment $X$, but with an
instrument $Z$ with $K$ states. These results were originally stated in
Richardson &amp;amp; Robins (2014), &quot;ACE Bounds; SEMS with Equilibrium Conditions,&quot;
&lt;a href=&quot;/abs/1410.0470&quot;&gt;arXiv:1410.0470&lt;/a&gt;.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Richardson_T/0/1/0/all/0/1&quot;&gt;Thomas S. Richardson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Robins_J/0/1/0/all/0/1&quot;&gt;James M. Robins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14011">
<title>CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning. (arXiv:2401.14011v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.14011</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal large language models(MLLMs) have achieved remarkable progress
and demonstrated powerful knowledge comprehension and reasoning abilities.
However, the mastery of domain-specific knowledge, which is essential for
evaluating the intelligence of MLLMs, continues to be a challenge. Current
multi-modal benchmarks for domain-specific knowledge concentrate on
multiple-choice questions and are predominantly available in English, which
imposes limitations on the comprehensiveness of the evaluation. To this end, we
introduce CMMU, a novel benchmark for multi-modal and multi-type question
understanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7
subjects, covering knowledge from primary to high school. The questions can be
categorized into 3 types: multiple-choice, multiple-response, and
fill-in-the-blank, bringing greater challenges to MLLMs. In addition, we
propose a rigorous evaluation strategy called ShiftCheck for assessing
multiple-choice questions. The strategy aims to reduce position bias, minimize
the influence of randomness on correctness, and perform a quantitative analysis
of position bias. We evaluate seven open-source MLLMs along with GPT4-V,
Gemini-Pro, and Qwen-VL-Plus. The results demonstrate that CMMU poses a
significant challenge to the recent MLLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zheqi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xinya Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pengfei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuan_R/0/1/0/all/0/1&quot;&gt;Richeng Xuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qiannan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hua Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13544">
<title>Piecewise polynomial regression of tame functions via integer programming. (arXiv:2311.13544v1 [math.OC] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2311.13544</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the task of estimating functions belonging to a specific class of
nonsmooth functions, namely so-called tame functions. These functions appear in
a wide range of applications: training deep learning, value functions of
mixed-integer programs, or wave functions of small molecules. We show that tame
functions are approximable by piecewise polynomials on any full-dimensional
cube. We then present the first ever mixed-integer programming formulation of
piecewise polynomial regression. Together, these can be used to estimate tame
functions. We demonstrate promising computational results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nemecek_J/0/1/0/all/0/1&quot;&gt;Jiri Nemecek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bareilles_G/0/1/0/all/0/1&quot;&gt;Gilles Bareilles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Aspman_J/0/1/0/all/0/1&quot;&gt;Johannes Aspman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Marecek_J/0/1/0/all/0/1&quot;&gt;Jakub Marecek&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>