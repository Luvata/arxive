<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-08-24T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12307" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12315" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12367" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12371" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12385" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12394" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12411" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12415" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12416" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12438" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12445" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12453" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12454" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12478" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12510" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12532" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12551" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12573" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12574" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12581" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12605" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12606" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12635" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12649" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12659" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12674" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12714" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12737" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12740" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12794" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12840" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12888" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12902" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12956" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12967" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.00350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.13310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.09107" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.07240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.12263" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.00939" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.08471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.00642" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01735" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.04997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.05976" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.06496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.09091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.00347" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.02169" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03543" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14706" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16556" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12907" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05983" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12213" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07445" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2308.12305">
<title>FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning. (arXiv:2308.12305v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.12305</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, foundation models have exhibited remarkable advancements in
multi-modal learning. These models, equipped with millions (or billions) of
parameters, typically require a substantial amount of data for finetuning.
However, collecting and centralizing training data from diverse sectors becomes
challenging due to distinct privacy regulations. Federated Learning (FL)
emerges as a promising solution, enabling multiple clients to collaboratively
train neural networks without centralizing their local data. To alleviate
client computation burdens and communication overheads, previous works have
adapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only a
small fraction of the model parameters are optimized and communicated during
federated communications. Nevertheless, most previous works have focused on a
single modality and neglected one common phenomenon, i.e., the presence of data
heterogeneity across the clients. Therefore, in this work, we propose a
finetuning framework tailored to heterogeneous multi-modal FL, called Federated
Dual-Aadapter Teacher (FedDAT). Specifically, our approach leverages a
Dual-Adapter Teacher (DAT) to address data heterogeneity by regularizing the
client local updates and applying Mutual Knowledge Distillation (MKD) for an
efficient knowledge transfer. FedDAT is the first approach that enables an
efficient distributed finetuning of foundation models for a variety of
heterogeneous Vision-Language tasks. To demonstrate its effectiveness, we
conduct extensive experiments on four multi-modality FL benchmarks with
different types of data heterogeneity, where FedDAT substantially outperforms
the existing centralized PEFT methods adapted for FL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haokun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krompass_D/0/1/0/all/0/1&quot;&gt;Denis Krompass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jindong Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1&quot;&gt;Volker Tresp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12307">
<title>Modeling Bends in Popular Music Guitar Tablatures. (arXiv:2308.12307v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2308.12307</link>
<description rdf:parseType="Literal">&lt;p&gt;Tablature notation is widely used in popular music to transcribe and share
guitar musical content. As a complement to standard score notation, tablatures
transcribe performance gesture information including finger positions and a
variety of guitar-specific playing techniques such as slides,
hammer-on/pull-off or bends.This paper focuses on bends, which enable to
progressively shift the pitch of a note, therefore circumventing physical
limitations of the discrete fretted fingerboard. In this paper, we propose a
set of 25 high-level features, computed for each note of the tablature, to
study how bend occurrences can be predicted from their past and future
short-term context. Experiments are performed on a corpus of 932 lead guitar
tablatures of popular music and show that a decision tree successfully predicts
bend occurrences with an F1 score of 0.71 anda limited amount of false positive
predictions, demonstrating promising applications to assist the arrangement of
non-guitar music into guitar tablatures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DHooge_A/0/1/0/all/0/1&quot;&gt;Alexandre D&amp;#x27;Hooge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bigo_L/0/1/0/all/0/1&quot;&gt;Louis Bigo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deguernel_K/0/1/0/all/0/1&quot;&gt;Ken D&amp;#xe9;guernel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12312">
<title>Physics informed Neural Networks applied to the description of wave-particle resonance in kinetic simulations of fusion plasmas. (arXiv:2308.12312v1 [physics.comp-ph])</title>
<link>http://arxiv.org/abs/2308.12312</link>
<description rdf:parseType="Literal">&lt;p&gt;The Vlasov-Poisson system is employed in its reduced form version (1D1V) as a
test bed for the applicability of Physics Informed Neural Network (PINN) to the
wave-particle resonance. Two examples are explored: the Landau damping and the
bump-on-tail instability. PINN is first tested as a compression method for the
solution of the Vlasov-Poisson system and compared to the standard neural
networks. Second, the application of PINN to solving the Vlasov-Poisson system
is also presented with the special emphasis on the integral part, which
motivates the implementation of a PINN variant, called Integrable PINN
(I-PINN), based on the automatic-differentiation to solve the partial
differential equation and on the automatic-integration to solve the integral
equation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kumar_J/0/1/0/all/0/1&quot;&gt;Jai Kumar&lt;/a&gt; (IRFM), &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zarzoso_D/0/1/0/all/0/1&quot;&gt;David Zarzoso&lt;/a&gt; (M2P2), &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Grandgirard_V/0/1/0/all/0/1&quot;&gt;Virginie Grandgirard&lt;/a&gt; (IRFM), &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ebert_J/0/1/0/all/0/1&quot;&gt;Jan Ebert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kesselheim_S/0/1/0/all/0/1&quot;&gt;Stefan Kesselheim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12315">
<title>Trustworthy Representation Learning Across Domains. (arXiv:2308.12315v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.12315</link>
<description rdf:parseType="Literal">&lt;p&gt;As AI systems have obtained significant performance to be deployed widely in
our daily live and human society, people both enjoy the benefits brought by
these technologies and suffer many social issues induced by these systems. To
make AI systems good enough and trustworthy, plenty of researches have been
done to build guidelines for trustworthy AI systems. Machine learning is one of
the most important parts for AI systems and representation learning is the
fundamental technology in machine learning. How to make the representation
learning trustworthy in real-world application, e.g., cross domain scenarios,
is very valuable and necessary for both machine learning and AI system fields.
Inspired by the concepts in trustworthy AI, we proposed the first trustworthy
representation learning across domains framework which includes four concepts,
i.e, robustness, privacy, fairness, and explainability, to give a comprehensive
literature review on this research direction. Specifically, we first introduce
the details of the proposed trustworthy framework for representation learning
across domains. Second, we provide basic notions and comprehensively summarize
existing methods for the trustworthy framework from four concepts. Finally, we
conclude this survey with insights and discussions on future research
directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1&quot;&gt;Ronghang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1&quot;&gt;Dongliang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_D/0/1/0/all/0/1&quot;&gt;Daiqing Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1&quot;&gt;Zhixuan Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xiang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sheng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12319">
<title>RemovalNet: DNN Fingerprint Removal Attacks. (arXiv:2308.12319v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12319</link>
<description rdf:parseType="Literal">&lt;p&gt;With the performance of deep neural networks (DNNs) remarkably improving,
DNNs have been widely used in many areas. Consequently, the DNN model has
become a valuable asset, and its intellectual property is safeguarded by
ownership verification techniques (e.g., DNN fingerprinting). However, the
feasibility of the DNN fingerprint removal attack and its potential influence
remains an open problem. In this paper, we perform the first comprehensive
investigation of DNN fingerprint removal attacks. Generally, the knowledge
contained in a DNN model can be categorized into general semantic and
fingerprint-specific knowledge. To this end, we propose a min-max bilevel
optimization-based DNN fingerprint removal attack named RemovalNet, to evade
model ownership verification. The lower-level optimization is designed to
remove fingerprint-specific knowledge. While in the upper-level optimization,
we distill the victim model&apos;s general semantic knowledge to maintain the
surrogate model&apos;s performance. We conduct extensive experiments to evaluate the
fidelity, effectiveness, and efficiency of the RemovalNet against four advanced
defense methods on six metrics. The empirical results demonstrate that (1) the
RemovalNet is effective. After our DNN fingerprint removal attack, the model
distance between the target and surrogate models is x100 times higher than that
of the baseline attacks, (2) the RemovalNet is efficient. It uses only 0.2%
(400 samples) of the substitute dataset and 1,000 iterations to conduct our
attack. Besides, compared with advanced model stealing attacks, the RemovalNet
saves nearly 85% of computational resources at most, (3) the RemovalNet
achieves high fidelity that the created surrogate model maintains high accuracy
after the DNN fingerprint removal process. Our code is available at:
https://github.com/grasses/RemovalNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Hongwei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kunzhe Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1&quot;&gt;Jian Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zhan Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1&quot;&gt;Kui Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12367">
<title>SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies. (arXiv:2308.12367v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.12367</link>
<description rdf:parseType="Literal">&lt;p&gt;With the growing use of machine learning (ML) models in critical domains such
as finance and healthcare, the need to offer recourse for those adversely
affected by the decisions of ML models has become more important; individuals
ought to be provided with recommendations on actions to take for improving
their situation and thus receive a favorable decision. Prior work on sequential
algorithmic recourse -- which recommends a series of changes -- focuses on
action feasibility and uses the proximity of feature changes to determine
action costs. However, the uncertainties of feature changes and the risk of
higher than average costs in recourse have not been considered. It is
undesirable if a recourse could (with some probability) result in a worse
situation from which recovery requires an extremely high cost. It is essential
to incorporate risks when computing and evaluating recourse. We call the
recourse computed with such risk considerations as Safer Algorithmic Recourse
(SafeAR). The objective is to empower people to choose a recourse based on
their risk tolerance. In this work, we discuss and show how existing recourse
desiderata can fail to capture the risk of higher costs. We present a method to
compute recourse policies that consider variability in cost and connect
algorithmic recourse literature with risk-sensitive reinforcement learning. We
also adopt measures ``Value at Risk&apos;&apos; and ``Conditional Value at Risk&apos;&apos; from
the financial literature to summarize risk concisely. We apply our method to
two real-world datasets and compare policies with different levels of
risk-aversion using risk measures and recourse desiderata (sparsity and
proximity).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haochen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1&quot;&gt;Shubham Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patra_S/0/1/0/all/0/1&quot;&gt;Sunandita Patra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalakrishnan_S/0/1/0/all/0/1&quot;&gt;Sriram Gopalakrishnan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12371">
<title>Open-set Face Recognition with Neural Ensemble, Maximal Entropy Loss and Feature Augmentation. (arXiv:2308.12371v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12371</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-set face recognition refers to a scenario in which biometric systems
have incomplete knowledge of all existing subjects. Therefore, they are
expected to prevent face samples of unregistered subjects from being identified
as previously enrolled identities. This watchlist context adds an arduous
requirement that calls for the dismissal of irrelevant faces by focusing mainly
on subjects of interest. As a response, this work introduces a novel method
that associates an ensemble of compact neural networks with a margin-based cost
function that explores additional samples. Supplementary negative samples can
be obtained from external databases or synthetically built at the
representation level in training time with a new mix-up feature augmentation
approach. Deep neural networks pre-trained on large face datasets serve as the
preliminary feature extraction module. We carry out experiments on well-known
LFW and IJB-C datasets where results show that the approach is able to boost
closed and open-set identification rates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vareto_R/0/1/0/all/0/1&quot;&gt;Rafael Henrique Vareto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunther_M/0/1/0/all/0/1&quot;&gt;Manuel G&amp;#xfc;nther&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwartz_W/0/1/0/all/0/1&quot;&gt;William Robson Schwartz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12381">
<title>Inferring gender from name: a large scale performance evaluation study. (arXiv:2308.12381v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12381</link>
<description rdf:parseType="Literal">&lt;p&gt;A person&apos;s gender is a crucial piece of information when performing research
across a wide range of scientific disciplines, such as medicine, sociology,
political science, and economics, to name a few. However, in increasing
instances, especially given the proliferation of big data, gender information
is not readily available. In such cases researchers need to infer gender from
readily available information, primarily from persons&apos; names. While inferring
gender from name may raise some ethical questions, the lack of viable
alternatives means that researchers have to resort to such approaches when the
goal justifies the means - in the majority of such studies the goal is to
examine patterns and determinants of gender disparities. The necessity of
name-to-gender inference has generated an ever-growing domain of algorithmic
approaches and software products. These approaches have been used throughout
the world in academia, industry, governmental and non-governmental
organizations. Nevertheless, the existing approaches have yet to be
systematically evaluated and compared, making it challenging to determine the
optimal approach for future research. In this work, we conducted a large scale
performance evaluation of existing approaches for name-to-gender inference.
Analysis are performed using a variety of large annotated datasets of names. We
further propose two new hybrid approaches that achieve better performance than
any single existing approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krstovski_K/0/1/0/all/0/1&quot;&gt;Kriste Krstovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Ye Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12383">
<title>With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning. (arXiv:2308.12383v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12383</link>
<description rdf:parseType="Literal">&lt;p&gt;Image captioning, like many tasks involving vision and language, currently
relies on Transformer-based architectures for extracting the semantics in an
image and translating it into linguistically coherent descriptions. Although
successful, the attention operator only considers a weighted summation of
projections of the current input sample, therefore ignoring the relevant
semantic information which can come from the joint observation of other
samples. In this paper, we devise a network which can perform attention over
activations obtained while processing other training samples, through a
prototypical memory model. Our memory models the distribution of past keys and
values through the definition of prototype vectors which are both
discriminative and compact. Experimentally, we assess the performance of the
proposed model on the COCO dataset, in comparison with carefully designed
baselines and state-of-the-art approaches, and by investigating the role of
each of the proposed components. We demonstrate that our proposal can increase
the performance of an encoder-decoder Transformer by 3.7 CIDEr points both when
training in cross-entropy only and when fine-tuning with self-critical sequence
training. Source code and trained models are available at:
https://github.com/aimagelab/PMA-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barraco_M/0/1/0/all/0/1&quot;&gt;Manuele Barraco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarto_S/0/1/0/all/0/1&quot;&gt;Sara Sarto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1&quot;&gt;Marcella Cornia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1&quot;&gt;Rita Cucchiara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12385">
<title>Handling the inconsistency of systems of $\min\rightarrow$ fuzzy relational equations. (arXiv:2308.12385v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.12385</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, we study the inconsistency of systems of $\min-\rightarrow$
fuzzy relational equations. We give analytical formulas for computing the
Chebyshev distances $\nabla = \inf_{d \in \mathcal{D}} \Vert \beta - d \Vert$
associated to systems of $\min-\rightarrow$ fuzzy relational equations of the
form $\Gamma \Box_{\rightarrow}^{\min} x = \beta$, where $\rightarrow$ is a
residual implicator among the G\&quot;odel implication $\rightarrow_G$, the Goguen
implication $\rightarrow_{GG}$ or Lukasiewicz&apos;s implication $\rightarrow_L$ and
$\mathcal{D}$ is the set of second members of consistent systems defined with
the same matrix $\Gamma$. The main preliminary result that allows us to obtain
these formulas is that the Chebyshev distance $\nabla$ is the lower bound of
the solutions of a vector inequality, whatever the residual implicator used.
Finally, we show that, in the case of the $\min-\rightarrow_{G}$ system, the
Chebyshev distance $\nabla$ may be an infimum, while it is always a minimum for
$\min-\rightarrow_{GG}$ and $\min-\rightarrow_{L}$ systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baaj_I/0/1/0/all/0/1&quot;&gt;Isma&amp;#xef;l Baaj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12394">
<title>Self-Supervised Learning for Endoscopic Video Analysis. (arXiv:2308.12394v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12394</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning (SSL) has led to important breakthroughs in computer
vision by allowing learning from large amounts of unlabeled data. As such, it
might have a pivotal role to play in biomedicine where annotating data requires
a highly specialized expertise. Yet, there are many healthcare domains for
which SSL has not been extensively explored. One such domain is endoscopy,
minimally invasive procedures which are commonly used to detect and treat
infections, chronic inflammatory diseases or cancer. In this work, we study the
use of a leading SSL framework, namely Masked Siamese Networks (MSNs), for
endoscopic video analysis such as colonoscopy and laparoscopy. To fully exploit
the power of SSL, we create sizable unlabeled endoscopic video datasets for
training MSNs. These strong image representations serve as a foundation for
secondary training with limited annotated datasets, resulting in
state-of-the-art performance in endoscopic benchmarks like surgical phase
recognition during laparoscopy and colonoscopic polyp characterization.
Additionally, we achieve a 50% reduction in annotated data size without
sacrificing performance. Thus, our work provides evidence that SSL can
dramatically reduce the need of annotated data in endoscopy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirsch_R/0/1/0/all/0/1&quot;&gt;Roy Hirsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1&quot;&gt;Mathilde Caron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_R/0/1/0/all/0/1&quot;&gt;Regev Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Livne_A/0/1/0/all/0/1&quot;&gt;Amir Livne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shapiro_R/0/1/0/all/0/1&quot;&gt;Ron Shapiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golany_T/0/1/0/all/0/1&quot;&gt;Tomer Golany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldenberg_R/0/1/0/all/0/1&quot;&gt;Roman Goldenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freedman_D/0/1/0/all/0/1&quot;&gt;Daniel Freedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivlin_E/0/1/0/all/0/1&quot;&gt;Ehud Rivlin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12411">
<title>A Theory of Intelligences: Concepts, Models, Implications. (arXiv:2308.12411v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.12411</link>
<description rdf:parseType="Literal">&lt;p&gt;Intelligence is a human construct to represent the ability to achieve goals.
Given this wide berth, intelligence has been defined countless times, studied
in a variety of ways and quantified using numerous measures. Understanding
intelligence ultimately requires theory and quantification, both of which are
elusive. My main objectives are to identify some of the central elements in and
surrounding intelligence, discuss some of its challenges and propose a theory
based on first principles. I focus on intelligence as defined by and for
humans, frequently in comparison to machines, with the intention of setting the
stage for more general characterizations in life, collectives, human designs
such as AI and in non-designed physical and chemical systems. I discuss key
features of intelligence, including path efficiency and goal accuracy,
intelligence as a Black Box, environmental influences, flexibility to deal with
surprisal, the regress of intelligence, the relativistic nature of intelligence
and difficulty, and temporal changes in intelligence including its evolution. I
present a framework for a first principles Theory of IntelligenceS (TIS), based
on the quantifiable macro-scale system features of difficulty, surprisal and
goal resolution accuracy. The proposed partitioning of uncertainty/solving and
accuracy/understanding is particularly novel since it predicts that paths to a
goal not only function to accurately achieve goals, but as experimentations
leading to higher probabilities for future attainable goals and increased
breadth to enter new goal spaces. TIS can therefore explain endeavors that do
not necessarily affect Darwinian fitness, such as leisure, politics, games and
art. I conclude with several conceptual advances of TIS including a compact
mathematical form of surprisal and difficulty, the theoretical basis of TIS,
and open questions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hochberg_M/0/1/0/all/0/1&quot;&gt;Michael E. Hochberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12415">
<title>Benchmarking Causal Study to Interpret Large Language Models for Source Code. (arXiv:2308.12415v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2308.12415</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the most common solutions adopted by software researchers to address
code generation is by training Large Language Models (LLMs) on massive amounts
of source code. Although a number of studies have shown that LLMs have been
effectively evaluated on popular accuracy metrics (e.g., BLEU, CodeBleu),
previous research has largely overlooked the role of Causal Inference as a
fundamental component of the interpretability of LLMs&apos; performance. Existing
benchmarks and datasets are meant to highlight the difference between the
expected and the generated outcome, but do not take into account confounding
variables (e.g., lines of code, prompt size) that equally influence the
accuracy metrics. The fact remains that, when dealing with generative software
tasks by LLMs, no benchmark is available to tell researchers how to quantify
neither the causal effect of SE-based treatments nor the correlation of
confounders to the model&apos;s performance. In an effort to bring statistical rigor
to the evaluation of LLMs, this paper introduces a benchmarking strategy named
Galeras comprised of curated testbeds for three SE tasks (i.e., code
completion, code summarization, and commit generation) to help aid the
interpretation of LLMs&apos; performance. We illustrate the insights of our
benchmarking strategy by conducting a case study on the performance of ChatGPT
under distinct prompt engineering methods. The results of the case study
demonstrate the positive causal influence of prompt semantics on ChatGPT&apos;s
generative performance by an average treatment effect of $\approx 3\%$.
Moreover, it was found that confounders such as prompt size are highly
correlated with accuracy metrics ($\approx 0.412\%$). The end result of our
case study is to showcase causal inference evaluations, in practice, to reduce
confounding bias. By reducing the bias, we offer an interpretable solution for
the accuracy metric under analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_Cardenas_D/0/1/0/all/0/1&quot;&gt;Daniel Rodriguez-Cardenas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palacio_D/0/1/0/all/0/1&quot;&gt;David N. Palacio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khati_D/0/1/0/all/0/1&quot;&gt;Dipin Khati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burke_H/0/1/0/all/0/1&quot;&gt;Henry Burke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poshyvanyk_D/0/1/0/all/0/1&quot;&gt;Denys Poshyvanyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12416">
<title>Reframing the Brain Age Prediction Problem to a More Interpretable and Quantitative Approach. (arXiv:2308.12416v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.12416</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models have achieved state-of-the-art results in estimating
brain age, which is an important brain health biomarker, from magnetic
resonance (MR) images. However, most of these models only provide a global age
prediction, and rely on techniques, such as saliency maps to interpret their
results. These saliency maps highlight regions in the input image that were
significant for the model&apos;s predictions, but they are hard to be interpreted,
and saliency map values are not directly comparable across different samples.
In this work, we reframe the age prediction problem from MR images to an
image-to-image regression problem where we estimate the brain age for each
brain voxel in MR images. We compare voxel-wise age prediction models against
global age prediction models and their corresponding saliency maps. The results
indicate that voxel-wise age prediction models are more interpretable, since
they provide spatial information about the brain aging process, and they
benefit from being quantitative.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gianchandani_N/0/1/0/all/0/1&quot;&gt;Neha Gianchandani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dibaji_M/0/1/0/all/0/1&quot;&gt;Mahsa Dibaji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bento_M/0/1/0/all/0/1&quot;&gt;Mariana Bento&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+MacDonald_E/0/1/0/all/0/1&quot;&gt;Ethan MacDonald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Souza_R/0/1/0/all/0/1&quot;&gt;Roberto Souza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12438">
<title>Deploying Deep Reinforcement Learning Systems: A Taxonomy of Challenges. (arXiv:2308.12438v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.12438</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning (DRL), leveraging Deep Learning (DL) in
reinforcement learning, has shown significant potential in achieving
human-level autonomy in a wide range of domains, including robotics, computer
vision, and computer games. This potential justifies the enthusiasm and growing
interest in DRL in both academia and industry. However, the community currently
focuses mostly on the development phase of DRL systems, with little attention
devoted to DRL deployment. In this paper, we propose an empirical study on
Stack Overflow (SO), the most popular Q&amp;amp;A forum for developers, to uncover and
understand the challenges practitioners faced when deploying DRL systems.
Specifically, we categorized relevant SO posts by deployment platforms:
server/cloud, mobile/embedded system, browser, and game engine. After filtering
and manual analysis, we examined 357 SO posts about DRL deployment,
investigated the current state, and identified the challenges related to
deploying DRL systems. Then, we investigate the prevalence and difficulty of
these challenges. Results show that the general interest in DRL deployment is
growing, confirming the study&apos;s relevance and importance. Results also show
that DRL deployment is more difficult than other DRL issues. Additionally, we
built a taxonomy of 31 unique challenges in deploying DRL to different
platforms. On all platforms, RL environment-related challenges are the most
popular, and communication-related challenges are the most difficult among
practitioners. We hope our study inspires future research and helps the
community overcome the most common and difficult challenges practitioners face
when deploying DRL systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yahmed_A/0/1/0/all/0/1&quot;&gt;Ahmed Haj Yahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbassi_A/0/1/0/all/0/1&quot;&gt;Altaf Allah Abbassi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikanjam_A/0/1/0/all/0/1&quot;&gt;Amin Nikanjam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Heng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1&quot;&gt;Foutse Khomh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12439">
<title>BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection. (arXiv:2308.12439v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2308.12439</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel defense, against backdoor attacks on Deep Neural Networks
(DNNs), wherein adversaries covertly implant malicious behaviors (backdoors)
into DNNs. Our defense falls within the category of post-development defenses
that operate independently of how the model was generated. The proposed defense
is built upon a novel reverse engineering approach that can directly extract
backdoor functionality of a given backdoored model to a backdoor expert model.
The approach is straightforward -- finetuning the backdoored model over a small
set of intentionally mislabeled clean samples, such that it unlearns the normal
functionality while still preserving the backdoor functionality, and thus
resulting in a model (dubbed a backdoor expert model) that can only recognize
backdoor inputs. Based on the extracted backdoor expert model, we show the
feasibility of devising highly accurate backdoor input detectors that filter
out the backdoor inputs during model inference. Further augmented by an
ensemble strategy with a finetuned auxiliary model, our defense, BaDExpert
(Backdoor Input Detection with Backdoor Expert), effectively mitigates 16 SOTA
backdoor attacks while minimally impacting clean utility. The effectiveness of
BaDExpert has been verified on multiple datasets (CIFAR10, GTSRB and ImageNet)
across various model architectures (ResNet, VGG, MobileNetV2 and Vision
Transformer).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1&quot;&gt;Tinghao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xiangyu Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1&quot;&gt;Ping He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiachen T. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1&quot;&gt;Prateek Mittal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12445">
<title>An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems. (arXiv:2308.12445v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.12445</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning (DRL) is increasingly applied in large-scale
productions like Netflix and Facebook. As with most data-driven systems, DRL
systems can exhibit undesirable behaviors due to environmental drifts, which
often occur in constantly-changing production settings. Continual Learning (CL)
is the inherent self-healing approach for adapting the DRL agent in response to
the environment&apos;s conditions shifts. However, successive shifts of considerable
magnitude may cause the production environment to drift from its original
state. Recent studies have shown that these environmental drifts tend to drive
CL into long, or even unsuccessful, healing cycles, which arise from
inefficiencies such as catastrophic forgetting, warm-starting failure, and slow
convergence. In this paper, we propose Dr. DRL, an effective self-healing
approach for DRL systems that integrates a novel mechanism of intentional
forgetting into vanilla CL to overcome its main issues. Dr. DRL deliberately
erases the DRL system&apos;s minor behaviors to systematically prioritize the
adaptation of the key problem-solving skills. Using well-established DRL
algorithms, Dr. DRL is compared with vanilla CL on various drifted
environments. Dr. DRL is able to reduce, on average, the healing time and
fine-tuning episodes by, respectively, 18.74% and 17.72%. Dr. DRL successfully
helps agents to adapt to 19.63% of drifted environments left unsolved by
vanilla CL while maintaining and even enhancing by up to 45% the obtained
rewards for drifted environments that are resolved by both approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yahmed_A/0/1/0/all/0/1&quot;&gt;Ahmed Haj Yahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouchoucha_R/0/1/0/all/0/1&quot;&gt;Rached Bouchoucha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braiek_H/0/1/0/all/0/1&quot;&gt;Houssem Ben Braiek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1&quot;&gt;Foutse Khomh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12453">
<title>Augmenting medical image classifiers with synthetic data from latent diffusion models. (arXiv:2308.12453v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12453</link>
<description rdf:parseType="Literal">&lt;p&gt;While hundreds of artificial intelligence (AI) algorithms are now approved or
cleared by the US Food and Drugs Administration (FDA), many studies have shown
inconsistent generalization or latent bias, particularly for underrepresented
populations. Some have proposed that generative AI could reduce the need for
real data, but its utility in model development remains unclear. Skin disease
serves as a useful case study in synthetic image generation due to the
diversity of disease appearance, particularly across the protected attribute of
skin tone. Here we show that latent diffusion models can scalably generate
images of skin disease and that augmenting model training with these data
improves performance in data-limited settings. These performance gains saturate
at synthetic-to-real image ratios above 10:1 and are substantially smaller than
the gains obtained from adding real images. As part of our analysis, we
generate and analyze a new dataset of 458,920 synthetic images produced using
several generation strategies. Our results suggest that synthetic data could
serve as a force-multiplier for model development, but the collection of
diverse real-world data remains the most important step to improve medical AI
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sagers_L/0/1/0/all/0/1&quot;&gt;Luke W. Sagers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diao_J/0/1/0/all/0/1&quot;&gt;James A. Diao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melas_Kyriazi_L/0/1/0/all/0/1&quot;&gt;Luke Melas-Kyriazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groh_M/0/1/0/all/0/1&quot;&gt;Matthew Groh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1&quot;&gt;Pranav Rajpurkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adamson_A/0/1/0/all/0/1&quot;&gt;Adewole S. Adamson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rotemberg_V/0/1/0/all/0/1&quot;&gt;Veronica Rotemberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daneshjou_R/0/1/0/all/0/1&quot;&gt;Roxana Daneshjou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manrai_A/0/1/0/all/0/1&quot;&gt;Arjun K. Manrai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12454">
<title>PFL-GAN: When Client Heterogeneity Meets Generative Models in Personalized Federated Learning. (arXiv:2308.12454v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.12454</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances of generative learning models are accompanied by the growing
interest in federated learning (FL) based on generative adversarial network
(GAN) models. In the context of FL, GAN can capture the underlying client data
structure, and regenerate samples resembling the original data distribution
without compromising the private raw data. Although most existing GAN-based FL
works focus on training a global model, Personalized FL (PFL) sometimes can be
more effective in view of client data heterogeneity in terms of distinct data
sample distributions, feature spaces, and labels. To cope with client
heterogeneity in GAN-based FL, we propose a novel GAN sharing and aggregation
strategy for PFL. The proposed PFL-GAN addresses the client heterogeneity in
different scenarios. More specially, we first learn the similarity among
clients and then develop an weighted collaborative data aggregation. The
empirical results through the rigorous experimentation on several well-known
datasets demonstrate the effectiveness of PFL-GAN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wijesinghe_A/0/1/0/all/0/1&quot;&gt;Achintha Wijesinghe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Songyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Zhi Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12466">
<title>Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis. (arXiv:2308.12466v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12466</link>
<description rdf:parseType="Literal">&lt;p&gt;Since the introduction of ChatGPT and GPT-4, these models have been tested
across a large number of tasks. Their adeptness across domains is evident, but
their aptitude in playing games and specifically their aptitude in the realm of
poker has remained unexplored. Poker is a game that requires decision making
under uncertainty and incomplete information. In this paper, we put ChatGPT and
GPT-4 through the poker test and evaluate their poker skills. Our findings
reveal that while both models display an advanced understanding of poker,
encompassing concepts like the valuation of starting hands, playing positions
and other intricacies of game theory optimal (GTO) poker, both ChatGPT and
GPT-4 are NOT game theory optimal poker players.
&lt;/p&gt;
&lt;p&gt;Through a series of experiments, we first discover the characteristics of
optimal prompts and model parameters for playing poker with these models. Our
observations then unveil the distinct playing personas of the two models. We
first conclude that GPT-4 is a more advanced poker player than ChatGPT. This
exploration then sheds light on the divergent poker tactics of the two models:
ChatGPT&apos;s conservativeness juxtaposed against GPT-4&apos;s aggression. In poker
vernacular, when tasked to play GTO poker, ChatGPT plays like a Nit, which
means that it has a propensity to only engage with premium hands and folds a
majority of hands. When subjected to the same directive, GPT-4 plays like a
maniac, showcasing a loose and aggressive style of play. Both strategies,
although relatively advanced, are not game theory optimal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Akshat Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12478">
<title>Attention-Based Acoustic Feature Fusion Network for Depression Detection. (arXiv:2308.12478v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2308.12478</link>
<description rdf:parseType="Literal">&lt;p&gt;Depression, a common mental disorder, significantly influences individuals
and imposes considerable societal impacts. The complexity and heterogeneity of
the disorder necessitate prompt and effective detection, which nonetheless,
poses a difficult challenge. This situation highlights an urgent requirement
for improved detection methods. Exploiting auditory data through advanced
machine learning paradigms presents promising research directions. Yet,
existing techniques mainly rely on single-dimensional feature models,
potentially neglecting the abundance of information hidden in various speech
characteristics. To rectify this, we present the novel Attention-Based Acoustic
Feature Fusion Network (ABAFnet) for depression detection. ABAFnet combines
four different acoustic features into a comprehensive deep learning model,
thereby effectively integrating and blending multi-tiered features. We present
a novel weight adjustment module for late fusion that boosts performance by
efficaciously synthesizing these features. The effectiveness of our approach is
confirmed via extensive validation on two clinical speech databases, CNRAC and
CS-NRAC, thereby outperforming previous methods in depression detection and
subtype classification. Further in-depth analysis confirms the key role of each
feature and highlights the importance of MFCCrelated features in speech-based
depression detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xinru Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xizhe Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12486">
<title>A Model of Sequential Learning based on Non-Axiomatic Logic. (arXiv:2308.12486v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.12486</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequential learning is a fundamental function of an intelligent agent. This
technical report introduces a model of sequential learning, which is
interpretable through Non-Axiomatic Logic. The learning procedure includes
three steps, hypothesizing, revising, and recycling, and can work under the
Assumption of Insufficient Knowledge and Resources. Although there are
limitations for the current design, the model has been proven effective in some
simple cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Bowen Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12488">
<title>GPTEval: A Survey on Assessments of ChatGPT and GPT-4. (arXiv:2308.12488v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.12488</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of ChatGPT has generated much speculation in the press about
its potential to disrupt social and economic systems. Its astonishing language
ability has aroused strong curiosity among scholars about its performance in
different domains. There have been many studies evaluating the ability of
ChatGPT and GPT-4 in different tasks and disciplines. However, a comprehensive
review summarizing the collective assessment findings is lacking. The objective
of this survey is to thoroughly analyze prior assessments of ChatGPT and GPT-4,
focusing on its language and reasoning abilities, scientific knowledge, and
ethical considerations. Furthermore, an examination of the existing evaluation
methods is conducted, offering several recommendations for future research in
evaluating large language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1&quot;&gt;Rui Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guanyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xulang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1&quot;&gt;Frank Guerin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1&quot;&gt;Erik Cambria&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12495">
<title>Source-Free Collaborative Domain Adaptation via Multi-Perspective Feature Enrichment for Functional MRI Analysis. (arXiv:2308.12495v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12495</link>
<description rdf:parseType="Literal">&lt;p&gt;Resting-state functional MRI (rs-fMRI) is increasingly employed in multi-site
research to aid neurological disorder analysis. Existing studies usually suffer
from significant cross-site/domain data heterogeneity caused by site effects
such as differences in scanners/protocols. Many methods have been proposed to
reduce fMRI heterogeneity between source and target domains, heavily relying on
the availability of source data. But acquiring source data is challenging due
to privacy concerns and/or data storage burdens in multi-site studies. To this
end, we design a source-free collaborative domain adaptation (SCDA) framework
for fMRI analysis, where only a pretrained source model and unlabeled target
data are accessible. Specifically, a multi-perspective feature enrichment
method (MFE) is developed for target fMRI analysis, consisting of multiple
collaborative branches to dynamically capture fMRI features of unlabeled target
data from multiple views. Each branch has a data-feeding module, a
spatiotemporal feature encoder, and a class predictor. A mutual-consistency
constraint is designed to encourage pair-wise consistency of latent features of
the same input generated from these branches for robust representation
learning. To facilitate efficient cross-domain knowledge transfer without
source data, we initialize MFE using parameters of a pretrained source model.
We also introduce an unsupervised pretraining strategy using 3,806 unlabeled
fMRIs from three large-scale auxiliary databases, aiming to obtain a general
feature encoder. Experimental results on three public datasets and one private
dataset demonstrate the efficacy of our method in cross-scanner and cross-study
prediction tasks. The model pretrained on large-scale rs-fMRI data has been
released to the public.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yuqi Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jinjian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qianqian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1&quot;&gt;Shijun Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bozoki_A/0/1/0/all/0/1&quot;&gt;Andrea Bozoki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Huaicheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mingxia Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12503">
<title>CGMI: Configurable General Multi-Agent Interaction Framework. (arXiv:2308.12503v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.12503</link>
<description rdf:parseType="Literal">&lt;p&gt;Benefiting from the powerful capabilities of large language models (LLMs),
agents based on LLMs have shown the potential to address domain-specific tasks
and emulate human behaviors. However, the content generated by these agents
remains somewhat superficial, owing to their limited domain expertise and the
absence of an effective cognitive architecture. To address this, we present the
Configurable General Multi-Agent Interaction (CGMI) framework, designed to
replicate human interactions in real-world scenarios. Specifically, we propose
a tree-structured methodology for the assignment, detection, and maintenance of
agent personality. Additionally, we designed a cognitive architecture equipped
with a skill library based on the ACT* model, which contains memory,
reflection, and planning modules. We have also integrated general agents to
augment the virtual environment&apos;s realism. Using the CGMI framework, we
simulated numerous classroom interactions between teacher and students. The
experiments indicate that aspects such as the teaching methodology, curriculum,
and student performance closely mirror real classroom settings. We will open
source our work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jinxin_S/0/1/0/all/0/1&quot;&gt;Shi Jinxin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiabao_Z/0/1/0/all/0/1&quot;&gt;Zhao Jiabao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yilei_W/0/1/0/all/0/1&quot;&gt;Wang Yilei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xingjiao_W/0/1/0/all/0/1&quot;&gt;Wu Xingjiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiawen_L/0/1/0/all/0/1&quot;&gt;Li Jiawen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;He Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12510">
<title>Masked Autoencoders are Efficient Class Incremental Learners. (arXiv:2308.12510v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12510</link>
<description rdf:parseType="Literal">&lt;p&gt;Class Incremental Learning (CIL) aims to sequentially learn new classes while
avoiding catastrophic forgetting of previous knowledge. We propose to use
Masked Autoencoders (MAEs) as efficient learners for CIL. MAEs were originally
designed to learn useful representations through reconstructive unsupervised
learning, and they can be easily integrated with a supervised loss for
classification. Moreover, MAEs can reliably reconstruct original input images
from randomly selected patches, which we use to store exemplars from past tasks
more efficiently for CIL. We also propose a bilateral MAE framework to learn
from image-level and embedding-level fusion, which produces better-quality
reconstructed images and more stable representations. Our experiments confirm
that our approach performs better than the state-of-the-art on CIFAR-100,
ImageNet-Subset, and ImageNet-Full. The code is available at
https://github.com/scok30/MAE-CIL .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_J/0/1/0/all/0/1&quot;&gt;Jiang-Tian Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xialei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagdanov_A/0/1/0/all/0/1&quot;&gt;Andrew D. Bagdanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Ke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Ming-Ming Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12512">
<title>I3DOD: Towards Incremental 3D Object Detection via Prompting. (arXiv:2308.12512v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12512</link>
<description rdf:parseType="Literal">&lt;p&gt;3D object detection has achieved significant performance in many fields,
e.g., robotics system, autonomous driving, and augmented reality. However, most
existing methods could cause catastrophic forgetting of old classes when
performing on the class-incremental scenarios. Meanwhile, the current
class-incremental 3D object detection methods neglect the relationships between
the object localization information and category semantic information and
assume all the knowledge of old model is reliable. To address the above
challenge, we present a novel Incremental 3D Object Detection framework with
the guidance of prompting, i.e., I3DOD. Specifically, we propose a task-shared
prompts mechanism to learn the matching relationships between the object
localization information and category semantic information. After training on
the current task, these prompts will be stored in our prompt pool, and perform
the relationship of old classes in the next task. Moreover, we design a
reliable distillation strategy to transfer knowledge from two aspects: a
reliable dynamic distillation is developed to filter out the negative knowledge
and transfer the reliable 3D knowledge to new detection model; the relation
feature is proposed to capture the responses relation in feature space and
protect plasticity of the model when learning novel 3D classes. To the end, we
conduct comprehensive experiments on two benchmark datasets and our method
outperforms the state-of-the-art object detection methods by 0.6% - 2.7% in
terms of mAP@0.25.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Wenqi Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1&quot;&gt;Gan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenxi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jiahua Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kangru Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12517">
<title>Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion. (arXiv:2308.12517v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2308.12517</link>
<description rdf:parseType="Literal">&lt;p&gt;Several earlier studies have shown impressive control performance in complex
robotic systems by designing the controller using a neural network and training
it with model-free reinforcement learning. However, these outstanding
controllers with natural motion style and high task performance are developed
through extensive reward engineering, which is a highly laborious and
time-consuming process of designing numerous reward terms and determining
suitable reward coefficients. In this work, we propose a novel reinforcement
learning framework for training neural network controllers for complex robotic
systems consisting of both rewards and constraints. To let the engineers
appropriately reflect their intent to constraints and handle them with minimal
computation overhead, two constraint types and an efficient policy optimization
algorithm are suggested. The learning framework is applied to train locomotion
controllers for several legged robots with different morphology and physical
attributes to traverse challenging terrains. Extensive simulation and
real-world experiments demonstrate that performant controllers can be trained
with significantly less reward engineering, by tuning only a single reward
coefficient. Furthermore, a more straightforward and intuitive engineering
process can be utilized, thanks to the interpretability and generalizability of
constraints. The summary video is available at https://youtu.be/KAlm3yskhvM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yunho Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1&quot;&gt;Hyunsik Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jeonghyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jinhyeok Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1&quot;&gt;Gwanghyeon Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_M/0/1/0/all/0/1&quot;&gt;Moonkyu Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Youm_D/0/1/0/all/0/1&quot;&gt;Donghoon Youm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwangbo_J/0/1/0/all/0/1&quot;&gt;Jemin Hwangbo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12532">
<title>FedSoL: Bridging Global Alignment and Local Generality in Federated Learning. (arXiv:2308.12532v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.12532</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) aggregates locally trained models from individual
clients to construct a global model. While FL enables learning a model with
data privacy, it often suffers from significant performance degradation when
client data distributions are heterogeneous. Many previous FL algorithms have
addressed this issue by introducing various proximal restrictions. These
restrictions aim to encourage global alignment by constraining the deviation of
local learning from the global objective. However, they inherently limit local
learning by interfering with the original local objectives. Recently, an
alternative approach has emerged to improve local learning generality. By
obtaining local models within a smooth loss landscape, this approach mitigates
conflicts among different local objectives of the clients. Yet, it does not
ensure stable global alignment, as local learning does not take the global
objective into account. In this study, we propose Federated Stability on
Learning (FedSoL), which combines both the concepts of global alignment and
local generality. In FedSoL, the local learning seeks a parameter region robust
against proximal perturbations. This strategy introduces an implicit proximal
restriction effect in local learning while maintaining the original local
objective for parameter update. Our experiments show that FedSoL consistently
achieves state-of-the-art performance on various setups.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gihun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1&quot;&gt;Minchan Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sangmook Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Jaehoon Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1&quot;&gt;Se-Young Yun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12539">
<title>CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias. (arXiv:2308.12539v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12539</link>
<description rdf:parseType="Literal">&lt;p&gt;As language models (LMs) become increasingly powerful, it is important to
quantify and compare them for sociodemographic bias with potential for harm.
Prior bias measurement datasets are sensitive to perturbations in their
manually designed templates, therefore unreliable. To achieve reliability, we
introduce the Comprehensive Assessment of Language Model bias (CALM), a
benchmark dataset to quantify bias in LMs across three tasks. We integrate 16
existing datasets across different domains, such as Wikipedia and news
articles, to filter 224 templates from which we construct a dataset of 78,400
examples. We compare the diversity of CALM with prior datasets on metrics such
as average semantic similarity, and variation in template length, and test the
sensitivity to small perturbations. We show that our dataset is more diverse
and reliable than previous datasets, thus better capture the breadth of
linguistic variation required to reliably evaluate model bias. We evaluate 20
large language models including six prominent families of LMs such as Llama-2.
In two LM series, OPT and Bloom, we found that larger parameter models are more
biased than lower parameter models. We found the T0 series of models to be the
least biased. Furthermore, we noticed a tradeoff between gender and racial bias
with increasing model size in some model series. The code is available at
https://github.com/vipulgupta1011/CALM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1&quot;&gt;Vipul Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkit_P/0/1/0/all/0/1&quot;&gt;Pranav Narayanan Venkit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laurencon_H/0/1/0/all/0/1&quot;&gt;Hugo Lauren&amp;#xe7;on&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1&quot;&gt;Shomir Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1&quot;&gt;Rebecca J. Passonneau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12549">
<title>Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking. (arXiv:2308.12549v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12549</link>
<description rdf:parseType="Literal">&lt;p&gt;Siamese network has been a de facto benchmark framework for 3D LiDAR object
tracking with a shared-parametric encoder extracting features from template and
search region, respectively. This paradigm relies heavily on an additional
matching network to model the cross-correlation/similarity of the template and
search region. In this paper, we forsake the conventional Siamese paradigm and
propose a novel single-branch framework, SyncTrack, synchronizing the feature
extracting and matching to avoid forwarding encoder twice for template and
search region as well as introducing extra parameters of matching network. The
synchronization mechanism is based on the dynamic affinity of the Transformer,
and an in-depth analysis of the relevance is provided theoretically. Moreover,
based on the synchronization, we introduce a novel Attentive Points-Sampling
strategy into the Transformer layers (APST), replacing the random/Farthest
Points Sampling (FPS) method with sampling under the supervision of attentive
relations between the template and search region. It implies connecting
point-wise sampling with the feature learning, beneficial to aggregating more
distinctive and geometric features for tracking with sparse points. Extensive
experiments on two benchmark datasets (KITTI and NuScenes) show that SyncTrack
achieves state-of-the-art performance in real-time tracking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Teli Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengmeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jimin Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Huifeng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12551">
<title>A Co-training Approach for Noisy Time Series Learning. (arXiv:2308.12551v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.12551</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we focus on robust time series representation learning. Our
assumption is that real-world time series is noisy and complementary
information from different views of the same time series plays an important
role while analyzing noisy input. Based on this, we create two views for the
input time series through two different encoders. We conduct co-training based
contrastive learning iteratively to learn the encoders. Our experiments
demonstrate that this co-training approach leads to a significant improvement
in performance. Especially, by leveraging the complementary information from
different views, our proposed TS-CoT method can mitigate the impact of data
noise and corruption. Empirical evaluations on four time series benchmarks in
unsupervised and semi-supervised settings reveal that TS-CoT outperforms
existing methods. Furthermore, the representations learned by TS-CoT can
transfer well to downstream tasks through fine-tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weiqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsung_F/0/1/0/all/0/1&quot;&gt;Fugee Tsung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12573">
<title>Conditional Kernel Imitation Learning for Continuous State Environments. (arXiv:2308.12573v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.12573</link>
<description rdf:parseType="Literal">&lt;p&gt;Imitation Learning (IL) is an important paradigm within the broader
reinforcement learning (RL) methodology. Unlike most of RL, it does not assume
availability of reward-feedback. Reward inference and shaping are known to be
difficult and error-prone methods particularly when the demonstration data
comes from human experts. Classical methods such as behavioral cloning and
inverse reinforcement learning are highly sensitive to estimation errors, a
problem that is particularly acute in continuous state space problems.
Meanwhile, state-of-the-art IL algorithms convert behavioral policy learning
problems into distribution-matching problems which often require additional
online interaction data to be effective. In this paper, we consider the problem
of imitation learning in continuous state space environments based solely on
observed behavior, without access to transition dynamics information, reward
structure, or, most importantly, any additional interactions with the
environment. Our approach is based on the Markov balance equation and
introduces a novel conditional kernel density estimation-based imitation
learning framework. It involves estimating the environment&apos;s transition
dynamics using conditional kernel density estimators and seeks to satisfy the
probabilistic balance equations for the environment. We establish that our
estimators satisfy basic asymptotic consistency requirements. Through a series
of numerical experiments on continuous state benchmark environments, we show
consistently superior empirical performance over many state-of-the-art IL
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_R/0/1/0/all/0/1&quot;&gt;Rishabh Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahlin_N/0/1/0/all/0/1&quot;&gt;Nathan Dahlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1&quot;&gt;Rahul Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nayyar_A/0/1/0/all/0/1&quot;&gt;Ashutosh Nayyar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12574">
<title>Exploring the Integration Strategies of Retriever and Large Language Models. (arXiv:2308.12574v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2308.12574</link>
<description rdf:parseType="Literal">&lt;p&gt;The integration of retrieved passages and large language models (LLMs), such
as ChatGPTs, has significantly contributed to improving open-domain question
answering. However, there is still a lack of exploration regarding the optimal
approach for incorporating retrieved passages into the answer generation
process. This paper aims to fill this gap by investigating different methods of
combining retrieved passages with LLMs to enhance answer generation. We begin
by examining the limitations of a commonly-used concatenation approach.
Surprisingly, this approach often results in generating &quot;unknown&quot; outputs, even
when the correct document is among the top-k retrieved passages. To address
this issue, we explore four alternative strategies for integrating the
retrieved passages with the LLMs. These strategies include two single-round
methods that utilize chain-of-thought reasoning and two multi-round strategies
that incorporate feedback loops. Through comprehensive analyses and
experiments, we provide insightful observations on how to effectively leverage
retrieved passages to enhance the answer generation capability of LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Ye Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1&quot;&gt;Semih Yavuz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1&quot;&gt;Rui Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moorthy_M/0/1/0/all/0/1&quot;&gt;Meghana Moorthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1&quot;&gt;Shafiq Joty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yingbo Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12577">
<title>REB: Reducing Biases in Representation for Industrial Anomaly Detection. (arXiv:2308.12577v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12577</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing K-nearest neighbor (KNN) retrieval-based methods usually conduct
industrial anomaly detection in two stages: obtain feature representations with
a pre-trained CNN model and perform distance measures for defect detection.
However, the features are not fully exploited as they ignore domain bias and
the difference of local density in feature space, which limits the detection
performance. In this paper, we propose Reducing Biases (REB) in representation
by considering the domain bias of the pre-trained model and building a
self-supervised learning task for better domain adaption with a defect
generation strategy (DefectMaker) imitating the natural defects. Additionally,
we propose a local density KNN (LDKNN) to reduce the local density bias and
obtain effective anomaly detection. We achieve a promising result of 99.5\%
AUROC on the widely used MVTec AD benchmark. We also achieve 88.0\% AUROC on
the challenging MVTec LOCO AD dataset and bring an improvement of 4.7\% AUROC
to the state-of-the-art result. All results are obtained with smaller backbone
networks such as Vgg11 and Resnet18, which indicates the effectiveness and
efficiency of REB for practical industrial applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Shuai Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_D/0/1/0/all/0/1&quot;&gt;Dongmei Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1&quot;&gt;Waikeung Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12578">
<title>Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models. (arXiv:2308.12578v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12578</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent researches indicate that Pre-trained Large Language Models (LLMs)
possess cognitive constructs similar to those observed in humans, prompting
researchers to investigate the cognitive aspects of LLMs. This paper focuses on
explicit and implicit social bias, a distinctive two-level cognitive construct
in psychology. It posits that individuals&apos; explicit social bias, which is their
conscious expression of bias in the statements, may differ from their implicit
social bias, which represents their unconscious bias. We propose a two-stage
approach and discover a parallel phenomenon in LLMs known as &quot;re-judge
inconsistency&quot; in social bias. In the initial stage, the LLM is tasked with
automatically completing statements, potentially incorporating implicit social
bias. However, in the subsequent stage, the same LLM re-judges the biased
statement generated by itself but contradicts it. We propose that this re-judge
inconsistency can be similar to the inconsistency between human&apos;s unaware
implicit social bias and their aware explicit social bias. Experimental
investigations on ChatGPT and GPT-4 concerning common gender biases examined in
psychology corroborate the highly stable nature of the re-judge inconsistency.
This finding may suggest that diverse cognitive constructs emerge as LLMs&apos;
capabilities strengthen. Consequently, leveraging psychological theories can
provide enhanced insights into the underlying mechanisms governing the
expressions of explicit and implicit constructs in LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yachao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Dongming Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ruifang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1&quot;&gt;Yuexian Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12581">
<title>A Huber Loss Minimization Approach to Byzantine Robust Federated Learning. (arXiv:2308.12581v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.12581</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning systems are susceptible to adversarial attacks. To combat
this, we introduce a novel aggregator based on Huber loss minimization, and
provide a comprehensive theoretical analysis. Under independent and identically
distributed (i.i.d) assumption, our approach has several advantages compared to
existing methods. Firstly, it has optimal dependence on $\epsilon$, which
stands for the ratio of attacked clients. Secondly, our approach does not need
precise knowledge of $\epsilon$. Thirdly, it allows different clients to have
unequal data sizes. We then broaden our analysis to include non-i.i.d data,
such that clients have slightly different distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Puning Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1&quot;&gt;Zhiguo Wan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12591">
<title>SICNN: Soft Interference Cancellation Inspired Neural Network Equalizers. (arXiv:2308.12591v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.12591</link>
<description rdf:parseType="Literal">&lt;p&gt;Equalization is an important task at the receiver side of a digital wireless
communication system, which is traditionally conducted with model-based
estimation methods. Among the numerous options for model-based equalization,
iterative soft interference cancellation (SIC) is a well-performing approach
since error propagation caused by hard decision data symbol estimation during
the iterative estimation procedure is avoided. However, the model-based method
suffers from high computational complexity and performance degradation due to
required approximations. In this work, we propose a novel neural network
(NN-)based equalization approach, referred to as SICNN, which is designed by
deep unfolding of a model-based iterative SIC method, eliminating the main
disadvantages of its model-based counterpart. We present different variants of
SICNN. SICNNv1 is very similar to the model-based method, and is specifically
tailored for single carrier frequency domain equalization systems, which is the
communication system we regard in this work. The second variant, SICNNv2, is
more universal, and is applicable as an equalizer in any communication system
with a block-based data transmission scheme. We highlight the pros and cons of
both variants. Moreover, for both SICNNv1 and SICNNv2 we present a version with
a highly reduced number of learnable parameters. We compare the achieved bit
error ratio performance of the proposed NN-based equalizers with
state-of-the-art model-based and NN-based approaches, highlighting the
superiority of SICNNv1 over all other methods. Also, we present a thorough
complexity analysis of the proposed NN-based equalization approaches, and we
investigate the influence of the training set size on the performance of
NN-based equalizers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baumgartner_S/0/1/0/all/0/1&quot;&gt;Stefan Baumgartner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lang_O/0/1/0/all/0/1&quot;&gt;Oliver Lang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huemer_M/0/1/0/all/0/1&quot;&gt;Mario Huemer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12605">
<title>APLA: Additional Perturbation for Latent Noise with Adversarial Training Enables Consistency. (arXiv:2308.12605v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12605</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have exhibited promising progress in video generation.
However, they often struggle to retain consistent details within local regions
across frames. One underlying cause is that traditional diffusion models
approximate Gaussian noise distribution by utilizing predictive noise, without
fully accounting for the impact of inherent information within the input
itself. Additionally, these models emphasize the distinction between
predictions and references, neglecting information intrinsic to the videos. To
address this limitation, inspired by the self-attention mechanism, we propose a
novel text-to-video (T2V) generation network structure based on diffusion
models, dubbed Additional Perturbation for Latent noise with Adversarial
training (APLA). Our approach only necessitates a single video as input and
builds upon pre-trained stable diffusion networks. Notably, we introduce an
additional compact network, known as the Video Generation Transformer (VGT).
This auxiliary component is designed to extract perturbations from the inherent
information contained within the input, thereby refining inconsistent pixels
during temporal predictions. We leverage a hybrid architecture of transformers
and convolutions to compensate for temporal intricacies, enhancing consistency
between different frames within the video. Experiments demonstrate a noticeable
improvement in the consistency of the generated videos both qualitatively and
quantitatively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yupu Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Shangqi Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zihan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Harry Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1&quot;&gt;Liang-Jian Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12606">
<title>A Greedy Approach for Offering to Telecom Subscribers. (arXiv:2308.12606v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2308.12606</link>
<description rdf:parseType="Literal">&lt;p&gt;Customer retention or churn prevention is a challenging task of a telecom
operator. One of the effective approaches is to offer some attractive incentive
or additional services or money to the subscribers for keeping them engaged and
make sure they stay in the operator&apos;s network for longer time. Often, operators
allocate certain amount of monetary budget to carry out the offer campaign. The
difficult part of this campaign is the selection of a set of customers from a
large subscriber-base and deciding the amount that should be offered to an
individual so that operator&apos;s objective is achieved. There may be multiple
objectives (e.g., maximizing revenue, minimizing number of churns) for
selection of subscriber and selection of an offer to the selected subscriber.
Apart from monetary benefit, offers may include additional data, SMS, hots-spot
tethering, and many more. This problem is known as offer optimization. In this
paper, we propose a novel combinatorial algorithm for solving offer
optimization under heterogeneous offers by maximizing expected revenue under
the scenario of subscriber churn, which is, in general, seen in telecom domain.
The proposed algorithm is efficient and accurate even for a very large
subscriber-base.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bhunre_P/0/1/0/all/0/1&quot;&gt;Piyush Kanti Bhunre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sen_T/0/1/0/all/0/1&quot;&gt;Tanmay Sen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sarkar_A/0/1/0/all/0/1&quot;&gt;Arijit Sarkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12634">
<title>Towards Hierarchical Regional Transformer-based Multiple Instance Learning. (arXiv:2308.12634v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12634</link>
<description rdf:parseType="Literal">&lt;p&gt;The classification of gigapixel histopathology images with deep multiple
instance learning models has become a critical task in digital pathology and
precision medicine. In this work, we propose a Transformer-based multiple
instance learning approach that replaces the traditional learned attention
mechanism with a regional, Vision Transformer inspired self-attention
mechanism. We present a method that fuses regional patch information to derive
slide-level predictions and show how this regional aggregation can be stacked
to hierarchically process features on different distance levels. To increase
predictive accuracy, especially for datasets with small, local morphological
features, we introduce a method to focus the image processing on high attention
regions during inference. Our approach is able to significantly improve
performance over the baseline on two histopathology datasets and points towards
promising directions for further research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cersovsky_J/0/1/0/all/0/1&quot;&gt;Josef Cersovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1&quot;&gt;Sadegh Mohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kainmueller_D/0/1/0/all/0/1&quot;&gt;Dagmar Kainmueller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoehne_J/0/1/0/all/0/1&quot;&gt;Johannes Hoehne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12635">
<title>Advancing Hungarian Text Processing with HuSpaCy: Efficient and Accurate NLP Pipelines. (arXiv:2308.12635v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12635</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a set of industrial-grade text processing models for
Hungarian that achieve near state-of-the-art performance while balancing
resource efficiency and accuracy. Models have been implemented in the spaCy
framework, extending the HuSpaCy toolkit with several improvements to its
architecture. Compared to existing NLP tools for Hungarian, all of our
pipelines feature all basic text processing steps including tokenization,
sentence-boundary detection, part-of-speech tagging, morphological feature
tagging, lemmatization, dependency parsing and named entity recognition with
high accuracy and throughput. We thoroughly evaluated the proposed
enhancements, compared the pipelines with state-of-the-art tools and
demonstrated the competitive performance of the new models in all text
preprocessing steps. All experiments are reproducible and the pipelines are
freely available under a permissive license.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orosz_G/0/1/0/all/0/1&quot;&gt;Gy&amp;#xf6;rgy Orosz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szabo_G/0/1/0/all/0/1&quot;&gt;Gerg&amp;#x151; Szab&amp;#xf3;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berkecz_P/0/1/0/all/0/1&quot;&gt;P&amp;#xe9;ter Berkecz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szanto_Z/0/1/0/all/0/1&quot;&gt;Zsolt Sz&amp;#xe1;nt&amp;#xf3;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farkas_R/0/1/0/all/0/1&quot;&gt;Rich&amp;#xe1;rd Farkas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12649">
<title>APART: Diverse Skill Discovery using All Pairs with Ascending Reward and DropouT. (arXiv:2308.12649v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.12649</link>
<description rdf:parseType="Literal">&lt;p&gt;We study diverse skill discovery in reward-free environments, aiming to
discover all possible skills in simple grid-world environments where prior
methods have struggled to succeed. This problem is formulated as mutual
training of skills using an intrinsic reward and a discriminator trained to
predict a skill given its trajectory. Our initial solution replaces the
standard one-vs-all (softmax) discriminator with a one-vs-one (all pairs)
discriminator and combines it with a novel intrinsic reward function and a
dropout regularization technique. The combined approach is named APART: Diverse
Skill Discovery using All Pairs with Ascending Reward and Dropout. We
demonstrate that APART discovers all the possible skills in grid worlds with
remarkably fewer samples than previous works. Motivated by the empirical
success of APART, we further investigate an even simpler algorithm that
achieves maximum skills by altering VIC, rescaling its intrinsic reward, and
tuning the temperature of its softmax discriminator. We believe our findings
shed light on the crucial factors underlying success of skill discovery
algorithms in reinforcement learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galler_H/0/1/0/all/0/1&quot;&gt;Hadar Schreiber Galler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zahavy_T/0/1/0/all/0/1&quot;&gt;Tom Zahavy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desjardins_G/0/1/0/all/0/1&quot;&gt;Guillaume Desjardins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1&quot;&gt;Alon Cohen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12659">
<title>kTrans: Knowledge-Aware Transformer for Binary Code Embedding. (arXiv:2308.12659v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2308.12659</link>
<description rdf:parseType="Literal">&lt;p&gt;Binary Code Embedding (BCE) has important applications in various reverse
engineering tasks such as binary code similarity detection, type recovery,
control-flow recovery and data-flow analysis. Recent studies have shown that
the Transformer model can comprehend the semantics of binary code to support
downstream tasks. However, existing models overlooked the prior knowledge of
assembly language. In this paper, we propose a novel Transformer-based
approach, namely kTrans, to generate knowledge-aware binary code embedding. By
feeding explicit knowledge as additional inputs to the Transformer, and fusing
implicit knowledge with a novel pre-training task, kTrans provides a new
perspective to incorporating domain knowledge into a Transformer framework. We
inspect the generated embeddings with outlier detection and visualization, and
also apply kTrans to 3 downstream tasks: Binary Code Similarity Detection
(BCSD), Function Type Recovery (FTR) and Indirect Call Recognition (ICR).
Evaluation results show that kTrans can generate high-quality binary code
embeddings, and outperforms state-of-the-art (SOTA) approaches on downstream
tasks by 5.2%, 6.8%, and 12.6% respectively. kTrans is publicly available at:
https://github.com/Learner0x5a/kTrans-release
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wenyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuchen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sha_Z/0/1/0/all/0/1&quot;&gt;Zihan Sha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zeyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12661">
<title>Don&apos;t Look into the Sun: Adversarial Solarization Attacks on Image Classifiers. (arXiv:2308.12661v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12661</link>
<description rdf:parseType="Literal">&lt;p&gt;Assessing the robustness of deep neural networks against out-of-distribution
inputs is crucial, especially in safety-critical domains like autonomous
driving, but also in safety systems where malicious actors can digitally alter
inputs to circumvent safety guards. However, designing effective
out-of-distribution tests that encompass all possible scenarios while
preserving accurate label information is a challenging task. Existing
methodologies often entail a compromise between variety and constraint levels
for attacks and sometimes even both. In a first step towards a more holistic
robustness evaluation of image classification models, we introduce an attack
method based on image solarization that is conceptually straightforward yet
avoids jeopardizing the global structure of natural images independent of the
intensity. Through comprehensive evaluations of multiple ImageNet models, we
demonstrate the attack&apos;s capacity to degrade accuracy significantly, provided
it is not integrated into the training augmentations. Interestingly, even then,
no full immunity to accuracy deterioration is achieved. In other settings, the
attack can often be simplified into a black-box attack with model-independent
parameters. Defenses against other corruptions do not consistently extend to be
effective against our specific attack.
&lt;/p&gt;
&lt;p&gt;Project website: https://github.com/paulgavrikov/adversarial_solarization
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gavrikov_P/0/1/0/all/0/1&quot;&gt;Paul Gavrikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1&quot;&gt;Janis Keuper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12674">
<title>Improving Translation Faithfulness of Large Language Models via Augmenting Instructions. (arXiv:2308.12674v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12674</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) present strong general capabilities, and a
current compelling challenge is stimulating their specialized capabilities,
such as machine translation, through low-cost instruction tuning. The standard
instruction-following data is sequentially organized as the concatenation of an
instruction, an input, and a response. As the attention mechanism of LLMs has
limitations on local focus, LLMs tend to focus more on the words or sentences
nearby at each position. This leads to a high risk of instruction forgetting
during decoding. To alleviate the above issues, We propose SWIE
(Segment-Weighted Instruction Embedding) and an instruction-following dataset
OVERMISS. SWIE improves the model instruction understanding by adding a global
instruction representation on the following input and response representations.
OVERMISS improves model faithfulness by comparing over-translation and
miss-translation results with the correct translation. We apply our methods to
two main-stream open-source LLMs, BLOOM and LLaMA. The experimental results
demonstrate significant improvements in translation performance with SWIE based
on BLOOMZ-3b, particularly in zero-shot and long text translations due to
reduced instruction forgetting risk. Additionally, OVERMISS outperforms the
baseline in translation performance (e.g. an increase in BLEU scores from 0.69
to 3.12 and an average improvement of 0.48 percentage comet scores for
LLaMA-7b) with further enhancements seen in models combining OVERMISS and SWIE
(e.g. the BLUE scores increase up to 0.56 from English to German across three
different backbones), and both exhibit improvements in the faithfulness metric
based on word alignment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yijie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yijin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1&quot;&gt;Fandong Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yufeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jinan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12681">
<title>LR-XFL: Logical Reasoning-based Explainable Federated Learning. (arXiv:2308.12681v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.12681</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) is an emerging approach for training machine learning
models collaboratively while preserving data privacy. The need for privacy
protection makes it difficult for FL models to achieve global transparency and
explainability. To address this limitation, we incorporate logic-based
explanations into FL by proposing the Logical Reasoning-based eXplainable
Federated Learning (LR-XFL) approach. Under LR-XFL, FL clients create local
logic rules based on their local data and send them, along with model updates,
to the FL server. The FL server connects the local logic rules through a proper
logical connector that is derived based on properties of client data, without
requiring access to the raw data. In addition, the server also aggregates the
local model updates with weight values determined by the quality of the
clients&apos; local data as reflected by their uploaded logic rules. The results
show that LR-XFL outperforms the most relevant baseline by 1.19%, 5.81% and
5.41% in terms of classification accuracy, rule accuracy and rule fidelity,
respectively. The explicit rule evaluation and expression under LR-XFL enable
human experts to validate and correct the rules on the server side, hence
improving the global FL model&apos;s robustness to errors. It has the potential to
enhance the transparency of FL models for areas like healthcare and finance
where both data privacy and explainability are important.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanci Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Han Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12682">
<title>SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge. (arXiv:2308.12682v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.12682</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have demonstrated impressive planning abilities
due to their vast &quot;world knowledge&quot;. Yet, obtaining plans that are both
feasible (grounded in affordances) and cost-effective (in plan length), remains
a challenge, despite recent progress. This contrasts with heuristic planning
methods that employ domain knowledge (formalized in action models such as PDDL)
and heuristic search to generate feasible, optimal plans. Inspired by this, we
propose to combine the power of LLMs and heuristic planning by leveraging the
world knowledge of LLMs and the principles of heuristic search. Our approach,
SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain
knowledge, that evaluates actions&apos; feasibility (Can) and long-term
reward/payoff (Pay), and heuristic search to select the best sequence of
actions. Our contributions are (1) a novel framing of the LLM planning problem
in the context of heuristic planning, (2) integrating grounding and
cost-effective elements into the generated plans, and (3) using heuristic
search over actions. Our extensive evaluations show that our model surpasses
other LLM planning approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hazra_R/0/1/0/all/0/1&quot;&gt;Rishi Hazra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martires_P/0/1/0/all/0/1&quot;&gt;Pedro Zuidberg Dos Martires&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raedt_L/0/1/0/all/0/1&quot;&gt;Luc De Raedt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12714">
<title>VIGC: Visual Instruction Generation and Correction. (arXiv:2308.12714v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12714</link>
<description rdf:parseType="Literal">&lt;p&gt;The integration of visual encoders and large language models (LLMs) has
driven recent progress in multimodal large language models (MLLMs). However,
the scarcity of high-quality instruction-tuning data for vision-language tasks
remains a challenge. The current leading paradigm, such as LLaVA, relies on
language-only GPT-4 to generate data, which requires pre-annotated image
captions and detection bounding boxes, suffering from understanding image
details. A practical solution to this problem would be to utilize the available
multimodal large language models (MLLMs) to generate instruction data for
vision-language tasks. However, it&apos;s worth noting that the currently accessible
MLLMs are not as powerful as their LLM counterparts, as they tend to produce
inadequate responses and generate false information. As a solution for
addressing the current issue, this paper proposes the Visual Instruction
Generation and Correction (VIGC) framework that enables multimodal large
language models to generate instruction-tuning data and progressively enhance
its quality on-the-fly. Specifically, Visual Instruction Generation (VIG)
guides the vision-language model to generate diverse instruction-tuning data.
To ensure generation quality, Visual Instruction Correction (VIC) adopts an
iterative update mechanism to correct any inaccuracies in data produced by VIG,
effectively reducing the risk of hallucination. Leveraging the diverse,
high-quality data generated by VIGC, we finetune mainstream models and validate
data quality based on various evaluations. Experimental results demonstrate
that VIGC not only compensates for the shortcomings of language-only data
generation methods, but also effectively enhances the benchmark performance.
The models, datasets, and code will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiao Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jiahui Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1&quot;&gt;Huaping Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xiaoyi Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weijia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1&quot;&gt;Conghui He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12726">
<title>Continuous Reinforcement Learning-based Dynamic Difficulty Adjustment in a Visual Working Memory Game. (arXiv:2308.12726v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2308.12726</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic Difficulty Adjustment (DDA) is a viable approach to enhance a
player&apos;s experience in video games. Recently, Reinforcement Learning (RL)
methods have been employed for DDA in non-competitive games; nevertheless, they
rely solely on discrete state-action space with a small search space. In this
paper, we propose a continuous RL-based DDA methodology for a visual working
memory (VWM) game to handle the complex search space for the difficulty of
memorization. The proposed RL-based DDA tailors game difficulty based on the
player&apos;s score and game difficulty in the last trial. We defined a continuous
metric for the difficulty of memorization. Then, we consider the task
difficulty and the vector of difficulty-score as the RL&apos;s action and state,
respectively. We evaluated the proposed method through a within-subject
experiment involving 52 subjects. The proposed approach was compared with two
rule-based difficulty adjustment methods in terms of player&apos;s score and game
experience measured by a questionnaire. The proposed RL-based approach resulted
in a significantly better game experience in terms of competence, tension, and
negative and positive affect. Players also achieved higher scores and win
rates. Furthermore, the proposed RL-based DDA led to a significantly less
decline in the score in a 20-trial session.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahimi_M/0/1/0/all/0/1&quot;&gt;Masoud Rahimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1&quot;&gt;Hadi Moradi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vahabie_A/0/1/0/all/0/1&quot;&gt;Abdol-hossein Vahabie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kebriaei_H/0/1/0/all/0/1&quot;&gt;Hamed Kebriaei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12727">
<title>DeepLOC: Deep Learning-based Bone Pathology Localization and Classification in Wrist X-ray Images. (arXiv:2308.12727v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12727</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, computer-aided diagnosis systems have shown great potential
in assisting radiologists with accurate and efficient medical image analysis.
This paper presents a novel approach for bone pathology localization and
classification in wrist X-ray images using a combination of YOLO (You Only Look
Once) and the Shifted Window Transformer (Swin) with a newly proposed block.
The proposed methodology addresses two critical challenges in wrist X-ray
analysis: accurate localization of bone pathologies and precise classification
of abnormalities. The YOLO framework is employed to detect and localize bone
pathologies, leveraging its real-time object detection capabilities.
Additionally, the Swin, a transformer-based module, is utilized to extract
contextual information from the localized regions of interest (ROIs) for
accurate classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dibo_R/0/1/0/all/0/1&quot;&gt;Razan Dibo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galichin_A/0/1/0/all/0/1&quot;&gt;Andrey Galichin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Astashev_P/0/1/0/all/0/1&quot;&gt;Pavel Astashev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dylov_D/0/1/0/all/0/1&quot;&gt;Dmitry V. Dylov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rogov_O/0/1/0/all/0/1&quot;&gt;Oleg Y. Rogov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12737">
<title>Asymmetric Co-Training with Explainable Cell Graph Ensembling for Histopathological Image Classification. (arXiv:2308.12737v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12737</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks excel in histopathological image
classification, yet their pixel-level focus hampers explainability. Conversely,
emerging graph convolutional networks spotlight cell-level features and medical
implications. However, limited by their shallowness and suboptimal use of
high-dimensional pixel data, GCNs underperform in multi-class histopathological
image classification. To make full use of pixel-level and cell-level features
dynamically, we propose an asymmetric co-training framework combining a deep
graph convolutional network and a convolutional neural network for multi-class
histopathological image classification. To improve the explainability of the
entire framework by embedding morphological and topological distribution of
cells, we build a 14-layer deep graph convolutional network to handle cell
graph data. For the further utilization and dynamic interactions between
pixel-level and cell-level information, we also design a co-training strategy
to integrate the two asymmetric branches. Notably, we collect a private
clinically acquired dataset termed LUAD7C, including seven subtypes of lung
adenocarcinoma, which is rare and more challenging. We evaluated our approach
on the private LUAD7C and public colorectal cancer datasets, showcasing its
superior performance, explainability, and generalizability in multi-class
histopathological image classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ziqi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhongyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiangde Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xingguang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dou Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chaoqun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1&quot;&gt;Xiaoying Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Meng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1&quot;&gt;Long Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12740">
<title>Human Comprehensible Active Learning of Genome-Scale Metabolic Networks. (arXiv:2308.12740v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.12740</link>
<description rdf:parseType="Literal">&lt;p&gt;An important application of Synthetic Biology is the engineering of the host
cell system to yield useful products. However, an increase in the scale of the
host system leads to huge design space and requires a large number of
validation trials with high experimental costs. A comprehensible machine
learning approach that efficiently explores the hypothesis space and guides
experimental design is urgently needed for the Design-Build-Test-Learn (DBTL)
cycle of the host cell system. We introduce a novel machine learning framework
ILP-iML1515 based on Inductive Logic Programming (ILP) that performs abductive
logical reasoning and actively learns from training examples. In contrast to
numerical models, ILP-iML1515 is built on comprehensible logical
representations of a genome-scale metabolic model and can update the model by
learning new logical structures from auxotrophic mutant trials. The ILP-iML1515
framework 1) allows high-throughput simulations and 2) actively selects
experiments that reduce the experimental cost of learning gene functions in
comparison to randomly selected experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ai_L/0/1/0/all/0/1&quot;&gt;Lun Ai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1&quot;&gt;Shi-Shun Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Wang-Zhou Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hallett_L/0/1/0/all/0/1&quot;&gt;Liam Hallett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muggleton_S/0/1/0/all/0/1&quot;&gt;Stephen H. Muggleton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldwin_G/0/1/0/all/0/1&quot;&gt;Geoff S. Baldwin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12747">
<title>Separating the Human Touch from AI-Generated Text using Higher Criticism: An Information-Theoretic Approach. (arXiv:2308.12747v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2308.12747</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method to determine whether a given article was entirely written
by a generative language model versus an alternative situation in which the
article includes some significant edits by a different author, possibly a
human. Our process involves many perplexity tests for the origin of individual
sentences or other text atoms, combining these multiple tests using Higher
Criticism (HC). As a by-product, the method identifies parts suspected to be
edited. The method is motivated by the convergence of the log-perplexity to the
cross-entropy rate and by a statistical model for edited text saying that
sentences are mostly generated by the language model, except perhaps for a few
sentences that might have originated via a different mechanism. We demonstrate
the effectiveness of our method using real data and analyze the factors
affecting its success. This analysis raises several interesting open challenges
whose resolution may improve the method&apos;s effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kipnis_A/0/1/0/all/0/1&quot;&gt;Alon Kipnis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12751">
<title>Motion In-Betweening with Phase Manifolds. (arXiv:2308.12751v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2308.12751</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel data-driven motion in-betweening system to
reach target poses of characters by making use of phases variables learned by a
Periodic Autoencoder. Our approach utilizes a mixture-of-experts neural network
model, in which the phases cluster movements in both space and time with
different expert weights. Each generated set of weights then produces a
sequence of poses in an autoregressive manner between the current and target
state of the character. In addition, to satisfy poses which are manually
modified by the animators or where certain end effectors serve as constraints
to be reached by the animation, a learned bi-directional control scheme is
implemented to satisfy such constraints. The results demonstrate that using
phases for motion in-betweening tasks sharpen the interpolated movements, and
furthermore stabilizes the learning process. Moreover, using phases for motion
in-betweening tasks can also synthesize more challenging movements beyond
locomotion behaviors. Additionally, style control is enabled between given
target keyframes. Our proposed framework can compete with popular
state-of-the-art methods for motion in-betweening in terms of motion quality
and generalization, especially in the existence of long transition durations.
Our framework contributes to faster prototyping workflows for creating animated
character sequences, which is of enormous interest for the game and film
industry.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Starke_P/0/1/0/all/0/1&quot;&gt;Paul Starke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Starke_S/0/1/0/all/0/1&quot;&gt;Sebastian Starke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1&quot;&gt;Taku Komura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinicke_F/0/1/0/all/0/1&quot;&gt;Frank Steinicke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12755">
<title>Acquiring Qualitative Explainable Graphs for Automated Driving Scene Interpretation. (arXiv:2308.12755v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.12755</link>
<description rdf:parseType="Literal">&lt;p&gt;The future of automated driving (AD) is rooted in the development of robust,
fair and explainable artificial intelligence methods. Upon request, automated
vehicles must be able to explain their decisions to the driver and the car
passengers, to the pedestrians and other vulnerable road users and potentially
to external auditors in case of accidents. However, nowadays, most explainable
methods still rely on quantitative analysis of the AD scene representations
captured by multiple sensors. This paper proposes a novel representation of AD
scenes, called Qualitative eXplainable Graph (QXG), dedicated to qualitative
spatiotemporal reasoning of long-term scenes. The construction of this graph
exploits the recent Qualitative Constraint Acquisition paradigm. Our
experimental results on NuScenes, an open real-world multi-modal dataset, show
that the qualitative eXplainable graph of an AD scene composed of 40 frames can
be computed in real-time and light in space storage which makes it a
potentially interesting tool for improved and more trustworthy perception and
control processes in AD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belmecheri_N/0/1/0/all/0/1&quot;&gt;Nassim Belmecheri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gotlieb_A/0/1/0/all/0/1&quot;&gt;Arnaud Gotlieb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazaar_N/0/1/0/all/0/1&quot;&gt;Nadjib Lazaar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spieker_H/0/1/0/all/0/1&quot;&gt;Helge Spieker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12794">
<title>Job Shop Scheduling Benchmark: Environments and Instances for Learning and Non-learning Methods. (arXiv:2308.12794v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.12794</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce an open-source GitHub repository containing comprehensive
benchmarks for a wide range of machine scheduling problems, including Job Shop
Scheduling (JSP), Flow Shop Scheduling (FSP), Flexible Job Shop Scheduling
(FJSP), FJSP with Assembly constraints (FAJSP), FJSP with Sequence-Dependent
Setup Times (FJSP-SDST), and the online FJSP (with online job arrivals). Our
primary goal is to provide a centralized hub for researchers, practitioners,
and enthusiasts interested in tackling machine scheduling challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reijnen_R/0/1/0/all/0/1&quot;&gt;Robbert Reijnen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Straaten_K/0/1/0/all/0/1&quot;&gt;Kjell van Straaten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bukhsh_Z/0/1/0/all/0/1&quot;&gt;Zaharah Bukhsh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yingqian Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12800">
<title>ICU Mortality Prediction Using Long Short-Term Memory Networks. (arXiv:2308.12800v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.12800</link>
<description rdf:parseType="Literal">&lt;p&gt;Extensive bedside monitoring in Intensive Care Units (ICUs) has resulted in
complex temporal data regarding patient physiology, which presents an upscale
context for clinical data analysis. In the other hand, identifying the
time-series patterns within these data may provide a high aptitude to predict
clinical events. Hence, we investigate, during this work, the implementation of
an automatic data-driven system, which analyzes large amounts of multivariate
temporal data derived from Electronic Health Records (EHRs), and extracts
high-level information so as to predict in-hospital mortality and Length of
Stay (LOS) early. Practically, we investigate the applicability of LSTM network
by reducing the time-frame to 6-hour so as to enhance clinical tasks. The
experimental results highlight the efficiency of LSTM model with rigorous
multivariate time-series measurements for building real-world prediction
engines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mili_M/0/1/0/all/0/1&quot;&gt;Manel Mili&lt;/a&gt; (FSM, TIM), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerkeni_A/0/1/0/all/0/1&quot;&gt;Asma Kerkeni&lt;/a&gt; (ISIMM, TIM), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdallah_A/0/1/0/all/0/1&quot;&gt;Asma Ben Abdallah&lt;/a&gt; (ISIMM, TIM), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bedoui_M/0/1/0/all/0/1&quot;&gt;Mohamed Hedi Bedoui&lt;/a&gt; (TIM)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12828">
<title>Short Run Transit Route Planning Decision Support System Using a Deep Learning-Based Weighted Graph. (arXiv:2308.12828v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.12828</link>
<description rdf:parseType="Literal">&lt;p&gt;Public transport routing plays a crucial role in transit network design,
ensuring a satisfactory level of service for passengers. However, current
routing solutions rely on traditional operational research heuristics, which
can be time-consuming to implement and lack the ability to provide quick
solutions. Here, we propose a novel deep learning-based methodology for a
decision support system that enables public transport (PT) planners to identify
short-term route improvements rapidly. By seamlessly adjusting specific
sections of routes between two stops during specific times of the day, our
method effectively reduces times and enhances PT services. Leveraging diverse
data sources such as GTFS and smart card data, we extract features and model
the transportation network as a directed graph. Using self-supervision, we
train a deep learning model for predicting lateness values for road segments.
&lt;/p&gt;
&lt;p&gt;These lateness values are then utilized as edge weights in the transportation
graph, enabling efficient path searching. Through evaluating the method on Tel
Aviv, we are able to reduce times on more than 9\% of the routes. The improved
routes included both intraurban and suburban routes showcasing a fact
highlighting the model&apos;s versatility. The findings emphasize the potential of
our data-driven decision support system to enhance public transport and city
logistics, promoting greater efficiency and reliability in PT services.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shalit_N/0/1/0/all/0/1&quot;&gt;Nadav Shalit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fire_M/0/1/0/all/0/1&quot;&gt;Michael Fire&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kagan_D/0/1/0/all/0/1&quot;&gt;Dima Kagan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Elia_E/0/1/0/all/0/1&quot;&gt;Eran Ben-Elia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12840">
<title>FaceTouch: Detecting hand-to-face touch with supervised contrastive learning to assist in tracing infectious disease. (arXiv:2308.12840v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12840</link>
<description rdf:parseType="Literal">&lt;p&gt;Through our respiratory system, many viruses and diseases frequently spread
and pass from one person to another. Covid-19 served as an example of how
crucial it is to track down and cut back on contacts to stop its spread. There
is a clear gap in finding automatic methods that can detect hand-to-face
contact in complex urban scenes or indoors. In this paper, we introduce a
computer vision framework, called FaceTouch, based on deep learning. It
comprises deep sub-models to detect humans and analyse their actions. FaceTouch
seeks to detect hand-to-face touches in the wild, such as through video chats,
bus footage, or CCTV feeds. Despite partial occlusion of faces, the introduced
system learns to detect face touches from the RGB representation of a given
scene by utilising the representation of the body gestures such as arm
movement. This has been demonstrated to be useful in complex urban scenarios
beyond simply identifying hand movement and its closeness to faces. Relying on
Supervised Contrastive Learning, the introduced model is trained on our
collected dataset, given the absence of other benchmark datasets. The framework
shows a strong validation in unseen datasets which opens the door for potential
deployment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1&quot;&gt;Mohamed R. Ibrahim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyons_T/0/1/0/all/0/1&quot;&gt;Terry Lyons&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12888">
<title>Inducing Causal Structure for Abstractive Text Summarization. (arXiv:2308.12888v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12888</link>
<description rdf:parseType="Literal">&lt;p&gt;The mainstream of data-driven abstractive summarization models tends to
explore the correlations rather than the causal relationships. Among such
correlations, there can be spurious ones which suffer from the language prior
learned from the training corpus and therefore undermine the overall
effectiveness of the learned model. To tackle this issue, we introduce a
Structural Causal Model (SCM) to induce the underlying causal structure of the
summarization data. We assume several latent causal factors and non-causal
factors, representing the content and style of the document and summary.
Theoretically, we prove that the latent factors in our SCM can be identified by
fitting the observed training data under certain conditions. On the basis of
this, we propose a Causality Inspired Sequence-to-Sequence model (CI-Seq2Seq)
to learn the causal representations that can mimic the causal factors, guiding
us to pursue causal information for summary generation. The key idea is to
reformulate the Variational Auto-encoder (VAE) to fit the joint distribution of
the document and summary variables from the training corpus. Experimental
results on two widely used text summarization datasets demonstrate the
advantages of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jiafeng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xueqi Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12890">
<title>Large Language Models Vote: Prompting for Rare Disease Identification. (arXiv:2308.12890v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12890</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of generative Large Language Models (LLMs) emphasizes the need
for accurate and efficient prompting approaches. LLMs are often applied in
Few-Shot Learning (FSL) contexts, where tasks are executed with minimal
training data. FSL has become popular in many Artificial Intelligence (AI)
subdomains, including AI for health. Rare diseases, affecting a small fraction
of the population, inherently require FSL techniques due to limited data
availability, though manual data collection and annotation is costly and
time-consuming. In this paper, we propose Models-Vote Prompting (MVP), a
flexible prompting approach for improving the performance of LLM queries in FSL
settings. MVP works by prompting numerous LLMs to perform the same tasks and
then conducting a majority vote on the resulting outputs. This method achieves
improved results to any one model in the ensemble on one-shot rare disease
identification and classification tasks. We also release a novel rare disease
dataset for FSL, available to those who agreed to the MIMIC-IV Data Use
Agreement (DUA). Furthermore, in using MVP, each model is prompted multiple
times, substantially increasing the time needed for manual annotation, and to
address this, we assess the feasibility of using JSON for automating generative
LLM evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oniani_D/0/1/0/all/0/1&quot;&gt;David Oniani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilsman_J/0/1/0/all/0/1&quot;&gt;Jordan Hilsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Fengyi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1&quot;&gt;Shiven Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanshan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12898">
<title>Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?. (arXiv:2308.12898v1 [cs.MM])</title>
<link>http://arxiv.org/abs/2308.12898</link>
<description rdf:parseType="Literal">&lt;p&gt;The multimedia community has shown a significant interest in perceiving and
representing the physical world with multimodal pretrained neural network
models, and among them, the visual-language pertaining (VLP) is, currently, the
most captivating topic. However, there have been few endeavors dedicated to the
exploration of 1) whether essential linguistic knowledge (e.g., semantics and
syntax) can be extracted during VLP, and 2) how such linguistic knowledge
impact or enhance the multimodal alignment. In response, here we aim to
elucidate the impact of comprehensive linguistic knowledge, including semantic
expression and syntactic structure, on multimodal alignment. Specifically, we
design and release the SNARE, the first large-scale multimodal alignment
probing benchmark, to detect the vital linguistic components, e.g., lexical,
semantic, and syntax knowledge, containing four tasks: Semantic structure,
Negation logic, Attribute ownership, and Relationship composition. Based on our
proposed probing benchmarks, our holistic analyses of five advanced VLP models
illustrate that the VLP model: i) shows insensitivity towards complex syntax
structures and relies on content words for sentence comprehension; ii)
demonstrates limited comprehension of combinations between sentences and
negations; iii) faces challenges in determining the presence of actions or
spatial relationships within visual information and struggles with verifying
the correctness of triple combinations. We make our benchmark and code
available at \url{https://github.com/WangFei-2019/SNARE/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Liang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1&quot;&gt;Jun Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Ye Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Changxing Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12902">
<title>CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement. (arXiv:2308.12902v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12902</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-light images, characterized by inadequate illumination, pose challenges
of diminished clarity, muted colors, and reduced details. Low-light image
enhancement, an essential task in computer vision, aims to rectify these issues
by improving brightness, contrast, and overall perceptual quality, thereby
facilitating accurate analysis and interpretation. This paper introduces the
Convolutional Dense Attention-guided Network (CDAN), a novel solution for
enhancing low-light images. CDAN integrates an autoencoder-based architecture
with convolutional and dense blocks, complemented by an attention mechanism and
skip connections. This architecture ensures efficient information propagation
and feature learning. Furthermore, a dedicated post-processing phase refines
color balance and contrast. Our approach demonstrates notable progress compared
to state-of-the-art results in low-light image enhancement, showcasing its
robustness across a wide range of challenging scenarios. Our model performs
remarkably on benchmark datasets, effectively mitigating under-exposure and
proficiently restoring textures and colors in diverse low-light scenarios. This
achievement underscores CDAN&apos;s potential for diverse computer vision tasks,
notably enabling robust object detection and recognition in challenging
low-light conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shakibania_H/0/1/0/all/0/1&quot;&gt;Hossein Shakibania&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raoufi_S/0/1/0/all/0/1&quot;&gt;Sina Raoufi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khotanlou_H/0/1/0/all/0/1&quot;&gt;Hassan Khotanlou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12915">
<title>Language as Reality: A Co-Creative Storytelling Game Experience in 1001 Nights using Generative AI. (arXiv:2308.12915v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2308.12915</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present &quot;1001 Nights&quot;, an AI-native game that allows
players lead in-game reality through co-created storytelling with the character
driven by large language model. The concept is inspired by Wittgenstein&apos;s idea
of the limits of one&apos;s world being determined by the bounds of their language.
Using advanced AI tools like GPT-4 and Stable Diffusion, the second iteration
of the game enables the protagonist, Shahrzad, to realize words and stories in
her world. The player can steer the conversation with the AI King towards
specific keywords, which then become battle equipment in the game. This blend
of interactive narrative and text-to-image transformation challenges the
conventional border between the game world and reality through a dual
perspective. We focus on Shahrzad, who seeks to alter her fate compared to the
original folklore, and the player, who collaborates with AI to craft narratives
and shape the game world. We explore the technical and design elements of
implementing such a game with an objective to enhance the narrative game genre
with AI-generated content and to delve into AI-native gameplay possibilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuqian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhouyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1&quot;&gt;Ke Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chang Hee Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asadipour_A/0/1/0/all/0/1&quot;&gt;Ali Asadipour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12918">
<title>Evaluating the Vulnerabilities in ML systems in terms of adversarial attacks. (arXiv:2308.12918v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.12918</link>
<description rdf:parseType="Literal">&lt;p&gt;There have been recent adversarial attacks that are difficult to find. These
new adversarial attacks methods may pose challenges to current deep learning
cyber defense systems and could influence the future defense of cyberattacks.
The authors focus on this domain in this research paper. They explore the
consequences of vulnerabilities in AI systems. This includes discussing how
they might arise, differences between randomized and adversarial examples and
also potential ethical implications of vulnerabilities. Moreover, it is
important to train the AI systems appropriately when they are in testing phase
and getting them ready for broader use.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harshith_J/0/1/0/all/0/1&quot;&gt;John Harshith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gill_M/0/1/0/all/0/1&quot;&gt;Mantej Singh Gill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jothimani_M/0/1/0/all/0/1&quot;&gt;Madhan Jothimani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12925">
<title>Low-count Time Series Anomaly Detection. (arXiv:2308.12925v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.12925</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-count time series describe sparse or intermittent events, which are
prevalent in large-scale online platforms that capture and monitor diverse data
types. Several distinct challenges surface when modelling low-count time
series, particularly low signal-to-noise ratios (when anomaly signatures are
provably undetectable), and non-uniform performance (when average metrics are
not representative of local behaviour). The time series anomaly detection
community currently lacks explicit tooling and processes to model and reliably
detect anomalies in these settings. We address this gap by introducing a novel
generative procedure for creating benchmark datasets comprising of low-count
time series with anomalous segments. Via a mixture of theoretical and empirical
analysis, our work explains how widely-used algorithms struggle with the
distribution overlap between normal and anomalous segments. In order to
mitigate this shortcoming, we then leverage our findings to demonstrate how
anomaly score smoothing consistently improves performance. The practical
utility of our analysis and recommendation is validated on a real-world dataset
containing sales data for retail stores.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renz_P/0/1/0/all/0/1&quot;&gt;Philipp Renz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cutajar_K/0/1/0/all/0/1&quot;&gt;Kurt Cutajar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Twomey_N/0/1/0/all/0/1&quot;&gt;Niall Twomey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_G/0/1/0/all/0/1&quot;&gt;Gavin K. C. Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1&quot;&gt;Hanting Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12956">
<title>DLIP: Distilling Language-Image Pre-training. (arXiv:2308.12956v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12956</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-Language Pre-training (VLP) shows remarkable progress with the
assistance of extremely heavy parameters, which challenges deployment in real
applications. Knowledge distillation is well recognized as the essential
procedure in model compression. However, existing knowledge distillation
techniques lack an in-depth investigation and analysis of VLP, and practical
guidelines for VLP-oriented distillation are still not yet explored. In this
paper, we present DLIP, a simple yet efficient Distilling Language-Image
Pre-training framework, through which we investigate how to distill a light VLP
model. Specifically, we dissect the model distillation from multiple
dimensions, such as the architecture characteristics of different modules and
the information transfer of different modalities. We conduct comprehensive
experiments and provide insights on distilling a light but performant VLP
model. Experimental results reveal that DLIP can achieve a state-of-the-art
accuracy/efficiency trade-off across diverse cross-modal tasks, e.g.,
image-text retrieval, image captioning and visual question answering. For
example, DLIP compresses BLIP by 1.9x, from 213M to 108M parameters, while
achieving comparable or better performance. Furthermore, DLIP succeeds in
retaining more than 95% of the performance with 22.4% parameters and 24.8%
FLOPs compared to the teacher model and accelerates inference speed by 2.7x.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_H/0/1/0/all/0/1&quot;&gt;Huafeng Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xiawu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Ming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xuefeng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1&quot;&gt;Min Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12967">
<title>NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes. (arXiv:2308.12967v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12967</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent implicit neural representations have shown great results for novel
view synthesis. However, existing methods require expensive per-scene
optimization from many views hence limiting their application to real-world
unbounded urban settings where the objects of interest or backgrounds are
observed from very few views. To mitigate this challenge, we introduce a new
approach called NeO 360, Neural fields for sparse view synthesis of outdoor
scenes. NeO 360 is a generalizable method that reconstructs 360{\deg} scenes
from a single or a few posed RGB images. The essence of our approach is in
capturing the distribution of complex real-world outdoor 3D scenes and using a
hybrid image-conditional triplanar representation that can be queried from any
world point. Our representation combines the best of both voxel-based and
bird&apos;s-eye-view (BEV) representations and is more effective and expressive than
each. NeO 360&apos;s representation allows us to learn from a large collection of
unbounded 3D scenes while offering generalizability to new views and novel
scenes from as few as a single image during inference. We demonstrate our
approach on the proposed challenging 360{\deg} unbounded dataset, called NeRDS
360, and show that NeO 360 outperforms state-of-the-art generalizable methods
for novel view synthesis while also offering editing and composition
capabilities. Project page:
https://zubair-irshad.github.io/projects/neo360.html
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irshad_M/0/1/0/all/0/1&quot;&gt;Muhammad Zubair Irshad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zakharov_S/0/1/0/all/0/1&quot;&gt;Sergey Zakharov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Katherine Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guizilini_V/0/1/0/all/0/1&quot;&gt;Vitor Guizilini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kollar_T/0/1/0/all/0/1&quot;&gt;Thomas Kollar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1&quot;&gt;Adrien Gaidon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1&quot;&gt;Zsolt Kira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1&quot;&gt;Rares Ambrus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.00350">
<title>The Interpretability of LSTM Models for Predicting Oil Company Stocks: Impact of Correlated Features. (arXiv:2201.00350v4 [q-fin.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2201.00350</link>
<description rdf:parseType="Literal">&lt;p&gt;Oil companies are among the largest companies in the world whose economic
indicators in the global stock market have a great impact on the world economy
and market due to their relation to gold, crude oil, and the dollar. This study
investigates the impact of correlated features on the interpretability of Long
Short-Term Memory (LSTM) models for predicting oil company stocks. To achieve
this, we designed a Standard Long Short-Term Memory (LSTM) network and trained
it using various correlated datasets. Our approach aims to improve the accuracy
of stock price prediction by considering the multiple factors affecting the
market, such as crude oil prices, gold prices, and the US dollar. The results
demonstrate that adding a feature correlated with oil stocks does not improve
the interpretability of LSTM models. These findings suggest that while LSTM
models may be effective in predicting stock prices, their interpretability may
be limited. Caution should be exercised when relying solely on LSTM models for
stock price prediction as their lack of interpretability may make it difficult
to fully understand the underlying factors driving stock price movements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Firouzjaee_J/0/1/0/all/0/1&quot;&gt;Javad T. Firouzjaee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Khaliliyan_P/0/1/0/all/0/1&quot;&gt;Pouriya Khaliliyan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.13310">
<title>MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection. (arXiv:2203.13310v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.13310</link>
<description rdf:parseType="Literal">&lt;p&gt;Monocular 3D object detection has long been a challenging task in autonomous
driving. Most existing methods follow conventional 2D detectors to first
localize object centers, and then predict 3D attributes by neighboring
features. However, only using local visual features is insufficient to
understand the scene-level 3D spatial structures and ignores the long-range
inter-object depth relations. In this paper, we introduce the first DETR
framework for Monocular DEtection with a depth-guided TRansformer, named
MonoDETR. We modify the vanilla transformer to be depth-aware and guide the
whole detection process by contextual depth cues. Specifically, concurrent to
the visual encoder that captures object appearances, we introduce to predict a
foreground depth map, and specialize a depth encoder to extract non-local depth
embeddings. Then, we formulate 3D object candidates as learnable queries and
propose a depth-guided decoder to conduct object-scene depth interactions. In
this way, each object query estimates its 3D attributes adaptively from the
depth-guided regions on the image and is no longer constrained to local visual
features. On KITTI benchmark with monocular images as input, MonoDETR achieves
state-of-the-art performance and requires no extra dense depth annotations.
Besides, our depth-guided modules can also be plug-and-play to enhance
multi-view 3D object detectors on nuScenes dataset, demonstrating our superior
generalization capacity. Code is available at
https://github.com/ZrrSkywalker/MonoDETR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Renrui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1&quot;&gt;Han Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Ziyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xuanzhuo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1&quot;&gt;Ziteng Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1&quot;&gt;Peng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.09107">
<title>Leveraging Global Binary Masks for Structure Segmentation in Medical Images. (arXiv:2205.09107v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.09107</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) models for medical image segmentation are highly
influenced by intensity variations of input images and lack generalization due
to primarily utilizing pixels&apos; intensity information for inference. Acquiring
sufficient training data is another challenge limiting models&apos; applications. We
proposed to leverage the consistency of organs&apos; anatomical shape and position
information in medical images. We introduced a framework leveraging recurring
anatomical patterns through global binary masks for organ segmentation. Two
scenarios were studied.1) Global binary masks were the only model&apos;s (i.e.
U-Net) input, forcing exclusively encoding organs&apos; position and shape
information for segmentation/localization.2) Global binary masks were
incorporated as an additional channel functioning as position/shape clues to
mitigate training data scarcity. Two datasets of the brain and heart CT images
with their ground-truth were split into (26:10:10) and (12:3:5) for training,
validation, and test respectively. Training exclusively on global binary masks
led to Dice scores of 0.77(0.06) and 0.85(0.04), with the average Euclidian
distance of 3.12(1.43)mm and 2.5(0.93)mm relative to the center of mass of the
ground truth for the brain and heart structures respectively. The outcomes
indicate that a surprising degree of position and shape information is encoded
through global binary masks. Incorporating global binary masks led to
significantly higher accuracy relative to the model trained on only CT images
in small subsets of training data; the performance improved by 4.3-125.3% and
1.3-48.1% for 1-8 training cases of the brain and heart datasets respectively.
The findings imply the advantages of utilizing global binary masks for building
generalizable models and to compensate for training data scarcity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kazemimoghadam_M/0/1/0/all/0/1&quot;&gt;Mahdieh Kazemimoghadam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingli Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Weiguo Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xuejun Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.07240">
<title>Test-Time Adaptation for Visual Document Understanding. (arXiv:2206.07240v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.07240</link>
<description rdf:parseType="Literal">&lt;p&gt;For visual document understanding (VDU), self-supervised pretraining has been
shown to successfully generate transferable representations, yet, effective
adaptation of such representations to distribution shifts at test-time remains
to be an unexplored area. We propose DocTTA, a novel test-time adaptation
method for documents, that does source-free domain adaptation using unlabeled
target document data. DocTTA leverages cross-modality self-supervised learning
via masked visual language modeling, as well as pseudo labeling to adapt models
learned on a \textit{source} domain to an unlabeled \textit{target} domain at
test time. We introduce new benchmarks using existing public datasets for
various VDU tasks, including entity recognition, key-value extraction, and
document visual question answering. DocTTA shows significant improvements on
these compared to the source model performance, up to 1.89\% in (F1 score),
3.43\% (F1 score), and 17.68\% (ANLS score), respectively. Our benchmark
datasets are available at \url{https://saynaebrahimi.github.io/DocTTA.html}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1&quot;&gt;Sayna Ebrahimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1&quot;&gt;Sercan O. Arik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1&quot;&gt;Tomas Pfister&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.12263">
<title>Augmenting Reinforcement Learning with Transformer-based Scene Representation Learning for Decision-making of Autonomous Driving. (arXiv:2208.12263v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.12263</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision-making for urban autonomous driving is challenging due to the
stochastic nature of interactive traffic participants and the complexity of
road structures. Although reinforcement learning (RL)-based decision-making
scheme is promising to handle urban driving scenarios, it suffers from low
sample efficiency and poor adaptability. In this paper, we propose Scene-Rep
Transformer to improve the RL decision-making capabilities with better scene
representation encoding and sequential predictive latent distillation.
Specifically, a multi-stage Transformer (MST) encoder is constructed to model
not only the interaction awareness between the ego vehicle and its neighbors
but also intention awareness between the agents and their candidate routes. A
sequential latent Transformer (SLT) with self-supervised learning objectives is
employed to distill the future predictive information into the latent scene
representation, in order to reduce the exploration space and speed up training.
The final decision-making module based on soft actor-critic (SAC) takes as
input the refined latent scene representation from the Scene-Rep Transformer
and outputs driving actions. The framework is validated in five challenging
simulated urban scenarios with dense traffic, and its performance is manifested
quantitatively by the substantial improvements in data efficiency and
performance in terms of success rate, safety, and efficiency. The qualitative
results reveal that our framework is able to extract the intentions of neighbor
agents to help make decisions and deliver more diversified driving behaviors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haochen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhiyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1&quot;&gt;Chen Lv&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.00939">
<title>Improving Sample Quality of Diffusion Models Using Self-Attention Guidance. (arXiv:2210.00939v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.00939</link>
<description rdf:parseType="Literal">&lt;p&gt;Denoising diffusion models (DDMs) have attracted attention for their
exceptional generation quality and diversity. This success is largely
attributed to the use of class- or text-conditional diffusion guidance methods,
such as classifier and classifier-free guidance. In this paper, we present a
more comprehensive perspective that goes beyond the traditional guidance
methods. From this generalized perspective, we introduce novel condition- and
training-free strategies to enhance the quality of generated images. As a
simple solution, blur guidance improves the suitability of intermediate samples
for their fine-scale information and structures, enabling diffusion models to
generate higher quality samples with a moderate guidance scale. Improving upon
this, Self-Attention Guidance (SAG) uses the intermediate self-attention maps
of diffusion models to enhance their stability and efficacy. Specifically, SAG
adversarially blurs only the regions that diffusion models attend to at each
iteration and guides them accordingly. Our experimental results show that our
SAG improves the performance of various diffusion models, including ADM, IDDPM,
Stable Diffusion, and DiT. Moreover, combining SAG with conventional guidance
methods leads to further improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Susung Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gyuseong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_W/0/1/0/all/0/1&quot;&gt;Wooseok Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seungryong Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.08471">
<title>Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion. (arXiv:2210.08471v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.08471</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based pre-trained models like BERT have achieved great progress
on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also
shown general benefits in multiple NLP tasks. However, how to efficiently
integrate dependency prior structure into pre-trained models to better model
complex semantic matching relations is still unsettled. In this paper, we
propose the \textbf{D}ependency-Enhanced \textbf{A}daptive \textbf{F}usion
\textbf{A}ttention (\textbf{DAFA}), which explicitly introduces dependency
structure into pre-trained models and adaptively fuses it with semantic
information. Specifically, \textbf{\emph{(i)}} DAFA first proposes a
structure-sensitive paradigm to construct a dependency matrix for calibrating
attention weights. It adopts an adaptive fusion module to integrate the
obtained dependency information and the original semantic signals. Moreover,
DAFA reconstructs the attention calculation flow and provides better
interpretability. By applying it on BERT, our method achieves state-of-the-art
or competitive performance on 10 public datasets, demonstrating the benefits of
adaptively fusing dependency structure in semantic matching task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jian Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1&quot;&gt;Di Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Rumei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuntao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sirui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1&quot;&gt;Minlong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yongxin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.00642">
<title>Farm-wide virtual load monitoring for offshore wind structures via Bayesian neural networks. (arXiv:2211.00642v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.00642</link>
<description rdf:parseType="Literal">&lt;p&gt;Offshore wind structures are subject to deterioration mechanisms throughout
their operational lifetime. Even if the deterioration evolution of structural
elements can be estimated through physics-based deterioration models, the
uncertainties involved in the process hurdle the selection of lifecycle
management decisions. In this scenario, the collection of relevant information
through an efficient monitoring system enables the reduction of uncertainties,
ultimately driving more optimal lifecycle decisions. However, a full monitoring
instrumentation implemented on all wind turbines in a farm might become
unfeasible due to practical and economical constraints. Besides, certain load
monitoring systems often become defective after a few years of marine
environment exposure. Addressing the aforementioned concerns, a farm-wide
virtual load monitoring scheme directed by a fleet-leader wind turbine offers
an attractive solution. Fetched with data retrieved from a fully-instrumented
wind turbine, a model can be trained and then deployed, thus yielding load
predictions of non-fully monitored wind turbines, from which only standard data
remains available. In this paper, we propose a virtual load monitoring
framework formulated via Bayesian neural networks (BNNs) and we provide
relevant implementation details needed for the construction, training, and
deployment of BNN data-based virtual monitoring models. As opposed to their
deterministic counterparts, BNNs intrinsically announce the uncertainties
associated with generated load predictions and allow to detect inaccurate load
estimations generated for non-fully monitored wind turbines. The proposed
virtual load monitoring is thoroughly tested through an experimental campaign
in an operational offshore wind farm and the results demonstrate the
effectiveness of BNN models for fleet-leader-based farm-wide virtual
monitoring.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hlaing_N/0/1/0/all/0/1&quot;&gt;N. Hlaing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morato_P/0/1/0/all/0/1&quot;&gt;Pablo G. Morato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_F/0/1/0/all/0/1&quot;&gt;F. d. N. Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weijtjens_W/0/1/0/all/0/1&quot;&gt;W. Weijtjens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Devriendt_C/0/1/0/all/0/1&quot;&gt;C. Devriendt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rigo_P/0/1/0/all/0/1&quot;&gt;P. Rigo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01735">
<title>Neural Fourier Filter Bank. (arXiv:2212.01735v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01735</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel method to provide efficient and highly detailed
reconstructions. Inspired by wavelets, we learn a neural field that decompose
the signal both spatially and frequency-wise. We follow the recent grid-based
paradigm for spatial decomposition, but unlike existing work, encourage
specific frequencies to be stored in each grid via Fourier features encodings.
We then apply a multi-layer perceptron with sine activations, taking these
Fourier encoded features in at appropriate layers so that higher-frequency
components are accumulated on top of lower-frequency components sequentially,
which we sum up to form the final output. We demonstrate that our method
outperforms the state of the art regarding model compactness and convergence
speed on multiple tasks: 2D image fitting, 3D shape reconstruction, and neural
radiance fields. Our code is available at https://github.com/ubc-vision/NFFB.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhijie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yuhe Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1&quot;&gt;Kwang Moo Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.04997">
<title>Regulating Gatekeeper AI and Data: Transparency, Access, and Fairness under the DMA, the GDPR, and beyond. (arXiv:2212.04997v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2212.04997</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence is not only increasingly used in business and
administration contexts, but a race for its regulation is also underway, with
the EU spearheading the efforts. Contrary to existing literature, this article
suggests, however, that the most far-reaching and effective EU rules for AI
applications in the digital economy will not be contained in the proposed AI
Act - but have just been enacted in the Digital Markets Act. We analyze the
impact of the DMA and related EU acts on AI models and their underlying data
across four key areas: disclosure requirements; the regulation of AI training
data; access rules; and the regime for fair rankings. The paper demonstrates
that fairness, in the sense of the DMA, goes beyond traditionally protected
categories of non-discrimination law on which scholarship at the intersection
of AI and law has so far largely focused on. Rather, we draw on competition law
and the FRAND criteria known from intellectual property law to interpret and
refine the DMA provisions on fair rankings. Moreover, we show how, based on
CJEU jurisprudence, a coherent interpretation of the concept of
non-discrimination in both traditional non-discrimination and competition law
may be found. The final part sketches specific proposals for a comprehensive
framework of transparency, access, and fairness under the DMA and beyond.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hacker_P/0/1/0/all/0/1&quot;&gt;Philipp Hacker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cordes_J/0/1/0/all/0/1&quot;&gt;Johann Cordes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rochon_J/0/1/0/all/0/1&quot;&gt;Janina Rochon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.05976">
<title>DexBERT: Effective, Task-Agnostic and Fine-grained Representation Learning of Android Bytecode. (arXiv:2212.05976v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2212.05976</link>
<description rdf:parseType="Literal">&lt;p&gt;The automation of a large number of software engineering tasks is becoming
possible thanks to Machine Learning (ML). Central to applying ML to software
artifacts (like source or executable code) is converting them into forms
suitable for learning. Traditionally, researchers have relied on manually
selected features, based on expert knowledge which is sometimes imprecise and
generally incomplete. Representation learning has allowed ML to automatically
choose suitable representations and relevant features. Yet, for Android-related
tasks, existing models like apk2vec focus on whole-app levels, or target
specific tasks like smali2vec, which limits their applicability. Our work is
part of a new line of research that investigates effective, task-agnostic, and
fine-grained universal representations of bytecode to mitigate both of these
two limitations. Such representations aim to capture information relevant to
various low-level downstream tasks (e.g., at the class-level). We are inspired
by the field of Natural Language Processing, where the problem of universal
representation was addressed by building Universal Language Models, such as
BERT, whose goal is to capture abstract semantic information about sentences,
in a way that is reusable for a variety of tasks. We propose DexBERT, a
BERT-like Language Model dedicated to representing chunks of DEX bytecode, the
main binary format used in Android applications. We empirically assess whether
DexBERT is able to model the DEX language and evaluate the suitability of our
model in three distinct class-level software engineering tasks: Malicious Code
Localization, Defect Prediction, and Component Type Classification. We also
experiment with strategies to deal with the problem of catering to apps having
vastly different sizes, and we demonstrate one example of using our technique
to investigate what information is relevant to a given task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tiezhu Sun&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allix_K/0/1/0/all/0/1&quot;&gt;Kevin Allix&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kisub Kim&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xin Zhou&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dongsun Kim&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_D/0/1/0/all/0/1&quot;&gt;David Lo&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bissyande_T/0/1/0/all/0/1&quot;&gt;Tegawend&amp;#xe9; F. Bissyand&amp;#xe9;&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_J/0/1/0/all/0/1&quot;&gt;Jacques Klein&lt;/a&gt; (1) ((1) University of Luxembourg, (2) Singapore Management University, (3) Kyungpook National University)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.06496">
<title>Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning. (arXiv:2301.06496v3 [physics.optics] UPDATED)</title>
<link>http://arxiv.org/abs/2301.06496</link>
<description rdf:parseType="Literal">&lt;p&gt;Retrieving images transmitted through multi-mode fibers is of growing
interest, thanks to their ability to confine and transport light efficiently in
a compact system. Here, we demonstrate machine-learning-based decoding of
large-scale digital images (pages), maximizing page capacity for optical
storage applications. Using a millimeter-sized square cross-section waveguide,
we image an 8-bit spatial light modulator, presenting data as a matrix of
symbols. Normally, decoders will incur a prohibitive O(n^2) computational
scaling to decode n symbols in spatially scrambled data. However, by combining
a digital twin of the setup with a U-Net, we can retrieve up to 66 kB using
efficient convolutional operations only. We compare trainable ray-tracing-based
with eigenmode-based twins and show the former to be superior thanks to its
ability to overcome the simulation-to-experiment gap by adjusting to optical
imperfections. We train the pipeline end-to-end using a differentiable
mutual-information estimator based on the von-Mises distribution, generally
applicable to phase-coding channels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lim_J/0/1/0/all/0/1&quot;&gt;Joowon Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gladrow_J/0/1/0/all/0/1&quot;&gt;Jannes Gladrow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kelly_D/0/1/0/all/0/1&quot;&gt;Douglas Kelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+OShea_G/0/1/0/all/0/1&quot;&gt;Greg O&amp;#x27;Shea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Verkes_G/0/1/0/all/0/1&quot;&gt;Govert Verkes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Stefanovici_I/0/1/0/all/0/1&quot;&gt;Ioan Stefanovici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Nowozin_S/0/1/0/all/0/1&quot;&gt;Sebastian Nowozin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Thomsen_B/0/1/0/all/0/1&quot;&gt;Benn Thomsen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.09091">
<title>BallGAN: 3D-aware Image Synthesis with a Spherical Background. (arXiv:2301.09091v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.09091</link>
<description rdf:parseType="Literal">&lt;p&gt;3D-aware GANs aim to synthesize realistic 3D scenes such that they can be
rendered in arbitrary perspectives to produce images. Although previous methods
produce realistic images, they suffer from unstable training or degenerate
solutions where the 3D geometry is unnatural. We hypothesize that the 3D
geometry is underdetermined due to the insufficient constraint, i.e., being
classified as real image to the discriminator is not enough. To solve this
problem, we propose to approximate the background as a spherical surface and
represent a scene as a union of the foreground placed in the sphere and the
thin spherical background. It reduces the degree of freedom in the background
field. Accordingly, we modify the volume rendering equation and incorporate
dedicated constraints to design a novel 3D-aware GAN framework named BallGAN.
BallGAN has multiple advantages as follows. 1) It produces more reasonable 3D
geometry; the images of a scene across different viewpoints have better
photometric consistency and fidelity than the state-of-the-art methods. 2) The
training becomes much more stable. 3) The foreground can be separately rendered
on top of different arbitrary backgrounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1&quot;&gt;Minjung Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1&quot;&gt;Yunji Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1&quot;&gt;Jeongmin Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Young Sun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyunsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1&quot;&gt;Hyeran Byun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1&quot;&gt;Youngjung Uh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.00347">
<title>Anderson Acceleration For Bioinformatics-Based Machine Learning. (arXiv:2302.00347v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.00347</link>
<description rdf:parseType="Literal">&lt;p&gt;Anderson acceleration (AA) is a well-known method for accelerating the
convergence of iterative algorithms, with applications in various fields
including deep learning and optimization. Despite its popularity in these
areas, the effectiveness of AA in classical machine learning classifiers has
not been thoroughly studied. Tabular data, in particular, presents a unique
challenge for deep learning models, and classical machine learning models are
known to perform better in these scenarios. However, the convergence analysis
of these models has received limited attention. To address this gap in
research, we implement a support vector machine (SVM) classifier variant that
incorporates AA to speed up convergence. We evaluate the performance of our SVM
with and without Anderson acceleration on several datasets from the biology
domain and demonstrate that the use of AA significantly improves convergence
and reduces the training loss as the number of iterations increases. Our
findings provide a promising perspective on the potential of Anderson
acceleration in the training of simple machine learning classifiers and
underscore the importance of further research in this area. By showing the
effectiveness of AA in this setting, we aim to inspire more studies that
explore the applications of AA in classical machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1&quot;&gt;Sarwan Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chourasia_P/0/1/0/all/0/1&quot;&gt;Prakash Chourasia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patterson_M/0/1/0/all/0/1&quot;&gt;Murray Patterson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11086">
<title>Pluralistic Aging Diffusion Autoencoder. (arXiv:2303.11086v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11086</link>
<description rdf:parseType="Literal">&lt;p&gt;Face aging is an ill-posed problem because multiple plausible aging patterns
may correspond to a given input. Most existing methods often produce one
deterministic estimation. This paper proposes a novel CLIP-driven Pluralistic
Aging Diffusion Autoencoder (PADA) to enhance the diversity of aging patterns.
First, we employ diffusion models to generate diverse low-level aging details
via a sequential denoising reverse process. Second, we present Probabilistic
Aging Embedding (PAE) to capture diverse high-level aging patterns, which
represents age information as probabilistic distributions in the common CLIP
latent space. A text-guided KL-divergence loss is designed to guide this
learning. Our method can achieve pluralistic face aging conditioned on
open-world aging texts and arbitrary unseen face images. Qualitative and
quantitative experiments demonstrate that our method can generate more diverse
and high-quality plausible aging results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peipei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Huaibo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ran He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhaofeng He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.02169">
<title>Synthesize High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model. (arXiv:2304.02169v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.02169</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthetic electronic health records (EHRs) that are both realistic and
preserve privacy can serve as an alternative to real EHRs for machine learning
(ML) modeling and statistical analysis. However, generating high-fidelity and
granular electronic health record (EHR) data in its original,
highly-dimensional form poses challenges for existing methods due to the
complexities inherent in high-dimensional data. In this paper, we propose
Hierarchical Autoregressive Language mOdel (HALO) for generating longitudinal
high-dimensional EHR, which preserve the statistical properties of real EHR and
can be used to train accurate ML models without privacy concerns. Our HALO
method, designed as a hierarchical autoregressive model, generates a
probability density function of medical codes, clinical visits, and patient
records, allowing for the generation of realistic EHR data in its original,
unaggregated form without the need for variable selection or aggregation.
Additionally, our model also produces high-quality continuous variables in a
longitudinal and probabilistic manner. We conducted extensive experiments and
demonstrate that HALO can generate high-fidelity EHR data with high-dimensional
disease code probabilities (d &amp;gt; 10,000), disease co-occurrence probabilities
within visits (d &amp;gt; 1,000,000), and conditional probabilities across consecutive
visits (d &amp;gt; 5,000,000) and achieve above 0.9 R2 correlation in comparison to
real EHR data. This performance then enables downstream ML models trained on
its synthetic data to achieve comparable accuracy to models trained on real
data (0.938 AUROC with HALO data vs. 0.943 with real data). Finally, using a
combination of real and synthetic data enhances the accuracy of ML models
beyond that achieved by using only real EHR data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theodorou_B/0/1/0/all/0/1&quot;&gt;Brandon Theodorou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Cao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jimeng Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03543">
<title>HyperTab: Hypernetwork Approach for Deep Learning on Small Tabular Datasets. (arXiv:2304.03543v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03543</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has achieved impressive performance in many domains, such as
computer vision and natural language processing, but its advantage over
classical shallow methods on tabular datasets remains questionable. It is
especially challenging to surpass the performance of tree-like ensembles, such
as XGBoost or Random Forests, on small-sized datasets (less than 1k samples).
To tackle this challenge, we introduce HyperTab, a hypernetwork-based approach
to solving small sample problems on tabular datasets. By combining the
advantages of Random Forests and neural networks, HyperTab generates an
ensemble of neural networks, where each target model is specialized to process
a specific lower-dimensional view of the data. Since each view plays the role
of data augmentation, we virtually increase the number of training samples
while keeping the number of trainable parameters unchanged, which prevents
model overfitting. We evaluated HyperTab on more than 40 tabular datasets of a
varying number of samples and domains of origin, and compared its performance
with shallow and deep learning models representing the current
state-of-the-art. We show that HyperTab consistently outranks other methods on
small data (with a statistically significant difference) and scores comparable
to them on larger datasets.
&lt;/p&gt;
&lt;p&gt;We make a python package with the code available to download at
https://pypi.org/project/hypertab/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wydmanski_W/0/1/0/all/0/1&quot;&gt;Witold Wydma&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bulenok_O/0/1/0/all/0/1&quot;&gt;Oleksii Bulenok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smieja_M/0/1/0/all/0/1&quot;&gt;Marek &amp;#x15a;mieja&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12241">
<title>Positive AI: Key Challenges for Designing Wellbeing-aligned Artificial Intelligence. (arXiv:2304.12241v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12241</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence (AI) is rapidly transforming society, creating an
urgent need to ensure its positive impact. In this article, we take a positive
design approach towards this issue, viewing it as a matter of designing AI
systems that actively support human wellbeing. However, designing
wellbeing-aligned AI systems is difficult. This article adopts a cybernetic
perspective to identify twelve key challenges across two categories: lack of
knowledge and lack of motivation. Knowledge barriers include challenges in
conceptualizing, measuring, and optimizing for wellbeing, then designing
appropriate AI actions. Motivation barriers include misaligned incentives,
financial and publicity risks, and a lack of data access preventing
(third-party) research on wellbeing. To address these challenges we have
captured our key takeaways in a research agenda related to 1) advancing the
scientific understanding of the impact of AI systems on wellbeing, and 2)
guiding design actions on how AI systems might be intentionally designed to
promote and sustain wellbeing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maden_W/0/1/0/all/0/1&quot;&gt;Willem van der Maden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lomas_D/0/1/0/all/0/1&quot;&gt;Derek Lomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadek_M/0/1/0/all/0/1&quot;&gt;Malak Sadek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hekkert_P/0/1/0/all/0/1&quot;&gt;Paul Hekkert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06152">
<title>Structure-CLIP: Towards Scene Graph Knowledge to Enhance Multi-modal Structured Representations. (arXiv:2305.06152v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06152</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale vision-language pre-training has achieved significant performance
in multi-modal understanding and generation tasks. However, existing methods
often perform poorly on image-text matching tasks that require structured
representations, i.e., representations of objects, attributes, and relations.
Previous models cannot make a distinction between ``An astronaut rides a horse&quot;
and ``A horse rides an astronaut&quot;. This is because they fail to fully leverage
structured knowledge when learning representations in multi-modal scenarios. In
this paper, we present an end-to-end framework Structure-CLIP, which integrates
Scene Graph Knowledge (SGK) to enhance multi-modal structured representations.
Firstly, we use scene graphs to guide the construction of semantic negative
examples, which results in an increased emphasis on learning structured
representations. Moreover, a Knowledge-Enhance Encoder (KEE) is proposed to
leverage SGK as input to further enhance structured representations. To verify
the effectiveness of the proposed framework, we pre-train our model with the
aforementioned approaches and conduct experiments on downstream tasks.
Experimental results demonstrate that Structure-CLIP achieves state-of-the-art
(SOTA) performance on VG-Attribution and VG-Relation datasets, with 12.5% and
4.1% ahead of the multi-modal SOTA model respectively. Meanwhile, the results
on MSCOCO indicate that Structure-CLIP significantly enhances the structured
representations while maintaining the ability of general representations. Our
code will be available soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yufeng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiji Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rongsheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weijie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhou Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1&quot;&gt;Tangjie Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wen Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14706">
<title>PruMUX: Augmenting Data Multiplexing with Model Compression. (arXiv:2305.14706v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14706</link>
<description rdf:parseType="Literal">&lt;p&gt;As language models increase in size by the day, methods for efficient
inference are critical to leveraging their capabilities for various
applications. Prior work has investigated techniques like model pruning,
knowledge distillation, and data multiplexing to increase model throughput
without sacrificing accuracy. In this paper, we combine two such methods --
structured pruning and data multiplexing -- to compound the speedup gains
obtained by either method. Our approach, PruMUX, obtains up to 7.5-29.5X
throughput improvement over BERT-base model with accuracy threshold from 80% to
74%. We further study various combinations of parameters (such as sparsity and
multiplexing factor) in the two techniques to provide a comprehensive analysis
of the tradeoff between accuracy and throughput in the resulting models. We
then propose Auto-PruMUX, a meta-level model that can predict the
high-performance parameters for pruning and multiplexing given a desired
accuracy loss budget, providing a practical method to leverage the combination
effectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yushan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murahari_V/0/1/0/all/0/1&quot;&gt;Vishvak Murahari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1&quot;&gt;Karthik Narasimhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kai Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16556">
<title>LANISTR: Multimodal Learning from Structured and Unstructured Data. (arXiv:2305.16556v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16556</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal large-scale pretraining has shown impressive performance for
unstructured data including language, image, audio, and video. However, a
prevalent real-world scenario involves the combination of structured data types
(tabular, time-series) with unstructured data which has so far been
understudied. To bridge this gap, we propose LANISTR, an attention-based
framework to learn from LANguage, Image, and STRuctured data. The core of
LANISTR&apos;s methodology is rooted in \textit{masking-based} training applied
across both unimodal and multimodal levels. In particular, we introduce a new
similarity-based multimodal masking loss that enables it to learn cross-modal
relations from large-scale multimodal data with missing modalities. On two
real-world datastes, MIMIC-IV (healthcare) and Amazon Product Review (retail),
LANISTR demonstrates remarkable absolute improvements of 6.6\% (AUROC) and up
to 14\% (accuracy) when fine-tuned on 0.1\% and 0.01\% of labeled data,
respectively, compared to the state-of-the-art alternatives. Notably, these
improvements are observed even in the presence of considerable missingness
ratios of 35.7\% and 99.8\%, in the respective datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1&quot;&gt;Sayna Ebrahimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1&quot;&gt;Sercan O. Arik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yihe Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1&quot;&gt;Tomas Pfister&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06494">
<title>Multi-modal Pre-training for Medical Vision-language Understanding and Generation: An Empirical Study with A New Benchmark. (arXiv:2306.06494v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06494</link>
<description rdf:parseType="Literal">&lt;p&gt;With the availability of large-scale, comprehensive, and general-purpose
vision-language (VL) datasets such as MSCOCO, vision-language pre-training
(VLP) has become an active area of research and proven to be effective for
various VL tasks such as visual-question answering. However, studies on VLP in
the medical domain have so far been scanty. To provide a comprehensive
perspective on VLP for medical VL tasks, we conduct a thorough experimental
analysis to study key factors that may affect the performance of VLP with a
unified vision-language Transformer. To allow making sound and quick
pre-training decisions, we propose RadioGraphy Captions (RGC), a high-quality,
multi-modality radiographic dataset containing 18,434 image-caption pairs
collected from an open-access online database MedPix. RGC can be used as a
pre-training dataset or a new benchmark for medical report generation and
medical image-text retrieval. By utilizing RGC and other available datasets for
pre-training, we develop several key insights that can guide future medical VLP
research and new strong baselines for various medical VL tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Li Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Ameer Hamza Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Lu Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiao-Ming Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00329">
<title>DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment. (arXiv:2307.00329v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00329</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models encode a vast amount of semantic knowledge and possess
remarkable understanding and reasoning capabilities. Previous research has
explored how to ground language models in robotic tasks to ensure that the
sequences generated by the language model are both logically correct and
practically executable. However, low-level execution may deviate from the
high-level plan due to environmental perturbations or imperfect controller
design. In this paper, we propose DoReMi, a novel language model grounding
framework that enables immediate Detection and Recovery from Misalignments
between plan and execution. Specifically, LLMs are leveraged for both planning
and generating constraints for planned steps. These constraints can indicate
plan-execution misalignments and we use a vision question answering (VQA) model
to check constraints during low-level skill execution. If certain misalignment
occurs, our method will call the language model to re-plan in order to recover
from misalignments. Experiments on various complex tasks including robot arms
and humanoid robots demonstrate that our method can lead to higher task success
rates and shorter task completion times. Videos of DoReMi are available at
https://sites.google.com/view/doremi-paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yanjiang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yen-Jen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_L/0/1/0/all/0/1&quot;&gt;Lihan Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zheyuan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianyu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06947">
<title>Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition. (arXiv:2307.06947v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06947</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent video recognition models utilize Transformer models for long-range
spatio-temporal context modeling. Video transformer designs are based on
self-attention that can model global context at a high computational cost. In
comparison, convolutional designs for videos offer an efficient alternative but
lack long-range dependency modeling. Towards achieving the best of both
designs, this work proposes Video-FocalNet, an effective and efficient
architecture for video recognition that models both local and global contexts.
Video-FocalNet is based on a spatio-temporal focal modulation architecture that
reverses the interaction and aggregation steps of self-attention for better
efficiency. Further, the aggregation step and the interaction step are both
implemented using efficient convolution and element-wise multiplication
operations that are computationally less expensive than their self-attention
counterparts on video representations. We extensively explore the design space
of focal modulation-based spatio-temporal context modeling and demonstrate our
parallel spatial and temporal encoding design to be the optimal choice.
Video-FocalNets perform favorably well against the state-of-the-art
transformer-based models for video recognition on five large-scale datasets
(Kinetics-400, Kinetics-600, SS-v2, Diving-48, and ActivityNet-1.3) at a lower
computational cost. Our code/models are released at
https://github.com/TalalWasim/Video-FocalNets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasim_S/0/1/0/all/0/1&quot;&gt;Syed Talal Wasim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khattak_M/0/1/0/all/0/1&quot;&gt;Muhammad Uzair Khattak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1&quot;&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mubarak Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11413">
<title>A Video-based Detector for Suspicious Activity in Examination with OpenPose. (arXiv:2307.11413v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.11413</link>
<description rdf:parseType="Literal">&lt;p&gt;Examinations are a crucial part of the learning process, and academic
institutions invest significant resources into maintaining their integrity by
preventing cheating from students or facilitators. However, cheating has become
rampant in examination setups, compromising their integrity. The traditional
method of relying on invigilators to monitor every student is impractical and
ineffective. To address this issue, there is a need to continuously record exam
sessions to monitor students for suspicious activities. However, these
recordings are often too lengthy for invigilators to analyze effectively, and
fatigue may cause them to miss significant details. To widen the coverage,
invigilators could use fixed overhead or wearable cameras. This paper
introduces a framework that uses automation to analyze videos and detect
suspicious activities during examinations efficiently and effectively. We
utilized the OpenPose framework and Convolutional Neural Network (CNN) to
identify students exchanging objects during exams. This detection system is
vital in preventing cheating and promoting academic integrity, fairness, and
quality education for institutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moyo_R/0/1/0/all/0/1&quot;&gt;Reuben Moyo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ndebvu_S/0/1/0/all/0/1&quot;&gt;Stanley Ndebvu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimba_M/0/1/0/all/0/1&quot;&gt;Michael Zimba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mbelwa_J/0/1/0/all/0/1&quot;&gt;Jimmy Mbelwa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12907">
<title>GridMM: Grid Memory Map for Vision-and-Language Navigation. (arXiv:2307.12907v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12907</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-and-language navigation (VLN) enables the agent to navigate to a
remote location following the natural language instruction in 3D environments.
To represent the previously visited environment, most approaches for VLN
implement memory using recurrent states, topological maps, or top-down semantic
maps. In contrast to these approaches, we build the top-down egocentric and
dynamically growing Grid Memory Map (i.e., GridMM) to structure the visited
environment. From a global perspective, historical observations are projected
into a unified grid map in a top-down view, which can better represent the
spatial relations of the environment. From a local perspective, we further
propose an instruction relevance aggregation method to capture fine-grained
visual clues in each grid region. Extensive experiments are conducted on both
the REVERIE, R2R, SOON datasets in the discrete environments, and the R2R-CE
dataset in the continuous environments, showing the superiority of our proposed
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiahao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yeqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Shuqiang Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04586">
<title>Developmental Bootstrapping: From Simple Competences to Intelligent Human-Compatible AIs. (arXiv:2308.04586v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04586</link>
<description rdf:parseType="Literal">&lt;p&gt;Although some AIs surpass human abilities in closed artificial worlds such as
board games, in the real world they make strange mistakes and do not notice
them. They cannot be instructed easily, fail to use common sense, and lack
curiosity. Mainstream approaches for creating AIs include the traditional
manually-constructed symbolic AI approach and the generative and deep learning
AI approaches including large language models (LLMs). Although it is outside of
the mainstream, the developmental bootstrapping approach may have more
potential. In developmental bootstrapping, AIs develop competences like human
children do. They start with innate competences. They interact with the
environment and learn from their interactions. They incrementally extend their
innate competences with self-developed competences. They interact and learn
from people and establish perceptual, cognitive, and common grounding. They
acquire the competences they need through competence bootstrapping. However,
developmental robotics has not yet produced AIs with robust adult-level
competences. Projects have typically stopped before reaching the Toddler
Barrier. This corresponds to human infant development at about two years of
age, before infant speech becomes fluent. They also do not bridge the Reading
Barrier, where they could skillfully and skeptically draw on the socially
developed online information resources that power LLMs. The next competences in
human cognitive development involve intrinsic motivation, imitation learning,
imagination, coordination, and communication. This position paper lays out the
logic, prospects, gaps, and challenges for extending the practice of
developmental bootstrapping to create robust, trustworthy, and human-compatible
AIs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stefik_M/0/1/0/all/0/1&quot;&gt;Mark Stefik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Price_R/0/1/0/all/0/1&quot;&gt;Robert Price&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05983">
<title>Face Encryption via Frequency-Restricted Identity-Agnostic Attacks. (arXiv:2308.05983v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.05983</link>
<description rdf:parseType="Literal">&lt;p&gt;Billions of people are sharing their daily live images on social media
everyday. However, malicious collectors use deep face recognition systems to
easily steal their biometric information (e.g., faces) from these images. Some
studies are being conducted to generate encrypted face photos using adversarial
attacks by introducing imperceptible perturbations to reduce face information
leakage. However, existing studies need stronger black-box scenario feasibility
and more natural visual appearances, which challenge the feasibility of privacy
protection. To address these problems, we propose a frequency-restricted
identity-agnostic (FRIA) framework to encrypt face images from unauthorized
face recognition without access to personal information. As for the weak
black-box scenario feasibility, we obverse that representations of the average
feature in multiple face recognition models are similar, thus we propose to
utilize the average feature via the crawled dataset from the Internet as the
target to guide the generation, which is also agnostic to identities of unknown
face recognition systems; in nature, the low-frequency perturbations are more
visually perceptible by the human vision system. Inspired by this, we restrict
the perturbation in the low-frequency facial regions by discrete cosine
transform to achieve the visual naturalness guarantee. Extensive experiments on
several face recognition models demonstrate that our FRIA outperforms other
state-of-the-art methods in generating more natural encrypted faces while
attaining high black-box attack success rates of 96%. In addition, we validate
the efficacy of FRIA using real-world black-box commercial API, which reveals
the potential of FRIA in practice. Our codes can be found in
https://github.com/XinDong10/FRIA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1&quot;&gt;Siyuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Aishan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1&quot;&gt;Lihua Jing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06534">
<title>Dealing with Small Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models. (arXiv:2308.06534v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06534</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning in medical imaging has the potential to minimize the risk of
diagnostic errors, reduce radiologist workload, and accelerate diagnosis.
Training such deep learning models requires large and accurate datasets, with
annotations for all training samples. However, in the medical imaging domain,
annotated datasets for specific tasks are often small due to the high
complexity of annotations, limited access, or the rarity of diseases. To
address this challenge, deep learning models can be pre-trained on large image
datasets without annotations using methods from the field of self-supervised
learning. After pre-training, small annotated datasets are sufficient to
fine-tune the models for a specific task. The most popular self-supervised
pre-training approaches in medical imaging are based on contrastive learning.
However, recent studies in natural image processing indicate a strong potential
for masked autoencoder approaches. Our work compares state-of-the-art
contrastive learning methods with the recently introduced masked autoencoder
approach &quot;SparK&quot; for convolutional neural networks (CNNs) on medical images.
Therefore we pre-train on a large unannotated CT image dataset and fine-tune on
several CT classification tasks. Due to the challenge of obtaining sufficient
annotated training data in medical imaging, it is of particular interest to
evaluate how the self-supervised pre-training methods perform when fine-tuning
on small datasets. By experimenting with gradually reducing the training
dataset size for fine-tuning, we find that the reduction has different effects
depending on the type of pre-training chosen. The SparK pre-training method is
more robust to the training dataset size than the contrastive methods. Based on
our results, we propose the SparK pre-training for medical imaging tasks with
only small annotated datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_D/0/1/0/all/0/1&quot;&gt;Daniel Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Payer_T/0/1/0/all/0/1&quot;&gt;Tristan Payer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lisson_C/0/1/0/all/0/1&quot;&gt;Catharina Silvia Lisson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lisson_C/0/1/0/all/0/1&quot;&gt;Christoph Gerhard Lisson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beer_M/0/1/0/all/0/1&quot;&gt;Meinrad Beer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1&quot;&gt;Timo Ropinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gotz_M/0/1/0/all/0/1&quot;&gt;Michael G&amp;#xf6;tz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07134">
<title>Natural Language is All a Graph Needs. (arXiv:2308.07134v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07134</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of large-scale pre-trained language models, such as ChatGPT,
has revolutionized various research fields in artificial intelligence.
Transformers-based large language models (LLMs) have gradually replaced CNNs
and RNNs to unify fields of computer vision and natural language processing.
Compared with the data that exists relatively independently such as images,
videos or texts, graph is a type of data that contains rich structural and
relational information. Meanwhile, natural language, as one of the most
expressive mediums, excels in describing complex structures. However, existing
work on incorporating graph learning problems into the generative language
modeling framework remains very limited. As the importance of large language
models continues to grow, it becomes essential to explore whether LLMs can also
replace GNNs as the foundation model for graphs. In this paper, we propose
InstructGLM (Instruction-finetuned Graph Language Model), systematically design
highly scalable prompts based on natural language instructions, and use natural
language to describe the geometric structure and node features of the graph for
instruction tuning an LLM to perform learning and inference on graphs in a
generative manner. Our method exceeds all competitive GNN baselines on
ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of
our method and sheds light on generative large language models as the
foundation model for graph machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1&quot;&gt;Ruosong Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Caiqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Runhui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08043">
<title>DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue. (arXiv:2308.08043v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08043</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs), such as ChatGPT, are becoming increasingly
sophisticated, demonstrating capabilities that closely resemble those of
humans. These AI models are playing an essential role in assisting humans with
a wide array of tasks in daily life. A significant application of AI is its use
as a chat agent, responding to human inquiries across various domains. Current
LLMs have shown proficiency in answering general questions. However, basic
question-answering dialogue often falls short in complex diagnostic scenarios,
such as legal or medical consultations. These scenarios typically necessitate
Task-Oriented Dialogue (TOD), wherein an AI chat agent needs to proactively
pose questions and guide users towards specific task completion. Previous
fine-tuning models have underperformed in TOD, and current LLMs do not
inherently possess this capability. In this paper, we introduce DiagGPT
(Dialogue in Diagnosis GPT), an innovative method that extends LLMs to TOD
scenarios. Our experiments reveal that DiagGPT exhibits outstanding performance
in conducting TOD with users, demonstrating its potential for practical
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1&quot;&gt;Lang Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09725">
<title>MoCLIM: Towards Accurate Cancer Subtyping via Multi-Omics Contrastive Learning with Omics-Inference Modeling. (arXiv:2308.09725v2 [q-bio.GN] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09725</link>
<description rdf:parseType="Literal">&lt;p&gt;Precision medicine fundamentally aims to establish causality between
dysregulated biochemical mechanisms and cancer subtypes. Omics-based cancer
subtyping has emerged as a revolutionary approach, as different level of omics
records the biochemical products of multistep processes in cancers. This paper
focuses on fully exploiting the potential of multi-omics data to improve cancer
subtyping outcomes, and hence developed MoCLIM, a representation learning
framework. MoCLIM independently extracts the informative features from distinct
omics modalities. Using a unified representation informed by contrastive
learning of different omics modalities, we can well-cluster the subtypes, given
cancer, into a lower latent space. This contrast can be interpreted as a
projection of inter-omics inference observed in biological networks.
Experimental results on six cancer datasets demonstrate that our approach
significantly improves data fit and subtyping performance in fewer
high-dimensional cancer instances. Moreover, our framework incorporates various
medical evaluations as the final component, providing high interpretability in
medical analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ziwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Matsubara_Y/0/1/0/all/0/1&quot;&gt;Yasuko Matsubara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sakurai_Y/0/1/0/all/0/1&quot;&gt;Yasushi Sakurai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10559">
<title>Metaverse: A Vision, Architectural Elements, and Future Directions for Scalable and Realtime Virtual Worlds. (arXiv:2308.10559v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10559</link>
<description rdf:parseType="Literal">&lt;p&gt;With the emergence of Cloud computing, Internet of Things-enabled
Human-Computer Interfaces, Generative Artificial Intelligence, and
high-accurate Machine and Deep-learning recognition and predictive models,
along with the Post Covid-19 proliferation of social networking, and remote
communications, the Metaverse gained a lot of popularity. Metaverse has the
prospective to extend the physical world using virtual and augmented reality so
the users can interact seamlessly with the real and virtual worlds using
avatars and holograms. It has the potential to impact people in the way they
interact on social media, collaborate in their work, perform marketing and
business, teach, learn, and even access personalized healthcare. Several works
in the literature examine Metaverse in terms of hardware wearable devices, and
virtual reality gaming applications. However, the requirements of realizing the
Metaverse in realtime and at a large-scale need yet to be examined for the
technology to be usable. To address this limitation, this paper presents the
temporal evolution of Metaverse definitions and captures its evolving
requirements. Consequently, we provide insights into Metaverse requirements. In
addition to enabling technologies, we lay out architectural elements for
scalable, reliable, and efficient Metaverse systems, and a classification of
existing Metaverse applications along with proposing required future research
directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ismail_L/0/1/0/all/0/1&quot;&gt;Leila Ismail&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buyya_R/0/1/0/all/0/1&quot;&gt;Rajkumar Buyya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10677">
<title>Visual Crowd Analysis: Open Research Problems. (arXiv:2308.10677v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10677</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the last decade, there has been a remarkable surge in interest in
automated crowd monitoring within the computer vision community. Modern
deep-learning approaches have made it possible to develop fully-automated
vision-based crowd-monitoring applications. However, despite the magnitude of
the issue at hand, the significant technological advancements, and the
consistent interest of the research community, there are still numerous
challenges that need to be overcome. In this article, we delve into six major
areas of visual crowd analysis, emphasizing the key developments in each of
these areas. We outline the crucial unresolved issues that must be tackled in
future works, in order to ensure that the field of automated crowd monitoring
continues to progress and thrive. Several surveys related to this topic have
been conducted in the past. Nonetheless, this article thoroughly examines and
presents a more intuitive categorization of works, while also depicting the
latest breakthroughs within the field, incorporating more recent studies
carried out within the last few years in a concise manner. By carefully
choosing prominent works with significant contributions in terms of novelty or
performance gains, this paper presents a more comprehensive exposition of
advancements in the current state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Muhammad Asif Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menouar_H/0/1/0/all/0/1&quot;&gt;Hamid Menouar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamila_R/0/1/0/all/0/1&quot;&gt;Ridha Hamila&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11217">
<title>Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models. (arXiv:2308.11217v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11217</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal data, which can comprehensively perceive and recognize the
physical world, has become an essential path towards general artificial
intelligence. However, multimodal large models trained on public datasets often
underperform in specific industrial domains. This paper proposes a multimodal
federated learning framework that enables multiple enterprises to utilize
private domain data to collaboratively train large models for vertical domains,
achieving intelligent services across scenarios. The authors discuss in-depth
the strategic transformation of federated learning in terms of intelligence
foundation and objectives in the era of big model, as well as the new
challenges faced in heterogeneous data, model aggregation, performance and cost
trade-off, data privacy, and incentive mechanism. The paper elaborates a case
study of leading enterprises contributing multimodal data and expert knowledge
to city safety operation management , including distributed deployment and
efficient coordination of the federated learning platform, technical
innovations on data quality improvement based on large model capabilities and
efficient joint fine-tuning approaches. Preliminary experiments show that
enterprises can enhance and accumulate intelligent capabilities through
multimodal model federated learning, thereby jointly creating an smart city
model that provides high-quality intelligent services covering energy
infrastructure safety, residential community security, and urban operation
management. The established federated learning cooperation ecosystem is
expected to further aggregate industry, academia, and research resources,
realize large models in multiple vertical domains, and promote the large-scale
industrial application of artificial intelligence and cutting-edge research on
multimodal federated learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zengxiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1&quot;&gt;Zhaoxiang Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Ying Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tongzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Longfei Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1&quot;&gt;Chao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chengyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weishan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zelei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Liang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11471">
<title>Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI). (arXiv:2308.11471v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11471</link>
<description rdf:parseType="Literal">&lt;p&gt;This work targets what we consider to be the foundational step for urban
airborne robots, a safe landing. Our attention is directed toward what we deem
the most crucial aspect of the safe landing perception stack: segmentation. We
present a streamlined reactive UAV system that employs visual servoing by
harnessing the capabilities of open vocabulary image segmentation. This
approach can adapt to various scenarios with minimal adjustments, bypassing the
necessity for extensive data accumulation for refining internal models, thanks
to its open vocabulary methodology. Given the limitations imposed by local
authorities, our primary focus centers on operations originating from altitudes
of 100 meters. This choice is deliberate, as numerous preceding works have
dealt with altitudes up to 30 meters, aligning with the capabilities of small
stereo cameras. Consequently, we leave the remaining 20m to be navigated using
conventional 3D path planning methods. Utilizing monocular cameras and image
segmentation, our findings demonstrate the system&apos;s capability to successfully
execute landing maneuvers at altitudes as low as 20 meters. However, this
approach is vulnerable to intermittent and occasionally abrupt fluctuations in
the segmentation between frames in a video stream. To address this challenge,
we enhance the image segmentation output by introducing what we call a dynamic
focus: a masking mechanism that self adjusts according to the current landing
stage. This dynamic focus guides the control system to avoid regions beyond the
drone&apos;s safety radius projected onto the ground, thus mitigating the problems
with fluctuations. Through the implementation of this supplementary layer, our
experiments have reached improvements in the landing success rate of almost
tenfold when compared to global segmentation. All the source code is open
source and available online (github.com/MISTLab/DOVESEI).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bong_H/0/1/0/all/0/1&quot;&gt;Haechan Mark Bong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rongge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azambuja_R/0/1/0/all/0/1&quot;&gt;Ricardo de Azambuja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beltrame_G/0/1/0/all/0/1&quot;&gt;Giovanni Beltrame&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11764">
<title>Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models. (arXiv:2308.11764v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11764</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP). Although convenient for research and practical applications, open-source
LLMs with fewer parameters often suffer from severe hallucinations compared to
their larger counterparts. This paper focuses on measuring and reducing
hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs
that are publicly available for research and commercial applications. We
introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed
to quantify the severity of hallucinations in LLMs. Additionally, we explore
techniques like knowledge injection and teacher-student approaches to alleviate
hallucinations in low-parameter LLMs. Our experiments effectively demonstrate
the reduction of hallucinations in challenging domains for these LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elaraby_M/0/1/0/all/0/1&quot;&gt;Mohamed Elaraby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1&quot;&gt;Mengyin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dunn_J/0/1/0/all/0/1&quot;&gt;Jacob Dunn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xueying Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shizhu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11877">
<title>Integrated Image and Location Analysis for Wound Classification: A Deep Learning Approach. (arXiv:2308.11877v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11877</link>
<description rdf:parseType="Literal">&lt;p&gt;The global burden of acute and chronic wounds presents a compelling case for
enhancing wound classification methods, a vital step in diagnosing and
determining optimal treatments. Recognizing this need, we introduce an
innovative multi-modal network based on a deep convolutional neural network for
categorizing wounds into four categories: diabetic, pressure, surgical, and
venous ulcers. Our multi-modal network uses wound images and their
corresponding body locations for more precise classification. A unique aspect
of our methodology is incorporating a body map system that facilitates accurate
wound location tagging, improving upon traditional wound image classification
techniques. A distinctive feature of our approach is the integration of models
such as VGG16, ResNet152, and EfficientNet within a novel architecture. This
architecture includes elements like spatial and channel-wise
Squeeze-and-Excitation modules, Axial Attention, and an Adaptive Gated
Multi-Layer Perceptron, providing a robust foundation for classification. Our
multi-modal network was trained and evaluated on two distinct datasets
comprising relevant images and corresponding location information. Notably, our
proposed network outperformed traditional methods, reaching an accuracy range
of 74.79% to 100% for Region of Interest (ROI) without location
classifications, 73.98% to 100% for ROI with location classifications, and
78.10% to 100% for whole image classifications. This marks a significant
enhancement over previously reported performance metrics in the literature. Our
results indicate the potential of our multi-modal network as an effective
decision-support tool for wound image classification, paving the way for its
application in various clinical contexts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_Y/0/1/0/all/0/1&quot;&gt;Yash Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_T/0/1/0/all/0/1&quot;&gt;Tirth Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhar_M/0/1/0/all/0/1&quot;&gt;Mrinal Kanti Dhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Taiyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niezgoda_J/0/1/0/all/0/1&quot;&gt;Jeffrey Niezgoda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalakrishnan_S/0/1/0/all/0/1&quot;&gt;Sandeep Gopalakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zeyun Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12044">
<title>A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12044</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparsity is a highly desired feature in deep neural networks (DNNs) since it
ensures numerical efficiency, improves the interpretability of models (due to
the smaller number of relevant features), and robustness. In machine learning
approaches based on linear models, it is well known that there exists a
connecting path between the sparsest solution in terms of the $\ell^1$ norm
(i.e., zero weights) and the non-regularized solution, which is called the
regularization path. Very recently, there was a first attempt to extend the
concept of regularization paths to DNNs by means of treating the empirical loss
and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the
resulting multiobjective optimization problem. However, due to the
non-smoothness of the $\ell^1$ norm and the high number of parameters, this
approach is not very efficient from a computational perspective. To overcome
this limitation, we present an algorithm that allows for the approximation of
the entire Pareto front for the above-mentioned objectives in a very efficient
manner. We present numerical examples using both deterministic and stochastic
gradients. We furthermore demonstrate that knowledge of the regularization path
allows for a well-generalizing network parametrization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amakor_A/0/1/0/all/0/1&quot;&gt;Augustina C. Amakor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonntag_K/0/1/0/all/0/1&quot;&gt;Konstantin Sonntag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peitz_S/0/1/0/all/0/1&quot;&gt;Sebastian Peitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12213">
<title>CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No. (arXiv:2308.12213v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12213</link>
<description rdf:parseType="Literal">&lt;p&gt;Out-of-distribution (OOD) detection refers to training the model on an
in-distribution (ID) dataset to classify whether the input images come from
unknown classes. Considerable effort has been invested in designing various OOD
detection methods based on either convolutional neural networks or
transformers. However, zero-shot OOD detection methods driven by CLIP, which
only require class names for ID, have received less attention. This paper
presents a novel method, namely CLIP saying no (CLIPN), which empowers the
logic of saying no within CLIP. Our key motivation is to equip CLIP with the
capability of distinguishing OOD and ID samples using positive-semantic prompts
and negation-semantic prompts. Specifically, we design a novel learnable no
prompt and a no text encoder to capture negation semantics within images.
Subsequently, we introduce two loss functions: the image-text binary-opposite
loss and the text semantic-opposite loss, which we use to teach CLIPN to
associate images with no prompts, thereby enabling it to identify unknown
samples. Furthermore, we propose two threshold-free inference algorithms to
perform OOD detection by utilizing negation semantics from no prompts and the
text encoder. Experimental results on 9 benchmark datasets (3 ID datasets and 6
OOD datasets) for the OOD detection task demonstrate that CLIPN, based on
ViT-B-16, outperforms 7 well-used algorithms by at least 2.34% and 11.64% in
terms of AUROC and FPR95 for zero-shot OOD detection on ImageNet-1K. Our CLIPN
can serve as a solid foundation for effectively leveraging CLIP in downstream
OOD tasks. The code is available on https://github.com/xmed-lab/CLIPN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hualiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Huifeng Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07445">
<title>Open-set Face Recognition using Ensembles trained on Clustered Data. (arXiv:2308.07445v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2308.07445</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-set face recognition describes a scenario where unknown subjects, unseen
during the training stage, appear on test time. Not only it requires methods
that accurately identify individuals of interest, but also demands approaches
that effectively deal with unfamiliar faces. This work details a scalable
open-set face identification approach to galleries composed of hundreds and
thousands of subjects. It is composed of clustering and an ensemble of binary
learning algorithms that estimates when query face samples belong to the face
gallery and then retrieves their correct identity. The approach selects the
most suitable gallery subjects and uses the ensemble to improve prediction
performance. We carry out experiments on well-known LFW and YTF benchmarks.
Results show that competitive performance can be achieved even when targeting
scalability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vareto_R/0/1/0/all/0/1&quot;&gt;Rafael Henrique Vareto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwartz_W/0/1/0/all/0/1&quot;&gt;William Robson Schwartz&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>