<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-09T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03201" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03311" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03315" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03419" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03465" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03505" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03575" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03592" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03637" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03659" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03678" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03692" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2004.11145" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.09836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.05745" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.03466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.08815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.09495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.05953" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.06489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.11498" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11814" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05412" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07675" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13264" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14275" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02427" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03070" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.03193">
<title>Finding differences in perspectives between designers and engineers to develop trustworthy AI for autonomous cars. (arXiv:2307.03193v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.03193</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of designing and implementing ethical Artificial Intelligence
(AI), varying perspectives exist regarding developing trustworthy AI for
autonomous cars. This study sheds light on the differences in perspectives and
provides recommendations to minimize such divergences. By exploring the diverse
viewpoints, we identify key factors contributing to the differences and propose
strategies to bridge the gaps. This study goes beyond the trolley problem to
visualize the complex challenges of trustworthy and ethical AI. Three pillars
of trustworthy AI have been defined: transparency, reliability, and safety.
This research contributes to the field of trustworthy AI for autonomous cars,
providing practical recommendations to enhance the development of AI systems
that prioritize both technological advancement and ethical principles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jonelid_G/0/1/0/all/0/1&quot;&gt;Gustav Jonelid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larsson_K/0/1/0/all/0/1&quot;&gt;K. R. Larsson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03195">
<title>A Comprehensive Survey of Artificial Intelligence Techniques for Talent Analytics. (arXiv:2307.03195v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.03195</link>
<description rdf:parseType="Literal">&lt;p&gt;In today&apos;s competitive and fast-evolving business environment, it is a
critical time for organizations to rethink how to make talent-related decisions
in a quantitative manner. Indeed, the recent development of Big Data and
Artificial Intelligence (AI) techniques have revolutionized human resource
management. The availability of large-scale talent and management-related data
provides unparalleled opportunities for business leaders to comprehend
organizational behaviors and gain tangible knowledge from a data science
perspective, which in turn delivers intelligence for real-time decision-making
and effective talent management at work for their organizations. In the last
decade, talent analytics has emerged as a promising field in applied data
science for human resource management, garnering significant attention from AI
communities and inspiring numerous research efforts. To this end, we present an
up-to-date and comprehensive survey on AI technologies used for talent
analytics in the field of human resource management. Specifically, we first
provide the background knowledge of talent analytics and categorize various
pertinent data. Subsequently, we offer a comprehensive taxonomy of relevant
research efforts, categorized based on three distinct application-driven
scenarios: talent management, organization management, and labor market
analysis. In conclusion, we summarize the open challenges and potential
prospects for future research directions in the domain of AI-driven talent
analytics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1&quot;&gt;Chuan Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Le Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_R/0/1/0/all/0/1&quot;&gt;Rui Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dazhong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Ying Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hengshu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03197">
<title>Analyzing the vulnerabilities in SplitFed Learning: Assessing the robustness against Data Poisoning Attacks. (arXiv:2307.03197v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03197</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed Collaborative Machine Learning (DCML) is a potential alternative
to address the privacy concerns associated with centralized machine learning.
The Split learning (SL) and Federated Learning (FL) are the two effective
learning approaches in DCML. Recently there have been an increased interest on
the hybrid of FL and SL known as the SplitFed Learning (SFL). This research is
the earliest attempt to study, analyze and present the impact of data poisoning
attacks in SFL. We propose three kinds of novel attack strategies namely
untargeted, targeted and distance-based attacks for SFL. All the attacks
strategies aim to degrade the performance of the DCML-based classifier. We test
the proposed attack strategies for two different case studies on
Electrocardiogram signal classification and automatic handwritten digit
recognition. A series of attack experiments were conducted by varying the
percentage of malicious clients and the choice of the model split layer between
the clients and the server. The results after the comprehensive analysis of
attack strategies clearly convey that untargeted and distance-based poisoning
attacks have greater impacts in evading the classifier outcomes compared to
targeted attacks in SFL
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ismail_A/0/1/0/all/0/1&quot;&gt;Aysha Thahsin Zahir Ismail&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shukla_R/0/1/0/all/0/1&quot;&gt;Raj Mani Shukla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03198">
<title>A multilevel framework for AI governance. (arXiv:2307.03198v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.03198</link>
<description rdf:parseType="Literal">&lt;p&gt;To realize the potential benefits and mitigate potential risks of AI, it is
necessary to develop a framework of governance that conforms to ethics and
fundamental human values. Although several organizations have issued guidelines
and ethical frameworks for trustworthy AI, without a mediating governance
structure, these ethical principles will not translate into practice. In this
paper, we propose a multilevel governance approach that involves three groups
of interdependent stakeholders: governments, corporations, and citizens. We
examine their interrelationships through dimensions of trust, such as
competence, integrity, and benevolence. The levels of governance combined with
the dimensions of trust in AI provide practical insights that can be used to
further enhance user experiences and inform public policy related to AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choung_H/0/1/0/all/0/1&quot;&gt;Hyesun Choung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+David_P/0/1/0/all/0/1&quot;&gt;Prabu David&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seberger_J/0/1/0/all/0/1&quot;&gt;John S. Seberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03200">
<title>Transcribing Educational Videos Using Whisper: A preliminary study on using AI for transcribing educational videos. (arXiv:2307.03200v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.03200</link>
<description rdf:parseType="Literal">&lt;p&gt;Videos are increasingly being used for e-learning, and transcripts are vital
to enhance the learning experience. The costs and delays of generating
transcripts can be alleviated by automatic speech recognition (ASR) systems. In
this article, we quantify the transcripts generated by whisper for 25
educational videos and identify some open avenues of research when leveraging
ASR for transcribing educational videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1&quot;&gt;Ashwin Rao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03201">
<title>Scaling Laws Do Not Scale. (arXiv:2307.03201v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03201</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work has proposed a power law relationship, referred to as ``scaling
laws,&apos;&apos; between the performance of artificial intelligence (AI) models and
aspects of those models&apos; design (e.g., dataset size). In other words, as the
size of a dataset (or model parameters, etc) increases, the performance of a
given model trained on that dataset will correspondingly increase. However,
while compelling in the aggregate, this scaling law relationship overlooks the
ways that metrics used to measure performance may be precarious and contested,
or may not correspond with how different groups of people may perceive the
quality of models&apos; output. In this paper, we argue that as the size of datasets
used to train large AI models grows, the number of distinct communities
(including demographic groups) whose data is included in a given dataset is
likely to grow, each of whom may have different values. As a result, there is
an increased risk that communities represented in a dataset may have values or
preferences not captured by (or in the worst case, at odds with) the metrics
used to evaluate model performance for scaling laws. We end the paper with
implications for AI scaling laws -- that models may not, in fact, continue to
improve as the datasets get larger -- at least not for all people or
communities impacted by those models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1&quot;&gt;Fernando Diaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madaio_M/0/1/0/all/0/1&quot;&gt;Michael Madaio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03212">
<title>Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings. (arXiv:2307.03212v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03212</link>
<description rdf:parseType="Literal">&lt;p&gt;Urban region embedding is an important and yet highly challenging issue due
to the complexity and constantly changing nature of urban data. To address the
challenges, we propose a Region-Wise Multi-View Representation Learning (ROMER)
to capture multi-view dependencies and learn expressive representations of
urban regions without the constraints of rigid neighbourhood region conditions.
Our model focus on learn urban region representation from multi-source urban
data. First, we capture the multi-view correlations from mobility flow
patterns, POI semantics and check-in dynamics. Then, we adopt global graph
attention networks to learn similarity of any two vertices in graphs. To
comprehensively consider and share features of multiple views, a two-stage
fusion module is further proposed to learn weights with external attention to
fuse multi-view embeddings. Extensive experiments for two downstream tasks on
real-world datasets demonstrate that our model outperforms state-of-the-art
methods by up to 17\% improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1&quot;&gt;Weiliang Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1&quot;&gt;Qianqian Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03254">
<title>Vision Language Transformers: A Survey. (arXiv:2307.03254v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03254</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision language tasks, such as answering questions about or generating
captions that describe an image, are difficult tasks for computers to perform.
A relatively recent body of research has adapted the pretrained transformer
architecture introduced in \citet{vaswani2017attention} to vision language
modeling. Transformer models have greatly improved performance and versatility
over previous vision language models. They do so by pretraining models on a
large generic datasets and transferring their learning to new tasks with minor
changes in architecture and parameter values. This type of transfer learning
has become the standard modeling practice in both natural language processing
and computer vision. Vision language transformers offer the promise of
producing similar advancements in tasks which require both vision and language.
In this paper, we provide a broad synthesis of the currently available research
on vision language transformer models and offer some analysis of their
strengths, limitations and some open questions that remain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fields_C/0/1/0/all/0/1&quot;&gt;Clayton Fields&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kennington_C/0/1/0/all/0/1&quot;&gt;Casey Kennington&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03274">
<title>It is not Sexually Suggestive, It is Educative. Separating Sex Education from Suggestive Content on TikTok Videos. (arXiv:2307.03274v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03274</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce SexTok, a multi-modal dataset composed of TikTok videos labeled
as sexually suggestive (from the annotator&apos;s point of view), sex-educational
content, or neither. Such a dataset is necessary to address the challenge of
distinguishing between sexually suggestive content and virtual sex education
videos on TikTok. Children&apos;s exposure to sexually suggestive videos has been
shown to have adversarial effects on their development. Meanwhile, virtual sex
education, especially on subjects that are more relevant to the LGBTQIA+
community, is very valuable. The platform&apos;s current system removes or penalizes
some of both types of videos, even though they serve different purposes. Our
dataset contains video URLs, and it is also audio transcribed. To validate its
importance, we explore two transformer-based models for classifying the videos.
Our preliminary results suggest that the task of distinguishing between these
types of videos is learnable but challenging. These experiments suggest that
this dataset is meaningful and invites further study on the subject.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+George_E/0/1/0/all/0/1&quot;&gt;Enfa George&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1&quot;&gt;Mihai Surdeanu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03305">
<title>A Vulnerability of Attribution Methods Using Pre-Softmax Scores. (arXiv:2307.03305v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03305</link>
<description rdf:parseType="Literal">&lt;p&gt;We discuss a vulnerability involving a category of attribution methods used
to provide explanations for the outputs of convolutional neural networks
working as classifiers. It is known that this type of networks are vulnerable
to adversarial attacks, in which imperceptible perturbations of the input may
alter the outputs of the model. In contrast, here we focus on effects that
small modifications in the model may cause on the attribution method without
altering the model outputs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lerma_M/0/1/0/all/0/1&quot;&gt;Miguel Lerma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucas_M/0/1/0/all/0/1&quot;&gt;Mirtha Lucas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03311">
<title>On Invariance, Equivariance, Correlation and Convolution of Spherical Harmonic Representations for Scalar and Vectorial Data. (arXiv:2307.03311v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03311</link>
<description rdf:parseType="Literal">&lt;p&gt;The mathematical representations of data in the Spherical Harmonic (SH)
domain has recently regained increasing interest in the machine learning
community. This technical report gives an in-depth introduction to the
theoretical foundation and practical implementation of SH representations,
summarizing works on rotation invariant and equivariant features, as well as
convolutions and exact correlations of signals on spheres. In extension, these
methods are then generalized from scalar SH representations to Vectorial
Harmonics (VH), providing the same capabilities for 3d vector fields on spheres
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1&quot;&gt;Janis Keuper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03315">
<title>Assisting Clinical Decisions for Scarcely Available Treatment via Disentangled Latent Representation. (arXiv:2307.03315v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03315</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracorporeal membrane oxygenation (ECMO) is an essential life-supporting
modality for COVID-19 patients who are refractory to conventional therapies.
However, the proper treatment decision has been the subject of significant
debate and it remains controversial about who benefits from this scarcely
available and technically complex treatment option. To support clinical
decisions, it is a critical need to predict the treatment need and the
potential treatment and no-treatment responses. Targeting this clinical
challenge, we propose Treatment Variational AutoEncoder (TVAE), a novel
approach for individualized treatment analysis. TVAE is specifically designed
to address the modeling challenges like ECMO with strong treatment selection
bias and scarce treatment cases. TVAE conceptualizes the treatment decision as
a multi-scale problem. We model a patient&apos;s potential treatment assignment and
the factual and counterfactual outcomes as part of their intrinsic
characteristics that can be represented by a deep latent variable model. The
factual and counterfactual prediction errors are alleviated via a
reconstruction regularization scheme together with semi-supervision, and the
selection bias and the scarcity of treatment cases are mitigated by the
disentangled and distribution-matched latent space and the label-balancing
generative strategy. We evaluate TVAE on two real-world COVID-19 datasets: an
international dataset collected from 1651 hospitals across 63 countries, and a
institutional dataset collected from 15 hospitals. The results show that TVAE
outperforms state-of-the-art treatment effect models in predicting both the
propensity scores and factual outcomes on heterogeneous COVID-19 datasets.
Additional experiments also show TVAE outperforms the best existing models in
individual treatment effect estimation on the synthesized IHDP benchmark
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1&quot;&gt;Bing Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Said_A/0/1/0/all/0/1&quot;&gt;Ahmed Sameh Said&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Ziqi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hanyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Neel Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hanqing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Payne_P/0/1/0/all/0/1&quot;&gt;Philip Payne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chenyang Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03360">
<title>Evaluating Biased Attitude Associations of Language Models in an Intersectional Context. (arXiv:2307.03360v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.03360</link>
<description rdf:parseType="Literal">&lt;p&gt;Language models are trained on large-scale corpora that embed implicit biases
documented in psychology. Valence associations (pleasantness/unpleasantness) of
social groups determine the biased attitudes towards groups and concepts in
social cognition. Building on this established literature, we quantify how
social groups are valenced in English language models using a sentence template
that provides an intersectional context. We study biases related to age,
education, gender, height, intelligence, literacy, race, religion, sex, sexual
orientation, social class, and weight. We present a concept projection approach
to capture the valence subspace through contextualized word embeddings of
language models. Adapting the projection-based approach to embedding
association tests that quantify bias, we find that language models exhibit the
most biased attitudes against gender identity, social class, and sexual
orientation signals in language. We find that the largest and better-performing
model that we study is also more biased as it effectively captures bias
embedded in sociocultural data. We validate the bias evaluation method by
overperforming on an intrinsic valence evaluation task. The approach enables us
to measure complex intersectional biases as they are known to manifest in the
outputs and applications of language models that perpetuate historical biases.
Moreover, our approach contributes to design justice as it studies the
associations of groups underrepresented in language such as transgender and
homosexual individuals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabbaghi_S/0/1/0/all/0/1&quot;&gt;Shiva Omrani Sabbaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolfe_R/0/1/0/all/0/1&quot;&gt;Robert Wolfe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1&quot;&gt;Aylin Caliskan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03362">
<title>Adaptation and Communication in Human-Robot Teaming to Handle Discrepancies in Agents&apos; Beliefs about Plans. (arXiv:2307.03362v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.03362</link>
<description rdf:parseType="Literal">&lt;p&gt;When agents collaborate on a task, it is important that they have some shared
mental model of the task routines -- the set of feasible plans towards
achieving the goals. However, in reality, situations often arise that such a
shared mental model cannot be guaranteed, such as in ad-hoc teams where agents
may follow different conventions or when contingent constraints arise that only
some agents are aware of. Previous work on human-robot teaming has assumed that
the team has a set of shared routines, which breaks down in these situations.
In this work, we leverage epistemic logic to enable agents to understand the
discrepancy in each other&apos;s beliefs about feasible plans and dynamically plan
their actions to adapt or communicate to resolve the discrepancy. We propose a
formalism that extends conditional doxastic logic to describe knowledge bases
in order to explicitly represent agents&apos; nested beliefs on the feasible plans
and state of execution. We provide an online execution algorithm based on Monte
Carlo Tree Search for the agent to plan its action, including communication
actions to explain the feasibility of plans, announce intent, and ask
questions. Finally, we evaluate the success rate and scalability of the
algorithm and show that our agent is better equipped to work in teams without
the guarantee of a shared mental model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuening Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1&quot;&gt;Brian C. Williams&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03373">
<title>All in One: Exploring Unified Vision-Language Tracking with Multi-Modal Alignment. (arXiv:2307.03373v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03373</link>
<description rdf:parseType="Literal">&lt;p&gt;Current mainstream vision-language (VL) tracking framework consists of three
parts, \ie a visual feature extractor, a language feature extractor, and a
fusion model. To pursue better performance, a natural modus operandi for VL
tracking is employing customized and heavier unimodal encoders, and multi-modal
fusion models. Albeit effective, existing VL trackers separate feature
extraction and feature integration, resulting in extracted features that lack
semantic guidance and have limited target-aware capability in complex
scenarios, \eg similar distractors and extreme illumination. In this work,
inspired by the recent success of exploring foundation models with unified
architecture for both natural language and computer vision tasks, we propose an
All-in-One framework, which learns joint feature extraction and interaction by
adopting a unified transformer backbone. Specifically, we mix raw vision and
language signals to generate language-injected vision tokens, which we then
concatenate before feeding into the unified backbone architecture. This
approach achieves feature integration in a unified backbone, removing the need
for carefully-designed fusion modules and resulting in a more effective and
efficient VL tracking framework. To further improve the learning efficiency, we
introduce a multi-modal alignment module based on cross-modal and intra-modal
contrastive objectives, providing more reasonable representations for the
unified All-in-One transformer backbone. Extensive experiments on five
benchmarks, \ie OTB99-L, TNL2K, LaSOT, LaSOT$_{\rm Ext}$ and WebUAV-3M,
demonstrate the superiority of the proposed tracker against existing
state-of-the-arts on VL tracking. Codes will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chunhui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Li Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yiqian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanfeng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03379">
<title>Efficient Ground Vehicle Path Following in Game AI. (arXiv:2307.03379v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.03379</link>
<description rdf:parseType="Literal">&lt;p&gt;This short paper presents an efficient path following solution for ground
vehicles tailored to game AI. Our focus is on adapting established techniques
to design simple solutions with parameters that are easily tunable for an
efficient benchmark path follower. Our solution pays particular attention to
computing a target speed which uses quadratic Bezier curves to estimate the
path curvature. The performance of the proposed path follower is evaluated
through a variety of test scenarios in a first-person shooter game,
demonstrating its effectiveness and robustness in handling different types of
paths and vehicles. We achieved a 70% decrease in the total number of stuck
events compared to an existing path following solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaetzen_R/0/1/0/all/0/1&quot;&gt;Rodrigue de Schaetzen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sestini_A/0/1/0/all/0/1&quot;&gt;Alessandro Sestini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03380">
<title>On Formal Feature Attribution and Its Approximation. (arXiv:2307.03380v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.03380</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed the widespread use of artificial intelligence
(AI) algorithms and machine learning (ML) models. Despite their tremendous
success, a number of vital problems like ML model brittleness, their fairness,
and the lack of interpretability warrant the need for the active developments
in explainable artificial intelligence (XAI) and formal ML model verification.
The two major lines of work in XAI include feature selection methods, e.g.
Anchors, and feature attribution techniques, e.g. LIME and SHAP. Despite their
promise, most of the existing feature selection and attribution approaches are
susceptible to a range of critical issues, including explanation unsoundness
and out-of-distribution sampling. A recent formal approach to XAI (FXAI)
although serving as an alternative to the above and free of these issues
suffers from a few other limitations. For instance and besides the scalability
limitation, the formal approach is unable to tackle the feature attribution
problem. Additionally, a formal explanation despite being formally sound is
typically quite large, which hampers its applicability in practical settings.
Motivated by the above, this paper proposes a way to apply the apparatus of
formal XAI to the case of feature attribution based on formal explanation
enumeration. Formal feature attribution (FFA) is argued to be advantageous over
the existing methods, both formal and non-formal. Given the practical
complexity of the problem, the paper then proposes an efficient technique for
approximating exact FFA. Finally, it offers experimental evidence of the
effectiveness of the proposed approximate FFA in comparison to the existing
feature attribution algorithms not only in terms of feature importance and but
also in terms of their relative order.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jinqiang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ignatiev_A/0/1/0/all/0/1&quot;&gt;Alexey Ignatiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stuckey_P/0/1/0/all/0/1&quot;&gt;Peter J. Stuckey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03393">
<title>Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. (arXiv:2307.03393v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03393</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning on Graphs has attracted immense attention due to its wide real-world
applications. The most popular pipeline for learning on graphs with textual
node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes
shallow text embedding as initial node representations, which has limitations
in general knowledge and profound semantic understanding. In recent years,
Large Language Models (LLMs) have been proven to possess extensive common
knowledge and powerful semantic comprehension abilities that have
revolutionized existing workflows to handle text data. In this paper, we aim to
explore the potential of LLMs in graph machine learning, especially the node
classification task, and investigate two possible pipelines: LLMs-as-Enhancers
and LLMs-as-Predictors. The former leverages LLMs to enhance nodes&apos; text
attributes with their massive knowledge and then generate predictions through
GNNs. The latter attempts to directly employ LLMs as standalone predictors. We
conduct comprehensive and systematical studies on these two pipelines under
various settings. From comprehensive empirical results, we make original
observations and find new insights that open new possibilities and suggest
promising directions to leverage LLMs for learning on graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhikai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1&quot;&gt;Haitao Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1&quot;&gt;Wei Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hongzhi Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiaochi Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuaiqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dawei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1&quot;&gt;Wenqi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiliang Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03406">
<title>Goal-Conditioned Predictive Coding as an Implicit Planner for Offline Reinforcement Learning. (arXiv:2307.03406v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03406</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work has demonstrated the effectiveness of formulating decision making
as a supervised learning problem on offline-collected trajectories. However,
the benefits of performing sequence modeling on trajectory data is not yet
clear. In this work we investigate if sequence modeling has the capability to
condense trajectories into useful representations that can contribute to policy
learning. To achieve this, we adopt a two-stage framework that first summarizes
trajectories with sequence modeling techniques, and then employs these
representations to learn a policy along with a desired goal. This design allows
many existing supervised offline RL methods to be considered as specific
instances of our framework. Within this framework, we introduce
Goal-Conditioned Predicitve Coding (GCPC), an approach that brings powerful
trajectory representations and leads to performant policies. We conduct
extensive empirical evaluations on AntMaze, FrankaKitchen and Locomotion
environments, and observe that sequence modeling has a significant impact on
some decision making tasks. In addition, we demonstrate that GCPC learns a
goal-conditioned latent representation about the future, which serves as an
&quot;implicit planner&quot;, and enables competitive performance on all three
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zilai Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Ce Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Chen Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03419">
<title>QI2 -- an Interactive Tool for Data Quality Assurance. (arXiv:2307.03419v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.03419</link>
<description rdf:parseType="Literal">&lt;p&gt;The importance of high data quality is increasing with the growing impact and
distribution of ML systems and big data. Also the planned AI Act from the
European commission defines challenging legal requirements for data quality
especially for the market introduction of safety relevant ML systems. In this
paper we introduce a novel approach that supports the data quality assurance
process of multiple data quality aspects. This approach enables the
verification of quantitative data quality requirements. The concept and
benefits are introduced and explained on small example data sets. How the
method is applied is demonstrated on the well known MNIST data set based an
handwritten digits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geerkens_S/0/1/0/all/0/1&quot;&gt;Simon Geerkens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sieberichs_C/0/1/0/all/0/1&quot;&gt;Christian Sieberichs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braun_A/0/1/0/all/0/1&quot;&gt;Alexander Braun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waschulzik_T/0/1/0/all/0/1&quot;&gt;Thomas Waschulzik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03421">
<title>Non-iterative Coarse-to-fine Transformer Networks for Joint Affine and Deformable Image Registration. (arXiv:2307.03421v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03421</link>
<description rdf:parseType="Literal">&lt;p&gt;Image registration is a fundamental requirement for medical image analysis.
Deep registration methods based on deep learning have been widely recognized
for their capabilities to perform fast end-to-end registration. Many deep
registration methods achieved state-of-the-art performance by performing
coarse-to-fine registration, where multiple registration steps were iterated
with cascaded networks. Recently, Non-Iterative Coarse-to-finE (NICE)
registration methods have been proposed to perform coarse-to-fine registration
in a single network and showed advantages in both registration accuracy and
runtime. However, existing NICE registration methods mainly focus on deformable
registration, while affine registration, a common prerequisite, is still
reliant on time-consuming traditional optimization-based methods or extra
affine registration networks. In addition, existing NICE registration methods
are limited by the intrinsic locality of convolution operations. Transformers
may address this limitation for their capabilities to capture long-range
dependency, but the benefits of using transformers for NICE registration have
not been explored. In this study, we propose a Non-Iterative Coarse-to-finE
Transformer network (NICE-Trans) for image registration. Our NICE-Trans is the
first deep registration method that (i) performs joint affine and deformable
coarse-to-fine registration within a single network, and (ii) embeds
transformers into a NICE registration framework to model long-range relevance
between images. Extensive experiments with seven public datasets show that our
NICE-Trans outperforms state-of-the-art registration methods on both
registration accuracy and runtime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1&quot;&gt;Mingyuan Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_L/0/1/0/all/0/1&quot;&gt;Lei Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fulham_M/0/1/0/all/0/1&quot;&gt;Michael Fulham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1&quot;&gt;Dagan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jinman Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03444">
<title>Towards Deep Network Steganography: From Networks to Networks. (arXiv:2307.03444v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.03444</link>
<description rdf:parseType="Literal">&lt;p&gt;With the widespread applications of the deep neural network (DNN), how to
covertly transmit the DNN models in public channels brings us the attention,
especially for those trained for secret-learning tasks. In this paper, we
propose deep network steganography for the covert communication of DNN models.
Unlike the existing steganography schemes which focus on the subtle
modification of the cover data to accommodate the secrets, our scheme is
learning task oriented, where the learning task of the secret DNN model (termed
as secret-learning task) is disguised into another ordinary learning task
conducted in a stego DNN model (termed as stego-learning task). To this end, we
propose a gradient-based filter insertion scheme to insert interference filters
into the important positions in the secret DNN model to form a stego DNN model.
These positions are then embedded into the stego DNN model using a key by side
information hiding. Finally, we activate the interference filters by a partial
optimization strategy, such that the generated stego DNN model works on the
stego-learning task. We conduct the experiments on both the intra-task
steganography and inter-task steganography (i.e., the secret and stego-learning
tasks belong to the same and different categories), both of which demonstrate
the effectiveness of our proposed method for covert communication of DNN
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guobiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Meiling Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1&quot;&gt;Zhenxing Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinpeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03465">
<title>TBGC: Task-level Backbone-Oriented Gradient Clip for Multi-Task Foundation Model Learning. (arXiv:2307.03465v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03465</link>
<description rdf:parseType="Literal">&lt;p&gt;The AllInOne training paradigm squeezes a wide range of tasks into a unified
model in a multi-task learning manner. However, optimization in multi-task
learning is more challenge than single-task learning, as the gradient norm from
different tasks may vary greatly, making the backbone overly biased towards one
specific task. To address this issue, we propose the task-level
backbone-oriented gradient clip paradigm, compared with the vanilla gradient
clip method, it has two points of emphasis:1) gradient clip is performed
independently for each task. 2) backbone gradients generated from each task are
rescaled to the same norm scale. Based on the experimental results, we argue
that the task-level backbone-oriented gradient clip paradigm can relieve the
gradient bias problem to some extent. We also propose a novel multi-branch data
augmentation strategy where conflict augmentations are placed in different
branches. Our approach has been shown to be effective and finally achieve 1st
place in the Leaderboard A and 2nd place in the Leaderboard B of the CVPR2023
Foundation Model Challenge. It&apos;s worth noting that instead of evaluating all
three tasks(detection, segmentation and fine-grained classification) in
Leaderboard A, the segmentation task is not evaluated in Leaderboard B, in
which our team has a huge advantage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zelun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xue Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03486">
<title>Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning. (arXiv:2307.03486v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03486</link>
<description rdf:parseType="Literal">&lt;p&gt;Discovering achievements with a hierarchical structure on procedurally
generated environments poses a significant challenge. This requires agents to
possess a broad range of abilities, including generalization and long-term
reasoning. Many prior methods are built upon model-based or hierarchical
approaches, with the belief that an explicit module for long-term planning
would be beneficial for learning hierarchical achievements. However, these
methods require an excessive amount of environment interactions or large model
sizes, limiting their practicality. In this work, we identify that proximal
policy optimization (PPO), a simple and versatile model-free algorithm,
outperforms the prior methods with recent implementation practices. Moreover,
we find that the PPO agent can predict the next achievement to be unlocked to
some extent, though with low confidence. Based on this observation, we propose
a novel contrastive learning method, called achievement distillation, that
strengthens the agent&apos;s capability to predict the next achievement. Our method
exhibits a strong capacity for discovering hierarchical achievements and shows
state-of-the-art performance on the challenging Crafter environment using fewer
model parameters in a sample-efficient regime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1&quot;&gt;Seungyong Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeom_J/0/1/0/all/0/1&quot;&gt;Junyoung Yeom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_B/0/1/0/all/0/1&quot;&gt;Bumsoo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Hyun Oh Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03492">
<title>Large AI Model-Based Semantic Communications. (arXiv:2307.03492v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.03492</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic communication (SC) is an emerging intelligent paradigm, offering
solutions for various future applications like metaverse, mixed-reality, and
the Internet of everything. However, in current SC systems, the construction of
the knowledge base (KB) faces several issues, including limited knowledge
representation, frequent knowledge updates, and insecure knowledge sharing.
Fortunately, the development of the large AI model provides new solutions to
overcome above issues. Here, we propose a large AI model-based SC framework
(LAM-SC) specifically designed for image data, where we first design the
segment anything model (SAM)-based KB (SKB) that can split the original image
into different semantic segments by universal semantic knowledge. Then, we
present an attention-based semantic integration (ASI) to weigh the semantic
segments generated by SKB without human participation and integrate them as the
semantic-aware image. Additionally, we propose an adaptive semantic compression
(ASC) encoding to remove redundant information in semantic features, thereby
reducing communication overhead. Finally, through simulations, we demonstrate
the effectiveness of the LAM-SC framework and the significance of the large AI
model-based KB development in future SC paradigms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1&quot;&gt;Feibo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yubo Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1&quot;&gt;Li Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kezhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1&quot;&gt;Cunhua Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1&quot;&gt;Xiaohu You&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03505">
<title>RCDN -- Robust X-Corner Detection Algorithm based on Advanced CNN Model. (arXiv:2307.03505v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03505</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate detection and localization of X-corner on both planar and non-planar
patterns is a core step in robotics and machine vision. However, previous works
could not make a good balance between accuracy and robustness, which are both
crucial criteria to evaluate the detectors performance. To address this
problem, in this paper we present a novel detection algorithm which can
maintain high sub-pixel precision on inputs under multiple interference, such
as lens distortion, extreme poses and noise. The whole algorithm, adopting a
coarse-to-fine strategy, contains a X-corner detection network and three
post-processing techniques to distinguish the correct corner candidates, as
well as a mixed sub-pixel refinement technique and an improved region growth
strategy to recover the checkerboard pattern partially visible or occluded
automatically. Evaluations on real and synthetic images indicate that the
presented algorithm has the higher detection rate, sub-pixel accuracy and
robustness than other commonly used methods. Finally, experiments of camera
calibration and pose estimation verify it can also get smaller re-projection
error in quantitative comparisons to the state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Ben Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caihua Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Quanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1&quot;&gt;Zhonghua Wan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03506">
<title>Derivative Free Weight-space Ensembling. (arXiv:2307.03506v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.03506</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work suggests that interpolating between the weights of two
specialized language models can transfer knowledge between tasks in a way that
multi-task learning cannot. However, very few have explored interpolation
between more than two models, where each has a distinct knowledge base. In this
paper, we introduce Derivative Free Weight-space Ensembling (DFWE), a new
few-sample task transfer approach for open-domain dialogue. Our framework
creates a set of diverse expert language models trained using a predefined set
of source tasks. Next, we finetune each of the expert models on the target
task, approaching the target task from several distinct knowledge bases.
Finally, we linearly interpolate between the model weights using a
gradient-free-optimization algorithm, to efficiently find a good interpolation
weighting. We demonstrate the effectiveness of the method on FETA-Friends
outperforming the standard pretrain-finetune approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ninalga_D/0/1/0/all/0/1&quot;&gt;Dean Ninalga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03512">
<title>Tranfer Learning of Semantic Segmentation Methods for Identifying Buried Archaeological Structures on LiDAR Data. (arXiv:2307.03512v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03512</link>
<description rdf:parseType="Literal">&lt;p&gt;When applying deep learning to remote sensing data in archaeological
research, a notable obstacle is the limited availability of suitable datasets
for training models. The application of transfer learning is frequently
employed to mitigate this drawback. However, there is still a need to explore
its effectiveness when applied across different archaeological datasets. This
paper compares the performance of various transfer learning configurations
using two semantic segmentation deep neural networks on two LiDAR datasets. The
experimental results indicate that transfer learning-based approaches in
archaeology can lead to performance improvements, although a systematic
enhancement has not yet been observed. We provide specific insights about the
validity of such techniques that can serve as a baseline for future works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soleni_P/0/1/0/all/0/1&quot;&gt;Paolo Soleni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaart_W/0/1/0/all/0/1&quot;&gt;Wouter B. Verschoof-van der Vaart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kokalj_Z/0/1/0/all/0/1&quot;&gt;&amp;#x17d;iga Kokalj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Traviglia_A/0/1/0/all/0/1&quot;&gt;Arianna Traviglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiorucci_M/0/1/0/all/0/1&quot;&gt;Marco Fiorucci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03539">
<title>Large Language Models as Batteries-Included Zero-Shot ESCO Skills Matchers. (arXiv:2307.03539v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.03539</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding labour market dynamics requires accurately identifying the
skills required for and possessed by the workforce. Automation techniques are
increasingly being developed to support this effort. However, automatically
extracting skills from job postings is challenging due to the vast number of
existing skills. The ESCO (European Skills, Competences, Qualifications and
Occupations) framework provides a useful reference, listing over 13,000
individual skills. However, skills extraction remains difficult and accurately
matching job posts to the ESCO taxonomy is an open problem. In this work, we
propose an end-to-end zero-shot system for skills extraction from job
descriptions based on large language models (LLMs). We generate synthetic
training data for the entirety of ESCO skills and train a classifier to extract
skill mentions from job posts. We also employ a similarity retriever to
generate skill candidates which are then re-ranked using a second LLM. Using
synthetic data achieves an RP@10 score 10 points higher than previous distant
supervision approaches. Adding GPT-4 re-ranking improves RP@10 by over 22
points over previous methods. We also show that Framing the task as mock
programming when prompting the LLM can lead to better performance than natural
language prompts, especially with weaker LLMs. We demonstrate the potential of
integrating large language models at both ends of skills matching pipelines.
Our approach requires no human annotations and achieve extremely promising
results on skills extraction against ESCO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clavie_B/0/1/0/all/0/1&quot;&gt;Benjamin Clavi&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soulie_G/0/1/0/all/0/1&quot;&gt;Guillaume Souli&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03575">
<title>Multimodal Deep Learning for Personalized Renal Cell Carcinoma Prognosis: Integrating CT Imaging and Clinical Data. (arXiv:2307.03575v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03575</link>
<description rdf:parseType="Literal">&lt;p&gt;Renal cell carcinoma represents a significant global health challenge with a
low survival rate. This research aimed to devise a comprehensive deep-learning
model capable of predicting survival probabilities in patients with renal cell
carcinoma by integrating CT imaging and clinical data and addressing the
limitations observed in prior studies. The aim is to facilitate the
identification of patients requiring urgent treatment. The proposed framework
comprises three modules: a 3D image feature extractor, clinical variable
selection, and survival prediction. The feature extractor module, based on the
3D CNN architecture, predicts the ISUP grade of renal cell carcinoma tumors
linked to mortality rates from CT images. A selection of clinical variables is
systematically chosen using the Spearman score and random forest importance
score as criteria. A deep learning-based network, trained with discrete
LogisticHazard-based loss, performs the survival prediction. Nine distinct
experiments are performed, with varying numbers of clinical variables
determined by different thresholds of the Spearman and importance scores. Our
findings demonstrate that the proposed strategy surpasses the current
literature on renal cancer prognosis based on CT scans and clinical factors.
The best-performing experiment yielded a concordance index of 0.84 and an area
under the curve value of 0.8 on the test cohort, which suggests strong
predictive power. The multimodal deep-learning approach developed in this study
shows promising results in estimating survival probabilities for renal cell
carcinoma patients using CT imaging and clinical data. This may have potential
implications in identifying patients who require urgent treatment, potentially
improving patient outcomes. The code created for this project is available for
the public on:
\href{https://github.com/Balasingham-AI-Group/Survival_CTplusClinical}{GitHub}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahootiha_M/0/1/0/all/0/1&quot;&gt;Maryamalsadat Mahootiha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qadir_H/0/1/0/all/0/1&quot;&gt;Hemin Ali Qadir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bergsland_J/0/1/0/all/0/1&quot;&gt;Jacob Bergsland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasingham_I/0/1/0/all/0/1&quot;&gt;Ilangko Balasingham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03591">
<title>Structure Guided Multi-modal Pre-trained Transformer for Knowledge Graph Reasoning. (arXiv:2307.03591v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.03591</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal knowledge graphs (MKGs), which intuitively organize information in
various modalities, can benefit multiple practical downstream tasks, such as
recommendation systems, and visual question answering. However, most MKGs are
still far from complete, which motivates the flourishing of MKG reasoning
models. Recently, with the development of general artificial architectures, the
pretrained transformer models have drawn increasing attention, especially for
multimodal scenarios. However, the research of multimodal pretrained
transformer (MPT) for knowledge graph reasoning (KGR) is still at an early
stage. As the biggest difference between MKG and other multimodal data, the
rich structural information underlying the MKG still cannot be fully leveraged
in existing MPT models. Most of them only utilize the graph structure as a
retrieval map for matching images and texts connected with the same entity.
This manner hinders their reasoning performances. To this end, we propose the
graph Structure Guided Multimodal Pretrained Transformer for knowledge graph
reasoning, termed SGMPT. Specifically, the graph structure encoder is adopted
for structural feature encoding. Then, a structure-guided fusion module with
two different strategies, i.e., weighted summation and alignment constraint, is
first designed to inject the structural information into both the textual and
visual features. To the best of our knowledge, SGMPT is the first MPT model for
multimodal KGR, which mines the structural information underlying the knowledge
graph. Extensive experiments on FB15k-237-IMG and WN18-IMG, demonstrate that
our SGMPT outperforms existing state-of-the-art models, and prove the
effectiveness of the designed strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Ke Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Sihang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1&quot;&gt;Lingyuan Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Meng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinwang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03592">
<title>VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis. (arXiv:2307.03592v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03592</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a data-driven generative framework for synthesizing blood vessel
3D geometry. This is a challenging task due to the complexity of vascular
systems, which are highly variating in shape, size, and structure. Existing
model-based methods provide some degree of control and variation in the
structures produced, but fail to capture the diversity of actual anatomical
data. We developed VesselVAE, a recursive variational Neural Network that fully
exploits the hierarchical organization of the vessel and learns a
low-dimensional manifold encoding branch connectivity along with geometry
features describing the target surface. After training, the VesselVAE latent
space can be sampled to generate new vessel geometries. To the best of our
knowledge, this work is the first to utilize this technique for synthesizing
blood vessels. We achieve similarities of synthetic and real data for radius
(.97), length (.95), and tortuosity (.96). By leveraging the power of deep
neural networks, we generate 3D models of blood vessels that are both accurate
and diverse, which is crucial for medical and surgical training, hemodynamic
simulations, and many other purposes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldman_P/0/1/0/all/0/1&quot;&gt;Paula Feldman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fainstein_M/0/1/0/all/0/1&quot;&gt;Miguel Fainstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siless_V/0/1/0/all/0/1&quot;&gt;Viviana Siless&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delrieux_C/0/1/0/all/0/1&quot;&gt;Claudio Delrieux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iarussi_E/0/1/0/all/0/1&quot;&gt;Emmanuel Iarussi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03595">
<title>GEANN: Scalable Graph Augmentations for Multi-Horizon Time Series Forecasting. (arXiv:2307.03595v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03595</link>
<description rdf:parseType="Literal">&lt;p&gt;Encoder-decoder deep neural networks have been increasingly studied for
multi-horizon time series forecasting, especially in real-world applications.
However, to forecast accurately, these sophisticated models typically rely on a
large number of time series examples with substantial history. A rapidly
growing topic of interest is forecasting time series which lack sufficient
historical data -- often referred to as the ``cold start&apos;&apos; problem. In this
paper, we introduce a novel yet simple method to address this problem by
leveraging graph neural networks (GNNs) as a data augmentation for enhancing
the encoder used by such forecasters. These GNN-based features can capture
complex inter-series relationships, and their generation process can be
optimized end-to-end with the forecasting task. We show that our architecture
can use either data-driven or domain knowledge-defined graphs, scaling to
incorporate information from multiple very large graphs with millions of nodes.
In our target application of demand forecasting for a large e-commerce
retailer, we demonstrate on both a small dataset of 100K products and a large
dataset with over 2 million products that our method improves overall
performance over competitive baseline models. More importantly, we show that it
brings substantially more gains to ``cold start&apos;&apos; products such as those newly
launched or recently out-of-stock.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Sitan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolff_M/0/1/0/all/0/1&quot;&gt;Malcolm Wolff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramasubramanian_S/0/1/0/all/0/1&quot;&gt;Shankar Ramasubramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quenneville_Belair_V/0/1/0/all/0/1&quot;&gt;Vincent Quenneville-Belair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metha_R/0/1/0/all/0/1&quot;&gt;Ronak Metha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1&quot;&gt;Michael W. Mahoney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03637">
<title>Discovering Variable Binding Circuitry with Desiderata. (arXiv:2307.03637v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.03637</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work has shown that computation in language models may be
human-understandable, with successful efforts to localize and intervene on both
single-unit features and input-output circuits. Here, we introduce an approach
which extends causal mediation experiments to automatically identify model
components responsible for performing a specific subtask by solely specifying a
set of \textit{desiderata}, or causal attributes of the model components
executing that subtask. As a proof of concept, we apply our method to
automatically discover shared \textit{variable binding circuitry} in LLaMA-13B,
which retrieves variable values for multiple arithmetic tasks. Our method
successfully localizes variable binding to only 9 attention heads (of the 1.6k)
and one MLP in the final token&apos;s residual stream.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davies_X/0/1/0/all/0/1&quot;&gt;Xander Davies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nadeau_M/0/1/0/all/0/1&quot;&gt;Max Nadeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakash_N/0/1/0/all/0/1&quot;&gt;Nikhil Prakash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaham_T/0/1/0/all/0/1&quot;&gt;Tamar Rott Shaham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1&quot;&gt;David Bau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03659">
<title>Decomposing the Generalization Gap in Imitation Learning for Visual Robotic Manipulation. (arXiv:2307.03659v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.03659</link>
<description rdf:parseType="Literal">&lt;p&gt;What makes generalization hard for imitation learning in visual robotic
manipulation? This question is difficult to approach at face value, but the
environment from the perspective of a robot can often be decomposed into
enumerable factors of variation, such as the lighting conditions or the
placement of the camera. Empirically, generalization to some of these factors
have presented a greater obstacle than others, but existing work sheds little
light on precisely how much each factor contributes to the generalization gap.
Towards an answer to this question, we study imitation learning policies in
simulation and on a real robot language-conditioned manipulation task to
quantify the difficulty of generalization to different (sets of) factors. We
also design a new simulated benchmark of 19 tasks with 11 factors of variation
to facilitate more controlled evaluations of generalization. From our study, we
determine an ordering of factors based on generalization difficulty, that is
consistent across simulation and our real robot setup.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_A/0/1/0/all/0/1&quot;&gt;Annie Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1&quot;&gt;Lisa Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Ted Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03678">
<title>Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations. (arXiv:2307.03678v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.03678</link>
<description rdf:parseType="Literal">&lt;p&gt;This research focuses on assessing the ability of large language models
(LLMs) in representing geometries and their spatial relations. We utilize LLMs
including GPT-2 and BERT to encode the well-known text (WKT) format of
geometries and then feed their embeddings into classifiers and regressors to
evaluate the effectiveness of the LLMs-generated embeddings for geometric
attributes. The experiments demonstrate that while the LLMs-generated
embeddings can preserve geometry types and capture some spatial relations (up
to 73% accuracy), challenges remain in estimating numeric values and retrieving
spatially related objects. This research highlights the need for improvement in
terms of capturing the nuances and complexities of the underlying geospatial
data and integrating domain knowledge to support various GeoAI applications
using foundation models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1&quot;&gt;Yuhan Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Song Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03681">
<title>Guideline for Trustworthy Artificial Intelligence -- AI Assessment Catalog. (arXiv:2307.03681v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.03681</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence (AI) has made impressive progress in recent years and
represents a key technology that has a crucial impact on the economy and
society. However, it is clear that AI and business models based on it can only
reach their full potential if AI applications are developed according to high
quality standards and are effectively protected against new AI risks. For
instance, AI bears the risk of unfair treatment of individuals when processing
personal data e.g., to support credit lending or staff recruitment decisions.
The emergence of these new risks is closely linked to the fact that the
behavior of AI applications, particularly those based on Machine Learning (ML),
is essentially learned from large volumes of data and is not predetermined by
fixed programmed rules.
&lt;/p&gt;
&lt;p&gt;Thus, the issue of the trustworthiness of AI applications is crucial and is
the subject of numerous major publications by stakeholders in politics,
business and society. In addition, there is mutual agreement that the
requirements for trustworthy AI, which are often described in an abstract way,
must now be made clear and tangible. One challenge to overcome here relates to
the fact that the specific quality criteria for an AI application depend
heavily on the application context and possible measures to fulfill them in
turn depend heavily on the AI technology used. Lastly, practical assessment
procedures are needed to evaluate whether specific AI applications have been
developed according to adequate quality standards. This AI assessment catalog
addresses exactly this point and is intended for two target groups: Firstly, it
provides developers with a guideline for systematically making their AI
applications trustworthy. Secondly, it guides assessors and auditors on how to
examine AI applications for trustworthiness in a structured way.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poretschkin_M/0/1/0/all/0/1&quot;&gt;Maximilian Poretschkin&lt;/a&gt; (1 and 2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmitz_A/0/1/0/all/0/1&quot;&gt;Anna Schmitz&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akila_M/0/1/0/all/0/1&quot;&gt;Maram Akila&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adilova_L/0/1/0/all/0/1&quot;&gt;Linara Adilova&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Becker_D/0/1/0/all/0/1&quot;&gt;Daniel Becker&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremers_A/0/1/0/all/0/1&quot;&gt;Armin B. Cremers&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hecker_D/0/1/0/all/0/1&quot;&gt;Dirk Hecker&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houben_S/0/1/0/all/0/1&quot;&gt;Sebastian Houben&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mock_M/0/1/0/all/0/1&quot;&gt;Michael Mock&lt;/a&gt; (1 and 2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenzweig_J/0/1/0/all/0/1&quot;&gt;Julia Rosenzweig&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sicking_J/0/1/0/all/0/1&quot;&gt;Joachim Sicking&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_E/0/1/0/all/0/1&quot;&gt;Elena Schulz&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voss_A/0/1/0/all/0/1&quot;&gt;Angelika Voss&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wrobel_S/0/1/0/all/0/1&quot;&gt;Stefan Wrobel&lt;/a&gt; (1 and 2) ((1) Fraunhofer Institute for Intelligent Analysis and Information Systems IAIS, Sankt Augustin, Germany, (2) Department of Computer Science, University of Bonn, Bonn, Germany)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03691">
<title>Comparing Apples to Apples: Generating Aspect-Aware Comparative Sentences from User Review. (arXiv:2307.03691v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.03691</link>
<description rdf:parseType="Literal">&lt;p&gt;It is time-consuming to find the best product among many similar
alternatives. Comparative sentences can help to contrast one item from others
in a way that highlights important features of an item that stand out. Given
reviews of one or multiple items and relevant item features, we generate
comparative review sentences to aid users to find the best fit. Specifically,
our model consists of three successive components in a transformer: (i) an item
encoding module to encode an item for comparison, (ii) a comparison generation
module that generates comparative sentences in an autoregressive manner, (iii)
a novel decoding method for user personalization. We show that our pipeline
generates fluent and diverse comparative sentences. We run experiments on the
relevance and fidelity of our generated sentences in a human evaluation study
and find that our algorithm creates comparative review sentences that are
relevant and truthful.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Echterhoff_J/0/1/0/all/0/1&quot;&gt;Jessica Echterhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1&quot;&gt;An Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1&quot;&gt;Julian McAuley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03692">
<title>Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning. (arXiv:2307.03692v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.03692</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce the Instruction Following Score (IFS), a metric
that detects language models&apos; ability to follow instructions. The metric has a
dual purpose. First, IFS can be used to distinguish between base and instruct
models. We benchmark publicly available base and instruct models, and show that
the ratio of well formatted responses to partial and full sentences can be an
effective measure between those two model classes. Secondly, the metric can be
used as an early stopping criteria for instruct tuning. We compute IFS for
Supervised Fine-Tuning (SFT) of 7B and 13B LLaMA models, showing that models
learn to follow instructions relatively early in the training process, and the
further finetuning can result in changes in the underlying base model
semantics. As an example of semantics change we show the objectivity of model
predictions, as defined by an auxiliary metric ObjecQA. We show that in this
particular case, semantic changes are the steepest when the IFS tends to
plateau. We hope that decomposing instruct tuning into IFS and semantic factors
starts a new trend in better controllable instruct tuning and opens
possibilities for designing minimal instruct interfaces querying foundation
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+AlShikh_W/0/1/0/all/0/1&quot;&gt;Waseem AlShikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daaboul_M/0/1/0/all/0/1&quot;&gt;Manhal Daaboul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goddard_K/0/1/0/all/0/1&quot;&gt;Kirk Goddard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imel_B/0/1/0/all/0/1&quot;&gt;Brock Imel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamble_K/0/1/0/all/0/1&quot;&gt;Kiran Kamble&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1&quot;&gt;Parikshith Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russak_M/0/1/0/all/0/1&quot;&gt;Melisa Russak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03694">
<title>Scalable Membership Inference Attacks via Quantile Regression. (arXiv:2307.03694v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03694</link>
<description rdf:parseType="Literal">&lt;p&gt;Membership inference attacks are designed to determine, using black box
access to trained models, whether a particular example was used in training or
not. Membership inference can be formalized as a hypothesis testing problem.
The most effective existing attacks estimate the distribution of some test
statistic (usually the model&apos;s confidence on the true label) on points that
were (and were not) used in training by training many \emph{shadow models} --
i.e. models of the same architecture as the model being attacked, trained on a
random subsample of data. While effective, these attacks are extremely
computationally expensive, especially when the model under attack is large.
&lt;/p&gt;
&lt;p&gt;We introduce a new class of attacks based on performing quantile regression
on the distribution of confidence scores induced by the model under attack on
points that are not used in training. We show that our method is competitive
with state-of-the-art shadow model attacks, while requiring substantially less
compute because our attack requires training only a single model. Moreover,
unlike shadow model attacks, our proposed attack does not require any knowledge
of the architecture of the model under attack and is therefore truly
``black-box&quot;. We show the efficacy of this approach in an extensive series of
experiments on various datasets and model architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertran_M/0/1/0/all/0/1&quot;&gt;Martin Bertran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shuai Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kearns_M/0/1/0/all/0/1&quot;&gt;Michael Kearns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morgenstern_J/0/1/0/all/0/1&quot;&gt;Jamie Morgenstern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1&quot;&gt;Aaron Roth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Steven Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03699">
<title>Unveiling the Potential of Knowledge-Prompted ChatGPT for Enhancing Drug Trafficking Detection on Social Media. (arXiv:2307.03699v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.03699</link>
<description rdf:parseType="Literal">&lt;p&gt;Social media platforms such as Instagram and Twitter have emerged as critical
channels for drug marketing and illegal sale. Detecting and labeling online
illicit drug trafficking activities becomes important in addressing this issue.
However, the effectiveness of conventional supervised learning methods in
detecting drug trafficking heavily relies on having access to substantial
amounts of labeled data, while data annotation is time-consuming and
resource-intensive. Furthermore, these models often face challenges in
accurately identifying trafficking activities when drug dealers use deceptive
language and euphemisms to avoid detection. To overcome this limitation, we
conduct the first systematic study on leveraging large language models (LLMs),
such as ChatGPT, to detect illicit drug trafficking activities on social media.
We propose an analytical framework to compose \emph{knowledge-informed
prompts}, which serve as the interface that humans can interact with and use
LLMs to perform the detection task. Additionally, we design a Monte Carlo
dropout based prompt optimization method to further to improve performance and
interpretability. Our experimental findings demonstrate that the proposed
framework outperforms other baseline language models in terms of drug
trafficking detection accuracy, showing a remarkable improvement of nearly
12\%. By integrating prior knowledge and the proposed prompts, ChatGPT can
effectively identify and label drug trafficking activities on social networks,
even in the presence of deceptive language and euphemisms used by drug dealers
to evade detection. The implications of our research extend to social networks,
emphasizing the importance of incorporating prior knowledge and scenario-based
prompts into analytical tools to improve online security and public safety.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1&quot;&gt;Chuanbo Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yanfang Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03705">
<title>Intelligent Robotic Sonographer: Mutual Information-based Disentangled Reward Learning from Few Demonstrations. (arXiv:2307.03705v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.03705</link>
<description rdf:parseType="Literal">&lt;p&gt;Ultrasound (US) imaging is widely used for biometric measurement and
diagnosis of internal organs due to the advantages of being real-time and
radiation-free. However, due to high inter-operator variability, resulting
images highly depend on operators&apos; experience. In this work, an intelligent
robotic sonographer is proposed to autonomously &quot;explore&quot; target anatomies and
navigate a US probe to a relevant 2D plane by learning from expert. The
underlying high-level physiological knowledge from experts is inferred by a
neural reward function, using a ranked pairwise image comparisons approach in a
self-supervised fashion. This process can be referred to as understanding the
&quot;language of sonography&quot;. Considering the generalization capability to overcome
inter-patient variations, mutual information is estimated by a network to
explicitly extract the task-related and domain features in latent space.
Besides, a Gaussian distribution-based filter is developed to automatically
evaluate and take the quality of the expert&apos;s demonstrations into account. The
robotic localization is carried out in coarse-to-fine mode based on the
predicted reward associated to B-mode images. To demonstrate the performance of
the proposed approach, representative experiments for the &quot;line&quot; target and
&quot;point&quot; target are performed on vascular phantom and two ex-vivo animal organ
phantoms (chicken heart and lamb kidney), respectively. The results
demonstrated that the proposed advanced framework can robustly work on
different kinds of known and unseen phantoms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhongliang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_Y/0/1/0/all/0/1&quot;&gt;Yuan Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingchuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Ying Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burke_M/0/1/0/all/0/1&quot;&gt;Michael Burke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_a/0/1/0/all/0/1&quot;&gt;and Nassir Navab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03718">
<title>Frontier AI Regulation: Managing Emerging Risks to Public Safety. (arXiv:2307.03718v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.03718</link>
<description rdf:parseType="Literal">&lt;p&gt;Advanced AI models hold the promise of tremendous benefits for humanity, but
society needs to proactively manage the accompanying risks. In this paper, we
focus on what we term &quot;frontier AI&quot; models: highly capable foundation models
that could possess dangerous capabilities sufficient to pose severe risks to
public safety. Frontier AI models pose a distinct regulatory challenge:
dangerous capabilities can arise unexpectedly; it is difficult to robustly
prevent a deployed model from being misused; and, it is difficult to stop a
model&apos;s capabilities from proliferating broadly. To address these challenges,
at least three building blocks for the regulation of frontier models are
needed: (1) standard-setting processes to identify appropriate requirements for
frontier AI developers, (2) registration and reporting requirements to provide
regulators with visibility into frontier AI development processes, and (3)
mechanisms to ensure compliance with safety standards for the development and
deployment of frontier AI models. Industry self-regulation is an important
first step. However, wider societal discussions and government intervention
will be needed to create standards and to ensure compliance with them. We
consider several options to this end, including granting enforcement powers to
supervisory authorities and licensure regimes for frontier AI models. Finally,
we propose an initial set of safety standards. These include conducting
pre-deployment risk assessments; external scrutiny of model behavior; using
risk assessments to inform deployment decisions; and monitoring and responding
to new information about model capabilities and uses post-deployment. We hope
this discussion contributes to the broader conversation on how to balance
public safety risks and innovation benefits from advances at the frontier of AI
development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderljung_M/0/1/0/all/0/1&quot;&gt;Markus Anderljung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnhart_J/0/1/0/all/0/1&quot;&gt;Joslyn Barnhart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leung_J/0/1/0/all/0/1&quot;&gt;Jade Leung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korinek_A/0/1/0/all/0/1&quot;&gt;Anton Korinek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OKeefe_C/0/1/0/all/0/1&quot;&gt;Cullen O&amp;#x27;Keefe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whittlestone_J/0/1/0/all/0/1&quot;&gt;Jess Whittlestone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avin_S/0/1/0/all/0/1&quot;&gt;Shahar Avin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brundage_M/0/1/0/all/0/1&quot;&gt;Miles Brundage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bullock_J/0/1/0/all/0/1&quot;&gt;Justin Bullock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cass_Beggs_D/0/1/0/all/0/1&quot;&gt;Duncan Cass-Beggs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1&quot;&gt;Ben Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_T/0/1/0/all/0/1&quot;&gt;Tantum Collins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fist_T/0/1/0/all/0/1&quot;&gt;Tim Fist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadfield_G/0/1/0/all/0/1&quot;&gt;Gillian Hadfield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayes_A/0/1/0/all/0/1&quot;&gt;Alan Hayes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_L/0/1/0/all/0/1&quot;&gt;Lewis Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1&quot;&gt;Sara Hooker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1&quot;&gt;Eric Horvitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolt_N/0/1/0/all/0/1&quot;&gt;Noam Kolt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuett_J/0/1/0/all/0/1&quot;&gt;Jonas Schuett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shavit_Y/0/1/0/all/0/1&quot;&gt;Yonadav Shavit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siddarth_D/0/1/0/all/0/1&quot;&gt;Divya Siddarth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trager_R/0/1/0/all/0/1&quot;&gt;Robert Trager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_K/0/1/0/all/0/1&quot;&gt;Kevin Wolf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03750">
<title>When does the ID algorithm fail?. (arXiv:2307.03750v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2307.03750</link>
<description rdf:parseType="Literal">&lt;p&gt;The ID algorithm solves the problem of identification of interventional
distributions of the form p(Y | do(a)) in graphical causal models, and has been
formulated in a number of ways [12, 9, 6]. The ID algorithm is sound (outputs
the correct functional of the observed data distribution whenever p(Y | do(a))
is identified in the causal model represented by the input graph), and complete
(explicitly flags as a failure any input p(Y | do(a)) whenever this
distribution is not identified in the causal model represented by the input
graph).
&lt;/p&gt;
&lt;p&gt;The reference [9] provides a result, the so called &quot;hedge criterion&quot;
(Corollary 3), which aims to give a graphical characterization of situations
when the ID algorithm fails to identify its input in terms of a structure in
the input graph called the hedge. While the ID algorithm is, indeed, a sound
and complete algorithm, and the hedge structure does arise whenever the input
distribution is not identified, Corollary 3 presented in [9] is incorrect as
stated. In this note, I outline the modern presentation of the ID algorithm,
discuss a simple counterexample to Corollary 3, and provide a number of
graphical characterizations of the ID algorithm failing to identify its input
distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shpitser_I/0/1/0/all/0/1&quot;&gt;Ilya Shpitser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2004.11145">
<title>F2A2: Flexible Fully-decentralized Approximate Actor-critic for Cooperative Multi-agent Reinforcement Learning. (arXiv:2004.11145v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2004.11145</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional centralized multi-agent reinforcement learning (MARL) algorithms
are sometimes unpractical in complicated applications, due to non-interactivity
between agents, curse of dimensionality and computation complexity. Hence,
several decentralized MARL algorithms are motivated. However, existing
decentralized methods only handle the fully cooperative setting where massive
information needs to be transmitted in training. The block coordinate gradient
descent scheme they used for successive independent actor and critic steps can
simplify the calculation, but it causes serious bias. In this paper, we propose
a flexible fully decentralized actor-critic MARL framework, which can combine
most of actor-critic methods, and handle large-scale general cooperative
multi-agent setting. A primal-dual hybrid gradient descent type algorithm
framework is designed to learn individual agents separately for
decentralization. From the perspective of each agent, policy improvement and
value evaluation are jointly optimized, which can stabilize multi-agent policy
learning. Furthermore, our framework can achieve scalability and stability for
large-scale environment and reduce information transmission, by the parameter
sharing mechanism and a novel modeling-other-agents methods based on
theory-of-mind and online supervised learning. Sufficient experiments in
cooperative Multi-agent Particle Environment and StarCraft II show that our
decentralized MARL instantiation algorithms perform competitively against
conventional centralized and decentralized methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1&quot;&gt;Bo Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiangfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1&quot;&gt;Hongyuan Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.09836">
<title>Creativity of AI: Hierarchical Planning Model Learning for Facilitating Deep Reinforcement Learning. (arXiv:2112.09836v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2112.09836</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite of achieving great success in real-world applications, Deep
Reinforcement Learning (DRL) is still suffering from three critical issues,
i.e., data efficiency, lack of the interpretability and transferability. Recent
research shows that embedding symbolic knowledge into DRL is promising in
addressing those challenges. Inspired by this, we introduce a novel deep
reinforcement learning framework with symbolic options. Our framework features
a loop training procedure, which enables guiding the improvement of policy by
planning with planning models (including action models and hierarchical task
network models) and symbolic options learned from interactive trajectories
automatically. The learned symbolic options alleviate the dense requirement of
expert domain knowledge and provide inherent interpretability of policies.
Moreover, the transferability and data efficiency can be further improved by
planning with the symbolic planning models. To validate the effectiveness of
our framework, we conduct experiments on two domains, Montezuma&apos;s Revenge and
Office World, respectively. The results demonstrate the comparable performance,
improved data efficiency, interpretability and transferability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_H/0/1/0/all/0/1&quot;&gt;Hankz Hankui Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Shuting Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1&quot;&gt;Mu Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhihao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1&quot;&gt;Kebing Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chao Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.05745">
<title>Deep Optimal Transport for Domain Adaptation on SPD Manifolds. (arXiv:2201.05745v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2201.05745</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, there has been significant interest in solving the domain
adaptation (DA) problem on symmetric positive definite (SPD) manifolds within
the machine learning community. This interest stems from the fact that complex
neurophysiological data generated by medical equipment, such as
electroencephalograms, magnetoencephalograms, and diffusion tensor imaging,
often exhibit a shift in data distribution across different domains. These data
representations, represented by signal covariance matrices, possess properties
of symmetry and positive definiteness. However, directly applying previous
experiences and solutions to the DA problem poses challenges due to the
manipulation complexities of covariance matrices.To address this, our research
introduces a category of deep learning-based transfer learning approaches
called deep optimal transport. This category utilizes optimal transport theory
and leverages the Log-Euclidean geometry for SPD manifolds. Additionally, we
present a comprehensive categorization of existing geometric methods to tackle
these problems effectively. This categorization provides practical solutions
for specific DA problems, including handling discrepancies in marginal and
conditional distributions between the source and target domains on the SPD
manifold. To evaluate the effectiveness, we conduct experiments on three
publicly available highly non-stationary cross-session brain-computer interface
scenarios. Moreover, we provide visualization results on the SPD cone to offer
further insights into the framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1&quot;&gt;Ce Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1&quot;&gt;Cuntai Guan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.03466">
<title>Reward-Respecting Subtasks for Model-Based Reinforcement Learning. (arXiv:2202.03466v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.03466</link>
<description rdf:parseType="Literal">&lt;p&gt;To achieve the ambitious goals of artificial intelligence, reinforcement
learning must include planning with a model of the world that is abstract in
state and time. Deep learning has made progress with state abstraction, but
temporal abstraction has rarely been used, despite extensively developed theory
based on the options framework. One reason for this is that the space of
possible options is immense, and the methods previously proposed for option
discovery do not take into account how the option models will be used in
planning. Options are typically discovered by posing subsidiary tasks, such as
reaching a bottleneck state or maximizing the cumulative sum of a sensory
signal other than reward. Each subtask is solved to produce an option, and then
a model of the option is learned and made available to the planning process. In
most previous work, the subtasks ignore the reward on the original problem,
whereas we propose subtasks that use the original reward plus a bonus based on
a feature of the state at the time the option terminates. We show that option
models obtained from such reward-respecting subtasks are much more likely to be
useful in planning than eigenoptions, shortest path options based on bottleneck
states, or reward-respecting options generated by the option-critic. Reward
respecting subtasks strongly constrain the space of options and thereby also
provide a partial solution to the problem of option discovery. Finally, we show
how values, policies, options, and models can all be learned online and
off-policy using standard algorithms and general value functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1&quot;&gt;Richard S. Sutton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Machado_M/0/1/0/all/0/1&quot;&gt;Marlos C. Machado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holland_G/0/1/0/all/0/1&quot;&gt;G. Zacharias Holland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szepesvari_D/0/1/0/all/0/1&quot;&gt;David Szepesvari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Timbers_F/0/1/0/all/0/1&quot;&gt;Finbarr Timbers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanner_B/0/1/0/all/0/1&quot;&gt;Brian Tanner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1&quot;&gt;Adam White&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.08815">
<title>GRAPHSHAP: Explaining Identity-Aware Graph Classifiers Through the Language of Motifs. (arXiv:2202.08815v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.08815</link>
<description rdf:parseType="Literal">&lt;p&gt;Most methods for explaining black-box classifiers (e.g. on tabular data,
images, or time series) rely on measuring the impact that removing/perturbing
features has on the model output. This forces the explanation language to match
the classifier&apos;s feature space. However, when dealing with graph data, in which
the basic features correspond to the edges describing the graph structure, this
matching between features space and explanation language might not be
appropriate. Decoupling the feature space (edges) from a desired high-level
explanation language (such as motifs) is thus a major challenge towards
developing actionable explanations for graph classification tasks. In this
paper we introduce GRAPHSHAP, a Shapley-based approach able to provide
motif-based explanations for identity-aware graph classifiers, assuming no
knowledge whatsoever about the model or its training data: the only requirement
is that the classifier can be queried as a black-box at will. For the sake of
computational efficiency we explore a progressive approximation strategy and
show how a simple kernel can efficiently approximate explanation scores, thus
allowing GRAPHSHAP to scale on scenarios with a large explanation space (i.e.
large number of motifs). We showcase GRAPHSHAP on a real-world brain-network
dataset consisting of patients affected by Autism Spectrum Disorder and a
control group. Our experiments highlight how the classification provided by a
black-box model can be effectively explained by few connectomics patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perotti_A/0/1/0/all/0/1&quot;&gt;Alan Perotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bajardi_P/0/1/0/all/0/1&quot;&gt;Paolo Bajardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonchi_F/0/1/0/all/0/1&quot;&gt;Francesco Bonchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panisson_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Panisson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.09495">
<title>Topical: Learning Repository Embeddings from Source Code using Attention. (arXiv:2208.09495v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2208.09495</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning on source code (MLOnCode) promises to transform how software
is delivered. By mining the context and relationship between software
artefacts, MLOnCode augments the software developers capabilities with code
auto-generation, code recommendation, code auto-tagging and other data-driven
enhancements. For many of these tasks a script level representation of code is
sufficient, however, in many cases a repository level representation that takes
into account various dependencies and repository structure is imperative, for
example, auto-tagging repositories with topics or auto-documentation of
repository code etc. Existing methods for computing repository level
representations suffer from (a) reliance on natural language documentation of
code (for example, README files) (b) naive aggregation of method/script-level
representation, for example, by concatenation or averaging. This paper
introduces Topical a deep neural network to generate repository level
embeddings of publicly available GitHub code repositories directly from source
code. Topical incorporates an attention mechanism that projects the source
code, the full dependency graph and the script level textual information into a
dense repository-level representation. To compute the repository-level
representations, Topical is trained to predict the topics associated with a
repository, on a dataset of publicly available GitHub repositories that were
crawled along with their ground truth topic tags. Our experiments show that the
embeddings computed by Topical are able to outperform multiple baselines,
including baselines that naively combine the method-level representations
through averaging or concatenation at the task of repository auto-tagging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lherondelle_A/0/1/0/all/0/1&quot;&gt;Agathe Lherondelle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babbar_V/0/1/0/all/0/1&quot;&gt;Varun Babbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Satsangi_Y/0/1/0/all/0/1&quot;&gt;Yash Satsangi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silavong_F/0/1/0/all/0/1&quot;&gt;Fran Silavong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eloul_S/0/1/0/all/0/1&quot;&gt;Shaltiel Eloul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1&quot;&gt;Sean Moran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.05953">
<title>Breadth-First Pipeline Parallelism. (arXiv:2211.05953v2 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/2211.05953</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Breadth-First Pipeline Parallelism, a novel training schedule
which optimizes the combination of pipeline and data parallelism. Breadth-First
Pipeline Parallelism lowers training time, cost and memory usage by combining a
high GPU utilization with a small batch size per GPU, and by making use of
fully sharded data parallelism. Experimentally, we observed an increase of up
to 43% in training throughput for a 52 billion-parameter model using a small
batch size per GPU compared to Megatron-LM, which would reduce the training
time and cost by the same amount on a large GPU cluster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamy_Poirier_J/0/1/0/all/0/1&quot;&gt;Joel Lamy-Poirier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.06489">
<title>Equivariance with Learned Canonicalization Functions. (arXiv:2211.06489v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.06489</link>
<description rdf:parseType="Literal">&lt;p&gt;Symmetry-based neural networks often constrain the architecture in order to
achieve invariance or equivariance to a group of transformations. In this
paper, we propose an alternative that avoids this architectural constraint by
learning to produce canonical representations of the data. These
canonicalization functions can readily be plugged into non-equivariant backbone
architectures. We offer explicit ways to implement them for some groups of
interest. We show that this approach enjoys universality while providing
interpretable insights. Our main hypothesis, supported by our empirical
results, is that learning a small neural network to perform canonicalization is
better than using predefined heuristics. Our experiments show that learning the
canonicalization function is competitive with existing techniques for learning
equivariant functions across many tasks, including image classification,
$N$-body dynamics prediction, point cloud classification and part segmentation,
while being faster across the board.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaba_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;kou-Oumar Kaba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1&quot;&gt;Arnab Kumar Mondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravanbakhsh_S/0/1/0/all/0/1&quot;&gt;Siamak Ravanbakhsh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09811">
<title>Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model. (arXiv:2212.09811v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09811</link>
<description rdf:parseType="Literal">&lt;p&gt;The recently released NLLB-200 is a set of multilingual Neural Machine
Translation models that cover 202 languages. The largest model is based on a
Mixture of Experts architecture and achieves SoTA results across many language
pairs. It contains 54.5B parameters and requires at least four 32GB GPUs just
for inference. In this work, we propose a pruning method that enables the
removal of up to 80% of experts without further finetuning and with a
negligible loss in translation quality, which makes it feasible to run the
model on a single 32GB GPU. Further analysis suggests that our pruning metrics
can identify language-specific experts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koishekenov_Y/0/1/0/all/0/1&quot;&gt;Yeskendir Koishekenov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berard_A/0/1/0/all/0/1&quot;&gt;Alexandre Berard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1&quot;&gt;Vassilina Nikoulina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10936">
<title>A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling. (arXiv:2212.10936v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10936</link>
<description rdf:parseType="Literal">&lt;p&gt;The following interdisciplinary article presents a memetic algorithm with
applying deep reinforcement learning (DRL) for solving practically oriented
dual resource constrained flexible job shop scheduling problems (DRC-FJSSP).
From research projects in industry, we recognize the need to consider flexible
machines, flexible human workers, worker capabilities, setup and processing
operations, material arrival times, complex job paths with parallel tasks for
bill of material (BOM) manufacturing, sequence-dependent setup times and
(partially) automated tasks in human-machine-collaboration. In recent years,
there has been extensive research on metaheuristics and DRL techniques but
focused on simple scheduling environments. However, there are few approaches
combining metaheuristics and DRL to generate schedules more reliably and
efficiently. In this paper, we first formulate a DRC-FJSSP to map complex
industry requirements beyond traditional job shop models. Then we propose a
scheduling framework integrating a discrete event simulation (DES) for schedule
evaluation, considering parallel computing and multicriteria optimization.
Here, a memetic algorithm is enriched with DRL to improve sequencing and
assignment decisions. Through numerical experiments with real-world production
data, we confirm that the framework generates feasible schedules efficiently
and reliably for a balanced optimization of makespan (MS) and total tardiness
(TT). Utilizing DRL instead of random metaheuristic operations leads to better
results in fewer algorithm iterations and outperforms traditional approaches in
such complex environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grumbach_F/0/1/0/all/0/1&quot;&gt;Felix Grumbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badr_N/0/1/0/all/0/1&quot;&gt;Nour Eldin Alaa Badr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reusch_P/0/1/0/all/0/1&quot;&gt;Pascal Reusch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trojahn_S/0/1/0/all/0/1&quot;&gt;Sebastian Trojahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.11498">
<title>Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with Robotic and Human Co-Workers. (arXiv:2212.11498v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.11498</link>
<description rdf:parseType="Literal">&lt;p&gt;We envision a warehouse in which dozens of mobile robots and human pickers
work together to collect and deliver items within the warehouse. The
fundamental problem we tackle, called the order-picking problem, is how these
worker agents must coordinate their movement and actions in the warehouse to
maximise performance (e.g. order throughput). Established industry methods
using heuristic approaches require large engineering efforts to optimise for
innately variable warehouse configurations. In contrast, multi-agent
reinforcement learning (MARL) can be flexibly applied to diverse warehouse
configurations (e.g. size, layout, number/types of workers, item replenishment
frequency), as the agents learn through experience how to optimally cooperate
with one another. We develop hierarchical MARL algorithms in which a manager
assigns goals to worker agents, and the policies of the manager and workers are
co-trained toward maximising a global objective (e.g. pick rate). Our
hierarchical algorithms achieve significant gains in sample efficiency and
overall pick rates over baseline MARL algorithms in diverse warehouse
configurations, and substantially outperform two established industry
heuristics for order-picking systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krnjaic_A/0/1/0/all/0/1&quot;&gt;Aleksandar Krnjaic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steleac_R/0/1/0/all/0/1&quot;&gt;Raul D. Steleac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomas_J/0/1/0/all/0/1&quot;&gt;Jonathan D. Thomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papoudakis_G/0/1/0/all/0/1&quot;&gt;Georgios Papoudakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schafer_L/0/1/0/all/0/1&quot;&gt;Lukas Sch&amp;#xe4;fer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+To_A/0/1/0/all/0/1&quot;&gt;Andrew Wing Keung To&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lao_K/0/1/0/all/0/1&quot;&gt;Kuan-Ho Lao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cubuktepe_M/0/1/0/all/0/1&quot;&gt;Murat Cubuktepe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haley_M/0/1/0/all/0/1&quot;&gt;Matthew Haley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borsting_P/0/1/0/all/0/1&quot;&gt;Peter B&amp;#xf6;rsting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1&quot;&gt;Stefano V. Albrecht&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10886">
<title>Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning. (arXiv:2301.10886v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10886</link>
<description rdf:parseType="Literal">&lt;p&gt;We present AIRS: Automatic Intrinsic Reward Shaping that intelligently and
adaptively provides high-quality intrinsic rewards to enhance exploration in
reinforcement learning (RL). More specifically, AIRS selects shaping function
from a predefined set based on the estimated task return in real-time,
providing reliable exploration incentives and alleviating the biased objective
problem. Moreover, we develop an intrinsic reward toolkit to provide efficient
and reliable implementations of diverse intrinsic reward approaches. We test
AIRS on various tasks of MiniGrid, Procgen, and DeepMind Control Suite.
Extensive simulation demonstrates that AIRS can outperform the benchmarking
schemes and achieve superior performance with simple architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Mingqi Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xin Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1&quot;&gt;Wenjun Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12589">
<title>Revisiting Modality Imbalance In Multimodal Pedestrian Detection. (arXiv:2302.12589v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12589</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal learning, particularly for pedestrian detection, has recently
received emphasis due to its capability to function equally well in several
critical autonomous driving scenarios such as low-light, night-time, and
adverse weather conditions. However, in most cases, the training distribution
largely emphasizes the contribution of one specific input that makes the
network biased towards one modality. Hence, the generalization of such models
becomes a significant problem where the non-dominant input modality during
training could be contributing more to the course of inference. Here, we
introduce a novel training setup with regularizer in the multimodal
architecture to resolve the problem of this disparity between the modalities.
Specifically, our regularizer term helps to make the feature fusion method more
robust by considering both the feature extractors equivalently important during
the training to extract the multimodal distribution which is referred to as
removing the imbalance problem. Furthermore, our decoupling concept of output
stream helps the detection task by sharing the spatial sensitive information
mutually. Extensive experiments of the proposed method on KAIST and UTokyo
datasets shows improvement of the respective state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Arindam Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1&quot;&gt;Sudip Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1&quot;&gt;Ganesh Sistu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horgan_J/0/1/0/all/0/1&quot;&gt;Jonathan Horgan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1&quot;&gt;Ujjwal Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_E/0/1/0/all/0/1&quot;&gt;Edward Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glavin_M/0/1/0/all/0/1&quot;&gt;Martin Glavin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eising_C/0/1/0/all/0/1&quot;&gt;Ciar&amp;#xe1;n Eising&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13300">
<title>The Innovation Paradox: Concept Space Expansion with Diminishing Originality and the Promise of Creative AI. (arXiv:2303.13300v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13300</link>
<description rdf:parseType="Literal">&lt;p&gt;Innovation, typically spurred by reusing, recombining, and synthesizing
existing concepts, is expected to result in an exponential growth of the
concept space over time. However, our statistical analysis of TechNet, which is
a comprehensive technology semantic network encompassing over four million
concepts derived from patent texts, reveals a linear rather than exponential
expansion of the overall technological concept space. Moreover, there is a
notable decline in the originality of newly created concepts. These trends can
be attributed to the constraints of human cognitive abilities to innovate
beyond an ever-growing space of prior art, among other factors. Integrating
creative artificial intelligence into the innovation process holds the
potential to overcome these limitations and alter the observed trends in the
future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarica_S/0/1/0/all/0/1&quot;&gt;Serhad Sarica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jianxi Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14102">
<title>SocNavGym: A Reinforcement Learning Gym for Social Navigation. (arXiv:2304.14102v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14102</link>
<description rdf:parseType="Literal">&lt;p&gt;It is essential for autonomous robots to be socially compliant while
navigating in human-populated environments. Machine Learning and, especially,
Deep Reinforcement Learning have recently gained considerable traction in the
field of Social Navigation. This can be partially attributed to the resulting
policies not being bound by human limitations in terms of code complexity or
the number of variables that are handled. Unfortunately, the lack of safety
guarantees and the large data requirements by DRL algorithms make learning in
the real world unfeasible. To bridge this gap, simulation environments are
frequently used. We propose SocNavGym, an advanced simulation environment for
social navigation that can generate a wide variety of social navigation
scenarios and facilitates the development of intelligent social agents.
SocNavGym is light-weight, fast, easy-to-use, and can be effortlessly
configured to generate different types of social navigation scenarios. It can
also be configured to work with different hand-crafted and data-driven social
reward signals and to yield a variety of evaluation metrics to benchmark
agents&apos; performance. Further, we also provide a case study where a Dueling-DQN
agent is trained to learn social-navigation policies using SocNavGym. The
results provides evidence that SocNavGym can be used to train an agent from
scratch to navigate in simple as well as complex social scenarios. Our
experiments also show that the agents trained using the data-driven reward
function displays more advanced social compliance in comparison to the
heuristic-based reward function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1&quot;&gt;Aditya Kapoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swamy_S/0/1/0/all/0/1&quot;&gt;Sushant Swamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manso_L/0/1/0/all/0/1&quot;&gt;Luis Manso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bachiller_P/0/1/0/all/0/1&quot;&gt;Pilar Bachiller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11814">
<title>Summarizing Strategy Card Game AI Competition. (arXiv:2305.11814v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11814</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper concludes five years of AI competitions based on Legends of Code
and Magic (LOCM), a small Collectible Card Game (CCG), designed with the goal
of supporting research and algorithm development. The game was used in a number
of events, including Community Contests on the CodinGame platform, and Strategy
Card Game AI Competition at the IEEE Congress on Evolutionary Computation and
IEEE Conference on Games. LOCM has been used in a number of publications
related to areas such as game tree search algorithms, neural networks,
evaluation functions, and CCG deckbuilding. We present the rules of the game,
the history of organized competitions, and a listing of the participant and
their approaches, as well as some general advice on organizing AI competitions
for the research community. Although the COG 2022 edition was announced to be
the last one, the game remains available and can be played using an online
leaderboard arena.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kowalski_J/0/1/0/all/0/1&quot;&gt;Jakub Kowalski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miernik_R/0/1/0/all/0/1&quot;&gt;Rados&amp;#x142;aw Miernik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16724">
<title>Code-Switched Text Synthesis in Unseen Language Pairs. (arXiv:2305.16724v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16724</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing efforts on text synthesis for code-switching mostly require training
on code-switched texts in the target language pairs, limiting the deployment of
the models to cases lacking code-switched data. In this work, we study the
problem of synthesizing code-switched texts for language pairs absent from the
training data. We introduce GLOSS, a model built on top of a pre-trained
multilingual machine translation model (PMMTM) with an additional
code-switching module. This module, either an adapter or extra prefixes, learns
code-switching patterns from code-switched data during training, while the
primary component of GLOSS, i.e., the PMMTM, is frozen. The design of only
adjusting the code-switching module prevents our model from overfitting to the
constrained training data for code-switching. Hence, GLOSS exhibits the ability
to generalize and synthesize code-switched texts across a broader spectrum of
language pairs. Additionally, we develop a self-training algorithm on target
language pairs further to enhance the reliability of GLOSS. Automatic
evaluations on four language pairs show that GLOSS achieves at least 55%
relative BLEU and METEOR scores improvements compared to strong baselines.
Human evaluations on two language pairs further validate the success of GLOSS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1&quot;&gt;I-Hung Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1&quot;&gt;Avik Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Shubham Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1&quot;&gt;Nanyun Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04719">
<title>Don&apos;t trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04719</link>
<description rdf:parseType="Literal">&lt;p&gt;How do neural networks extract patterns from pixels? Feature visualizations
attempt to answer this important question by visualizing highly activating
patterns through optimization. Today, visualization methods form the foundation
of our knowledge about the internal workings of neural networks, as a type of
mechanistic interpretability. Here we ask: How reliable are feature
visualizations? We start our investigation by developing network circuits that
trick feature visualizations into showing arbitrary patterns that are
completely disconnected from normal network behavior on natural input. We then
provide evidence for a similar phenomenon occurring in standard, unmanipulated
networks: feature visualizations are processed very differently from standard
input, casting doubt on their ability to &quot;explain&quot; how neural networks process
natural images. We underpin this empirical finding by theory proving that the
set of functions that can be reliably understood by feature visualization is
extremely small and does not include general black-box neural networks.
Therefore, a promising way forward could be the development of networks that
enforce certain structures in order to ensure more reliable feature
visualizations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1&quot;&gt;Robert Geirhos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1&quot;&gt;Roland S. Zimmermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilodeau_B/0/1/0/all/0/1&quot;&gt;Blair Bilodeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1&quot;&gt;Wieland Brendel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Been Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05412">
<title>Offline Prioritized Experience Replay. (arXiv:2306.05412v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05412</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline reinforcement learning (RL) is challenged by the distributional shift
problem. To address this problem, existing works mainly focus on designing
sophisticated policy constraints between the learned policy and the behavior
policy. However, these constraints are applied equally to well-performing and
inferior actions through uniform sampling, which might negatively affect the
learned policy. To alleviate this issue, we propose Offline Prioritized
Experience Replay (OPER), featuring a class of priority functions designed to
prioritize highly-rewarding transitions, making them more frequently visited
during training. Through theoretical analysis, we show that this class of
priority functions induce an improved behavior policy, and when constrained to
this improved policy, a policy-constrained offline RL algorithm is likely to
yield a better solution. We develop two practical strategies to obtain priority
weights by estimating advantages based on a fitted value network (OPER-A) or
utilizing trajectory returns (OPER-R) for quick computation. OPER is a
plug-and-play component for offline RL algorithms. As case studies, we evaluate
OPER on five different algorithms, including BC, TD3+BC, Onestep RL, CQL, and
IQL. Extensive experiments demonstrate that both OPER-A and OPER-R
significantly improve the performance for all baseline methods. Codes and
priority weights are availiable at https://github.com/sail-sg/OPER.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yang Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1&quot;&gt;Bingyi Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Gao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shiji Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shuicheng Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06722">
<title>$E(2)$-Equivariant Vision Transformer. (arXiv:2306.06722v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06722</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformer (ViT) has achieved remarkable performance in computer
vision. However, positional encoding in ViT makes it substantially difficult to
learn the intrinsic equivariance in data. Initial attempts have been made on
designing equivariant ViT but are proved defective in some cases in this paper.
To address this issue, we design a Group Equivariant Vision Transformer
(GE-ViT) via a novel, effective positional encoding operator. We prove that
GE-ViT meets all the theoretical requirements of an equivariant neural network.
Comprehensive experiments are conducted on standard benchmark datasets,
demonstrating that GE-ViT significantly outperforms non-equivariant
self-attention networks. The code is available at
https://github.com/ZJUCDSYangKaifan/GEVit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Renjun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kaifan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Ke Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1&quot;&gt;Fengxiang He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07675">
<title>An Interleaving Semantics of the Timed Concurrent Language for Argumentation to Model Debates and Dialogue Games. (arXiv:2306.07675v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07675</link>
<description rdf:parseType="Literal">&lt;p&gt;Time is a crucial factor in modelling dynamic behaviours of intelligent
agents: activities have a determined temporal duration in a real-world
environment, and previous actions influence agents&apos; behaviour. In this paper,
we propose a language for modelling concurrent interaction between agents that
also allows the specification of temporal intervals in which particular actions
occur. Such a language exploits a timed version of Abstract Argumentation
Frameworks to realise a shared memory used by the agents to communicate and
reason on the acceptability of their beliefs with respect to a given time
interval. An interleaving model on a single processor is used for basic
computation steps, with maximum parallelism for time elapsing. Following this
approach, only one of the enabled agents is executed at each moment. To
demonstrate the capabilities of language, we also show how it can be used to
model interactions such as debates and dialogue games taking place between
intelligent agents. Lastly, we present an implementation of the language that
can be accessed via a web interface. Under consideration in Theory and Practice
of Logic Programming (TPLP).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bistarelli_S/0/1/0/all/0/1&quot;&gt;Stefano Bistarelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meo_M/0/1/0/all/0/1&quot;&gt;Maria Chiara Meo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taticchi_C/0/1/0/all/0/1&quot;&gt;Carlo Taticchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13264">
<title>FedSelect: Customized Selection of Parameters for Fine-Tuning during Personalized Federated Learning. (arXiv:2306.13264v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13264</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in federated learning (FL) seek to increase client-level
performance by fine-tuning client parameters on local data or personalizing
architectures for the local task. Existing methods for such personalization
either prune a global model or fine-tune a global model on a local client
distribution. However, these existing methods either personalize at the expense
of retaining important global knowledge, or predetermine network layers for
fine-tuning, resulting in suboptimal storage of global knowledge within client
models. Enlightened by the lottery ticket hypothesis, we first introduce a
hypothesis for finding optimal client subnetworks to locally fine-tune while
leaving the rest of the parameters frozen. We then propose a novel FL
framework, FedSelect, using this procedure that directly personalizes both
client subnetwork structure and parameters, via the simultaneous discovery of
optimal parameters for personalization and the rest of parameters for global
aggregation during training. We show that this method achieves promising
results on CIFAR-10.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamirisa_R/0/1/0/all/0/1&quot;&gt;Rishub Tamirisa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Won_J/0/1/0/all/0/1&quot;&gt;John Won&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chengjun Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arel_R/0/1/0/all/0/1&quot;&gt;Ron Arel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Andy Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14275">
<title>Enhancing Adversarial Training via Reweighting Optimization Trajectory. (arXiv:2306.14275v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14275</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the fact that adversarial training has become the de facto method for
improving the robustness of deep neural networks, it is well-known that vanilla
adversarial training suffers from daunting robust overfitting, resulting in
unsatisfactory robust generalization. A number of approaches have been proposed
to address these drawbacks such as extra regularization, adversarial weights
perturbation, and training with more data over the last few years. However, the
robust generalization improvement is yet far from satisfactory. In this paper,
we approach this challenge with a brand new perspective -- refining historical
optimization trajectories. We propose a new method named \textbf{Weighted
Optimization Trajectories (WOT)} that leverages the optimization trajectories
of adversarial training in time. We have conducted extensive experiments to
demonstrate the effectiveness of WOT under various state-of-the-art adversarial
attacks. Our results show that WOT integrates seamlessly with the existing
adversarial training methods and consistently overcomes the robust overfitting
issue, resulting in better adversarial robustness. For example, WOT boosts the
robust accuracy of AT-PGD under AA-$L_{\infty}$ attack by 1.53\% $\sim$ 6.11\%
and meanwhile increases the clean accuracy by 0.55\%$\sim$5.47\% across SVHN,
CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tianjin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shiwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianlong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1&quot;&gt;Meng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1&quot;&gt;Vlaod Menkovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1&quot;&gt;Lu Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_Y/0/1/0/all/0/1&quot;&gt;Yulong Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1&quot;&gt;Mykola Pechenizkiy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17258">
<title>Suffering Toasters -- A New Self-Awareness Test for AI. (arXiv:2306.17258v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17258</link>
<description rdf:parseType="Literal">&lt;p&gt;A widely accepted definition of intelligence in the context of Artificial
Intelligence (AI) still eludes us. Due to our exceedingly rapid development of
AI paradigms, architectures, and tools, the prospect of naturally arising AI
consciousness seems more likely than ever. In this paper, we claim that all
current intelligence tests are insufficient to point to the existence or lack
of intelligence \textbf{as humans intuitively perceive it}. We draw from ideas
in the philosophy of science, psychology, and other areas of research to
provide a clearer definition of the problems of artificial intelligence,
self-awareness, and agency. We furthermore propose a new heuristic approach to
test for artificial self-awareness and outline a possible implementation.
Finally, we discuss some of the questions that arise from this new heuristic,
be they philosophical or implementation-oriented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolfson_I/0/1/0/all/0/1&quot;&gt;Ira Wolfson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02427">
<title>FOCUS: Object-Centric World Models for Robotics Manipulation. (arXiv:2307.02427v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02427</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the world in terms of objects and the possible interplays with
them is an important cognition ability, especially in robotics manipulation,
where many tasks require robot-object interactions. However, learning such a
structured world model, which specifically captures entities and relationships,
remains a challenging and underexplored problem. To address this, we propose
FOCUS, a model-based agent that learns an object-centric world model. Thanks to
a novel exploration bonus that stems from the object-centric representation,
FOCUS can be deployed on robotics manipulation tasks to explore object
interactions more easily. Evaluating our approach on manipulation tasks across
different settings, we show that object-centric world models allow the agent to
solve tasks more efficiently and enable consistent exploration of robot-object
interactions. Using a Franka Emika robot arm, we also showcase how FOCUS could
be adopted in real-world settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferraro_S/0/1/0/all/0/1&quot;&gt;Stefano Ferraro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazzaglia_P/0/1/0/all/0/1&quot;&gt;Pietro Mazzaglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbelen_T/0/1/0/all/0/1&quot;&gt;Tim Verbelen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhoedt_B/0/1/0/all/0/1&quot;&gt;Bart Dhoedt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02484">
<title>Elastic Decision Transformer. (arXiv:2307.02484v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02484</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces Elastic Decision Transformer (EDT), a significant
advancement over the existing Decision Transformer (DT) and its variants.
Although DT purports to generate an optimal trajectory, empirical evidence
suggests it struggles with trajectory stitching, a process involving the
generation of an optimal or near-optimal trajectory from the best parts of a
set of sub-optimal trajectories. The proposed EDT differentiates itself by
facilitating trajectory stitching during action inference at test time,
achieved by adjusting the history length maintained in DT. Further, the EDT
optimizes the trajectory by retaining a longer history when the previous
trajectory is optimal and a shorter one when it is sub-optimal, enabling it to
&quot;stitch&quot; with a more optimal trajectory. Extensive experimentation demonstrates
EDT&apos;s ability to bridge the performance gap between DT-based and Q
Learning-based approaches. In particular, the EDT outperforms Q Learning-based
methods in a multi-task regime on the D4RL locomotion benchmark and Atari
games. Videos are available at: https://kristery.github.io/edt/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yueh-Hua Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamaya_M/0/1/0/all/0/1&quot;&gt;Masashi Hamaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02792">
<title>What Should Data Science Education Do with Large Language Models?. (arXiv:2307.02792v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02792</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid advances of large language models (LLMs), such as ChatGPT, are
revolutionizing data science and statistics. These state-of-the-art tools can
streamline complex processes. As a result, it reshapes the role of data
scientists. We argue that LLMs are transforming the responsibilities of data
scientists, shifting their focus from hands-on coding, data-wrangling and
conducting standard analyses to assessing and managing analyses performed by
these automated AIs. This evolution of roles is reminiscent of the transition
from a software engineer to a product manager. We illustrate this transition
with concrete data science case studies using LLMs in this paper. These
developments necessitate a meaningful evolution in data science education.
Pedagogy must now place greater emphasis on cultivating diverse skillsets among
students, such as LLM-informed creativity, critical thinking, AI-guided
programming. LLMs can also play a significant role in the classroom as
interactive teaching and learning tools, contributing to personalized
education. This paper discusses the opportunities, resources and open
challenges for each of these directions. As with any transformative technology,
integrating LLMs into education calls for careful consideration. While LLMs can
perform repetitive tasks efficiently, it&apos;s crucial to remember that their role
is to supplement human intelligence and creativity, not to replace it.
Therefore, the new era of data science education should balance the benefits of
LLMs while fostering complementary human expertise and innovations. In
conclusion, the rise of LLMs heralds a transformative period for data science
and its education. This paper seeks to shed light on the emerging trends,
potential opportunities, and challenges accompanying this paradigm shift,
hoping to spark further discourse and investigation into this exciting,
uncharted territory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_X/0/1/0/all/0/1&quot;&gt;Xinming Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Weijie J. Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Linjun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03070">
<title>Hybrid Knowledge-Data Driven Channel Semantic Acquisition and Beamforming for Cell-Free Massive MIMO. (arXiv:2307.03070v1 [eess.SP] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.03070</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper focuses on advancing outdoor wireless systems to better support
ubiquitous extended reality (XR) applications, and close the gap with current
indoor wireless transmission capabilities. We propose a hybrid knowledge-data
driven method for channel semantic acquisition and multi-user beamforming in
cell-free massive multiple-input multiple-output (MIMO) systems. Specifically,
we firstly propose a data-driven multiple layer perceptron (MLP)-Mixer-based
auto-encoder for channel semantic acquisition, where the pilot signals, CSI
quantizer for channel semantic embedding, and CSI reconstruction for channel
semantic extraction are jointly optimized in an end-to-end manner. Moreover,
based on the acquired channel semantic, we further propose a knowledge-driven
deep-unfolding multi-user beamformer, which is capable of achieving good
spectral efficiency with robustness to imperfect CSI in outdoor XR scenarios.
By unfolding conventional successive over-relaxation (SOR)-based linear
beamforming scheme with deep learning, the proposed beamforming scheme is
capable of adaptively learning the optimal parameters to accelerate convergence
and improve the robustness to imperfect CSI. The proposed deep unfolding
beamforming scheme can be used for access points (APs) with fully-digital array
and APs with hybrid analog-digital array structure. Simulation results
demonstrate the effectiveness of our proposed scheme in improving the accuracy
of channel acquisition, as well as reducing complexity in both CSI acquisition
and beamformer design. The proposed beamforming method achieves approximately
96% of the converged spectrum efficiency performance after only three
iterations in downlink transmission, demonstrating its efficacy and potential
to improve outdoor XR applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zhen Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shicong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yu Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhongxiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zheng_D/0/1/0/all/0/1&quot;&gt;Dezhi Zheng&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>