<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-08T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02974" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02976" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02982" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02985" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02987" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02991" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02992" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02993" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02994" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02995" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03082" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03131" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03138" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03158" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03160" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03167" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03175" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03188" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03194" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03196" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03214" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03238" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03275" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03302" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03315" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03322" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03346" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2108.11092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.11567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.02690" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.03208" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.05631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.04370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.11870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.12767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.03068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.05449" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.04751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.14806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07061" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10037" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12453" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03034" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15079" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15909" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15504" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14284" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00096" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.04339" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08637" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09150" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09270" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09272" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01775" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06441" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03035" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14457" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16167" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09007" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15186" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.17581" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00315" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00608" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01204" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01519" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01523" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01843" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01943" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02740" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.02962">
<title>Automated Localization of Blood Vessels in Retinal Images. (arXiv:2401.02962v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.02962</link>
<description rdf:parseType="Literal">&lt;p&gt;Vessel structure is one of the most important parts of the retina which
physicians can detect many diseases by analysing its features. Localization of
blood vessels in retina images is an important process in medical image
analysis. This process is also more challenging with the presence of bright and
dark lesions. In this thesis, two automated vessel localization methods to
handle both healthy and unhealthy (pathological) retina images are analyzed.
Each method consists of two major steps and the second step is the same in the
two methods. In the first step, an algorithm is used to decrease the effect of
bright lesions. In Method 1, this algorithm is based on K- Means segmentation,
and in Method 2, it is based on a regularization procedure. In the second step
of both methods, a multi-scale line operator is used to localize the
line-shaped vascular structures and ignore the dark lesions which are generally
assumed to have irregular patterns. After the introduction of the methods, a
detailed quantitative and qualitative comparison of the methods with one
another as well as the state-of-the-art solutions in the literature based on
the segmentation results on the images of the two publicly available datasets,
DRIVE and STARE, is reported. The results demonstrate that the methods are
highly comparable with other solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Safarzadeh_V/0/1/0/all/0/1&quot;&gt;Vahid Mohammadi Safarzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02971">
<title>Deep Anomaly Detection in Text. (arXiv:2401.02971v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.02971</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep anomaly detection methods have become increasingly popular in recent
years, with methods like Stacked Autoencoders, Variational Autoencoders, and
Generative Adversarial Networks greatly improving the state-of-the-art. Other
methods rely on augmenting classical models (such as the One-Class Support
Vector Machine), by learning an appropriate kernel function using Neural
Networks. Recent developments in representation learning by self-supervision
are proving to be very beneficial in the context of anomaly detection. Inspired
by the advancements in anomaly detection using self-supervised learning in the
field of computer vision, this thesis aims to develop a method for detecting
anomalies by exploiting pretext tasks tailored for text corpora. This approach
greatly improves the state-of-the-art on two datasets, 20Newsgroups, and AG
News, for both semi-supervised and unsupervised anomaly detection, thus proving
the potential for self-supervised anomaly detectors in the field of natural
language processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manolache_A/0/1/0/all/0/1&quot;&gt;Andrei Manolache&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02974">
<title>Efficacy of Utilizing Large Language Models to Detect Public Threat Posted Online. (arXiv:2401.02974v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.02974</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper examines the efficacy of utilizing large language models (LLMs) to
detect public threats posted online. Amid rising concerns over the spread of
threatening rhetoric and advance notices of violence, automated content
analysis techniques may aid in early identification and moderation. Custom data
collection tools were developed to amass post titles from a popular Korean
online community, comprising 500 non-threat examples and 20 threats. Various
LLMs (GPT-3.5, GPT-4, PaLM) were prompted to classify individual posts as
either &quot;threat&quot; or &quot;safe.&quot; Statistical analysis found all models demonstrated
strong accuracy, passing chi-square goodness of fit tests for both threat and
non-threat identification. GPT-4 performed best overall with 97.9% non-threat
and 100% threat accuracy. Affordability analysis also showed PaLM API pricing
as highly cost-efficient. The findings indicate LLMs can effectively augment
human content moderation at scale to help mitigate emerging online risks.
However, biases, transparency, and ethical oversight remain vital
considerations before real-world implementation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_T/0/1/0/all/0/1&quot;&gt;Taeksoo Kwon&lt;/a&gt; (Algorix Convergence Research Office), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1&quot;&gt;Connor Kim&lt;/a&gt; (Centennial High School)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02976">
<title>Trace and Edit Relation Associations in GPT. (arXiv:2401.02976v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.02976</link>
<description rdf:parseType="Literal">&lt;p&gt;This study introduces a novel approach for analyzing and modifying entity
relationships in GPT models, diverging from ROME&apos;s entity-focused methods. We
develop a relation tracing technique to understand the influence of language
model computations on relationship judgments. Using the FewRel dataset, we
identify key roles of MLP modules and attention mechanisms in processing
relationship information. Our method, tested against ROME on a new dataset,
shows improved balance in specificity and generalization, underscoring the
potential of manipulating early-layer modules for enhanced model understanding
and accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiahang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Taoyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuanli Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02978">
<title>Learning from a Generative AI Predecessor -- The Many Motivations for Interacting with Conversational Agents. (arXiv:2401.02978v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.02978</link>
<description rdf:parseType="Literal">&lt;p&gt;For generative AI to succeed, how engaging a conversationalist must it be?
For almost sixty years, some conversational agents have responded to any
question or comment to keep a conversation going. In recent years, several
utilized machine learning or sophisticated language processing, such as Tay,
Xiaoice, Zo, Hugging Face, Kuki, and Replika. Unlike generative AI, they
focused on engagement, not expertise. Millions of people were motivated to
engage with them. What were the attractions? Will generative AI do better if it
is equally engaging, or should it be less engaging? Prior to the emergence of
generative AI, we conducted a large-scale quantitative and qualitative analysis
to learn what motivated millions of people to engage with one such &apos;virtual
companion,&apos; Microsoft&apos;s Zo. We examined the complete chat logs of 2000
anonymized people. We identified over a dozen motivations that people had for
interacting with this software. Designers learned different ways to increase
engagement. Generative conversational AI does not yet have a clear revenue
model to address its high cost. It might benefit from being more engaging, even
as it supports productivity and creativity. Our study and analysis point to
opportunities and challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brinkman_D/0/1/0/all/0/1&quot;&gt;Donald Brinkman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grudin_J/0/1/0/all/0/1&quot;&gt;Jonathan Grudin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02979">
<title>Are we describing the same sound? An analysis of word embedding spaces of expressive piano performance. (arXiv:2401.02979v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.02979</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic embeddings play a crucial role in natural language-based information
retrieval. Embedding models represent words and contexts as vectors whose
spatial configuration is derived from the distribution of words in large text
corpora. While such representations are generally very powerful, they might
fail to account for fine-grained domain-specific nuances. In this article, we
investigate this uncertainty for the domain of characterizations of expressive
piano performance. Using a music research dataset of free text performance
characterizations and a follow-up study sorting the annotations into clusters,
we derive a ground truth for a domain-specific semantic similarity structure.
We test five embedding models and their similarity structure for correspondence
with the ground truth. We further assess the effects of contextualizing
prompts, hubness reduction, cross-modal similarity, and k-means clustering. The
quality of embedding models shows great variability with respect to this task;
more general models perform better than domain-adapted ones and the best model
configurations reach human-level agreement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peter_S/0/1/0/all/0/1&quot;&gt;Silvan David Peter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1&quot;&gt;Shreyan Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cancino_Chacon_C/0/1/0/all/0/1&quot;&gt;Carlos Eduardo Cancino-Chac&amp;#xf3;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Widmer_G/0/1/0/all/0/1&quot;&gt;Gerhard Widmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02981">
<title>Fine-tuning and Utilization Methods of Domain-specific LLMs. (arXiv:2401.02981v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.02981</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent releases of pre-trained Large Language Models (LLMs) have gained
considerable traction, yet research on fine-tuning and employing
domain-specific LLMs remains scarce. This study investigates approaches for
fine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs,
foundational models, and methods for domain-specific pre-training. Focusing on
the financial sector, it details dataset selection, preprocessing, model
choice, and considerations crucial for LLM fine-tuning in finance. Addressing
the unique characteristics of financial data, the study explores the
construction of domain-specific vocabularies and considerations for security
and regulatory compliance. In the practical application of LLM fine-tuning, the
study outlines the procedure and implementation for generating domain-specific
LLMs in finance. Various financial cases, including stock price prediction,
sentiment analysis of financial news, automated document processing, research,
information extraction, and customer service enhancement, are exemplified. The
study explores the potential of LLMs in the financial domain, identifies
limitations, and proposes directions for improvement, contributing valuable
insights for future research. Ultimately, it advances natural language
processing technology in business, suggesting proactive LLM utilization in
financial services across industries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_C/0/1/0/all/0/1&quot;&gt;Cheonsu Jeong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02982">
<title>BIBench: Benchmarking Data Analysis Knowledge of Large Language Models. (arXiv:2401.02982v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.02982</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have demonstrated impressive capabilities across
a wide range of tasks. However, their proficiency and reliability in the
specialized domain of Data Analysis, particularly with a focus on data-driven
thinking, remain uncertain. To bridge this gap, we introduce BIBench, a
comprehensive benchmark designed to evaluate the data analysis capabilities of
LLMs within the context of Business Intelligence (BI). BIBench assesses LLMs
across three dimensions: 1) BI foundational knowledge, evaluating the models&apos;
numerical reasoning and familiarity with financial concepts; 2) BI knowledge
application, determining the models&apos; ability to quickly comprehend textual
information and generate analysis questions from multiple views; and 3) BI
technical skills, examining the models&apos; use of technical knowledge to address
real-world data analysis challenges. BIBench comprises 11 sub-tasks, spanning
three categories of task types: classification, extraction, and generation.
Additionally, we&apos;ve developed BIChat, a domain-specific dataset with over a
million data points, to fine-tune LLMs. We will release BIBenchmark, BIChat,
and the evaluation scripts at \url{https://github.com/cubenlp/BIBench}. This
benchmark aims to provide a measure for in-depth analysis of LLM abilities and
foster the advancement of LLMs in the field of data analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shangqing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1&quot;&gt;Chenghao Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1&quot;&gt;Xinlin Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_Z/0/1/0/all/0/1&quot;&gt;Zhaoguang Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1&quot;&gt;Man Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02984">
<title>Large Language Models in Mental Health Care: a Scoping Review. (arXiv:2401.02984v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.02984</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: The growing use of large language models (LLMs) stimulates a need
for a comprehensive review of their applications and outcomes in mental health
care contexts. This scoping review aims to critically analyze the existing
development and applications of LLMs in mental health care, highlighting their
successes and identifying their challenges and limitations in these specialized
fields. Materials and Methods: A broad literature search was conducted in
November 2023 using six databases (PubMed, Web of Science, Google Scholar,
arXiv, medRxiv, and PsyArXiv) following the 2020 version of the Preferred
Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. A
total of 313 publications were initially identified, and after applying the
study inclusion criteria, 34 publications were selected for the final review.
Results: We identified diverse applications of LLMs in mental health care,
including diagnosis, therapy, patient engagement enhancement, etc. Key
challenges include data availability and reliability, nuanced handling of
mental states, and effective evaluation methods. Despite successes in accuracy
and accessibility improvement, gaps in clinical applicability and ethical
considerations were evident, pointing to the need for robust data, standardized
evaluations, and interdisciplinary collaboration. Conclusion: LLMs show
promising potential in advancing mental health care, with applications in
diagnostics, and patient support. Continued advancements depend on
collaborative, multidisciplinary efforts focused on framework enhancement,
rigorous dataset development, technological refinement, and ethical integration
to ensure the effective and safe application of LLMs in mental health care.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1&quot;&gt;Yining Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fenglin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kailai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zehan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheu_Y/0/1/0/all/0/1&quot;&gt;Yi-han Sheu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Peilin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moran_L/0/1/0/all/0/1&quot;&gt;Lauren V. Moran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1&quot;&gt;Sophia Ananiadou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beam_A/0/1/0/all/0/1&quot;&gt;Andrew Beam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02985">
<title>Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education. (arXiv:2401.02985v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.02985</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid evolution of artificial intelligence (AI), especially in the domain
of Large Language Models (LLMs) and generative AI, has opened new avenues for
application across various fields, yet its role in business education remains
underexplored. This study introduces the first benchmark to assess the
performance of seven major LLMs, OpenAI&apos;s models (GPT-3.5 Turbo, GPT-4, and
GPT-4 Turbo), Google&apos;s models (PaLM 2, Gemini 1.0 Pro), and Anthropic&apos;s models
(Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission
process for graduate business programs. Our analysis shows that most LLMs
outperform human candidates, with GPT-4 Turbo not only outperforming the other
models but also surpassing the average scores of graduate students at top
business schools. Through a case study, this research examines GPT-4 Turbo&apos;s
ability to explain answers, evaluate responses, identify errors, tailor
instructions, and generate alternative scenarios. The latest LLM versions,
GPT-4 Turbo, Claude 2.1, and Gemini 1.0 Pro, show marked improvements in
reasoning tasks compared to their predecessors, underscoring their potential
for complex problem-solving. While AI&apos;s promise in education, assessment, and
tutoring is clear, challenges remain. Our study not only sheds light on LLMs&apos;
academic potential but also emphasizes the need for careful development and
application of AI in education. As AI technology advances, it is imperative to
establish frameworks and protocols for AI interaction, verify the accuracy of
AI-generated content, ensure worldwide access for diverse learners, and create
an educational environment where AI supports human expertise. This research
sets the stage for further exploration into the responsible use of AI to enrich
educational experiences and improve exam preparation and assessment methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashrafimoghari_V/0/1/0/all/0/1&quot;&gt;Vahid Ashrafimoghari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurkan_N/0/1/0/all/0/1&quot;&gt;Necdet G&amp;#xfc;rkan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suchow_J/0/1/0/all/0/1&quot;&gt;Jordan W. Suchow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02986">
<title>Identification of Regulatory Requirements Relevant to Business Processes: A Comparative Study on Generative AI, Embedding-based Ranking, Crowd and Expert-driven Methods. (arXiv:2401.02986v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.02986</link>
<description rdf:parseType="Literal">&lt;p&gt;Organizations face the challenge of ensuring compliance with an increasing
amount of requirements from various regulatory documents. Which requirements
are relevant depends on aspects such as the geographic location of the
organization, its domain, size, and business processes. Considering these
contextual factors, as a first step, relevant documents (e.g., laws,
regulations, directives, policies) are identified, followed by a more detailed
analysis of which parts of the identified documents are relevant for which step
of a given business process. Nowadays the identification of regulatory
requirements relevant to business processes is mostly done manually by domain
and legal experts, posing a tremendous effort on them, especially for a large
number of regulatory documents which might frequently change. Hence, this work
examines how legal and domain experts can be assisted in the assessment of
relevant requirements. For this, we compare an embedding-based NLP ranking
method, a generative AI method using GPT-4, and a crowdsourced method with the
purely manual method of creating relevancy labels by experts. The proposed
methods are evaluated based on two case studies: an Australian insurance case
created with domain experts and a global banking use case, adapted from SAP
Signavio&apos;s workflow example of an international guideline. A gold standard is
created for both BPMN2.0 processes and matched to real-world textual
requirements from multiple regulatory documents. The evaluation and discussion
provide insights into strengths and weaknesses of each method regarding
applicability, automation, transparency, and reproducibility and provide
guidelines on which method combinations will maximize benefits for given
characteristics such as process usage, impact, and dynamics of an application
scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sai_C/0/1/0/all/0/1&quot;&gt;Catherine Sai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadiq_S/0/1/0/all/0/1&quot;&gt;Shazia Sadiq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Lei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demartini_G/0/1/0/all/0/1&quot;&gt;Gianluca Demartini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rinderle_Ma_S/0/1/0/all/0/1&quot;&gt;Stefanie Rinderle-Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02987">
<title>Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach. (arXiv:2401.02987v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.02987</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of pretrained models has significantly impacted from Natural
Language Processing (NLP) and Computer Vision to relational datasets.
Traditionally, these models are assessed through fine-tuned downstream tasks.
However, this raises the question of how to evaluate these models more
efficiently and more effectively. In this study, we explore a novel approach
where we leverage the meta features associated with each entity as a source of
worldly knowledge and employ entity representations from the models. We propose
using the consistency between these representations and the meta features as a
metric for evaluating pretrained models. Our method&apos;s effectiveness is
demonstrated across various domains, including models with relational datasets,
large language models and images models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aboagye_P/0/1/0/all/0/1&quot;&gt;Prince Aboagye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saini_U/0/1/0/all/0/1&quot;&gt;Uday Singh Saini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xin Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_M/0/1/0/all/0/1&quot;&gt;Michael Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yujie Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1&quot;&gt;Zhongfang Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Shubham Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02991">
<title>GLIDE-RL: Grounded Language Instruction through DEmonstration in RL. (arXiv:2401.02991v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.02991</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the final frontiers in the development of complex human - AI
collaborative systems is the ability of AI agents to comprehend the natural
language and perform tasks accordingly. However, training efficient
Reinforcement Learning (RL) agents grounded in natural language has been a
long-standing challenge due to the complexity and ambiguity of the language and
sparsity of the rewards, among other factors. Several advances in reinforcement
learning, curriculum learning, continual learning, language models have
independently contributed to effective training of grounded agents in various
environments. Leveraging these developments, we present a novel algorithm,
Grounded Language Instruction through DEmonstration in RL (GLIDE-RL) that
introduces a teacher-instructor-student curriculum learning framework for
training an RL agent capable of following natural language instructions that
can generalize to previously unseen language instructions. In this multi-agent
framework, the teacher and the student agents learn simultaneously based on the
student&apos;s current skill level. We further demonstrate the necessity for
training the student agent with not just one, but multiple teacher agents.
Experiments on a complex sparse reward environment validates the effectiveness
of our proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kharyal_C/0/1/0/all/0/1&quot;&gt;Chaitanya Kharyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottipati_S/0/1/0/all/0/1&quot;&gt;Sai Krishna Gottipati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_T/0/1/0/all/0/1&quot;&gt;Tanmay Kumar Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1&quot;&gt;Srijita Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1&quot;&gt;Matthew E. Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02992">
<title>Advanced Unstructured Data Processing for ESG Reports: A Methodology for Structured Transformation and Enhanced Analysis. (arXiv:2401.02992v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.02992</link>
<description rdf:parseType="Literal">&lt;p&gt;In the evolving field of corporate sustainability, analyzing unstructured
Environmental, Social, and Governance (ESG) reports is a complex challenge due
to their varied formats and intricate content. This study introduces an
innovative methodology utilizing the &quot;Unstructured Core Library&quot;, specifically
tailored to address these challenges by transforming ESG reports into
structured, analyzable formats. Our approach significantly advances the
existing research by offering high-precision text cleaning, adept
identification and extraction of text from images, and standardization of
tables within these reports. Emphasizing its capability to handle diverse data
types, including text, images, and tables, the method adeptly manages the
nuances of differing page layouts and report styles across industries. This
research marks a substantial contribution to the fields of industrial ecology
and corporate sustainability assessment, paving the way for the application of
advanced NLP technologies and large language models in the analysis of
corporate governance and sustainability. Our code is available at
https://github.com/linancn/TianGong-AI-Unstructure.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jiahui Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jing Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1&quot;&gt;Xin Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jing Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1&quot;&gt;Jianchuan Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruiqiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1&quot;&gt;Nan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Ming Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02993">
<title>Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion. (arXiv:2401.02993v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.02993</link>
<description rdf:parseType="Literal">&lt;p&gt;Retrieval-based augmentations that aim to incorporate knowledge from an
external database into language models have achieved great success in various
knowledge-intensive (KI) tasks, such as question-answering and text generation.
However, integrating retrievals in non-knowledge-intensive (NKI) tasks, such as
text classification, is still challenging. Existing works focus on
concatenating retrievals to inputs as context to form the prompt-based inputs.
Unfortunately, such methods require language models to have the capability to
handle long texts. Besides, inferring such concatenated data would also consume
a significant amount of computational resources.
&lt;/p&gt;
&lt;p&gt;To solve these challenges, we propose \textbf{ReFusion} in this paper, a
computation-efficient \textbf{Re}trieval representation \textbf{Fusion} with
neural architecture search. The main idea is to directly fuse the retrieval
representations into the language models. Specifically, we first propose an
online retrieval module that retrieves representations of similar sentences.
Then, we present a retrieval fusion module including two effective ranking
schemes, i.e., reranker-based scheme and ordered-mask-based scheme, to fuse the
retrieval representations with hidden states. Furthermore, we use Neural
Architecture Search (NAS) to seek the optimal fusion structure across different
layers. Finally, we conduct comprehensive experiments, and the results
demonstrate our ReFusion can achieve superior and robust performance on various
NKI tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shangyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Ying Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yufei Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1&quot;&gt;Buzhou Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuo_T/0/1/0/all/0/1&quot;&gt;Tei-Wei Kuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1&quot;&gt;Chun Jason Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02994">
<title>Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM. (arXiv:2401.02994v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.02994</link>
<description rdf:parseType="Literal">&lt;p&gt;In conversational AI research, there&apos;s a noticeable trend towards developing
models with a larger number of parameters, exemplified by models like ChatGPT.
While these expansive models tend to generate increasingly better chat
responses, they demand significant computational resources and memory. This
study explores a pertinent question: Can a combination of smaller models
collaboratively achieve comparable or enhanced performance relative to a
singular large model? We introduce an approach termed &quot;blending&quot;, a
straightforward yet effective method of integrating multiple chat AIs. Our
empirical evidence suggests that when specific smaller models are
synergistically blended, they can potentially outperform or match the
capabilities of much larger counterparts. For instance, integrating just three
models of moderate size (6B/13B paramaeters) can rival or even surpass the
performance metrics of a substantially larger model like ChatGPT (175B+
paramaters). This hypothesis is rigorously tested using A/B testing
methodologies with a large user base on the Chai research platform over a span
of thirty days. The findings underscore the potential of the &quot;blending&quot;
strategy as a viable approach for enhancing chat AI efficacy without a
corresponding surge in computational demands.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiaoding Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liusie_A/0/1/0/all/0/1&quot;&gt;Adian Liusie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1&quot;&gt;Vyas Raina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuwen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beauchamp_W/0/1/0/all/0/1&quot;&gt;William Beauchamp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02995">
<title>CANAMRF: An Attention-Based Model for Multimodal Depression Detection. (arXiv:2401.02995v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.02995</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal depression detection is an important research topic that aims to
predict human mental states using multimodal data. Previous methods treat
different modalities equally and fuse each modality by na\&quot;ive mathematical
operations without measuring the relative importance between them, which cannot
obtain well-performed multimodal representations for downstream depression
tasks. In order to tackle the aforementioned concern, we present a Cross-modal
Attention Network with Adaptive Multi-modal Recurrent Fusion (CANAMRF) for
multimodal depression detection. CANAMRF is constructed by a multimodal feature
extractor, an Adaptive Multimodal Recurrent Fusion module, and a Hybrid
Attention Module. Through experimentation on two benchmark datasets, CANAMRF
demonstrates state-of-the-art performance, underscoring the effectiveness of
our proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yuntao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02997">
<title>Blar-SQL: Faster, Stronger, Smaller NL2SQL. (arXiv:2401.02997v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.02997</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have gained considerable notoriety in the field
of natural language to SQL tasks (NL2SQL). In this study, we show how task
decomposition can greatly benefit LLMs in database understanding and query
generation in order to answer human questions with an SQL query.
&lt;/p&gt;
&lt;p&gt;We fined-tuned open source models, specifically Llama-2 and Code Llama, by
combining 2 different models each designated to focus on one of two tasks in
order to leverage each model&apos;s core competency to further increase the accuracy
of the final SQL query.
&lt;/p&gt;
&lt;p&gt;We propose a new framework to divide the schema into chunks in order to fit
more information into a limited context. Our results are comparable with those
obtained by GPT-4 at the same time being 135 times smaller, 90 times faster and
more than 100 times cheaper than GPT-4.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dominguez_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Manuel Dom&amp;#xed;nguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Errazuriz_B/0/1/0/all/0/1&quot;&gt;Benjam&amp;#xed;n Err&amp;#xe1;zuriz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daher_P/0/1/0/all/0/1&quot;&gt;Patricio Daher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03000">
<title>Bridging Modalities: Knowledge Distillation and Masked Training for Translating Multi-Modal Emotion Recognition to Uni-Modal, Speech-Only Emotion Recognition. (arXiv:2401.03000v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.03000</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an innovative approach to address the challenges of
translating multi-modal emotion recognition models to a more practical and
resource-efficient uni-modal counterpart, specifically focusing on speech-only
emotion recognition. Recognizing emotions from speech signals is a critical
task with applications in human-computer interaction, affective computing, and
mental health assessment. However, existing state-of-the-art models often rely
on multi-modal inputs, incorporating information from multiple sources such as
facial expressions and gestures, which may not be readily available or feasible
in real-world scenarios. To tackle this issue, we propose a novel framework
that leverages knowledge distillation and masked training techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muaz_M/0/1/0/all/0/1&quot;&gt;Muhammad Muaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paull_N/0/1/0/all/0/1&quot;&gt;Nathan Paull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malagavalli_J/0/1/0/all/0/1&quot;&gt;Jahnavi Malagavalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03006">
<title>The Rise of Diffusion Models in Time-Series Forecasting. (arXiv:2401.03006v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03006</link>
<description rdf:parseType="Literal">&lt;p&gt;This survey delves into the application of diffusion models in time-series
forecasting. Diffusion models are demonstrating state-of-the-art results in
various fields of generative AI. The paper includes comprehensive background
information on diffusion models, detailing their conditioning methods and
reviewing their use in time-series forecasting. The analysis covers 11 specific
time-series implementations, the intuition and theory behind them, the
effectiveness on different datasets, and a comparison among each other. Key
contributions of this work are the thorough exploration of diffusion models&apos;
applications in time-series forecasting and a chronologically ordered overview
of these models. Additionally, the paper offers an insightful discussion on the
current state-of-the-art in this domain and outlines potential future research
directions. This serves as a valuable resource for researchers in AI and
time-series analysis, offering a clear view of the latest advancements and
future potential of diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meijer_C/0/1/0/all/0/1&quot;&gt;Caspar Meijer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lydia Y. Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03040">
<title>AccidentGPT: Large Multi-Modal Foundation Model for Traffic Accident Analysis. (arXiv:2401.03040v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03040</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic accident analysis is pivotal for enhancing public safety and
developing road regulations. Traditional approaches, although widely used, are
often constrained by manual analysis processes, subjective decisions, uni-modal
outputs, as well as privacy issues related to sensitive data. This paper
introduces the idea of AccidentGPT, a foundation model of traffic accident
analysis, which incorporates multi-modal input data to automatically
reconstruct the accident process video with dynamics details, and furthermore
provide multi-task analysis with multi-modal outputs. The design of the
AccidentGPT is empowered with a multi-modality prompt with feedback for
task-oriented adaptability, a hybrid training schema to leverage labelled and
unlabelled data, and a edge-cloud split configuration for data privacy. To
fully realize the functionalities of this model, we proposes several research
opportunities. This paper serves as the stepping stone to fill the gaps in
traditional approaches of traffic accident analysis and attract the research
community attention for automatic, objective, and privacy-preserving traffic
accident analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1&quot;&gt;Kebin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xiaofei Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03059">
<title>Reliability-Optimized User Admission Control for URLLC Traffic: A Neural Contextual Bandit Approach. (arXiv:2401.03059v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03059</link>
<description rdf:parseType="Literal">&lt;p&gt;Ultra-reliable low-latency communication (URLLC) is the cornerstone for a
broad range of emerging services in next-generation wireless networks. URLLC
fundamentally relies on the network&apos;s ability to proactively determine whether
sufficient resources are available to support the URLLC traffic, and thus,
prevent so-called cell overloads. Nonetheless, achieving accurate
quality-of-service (QoS) predictions for URLLC user equipment (UEs) and
preventing cell overloads are very challenging tasks. This is due to dependency
of the QoS metrics (latency and reliability) on traffic and channel statistics,
users&apos; mobility, and interdependent performance across UEs. In this paper, a
new QoS-aware UE admission control approach is developed to proactively
estimate QoS for URLLC UEs, prior to associating them with a cell, and
accordingly, admit only a subset of UEs that do not lead to a cell overload. To
this end, an optimization problem is formulated to find an efficient UE
admission control policy, cognizant of UEs&apos; QoS requirements and cell-level
load dynamics. To solve this problem, a new machine learning based method is
proposed that builds on (deep) neural contextual bandits, a suitable framework
for dealing with nonlinear bandit problems. In fact, the UE admission
controller is treated as a bandit agent that observes a set of network
measurements (context) and makes admission control decisions based on
context-dependent QoS (reward) predictions. The simulation results show that
the proposed scheme can achieve near-optimal performance and yield substantial
gains in terms of cell-level service reliability and efficient resource
utilization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Semiari_O/0/1/0/all/0/1&quot;&gt;Omid Semiari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikopour_H/0/1/0/all/0/1&quot;&gt;Hosein Nikopour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talwar_S/0/1/0/all/0/1&quot;&gt;Shilpa Talwar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03065">
<title>CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution. (arXiv:2401.03065v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.03065</link>
<description rdf:parseType="Literal">&lt;p&gt;We present CRUXEval (Code Reasoning, Understanding, and eXecution
Evaluation), a benchmark consisting of 800 Python functions (3-13 lines). Each
function comes with an input-output pair, leading to two natural tasks: input
prediction and output prediction. First, we propose a generic recipe for
generating our execution benchmark which can be used to create future variation
of the benchmark. Second, we evaluate twenty code models on our benchmark and
discover that many recent high-scoring models on HumanEval do not show the same
improvements on our benchmark. Third, we show that simple CoT and fine-tuning
schemes can improve performance on our benchmark but remain far from solving
it. The best setup, GPT-4 with chain of thought (CoT), achieves a pass@1 of 75%
and 81% on input and output prediction, respectively. In contrast, Code Llama
34B achieves a pass@1 of 50% and 46% on input and output prediction,
highlighting the gap between open and closed source models. As no model is
close to acing CRUXEval, we provide examples of consistent GPT-4 failures on
simple programs as a lens into its code reasoning capabilities and areas for
improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1&quot;&gt;Alex Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roziere_B/0/1/0/all/0/1&quot;&gt;Baptiste Rozi&amp;#xe8;re&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leather_H/0/1/0/all/0/1&quot;&gt;Hugh Leather&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solar_Lezama_A/0/1/0/all/0/1&quot;&gt;Armando Solar-Lezama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1&quot;&gt;Gabriel Synnaeve&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sida I. Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03082">
<title>UMIE: Unified Multimodal Information Extraction with Instruction Tuning. (arXiv:2401.03082v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.03082</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal information extraction (MIE) gains significant attention as the
popularity of multimedia content increases. However, current MIE methods often
resort to using task-specific model structures, which results in limited
generalizability across tasks and underutilizes shared knowledge across MIE
tasks. To address these issues, we propose UMIE, a unified multimodal
information extractor to unify three MIE tasks as a generation problem using
instruction tuning, being able to effectively extract both textual and visual
mentions. Extensive experiments show that our single UMIE outperforms various
state-of-the-art (SoTA) methods across six MIE datasets on three tasks.
Furthermore, in-depth analysis demonstrates UMIE&apos;s strong generalization in the
zero-shot setting, robustness to instruction variants, and interpretability.
Our research serves as an initial step towards a unified MIE model and
initiates the exploration into both instruction tuning and large language
models within the MIE domain. Our code, data, and model are available at
https://github.com/ZUCC-AI/UMIE
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qingyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_R/0/1/0/all/0/1&quot;&gt;Renze Lou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03093">
<title>A white box solution to the black box problem of AI. (arXiv:2401.03093v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.03093</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence based on neural networks has made significant
progress. However, there are concerns about the reliability and security of
this approach due to its lack of transparency. This is the black box problem of
AI. Here we show how this problem can be solved using symbolic AI, which has a
transparent white box nature. The widespread use of symbolic AI is hindered by
the opacity of mathematical models and natural language terms, the lack of a
unified ontology, and the combinatorial explosion of search options. To solve
the AI black box problem and to implement general-purpose symbolic AI, we
propose to use deterministic logic cellular automata with rules based on first
principles of the general theory of the relevant domain. In this case, the
general theory of the relevant domain plays the role of a knowledge base for
the cellular automaton inference. A cellular automaton implements automatic
parallel logical inference at three levels of organization of a complex system.
Our verification of several ecological hypotheses provides a successful
precedent for the implementation of white-box AI. Finally, we discuss a program
for creating a general-purpose symbolic AI capable of processing knowledge and
ensuring the reliability and safety of automated decisions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalmykov_V/0/1/0/all/0/1&quot;&gt;V. L. Kalmykov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalmykov_L/0/1/0/all/0/1&quot;&gt;L.V. Kalmykov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03128">
<title>Manifold-based Shapley for SAR Recognization Network Explanation. (arXiv:2401.03128v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.03128</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainable artificial intelligence (XAI) holds immense significance in
enhancing the deep neural network&apos;s transparency and credibility, particularly
in some risky and high-cost scenarios, like synthetic aperture radar (SAR).
Shapley is a game-based explanation technique with robust mathematical
foundations. However, Shapley assumes that model&apos;s features are independent,
rendering Shapley explanation invalid for high dimensional models. This study
introduces a manifold-based Shapley method by projecting high-dimensional
features into low-dimensional manifold features and subsequently obtaining
Fusion-Shap, which aims at (1) addressing the issue of erroneous explanations
encountered by traditional Shap; (2) resolving the challenge of
interpretability that traditional Shap faces in complex scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xuran Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Mingzhe Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuanjing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zhenpeng Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stankovic_L/0/1/0/all/0/1&quot;&gt;LJubisa Stankovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03131">
<title>A Physics-guided Generative AI Toolkit for Geophysical Monitoring. (arXiv:2401.03131v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03131</link>
<description rdf:parseType="Literal">&lt;p&gt;Full-waveform inversion (FWI) plays a vital role in geoscience to explore the
subsurface. It utilizes the seismic wave to image the subsurface velocity map.
As the machine learning (ML) technique evolves, the data-driven approaches
using ML for FWI tasks have emerged, offering enhanced accuracy and reduced
computational cost compared to traditional physics-based methods. However, a
common challenge in geoscience, the unprivileged data, severely limits ML
effectiveness. The issue becomes even worse during model pruning, a step
essential in geoscience due to environmental complexities. To tackle this, we
introduce the EdGeo toolkit, which employs a diffusion-based model guided by
physics principles to generate high-fidelity velocity maps. The toolkit uses
the acoustic wave equation to generate corresponding seismic waveform data,
facilitating the fine-tuning of pruned ML models. Our results demonstrate
significant improvements in SSIM scores and reduction in both MAE and MSE
across various pruning ratios. Notably, the ML model fine-tuned using data
generated by EdGeo yields superior quality of velocity maps, especially in
representing unprivileged features, outperforming other existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Junhuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanchen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1&quot;&gt;Yi Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Youzuo Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03134">
<title>TimeGraphs: Graph-based Temporal Reasoning. (arXiv:2401.03134v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03134</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real-world systems exhibit temporal, dynamic behaviors, which are
captured as time series of complex agent interactions. To perform temporal
reasoning, current methods primarily encode temporal dynamics through simple
sequence-based models. However, in general these models fail to efficiently
capture the full spectrum of rich dynamics in the input, since the dynamics is
not uniformly distributed. In particular, relevant information might be harder
to extract and computing power is wasted for processing all individual
timesteps, even if they contain no significant changes or no new information.
Here we propose TimeGraphs, a novel approach that characterizes dynamic
interactions as a hierarchical temporal graph, diverging from traditional
sequential representations. Our approach models the interactions using a
compact graph-based representation, enabling adaptive reasoning across diverse
time scales. Adopting a self-supervised method, TimeGraphs constructs a
multi-level event hierarchy from a temporal input, which is then used to
efficiently reason about the unevenly distributed dynamics. This construction
process is scalable and incremental to accommodate streaming data. We evaluate
TimeGraphs on multiple datasets with complex, dynamic agent interactions,
including a football simulator, the Resistance game, and the MOMA human
activity dataset. The results demonstrate both robustness and efficiency of
TimeGraphs on a range of temporal reasoning tasks. Our approach obtains
state-of-the-art performance and leads to a performance increase of up to 12.2%
on event prediction and recognition tasks over current approaches. Our
experiments further demonstrate a wide array of capabilities including
zero-shot generalization, robustness in case of data sparsity, and adaptability
to streaming data flow.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maheshwari_P/0/1/0/all/0/1&quot;&gt;Paridhi Maheshwari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Hongyu Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sosic_R/0/1/0/all/0/1&quot;&gt;Rok Sosic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1&quot;&gt;Jure Leskovec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03137">
<title>SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning. (arXiv:2401.03137v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03137</link>
<description rdf:parseType="Literal">&lt;p&gt;Alleviating overestimation bias is a critical challenge for deep
reinforcement learning to achieve successful performance on more complex tasks
or offline datasets containing out-of-distribution data. In order to overcome
overestimation bias, ensemble methods for Q-learning have been investigated to
exploit the diversity of multiple Q-functions. Since network initialization has
been the predominant approach to promote diversity in Q-functions,
heuristically designed diversity injection methods have been studied in the
literature. However, previous studies have not attempted to approach guaranteed
independence over an ensemble from a theoretical perspective. By introducing a
novel regularization loss for Q-ensemble independence based on random matrix
theory, we propose spiked Wishart Q-ensemble independence regularization (SPQR)
for reinforcement learning. Specifically, we modify the intractable hypothesis
testing criterion for the Q-ensemble independence into a tractable KL
divergence between the spectral distribution of the Q-ensemble and the target
Wigner&apos;s semicircle distribution. We implement SPQR in several online and
offline ensemble Q-learning algorithms. In the experiments, SPQR outperforms
the baseline algorithms in both online and offline RL benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dohyeok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Seungyub Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_T/0/1/0/all/0/1&quot;&gt;Taehyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jungwoo Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03138">
<title>TelTrans: Applying Multi-Type Telecom Data to Transportation Evaluation and Prediction via Multifaceted Graph Modeling. (arXiv:2401.03138v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03138</link>
<description rdf:parseType="Literal">&lt;p&gt;To address the limitations of traffic prediction from location-bound
detectors, we present Geographical Cellular Traffic (GCT) flow, a novel data
source that leverages the extensive coverage of cellular traffic to capture
mobility patterns. Our extensive analysis validates its potential for
transportation. Focusing on vehicle-related GCT flow prediction, we propose a
graph neural network that integrates multivariate, temporal, and spatial facets
for improved accuracy. Experiments reveal our model&apos;s superiority over
baselines, especially in long-term predictions. We also highlight the potential
for GCT flow integration into transportation systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;ChungYi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tung_S/0/1/0/all/0/1&quot;&gt;Shen-Lung Tung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hung-Ting Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1&quot;&gt;Winston H. Hsu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03154">
<title>Decentralized Multi-Agent Active Search and Tracking when Targets Outnumber Agents. (arXiv:2401.03154v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.03154</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-agent multi-target tracking has a wide range of applications, including
wildlife patrolling, security surveillance or environment monitoring. Such
algorithms often make restrictive assumptions: the number of targets and/or
their initial locations may be assumed known, or agents may be pre-assigned to
monitor disjoint partitions of the environment, reducing the burden of
exploration. This also limits applicability when there are fewer agents than
targets, since agents are unable to continuously follow the targets in their
fields of view. Multi-agent tracking algorithms additionally assume inter-agent
synchronization of observations, or the presence of a central controller to
coordinate joint actions. Instead, we focus on the setting of decentralized
multi-agent, multi-target, simultaneous active search-and-tracking with
asynchronous inter-agent communication. Our proposed algorithm DecSTER uses a
sequential monte carlo implementation of the probability hypothesis density
filter for posterior inference combined with Thompson sampling for
decentralized multi-agent decision making. We compare different action
selection policies, focusing on scenarios where targets outnumber agents. In
simulation, we demonstrate that DecSTER is robust to unreliable inter-agent
communication and outperforms information-greedy baselines in terms of the
Optimal Sub-Pattern Assignment (OSPA) metric for different numbers of targets
and varying teamsizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1&quot;&gt;Arundhati Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jeff Schneider&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03158">
<title>Quartet Logic: A Four-Step Reasoning (QLFR) framework for advancing Short Text Classification. (arXiv:2401.03158v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.03158</link>
<description rdf:parseType="Literal">&lt;p&gt;Short Text Classification (STC) is crucial for processing and comprehending
the brief but substantial content prevalent on contemporary digital platforms.
The STC encounters difficulties in grasping semantic and syntactic intricacies,
an issue that is apparent in traditional pre-trained language models. Although
Graph Convolutional Networks enhance performance by integrating external
knowledge bases, these methods are limited by the quality and extent of the
knowledge applied. Recently, the emergence of Large Language Models (LLMs) and
Chain-of-Thought (CoT) has significantly improved the performance of complex
reasoning tasks. However, some studies have highlighted the limitations of
their application in fundamental NLP tasks. Consequently, this study sought to
employ CoT to investigate the capabilities of LLMs in STC tasks. This study
introduces Quartet Logic: A Four-Step Reasoning (QLFR) framework. This
framework primarily incorporates Syntactic and Semantic Enrichment CoT,
effectively decomposing the STC task into four distinct steps: (i) essential
concept identification, (ii) common-sense knowledge retrieval, (iii) text
rewriting, and (iv) classification. This elicits the inherent knowledge and
abilities of LLMs to address the challenges in STC. Surprisingly, we found that
QLFR can also improve the performance of smaller models. Therefore, we
developed a CoT-Driven Multi-task learning (QLFR-CML) method to facilitate the
knowledge transfer from LLMs to smaller models. Extensive experimentation
across six short-text benchmarks validated the efficacy of the proposed
methods. Notably, QLFR achieved state-of-the-art performance on all datasets,
with significant improvements, particularly on the Ohsumed and TagMyNews
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuanben Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhonghe Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1&quot;&gt;Yingyan Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Siye Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Q/0/1/0/all/0/1&quot;&gt;Qihang Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yunping Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03160">
<title>Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving. (arXiv:2401.03160v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03160</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite significant progress in autonomous vehicles (AVs), the development of
driving policies that ensure both the safety of AVs and traffic flow efficiency
has not yet been fully explored. In this paper, we propose an enhanced
human-in-the-loop reinforcement learning method, termed the Human as AI
mentor-based deep reinforcement learning (HAIM-DRL) framework, which
facilitates safe and efficient autonomous driving in mixed traffic platoon.
Drawing inspiration from the human learning process, we first introduce an
innovative learning paradigm that effectively injects human intelligence into
AI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves
as a mentor to the AI agent. While allowing the agent to sufficiently explore
uncertain environments, the human expert can take control in dangerous
situations and demonstrate correct actions to avoid potential accidents. On the
other hand, the agent could be guided to minimize traffic flow disturbance,
thereby optimizing traffic flow efficiency. In detail, HAIM-DRL leverages data
collected from free exploration and partial human demonstrations as its two
training sources. Remarkably, we circumvent the intricate process of manually
designing reward functions; instead, we directly derive proxy state-action
values from partial human demonstrations to guide the agents&apos; policy learning.
Additionally, we employ a minimal intervention technique to reduce the human
mentor&apos;s cognitive load. Comparative results show that HAIM-DRL outperforms
traditional methods in driving safety, sampling efficiency, mitigation of
traffic flow disturbance, and generalizability to unseen traffic scenarios. The
code and demo videos for this paper can be accessed at:
https://zilin-huang.github.io/HAIM-DRL-website/}{https://zilin-huang.github.io/HAIM-DRL-website/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zilin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1&quot;&gt;Zihao Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chengyuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sikai Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03167">
<title>PosDiffNet: Positional Neural Diffusion for Point Cloud Registration in a Large Field of View with Perturbations. (arXiv:2401.03167v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03167</link>
<description rdf:parseType="Literal">&lt;p&gt;Point cloud registration is a crucial technique in 3D computer vision with a
wide range of applications. However, this task can be challenging, particularly
in large fields of view with dynamic objects, environmental noise, or other
perturbations. To address this challenge, we propose a model called PosDiffNet.
Our approach performs hierarchical registration based on window-level,
patch-level, and point-level correspondence. We leverage a graph neural partial
differential equation (PDE) based on Beltrami flow to obtain high-dimensional
features and position embeddings for point clouds. We incorporate position
embeddings into a Transformer module based on a neural ordinary differential
equation (ODE) to efficiently represent patches within points. We employ the
multi-level correspondence derived from the high feature similarity scores to
facilitate alignment between point clouds. Subsequently, we use registration
methods such as SVD-based algorithms to predict the transformation using
corresponding point pairs. We evaluate PosDiffNet on several 3D point cloud
datasets, verifying that it achieves state-of-the-art (SOTA) performance for
point cloud registration in large fields of view with perturbations. The
implementation code of experiments is available at
https://github.com/AI-IT-AVs/PosDiffNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+She_R/0/1/0/all/0/1&quot;&gt;Rui She&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Q/0/1/0/all/0/1&quot;&gt;Qiyu Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Kai Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tay_W/0/1/0/all/0/1&quot;&gt;Wee Peng Tay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_T/0/1/0/all/0/1&quot;&gt;Tianyu Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jian_X/0/1/0/all/0/1&quot;&gt;Xingchao Jian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03171">
<title>Exploration of Adolescent Depression Risk Prediction Based on Census Surveys and General Life Issues. (arXiv:2401.03171v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03171</link>
<description rdf:parseType="Literal">&lt;p&gt;In contemporary society, the escalating pressures of life and work have
propelled psychological disorders to the forefront of modern health concerns,
an issue that has been further accentuated by the COVID-19 pandemic. The
prevalence of depression among adolescents is steadily increasing, and
traditional diagnostic methods, which rely on scales or interviews, prove
particularly inadequate for detecting depression in young people. Addressing
these challenges, numerous AI-based methods for assisting in the diagnosis of
mental health issues have emerged. However, most of these methods center around
fundamental issues with scales or use multimodal approaches like facial
expression recognition. Diagnosis of depression risk based on everyday habits
and behaviors has been limited to small-scale qualitative studies. Our research
leverages adolescent census data to predict depression risk, focusing on
children&apos;s experiences with depression and their daily life situations. We
introduced a method for managing severely imbalanced high-dimensional data and
an adaptive predictive approach tailored to data structure characteristics.
Furthermore, we proposed a cloud-based architecture for automatic online
learning and data updates. This study utilized publicly available NSCH youth
census data from 2020 to 2022, encompassing nearly 150,000 data entries. We
conducted basic data analyses and predictive experiments, demonstrating
significant performance improvements over standard machine learning and deep
learning algorithms. This affirmed our data processing method&apos;s broad
applicability in handling imbalanced medical data. Diverging from typical
predictive method research, our study presents a comprehensive architectural
solution, considering a wider array of user needs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yufeng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hefeng Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03175">
<title>Part-of-Speech Tagger for Bodo Language using Deep Learning approach. (arXiv:2401.03175v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.03175</link>
<description rdf:parseType="Literal">&lt;p&gt;Language Processing systems such as Part-of-speech tagging, Named entity
recognition, Machine translation, Speech recognition, and Language modeling
(LM) are well-studied in high-resource languages. Nevertheless, research on
these systems for several low-resource languages, including Bodo, Mizo,
Nagamese, and others, is either yet to commence or is in its nascent stages.
Language model plays a vital role in the downstream tasks of modern NLP.
Extensive studies are carried out on LMs for high-resource languages.
Nevertheless, languages such as Bodo, Rabha, and Mising continue to lack
coverage. In this study, we first present BodoBERT, a language model for the
Bodo language. To the best of our knowledge, this work is the first such effort
to develop a language model for Bodo. Secondly, we present an ensemble DL-based
POS tagging model for Bodo. The POS tagging model is based on combinations of
BiLSTM with CRF and stacked embedding of BodoBERT with BytePairEmbeddings. We
cover several language models in the experiment to see how well they work in
POS tagging tasks. The best-performing model achieves an F1 score of 0.8041. A
comparative experiment was also conducted on Assamese POS taggers, considering
that the language is spoken in the same region as Bodo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1&quot;&gt;Dhrubajyoti Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narzary_S/0/1/0/all/0/1&quot;&gt;Sanjib Narzary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nandi_S/0/1/0/all/0/1&quot;&gt;Sukumar Nandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Som_B/0/1/0/all/0/1&quot;&gt;Bidisha Som&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03188">
<title>A Survey on Verification and Validation, Testing and Evaluations of Neurosymbolic Artificial Intelligence. (arXiv:2401.03188v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.03188</link>
<description rdf:parseType="Literal">&lt;p&gt;Neurosymbolic artificial intelligence (AI) is an emerging branch of AI that
combines the strengths of symbolic AI and sub-symbolic AI. A major drawback of
sub-symbolic AI is that it acts as a &quot;black box&quot;, meaning that predictions are
difficult to explain, making the testing &amp;amp; evaluation (T&amp;amp;E) and validation &amp;amp;
verification (V&amp;amp;V) processes of a system that uses sub-symbolic AI a challenge.
Since neurosymbolic AI combines the advantages of both symbolic and
sub-symbolic AI, this survey explores how neurosymbolic applications can ease
the V&amp;amp;V process. This survey considers two taxonomies of neurosymbolic AI,
evaluates them, and analyzes which algorithms are commonly used as the symbolic
and sub-symbolic components in current applications. Additionally, an overview
of current techniques for the T&amp;amp;E and V&amp;amp;V processes of these components is
provided. Furthermore, it is investigated how the symbolic part is used for T&amp;amp;E
and V&amp;amp;V purposes in current neurosymbolic applications. Our research shows that
neurosymbolic AI as great potential to ease the T&amp;amp;E and V&amp;amp;V processes of
sub-symbolic AI by leveraging the possibilities of symbolic AI. Additionally,
the applicability of current T&amp;amp;E and V&amp;amp;V methods to neurosymbolic AI is
assessed, and how different neurosymbolic architectures can impact these
methods is explored. It is found that current T&amp;amp;E and V&amp;amp;V techniques are partly
sufficient to test, evaluate, verify, or validate the symbolic and sub-symbolic
part of neurosymbolic applications independently, while some of them use
approaches where current T&amp;amp;E and V&amp;amp;V methods are not applicable by default, and
adjustments or even new approaches are needed. Our research shows that there is
great potential in using symbolic AI to test, evaluate, verify, or validate the
predictions of a sub-symbolic model, making neurosymbolic AI an interesting
research direction for safe, secure, and trustworthy AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renkhoff_J/0/1/0/all/0/1&quot;&gt;Justus Renkhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1&quot;&gt;Ke Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meier_Doernberg_M/0/1/0/all/0/1&quot;&gt;Marc Meier-Doernberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velasquez_A/0/1/0/all/0/1&quot;&gt;Alvaro Velasquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Houbing Herbert Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03190">
<title>MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model Editing. (arXiv:2401.03190v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.03190</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models are known for encoding a vast amount of factual
knowledge, but they often becomes outdated due to the ever-changing nature of
external information. A promising solution to this challenge is the utilization
of model editing methods to update the knowledge in an efficient manner.
However, the majority of existing model editing techniques are limited to
monolingual frameworks, thus failing to address the crucial issue of
cross-lingual knowledge synchronization for multilingual models. To tackle this
problem, we propose a simple yet effective method that trains multilingual
patch neuron to store cross-lingual knowledge. It can be easily adapted to
existing approaches to enhance their cross-lingual editing capabilities. To
evaluate our method, we conduct experiments using both the XNLI dataset and a
self-constructed XFEVER dataset. Experimental results demonstrate that our
proposed method achieves improved performance in cross-lingual editing tasks
without requiring excessive modifications to the original methodology, thereby
showcasing its user-friendly characteristics. Codes will be released soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_N/0/1/0/all/0/1&quot;&gt;Nianwen Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weiqiang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03194">
<title>Learning Persistent Community Structures in Dynamic Networks via Topological Data Analysis. (arXiv:2401.03194v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.03194</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic community detection methods often lack effective mechanisms to ensure
temporal consistency, hindering the analysis of network evolution. In this
paper, we propose a novel deep graph clustering framework with temporal
consistency regularization on inter-community structures, inspired by the
concept of minimal network topological changes within short intervals.
Specifically, to address the representation collapse problem, we first
introduce MFC, a matrix factorization-based deep graph clustering algorithm
that preserves node embedding. Based on static clustering results, we construct
probabilistic community networks and compute their persistence homology, a
robust topological measure, to assess structural similarity between them.
Moreover, a novel neural network regularization TopoReg is introduced to ensure
the preservation of topological similarity between inter-community structures
over time intervals. Our approach enhances temporal consistency and clustering
accuracy on real-world datasets with both fixed and varying numbers of
communities. It is also a pioneer application of TDA in temporally persistent
community detection, offering an insightful contribution to field of network
analysis. Code and data are available at the public git repository:
https://github.com/kundtx/MFC_TopoReg
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1&quot;&gt;Dexu Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Anping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03196">
<title>SecureReg: A Combined Framework for Proactively Exposing Malicious Domain Name Registrations. (arXiv:2401.03196v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.03196</link>
<description rdf:parseType="Literal">&lt;p&gt;Rising cyber threats, with miscreants registering thousands of new domains
daily for Internet-scale attacks like spam, phishing, and drive-by downloads,
emphasize the need for innovative detection methods. This paper introduces a
cutting-edge approach for identifying suspicious domains at the onset of the
registration process. The accompanying data pipeline generates crucial features
by comparing new domains to registered domains,emphasizing the crucial
similarity score. Leveraging a novel combination of Natural Language Processing
(NLP) techniques, including a pretrained Canine model, and Multilayer
Perceptron (MLP) models, our system analyzes semantic and numerical attributes,
providing a robust solution for early threat detection. This integrated
approach significantly reduces the window of vulnerability, fortifying defenses
against potential threats. The findings demonstrate the effectiveness of the
integrated approach and contribute to the ongoing efforts in developing
proactive strategies to mitigate the risks associated with illicit online
activities through the early identification of suspicious domain registrations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colhak_F/0/1/0/all/0/1&quot;&gt;Furkan &amp;#xc7;olhak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ecevit_M/0/1/0/all/0/1&quot;&gt;Mert &amp;#x130;lhan Ecevit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dag_H/0/1/0/all/0/1&quot;&gt;Hasan Da&amp;#x11f;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Creutzburg_R/0/1/0/all/0/1&quot;&gt;Reiner Creutzburg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03197">
<title>Decision Making in Non-Stationary Environments with Policy-Augmented Search. (arXiv:2401.03197v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.03197</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequential decision-making under uncertainty is present in many important
problems. Two popular approaches for tackling such problems are reinforcement
learning and online search (e.g., Monte Carlo tree search). While the former
learns a policy by interacting with the environment (typically done before
execution), the latter uses a generative model of the environment to sample
promising action trajectories at decision time. Decision-making is particularly
challenging in non-stationary environments, where the environment in which an
agent operates can change over time. Both approaches have shortcomings in such
settings -- on the one hand, policies learned before execution become stale
when the environment changes and relearning takes both time and computational
effort. Online search, on the other hand, can return sub-optimal actions when
there are limitations on allowed runtime. In this paper, we introduce
\textit{Policy-Augmented Monte Carlo tree search} (PA-MCTS), which combines
action-value estimates from an out-of-date policy with an online search using
an up-to-date model of the environment. We prove theoretical results showing
conditions under which PA-MCTS selects the one-step optimal action and also
bound the error accrued while following PA-MCTS as a policy. We compare and
contrast our approach with AlphaZero, another hybrid planning approach, and
Deep Q Learning on several OpenAI Gym environments. Through extensive
experiments, we show that under non-stationary settings with limited time
constraints, PA-MCTS outperforms these baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pettet_A/0/1/0/all/0/1&quot;&gt;Ava Pettet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Baiting Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wray_K/0/1/0/all/0/1&quot;&gt;Kyle Wray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baier_H/0/1/0/all/0/1&quot;&gt;Hendrik Baier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laszka_A/0/1/0/all/0/1&quot;&gt;Aron Laszka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1&quot;&gt;Abhishek Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukhopadhyay_A/0/1/0/all/0/1&quot;&gt;Ayan Mukhopadhyay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03214">
<title>Understanding Representation Learnability of Nonlinear Self-Supervised Learning. (arXiv:2401.03214v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03214</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning (SSL) has empirically shown its data representation
learnability in many downstream tasks. There are only a few theoretical works
on data representation learnability, and many of those focus on final data
representation, treating the nonlinear neural network as a ``black box&quot;.
However, the accurate learning results of neural networks are crucial for
describing the data distribution features learned by SSL models. Our paper is
the first to analyze the learning results of the nonlinear SSL model
accurately. We consider a toy data distribution that contains two features: the
label-related feature and the hidden feature. Unlike previous linear setting
work that depends on closed-form solutions, we use the gradient descent
algorithm to train a 1-layer nonlinear SSL model with a certain initialization
region and prove that the model converges to a local minimum. Furthermore,
different from the complex iterative analysis, we propose a new analysis
process which uses the exact version of Inverse Function Theorem to accurately
describe the features learned by the local minimum. With this local minimum, we
prove that the nonlinear SSL model can capture the label-related feature and
hidden feature at the same time. In contrast, the nonlinear supervised learning
(SL) model can only learn the label-related feature. We also present the
learning processes and results of the nonlinear SSL and SL model via simulation
experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Ruofeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuai Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03221">
<title>MirrorDiffusion: Stabilizing Diffusion Process in Zero-shot Image Translation by Prompts Redescription and Beyond. (arXiv:2401.03221v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03221</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, text-to-image diffusion models become a new paradigm in image
processing fields, including content generation, image restoration and
image-to-image translation. Given a target prompt, Denoising Diffusion
Probabilistic Models (DDPM) are able to generate realistic yet eligible images.
With this appealing property, the image translation task has the potential to
be free from target image samples for supervision. By using a target text
prompt for domain adaption, the diffusion model is able to implement zero-shot
image-to-image translation advantageously. However, the sampling and inversion
processes of DDPM are stochastic, and thus the inversion process often fail to
reconstruct the input content. Specifically, the displacement effect will
gradually accumulated during the diffusion and inversion processes, which led
to the reconstructed results deviating from the source domain. To make
reconstruction explicit, we propose a prompt redescription strategy to realize
a mirror effect between the source and reconstructed image in the diffusion
model (MirrorDiffusion). More specifically, a prompt redescription mechanism is
investigated to align the text prompts with latent code at each time step of
the Denoising Diffusion Implicit Models (DDIM) inversion to pursue a
structure-preserving reconstruction. With the revised DDIM inversion,
MirrorDiffusion is able to realize accurate zero-shot image translation by
editing optimized text prompts and latent code. Extensive experiments
demonstrate that MirrorDiffusion achieves superior performance over the
state-of-the-art methods on zero-shot image translation benchmarks by clear
margins and practical model stability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yupei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xian_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Xian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yukai Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liang Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03233">
<title>Convergence Rate Maximization for Split Learning-based Control of EMG Prosthetic Devices. (arXiv:2401.03233v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03233</link>
<description rdf:parseType="Literal">&lt;p&gt;Split Learning (SL) is a promising Distributed Learning approach in
electromyography (EMG) based prosthetic control, due to its applicability
within resource-constrained environments. Other learning approaches, such as
Deep Learning and Federated Learning (FL), provide suboptimal solutions, since
prosthetic devices are extremely limited in terms of processing power and
battery life. The viability of implementing SL in such scenarios is caused by
its inherent model partitioning, with clients executing the smaller model
segment. However, selecting an inadequate cut layer hinders the training
process in SL systems. This paper presents an algorithm for optimal cut layer
selection in terms of maximizing the convergence rate of the model. The
performance evaluation demonstrates that the proposed algorithm substantially
accelerates the convergence in an EMG pattern recognition task for improving
prosthetic device control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marinova_M/0/1/0/all/0/1&quot;&gt;Matea Marinova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denkovski_D/0/1/0/all/0/1&quot;&gt;Daniel Denkovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gjoreski_H/0/1/0/all/0/1&quot;&gt;Hristijan Gjoreski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadzi_Velkov_Z/0/1/0/all/0/1&quot;&gt;Zoran Hadzi-Velkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakovic_V/0/1/0/all/0/1&quot;&gt;Valentin Rakovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03238">
<title>Using Large Language Models to Assess Tutors&apos; Performance in Reacting to Students Making Math Errors. (arXiv:2401.03238v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.03238</link>
<description rdf:parseType="Literal">&lt;p&gt;Research suggests that tutors should adopt a strategic approach when
addressing math errors made by low-efficacy students. Rather than drawing
direct attention to the error, tutors should guide the students to identify and
correct their mistakes on their own. While tutor lessons have introduced this
pedagogical skill, human evaluation of tutors applying this strategy is arduous
and time-consuming. Large language models (LLMs) show promise in providing
real-time assessment to tutors during their actual tutoring sessions, yet
little is known regarding their accuracy in this context. In this study, we
investigate the capacity of generative AI to evaluate real-life tutors&apos;
performance in responding to students making math errors. By analyzing 50
real-life tutoring dialogues, we find both GPT-3.5-Turbo and GPT-4 demonstrate
proficiency in assessing the criteria related to reacting to students making
errors. However, both models exhibit limitations in recognizing instances where
the student made an error. Notably, GPT-4 tends to overidentify instances of
students making errors, often attributing student uncertainty or inferring
potential errors where human evaluators did not. Future work will focus on
enhancing generalizability by assessing a larger dataset of dialogues and
evaluating learning transfer. Specifically, we will analyze the performance of
tutors in real-life scenarios when responding to students&apos; math errors before
and after lesson completion on this crucial tutoring skill.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kakarla_S/0/1/0/all/0/1&quot;&gt;Sanjit Kakarla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomas_D/0/1/0/all/0/1&quot;&gt;Danielle Thomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jionghao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Shivang Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koedinger_K/0/1/0/all/0/1&quot;&gt;Kenneth R. Koedinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03244">
<title>Artificial Intelligence for Operations Research: Revolutionizing the Operations Research Process. (arXiv:2401.03244v1 [math.OC])</title>
<link>http://arxiv.org/abs/2401.03244</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid advancement of artificial intelligence (AI) techniques has opened
up new opportunities to revolutionize various fields, including operations
research (OR). This survey paper explores the integration of AI within the OR
process (AI4OR) to enhance its effectiveness and efficiency across multiple
stages, such as parameter generation, model formulation, and model
optimization. By providing a comprehensive overview of the state-of-the-art and
examining the potential of AI to transform OR, this paper aims to inspire
further research and innovation in the development of AI-enhanced OR methods
and tools. The synergy between AI and OR is poised to drive significant
advancements and novel solutions in a multitude of domains, ultimately leading
to more effective and efficient decision-making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zhenan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ghaddar_B/0/1/0/all/0/1&quot;&gt;Bissan Ghaddar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinglu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xing_L/0/1/0/all/0/1&quot;&gt;Linzi Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zirui Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03246">
<title>SeqNAS: Neural Architecture Search for Event Sequence Classification. (arXiv:2401.03246v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03246</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Architecture Search (NAS) methods are widely used in various
industries to obtain high quality taskspecific solutions with minimal human
intervention. Event Sequences find widespread use in various industrial
applications including churn prediction customer segmentation fraud detection
and fault diagnosis among others. Such data consist of categorical and
real-valued components with irregular timestamps. Despite the usefulness of NAS
methods previous approaches only have been applied to other domains images
texts or time series. Our work addresses this limitation by introducing a novel
NAS algorithm SeqNAS specifically designed for event sequence classification.
We develop a simple yet expressive search space that leverages commonly used
building blocks for event sequence classification including multihead self
attention convolutions and recurrent cells. To perform the search we adopt
sequential Bayesian Optimization and utilize previously trained models as an
ensemble of teachers to augment knowledge distillation. As a result of our work
we demonstrate that our method surpasses state of the art NAS methods and
popular architectures suitable for sequence classification and holds great
potential for various industrial applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Udovichenko_I/0/1/0/all/0/1&quot;&gt;Igor Udovichenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shvetsov_E/0/1/0/all/0/1&quot;&gt;Egor Shvetsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Divitsky_D/0/1/0/all/0/1&quot;&gt;Denis Divitsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osin_D/0/1/0/all/0/1&quot;&gt;Dmitry Osin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trofimov_I/0/1/0/all/0/1&quot;&gt;Ilya Trofimov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glushenko_A/0/1/0/all/0/1&quot;&gt;Anatoly Glushenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sukharev_I/0/1/0/all/0/1&quot;&gt;Ivan Sukharev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berestenev_D/0/1/0/all/0/1&quot;&gt;Dmitry Berestenev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1&quot;&gt;Evgeny Burnaev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03267">
<title>Autonomous Navigation in Complex Environments. (arXiv:2401.03267v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.03267</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the application of CNN-DNN network fusion to construct a
robot navigation controller within a simulated environment. The simulated
environment is constructed to model a subterranean rescue situation, such that
an autonomous agent is tasked with finding a goal within an unknown cavernous
system. Imitation learning is used to train the control algorithm to use LiDAR
and camera data to navigate the space and find the goal. The trained model is
then tested for robustness using Monte-Carlo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerstenslager_A/0/1/0/all/0/1&quot;&gt;Andrew Gerstenslager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_J/0/1/0/all/0/1&quot;&gt;Jomol Lewis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McKenna_L/0/1/0/all/0/1&quot;&gt;Liam McKenna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_P/0/1/0/all/0/1&quot;&gt;Poorva Patel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03275">
<title>Real Time Human Detection by Unmanned Aerial Vehicles. (arXiv:2401.03275v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03275</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the most important problems in computer vision and remote sensing is
object detection, which identifies particular categories of diverse things in
pictures. Two crucial data sources for public security are the thermal infrared
(TIR) remote sensing multi-scenario photos and videos produced by unmanned
aerial vehicles (UAVs). Due to the small scale of the target, complex scene
information, low resolution relative to the viewable videos, and dearth of
publicly available labeled datasets and training models, their object detection
procedure is still difficult. A UAV TIR object detection framework for pictures
and videos is suggested in this study. The Forward-looking Infrared (FLIR)
cameras used to gather ground-based TIR photos and videos are used to create
the ``You Only Look Once&apos;&apos; (YOLO) model, which is based on CNN architecture.
Results indicated that in the validating task, detecting human object had an
average precision at IOU (Intersection over Union) = 0.5, which was 72.5\%,
using YOLOv7 (YOLO version 7) state of the art model \cite{1}, while the
detection speed around 161 frames per second (FPS/second). The usefulness of
the YOLO architecture is demonstrated in the application, which evaluates the
cross-detection performance of people in UAV TIR videos under a YOLOv7 model in
terms of the various UAVs&apos; observation angles. The qualitative and quantitative
evaluation of object detection from TIR pictures and videos using deep-learning
models is supported favorably by this work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guettala_W/0/1/0/all/0/1&quot;&gt;Walid Guettala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sayah_A/0/1/0/all/0/1&quot;&gt;Ali Sayah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahloul_L/0/1/0/all/0/1&quot;&gt;Laid Kahloul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tibermacine_A/0/1/0/all/0/1&quot;&gt;Ahmed Tibermacine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03302">
<title>Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT. (arXiv:2401.03302v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.03302</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of medical sciences, reliable detection and classification of
brain tumors from images remains a formidable challenge due to the rarity of
tumors within the population of patients. Therefore, the ability to detect
tumors in anomaly scenarios is paramount for ensuring timely interventions and
improved patient outcomes. This study addresses the issue by leveraging deep
learning (DL) techniques to detect and classify brain tumors in challenging
situations. The curated data set from the National Brain Mapping Lab (NBML)
comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The
detection and classification pipelines are separated into two consecutive
tasks. The detection phase involved comprehensive data analysis and
pre-processing to modify the number of image samples and the number of patients
of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with
real world scenarios. Next, in addition to common evaluation metrics for the
testing, we employed a novel performance evaluation method called Patient to
Patient (PTP), focusing on the realistic evaluation of the model. In the
detection phase, we fine-tuned a YOLOv8n detection model to detect the tumor
region. Subsequent testing and evaluation yielded competitive performance both
in Common Evaluation Metrics and PTP metrics. Furthermore, using the Data
Efficient Image Transformer (DeiT) module, we distilled a Vision Transformer
(ViT) model from a fine-tuned ResNet152 as a teacher in the classification
phase. This approach demonstrates promising strides in reliable tumor detection
and classification, offering potential advancements in tumor diagnosis for
real-world medical imaging scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hashemi_S/0/1/0/all/0/1&quot;&gt;Seyed Mohammad Hossein Hashemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Safari_L/0/1/0/all/0/1&quot;&gt;Leila Safari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Taromi_A/0/1/0/all/0/1&quot;&gt;Amirhossein Dadashzade Taromi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03306">
<title>MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot Learning. (arXiv:2401.03306v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03306</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of offline pre-training and online fine-tuning for
reinforcement learning from high-dimensional observations in the context of
realistic robot tasks. Recent offline model-free approaches successfully use
online fine-tuning to either improve the performance of the agent over the data
collection policy or adapt to novel tasks. At the same time, model-based RL
algorithms have achieved significant progress in sample efficiency and the
complexity of the tasks they can solve, yet remain under-utilized in the
fine-tuning setting. In this work, we argue that existing model-based offline
RL methods are not suitable for offline-to-online fine-tuning in
high-dimensional domains due to issues with distribution shifts, off-dynamics
data, and non-stationary rewards. We propose an on-policy model-based method
that can efficiently reuse prior data through model-based value expansion and
policy regularization, while preventing model exploitation by controlling
epistemic uncertainty. We find that our approach successfully solves tasks from
the MetaWorld benchmark, as well as the Franka Kitchen robot manipulation
environment completely from images. To the best of our knowledge, MOTO is the
first method to solve this environment from pixels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rafailov_R/0/1/0/all/0/1&quot;&gt;Rafael Rafailov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatch_K/0/1/0/all/0/1&quot;&gt;Kyle Hatch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolev_V/0/1/0/all/0/1&quot;&gt;Victor Kolev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1&quot;&gt;John D. Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phielipp_M/0/1/0/all/0/1&quot;&gt;Mariano Phielipp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03310">
<title>CAVIAR: Co-simulation of 6G Communications, 3D Scenarios and AI for Digital Twins. (arXiv:2401.03310v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2401.03310</link>
<description rdf:parseType="Literal">&lt;p&gt;Digital twins are an important technology for advancing mobile
communications, specially in use cases that require simultaneously simulating
the wireless channel, 3D scenes and machine learning. Aiming at providing a
solution to this demand, this work describes a modular co-simulation
methodology called CAVIAR. Here, CAVIAR is upgraded to support a message
passing library and enable the virtual counterpart of a digital twin system
using different 6G-related simulators. The main contributions of this work are
the detailed description of different CAVIAR architectures, the implementation
of this methodology to assess a 6G use case of UAV-based search and rescue
mission (SAR), and the generation of benchmarking data about the computational
resource usage. For executing the SAR co-simulation we adopt five open-source
solutions: the physical and link level network simulator Sionna, the simulator
for autonomous vehicles AirSim, scikit-learn for training a decision tree for
MIMO beam selection, Yolov8 for the detection of rescue targets and NATS for
message passing. Results for the implemented SAR use case suggest that the
methodology can run in a single machine, with the main demanded resources being
the CPU processing and the GPU memory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borges_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Borges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bastos_F/0/1/0/all/0/1&quot;&gt;Felipe Bastos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Correa_I/0/1/0/all/0/1&quot;&gt;Ilan Correa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batista_P/0/1/0/all/0/1&quot;&gt;Pedro Batista&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klautau_A/0/1/0/all/0/1&quot;&gt;Aldebaro Klautau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03312">
<title>Exploiting Data Hierarchy as a New Modality for Contrastive Learning. (arXiv:2401.03312v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03312</link>
<description rdf:parseType="Literal">&lt;p&gt;This work investigates how hierarchically structured data can help neural
networks learn conceptual representations of cathedrals. The underlying
WikiScenes dataset provides a spatially organized hierarchical structure of
cathedral components. We propose a novel hierarchical contrastive training
approach that leverages a triplet margin loss to represent the data&apos;s spatial
hierarchy in the encoder&apos;s latent space. As such, the proposed approach
investigates if the dataset structure provides valuable information for
self-supervised learning. We apply t-SNE to visualize the resultant latent
space and evaluate the proposed approach by comparing it with other
dataset-specific contrastive learning methods using a common downstream
classification task. The proposed method outperforms the comparable
weakly-supervised and baseline methods. Our findings suggest that dataset
structure is a valuable modality for weakly-supervised learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhalla_A/0/1/0/all/0/1&quot;&gt;Arjun Bhalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levenson_D/0/1/0/all/0/1&quot;&gt;Daniel Levenson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernhard_J/0/1/0/all/0/1&quot;&gt;Jan Bernhard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abilov_A/0/1/0/all/0/1&quot;&gt;Anton Abilov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03314">
<title>Enhancing Context Through Contrast. (arXiv:2401.03314v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.03314</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural machine translation benefits from semantically rich representations.
Considerable progress in learning such representations has been achieved by
language modelling and mutual information maximization objectives using
contrastive learning. The language-dependent nature of language modelling
introduces a trade-off between the universality of the learned representations
and the model&apos;s performance on the language modelling tasks. Although
contrastive learning improves performance, its success cannot be attributed to
mutual information alone. We propose a novel Context Enhancement step to
improve performance on neural machine translation by maximizing mutual
information using the Barlow Twins loss. Unlike other approaches, we do not
explicitly augment the data but view languages as implicit augmentations,
eradicating the risk of disrupting semantic information. Further, our method
does not learn embeddings from scratch and can be generalised to any set of
pre-trained embeddings. Finally, we evaluate the language-agnosticism of our
embeddings through language classification and use them for neural machine
translation to compare with state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ambilduke_K/0/1/0/all/0/1&quot;&gt;Kshitij Ambilduke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shetye_A/0/1/0/all/0/1&quot;&gt;Aneesh Shetye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagade_D/0/1/0/all/0/1&quot;&gt;Diksha Bagade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhagwatkar_R/0/1/0/all/0/1&quot;&gt;Rishika Bhagwatkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fitter_K/0/1/0/all/0/1&quot;&gt;Khurshed Fitter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vagdargi_P/0/1/0/all/0/1&quot;&gt;Prasad Vagdargi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiddarwar_S/0/1/0/all/0/1&quot;&gt;Shital Chiddarwar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03315">
<title>Malla: Demystifying Real-world Large Language Model Integrated Malicious Services. (arXiv:2401.03315v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.03315</link>
<description rdf:parseType="Literal">&lt;p&gt;The underground exploitation of large language models (LLMs) for malicious
services (i.e., Malla) is witnessing an uptick, amplifying the cyber threat
landscape and posing questions about the trustworthiness of LLM technologies.
However, there has been little effort to understand this new cybercrime, in
terms of its magnitude, impact, and techniques. In this paper, we conduct the
first systematic study on 212 real-world Mallas, uncovering their proliferation
in underground marketplaces and exposing their operational modalities. Our
study discloses the Malla ecosystem, revealing its significant growth and
impact on today&apos;s public LLM services. Through examining 212 Mallas, we
uncovered eight backend LLMs used by Mallas, along with 182 prompts that
circumvent the protective measures of public LLM APIs. We further demystify the
tactics employed by Mallas, including the abuse of uncensored LLMs and the
exploitation of public LLM APIs through jailbreak prompts. Our findings enable
a better understanding of the real-world exploitation of LLMs by
cybercriminals, offering insights into strategies to counteract this
cybercrime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zilong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jian Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1&quot;&gt;Xiaojing Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;XiaoFeng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03319">
<title>Comparison of Microservice Call Rate Predictions for Replication in the Cloud. (arXiv:2401.03319v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2401.03319</link>
<description rdf:parseType="Literal">&lt;p&gt;Today, many users deploy their microservice-based applications with various
interconnections on a cluster of Cloud machines, subject to stochastic changes
due to dynamic user requirements. To address this problem, we compare three
machine learning (ML) models for predicting the microservice call rates based
on the microservice times and aiming at estimating the scalability
requirements. We apply the linear regression (LR), multilayer perception (MLP),
and gradient boosting regression (GBR) models on the Alibaba microservice
traces. The prediction results reveal that the LR model reaches a lower
training time than the GBR and MLP models. However, the GBR reduces the mean
absolute error and the mean absolute percentage error compared to LR and MLP
models. Moreover, the prediction results show that the required number of
replicas for each microservice by the gradient boosting model is close to the
actual test data without any prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehran_N/0/1/0/all/0/1&quot;&gt;Narges Mehran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haghighi_A/0/1/0/all/0/1&quot;&gt;Arman Haghighi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aminharati_P/0/1/0/all/0/1&quot;&gt;Pedram Aminharati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolov_N/0/1/0/all/0/1&quot;&gt;Nikolay Nikolov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soylu_A/0/1/0/all/0/1&quot;&gt;Ahmet Soylu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roman_D/0/1/0/all/0/1&quot;&gt;Dumitru Roman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prodan_R/0/1/0/all/0/1&quot;&gt;Radu Prodan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03322">
<title>Attention and Autoencoder Hybrid Model for Unsupervised Online Anomaly Detection. (arXiv:2401.03322v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03322</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a hybrid attention and autoencoder (AE) model for
unsupervised online anomaly detection in time series. The autoencoder captures
local structural patterns in short embeddings, while the attention model learns
long-term features, facilitating parallel computing with positional encoding.
Unique in its approach, our proposed hybrid model combines attention and
autoencoder for the first time in time series anomaly detection. It employs an
attention-based mechanism, akin to the deep transformer model, with key
architectural modifications for predicting the next time step window in the
autoencoder&apos;s latent space. The model utilizes a threshold from the validation
dataset for anomaly detection and introduces an alternative method based on
analyzing the first statistical moment of error, improving accuracy without
dependence on a validation dataset. Evaluation on diverse real-world benchmark
datasets and comparing with other well-established models, confirms the
effectiveness of our proposed model in anomaly detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najafi_S/0/1/0/all/0/1&quot;&gt;Seyed Amirhossein Najafi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asemani_M/0/1/0/all/0/1&quot;&gt;Mohammad Hassan Asemani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Setoodeh_P/0/1/0/all/0/1&quot;&gt;Peyman Setoodeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03337">
<title>MTAC: Hierarchical Reinforcement Learning-based Multi-gait Terrain-adaptive Quadruped Controller. (arXiv:2401.03337v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.03337</link>
<description rdf:parseType="Literal">&lt;p&gt;Urban search and rescue missions require rapid first response to minimize
loss of life and damage. Often, such efforts are assisted by humanitarian
robots which need to handle dynamic operational conditions such as uneven and
rough terrains, especially during mass casualty incidents like an earthquake.
Quadruped robots, owing to their versatile design, have the potential to assist
in such scenarios. However, control of quadruped robots in dynamic and rough
terrain environments is a challenging problem due to the many degrees of
freedom of these robots. Current locomotion controllers for quadrupeds are
limited in their ability to produce multiple adaptive gaits, solve tasks in a
time and resource-efficient manner, and require tedious training and manual
tuning procedures. To address these challenges, we propose MTAC: a multi-gait
terrain-adaptive controller, which utilizes a Hierarchical reinforcement
learning (HRL) approach while being time and memory-efficient. We show that our
proposed method scales well to a diverse range of environments with similar
compute times as state-of-the-art methods. Our method showed greater than 75%
on most tasks, outperforming previous work on the majority of test cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Nishaant Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiwari_K/0/1/0/all/0/1&quot;&gt;Kshitij Tiwari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1&quot;&gt;Aniket Bera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03343">
<title>Rediscovering Ranganathan: A Prismatic View of His Life through the Knowledge Graph Spectrum. (arXiv:2401.03343v1 [cs.DL])</title>
<link>http://arxiv.org/abs/2401.03343</link>
<description rdf:parseType="Literal">&lt;p&gt;The present study puts forward a novel biographical knowledge graph (KG) on
Prof. S. R. Ranganathan, one of the pioneering figures in the Library and
Information Science (LIS) domain. It has been found that most of the relevant
facts about Ranganathan exist in a variety of resources (e.g., books, essays,
journal articles, websites, blogs, etc.), offering information in a fragmented
and piecemeal way. With this dedicated KG (henceforth known as RKG), we hope to
furnish a 360-degree view of his life and achievements. To the best of our
knowledge, such a dedicated representation is unparalleled in its scope and
coverage: using state-of-the-art technology for anyone to openly access,
use/re-use, and contribute. Inspired by Ranganathan&apos;s theories and ideas, the
KG was developed using a &quot;facet-based methodology&quot; at two levels: in the
identification of the vital biographical aspects and the development of the
ontological model. Finally, with this study, we call for a community-driven
effort to enhance the KG and pay homage to the Father of Library Science on the
hundredth anniversary of his revitalizing the LIS domain through his enduring
participation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_B/0/1/0/all/0/1&quot;&gt;B. Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arzoo_S/0/1/0/all/0/1&quot;&gt;S. Arzoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03346">
<title>An Investigation of Large Language Models for Real-World Hate Speech Detection. (arXiv:2401.03346v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.03346</link>
<description rdf:parseType="Literal">&lt;p&gt;Hate speech has emerged as a major problem plaguing our social spaces today.
While there have been significant efforts to address this problem, existing
methods are still significantly limited in effectively detecting hate speech
online. A major limitation of existing methods is that hate speech detection is
a highly contextual problem, and these methods cannot fully capture the context
of hate speech to make accurate predictions. Recently, large language models
(LLMs) have demonstrated state-of-the-art performance in several natural
language tasks. LLMs have undergone extensive training using vast amounts of
natural language data, enabling them to grasp intricate contextual details.
Hence, they could be used as knowledge bases for context-aware hate speech
detection. However, a fundamental problem with using LLMs to detect hate speech
is that there are no studies on effectively prompting LLMs for context-aware
hate speech detection. In this study, we conduct a large-scale study of hate
speech detection, employing five established hate speech datasets. We discover
that LLMs not only match but often surpass the performance of current benchmark
machine learning models in identifying hate speech. By proposing four diverse
prompting strategies that optimize the use of LLMs in detecting hate speech.
Our study reveals that a meticulously crafted reasoning prompt can effectively
capture the context of hate speech by fully utilizing the knowledge base in
LLMs, significantly outperforming existing techniques. Furthermore, although
LLMs can provide a rich knowledge base for the contextual detection of hate
speech, suitable prompting strategies play a crucial role in effectively
leveraging this knowledge base for efficient detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1&quot;&gt;Keyan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1&quot;&gt;Alexander Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1&quot;&gt;Jaden Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Ziheng Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Ziming Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishwamitra_N/0/1/0/all/0/1&quot;&gt;Nishant Vishwamitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hongxin Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2108.11092">
<title>INVIGORATE: Interactive Visual Grounding and Grasping in Clutter. (arXiv:2108.11092v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2108.11092</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents INVIGORATE, a robot system that interacts with human
through natural language and grasps a specified object in clutter. The objects
may occlude, obstruct, or even stack on top of one another. INVIGORATE embodies
several challenges: (i) infer the target object among other occluding objects,
from input language expressions and RGB images, (ii) infer object blocking
relationships (OBRs) from the images, and (iii) synthesize a multi-step plan to
ask questions that disambiguate the target object and to grasp it successfully.
We train separate neural networks for object detection, for visual grounding,
for question generation, and for OBR detection and grasping. They allow for
unrestricted object categories and language expressions, subject to the
training datasets. However, errors in visual perception and ambiguity in human
languages are inevitable and negatively impact the robot&apos;s performance. To
overcome these uncertainties, we build a partially observable Markov decision
process (POMDP) that integrates the learned neural network modules. Through
approximate POMDP planning, the robot tracks the history of observations and
asks disambiguation questions in order to achieve a near-optimal sequence of
actions that identify and grasp the target object. INVIGORATE combines the
benefits of model-based POMDP planning and data-driven deep learning.
Preliminary experiments with INVIGORATE on a Fetch robot show significant
benefits of this integrated approach to object grasping in clutter with natural
language interactions. A demonstration video is available at
https://youtu.be/zYakh80SGcU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanbo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yunfan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Cunjun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1&quot;&gt;David Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1&quot;&gt;Xuguang Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1&quot;&gt;Nanning Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.11567">
<title>Logical Assessment Formula and Its Principles for Evaluations with Inaccurate Ground-Truth Labels. (arXiv:2110.11567v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2110.11567</link>
<description rdf:parseType="Literal">&lt;p&gt;Evaluations with accurate ground-truth labels (AGTLs) have been widely
employed to assess predictive models for artificial intelligence applications.
However, in some specific fields, such as medical histopathology whole slide
image analysis, it is quite usual the situation that AGTLs are difficult to be
precisely defined or even do not exist. To alleviate this situation, we propose
logical assessment formula (LAF) and reveal its principles for evaluations with
inaccurate ground-truth labels (IAGTLs) via logical reasoning under
uncertainty. From the revealed principles of LAF, we summarize the
practicability of LAF: 1) LAF can be applied for evaluations with IAGTLs on a
more difficult task, able to act like usual strategies for evaluations with
AGTLs reasonably; 2) LAF can be applied for evaluations with IAGTLs from the
logical perspective on an easier task, unable to act like usual strategies for
evaluations with AGTLs confidently.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yongquan Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.02690">
<title>Open Vocabulary Electroencephalography-To-Text Decoding and Zero-shot Sentiment Classification. (arXiv:2112.02690v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2112.02690</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art brain-to-text systems have achieved great success in
decoding language directly from brain signals using neural networks. However,
current approaches are limited to small closed vocabularies which are far from
enough for natural communication. In addition, most of the high-performing
approaches require data from invasive devices (e.g., ECoG). In this paper, we
extend the problem to open vocabulary Electroencephalography(EEG)-To-Text
Sequence-To-Sequence decoding and zero-shot sentence sentiment classification
on natural reading tasks. We hypothesis that the human brain functions as a
special text encoder and propose a novel framework leveraging pre-trained
language models (e.g., BART). Our model achieves a 40.1% BLEU-1 score on
EEG-To-Text decoding and a 55.6% F1 score on zero-shot EEG-based ternary
sentiment classification, which significantly outperforms supervised baselines.
Furthermore, we show that our proposed model can handle data from various
subjects and sources, showing great potential for a high-performance open
vocabulary brain-to-text system once sufficient data is available
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenhailong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.03208">
<title>From Attribution Maps to Human-Understandable Explanations through Concept Relevance Propagation. (arXiv:2206.03208v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.03208</link>
<description rdf:parseType="Literal">&lt;p&gt;The field of eXplainable Artificial Intelligence (XAI) aims to bring
transparency to today&apos;s powerful but opaque deep learning models. While local
XAI methods explain individual predictions in form of attribution maps, thereby
identifying where important features occur (but not providing information about
what they represent), global explanation techniques visualize what concepts a
model has generally learned to encode. Both types of methods thus only provide
partial insights and leave the burden of interpreting the model&apos;s reasoning to
the user. In this work we introduce the Concept Relevance Propagation (CRP)
approach, which combines the local and global perspectives and thus allows
answering both the &quot;where&quot; and &quot;what&quot; questions for individual predictions. We
demonstrate the capability of our method in various settings, showcasing that
CRP leads to more human interpretable explanations and provides deep insights
into the model&apos;s representation and reasoning through concept atlases, concept
composition analyses, and quantitative investigations of concept subspaces and
their role in fine-grained decision making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achtibat_R/0/1/0/all/0/1&quot;&gt;Reduan Achtibat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1&quot;&gt;Maximilian Dreyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisenbraun_I/0/1/0/all/0/1&quot;&gt;Ilona Eisenbraun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosse_S/0/1/0/all/0/1&quot;&gt;Sebastian Bosse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiegand_T/0/1/0/all/0/1&quot;&gt;Thomas Wiegand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1&quot;&gt;Wojciech Samek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1&quot;&gt;Sebastian Lapuschkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.05631">
<title>DGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization. (arXiv:2207.05631v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.05631</link>
<description rdf:parseType="Literal">&lt;p&gt;Most reinforcement learning algorithms seek a single optimal strategy that
solves a given task. However, it can often be valuable to learn a diverse set
of solutions, for instance, to make an agent&apos;s interaction with users more
engaging, or improve the robustness of a policy to an unexpected perturbance.
We propose Diversity-Guided Policy Optimization (DGPO), an on-policy algorithm
that discovers multiple strategies for solving a given task. Unlike prior work,
it achieves this with a shared policy network trained over a single run.
Specifically, we design an intrinsic reward based on an information-theoretic
diversity objective. Our final objective alternately constraints on the
diversity of the strategies and on the extrinsic reward. We solve the
constrained optimization problem by casting it as a probabilistic inference
task and use policy iteration to maximize the derived lower bound. Experimental
results show that our method efficiently discovers diverse strategies in a wide
variety of reinforcement learning tasks. Compared to baseline methods, DGPO
achieves comparable rewards, while discovering more diverse strategies, and
often with better sample efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wentse Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shiyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_Y/0/1/0/all/0/1&quot;&gt;Yuan Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pearce_T/0/1/0/all/0/1&quot;&gt;Tim Pearce&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1&quot;&gt;Wei-Wei Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Ting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.04370">
<title>NESTER: An Adaptive Neurosymbolic Method for Causal Effect Estimation. (arXiv:2211.04370v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2211.04370</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal effect estimation from observational data is a central problem in
causal inference. Methods based on potential outcomes framework solve this
problem by exploiting inductive biases and heuristics from causal inference.
Each of these methods addresses a specific aspect of causal effect estimation,
such as controlling propensity score, enforcing randomization, etc., by
designing neural network (NN) architectures and regularizers. In this paper, we
propose an adaptive method called Neurosymbolic Causal Effect Estimator
(NESTER), a generalized method for causal effect estimation. NESTER integrates
the ideas used in existing methods based on multi-head NNs for causal effect
estimation into one framework. We design a Domain Specific Language (DSL)
tailored for causal effect estimation based on causal inductive biases used in
literature. We conduct a theoretical analysis to investigate NESTER&apos;s efficacy
in estimating causal effects. Our comprehensive empirical results show that
NESTER performs better than state-of-the-art methods on benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_A/0/1/0/all/0/1&quot;&gt;Abbavaram Gowtham Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1&quot;&gt;Vineeth N Balasubramanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11010">
<title>Revisiting Color-Event based Tracking: A Unified Network, Dataset, and Metric. (arXiv:2211.11010v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11010</link>
<description rdf:parseType="Literal">&lt;p&gt;Combining the Color and Event cameras (also called Dynamic Vision Sensors,
DVS) for robust object tracking is a newly emerging research topic in recent
years. Existing color-event tracking framework usually contains multiple
scattered modules which may lead to low efficiency and high computational
complexity, including feature extraction, fusion, matching, interactive
learning, etc. In this paper, we propose a single-stage backbone network for
Color-Event Unified Tracking (CEUTrack), which achieves the above functions
simultaneously. Given the event points and RGB frames, we first transform the
points into voxels and crop the template and search regions for both
modalities, respectively. Then, these regions are projected into tokens and
parallelly fed into the unified Transformer backbone network. The output
features will be fed into a tracking head for target object localization. Our
proposed CEUTrack is simple, effective, and efficient, which achieves over 75
FPS and new SOTA performance. To better validate the effectiveness of our model
and address the data deficiency of this task, we also propose a generic and
large-scale benchmark dataset for color-event tracking, termed COESOT, which
contains 90 categories and 1354 video sequences. Additionally, a new evaluation
metric named BOC is proposed in our evaluation toolkit to evaluate the
prominence with respect to the baseline methods. We hope the newly proposed
method, dataset, and evaluation metric provide a better platform for
color-event-based tracking. The dataset, toolkit, and source code will be
released on: \url{https://github.com/Event-AHU/COESOT}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chuanming Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Ju Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianlin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yonghong Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.11870">
<title>Impossibility Theorems for Feature Attribution. (arXiv:2212.11870v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.11870</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite a sea of interpretability methods that can produce plausible
explanations, the field has also empirically seen many failure cases of such
methods. In light of these results, it remains unclear for practitioners how to
use these methods and choose between them in a principled way. In this paper,
we show that for moderately rich model classes (easily satisfied by neural
networks), any feature attribution method that is complete and linear -- for
example, Integrated Gradients and SHAP -- can provably fail to improve on
random guessing for inferring model behaviour. Our results apply to common
end-tasks such as characterizing local model behaviour, identifying spurious
features, and algorithmic recourse. One takeaway from our work is the
importance of concretely defining end-tasks: once such an end-task is defined,
a simple and direct approach of repeated model evaluations can outperform many
other complex feature attribution methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilodeau_B/0/1/0/all/0/1&quot;&gt;Blair Bilodeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaques_N/0/1/0/all/0/1&quot;&gt;Natasha Jaques&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1&quot;&gt;Pang Wei Koh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Been Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.12767">
<title>Compression, Generalization and Learning. (arXiv:2301.12767v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.12767</link>
<description rdf:parseType="Literal">&lt;p&gt;A compression function is a map that slims down an observational set into a
subset of reduced size, while preserving its informational content. In multiple
applications, the condition that one new observation makes the compressed set
change is interpreted that this observation brings in extra information and, in
learning theory, this corresponds to misclassification, or misprediction. In
this paper, we lay the foundations of a new theory that allows one to keep
control on the probability of change of compression (which maps into the
statistical &quot;risk&quot; in learning applications). Under suitable conditions, the
cardinality of the compressed set is shown to be a consistent estimator of the
probability of change of compression (without any upper limit on the size of
the compressed set); moreover, unprecedentedly tight finite-sample bounds to
evaluate the probability of change of compression are obtained under a
generally applicable condition of preference. All results are usable in a fully
agnostic setup, i.e., without requiring any a priori knowledge on the
probability distribution of the observations. Not only these results offer a
valid support to develop trust in observation-driven methodologies, they also
play a fundamental role in learning techniques as a tool for hyper-parameter
tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campi_M/0/1/0/all/0/1&quot;&gt;Marco C. Campi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garatti_S/0/1/0/all/0/1&quot;&gt;Simone Garatti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13126">
<title>LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain. (arXiv:2301.13126v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13126</link>
<description rdf:parseType="Literal">&lt;p&gt;Lately, propelled by the phenomenal advances around the transformer
architecture, the legal NLP field has enjoyed spectacular growth. To measure
progress, well curated and challenging benchmarks are crucial. However, most
benchmarks are English only and in legal NLP specifically there is no
multilingual benchmark available yet. Additionally, many benchmarks are
saturated, with the best models clearly outperforming the best humans and
achieving near perfect scores. We survey the legal NLP literature and select 11
datasets covering 24 languages, creating LEXTREME. To provide a fair
comparison, we propose two aggregate scores, one based on the datasets and one
on the languages. The best baseline (XLM-R large) achieves both a dataset
aggregate score a language aggregate score of 61.3. This indicates that
LEXTREME is still very challenging and leaves ample room for improvement. To
make it easy for researchers and practitioners to use, we release LEXTREME on
huggingface together with all the code required to evaluate models and a public
Weights and Biases project with all the runs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niklaus_J/0/1/0/all/0/1&quot;&gt;Joel Niklaus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matoshi_V/0/1/0/all/0/1&quot;&gt;Veton Matoshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rani_P/0/1/0/all/0/1&quot;&gt;Pooja Rani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galassi_A/0/1/0/all/0/1&quot;&gt;Andrea Galassi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sturmer_M/0/1/0/all/0/1&quot;&gt;Matthias St&amp;#xfc;rmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1&quot;&gt;Ilias Chalkidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.03068">
<title>Evaluating Self-Supervised Learning via Risk Decomposition. (arXiv:2302.03068v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.03068</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning (SSL) pipelines differ in many design choices such
as the architecture, augmentations, or pretraining data. Yet SSL is typically
evaluated using a single metric: linear probing on ImageNet. This does not
provide much insight into why or when a model is better, now how to improve it.
To address this, we propose an SSL risk decomposition, which generalizes the
classical supervised approximation-estimation decomposition by considering
errors arising from the representation learning step. Our decomposition
consists of four error components: approximation, representation usability,
probe generalization, and encoder generalization. We provide efficient
estimators for each component and use them to analyze the effect of 30 design
choices on 169 SSL vision models evaluated on ImageNet. Our analysis gives
valuable insights for designing and using SSL models. For example, it
highlights the main sources of error and shows how to improve SSL in specific
settings (full- vs few-shot) by trading off error components. All results and
pretrained models are at https://github.com/YannDubs/SSL-Risk-Decomposition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1&quot;&gt;Yann Dubois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1&quot;&gt;Tatsunori Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Percy Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.05449">
<title>Heckerthoughts. (arXiv:2302.05449v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2302.05449</link>
<description rdf:parseType="Literal">&lt;p&gt;This manuscript is technical memoir about my work at Stanford and Microsoft
Research. Included are fundamental concepts central to machine learning and
artificial intelligence, applications of these concepts, and stories behind
their creation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1&quot;&gt;David Heckerman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.04751">
<title>Multimodal Parameter-Efficient Few-Shot Class Incremental Learning. (arXiv:2303.04751v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.04751</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-Shot Class Incremental Learning (FSCIL) is a challenging continual
learning task, where limited training examples are available during several
learning sessions. To succeed in this task, it is necessary to avoid
over-fitting new classes caused by biased distributions in the few-shot
training sets. The general approach to address this issue involves enhancing
the representational capability of a pre-defined backbone architecture by
adding special modules for backward compatibility with older classes. However,
this approach has not yet solved the dilemma of ensuring high classification
accuracy over time while reducing the gap between the performance obtained on
larger training sets and the smaller ones. In this work, we propose an
alternative approach called Continual Parameter-Efficient CLIP (CPE-CLIP) to
reduce the loss of information between different learning sessions. Instead of
adapting additional modules to address information loss, we leverage the vast
knowledge acquired by CLIP in large-scale pre-training and its effectiveness in
generalizing to new concepts. Our approach is multimodal and
parameter-efficient, relying on learnable prompts for both the language and
vision encoders to enable transfer learning across sessions. We also introduce
prompt regularization to improve performance and prevent forgetting. Our
experimental results demonstrate that CPE-CLIP significantly improves FSCIL
performance compared to state-of-the-art proposals while also drastically
reducing the number of learnable parameters and training costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DAlessandro_M/0/1/0/all/0/1&quot;&gt;Marco D&amp;#x27;Alessandro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_A/0/1/0/all/0/1&quot;&gt;Alberto Alonso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calabres_E/0/1/0/all/0/1&quot;&gt;Enrique Calabr&amp;#xe9;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galar_M/0/1/0/all/0/1&quot;&gt;Mikel Galar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13850">
<title>Towards Learning and Explaining Indirect Causal Effects in Neural Networks. (arXiv:2303.13850v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13850</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, there has been a growing interest in learning and explaining causal
effects within Neural Network (NN) models. By virtue of NN architectures,
previous approaches consider only direct and total causal effects assuming
independence among input variables. We view an NN as a structural causal model
(SCM) and extend our focus to include indirect causal effects by introducing
feedforward connections among input neurons. We propose an ante-hoc method that
captures and maintains direct, indirect, and total causal effects during NN
model training. We also propose an algorithm for quantifying learned causal
effects in an NN model and efficient approximation strategies for quantifying
causal effects in high-dimensional data. Extensive experiments conducted on
synthetic and real-world datasets demonstrate that the causal effects learned
by our ante-hoc method better approximate the ground truth effects compared to
existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_A/0/1/0/all/0/1&quot;&gt;Abbavaram Gowtham Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bachu_S/0/1/0/all/0/1&quot;&gt;Saketh Bachu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_H/0/1/0/all/0/1&quot;&gt;Harsharaj Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Godfrey_B/0/1/0/all/0/1&quot;&gt;Benin L Godfrey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1&quot;&gt;Vineeth N. Balasubramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+V_V/0/1/0/all/0/1&quot;&gt;Varshaneya V&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kar_S/0/1/0/all/0/1&quot;&gt;Satya Narayanan Kar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.14806">
<title>A Contrastive Learning Scheme with Transformer Innate Patches. (arXiv:2303.14806v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.14806</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents Contrastive Transformer, a contrastive learning scheme
using the Transformer innate patches. Contrastive Transformer enables existing
contrastive learning techniques, often used for image classification, to
benefit dense downstream prediction tasks such as semantic segmentation. The
scheme performs supervised patch-level contrastive learning, selecting the
patches based on the ground truth mask, subsequently used for hard-negative and
hard-positive sampling. The scheme applies to all vision-transformer
architectures, is easy to implement, and introduces minimal additional memory
footprint. Additionally, the scheme removes the need for huge batch sizes, as
each patch is treated as an image.
&lt;/p&gt;
&lt;p&gt;We apply and test Contrastive Transformer for the case of aerial image
segmentation, known for low-resolution data, large class imbalance, and similar
semantic classes. We perform extensive experiments to show the efficacy of the
Contrastive Transformer scheme on the ISPRS Potsdam aerial image segmentation
dataset. Additionally, we show the generalizability of our scheme by applying
it to multiple inherently different Transformer architectures. Ultimately, the
results show a consistent increase in mean IoU across all classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jyhne_S/0/1/0/all/0/1&quot;&gt;Sander Riis&amp;#xf8;en Jyhne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andersen_P/0/1/0/all/0/1&quot;&gt;Per-Arne Andersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1&quot;&gt;Morten Goodwin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07061">
<title>DroidBot-GPT: GPT-powered UI Automation for Android. (arXiv:2304.07061v5 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07061</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces DroidBot-GPT, a tool that utilizes GPT-like large
language models (LLMs) to automate the interactions with Android mobile
applications. Given a natural language description of a desired task,
DroidBot-GPT can automatically generate and execute actions that navigate the
app to complete the task. It works by translating the app GUI state information
and the available actions on the smartphone screen to natural language prompts
and asking the LLM to make a choice of actions. Since the LLM is typically
trained on a large amount of data including the how-to manuals of diverse
software applications, it has the ability to make reasonable choices of actions
based on the provided information. We evaluate DroidBot-GPT with a self-created
dataset that contains 33 tasks collected from 17 Android applications spanning
10 categories. It can successfully complete 39.39% of the tasks, and the
average partial completion progress is about 66.76%. Given the fact that our
method is fully unsupervised (no modification required from both the app and
the LLM), we believe there is great potential to enhance automation performance
with better app development paradigms and/or custom model training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hao Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaxuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanchun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05126">
<title>Comparing Foundation Models using Data Kernels. (arXiv:2305.05126v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05126</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in self-supervised learning and neural network scaling have
enabled the creation of large models, known as foundation models, which can be
easily adapted to a wide range of downstream tasks. The current paradigm for
comparing foundation models involves evaluating them with aggregate metrics on
various benchmark datasets. This method of model comparison is heavily
dependent on the chosen evaluation metric, which makes it unsuitable for
situations where the ideal metric is either not obvious or unavailable. In this
work, we present a methodology for directly comparing the embedding space
geometry of foundation models, which facilitates model comparison without the
need for an explicit evaluation metric. Our methodology is grounded in random
graph theory and enables valid hypothesis testing of embedding similarity on a
per-datum basis. Further, we demonstrate how our methodology can be extended to
facilitate population level model comparison. In particular, we show how our
framework can induce a manifold of models equipped with a distance function
that correlates strongly with several downstream metrics. We remark on the
utility of this population level model comparison as a first step towards a
taxonomic science of foundation models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duderstadt_B/0/1/0/all/0/1&quot;&gt;Brandon Duderstadt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Helm_H/0/1/0/all/0/1&quot;&gt;Hayden S. Helm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1&quot;&gt;Carey E. Priebe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07789">
<title>HPE:Answering Complex Questions over Text by Hybrid Question Parsing and Execution. (arXiv:2305.07789v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07789</link>
<description rdf:parseType="Literal">&lt;p&gt;The dominant paradigm of textual question answering systems is based on
end-to-end neural networks, which excels at answering natural language
questions but falls short on complex ones. This stands in contrast to the broad
adaptation of semantic parsing approaches over structured data sources (e.g.,
relational database, knowledge graphs), that convert natural language questions
to logical forms and execute them with query engines. Towards combining the
strengths of neural and symbolic methods, we propose a framework of question
parsing and execution on textual QA. It comprises two central pillars: (1) We
parse the question of varying complexity into an intermediate representation,
named H-expression, which is composed of simple questions as the primitives and
symbolic operations representing the relationships among them; (2) To execute
the resulting H-expressions, we design a hybrid executor, which integrates the
deterministic rules to translate the symbolic operations with a drop-in neural
reader network to answer each decomposed simple question. Hence, the proposed
framework can be viewed as a top-down question parsing followed by a bottom-up
answer backtracking. The resulting H-expressions closely guide the execution
process, offering higher precision besides better interpretability while still
preserving the advantages of the neural readers for resolving its primitive
elements. Our extensive experiments on MuSiQue, 2WikiQA, HotpotQA, and NQ show
that the proposed parsing and hybrid execution framework outperforms existing
approaches in supervised, few-shot, and zero-shot settings, while also
effectively exposing its underlying reasoning process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Ye Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1&quot;&gt;Semih Yavuz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1&quot;&gt;Rui Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1&quot;&gt;Dragomir Radev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yingbo Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10037">
<title>Can Language Models Solve Graph Problems in Natural Language?. (arXiv:2305.10037v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10037</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are increasingly adopted for a variety of tasks
with implicit graphical structures, such as planning in robotics, multi-hop
question answering or knowledge probing, structured commonsense reasoning, and
more. While LLMs have advanced the state-of-the-art on these tasks with
structure implications, whether LLMs could explicitly process textual
descriptions of graphs and structures, map them to grounded conceptual spaces,
and perform structured operations remains underexplored. To this end, we
propose NLGraph (Natural Language Graph), a comprehensive benchmark of
graph-based problem solving designed in natural language. NLGraph contains
29,370 problems, covering eight graph reasoning tasks with varying complexity
from simple tasks such as connectivity and shortest path up to complex problems
such as maximum flow and simulating graph neural networks. We evaluate LLMs
(GPT-3/4) with various prompting approaches on the NLGraph benchmark and find
that 1) language models do demonstrate preliminary graph reasoning abilities,
2) the benefit of advanced prompting and in-context learning diminishes on more
complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the
face of spurious correlations in graph and problem settings. We then propose
Build-a-Graph Prompting and Algorithmic Prompting, two instruction-based
approaches to enhance LLMs in solving natural language graph problems.
Build-a-Graph and Algorithmic prompting improve the performance of LLMs on
NLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to
solve the most complicated graph reasoning tasks in our setup with language
models remains an open research question. The NLGraph benchmark and evaluation
code are available at https://github.com/Arthur-Heng/NLGraph.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Heng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shangbin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tianxing He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zhaoxuan Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiaochuang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1&quot;&gt;Yulia Tsvetkov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12453">
<title>Non-flat ABA is an Instance of Bipolar Argumentation. (arXiv:2305.12453v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12453</link>
<description rdf:parseType="Literal">&lt;p&gt;Assumption-based Argumentation (ABA) is a well-known structured argumentation
formalism, whereby arguments and attacks between them are drawn from rules,
defeasible assumptions and their contraries. A common restriction imposed on
ABA frameworks (ABAFs) is that they are flat, i.e., each of the defeasible
assumptions can only be assumed, but not derived. While it is known that flat
ABAFs can be translated into abstract argumentation frameworks (AFs) as
proposed by Dung, no translation exists from general, possibly non-flat ABAFs
into any kind of abstract argumentation formalism. In this paper, we close this
gap and show that bipolar AFs (BAFs) can instantiate general ABAFs. To this end
we develop suitable, novel BAF semantics which borrow from the notion of
deductive support. We investigate basic properties of our BAFs, including
computational complexity, and prove the desired relation to ABAFs under several
semantics. Finally, in order to support computation and explainability, we
propose the notion of dispute trees for our BAF semantics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulbricht_M/0/1/0/all/0/1&quot;&gt;Markus Ulbricht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potyka_N/0/1/0/all/0/1&quot;&gt;Nico Potyka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rapberger_A/0/1/0/all/0/1&quot;&gt;Anna Rapberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1&quot;&gt;Francesca Toni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14387">
<title>AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14387</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) such as ChatGPT have seen widespread adoption
due to their strong instruction-following abilities. Developing these LLMs
involves a complex yet poorly understood workflow requiring training with human
feedback. Replicating and understanding this instruction-following requires
tackling three major challenges: the high cost of data collection, the lack of
trustworthy evaluation, and the absence of reference method implementations. We
address these challenges with AlpacaFarm, a simulator that enables research and
development for learning from feedback at a low cost. First, we design LLM
prompts to simulate human feedback that are 50x cheaper than crowdworkers and
display high agreement with humans. Second, we propose an automatic evaluation
and validate it against human instructions obtained on real-world interactions.
Third, we contribute reference implementations for several methods (PPO, DPO,
best-of-n, expert iteration, and more) that learn from pairwise feedback.
Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate
eleven models on 10k pairs of real human feedback and show that rankings of
models trained in AlpacaFarm match rankings of models trained on human data. As
a demonstration of the research possible in AlpacaFarm, we find that methods
that use a reward model can substantially improve over supervised fine-tuning
and that our reference PPO implementation leads to a +10% improvement in
win-rate against Davinci003. We release all components of AlpacaFarm at
https://github.com/tatsu-lab/alpaca_farm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1&quot;&gt;Yann Dubois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuechen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1&quot;&gt;Rohan Taori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulrajani_I/0/1/0/all/0/1&quot;&gt;Ishaan Gulrajani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ba_J/0/1/0/all/0/1&quot;&gt;Jimmy Ba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guestrin_C/0/1/0/all/0/1&quot;&gt;Carlos Guestrin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Percy Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1&quot;&gt;Tatsunori B. Hashimoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16727">
<title>A Novel real-time arrhythmia detection model using YOLOv8. (arXiv:2305.16727v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16727</link>
<description rdf:parseType="Literal">&lt;p&gt;In a landscape characterized by heightened connectivity and mobility, coupled
with a surge in cardiovascular ailments, the imperative to curtail healthcare
expenses through remote monitoring of cardiovascular health has become more
pronounced. The accurate detection and classification of cardiac arrhythmias
are pivotal for diagnosing individuals with heart irregularities. This study
underscores the feasibility of employing electrocardiograms (ECG) measurements
in the home environment for real-time arrhythmia detection. Presenting a fresh
application for arrhythmia detection, this paper leverages the cutting-edge
You-Only-Look-Once (YOLO)v8 algorithm to categorize single-lead ECG signals. We
introduce a novel loss-modified YOLOv8 model, fine-tuned on the MIT-BIH
arrhythmia dataset, enabling real-time continuous monitoring. The obtained
results substantiate the efficacy of our approach, with the model attaining an
average accuracy of 99.5% and 0.992 mAP@50, and a rapid detection time of 0.002
seconds on an NVIDIA Tesla V100. Our investigation exemplifies the potential of
real-time arrhythmia detection, enabling users to visually interpret the model
output within the comfort of their homes. Furthermore, this study lays the
groundwork for an extension into a real-time explainable AI (XAI) model capable
of deployment in the healthcare sector, thereby significantly advancing the
realm of healthcare solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ang_G/0/1/0/all/0/1&quot;&gt;Guang Jun Nicholas Ang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goil_A/0/1/0/all/0/1&quot;&gt;Aritejh Kr Goil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1&quot;&gt;Henryk Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lew_J/0/1/0/all/0/1&quot;&gt;Jieyi Jeric Lew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_X/0/1/0/all/0/1&quot;&gt;Xin Chun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustaffa_R/0/1/0/all/0/1&quot;&gt;Raihan Bin Ahmad Mustaffa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jason_T/0/1/0/all/0/1&quot;&gt;Timotius Jason&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woon_Z/0/1/0/all/0/1&quot;&gt;Ze Ting Woon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1&quot;&gt;Bingquan Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00789">
<title>Cross-Lingual Transfer Learning for Low-Resource Speech Translation. (arXiv:2306.00789v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00789</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper presents a novel three-step transfer learning framework for
enhancing cross-lingual transfer from high- to low-resource languages in the
downstream application of Automatic Speech Translation. The approach integrates
a semantic knowledge-distillation step into the existing two-step cross-lingual
transfer learning framework XLS-R. This extra step aims to encode semantic
knowledge in the multilingual speech encoder pre-trained via Self-Supervised
Learning using unlabeled speech. Our proposed three-step cross-lingual transfer
learning framework addresses the large cross-lingual transfer gap (TRFGap)
observed in the XLS-R framework between high-resource and low-resource
languages. We validate our proposal through extensive experiments and
comparisons on the CoVoST-2 benchmark, showing significant improvements in
translation performance, especially for low-resource languages, and a notable
reduction in the TRFGap.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khurana_S/0/1/0/all/0/1&quot;&gt;Sameer Khurana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dawalatabad_N/0/1/0/all/0/1&quot;&gt;Nauman Dawalatabad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laurent_A/0/1/0/all/0/1&quot;&gt;Antoine Laurent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vicente_L/0/1/0/all/0/1&quot;&gt;Luis Vicente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gimeno_P/0/1/0/all/0/1&quot;&gt;Pablo Gimeno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mingote_V/0/1/0/all/0/1&quot;&gt;Victoria Mingote&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1&quot;&gt;James Glass&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03034">
<title>Tackling Cooperative Incompatibility for Zero-Shot Human-AI Coordination. (arXiv:2306.03034v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03034</link>
<description rdf:parseType="Literal">&lt;p&gt;Securing coordination between AI agent and teammates (human players or AI
agents) in contexts involving unfamiliar humans continues to pose a significant
challenge in Zero-Shot Coordination. The issue of cooperative incompatibility
becomes particularly prominent when an AI agent is unsuccessful in
synchronizing with certain previously unknown partners. Traditional algorithms
have aimed to collaborate with partners by optimizing fixed objectives within a
population, fostering diversity in strategies and behaviors. However, these
techniques may lead to learning loss and an inability to cooperate with
specific strategies within the population, a phenomenon named cooperative
incompatibility in learning. In order to solve cooperative incompatibility in
learning and effectively address the problem in the context of ZSC, we
introduce the Cooperative Open-ended LEarning (COLE) framework, which
formulates open-ended objectives in cooperative games with two players using
perspectives of graph theory to evaluate and pinpoint the cooperative capacity
of each strategy. We present two practical algorithms, specifically \algo and
\algoR, which incorporate insights from game theory and graph theory. We also
show that COLE could effectively overcome the cooperative incompatibility from
theoretical and empirical analysis. Subsequently, we created an online
Overcooked human-AI experiment platform, the COLE platform, which enables easy
customization of questionnaires, model weights, and other aspects. Utilizing
the COLE platform, we enlist 130 participants for human experiments. Our
findings reveal a preference for our approach over state-of-the-art methods
using a variety of subjective metrics. Moreover, objective experimental
outcomes in the Overcooked game environment indicate that our method surpasses
existing ones when coordinating with previously unencountered AI agents and the
human proxy model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jichen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yali Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Ying Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinbing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1&quot;&gt;Wei Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11335">
<title>DamWorld: Progressive Reasoning with World Models for Robotic Manipulation. (arXiv:2306.11335v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11335</link>
<description rdf:parseType="Literal">&lt;p&gt;The research on embodied AI has greatly promoted the development of robot
manipulation. However, it still faces significant challenges in various aspects
such as benchmark construction, multi-modal perception and decision-making, and
physical execution. Previous robot manipulation simulators were primarily
designed to enrich manipulation types and types of objects while neglecting the
balance between physical manipulation and language instruction complexity in
multi-modal environments. This paper proposes a new robot manipulation
simulator and builds a comprehensive and systematic robot manipulation
benchmark with progressive reasoning tasks called SeaWave (i.e., a progressive
reasoning benchmark). It provides a standard test platform for embedded AI
agents in a multi-modal environment, which can evaluate and execute four levels
of human natural language instructions at the same time.
&lt;/p&gt;
&lt;p&gt;Previous world model-based robot manipulation work lacked research on the
perception and decision-making of complex instructions in multi-modal
environments. To this end, we propose a new world model tailored for
cross-modal robot manipulation called DamWorld. Specifically, DamWorld takes
the current visual scene and predicted execution actions based on natural
language instructions as input, and uses the next action frame to supervise the
output of the world model to force the model to learn robot manipulation
consistent with world knowledge. Compared with the renowned baselines (e.g.,
RT-1), our DamWorld improves the manipulation success rate by 5.6% on average
on four levels of progressive reasoning tasks. It is worth noting that on the
most challenging level 4 manipulation task, DamWorld still improved by 9.0%
compared to prior works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1&quot;&gt;Pengzhen Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaidong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hetao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zixuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yuhang Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Fengda Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Mas Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15079">
<title>A direct optimization algorithm for input-constrained MPC. (arXiv:2306.15079v5 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15079</link>
<description rdf:parseType="Literal">&lt;p&gt;A challenge of running a model predictive control (MPC) algorithm in a
production-embedded platform is to provide the certificate of worst-case
computation complexity, that is, its maximum execution time needs to always be
smaller than the sampling time. This article proposes for the first time a
\textit{direct} optimization algorithm for input-constrained MPC: the number of
iterations is data-independent and dependent on the problem dimension $n$, with
exact value
$\left\lceil\frac{\log(\frac{2n}{\epsilon})}{-2\log(\frac{\sqrt{2n}}{\sqrt{2n}+\sqrt{2}-1})}\right\rceil
+ 1$, where $\epsilon$ denotes a given stopping accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Liang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Braatz_R/0/1/0/all/0/1&quot;&gt;Richard D Braatz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15909">
<title>RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$. (arXiv:2306.15909v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15909</link>
<description rdf:parseType="Literal">&lt;p&gt;Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as
promising approaches for learning data-efficient RL algorithms tailored to a
given task distribution. However, these RL algorithms struggle with
long-horizon tasks and out-of-distribution tasks since they rely on recurrent
neural networks to process the sequence of experiences instead of summarizing
them into general RL components such as value functions. Moreover, even
transformers have a practical limit to the length of histories they can
efficiently reason about before training and inference costs become
prohibitive. In contrast, traditional RL algorithms are data-inefficient since
they do not leverage domain knowledge, but they do converge to an optimal
policy as more data becomes available. In this paper, we propose RL$^3$, a
principled hybrid approach that combines traditional RL and meta-RL by
incorporating task-specific action-values learned through traditional RL as an
input to the meta-RL neural network. We show that RL$^3$ earns greater
cumulative reward on long-horizon and out-of-distribution tasks compared to
RL$^2$, while maintaining the efficiency of the latter in the short term.
Experiments are conducted on both custom and benchmark discrete domains from
the meta-RL literature that exhibit a range of short-term, long-term, and
complex dependencies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatia_A/0/1/0/all/0/1&quot;&gt;Abhinav Bhatia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nashed_S/0/1/0/all/0/1&quot;&gt;Samer B. Nashed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zilberstein_S/0/1/0/all/0/1&quot;&gt;Shlomo Zilberstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15504">
<title>Exploring Format Consistency for Instruction Tuning. (arXiv:2307.15504v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15504</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction tuning has emerged as a promising approach to enhancing large
language models in following human instructions. It is shown that increasing
the diversity and number of instructions in the training data can consistently
enhance generalization performance, which facilitates a recent endeavor to
collect various instructions and integrate existing instruction tuning datasets
into larger collections. However, different users have their unique ways of
expressing instructions, and there often exist variations across different
datasets in the instruction styles and formats, i.e., format inconsistency. In
this work, we propose a framework named Unified Instruction Tuning (UIT), which
calls OpenAI APIs for automatic format transfer among different instruction
tuning datasets such as PromptSource, FLAN and CrossFit. With the framework, we
(1) demonstrate the necessity of maintaining format consistency in instruction
tuning; (2) improve the generalization performance on unseen instructions on
T5-LM-xl; (3) provide a novel perplexity-based denoising method to reduce the
noise of automatic format transfer to make the UIT framework more practical and
a smaller offline model based on GPT-J that achieves comparable format transfer
capability to OpenAI APIs to reduce costs in practice. Further analysis
regarding variations of targeted formats and other effects is intended.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1&quot;&gt;Shihao Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1&quot;&gt;Runchu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kunlun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yujia Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huadong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1&quot;&gt;Xin Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaojiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Maosong Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14284">
<title>LLM Powered Sim-to-real Transfer for Traffic Signal Control. (arXiv:2308.14284v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14284</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerous solutions are proposed for the Traffic Signal Control (TSC) tasks
aiming to provide efficient transportation and mitigate congestion waste. In
recent, promising results have been attained by Reinforcement Learning (RL)
methods through trial and error in simulators, bringing confidence in solving
cities&apos; congestion headaches. However, there still exist performance gaps when
simulator-trained policies are deployed to the real world. This issue is mainly
introduced by the system dynamic difference between the training simulator and
the real-world environments. The Large Language Models (LLMs) are trained on
mass knowledge and proved to be equipped with astonishing inference abilities.
In this work, we leverage LLMs to understand and profile the system dynamics by
a prompt-based grounded action transformation. Accepting the cloze prompt
template, and then filling in the answer based on accessible context, the
pre-trained LLM&apos;s inference ability is exploited and applied to understand how
weather conditions, traffic states, and road types influence traffic dynamics,
being aware of this, the policies&apos; action is taken and grounded based on
realistic dynamics, thus help the agent learn a more realistic policy. We
conduct experiments using DQN to show the effectiveness of the proposed
PromptGAT&apos;s ability in mitigating the performance gap from simulation to
reality (sim-to-real).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Da_L/0/1/0/all/0/1&quot;&gt;Longchao Da&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1&quot;&gt;Minchiuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1&quot;&gt;Hao Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hua Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00096">
<title>AttrSeg: Open-Vocabulary Semantic Segmentation via Attribute Decomposition-Aggregation. (arXiv:2309.00096v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00096</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-vocabulary semantic segmentation is a challenging task that requires
segmenting novel object categories at inference time. Recent studies have
explored vision-language pre-training to handle this task, but suffer from
unrealistic assumptions in practical scenarios, i.e., low-quality textual
category names. For example, this paradigm assumes that new textual categories
will be accurately and completely provided, and exist in lexicons during
pre-training. However, exceptions often happen when encountering ambiguity for
brief or incomplete names, new words that are not present in the pre-trained
lexicons, and difficult-to-describe categories for users. To address these
issues, this work proposes a novel attribute decomposition-aggregation
framework, AttrSeg, inspired by human cognition in understanding new concepts.
Specifically, in the decomposition stage, we decouple class names into diverse
attribute descriptions to complement semantic contexts from multiple
perspectives. Two attribute construction strategies are designed: using large
language models for common categories, and involving manually labeling for
human-invented categories. In the aggregation stage, we group diverse
attributes into an integrated global description, to form a discriminative
classifier that distinguishes the target object from others. One hierarchical
aggregation architecture is further proposed to achieve multi-level
aggregations, leveraging the meticulously designed clustering module. The final
results are obtained by computing the similarity between aggregated attributes
and images embeddings. To evaluate the effectiveness, we annotate three types
of datasets with attribute descriptions, and conduct extensive experiments and
ablation studies. The results show the superior performance of attribute
decomposition-aggregation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chaofan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuhuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1&quot;&gt;Chen Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanfeng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02045">
<title>Enhance Multi-domain Sentiment Analysis of Review Texts through Prompting Strategies. (arXiv:2309.02045v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02045</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have made significant strides in both scientific
research and practical applications. Existing studies have demonstrated the
state-of-the-art (SOTA) performance of LLMs in various natural language
processing tasks. However, the question of how to further enhance LLMs&apos;
performance in specific task using prompting strategies remains a pivotal
concern. This paper explores the enhancement of LLMs&apos; performance in sentiment
analysis through the application of prompting strategies. We formulate the
process of prompting for sentiment analysis tasks and introduce two novel
strategies tailored for sentiment analysis: RolePlaying (RP) prompting and
Chain-of-thought (CoT) prompting. Specifically, we also propose the RP-CoT
prompting strategy which is a combination of RP prompting and CoT prompting. We
conduct comparative experiments on three distinct domain datasets to evaluate
the effectiveness of the proposed sentiment analysis strategies. The results
demonstrate that the adoption of the proposed prompting strategies leads to a
increasing enhancement in sentiment analysis accuracy. Further, the CoT
prompting strategy exhibits a notable impact on implicit sentiment analysis,
with the RP-CoT prompting strategy delivering the most superior performance
among all strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yajing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zongwei Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.04339">
<title>Online Submodular Maximization via Online Convex Optimization. (arXiv:2309.04339v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.04339</link>
<description rdf:parseType="Literal">&lt;p&gt;We study monotone submodular maximization under general matroid constraints
in the online setting. We prove that online optimization of a large class of
submodular functions, namely, weighted threshold potential functions, reduces
to online convex optimization (OCO). This is precisely because functions in
this class admit a concave relaxation; as a result, OCO policies, coupled with
an appropriate rounding scheme, can be used to achieve sublinear regret in the
combinatorial setting. We show that our reduction extends to many different
versions of the online learning problem, including the dynamic regret, bandit,
and optimistic-learning settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salem_T/0/1/0/all/0/1&quot;&gt;Tareq Si Salem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozcan_G/0/1/0/all/0/1&quot;&gt;G&amp;#xf6;zde &amp;#xd6;zcan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolaou_I/0/1/0/all/0/1&quot;&gt;Iasonas Nikolaou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terzi_E/0/1/0/all/0/1&quot;&gt;Evimaria Terzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ioannidis_S/0/1/0/all/0/1&quot;&gt;Stratis Ioannidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08637">
<title>TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild. (arXiv:2309.08637v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08637</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models with instruction-following abilities have
revolutionized the field of artificial intelligence. These models show
exceptional generalizability to tackle various real-world tasks through their
natural language interfaces. However, their performance heavily relies on
high-quality exemplar data, which is often difficult to obtain. This challenge
is further exacerbated when it comes to multimodal instruction following. We
introduce TextBind, an almost annotation-free framework for empowering larger
language models with the multi-turn interleaved multimodal
instruction-following capabilities. Our approach requires only image-caption
pairs and generates multi-turn multimodal instruction-response conversations
from a language model. To accommodate interleaved image-text inputs and
outputs, we devise MIM, a language model-centric architecture that seamlessly
integrates image encoder and decoder models. We release our dataset, model, and
demo to foster future research in the area of multimodal instruction following.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huayang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Siheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1&quot;&gt;Deng Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lemao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1&quot;&gt;Taro Watanabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yujiu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1&quot;&gt;Shuming Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09150">
<title>Can Large Language Models Understand Real-World Complex Instructions?. (arXiv:2309.09150v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09150</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) can understand human instructions, showing their
potential for pragmatic applications beyond traditional NLP tasks. However,
they still struggle with complex instructions, which can be either complex task
descriptions that require multiple tasks and constraints, or complex input that
contains long context, noise, heterogeneous information and multi-turn format.
Due to these features, LLMs often ignore semantic constraints from task
descriptions, generate incorrect formats, violate length or sample count
constraints, and be unfaithful to the input text. Existing benchmarks are
insufficient to assess LLMs&apos; ability to understand complex instructions, as
they are close-ended and simple. To bridge this gap, we propose CELLO, a
benchmark for evaluating LLMs&apos; ability to follow complex instructions
systematically. We design eight features for complex instructions and construct
a comprehensive evaluation dataset from real-world scenarios. We also establish
four criteria and develop corresponding metrics, as current ones are
inadequate, biased or too strict and coarse-grained. We compare the performance
of representative Chinese-oriented and English-oriented models in following
complex instructions through extensive experiments. Resources of CELLO are
publicly available at https://github.com/Abbey4799/CELLO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qianyu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1&quot;&gt;Jie Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenhao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lina Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jin Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qianxi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xunzhe Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lida Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuncheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1&quot;&gt;Haoning Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zihan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shisong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yikai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1&quot;&gt;Zhouhong Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jiaqing Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yanghua Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09270">
<title>Continuous Modeling of the Denoising Process for Speech Enhancement Based on Deep Learning. (arXiv:2309.09270v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09270</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we explore a continuous modeling approach for
deep-learning-based speech enhancement, focusing on the denoising process. We
use a state variable to indicate the denoising process. The starting state is
noisy speech and the ending state is clean speech. The noise component in the
state variable decreases with the change of the state index until the noise
component is 0. During training, a UNet-like neural network learns to estimate
every state variable sampled from the continuous denoising process. In testing,
we introduce a controlling factor as an embedding, ranging from zero to one, to
the neural network, allowing us to control the level of noise reduction. This
approach enables controllable speech enhancement and is adaptable to various
application scenarios. Experimental results indicate that preserving a small
amount of noise in the clean target benefits speech enhancement, as evidenced
by improvements in both objective speech measures and automatic speech
recognition performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zilu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;Jun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;CHin-Hui Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09272">
<title>Deep Neighbor Layer Aggregation for Lightweight Self-Supervised Monocular Depth Estimation. (arXiv:2309.09272v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09272</link>
<description rdf:parseType="Literal">&lt;p&gt;With the frequent use of self-supervised monocular depth estimation in
robotics and autonomous driving, the model&apos;s efficiency is becoming
increasingly important. Most current approaches apply much larger and more
complex networks to improve the precision of depth estimation. Some researchers
incorporated Transformer into self-supervised monocular depth estimation to
achieve better performance. However, this method leads to high parameters and
high computation. We present a fully convolutional depth estimation network
using contextual feature fusion. Compared to UNet++ and HRNet, we use
high-resolution and low-resolution features to reserve information on small
targets and fast-moving objects instead of long-range fusion. We further
promote depth estimation results employing lightweight channel attention based
on convolution in the decoder stage. Our method reduces the parameters without
sacrificing accuracy. Experiments on the KITTI benchmark show that our method
can get better results than many large models, such as Monodepth2, with only 30
parameters. The source code is available at
https://github.com/boyagesmile/DNA-Depth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boya_W/0/1/0/all/0/1&quot;&gt;Wang Boya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shuo_W/0/1/0/all/0/1&quot;&gt;Wang Shuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Ye Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziwen_D/0/1/0/all/0/1&quot;&gt;Dou Ziwen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01132">
<title>Automated Evaluation of Classroom Instructional Support with LLMs and BoWs: Connecting Global Predictions to Specific Feedback. (arXiv:2310.01132v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01132</link>
<description rdf:parseType="Literal">&lt;p&gt;With the aim to provide teachers with more specific, frequent, and actionable
feedback about their teaching, we explore how Large Language Models (LLMs) can
be used to estimate ``Instructional Support&apos;&apos; domain scores of the CLassroom
Assessment Scoring System (CLASS), a widely used observation protocol. We
design a machine learning architecture that uses either zero-shot prompting of
Meta&apos;s Llama2, and/or a classic Bag of Words (BoW) model, to classify
individual utterances of teachers&apos; speech (transcribed automatically using
OpenAI&apos;s Whisper) for the presence of Instructional Support. Then, these
utterance-level judgments are aggregated over an entire 15-min observation
session to estimate a global CLASS score. Experiments on two CLASS-coded
datasets of toddler and pre-kindergarten classrooms indicate that (1) automatic
CLASS Instructional Support estimation accuracy using the proposed method
(Pearson $R$ up to $0.47$) approaches human inter-rater reliability (up to
$R=0.55$); (2) LLMs yield slightly greater accuracy than BoW for this task,
though the best models often combined features extracted from both LLM and BoW;
and (3) for classifying individual utterances, there is still room for
improvement of automated methods compared to human-level judgments. Finally,
(4) we illustrate how the model&apos;s outputs can be visualized at the utterance
level to provide teachers with explainable feedback on which utterances were
most positively or negatively correlated with specific CLASS dimensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whitehill_J/0/1/0/all/0/1&quot;&gt;Jacob Whitehill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+LoCasale_Crouch_J/0/1/0/all/0/1&quot;&gt;Jennifer LoCasale-Crouch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01775">
<title>STAMP: Differentiable Task and Motion Planning via Stein Variational Gradient Descent. (arXiv:2310.01775v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01775</link>
<description rdf:parseType="Literal">&lt;p&gt;Planning for many manipulation tasks, such as using tools or assembling
parts, often requires both symbolic and geometric reasoning. Task and Motion
Planning (TAMP) algorithms typically solve these problems by conducting a tree
search over high-level task sequences while checking for kinematic and dynamic
feasibility. This can be inefficient as the width of the tree can grow
exponentially with the number of possible actions and objects. In this paper,
we propose a novel approach to TAMP that relaxes discrete-and-continuous TAMP
problems into inference problems on a continuous domain. Our method, Stein Task
and Motion Planning (STAMP) subsequently solves this new problem using a
gradient-based variational inference algorithm called Stein Variational
Gradient Descent, by obtaining gradients from a parallelized differentiable
physics simulator. By introducing relaxations to the discrete variables,
leveraging parallelization, and approaching TAMP as an Bayesian inference
problem, our method is able to efficiently find multiple diverse plans in a
single optimization run. We demonstrate our method on two TAMP problems and
benchmark them against existing TAMP baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yewon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Philip Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jatavallabhula_K/0/1/0/all/0/1&quot;&gt;Krishna Murthy Jatavallabhula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Andrew Z. Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damken_F/0/1/0/all/0/1&quot;&gt;Fabian Damken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heiden_E/0/1/0/all/0/1&quot;&gt;Eric Heiden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1&quot;&gt;Kevin Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowrouzezahrai_D/0/1/0/all/0/1&quot;&gt;Derek Nowrouzezahrai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramos_F/0/1/0/all/0/1&quot;&gt;Fabio Ramos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shkurti_F/0/1/0/all/0/1&quot;&gt;Florian Shkurti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04477">
<title>Higher-Order DeepTrails: Unified Approach to *Trails. (arXiv:2310.04477v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04477</link>
<description rdf:parseType="Literal">&lt;p&gt;Analyzing, understanding, and describing human behavior is advantageous in
different settings, such as web browsing or traffic navigation. Understanding
human behavior naturally helps to improve and optimize the underlying
infrastructure or user interfaces. Typically, human navigation is represented
by sequences of transitions between states. Previous work suggests to use
hypotheses, representing different intuitions about the navigation to analyze
these transitions. To mathematically grasp this setting, first-order Markov
chains are used to capture the behavior, consequently allowing to apply
different kinds of graph comparisons, but comes with the inherent drawback of
losing information about higher-order dependencies within the sequences. To
this end, we propose to analyze entire sequences using autoregressive language
models, as they are traditionally used to model higher-order dependencies in
sequences. We show that our approach can be easily adapted to model different
settings introduced in previous work, namely HypTrails, MixedTrails and even
SubTrails, while at the same time bringing unique advantages: 1. Modeling
higher-order dependencies between state transitions, while 2. being able to
identify short comings in proposed hypotheses, and 3. naturally introducing a
unified approach to model all settings. To show the expressiveness of our
approach, we evaluate our approach on different synthetic datasets and conclude
with an exemplary analysis of a real-world dataset, examining the behavior of
users who interact with voice assistants.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koopmann_T/0/1/0/all/0/1&quot;&gt;Tobias Koopmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_J/0/1/0/all/0/1&quot;&gt;Jan Pfister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markus_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Markus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carolus_A/0/1/0/all/0/1&quot;&gt;Astrid Carolus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wienrich_C/0/1/0/all/0/1&quot;&gt;Carolin Wienrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hotho_A/0/1/0/all/0/1&quot;&gt;Andreas Hotho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06441">
<title>Stepwise functional refoundation of relational concept analysis. (arXiv:2310.06441v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06441</link>
<description rdf:parseType="Literal">&lt;p&gt;Relational concept analysis (RCA) is an extension of formal concept analysis
allowing to deal with several related contexts simultaneously. It has been
designed for learning description logic theories from data and used within
various applications. A puzzling observation about RCA is that it returns a
single family of concept lattices although, when the data feature circular
dependencies, other solutions may be considered acceptable. The semantics of
RCA, provided in an operational way, does not shed light on this issue. In this
report, we define these acceptable solutions as those families of concept
lattices which belong to the space determined by the initial contexts
(well-formed), cannot scale new attributes (saturated), and refer only to
concepts of the family (self-supported). We adopt a functional view on the RCA
process by defining the space of well-formed solutions and two functions on
that space: one expansive and the other contractive. We show that the
acceptable solutions are the common fixed points of both functions. This is
achieved step-by-step by starting from a minimal version of RCA that considers
only one single context defined on a space of contexts and a space of lattices.
These spaces are then joined into a single space of context-lattice pairs,
which is further extended to a space of indexed families of context-lattice
pairs representing the objects manippulated by RCA. We show that RCA returns
the least element of the set of acceptable solutions. In addition, it is
possible to build dually an operation that generates its greatest element. The
set of acceptable solutions is a complete sublattice of the interval between
these two elements. Its structure and how the defined functions traverse it are
studied in detail.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Euzenat_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xf4;me Euzenat&lt;/a&gt; (MOEX )</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07535">
<title>Fairness under Covariate Shift: Improving Fairness-Accuracy tradeoff with few Unlabeled Test Samples. (arXiv:2310.07535v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07535</link>
<description rdf:parseType="Literal">&lt;p&gt;Covariate shift in the test data is a common practical phenomena that can
significantly downgrade both the accuracy and the fairness performance of the
model. Ensuring fairness across different sensitive groups under covariate
shift is of paramount importance due to societal implications like criminal
justice. We operate in the unsupervised regime where only a small set of
unlabeled test samples along with a labeled training set is available. Towards
improving fairness under this highly challenging yet realistic scenario, we
make three contributions. First is a novel composite weighted entropy based
objective for prediction accuracy which is optimized along with a
representation matching loss for fairness. We experimentally verify that
optimizing with our loss formulation outperforms a number of state-of-the-art
baselines in the pareto sense with respect to the fairness-accuracy tradeoff on
several standard datasets. Our second contribution is a new setting we term
Asymmetric Covariate Shift that, to the best of our knowledge, has not been
studied before. Asymmetric covariate shift occurs when distribution of
covariates of one group shifts significantly compared to the other groups and
this happens when a dominant group is over-represented. While this setting is
extremely challenging for current baselines, We show that our proposed method
significantly outperforms them. Our third contribution is theoretical, where we
show that our weighted entropy term along with prediction loss on the training
set approximates test loss under covariate shift. Empirically and through
formal sample complexity bounds, we show that this approximation to the unseen
test loss does not depend on importance sampling variance which affects many
other baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1&quot;&gt;Shreyas Havaldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chauhan_J/0/1/0/all/0/1&quot;&gt;Jatin Chauhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1&quot;&gt;Karthikeyan Shanmugam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nandy_J/0/1/0/all/0/1&quot;&gt;Jay Nandy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghuveer_A/0/1/0/all/0/1&quot;&gt;Aravindan Raghuveer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09866">
<title>Federated Multi-Objective Learning. (arXiv:2310.09866v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09866</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, multi-objective optimization (MOO) emerges as a foundational
problem underpinning many multi-agent multi-task learning applications.
However, existing algorithms in MOO literature remain limited to centralized
learning settings, which do not satisfy the distributed nature and data privacy
needs of such multi-agent multi-task learning applications. This motivates us
to propose a new federated multi-objective learning (FMOL) framework with
multiple clients distributively and collaboratively solving an MOO problem
while keeping their training data private. Notably, our FMOL framework allows a
different set of objective functions across different clients to support a wide
range of applications, which advances and generalizes the MOO formulation to
the federated learning paradigm for the first time. For this FMOL framework, we
propose two new federated multi-objective optimization (FMOO) algorithms called
federated multi-gradient descent averaging (FMGDA) and federated stochastic
multi-gradient descent averaging (FSMGDA). Both algorithms allow local updates
to significantly reduce communication costs, while achieving the {\em same}
convergence rates as those of their algorithmic counterparts in the
single-objective federated learning. Our extensive experiments also corroborate
the efficacy of our proposed FMOO algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Haibo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhuqing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1&quot;&gt;Chaosheng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Momma_M/0/1/0/all/0/1&quot;&gt;Michinari Momma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19251">
<title>Pre-trained Recommender Systems: A Causal Debiasing Perspective. (arXiv:2310.19251v4 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19251</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies on pre-trained vision/language models have demonstrated the
practical benefit of a new, promising solution-building paradigm in AI where
models can be pre-trained on broad data describing a generic task space and
then adapted successfully to solve a wide range of downstream tasks, even when
training data is severely limited (e.g., in zero- or few-shot learning
scenarios). Inspired by such progress, we investigate in this paper the
possibilities and challenges of adapting such a paradigm to the context of
recommender systems, which is less investigated from the perspective of
pre-trained model. In particular, we propose to develop a generic recommender
that captures universal interaction patterns by training on generic user-item
interaction data extracted from different domains, which can then be fast
adapted to improve few-shot learning performance in unseen new domains (with
limited data).
&lt;/p&gt;
&lt;p&gt;However, unlike vision/language data which share strong conformity in the
semantic space, universal patterns underlying recommendation data collected
across different domains (e.g., different countries or different E-commerce
platforms) are often occluded by both in-domain and cross-domain biases
implicitly imposed by the cultural differences in their user and item bases, as
well as their uses of different e-commerce platforms. As shown in our
experiments, such heterogeneous biases in the data tend to hinder the
effectiveness of the pre-trained model. To address this challenge, we further
introduce and formalize a causal debiasing perspective, which is substantiated
via a hierarchical Bayesian deep learning model, named PreRec. Our empirical
studies on real-world data show that the proposed model could significantly
improve the recommendation performance in zero- and few-shot learning settings
under both cross-market and cross-platform scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Ziqian Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Hao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_N/0/1/0/all/0/1&quot;&gt;Nghia Trong Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1&quot;&gt;Branislav Kveton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deoras_A/0/1/0/all/0/1&quot;&gt;Anoop Deoras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02462">
<title>Levels of AGI: Operationalizing Progress on the Path to AGI. (arXiv:2311.02462v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02462</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a framework for classifying the capabilities and behavior of
Artificial General Intelligence (AGI) models and their precursors. This
framework introduces levels of AGI performance, generality, and autonomy. It is
our hope that this framework will be useful in an analogous way to the levels
of autonomous driving, by providing a common language to compare models, assess
risks, and measure progress along the path to AGI. To develop our framework, we
analyze existing definitions of AGI, and distill six principles that a useful
ontology for AGI should satisfy. These principles include focusing on
capabilities rather than mechanisms; separately evaluating generality and
performance; and defining stages along the path toward AGI, rather than
focusing on the endpoint. With these principles in mind, we propose &apos;Levels of
AGI&apos; based on depth (performance) and breadth (generality) of capabilities, and
reflect on how current systems fit into this ontology. We discuss the
challenging requirements for future benchmarks that quantify the behavior and
capabilities of AGI models against these levels. Finally, we discuss how these
levels of AGI interact with deployment considerations such as autonomy and
risk, and emphasize the importance of carefully selecting Human-AI Interaction
paradigms for responsible and safe deployment of highly capable AI systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morris_M/0/1/0/all/0/1&quot;&gt;Meredith Ringel Morris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohl_dickstein_J/0/1/0/all/0/1&quot;&gt;Jascha Sohl-dickstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiedel_N/0/1/0/all/0/1&quot;&gt;Noah Fiedel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warkentin_T/0/1/0/all/0/1&quot;&gt;Tris Warkentin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dafoe_A/0/1/0/all/0/1&quot;&gt;Allan Dafoe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1&quot;&gt;Aleksandra Faust&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farabet_C/0/1/0/all/0/1&quot;&gt;Clement Farabet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Legg_S/0/1/0/all/0/1&quot;&gt;Shane Legg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03035">
<title>GTP-ViT: Efficient Vision Transformers via Graph-based Token Propagation. (arXiv:2311.03035v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03035</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) have revolutionized the field of computer vision,
yet their deployments on resource-constrained devices remain challenging due to
high computational demands. To expedite pre-trained ViTs, token pruning and
token merging approaches have been developed, which aim at reducing the number
of tokens involved in the computation. However, these methods still have some
limitations, such as image information loss from pruned tokens and inefficiency
in the token-matching process. In this paper, we introduce a novel Graph-based
Token Propagation (GTP) method to resolve the challenge of balancing model
efficiency and information preservation for efficient ViTs. Inspired by graph
summarization algorithms, GTP meticulously propagates less significant tokens&apos;
information to spatially and semantically connected tokens that are of greater
importance. Consequently, the remaining few tokens serve as a summarization of
the entire token graph, allowing the method to reduce computational complexity
while preserving essential information of eliminated tokens. Combined with an
innovative token selection strategy, GTP can efficiently identify image tokens
to be propagated. Extensive experiments have validated GTP&apos;s effectiveness,
demonstrating both efficiency and performance improvements. Specifically, GTP
decreases the computational complexity of both DeiT-S and DeiT-B by up to 26%
with only a minimal 0.3% accuracy drop on ImageNet-1K without finetuning, and
remarkably surpasses the state-of-the-art token merging method on various
backbones at an even faster inference speed. The source code is available at
https://github.com/Ackesnal/GTP-ViT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xuwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yudong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yanping Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhewei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiajun Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07946">
<title>The Impact of Adversarial Node Placement in Decentralized Federated Learning Networks. (arXiv:2311.07946v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07946</link>
<description rdf:parseType="Literal">&lt;p&gt;As Federated Learning (FL) grows in popularity, new decentralized frameworks
are becoming widespread. These frameworks leverage the benefits of
decentralized environments to enable fast and energy-efficient inter-device
communication. However, this growing popularity also intensifies the need for
robust security measures. While existing research has explored various aspects
of FL security, the role of adversarial node placement in decentralized
networks remains largely unexplored. This paper addresses this gap by analyzing
the performance of decentralized FL for various adversarial placement
strategies when adversaries can jointly coordinate their placement within a
network. We establish two baseline strategies for placing adversarial node:
random placement and network centrality-based placement. Building on this
foundation, we propose a novel attack algorithm that prioritizes adversarial
spread over adversarial centrality by maximizing the average network distance
between adversaries. We show that the new attack algorithm significantly
impacts key performance metrics such as testing accuracy, outperforming the
baseline frameworks by between 9% and 66.5% for the considered setups. Our
findings provide valuable insights into the vulnerabilities of decentralized FL
systems, setting the stage for future research aimed at developing more secure
and robust decentralized FL frameworks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piaseczny_A/0/1/0/all/0/1&quot;&gt;Adam Piaseczny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruzomberka_E/0/1/0/all/0/1&quot;&gt;Eric Ruzomberka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parasnis_R/0/1/0/all/0/1&quot;&gt;Rohit Parasnis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1&quot;&gt;Christopher G. Brinton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08648">
<title>Explore Spurious Correlations at the Concept Level in Language Models for Text Classification. (arXiv:2311.08648v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08648</link>
<description rdf:parseType="Literal">&lt;p&gt;Language models (LMs) have achieved notable success in numerous NLP tasks,
employing both fine-tuning and in-context learning (ICL) methods. While
language models demonstrate exceptional performance, they face robustness
challenges due to spurious correlations arising from imbalanced label
distributions in training data or ICL exemplars. Previous research has
primarily concentrated on word, phrase, and syntax features, neglecting the
concept level, often due to the absence of concept labels and difficulty in
identifying conceptual content in input texts. This paper introduces two main
contributions. First, we employ ChatGPT to assign concept labels to texts,
assessing concept bias in models during fine-tuning or ICL on test data. We
find that LMs, when encountering spurious correlations between a concept and a
label in training or prompts, resort to shortcuts for predictions. Second, we
introduce a data rebalancing technique that incorporates ChatGPT-generated
counterfactual data, thereby balancing label distribution and mitigating
spurious correlations. Our method&apos;s efficacy, surpassing traditional token
removal approaches, is validated through extensive testing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuhang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Paiheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1&quot;&gt;Bang An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ai_W/0/1/0/all/0/1&quot;&gt;Wei Ai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Furong Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10776">
<title>Retrieval-Augmented Generative Agent for Reaction Condition Recommendation in Chemical Synthesis. (arXiv:2311.10776v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10776</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent artificial intelligence (AI) research plots a promising future of
automatic chemical reactions within the chemistry society. This study presents
a transformative AI agent that automates the reaction condition recommendation
(RCR) task in chemistry using retrieval-augmented generation (RAG) technology.
By emulating expert chemists search and analysis strategies, the agent employs
large language models (LLMs) to interrogate molecular databases and distill
critical data from online literature. Further, the AI agent is equipped with
our novel reaction fingerprint developed for the RCR task. Thanks to the RAG
technology, our agent uses updated online databases as knowledge sources,
significantly outperforming conventional AIs confined to the fixed knowledge
within its training data. The resulting system can significantly reduce
chemists workload, allowing them to focus on more fundamental and creative
scientific problems. This significant advancement brings closer computational
techniques and chemical research, marking a considerable leap toward harnessing
AI&apos;s full capabilities in chemical discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kexin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junyou Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kunyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yuyang Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jiahui Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiamin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lanqing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jiezhong Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jianzhang Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1&quot;&gt;Qun Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1&quot;&gt;Pheng Ann Heng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guangyong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11249">
<title>Open Set Dandelion Network for IoT Intrusion Detection. (arXiv:2311.11249v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11249</link>
<description rdf:parseType="Literal">&lt;p&gt;As IoT devices become widely, it is crucial to protect them from malicious
intrusions. However, the data scarcity of IoT limits the applicability of
traditional intrusion detection methods, which are highly data-dependent. To
address this, in this paper we propose the Open-Set Dandelion Network (OSDN)
based on unsupervised heterogeneous domain adaptation in an open-set manner.
The OSDN model performs intrusion knowledge transfer from the knowledge-rich
source network intrusion domain to facilitate more accurate intrusion detection
for the data-scarce target IoT intrusion domain. Under the open-set setting, it
can also detect newly-emerged target domain intrusions that are not observed in
the source domain. To achieve this, the OSDN model forms the source domain into
a dandelion-like feature space in which each intrusion category is compactly
grouped and different intrusion categories are separated, i.e., simultaneously
emphasising inter-category separability and intra-category compactness. The
dandelion-based target membership mechanism then forms the target dandelion.
Then, the dandelion angular separation mechanism achieves better inter-category
separability, and the dandelion embedding alignment mechanism further aligns
both dandelions in a finer manner. To promote intra-category compactness, the
discriminating sampled dandelion mechanism is used. Assisted by the intrusion
classifier trained using both known and generated unknown intrusion knowledge,
a semantic dandelion correction mechanism emphasises easily-confused categories
and guides better inter-category separability. Holistically, these mechanisms
form the OSDN model that effectively performs intrusion knowledge transfer to
benefit IoT intrusion detection. Comprehensive experiments on several intrusion
datasets verify the effectiveness of the OSDN model, outperforming three
state-of-the-art baseline methods by 16.9%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiashu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Hao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kent_K/0/1/0/all/0/1&quot;&gt;Kenneth B. Kent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yen_J/0/1/0/all/0/1&quot;&gt;Jerome Yen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chengzhong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14457">
<title>How to ensure a safe control strategy? Towards a SRL for urban transit autonomous operation. (arXiv:2311.14457v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14457</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning has gradually shown its latent decision-making
ability in urban rail transit autonomous operation. However, since
reinforcement learning can not neither guarantee safety during learning nor
execution, this is still one of the major obstacles to the practical
application of reinforcement learning. Given this drawback, reinforcement
learning applied in the safety-critical autonomous operation domain remains
challenging without generating a safe control command sequence that avoids
overspeed operations. Therefore, a SSA-DRL framework is proposed in this paper
for safe intelligent control of urban rail transit autonomous operation trains.
The proposed framework is combined with linear temporal logic, reinforcement
learning and Monte Carlo tree search and consists of four mainly module: a
post-posed shielding, a searching tree module, a DRL framework and an
additional actor. Furthermore, the output of the framework can meet speed
constraint, schedule constraint and optimize the operation process. Finally,
the proposed SSA-DRL framework for decision-making in urban rail transit
autonomous operation is evaluated in sixteen different sections, and its
effectiveness is demonstrated through an ablation experiment and comparison
with the scheduled operation plan.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zicong Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16167">
<title>Moving Sampling Physics-informed Neural Networks induced by Moving Mesh PDE. (arXiv:2311.16167v2 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16167</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose an end-to-end adaptive sampling neural network
(MMPDE-Net) based on the moving mesh method, which can adaptively generate new
sampling points by solving the moving mesh PDE. This model focuses on improving
the quality of sampling points generation. Moreover, we develop an iterative
algorithm based on MMPDE-Net, which makes the sampling points more precise and
controllable. Since MMPDE-Net is a framework independent of the deep learning
solver, we combine it with physics-informed neural networks (PINN) to propose
moving sampling PINN (MS-PINN) and demonstrate its effectiveness by error
analysis under some assumptions. Finally, we demonstrate the performance
improvement of MS-PINN compared to PINN through numerical experiments of four
typical examples, which numerically verify the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qihong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yangtao Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qiaolin He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18520">
<title>Calibration-free online test-time adaptation for electroencephalography motor imagery decoding. (arXiv:2311.18520v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18520</link>
<description rdf:parseType="Literal">&lt;p&gt;Providing a promising pathway to link the human brain with external devices,
Brain-Computer Interfaces (BCIs) have seen notable advancements in decoding
capabilities, primarily driven by increasingly sophisticated techniques,
especially deep learning. However, achieving high accuracy in real-world
scenarios remains a challenge due to the distribution shift between sessions
and subjects. In this paper we will explore the concept of online test-time
adaptation (OTTA) to continuously adapt the model in an unsupervised fashion
during inference time. Our approach guarantees the preservation of privacy by
eliminating the requirement to access the source data during the adaptation
process. Additionally, OTTA achieves calibration-free operation by not
requiring any session- or subject-specific data. We will investigate the task
of electroencephalography (EEG) motor imagery decoding using a lightweight
architecture together with different OTTA techniques like alignment, adaptive
batch normalization, and entropy minimization. We examine two datasets and
three distinct data settings for a comprehensive analysis. Our adaptation
methods produce state-of-the-art results, potentially instigating a shift in
transfer learning for BCI decoding towards online adaptation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wimpff_M/0/1/0/all/0/1&quot;&gt;Martin Wimpff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dobler_M/0/1/0/all/0/1&quot;&gt;Mario D&amp;#xf6;bler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06786">
<title>Mixture-of-Linear-Experts for Long-term Time Series Forecasting. (arXiv:2312.06786v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06786</link>
<description rdf:parseType="Literal">&lt;p&gt;Long-term time series forecasting (LTSF) aims to predict future values of a
time series given the past values. The current state-of-the-art (SOTA) on this
problem is attained in some cases by linear-centric models, which primarily
feature a linear mapping layer. However, due to their inherent simplicity, they
are not able to adapt their prediction rules to periodic changes in time series
patterns. To address this challenge, we propose a Mixture-of-Experts-style
augmentation for linear-centric models and propose Mixture-of-Linear-Experts
(MoLE). Instead of training a single model, MoLE trains multiple linear-centric
models (i.e., experts) and a router model that weighs and mixes their outputs.
While the entire framework is trained end-to-end, each expert learns to
specialize in a specific temporal pattern, and the router model learns to
compose the experts adaptively. Experiments show that MoLE reduces forecasting
error of linear-centric models, including DLinear, RLinear, and RMLP, in over
78% of the datasets and settings we evaluated. By using MoLE existing
linear-centric models can achieve SOTA LTSF results in 68% of the experiments
that PatchTST reports and we compare to, whereas existing single-head
linear-centric models achieve SOTA results in only 25% of cases. Additionally,
MoLE models achieve SOTA in all settings for the newly released Weather2K
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_R/0/1/0/all/0/1&quot;&gt;Ronghao Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zinan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuaiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fanti_G/0/1/0/all/0/1&quot;&gt;Giulia Fanti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09007">
<title>LLMind: Orchestrating AI and IoT with LLMs for Complex Task Execution. (arXiv:2312.09007v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09007</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce LLMind, an AI framework that utilizes large
language models (LLMs) as a central orchestrator. The framework integrates LLMs
with domain-specific AI modules, enabling IoT devices to collaborate
effectively in executing complex tasks. The LLM engages in natural
conversations with human users via a user-friendly social media platform to
come up with a plan to execute complex tasks. In particular, the execution of a
complex task, which may involve the collaborations of multiple domain-specific
AI modules and IoT devices, is realized through a control script. The LLM
generates the control script using a Language-Code transformation approach
based on finite-state machines (FSMs). The framework also incorporates semantic
analysis and response optimization techniques to enhance speed and
effectiveness. Ultimately, this framework is designed not only to innovate IoT
device control and enrich user experiences but also to foster an intelligent
and integrated IoT device ecosystem that evolves and becomes more sophisticated
through continuing user and machine interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1&quot;&gt;Hongwei Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yuyang Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1&quot;&gt;Yulin Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liew_S/0/1/0/all/0/1&quot;&gt;Soung Chang Liew&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09963">
<title>Symbolic Numeric Planning with Patterns. (arXiv:2312.09963v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09963</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel approach for solving linear numeric
planning problems, called Symbolic Pattern Planning. Given a planning problem
$\Pi$, a bound $n$ and a pattern -- defined as an arbitrary sequence of actions
-- we encode the problem of finding a plan for $\Pi$ with bound $n$ as a
formula with fewer variables and/or clauses than the state-of-the-art rolled-up
and relaxed-relaxed-$\exists$ encodings. More importantly, we prove that for
any given bound, it is never the case that the latter two encodings allow
finding a valid plan while ours does not. On the experimental side, we consider
6 other planning systems -- including the ones which participated in this
year&apos;s International Planning Competition (IPC) -- and we show that our planner
Patty has remarkably good comparative performances on this year&apos;s IPC problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardellini_M/0/1/0/all/0/1&quot;&gt;Matteo Cardellini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giunchiglia_E/0/1/0/all/0/1&quot;&gt;Enrico Giunchiglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maratea_M/0/1/0/all/0/1&quot;&gt;Marco Maratea&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10578">
<title>SAME: Sample Reconstruction against Model Extraction Attacks. (arXiv:2312.10578v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10578</link>
<description rdf:parseType="Literal">&lt;p&gt;While deep learning models have shown significant performance across various
domains, their deployment needs extensive resources and advanced computing
infrastructure. As a solution, Machine Learning as a Service (MLaaS) has
emerged, lowering the barriers for users to release or productize their deep
learning models. However, previous studies have highlighted potential privacy
and security concerns associated with MLaaS, and one primary threat is model
extraction attacks. To address this, there are many defense solutions but they
suffer from unrealistic assumptions and generalization issues, making them less
practical for reliable protection. Driven by these limitations, we introduce a
novel defense mechanism, SAME, based on the concept of sample reconstruction.
This strategy imposes minimal prerequisites on the defender&apos;s capabilities,
eliminating the need for auxiliary Out-of-Distribution (OOD) datasets, user
query history, white-box model access, and additional intervention during model
training. It is compatible with existing active defense methods. Our extensive
experiments corroborate the superior efficacy of SAME over state-of-the-art
solutions. Our code is available at https://github.com/xythink/SAME.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shiqian Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11193">
<title>&quot;Paraphrasing The Original Text&quot; Makes High Accuracy Long-Context QA. (arXiv:2312.11193v6 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11193</link>
<description rdf:parseType="Literal">&lt;p&gt;Most open-source generative language models currently have a context window
of no more than 4k, limiting their ability when facing long text. Even models
with longer context windows cannot guarantee satisfactory accuracy on
long-context problems. To tackle this issue, we explore from the perspective of
training data and theoretically demonstrate that improving the capability to
handle long contexts requires &quot;effective&quot; rather than simply &quot;long&quot; data. Based
on this insight, we propose using the &quot;original text paraphrasing&quot; task and
successfully extend the context window of existing models to 32k through a
low-cost and effective method. Our fine-tuned model achieves state-of-the-art
accuracy in multi-document-QA among models of comparable scale. The model and
training data have been made available on
HuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) and
WiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yijiong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11973">
<title>Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11973</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the
existence of efficient subnetworks within larger, dense networks, a
high-performing Winning Subnetwork (WSN) in terms of task performance under
appropriate sparsity conditions is considered for various continual learning
tasks. It leverages pre-existing weights from dense networks to achieve
efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot
Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft
subnetwork (SoftNet) is designed to prevent overfitting when the data samples
are scarce. Furthermore, the sparse reuse of WSN weights is considered for
Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO)
within WSN is considered. It enables compact encoding of videos and identifies
reusable subnetworks across varying bandwidths. We have integrated FSO into
different architectural frameworks for continual learning, including VIL, TIL,
and FSCIL. Our comprehensive experiments demonstrate FSO&apos;s effectiveness,
significantly improving task performance at various convolutional
representational levels. Specifically, FSO enhances higher-layer performance in
TIL and FSCIL and lower-layer performance in VIL
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1&quot;&gt;Haeyong Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jaehong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sung Ju Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1&quot;&gt;Chang D. Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13314">
<title>Unlocking Pre-trained Image Backbones for Semantic Image Synthesis. (arXiv:2312.13314v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13314</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic image synthesis, i.e., generating images from user-provided semantic
label maps, is an important conditional image generation task as it allows to
control both the content as well as the spatial layout of generated images.
Although diffusion models have pushed the state of the art in generative image
modeling, the iterative nature of their inference process makes them
computationally demanding. Other approaches such as GANs are more efficient as
they only need a single feed-forward pass for generation, but the image quality
tends to suffer on large and diverse datasets. In this work, we propose a new
class of GAN discriminators for semantic image synthesis that generates highly
realistic images by exploiting feature backbone networks pre-trained for tasks
such as image classification. We also introduce a new generator architecture
with better context modeling and using cross-attention to inject noise into
latent variables, leading to more diverse generated images. Our model, which we
dub DP-SIMS, achieves state-of-the-art results in terms of image quality and
consistency with the input label maps on ADE-20K, COCO-Stuff, and Cityscapes,
surpassing recent diffusion models while requiring two orders of magnitude less
compute for inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berrada_T/0/1/0/all/0/1&quot;&gt;Tariq Berrada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1&quot;&gt;Jakob Verbeek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couprie_C/0/1/0/all/0/1&quot;&gt;Camille Couprie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1&quot;&gt;Karteek Alahari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14721">
<title>Gerrymandering Planar Graphs. (arXiv:2312.14721v2 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14721</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the computational complexity of the map redistricting problem
(gerrymandering). Mathematically, the electoral district designer
(gerrymanderer) attempts to partition a weighted graph into $k$ connected
components (districts) such that its candidate (party) wins as many districts
as possible. Prior work has principally concerned the special cases where the
graph is a path or a tree. Our focus concerns the realistic case where the
graph is planar. We prove that the gerrymandering problem is solvable in
polynomial time in $\lambda$-outerplanar graphs, when the number of candidates
and $\lambda$ are constants and the vertex weights (voting weights) are
polynomially bounded. In contrast, the problem is NP-complete in general planar
graphs even with just two candidates. This motivates the study of approximation
algorithms for gerrymandering planar graphs. However, when the number of
candidates is large, we prove it is hard to distinguish between instances where
the gerrymanderer cannot win a single district and instances where the
gerrymanderer can win at least one district. This immediately implies that the
redistricting problem is inapproximable in polynomial time in planar graphs,
unless P=NP. This conclusion appears terminal for the design of good
approximation algorithms -- but it is not. The inapproximability bound can be
circumvented as it only applies when the maximum number of districts the
gerrymanderer can win is extremely small, say one. Indeed, for a fixed number
of candidates, our main result is that there is a constant factor approximation
algorithm for redistricting unweighted planar graphs, provided the optimal
value is a large enough constant.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dippel_J/0/1/0/all/0/1&quot;&gt;Jack Dippel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tour_M/0/1/0/all/0/1&quot;&gt;Max Dupr&amp;#xe9; la Tour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_A/0/1/0/all/0/1&quot;&gt;April Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Sanjukta Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vetta_A/0/1/0/all/0/1&quot;&gt;Adrian Vetta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14924">
<title>Training Convolutional Neural Networks with the Forward-Forward algorithm. (arXiv:2312.14924v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14924</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent successes in analyzing images with deep neural networks are almost
exclusively achieved with Convolutional Neural Networks (CNNs). The training of
these CNNs, and in fact of all deep neural network architectures, uses the
backpropagation algorithm where the output of the network is compared with the
desired result and the difference is then used to tune the weights of the
network towards the desired outcome. In a 2022 preprint, Geoffrey Hinton
suggested an alternative way of training which passes the desired results
together with the images at the input of the network. This so called Forward
Forward (FF) algorithm has up to now only been used in fully connected
networks. In this paper, we show how the FF paradigm can be extended to CNNs.
Our FF-trained CNN, featuring a novel spatially-extended labeling technique,
achieves a classification accuracy of 99.16% on the MNIST hand-written digits
dataset. We show how different hyperparameters affect the performance of the
proposed algorithm and compare the results with CNN trained with the standard
backpropagation approach. Furthermore, we use Class Activation Maps to
investigate which type of features are learnt by the FF algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scodellaro_R/0/1/0/all/0/1&quot;&gt;Riccardo Scodellaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1&quot;&gt;Ajinkya Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alves_F/0/1/0/all/0/1&quot;&gt;Frauke Alves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroter_M/0/1/0/all/0/1&quot;&gt;Matthias Schr&amp;#xf6;ter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15186">
<title>Efficient Asynchronous Federated Learning with Sparsification and Quantization. (arXiv:2312.15186v2 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15186</link>
<description rdf:parseType="Literal">&lt;p&gt;While data is distributed in multiple edge devices, Federated Learning (FL)
is attracting more and more attention to collaboratively train a machine
learning model without transferring raw data. FL generally exploits a parameter
server and a large number of edge devices during the whole process of the model
training, while several devices are selected in each round. However, straggler
devices may slow down the training process or even make the system crash during
training. Meanwhile, other idle edge devices remain unused. As the bandwidth
between the devices and the server is relatively low, the communication of
intermediate data becomes a bottleneck. In this paper, we propose
Time-Efficient Asynchronous federated learning with Sparsification and
Quantization, i.e., TEASQ-Fed. TEASQ-Fed can fully exploit edge devices to
asynchronously participate in the training process by actively applying for
tasks. We utilize control parameters to choose an appropriate number of
parallel edge devices, which simultaneously execute the training tasks. In
addition, we introduce a caching mechanism and weighted averaging with respect
to model staleness to further improve the accuracy. Furthermore, we propose a
sparsification and quantitation approach to compress the intermediate data to
accelerate the training. The experimental results reveal that TEASQ-Fed
improves the accuracy (up to 16.67% higher) while accelerating the convergence
of model training (up to twice faster).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Juncheng Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Ji Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chendi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1&quot;&gt;Hao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1&quot;&gt;Mianxiong Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1&quot;&gt;Dejing Dou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15195">
<title>Mutual Information as Intrinsic Reward of Reinforcement Learning Agents for On-demand Ride Pooling. (arXiv:2312.15195v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15195</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of on-demand ride pooling services allows each vehicle to serve
multiple passengers at a time, thus increasing drivers&apos; income and enabling
passengers to travel at lower prices than taxi/car on-demand services (only one
passenger can be assigned to a car at a time like UberX and Lyft). Although
on-demand ride pooling services can bring so many benefits, ride pooling
services need a well-defined matching strategy to maximize the benefits for all
parties (passengers, drivers, aggregation companies and environment), in which
the regional dispatching of vehicles has a significant impact on the matching
and revenue. Existing algorithms often only consider revenue maximization,
which makes it difficult for requests with unusual distribution to get a ride.
How to increase revenue while ensuring a reasonable assignment of requests
brings a challenge to ride pooling service companies (aggregation companies).
In this paper, we propose a framework for vehicle dispatching for ride pooling
tasks, which splits the city into discrete dispatching regions and uses the
reinforcement learning (RL) algorithm to dispatch vehicles in these regions. We
also consider the mutual information (MI) between vehicle and order
distribution as the intrinsic reward of the RL algorithm to improve the
correlation between their distributions, thus ensuring the possibility of
getting a ride for unusually distributed requests. In experimental results on a
real-world taxi dataset, we demonstrate that our framework can significantly
increase revenue up to an average of 3\% over the existing best on-demand ride
pooling method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xianjie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiahao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1&quot;&gt;Chen Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yifei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15548">
<title>YAYI-UIE: A Chat-Enhanced Instruction Tuning Framework for Universal Information Extraction. (arXiv:2312.15548v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15548</link>
<description rdf:parseType="Literal">&lt;p&gt;The difficulty of the information extraction task lies in dealing with the
task-specific label schemas and heterogeneous data structures. Recent work has
proposed methods based on large language models to uniformly model different
information extraction tasks. However, these existing methods are deficient in
their information extraction capabilities for Chinese languages other than
English. In this paper, we propose an end-to-end chat-enhanced instruction
tuning framework for universal information extraction (YAYI-UIE), which
supports both Chinese and English. Specifically, we utilize dialogue data and
information extraction data to enhance the information extraction performance
jointly. Experimental results show that our proposed framework achieves
state-of-the-art performance on Chinese datasets while also achieving
comparable performance on English datasets under both supervised settings and
zero-shot settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xinglin Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1&quot;&gt;Nan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hanxuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Minzheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1&quot;&gt;Wenji Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1&quot;&gt;Daniel Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16430">
<title>Preference as Reward, Maximum Preference Optimization with Importance Sampling. (arXiv:2312.16430v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16430</link>
<description rdf:parseType="Literal">&lt;p&gt;Preference learning is a key technology for aligning language models with
human values. Reinforcement Learning from Human Feedback (RLHF) is a model
based algorithm to optimize preference learning, which first fitting a reward
model for preference score, and then optimizing generating policy with
on-policy PPO algorithm to maximize the reward. The processing of RLHF is
complex, time-consuming and unstable. Direct Preference Optimization (DPO)
algorithm using off-policy algorithm to direct optimize generating policy and
eliminating the need for reward model, which is data efficient and stable. DPO
use Bradley-Terry model and log-loss which leads to over-fitting to the
preference data at the expense of ignoring KL-regularization term when
preference is deterministic. IPO uses a root-finding MSE loss to solve the
ignoring KL-regularization problem. In this paper, we&apos;ll figure out, although
IPO fix the problem when preference is deterministic, but both DPO and IPO
fails the KL-regularization term because the support of preference distribution
not equal to reference distribution. Then, we design a simple and intuitive
off-policy preference optimization algorithm from an importance sampling view,
which we call Maximum Preference Optimization (MPO), and add off-policy
KL-regularization terms which makes KL-regularization truly effective. The
objective of MPO bears resemblance to RLHF&apos;s objective, and likes IPO, MPO is
off-policy. So, MPO attains the best of both worlds. To simplify the learning
process and save memory usage, MPO eliminates the needs for both reward model
and reference policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zaifan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1&quot;&gt;Chao Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16451">
<title>Domain Generalization with Vital Phase Augmentation. (arXiv:2312.16451v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16451</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have shown remarkable performance in image
classification. However, their performance significantly deteriorates with
corrupted input data. Domain generalization methods have been proposed to train
robust models against out-of-distribution data. Data augmentation in the
frequency domain is one of such approaches that enable a model to learn phase
features to establish domain-invariant representations. This approach changes
the amplitudes of the input data while preserving the phases. However, using
fixed phases leads to susceptibility to phase fluctuations because amplitudes
and phase fluctuations commonly occur in out-of-distribution. In this study, to
address this problem, we introduce an approach using finite variation of the
phases of input data rather than maintaining fixed phases. Based on the
assumption that the degree of domain-invariant features varies for each phase,
we propose a method to distinguish phases based on this degree. In addition, we
propose a method called vital phase augmentation (VIPAug) that applies the
variation to the phases differently according to the degree of domain-invariant
features of given phases. The model depends more on the vital phases that
contain more domain-invariant features for attaining robustness to amplitude
and phase fluctuations. We present experimental evaluations of our proposed
approach, which exhibited improved performance for both clean and corrupted
data. VIPAug achieved SOTA performance on the benchmark CIFAR-10 and CIFAR-100
datasets, as well as near-SOTA performance on the ImageNet-100 and ImageNet
datasets. Our code is available at https://github.com/excitedkid/vipaug.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;Ingyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1&quot;&gt;Wooju Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1&quot;&gt;Hyun Myung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.17581">
<title>Action-Item-Driven Summarization of Long Meeting Transcripts. (arXiv:2312.17581v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.17581</link>
<description rdf:parseType="Literal">&lt;p&gt;The increased prevalence of online meetings has significantly enhanced the
practicality of a model that can automatically generate the summary of a given
meeting. This paper introduces a novel and effective approach to automate the
generation of meeting summaries. Current approaches to this problem generate
general and basic summaries, considering the meeting simply as a long dialogue.
However, our novel algorithms can generate abstractive meeting summaries that
are driven by the action items contained in the meeting transcript. This is
done by recursively generating summaries and employing our action-item
extraction algorithm for each section of the meeting in parallel. All of these
sectional summaries are then combined and summarized together to create a
coherent and action-item-driven summary. In addition, this paper introduces
three novel methods for dividing up long transcripts into topic-based sections
to improve the time efficiency of our algorithm, as well as to resolve the
issue of large language models (LLMs) forgetting long-term dependencies. Our
pipeline achieved a BERTScore of 64.98 across the AMI corpus, which is an
approximately 4.98% increase from the current state-of-the-art result produced
by a fine-tuned BART (Bidirectional and Auto-Regressive Transformers) model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golia_L/0/1/0/all/0/1&quot;&gt;Logan Golia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1&quot;&gt;Jugal Kalita&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00009">
<title>Turing&apos;s Test, a Beautiful Thought Experiment. (arXiv:2401.00009v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00009</link>
<description rdf:parseType="Literal">&lt;p&gt;In the wake of large language models, there has been a resurgence of claims
and questions about the Turing test and its value for AI, which are reminiscent
of decades of practical &quot;Turing&quot; tests. If AI were quantum physics, by now
several &quot;Schr\&quot;odinger&apos;s&quot; cats could have been killed. Better late than never,
it is time for a historical reconstruction of Turing&apos;s beautiful thought
experiment. In this paper I present a wealth of evidence, including new
archival sources, give original answers to several open questions about
Turing&apos;s 1950 paper, and address the core question of the value of Turing&apos;s
test.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goncalves_B/0/1/0/all/0/1&quot;&gt;Bernardo Gon&amp;#xe7;alves&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00315">
<title>Bidirectional Temporal Plan Graph: Enabling Switchable Passing Orders for More Efficient Multi-Agent Path Finding Plan Execution. (arXiv:2401.00315v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00315</link>
<description rdf:parseType="Literal">&lt;p&gt;The Multi-Agent Path Finding (MAPF) problem involves planning collision-free
paths for multiple agents in a shared environment. The majority of MAPF solvers
rely on the assumption that an agent can arrive at a specific location at a
specific timestep. However, real-world execution uncertainties can cause agents
to deviate from this assumption, leading to collisions and deadlocks. Prior
research solves this problem by having agents follow a Temporal Plan Graph
(TPG), enforcing a consistent passing order at every location as defined in the
MAPF plan. However, we show that TPGs are overly strict because, in some
circumstances, satisfying the passing order requires agents to wait
unnecessarily, leading to longer execution time. To overcome this issue, we
introduce a new graphical representation called a Bidirectional Temporal Plan
Graph (BTPG), which allows switching passing orders during execution to avoid
unnecessary waiting time. We design two anytime algorithms for constructing a
BTPG: BTPG-na\&quot;ive and BTPG-optimized. Experimental results show that following
BTPGs consistently outperforms following TPGs, reducing unnecessary waits by
8-20%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yifan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veerapaneni_R/0/1/0/all/0/1&quot;&gt;Rishi Veerapaneni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiaoyang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00608">
<title>Bringing Back the Context: Camera Trap Species Identification as Link Prediction on Multimodal Knowledge Graphs. (arXiv:2401.00608v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00608</link>
<description rdf:parseType="Literal">&lt;p&gt;Camera traps are valuable tools in animal ecology for biodiversity monitoring
and conservation. However, challenges like poor generalization to deployment at
new unseen locations limit their practical application. Images are naturally
associated with heterogeneous forms of context possibly in different
modalities. In this work, we leverage the structured context associated with
the camera trap images to improve out-of-distribution generalization for the
task of species identification in camera traps. For example, a photo of a wild
animal may be associated with information about where and when it was taken, as
well as structured biology knowledge about the animal species. While typically
overlooked by existing work, bringing back such context offers several
potential benefits for better image understanding, such as addressing data
scarcity and enhancing generalization. However, effectively integrating such
heterogeneous context into the visual domain is a challenging problem. To
address this, we propose a novel framework that reformulates species
classification as link prediction in a multimodal knowledge graph (KG). This
framework seamlessly integrates various forms of multimodal context for visual
recognition. We apply this framework for out-of-distribution species
classification on the iWildCam2020-WILDS and Snapshot Mountain Zebra datasets
and achieve competitive performance with state-of-the-art approaches.
Furthermore, our framework successfully incorporates biological taxonomy for
improved generalization and enhances sample efficiency for recognizing
under-represented species.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pahuja_V/0/1/0/all/0/1&quot;&gt;Vardaan Pahuja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1&quot;&gt;Weidi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yu Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_C/0/1/0/all/0/1&quot;&gt;Cheng-Hao Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hong-You Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berger_Wolf_T/0/1/0/all/0/1&quot;&gt;Tanya Berger-Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stewart_C/0/1/0/all/0/1&quot;&gt;Charles Stewart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Song Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1&quot;&gt;Wei-Lun Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yu Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00926">
<title>Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level Feature Fusion for Aiding Diagnosis of Blood Diseases. (arXiv:2401.00926v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00926</link>
<description rdf:parseType="Literal">&lt;p&gt;In standard hospital blood tests, the traditional process requires doctors to
manually isolate leukocytes from microscopic images of patients&apos; blood using
microscopes. These isolated leukocytes are then categorized via automatic
leukocyte classifiers to determine the proportion and volume of different types
of leukocytes present in the blood samples, aiding disease diagnosis. This
methodology is not only time-consuming and labor-intensive, but it also has a
high propensity for errors due to factors such as image quality and
environmental conditions, which could potentially lead to incorrect subsequent
classifications and misdiagnosis. To address these issues, this paper proposes
an innovative method of leukocyte detection: the Multi-level Feature Fusion and
Deformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte
scale disparity, we designed the High-level Screening-feature Fusion Pyramid
(HS-FPN), enabling multi-level fusion. This model uses high-level features as
weights to filter low-level feature information via a channel attention module
and then merges the screened information with the high-level features, thus
enhancing the model&apos;s feature expression capability. Further, we address the
issue of leukocyte feature scarcity by incorporating a multi-scale deformable
self-attention module in the encoder and using the self-attention and
cross-deformable attention mechanisms in the decoder, which aids in the
extraction of the global features of the leukocyte feature maps. The
effectiveness, superiority, and generalizability of the proposed MFDS-DETR
method are confirmed through comparisons with other cutting-edge leukocyte
detection models using the private WBCDD, public LISC and BCCD datasets. Our
source code and private WBCCD dataset are available at
https://github.com/JustlfC03/MFDS-DETR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenyan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Ben Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changmiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xianjun Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yuxing Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1&quot;&gt;Feiwei Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yu Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01204">
<title>PPBFL: A Privacy Protected Blockchain-based Federated Learning Model. (arXiv:2401.01204v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01204</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of machine learning and a growing concern for data
privacy, federated learning has become a focal point of attention. However,
attacks on model parameters and a lack of incentive mechanisms hinder the
effectiveness of federated learning. Therefore, we propose A Privacy Protected
Blockchain-based Federated Learning Model (PPBFL) to enhance the security of
federated learning and encourage active participation of nodes in model
training. Blockchain technology ensures the integrity of model parameters
stored in the InterPlanetary File System (IPFS), providing protection against
tampering. Within the blockchain, we introduce a Proof of Training Work (PoTW)
consensus algorithm tailored for federated learning, aiming to incentive
training nodes. This algorithm rewards nodes with greater computational power,
promoting increased participation and effort in the federated learning process.
A novel adaptive differential privacy algorithm is simultaneously applied to
local and global models. This safeguards the privacy of local data at training
clients, preventing malicious nodes from launching inference attacks.
Additionally, it enhances the security of the global model, preventing
potential security degradation resulting from the combination of numerous local
models. The possibility of security degradation is derived from the composition
theorem. By introducing reverse noise in the global model, a zero-bias estimate
of differential privacy noise between local and global models is achieved.
Furthermore, we propose a new mix transactions mechanism utilizing ring
signature technology to better protect the identity privacy of local training
clients. Security analysis and experimental results demonstrate that PPBFL,
compared to baseline methods, not only exhibits superior model performance but
also achieves higher security.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1&quot;&gt;Chunhe Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wanshuang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianbo Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01286">
<title>A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01286</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown extraordinary capabilities in
understanding and generating text that closely mirrors human communication.
However, a primary limitation lies in the significant computational demands
during training, arising from their extensive parameterization. This challenge
is further intensified by the dynamic nature of the world, necessitating
frequent updates to LLMs to correct outdated information or integrate new
knowledge, thereby ensuring their continued relevance. Note that many
applications demand continual model adjustments post-training to address
deficiencies or undesirable behaviors. There is an increasing interest in
efficient, lightweight methods for on-the-fly model modifications. To this end,
recent years have seen a burgeoning in the techniques of knowledge editing for
LLMs, which aim to efficiently modify LLMs&apos; behaviors within specific domains
while preserving overall performance across various inputs. In this paper, we
first define the knowledge editing problem and then provide a comprehensive
review of cutting-edge approaches. Drawing inspiration from educational and
cognitive research theories, we propose a unified categorization criterion that
classifies knowledge editing methods into three groups: resorting to external
knowledge, merging knowledge into the model, and editing intrinsic knowledge.
Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive
empirical evaluation of representative knowledge editing approaches.
Additionally, we provide an in-depth analysis of knowledge location, which can
provide a deeper understanding of the knowledge structures inherent within
LLMs. Finally, we discuss several potential applications of knowledge editing,
outlining its broad and impactful implications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yunzhi Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1&quot;&gt;Bozhong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Shumin Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengru Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1&quot;&gt;Zekun Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1&quot;&gt;Shengyu Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jintian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1&quot;&gt;Yuansheng Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Siyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Ziwen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jia-Chen Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1&quot;&gt;Pengjun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Lei Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaowei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01383">
<title>Predicting Infant Brain Connectivity with Federated Multi-Trajectory GNNs using Scarce Data. (arXiv:2401.01383v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01383</link>
<description rdf:parseType="Literal">&lt;p&gt;The understanding of the convoluted evolution of infant brain networks during
the first postnatal year is pivotal for identifying the dynamics of early brain
connectivity development. Existing deep learning solutions suffer from three
major limitations. First, they cannot generalize to multi-trajectory prediction
tasks, where each graph trajectory corresponds to a particular imaging modality
or connectivity type (e.g., T1-w MRI). Second, existing models require
extensive training datasets to achieve satisfactory performance which are often
challenging to obtain. Third, they do not efficiently utilize incomplete time
series data. To address these limitations, we introduce FedGmTE-Net++, a
federated graph-based multi-trajectory evolution network. Using the power of
federation, we aggregate local learnings among diverse hospitals with limited
datasets. As a result, we enhance the performance of each hospital&apos;s local
generative model, while preserving data privacy. The three key innovations of
FedGmTE-Net++ are: (i) presenting the first federated learning framework
specifically designed for brain multi-trajectory evolution prediction in a
data-scarce environment, (ii) incorporating an auxiliary regularizer in the
local objective function to exploit all the longitudinal brain connectivity
within the evolution trajectory and maximize data utilization, (iii)
introducing a two-step imputation process, comprising a preliminary KNN-based
precompletion followed by an imputation refinement step that employs regressors
to improve similarity scores and refine imputations. Our comprehensive
experimental results showed the outperformance of FedGmTE-Net++ in brain
multi-trajectory prediction from a single baseline graph in comparison with
benchmark methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Pistos_M/0/1/0/all/0/1&quot;&gt;Michalis Pistos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weili Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dinggang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rekik_I/0/1/0/all/0/1&quot;&gt;Islem Rekik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01470">
<title>TPC-ViT: Token Propagation Controller for Efficient Vision Transformer. (arXiv:2401.01470v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01470</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers (ViTs) have achieved promising results on a variety of
Computer Vision tasks, however their quadratic complexity in the number of
input tokens has limited their application specially in resource-constrained
settings. Previous approaches that employ gradual token reduction to address
this challenge assume that token redundancy in one layer implies redundancy in
all the following layers. We empirically demonstrate that this assumption is
often not correct, i.e., tokens that are redundant in one layer can be useful
in later layers. We employ this key insight to propose a novel token
propagation controller (TPC) that incorporates two different
token-distributions, i.e., pause probability and restart probability to control
the reduction and reuse of tokens respectively, which results in more efficient
token utilization. To improve the estimates of token distributions, we propose
a smoothing mechanism that acts as a regularizer and helps remove noisy
outliers. Furthermore, to improve the training-stability of our proposed TPC,
we introduce a model stabilizer that is able to implicitly encode local image
structures and minimize accuracy fluctuations during model training. We present
extensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT
and Swin models to demonstrate the effectiveness of our proposed method. For
example, compared to baseline models, our proposed method improves the
inference speed of the DeiT-S by 250% while increasing the classification
accuracy by 1.0%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wentao Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01519">
<title>Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review. (arXiv:2401.01519v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01519</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the frontiers of large language models (LLMs) in
psychology applications. Psychology has undergone several theoretical changes,
and the current use of Artificial Intelligence (AI) and Machine Learning,
particularly LLMs, promises to open up new research directions. We provide a
detailed exploration of how LLMs like ChatGPT are transforming psychological
research. It discusses the impact of LLMs across various branches of
psychology, including cognitive and behavioral, clinical and counseling,
educational and developmental, and social and cultural psychology, highlighting
their potential to simulate aspects of human cognition and behavior. The paper
delves into the capabilities of these models to emulate human-like text
generation, offering innovative tools for literature review, hypothesis
generation, experimental design, experimental subjects, data analysis, academic
writing, and peer review in psychology. While LLMs are essential in advancing
research methodologies in psychology, the paper also cautions about their
technical and ethical challenges. There are issues like data privacy, the
ethical implications of using LLMs in psychological research, and the need for
a deeper understanding of these models&apos; limitations. Researchers should
responsibly use LLMs in psychological studies, adhering to ethical standards
and considering the potential consequences of deploying these technologies in
sensitive areas. Overall, the article provides a comprehensive overview of the
current state of LLMs in psychology, exploring potential benefits and
challenges. It serves as a call to action for researchers to leverage LLMs&apos;
advantages responsibly while addressing associated risks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1&quot;&gt;Luoma Ke&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1&quot;&gt;Song Tong&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1&quot;&gt;Peng Cheng&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1&quot;&gt;Kaiping Peng&lt;/a&gt; (1) ((1) Department of Psychology, Tsinghua University, (2) School of Social Science, Tsinghua University)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01523">
<title>GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse. (arXiv:2401.01523v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01523</link>
<description rdf:parseType="Literal">&lt;p&gt;The exponential growth of social media has profoundly transformed how
information is created, disseminated, and absorbed, exceeding any precedent in
the digital age. Regrettably, this explosion has also spawned a significant
increase in the online abuse of memes. Evaluating the negative impact of memes
is notably challenging, owing to their often subtle and implicit meanings,
which are not directly conveyed through the overt text and imagery. In light of
this, large multimodal models (LMMs) have emerged as a focal point of interest
due to their remarkable capabilities in handling diverse multimodal tasks. In
response to this development, our paper aims to thoroughly examine the capacity
of various LMMs (e.g. GPT-4V) to discern and respond to the nuanced aspects of
social abuse manifested in memes. We introduce the comprehensive meme
benchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes
such as implicit hate speech, sexism, and cyberbullying, etc. Utilizing
GOAT-Bench, we delve into the ability of LMMs to accurately assess hatefulness,
misogyny, offensiveness, sarcasm, and harmful content. Our extensive
experiments across a range of LMMs reveal that current models still exhibit a
deficiency in safety awareness, showing insensitivity to various forms of
implicit abuse. We posit that this shortfall represents a critical impediment
to the realization of safe artificial intelligence. The GOAT-Bench and
accompanying resources are publicly accessible at https://goatlmm.github.io/,
contributing to ongoing research in this vital field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hongzhan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Ziyang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Ruichao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jing Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01651">
<title>AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated by AI. (arXiv:2401.01651v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01651</link>
<description rdf:parseType="Literal">&lt;p&gt;The burgeoning field of Artificial Intelligence Generated Content (AIGC) is
witnessing rapid advancements, particularly in video generation. This paper
introduces AIGCBench, a pioneering comprehensive and scalable benchmark
designed to evaluate a variety of video generation tasks, with a primary focus
on Image-to-Video (I2V) generation. AIGCBench tackles the limitations of
existing benchmarks, which suffer from a lack of diverse datasets, by including
a varied and open-domain image-text dataset that evaluates different
state-of-the-art algorithms under equivalent conditions. We employ a novel text
combiner and GPT-4 to create rich text prompts, which are then used to generate
images via advanced Text-to-Image models. To establish a unified evaluation
framework for video generation tasks, our benchmark includes 11 metrics
spanning four dimensions to assess algorithm performance. These dimensions are
control-video alignment, motion effects, temporal consistency, and video
quality. These metrics are both reference video-dependent and video-free,
ensuring a comprehensive evaluation strategy. The evaluation standard proposed
correlates well with human judgment, providing insights into the strengths and
weaknesses of current I2V algorithms. The findings from our extensive
experiments aim to stimulate further research and development in the I2V field.
AIGCBench represents a significant step toward creating standardized benchmarks
for the broader AIGC landscape, proposing an adaptable and equitable framework
for future assessments of video generation tasks. We have open-sourced the
dataset and evaluation code on the project website:
https://www.benchcouncil.org/AIGCBench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1&quot;&gt;Fanda Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chunjie Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wanling Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1&quot;&gt;Jianfeng Zhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01843">
<title>Investigating Semi-Supervised Learning Algorithms in Text Datasets. (arXiv:2401.01843v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01843</link>
<description rdf:parseType="Literal">&lt;p&gt;Using large training datasets enhances the generalization capabilities of
neural networks. Semi-supervised learning (SSL) is useful when there are few
labeled data and a lot of unlabeled data. SSL methods that use data
augmentation are most successful for image datasets. In contrast, texts do not
have consistent augmentation methods as images. Consequently, methods that use
augmentation are not as effective in text data as they are in image data. In
this study, we compared SSL algorithms that do not require augmentation; these
are self-training, co-training, tri-training, and tri-training with
disagreement. In the experiments, we used 4 different text datasets for
different tasks. We examined the algorithms from a variety of perspectives by
asking experiment questions and suggested several improvements. Among the
algorithms, tri-training with disagreement showed the closest performance to
the Oracle; however, performance gap shows that new semi-supervised algorithms
or improvements in existing methods are needed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kesgin_H/0/1/0/all/0/1&quot;&gt;Himmet Toprak Kesgin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amasyali_M/0/1/0/all/0/1&quot;&gt;Mehmet Fatih Amasyali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01943">
<title>Generalist embedding models are better at short-context clinical semantic search than specialized embedding models. (arXiv:2401.01943v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01943</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing use of tools and solutions based on Large Language Models
(LLMs) for various tasks in the medical domain has become a prominent trend.
Their use in this highly critical and sensitive domain has thus raised
important questions about their robustness, especially in response to
variations in input, and the reliability of the generated outputs. This study
addresses these questions by constructing a textual dataset based on the
ICD-10-CM code descriptions, widely used in US hospitals and containing many
clinical terms, and their easily reproducible rephrasing. We then benchmarked
existing embedding models, either generalist or specialized in the clinical
domain, in a semantic search task where the goal was to correctly match the
rephrased text to the original description. Our results showed that generalist
models performed better than clinical models, suggesting that existing clinical
specialized models are more sensitive to small changes in input that confuse
them. The highlighted problem of specialized models may be due to the fact that
they have not been trained on sufficient data, and in particular on datasets
that are not diverse enough to have a reliable global language understanding,
which is still necessary for accurate handling of medical documents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Excoffier_J/0/1/0/all/0/1&quot;&gt;Jean-Baptiste Excoffier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roehr_T/0/1/0/all/0/1&quot;&gt;Tom Roehr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Figueroa_A/0/1/0/all/0/1&quot;&gt;Alexei Figueroa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papaioannou_J/0/1/0/all/0/1&quot;&gt;Jens-Michalis Papaioannou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bressem_K/0/1/0/all/0/1&quot;&gt;Keno Bressem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortala_M/0/1/0/all/0/1&quot;&gt;Matthieu Ortala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02452">
<title>The Compute Divide in Machine Learning: A Threat to Academic Contribution and Scrutiny?. (arXiv:2401.02452v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02452</link>
<description rdf:parseType="Literal">&lt;p&gt;There are pronounced differences in the extent to which industrial and
academic AI labs use computing resources. We provide a data-driven survey of
the role of the compute divide in shaping machine learning research. We show
that a compute divide has coincided with a reduced representation of
academic-only research teams in compute intensive research topics, especially
foundation models. We argue that, academia will likely play a smaller role in
advancing the associated techniques, providing critical evaluation and
scrutiny, and in the diffusion of such models. Concurrent with this change in
research focus, there is a noticeable shift in academic research towards
embracing open source, pre-trained models developed within the industry. To
address the challenges arising from this trend, especially reduced scrutiny of
influential models, we recommend approaches aimed at thoughtfully expanding
academic insights. Nationally-sponsored computing infrastructure coupled with
open science initiatives could judiciously boost academic compute access,
prioritizing research on interpretability, safety and security. Structured
access programs and third-party auditing may also allow measured external
evaluation of industry systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Besiroglu_T/0/1/0/all/0/1&quot;&gt;Tamay Besiroglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bergerson_S/0/1/0/all/0/1&quot;&gt;Sage Andrus Bergerson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michael_A/0/1/0/all/0/1&quot;&gt;Amelia Michael&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heim_L/0/1/0/all/0/1&quot;&gt;Lennart Heim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xueyun Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thompson_N/0/1/0/all/0/1&quot;&gt;Neil Thompson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02731">
<title>Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks. (arXiv:2401.02731v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02731</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have demonstrated considerable proficiency in
general natural language processing (NLP) tasks. Instruction tuning, a
successful paradigm, enhances the ability of LLMs to follow natural language
instructions and exhibit robust generalization across a wide range of tasks.
However, these models often encounter performance limitations across multiple
tasks due to constrained model capacity. Expanding this capacity during the
instruction tuning phase poses significant challenges. To address this issue,
we introduce a novel approach, Parameter-Efficient Sparsity Crafting (PESC),
which transitions dense models to sparse models using a Mixture of Experts
(MoE) architecture. PESC integrates adapters into the MoE layers of sparse
models, differentiating experts without altering the individual weights within
these layers. This method significantly reduces computational costs and GPU
memory requirements, facilitating model capacity expansion through a minimal
increase in parameters via the inserted adapters. Our empirical evaluation
demonstrates the effectiveness of the PESC method. Using PESC during
instruction tuning, our sparse models, dubbed Camelidae outperform all other
opensource sparse models and exhibit superior general capabilities compared to
GPT3.5.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haoyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Haisheng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bei Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02740">
<title>Fairness-Aware Job Scheduling for Multi-Job Federated Learning. (arXiv:2401.02740v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02740</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) enables multiple data owners (a.k.a. FL clients) to
collaboratively train machine learning models without disclosing sensitive
private data. Existing FL research mostly focuses on the monopoly scenario in
which a single FL server selects a subset of FL clients to update their local
models in each round of training. In practice, there can be multiple FL servers
simultaneously trying to select clients from the same pool. In this paper, we
propose a first-of-its-kind Fairness-aware Federated Job Scheduling (FairFedJS)
approach to bridge this gap. Based on Lyapunov optimization, it ensures fair
allocation of high-demand FL client datasets to FL jobs in need of them, by
jointly considering the current demand and the job payment bids, in order to
prevent prolonged waiting. Extensive experiments comparing FairFedJS against
four state-of-the-art approaches on two datasets demonstrate its significant
advantages. It outperforms the best baseline by 31.9% and 1.0% on average in
terms of scheduling fairness and convergence time, respectively, while
achieving comparable test accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yuxin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Han Yu&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>