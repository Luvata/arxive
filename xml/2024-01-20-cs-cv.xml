<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-18T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09428" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09443" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09450" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09454" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09603" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09606" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09607" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09624" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09627" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09638" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09639" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09671" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09673" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09712" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09732" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09759" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09774" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09791" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09794" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09826" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09833" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09852" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09861" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09883" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09895" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09900" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09921" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09939" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09980" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09985" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10005" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10011" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10037" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10041" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10061" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10083" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10113" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10139" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10148" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10150" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10178" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10208" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10213" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10219" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10220" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10222" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10226" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10228" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10229" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.08219" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.07929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.08044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.00114" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.06657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.02070" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.02472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12307" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.00429" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.05156" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09172" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.00163" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00977" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07142" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11164" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16573" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.04780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09760" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10642" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12474" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04938" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15111" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12838" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.17670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05163" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07402" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07450" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08140" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09180" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.09424">
<title>Precipitation Prediction Using an Ensemble of Lightweight Learners. (arXiv:2401.09424v1 [physics.ao-ph])</title>
<link>http://arxiv.org/abs/2401.09424</link>
<description rdf:parseType="Literal">&lt;p&gt;Precipitation prediction plays a crucial role in modern agriculture and
industry. However, it poses significant challenges due to the diverse patterns
and dynamics in time and space, as well as the scarcity of high precipitation
events.
&lt;/p&gt;
&lt;p&gt;To address this challenge, we propose an ensemble learning framework that
leverages multiple learners to capture the diverse patterns of precipitation
distribution. Specifically, the framework consists of a precipitation predictor
with multiple lightweight heads (learners) and a controller that combines the
outputs from these heads. The learners and the controller are separately
optimized with a proposed 3-stage training scheme.
&lt;/p&gt;
&lt;p&gt;By utilizing provided satellite images, the proposed approach can effectively
model the intricate rainfall patterns, especially for high precipitation
events. It achieved 1st place on the core test as well as the nowcasting
leaderboards of the Weather4Cast 2023 competition. For detailed implementation,
please refer to our GitHub repository at:
https://github.com/lxz1217/weather4cast-2023-lxz.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinzhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Rui_S/0/1/0/all/0/1&quot;&gt;Sun Rui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Niu_Y/0/1/0/all/0/1&quot;&gt;Yiming Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09428">
<title>Multispectral Stereo-Image Fusion for 3D Hyperspectral Scene Reconstruction. (arXiv:2401.09428v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.09428</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral imaging enables the analysis of optical material properties that are
invisible to the human eye. Different spectral capturing setups, e.g., based on
filter-wheel, push-broom, line-scanning, or mosaic cameras, have been
introduced in the last years to support a wide range of applications in
agriculture, medicine, and industrial surveillance. However, these systems
often suffer from different disadvantages, such as lack of real-time
capability, limited spectral coverage or low spatial resolution. To address
these drawbacks, we present a novel approach combining two calibrated
multispectral real-time capable snapshot cameras, covering different spectral
ranges, into a stereo-system. Therefore, a hyperspectral data-cube can be
continuously captured. The combined use of different multispectral snapshot
cameras enables both 3D reconstruction and spectral analysis. Both captured
images are demosaicked avoiding spatial resolution loss. We fuse the spectral
data from one camera into the other to receive a spatially and spectrally high
resolution video stream. Experiments demonstrate the feasibility of this
approach and the system is investigated with regard to its applicability for
surgical assistance monitoring.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wisotzky_E/0/1/0/all/0/1&quot;&gt;Eric L. Wisotzky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Triller_J/0/1/0/all/0/1&quot;&gt;Jost Triller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hilsmann_A/0/1/0/all/0/1&quot;&gt;Anna Hilsmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eisert_P/0/1/0/all/0/1&quot;&gt;Peter Eisert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09442">
<title>Object Attribute Matters in Visual Question Answering. (arXiv:2401.09442v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09442</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual question answering is a multimodal task that requires the joint
comprehension of visual and textual information. However, integrating visual
and textual semantics solely through attention layers is insufficient to
comprehensively understand and align information from both modalities.
Intuitively, object attributes can naturally serve as a bridge to unify them,
which has been overlooked in previous research. In this paper, we propose a
novel VQA approach from the perspective of utilizing object attribute, aiming
to achieve better object-level visual-language alignment and multimodal scene
understanding. Specifically, we design an attribute fusion module and a
contrastive knowledge distillation module. The attribute fusion module
constructs a multimodal graph neural network to fuse attributes and visual
features through message passing. The enhanced object-level visual features
contribute to solving fine-grained problem like counting-question. The better
object-level visual-language alignment aids in understanding multimodal scenes,
thereby improving the model&apos;s robustness. Furthermore, to augment scene
understanding and the out-of-distribution performance, the contrastive
knowledge distillation module introduces a series of implicit knowledge. We
distill knowledge into attributes through contrastive loss, which further
strengthens the representation learning of attribute features and facilitates
visual-linguistic alignment. Intensive experiments on six datasets, COCO-QA,
VQAv2, VQA-CPv2, VQA-CPv1, VQAvs and TDIUC, show the superiority of the
proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peize Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_Q/0/1/0/all/0/1&quot;&gt;Qingyi Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_P/0/1/0/all/0/1&quot;&gt;Peng Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zheng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09443">
<title>CRD: Collaborative Representation Distance for Practical Anomaly Detection. (arXiv:2401.09443v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09443</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual defect detection plays an important role in intelligent industry.
Patch based methods consider visual images as a collection of image patches
according to positions, which have stronger discriminative ability for small
defects in products, e.g. scratches on pills. However, the nearest neighbor
search for the query image and the stored patches will occupy $O(n)$ complexity
in terms of time and space requirements, posing strict challenges for
deployment in edge environments. In this paper, we propose an alternative
approach to the distance calculation of image patches via collaborative
representation models. Starting from the nearest neighbor distance with $L_0$
constraint, we relax the constraint to $L_2$ constraint and solve the distance
quickly in close-formed without actually accessing the original stored
collection of image patches. Furthermore, we point out that the main
computational burden of this close-formed solution can be pre-computed by
high-performance server before deployment. Consequently, the distance
calculation on edge devices only requires a simple matrix multiplication, which
is extremely lightweight and GPU-friendly. Performance on real industrial
scenarios demonstrates that compared to the existing state-of-the-art methods,
this distance achieves several hundred times improvement in computational
efficiency with slight performance drop, while greatly reducing memory
overhead.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Chao Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yudong Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09446">
<title>Explainable Multimodal Sentiment Analysis on Bengali Memes. (arXiv:2401.09446v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09446</link>
<description rdf:parseType="Literal">&lt;p&gt;Memes have become a distinctive and effective form of communication in the
digital era, attracting online communities and cutting across cultural
barriers. Even though memes are frequently linked with humor, they have an
amazing capacity to convey a wide range of emotions, including happiness,
sarcasm, frustration, and more. Understanding and interpreting the sentiment
underlying memes has become crucial in the age of information. Previous
research has explored text-based, image-based, and multimodal approaches,
leading to the development of models like CAPSAN and PromptHate for detecting
various meme categories. However, the study of low-resource languages like
Bengali memes remains scarce, with limited availability of publicly accessible
datasets. A recent contribution includes the introduction of the MemoSen
dataset. However, the achieved accuracy is notably low, and the dataset suffers
from imbalanced distribution. In this study, we employed a multimodal approach
using ResNet50 and BanglishBERT and achieved a satisfactory result of 0.71
weighted F1-score, performed comparison with unimodal approaches, and
interpreted behaviors of the models using explainable artificial intelligence
(XAI) techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elahi_K/0/1/0/all/0/1&quot;&gt;Kazi Toufique Elahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_T/0/1/0/all/0/1&quot;&gt;Tasnuva Binte Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1&quot;&gt;Shakil Shahriar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarker_S/0/1/0/all/0/1&quot;&gt;Samir Sarker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joy_S/0/1/0/all/0/1&quot;&gt;Sajib Kumar Saha Joy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_F/0/1/0/all/0/1&quot;&gt;Faisal Muhammad Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09450">
<title>Joining Forces for Pathology Diagnostics with AI Assistance: The EMPAIA Initiative. (arXiv:2401.09450v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.09450</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past decade, artificial intelligence (AI) methods in pathology have
advanced substantially. However, integration into routine clinical practice has
been slow due to numerous challenges, including technical and regulatory
hurdles in translating research results into clinical diagnostic products and
the lack of standardized interfaces. The open and vendor-neutral EMPAIA
initiative addresses these challenges. Here, we provide an overview of EMPAIA&apos;s
achievements and lessons learned. EMPAIA integrates various stakeholders of the
pathology AI ecosystem, i.e., pathologists, computer scientists, and industry.
In close collaboration, we developed technical interoperability standards,
recommendations for AI testing and product development, and explainability
methods. We implemented the modular and open-source EMPAIA platform and
successfully integrated 11 AI-based image analysis apps from 6 different
vendors, demonstrating how different apps can use a single standardized
interface. We prioritized requirements and evaluated the use of AI in real
clinical settings with 14 different pathology laboratories in Europe and Asia.
In addition to technical developments, we created a forum for all stakeholders
to share information and experiences on digital pathology and AI. Commercial,
clinical, and academic stakeholders can now adopt EMPAIA&apos;s common open-source
interfaces, providing a unique opportunity for large-scale standardization and
streamlining of processes. Further efforts are needed to effectively and
broadly establish AI assistance in routine laboratory use. To this end, a
sustainable infrastructure, the non-profit association EMPAIA International,
has been established to continue standardization and support broad
implementation and advocacy for an AI-assisted digital pathology future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zerbe_N/0/1/0/all/0/1&quot;&gt;Norman Zerbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwen_L/0/1/0/all/0/1&quot;&gt;Lars Ole Schwen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geissler_C/0/1/0/all/0/1&quot;&gt;Christian Gei&amp;#xdf;ler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiesemann_K/0/1/0/all/0/1&quot;&gt;Katja Wiesemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisson_T/0/1/0/all/0/1&quot;&gt;Tom Bisson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boor_P/0/1/0/all/0/1&quot;&gt;Peter Boor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carvalho_R/0/1/0/all/0/1&quot;&gt;Rita Carvalho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franz_M/0/1/0/all/0/1&quot;&gt;Michael Franz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jansen_C/0/1/0/all/0/1&quot;&gt;Christoph Jansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiehl_T/0/1/0/all/0/1&quot;&gt;Tim-Rasmus Kiehl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindequist_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Lindequist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pohlan_N/0/1/0/all/0/1&quot;&gt;Nora Charlotte Pohlan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmell_S/0/1/0/all/0/1&quot;&gt;Sarah Schmell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strohmenger_K/0/1/0/all/0/1&quot;&gt;Klaus Strohmenger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zakrzewski_F/0/1/0/all/0/1&quot;&gt;Falk Zakrzewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plass_M/0/1/0/all/0/1&quot;&gt;Markus Plass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takla_M/0/1/0/all/0/1&quot;&gt;Michael Takla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuster_T/0/1/0/all/0/1&quot;&gt;Tobias K&amp;#xfc;ster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Homeyer_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Homeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hufnagl_P/0/1/0/all/0/1&quot;&gt;Peter Hufnagl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09454">
<title>Voila-A: Aligning Vision-Language Models with User&apos;s Gaze Attention. (arXiv:2401.09454v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09454</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the integration of vision and language understanding has led
to significant advancements in artificial intelligence, particularly through
Vision-Language Models (VLMs). However, existing VLMs face challenges in
handling real-world applications with complex scenes and multiple objects, as
well as aligning their focus with the diverse attention patterns of human
users. In this paper, we introduce gaze information, feasibly collected by AR
or VR devices, as a proxy for human attention to guide VLMs and propose a novel
approach, Voila-A, for gaze alignment to enhance the interpretability and
effectiveness of these models in real-world applications. First, we collect
hundreds of minutes of gaze data to demonstrate that we can mimic human gaze
modalities using localized narratives. We then design an automatic data
annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset.
Additionally, we innovate the Voila Perceiver modules to integrate gaze
information into VLMs while preserving their pretrained knowledge. We evaluate
Voila-A using a hold-out validation set and a newly collected VOILA-GAZE
Testset, which features real-life scenarios captured with a gaze-tracking
device. Our experimental results demonstrate that Voila-A significantly
outperforms several baseline models. By aligning model attention with human
gaze patterns, Voila-A paves the way for more intuitive, user-centric VLMs and
fosters engaging human-AI interaction across a wide range of applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Kun Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1&quot;&gt;Lei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuntao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1&quot;&gt;Nan Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shuai Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09466">
<title>Self Supervised Vision for Climate Downscaling. (arXiv:2401.09466v1 [physics.ao-ph])</title>
<link>http://arxiv.org/abs/2401.09466</link>
<description rdf:parseType="Literal">&lt;p&gt;Climate change is one of the most critical challenges that our planet is
facing today. Rising global temperatures are already bringing noticeable
changes to Earth&apos;s weather and climate patterns with an increased frequency of
unpredictable and extreme weather events. Future projections for climate change
research are based on Earth System Models (ESMs), the computer models that
simulate the Earth&apos;s climate system. ESMs provide a framework to integrate
various physical systems, but their output is bound by the enormous
computational resources required for running and archiving higher-resolution
simulations. For a given resource budget, the ESMs are generally run on a
coarser grid, followed by a computationally lighter $downscaling$ process to
obtain a finer-resolution output. In this work, we present a deep-learning
model for downscaling ESM simulation data that does not require high-resolution
ground truth data for model optimization. This is realized by leveraging
salient data distribution patterns and the hidden dependencies between weather
variables for an $\textit{individual}$ data point at $\textit{runtime}$.
Extensive evaluation with $2$x, $3$x, and $4$x scaling factors demonstrates
that the proposed model consistently obtains superior performance over that of
various baselines. The improved downscaling performance and no dependence on
high-resolution ground truth data make the proposed method a valuable tool for
climate research and mark it as a promising direction for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Karandeep Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Jeong_C/0/1/0/all/0/1&quot;&gt;Chaeyoon Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shidqi_N/0/1/0/all/0/1&quot;&gt;Naufal Shidqi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sungwon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Nellikkattil_A/0/1/0/all/0/1&quot;&gt;Arjun Nellikkattil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zeller_E/0/1/0/all/0/1&quot;&gt;Elke Zeller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cha_M/0/1/0/all/0/1&quot;&gt;Meeyoung Cha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09467">
<title>Offline Handwriting Signature Verification: A Transfer Learning and Feature Selection Approach. (arXiv:2401.09467v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09467</link>
<description rdf:parseType="Literal">&lt;p&gt;Handwritten signature verification poses a formidable challenge in biometrics
and document authenticity. The objective is to ascertain the authenticity of a
provided handwritten signature, distinguishing between genuine and forged ones.
This issue has many applications in sectors such as finance, legal
documentation, and security. Currently, the field of computer vision and
machine learning has made significant progress in the domain of handwritten
signature verification. The outcomes, however, may be enhanced depending on the
acquired findings, the structure of the datasets, and the used models. Four
stages make up our suggested strategy. First, we collected a large dataset of
12600 images from 420 distinct individuals, and each individual has 30
signatures of a certain kind (All authors signatures are genuine). In the
subsequent stage, the best features from each image were extracted using a deep
learning model named MobileNetV2. During the feature selection step, three
selectors neighborhood component analysis (NCA), Chi2, and mutual info (MI)
were used to pull out 200, 300, 400, and 500 features, giving a total of 12
feature vectors. Finally, 12 results have been obtained by applying machine
learning techniques such as SVM with kernels (rbf, poly, and linear), KNN, DT,
Linear Discriminant Analysis, and Naive Bayes. Without employing feature
selection techniques, our suggested offline signature verification achieved a
classification accuracy of 91.3%, whereas using the NCA feature selection
approach with just 300 features it achieved a classification accuracy of 97.7%.
High classification accuracy was achieved using the designed and suggested
model, which also has the benefit of being a self-organized framework.
Consequently, using the optimum minimally chosen features, the proposed method
could identify the best model performance and result validation prediction
vectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozyurt_F/0/1/0/all/0/1&quot;&gt;Fatih Ozyurt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majidpour_J/0/1/0/all/0/1&quot;&gt;Jafar Majidpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashid_T/0/1/0/all/0/1&quot;&gt;Tarik A. Rashid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koc_C/0/1/0/all/0/1&quot;&gt;Canan Koc&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09471">
<title>Brain Tumor Radiogenomic Classification. (arXiv:2401.09471v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.09471</link>
<description rdf:parseType="Literal">&lt;p&gt;The RSNA-MICCAI brain tumor radiogenomic classification challenge aimed to
predict MGMT biomarker status in glioblastoma through binary classification on
Multi parameter mpMRI scans: T1w, T1wCE, T2w and FLAIR. The dataset is splitted
into three main cohorts: training set, validation set which were used during
training, and the testing were only used during final evaluation. Images were
either in a DICOM format or in Png format. different architectures were used to
investigate the problem including the 3D version of Vision Transformer (ViT3D),
ResNet50, Xception and EfficientNet-B3. AUC was used as the main evaluation
metric and the results showed an advantage for both the ViT3D and the Xception
models achieving 0.6015 and 0.61745 respectively on the testing set. compared
to other results, our results proved to be valid given the complexity of the
task. further improvements can be made through exploring different strategies,
different architectures and more diverse datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mohamed_A/0/1/0/all/0/1&quot;&gt;Amr Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rabea_M/0/1/0/all/0/1&quot;&gt;Mahmoud Rabea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sameh_A/0/1/0/all/0/1&quot;&gt;Aya Sameh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kamal_E/0/1/0/all/0/1&quot;&gt;Ehab Kamal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09472">
<title>Plug-in for visualizing 3D tool tracking from videos of Minimally Invasive Surgeries. (arXiv:2401.09472v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09472</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper tackles instrument tracking and 3D visualization challenges in
minimally invasive surgery (MIS), crucial for computer-assisted interventions.
Conventional and robot-assisted MIS encounter issues with limited 2D camera
projections and minimal hardware integration. The objective is to track and
visualize the entire surgical instrument, including shaft and metallic clasper,
enabling safe navigation within the surgical environment. The proposed method
involves 2D tracking based on segmentation maps, facilitating creation of
labeled dataset without extensive ground-truth knowledge. Geometric changes in
2D intervals express motion, and kinematics based algorithms process results
into 3D tracking information. Synthesized and experimental results in 2D and 3D
motion estimates demonstrate negligible errors, validating the method for
labeling and motion tracking of instruments in MIS videos. The conclusion
underscores the proposed 2D segmentation technique&apos;s simplicity and
computational efficiency, emphasizing its potential as direct plug-in for 3D
visualization in instrument tracking and MIS practices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nema_S/0/1/0/all/0/1&quot;&gt;Shubhangi Nema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1&quot;&gt;Abhishek Mathur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vachhani_L/0/1/0/all/0/1&quot;&gt;Leena Vachhani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09475">
<title>Triamese-ViT: A 3D-Aware Method for Robust Brain Age Estimation from MRIs. (arXiv:2401.09475v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09475</link>
<description rdf:parseType="Literal">&lt;p&gt;The integration of machine learning in medicine has significantly improved
diagnostic precision, particularly in the interpretation of complex structures
like the human brain. Diagnosing challenging conditions such as Alzheimer&apos;s
disease has prompted the development of brain age estimation techniques. These
methods often leverage three-dimensional Magnetic Resonance Imaging (MRI)
scans, with recent studies emphasizing the efficacy of 3D convolutional neural
networks (CNNs) like 3D ResNet. However, the untapped potential of Vision
Transformers (ViTs), known for their accuracy and interpretability, persists in
this domain due to limitations in their 3D versions. This paper introduces
Triamese-ViT, an innovative adaptation of the ViT model for brain age
estimation. Our model uniquely combines ViTs from three different orientations
to capture 3D information, significantly enhancing accuracy and
interpretability. Tested on a dataset of 1351 MRI scans, Triamese-ViT achieves
a Mean Absolute Error (MAE) of 3.84, a 0.9 Spearman correlation coefficient
with chronological age, and a -0.29 Spearman correlation coefficient between
the brain age gap (BAG) and chronological age, significantly better than
previous methods for brian age estimation. A key innovation of Triamese-ViT is
its capacity to generate a comprehensive 3D-like attention map, synthesized
from 2D attention maps of each orientation-specific ViT. This feature is
particularly beneficial for in-depth brain age analysis and disease diagnosis,
offering deeper insights into brain health and the mechanisms of age-related
neural changes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhaonian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1&quot;&gt;Richard Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09495">
<title>IPR-NeRF: Ownership Verification meets Neural Radiance Field. (arXiv:2401.09495v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09495</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Field (NeRF) models have gained significant attention in the
computer vision community in the recent past with state-of-the-art visual
quality and produced impressive demonstrations. Since then, technopreneurs have
sought to leverage NeRF models into a profitable business. Therefore, NeRF
models make it worth the risk of plagiarizers illegally copying,
re-distributing, or misusing those models. This paper proposes a comprehensive
intellectual property (IP) protection framework for the NeRF model in both
black-box and white-box settings, namely IPR-NeRF. In the black-box setting, a
diffusion-based solution is introduced to embed and extract the watermark via a
two-stage optimization process. In the white-box setting, a designated digital
signature is embedded into the weights of the NeRF model by adopting the sign
loss objective. Our extensive experiments demonstrate that not only does our
approach maintain the fidelity (\ie, the rendering quality) of IPR-NeRF models,
but it is also robust against both ambiguity and removal attacks compared to
prior arts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_W/0/1/0/all/0/1&quot;&gt;Win Kent Ong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_K/0/1/0/all/0/1&quot;&gt;Kam Woh Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1&quot;&gt;Chee Seng Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yi Zhe Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tao Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09496">
<title>Learning to Generalize over Subpartitions for Heterogeneity-aware Domain Adaptive Nuclei Segmentation. (arXiv:2401.09496v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09496</link>
<description rdf:parseType="Literal">&lt;p&gt;Annotation scarcity and cross-modality/stain data distribution shifts are two
major obstacles hindering the application of deep learning models for nuclei
analysis, which holds a broad spectrum of potential applications in digital
pathology. Recently, unsupervised domain adaptation (UDA) methods have been
proposed to mitigate the distributional gap between different imaging
modalities for unsupervised nuclei segmentation in histopathology images.
However, existing UDA methods are built upon the assumption that data
distributions within each domain should be uniform. Based on the
over-simplified supposition, they propose to align the histopathology target
domain with the source domain integrally, neglecting severe intra-domain
discrepancy over subpartitions incurred by mixed cancer types and sampling
organs. In this paper, for the first time, we propose to explicitly consider
the heterogeneity within the histopathology domain and introduce open compound
domain adaptation (OCDA) to resolve the crux. In specific, a two-stage
disentanglement framework is proposed to acquire domain-invariant feature
representations at both image and instance levels. The holistic design
addresses the limitations of existing OCDA approaches which struggle to capture
instance-wise variations. Two regularization strategies are specifically
devised herein to leverage the rich subpartition-specific characteristics in
histopathology images and facilitate subdomain decomposition. Moreover, we
propose a dual-branch nucleus shape and structure preserving module to prevent
nucleus over-generation and deformation in the synthesized images. Experimental
results on both cross-modality and cross-stain scenarios over a broad range of
diverse datasets demonstrate the superiority of our method compared with
state-of-the-art UDA and OCDA methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jianan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dongnan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Hang Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Weidong Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09515">
<title>Enhancing Surveillance Camera FOV Quality via Semantic Line Detection and Classification with Deep Hough Transform. (arXiv:2401.09515v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09515</link>
<description rdf:parseType="Literal">&lt;p&gt;The quality of recorded videos and images is significantly influenced by the
camera&apos;s field of view (FOV). In critical applications like surveillance
systems and self-driving cars, an inadequate FOV can give rise to severe safety
and security concerns, including car accidents and thefts due to the failure to
detect individuals and objects. The conventional methods for establishing the
correct FOV heavily rely on human judgment and lack automated mechanisms to
assess video and image quality based on FOV. In this paper, we introduce an
innovative approach that harnesses semantic line detection and classification
alongside deep Hough transform to identify semantic lines, thus ensuring a
suitable FOV by understanding 3D view through parallel lines. Our approach
yields an effective F1 score of 0.729 on the public EgoCart dataset, coupled
with a notably high median score in the line placement metric. We illustrate
that our method offers a straightforward means of assessing the quality of the
camera&apos;s field of view, achieving a classification accuracy of 83.8\%. This
metric can serve as a proxy for evaluating the potential performance of video
and image quality applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freeman_A/0/1/0/all/0/1&quot;&gt;Andrew C. Freeman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Wenjing Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_B/0/1/0/all/0/1&quot;&gt;Bin Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09518">
<title>On-Off Pattern Encoding and Path-Count Encoding as Deep Neural Network Representations. (arXiv:2401.09518v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09518</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the encoded representation of Deep Neural Networks (DNNs) has
been a fundamental yet challenging objective. In this work, we focus on two
possible directions for analyzing representations of DNNs by studying simple
image classification tasks. Specifically, we consider \textit{On-Off pattern}
and \textit{PathCount} for investigating how information is stored in deep
representations. On-off pattern of a neuron is decided as `on&apos; or `off&apos;
depending on whether the neuron&apos;s activation after ReLU is non-zero or zero.
PathCount is the number of paths that transmit non-zero energy from the input
to a neuron. We investigate how neurons in the network encodes information by
replacing each layer&apos;s activation with On-Off pattern or PathCount and
evaluating its effect on classification performance. We also examine
correlation between representation and PathCount. Finally, we show a possible
way to improve an existing DNN interpretation method, Class Activation Map
(CAM), by directly utilizing On-Off or PathCount.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_E/0/1/0/all/0/1&quot;&gt;Euna Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jaekeol Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_E/0/1/0/all/0/1&quot;&gt;EungGu Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1&quot;&gt;Wonjong Rhee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09596">
<title>Efficient generative adversarial networks using linear additive-attention Transformers. (arXiv:2401.09596v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09596</link>
<description rdf:parseType="Literal">&lt;p&gt;Although the capacity of deep generative models for image generation, such as
Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has
dramatically improved in recent years, much of their success can be attributed
to computationally expensive architectures. This has limited their adoption and
use to research laboratories and companies with large resources, while
significantly raising the carbon footprint for training, fine-tuning, and
inference. In this work, we present LadaGAN, an efficient generative
adversarial network that is built upon a novel Transformer block named
Ladaformer. The main component of this block is a linear additive-attention
mechanism that computes a single attention vector per head instead of the
quadratic dot-product attention. We employ Ladaformer in both the generator and
discriminator, which reduces the computational complexity and overcomes the
training instabilities often associated with Transformer GANs. LadaGAN
consistently outperforms existing convolutional and Transformer GANs on
benchmark datasets at different resolutions while being significantly more
efficient. Moreover, LadaGAN shows competitive performance compared to
state-of-the-art multi-step generative models (e.g. DMs) using orders of
magnitude less computational resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morales_Juarez_E/0/1/0/all/0/1&quot;&gt;Emilio Morales-Juarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuentes_Pineda_G/0/1/0/all/0/1&quot;&gt;Gibran Fuentes-Pineda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09603">
<title>Rethinking FID: Towards a Better Evaluation Metric for Image Generation. (arXiv:2401.09603v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09603</link>
<description rdf:parseType="Literal">&lt;p&gt;As with many machine learning problems, the progress of image generation
methods hinges on good evaluation metrics. One of the most popular is the
Frechet Inception Distance (FID). FID estimates the distance between a
distribution of Inception-v3 features of real images, and those of images
generated by the algorithm. We highlight important drawbacks of FID:
Inception&apos;s poor representation of the rich and varied content generated by
modern text-to-image models, incorrect normality assumptions, and poor sample
complexity. We call for a reevaluation of FID&apos;s use as the primary quality
metric for generated images. We empirically demonstrate that FID contradicts
human raters, it does not reflect gradual improvement of iterative
text-to-image models, it does not capture distortion levels, and that it
produces inconsistent results when varying the sample size. We also propose an
alternative new metric, CMMD, based on richer CLIP embeddings and the maximum
mean discrepancy distance with the Gaussian RBF kernel. It is an unbiased
estimator that does not make any assumptions on the probability distribution of
the embeddings and is sample efficient. Through extensive experiments and
analysis, we demonstrate that FID-based evaluations of text-to-image models may
be unreliable, and that CMMD offers a more robust and reliable assessment of
image quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayasumana_S/0/1/0/all/0/1&quot;&gt;Sadeep Jayasumana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1&quot;&gt;Srikumar Ramalingam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1&quot;&gt;Andreas Veit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glasner_D/0/1/0/all/0/1&quot;&gt;Daniel Glasner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1&quot;&gt;Ayan Chakrabarti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sanjiv Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09604">
<title>MedBlindTuner: Towards Privacy-preserving Fine-tuning on Biomedical Images with Transformers and Fully Homomorphic Encryption. (arXiv:2401.09604v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.09604</link>
<description rdf:parseType="Literal">&lt;p&gt;Advancements in machine learning (ML) have significantly revolutionized
medical image analysis, prompting hospitals to rely on external ML services.
However, the exchange of sensitive patient data, such as chest X-rays, poses
inherent privacy risks when shared with third parties. Addressing this concern,
we propose MedBlindTuner, a privacy-preserving framework leveraging fully
homomorphic encryption (FHE) and a data-efficient image transformer (DEiT).
MedBlindTuner enables the training of ML models exclusively on FHE-encrypted
medical images. Our experimental evaluation demonstrates that MedBlindTuner
achieves comparable accuracy to models trained on non-encrypted images,
offering a secure solution for outsourcing ML computations while preserving
patient data privacy. To the best of our knowledge, this is the first work that
uses data-efficient image transformers and fully homomorphic encryption in this
domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panzade_P/0/1/0/all/0/1&quot;&gt;Prajwal Panzade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takabi_D/0/1/0/all/0/1&quot;&gt;Daniel Takabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09606">
<title>Robustness Evaluation of Machine Learning Models for Robot Arm Action Recognition in Noisy Environments. (arXiv:2401.09606v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09606</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of robot action recognition, identifying distinct but spatially
proximate arm movements using vision systems in noisy environments poses a
significant challenge. This paper studies robot arm action recognition in noisy
environments using machine learning techniques. Specifically, a vision system
is used to track the robot&apos;s movements followed by a deep learning model to
extract the arm&apos;s key points. Through a comparative analysis of machine
learning methods, the effectiveness and robustness of this model are assessed
in noisy environments. A case study was conducted using the Tic-Tac-Toe game in
a 3-by-3 grid environment, where the focus is to accurately identify the
actions of the arms in selecting specific locations within this constrained
environment. Experimental results show that our approach can achieve precise
key point detection and action classification despite the addition of noise and
uncertainties to the dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Motamedi_E/0/1/0/all/0/1&quot;&gt;Elaheh Motamedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behzad_K/0/1/0/all/0/1&quot;&gt;Kian Behzad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zandi_R/0/1/0/all/0/1&quot;&gt;Rojin Zandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salehinejad_H/0/1/0/all/0/1&quot;&gt;Hojjat Salehinejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siami_M/0/1/0/all/0/1&quot;&gt;Milad Siami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09607">
<title>Land Cover Image Classification. (arXiv:2401.09607v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09607</link>
<description rdf:parseType="Literal">&lt;p&gt;Land Cover (LC) image classification has become increasingly significant in
understanding environmental changes, urban planning, and disaster management.
However, traditional LC methods are often labor-intensive and prone to human
error. This paper explores state-of-the-art deep learning models for enhanced
accuracy and efficiency in LC analysis. We compare convolutional neural
networks (CNN) against transformer-based methods, showcasing their applications
and advantages in LC studies. We used EuroSAT, a patch-based LC classification
data set based on Sentinel-2 satellite images and achieved state-of-the-art
results using current transformer models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rangel_A/0/1/0/all/0/1&quot;&gt;Antonio Rangel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terven_J/0/1/0/all/0/1&quot;&gt;Juan Terven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cordova_Esparza_D/0/1/0/all/0/1&quot;&gt;Diana M. Cordova-Esparza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chavez_Urbiola_E/0/1/0/all/0/1&quot;&gt;E.A. Chavez-Urbiola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09624">
<title>MITS-GAN: Safeguarding Medical Imaging from Tampering with Generative Adversarial Networks. (arXiv:2401.09624v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.09624</link>
<description rdf:parseType="Literal">&lt;p&gt;The progress in generative models, particularly Generative Adversarial
Networks (GANs), opened new possibilities for image generation but raised
concerns about potential malicious uses, especially in sensitive areas like
medical imaging. This study introduces MITS-GAN, a novel approach to prevent
tampering in medical images, with a specific focus on CT scans. The approach
disrupts the output of the attacker&apos;s CT-GAN architecture by introducing
imperceptible but yet precise perturbations. Specifically, the proposed
approach involves the introduction of appropriate Gaussian noise to the input
as a protective measure against various attacks. Our method aims to enhance
tamper resistance, comparing favorably to existing techniques. Experimental
results on a CT scan dataset demonstrate MITS-GAN&apos;s superior performance,
emphasizing its ability to generate tamper-resistant images with negligible
artifacts. As image tampering in medical domains poses life-threatening risks,
our proactive approach contributes to the responsible and ethical use of
generative models. This work provides a foundation for future research in
countering cyber threats in medical imaging. Models and codes are publicly
available at the following link
\url{https://iplab.dmi.unict.it/MITS-GAN-2024/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pasqualino_G/0/1/0/all/0/1&quot;&gt;Giovanni Pasqualino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guarnera_L/0/1/0/all/0/1&quot;&gt;Luca Guarnera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ortis_A/0/1/0/all/0/1&quot;&gt;Alessandro Ortis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Battiato_S/0/1/0/all/0/1&quot;&gt;Sebastiano Battiato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09627">
<title>SymTC: A Symbiotic Transformer-CNN Net for Instance Segmentation of Lumbar Spine MRI. (arXiv:2401.09627v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.09627</link>
<description rdf:parseType="Literal">&lt;p&gt;Intervertebral disc disease, a prevalent ailment, frequently leads to
intermittent or persistent low back pain, and diagnosing and assessing of this
disease rely on accurate measurement of vertebral bone and intervertebral disc
geometries from lumbar MR images. Deep neural network (DNN) models may assist
clinicians with more efficient image segmentation of individual instances
(disks and vertebrae) of the lumbar spine in an automated way, which is termed
as instance image segmentation. In this work, we proposed SymTC, an innovative
lumbar spine MR image segmentation model that combines the strengths of
Transformer and Convolutional Neural Network (CNN). Specifically, we designed a
parallel dual-path architecture to merge CNN layers and Transformer layers, and
we integrated a novel position embedding into the self-attention module of
Transformer, enhancing the utilization of positional information for more
accurate segmentation. To further improves model performance, we introduced a
new data augmentation technique to create synthetic yet realistic MR image
dataset, named SSMSpine, which is made publicly available. We evaluated our
SymTC and the other 15 existing image segmentation models on our private
in-house dataset and the public SSMSpine dataset, using two metrics, Dice
Similarity Coefficient and 95% Hausdorff Distance. The results show that our
SymTC has the best performance for segmenting vertebral bones and
intervertebral discs in lumbar spine MR images. The SymTC code and SSMSpine
dataset are available at https://github.com/jiasongchen/SymTC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiasong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qian_L/0/1/0/all/0/1&quot;&gt;Linchen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Linhai Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Urakov_T/0/1/0/all/0/1&quot;&gt;Timur Urakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gu_W/0/1/0/all/0/1&quot;&gt;Weiyong Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Liang Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09630">
<title>CT Liver Segmentation via PVT-based Encoding and Refined Decoding. (arXiv:2401.09630v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.09630</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate liver segmentation from CT scans is essential for computer-aided
diagnosis and treatment planning. Recently, Vision Transformers achieved a
competitive performance in computer vision tasks compared to convolutional
neural networks due to their exceptional ability to learn global
representations. However, they often struggle with scalability, memory
constraints, and computational inefficiency, particularly in handling
high-resolution medical images. To overcome scalability and efficiency issues,
we propose a novel deep learning approach, \textit{\textbf{PVTFormer}}, that is
built upon a pretrained pyramid vision transformer (PVT v2) combined with
advanced residual upsampling and decoder block. By integrating a refined
feature channel approach with hierarchical decoding strategy, PVTFormer
generates high quality segmentation masks by enhancing semantic features.
Rigorous evaluation of the proposed method on Liver Tumor Segmentation
Benchmark (LiTS) 2017 demonstrates that our proposed architecture not only
achieves a high dice coefficient of 86.78\%, mIoU of 78.46\%, but also obtains
a low HD of 3.50. The results underscore PVTFormer&apos;s efficacy in setting a new
benchmark for state-of-the-art liver segmentation methods. The source code of
the proposed PVTFormer is available at
\url{https://github.com/DebeshJha/PVTFormer}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jha_D/0/1/0/all/0/1&quot;&gt;Debesh Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tomar_N/0/1/0/all/0/1&quot;&gt;Nikhil Kumar Tomar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Biswas_K/0/1/0/all/0/1&quot;&gt;Koushik Biswas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Durak_G/0/1/0/all/0/1&quot;&gt;Gorkem Durak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Medetalibeyoglu_A/0/1/0/all/0/1&quot;&gt;Alpay Medetalibeyoglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Antalek_M/0/1/0/all/0/1&quot;&gt;Matthew Antalek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Velichko_Y/0/1/0/all/0/1&quot;&gt;Yury Velichko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ladner_D/0/1/0/all/0/1&quot;&gt;Daniela Ladner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Borhani_A/0/1/0/all/0/1&quot;&gt;Amir Borhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bagci_U/0/1/0/all/0/1&quot;&gt;Ulas Bagci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09638">
<title>Automatic 3D Multi-modal Ultrasound Segmentation of Human Placenta using Fusion Strategies and Deep Learning. (arXiv:2401.09638v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.09638</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: Ultrasound is the most commonly used medical imaging modality for
diagnosis and screening in clinical practice. Due to its safety profile,
noninvasive nature and portability, ultrasound is the primary imaging modality
for fetal assessment in pregnancy. Current ultrasound processing methods are
either manual or semi-automatic and are therefore laborious, time-consuming and
prone to errors, and automation would go a long way in addressing these
challenges. Automated identification of placental changes at earlier gestation
could facilitate potential therapies for conditions such as fetal growth
restriction and pre-eclampsia that are currently detected only at late
gestational age, potentially preventing perinatal morbidity and mortality.
&lt;/p&gt;
&lt;p&gt;Methods: We propose an automatic three-dimensional multi-modal (B-mode and
power Doppler) ultrasound segmentation of the human placenta using deep
learning combined with different fusion strategies.We collected data containing
Bmode and power Doppler ultrasound scans for 400 studies.
&lt;/p&gt;
&lt;p&gt;Results: We evaluated different fusion strategies and state-of-the-art image
segmentation networks for placenta segmentation based on standard overlap- and
boundary-based metrics. We found that multimodal information in the form of
B-mode and power Doppler scans outperform any single modality. Furthermore, we
found that B-mode and power Doppler input scans fused at the data level provide
the best results with a mean Dice Similarity Coefficient (DSC) of 0.849.
&lt;/p&gt;
&lt;p&gt;Conclusion: We conclude that the multi-modal approach of combining B-mode and
power Doppler scans is effective in segmenting the placenta from 3D ultrasound
scans in a fully automated manner and is robust to quality variation of the
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sonit Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Stevenson_G/0/1/0/all/0/1&quot;&gt;Gordon Stevenson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mein_B/0/1/0/all/0/1&quot;&gt;Brendan Mein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Welsh_A/0/1/0/all/0/1&quot;&gt;Alec Welsh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sowmya_A/0/1/0/all/0/1&quot;&gt;Arcot Sowmya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09639">
<title>Uncertainty Modeling in Ultrasound Image Segmentation for Precise Fetal Biometric Measurements. (arXiv:2401.09639v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.09639</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image segmentation, particularly in the context of ultrasound data,
is a crucial aspect of computer vision and medical imaging. This paper delves
into the complexities of uncertainty in the segmentation process, focusing on
fetal head and femur ultrasound images. The proposed methodology involves
extracting target contours and exploring techniques for precise parameter
measurement. Uncertainty modeling methods are employed to enhance the training
and testing processes of the segmentation network. The study reveals that the
average absolute error in fetal head circumference measurement is 8.0833mm,
with a relative error of 4.7347%. Similarly, the average absolute error in
fetal femur measurement is 2.6163mm, with a relative error of 6.3336%.
Uncertainty modeling experiments employing Test-Time Augmentation (TTA)
demonstrate effective interpretability of data uncertainty on both datasets.
This suggests that incorporating data uncertainty based on the TTA method can
support clinical practitioners in making informed decisions and obtaining more
reliable measurement results in practical clinical applications. The paper
contributes to the advancement of ultrasound image segmentation, addressing
critical challenges and improving the reliability of biometric measurements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lei_S/0/1/0/all/0/1&quot;&gt;Shuge Lei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09671">
<title>Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach. (arXiv:2401.09671v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09671</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain translation (UDT) aims to find functions that convert
samples from one domain (e.g., sketches) to another domain (e.g., photos)
without changing the high-level semantic meaning (also referred to as
``content&apos;&apos;). The translation functions are often sought by probability
distribution matching of the transformed source domain and target domain.
CycleGAN stands as arguably the most representative approach among this line of
work. However, it was noticed in the literature that CycleGAN and variants
could fail to identify the desired translation functions and produce
content-misaligned translations. This limitation arises due to the presence of
multiple translation functions -- referred to as ``measure-preserving
automorphism&quot; (MPA) -- in the solution space of the learning criteria. Despite
awareness of such identifiability issues, solutions have remained elusive. This
study delves into the core identifiability inquiry and introduces an MPA
elimination theory. Our analysis shows that MPA is unlikely to exist, if
multiple pairs of diverse cross-domain conditional distributions are matched by
the learning function. Our theory leads to a UDT learner using distribution
matching over auxiliary variable-induced subsets of the domains -- other than
over the entire data domains as in the classical approaches. The proposed
framework is the first to rigorously establish translation identifiability
under reasonable UDT settings, to our best knowledge. Experiments corroborate
with our theoretical claims.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrestha_S/0/1/0/all/0/1&quot;&gt;Sagar Shrestha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xiao Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09673">
<title>Artwork Protection Against Neural Style Transfer Using Locally Adaptive Adversarial Color Attack. (arXiv:2401.09673v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09673</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural style transfer (NST) is widely adopted in computer vision to generate
new images with arbitrary styles. This process leverages neural networks to
merge aesthetic elements of a style image with the structural aspects of a
content image into a harmoniously integrated visual result. However,
unauthorized NST can exploit artwork. Such misuse raises socio-technical
concerns regarding artists&apos; rights and motivates the development of technical
approaches for the proactive protection of original creations. Adversarial
attack is a concept primarily explored in machine learning security. Our work
introduces this technique to protect artists&apos; intellectual property. In this
paper Locally Adaptive Adversarial Color Attack (LAACA), a method for altering
images in a manner imperceptible to the human eyes but disruptive to NST.
Specifically, we design perturbations targeting image areas rich in
high-frequency content, generated by disrupting intermediate features. Our
experiments and user study confirm that by attacking NST using the proposed
method results in visually worse neural style transfer, thus making it an
effective solution for visual artwork protection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhongliang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaixuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weiye Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yifei Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1&quot;&gt;Ognjen Arandjelovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1&quot;&gt;Lei Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09677">
<title>Eye Motion Matters for 3D Face Reconstruction. (arXiv:2401.09677v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09677</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in single-image 3D face reconstruction have shown remarkable
progress in various applications. Nevertheless, prevailing techniques tend to
prioritize the global facial contour and expression, often neglecting the
nuanced dynamics of the eye region. In response, we introduce an Eye Landmark
Adjustment Module, complemented by a Local Dynamic Loss, designed to capture
the dynamic features of the eyes area. Our module allows for flexible
adjustment of landmarks, resulting in accurate recreation of various eye
states. In this paper, we present a comprehensive evaluation of our approach,
conducting extensive experiments on two datasets. The results underscore the
superior performance of our approach, highlighting its significant
contributions in addressing this particular challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mengyuan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09709">
<title>P2Seg: Pointly-supervised Segmentation via Mutual Distillation. (arXiv:2401.09709v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09709</link>
<description rdf:parseType="Literal">&lt;p&gt;Point-level Supervised Instance Segmentation (PSIS) aims to enhance the
applicability and scalability of instance segmentation by utilizing low-cost
yet instance-informative annotations. Existing PSIS methods usually rely on
positional information to distinguish objects, but predicting precise
boundaries remains challenging due to the lack of contour annotations.
Nevertheless, weakly supervised semantic segmentation methods are proficient in
utilizing intra-class feature consistency to capture the boundary contours of
the same semantic regions. In this paper, we design a Mutual Distillation
Module (MDM) to leverage the complementary strengths of both instance position
and semantic information and achieve accurate instance-level object perception.
The MDM consists of Semantic to Instance (S2I) and Instance to Semantic (I2S).
S2I is guided by the precise boundaries of semantic regions to learn the
association between annotated points and instance contours. I2S leverages
discriminative relationships between instances to facilitate the
differentiation of various objects within the semantic map. Extensive
experiments substantiate the efficacy of MDM in fostering the synergy between
instance and semantic information, consequently improving the quality of
instance-level object representations. Our method achieves 55.7 mAP$_{50}$ and
17.6 mAP on the PASCAL VOC and MS COCO datasets, significantly outperforming
recent PSIS methods and several box-supervised instance segmentation
competitors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zipeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xuehui Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xumeng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wenwen Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhixun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1&quot;&gt;Jianbin Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhenjun Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09712">
<title>SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model. (arXiv:2401.09712v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09712</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have recently been extended to the
vision-language realm, obtaining impressive general multi-modal capabilities.
However, the exploration of multi-modal large language models (MLLMs) for
remote sensing (RS) data is still in its infancy, and the performance is not
satisfactory. In this work, we introduce SkyEyeGPT, a unified multi-modal large
language model specifically designed for RS vision-language understanding. To
this end, we meticulously curate an RS multi-modal instruction tuning dataset,
including single-task and multi-task conversation instructions. After manual
verification, we obtain a high-quality RS instruction-following dataset with
968k samples. Our research demonstrates that with a simple yet effective
design, SkyEyeGPT works surprisingly well on considerably different tasks
without the need for extra encoding modules. Specifically, after projecting RS
visual features to the language domain via an alignment layer, they are fed
jointly with task-specific instructions into an LLM-based RS decoder to predict
answers for RS open-ended tasks. In addition, we design a two-stage tuning
method to enhance instruction-following and multi-turn dialogue ability at
different granularities. Experiments on 8 datasets for RS vision-language tasks
demonstrate SkyEyeGPT&apos;s superiority in image-level and region-level tasks, such
as captioning and visual grounding. In particular, SkyEyeGPT exhibits
encouraging results compared to GPT-4V in some qualitative tests. The online
demo, code, and dataset will be released in
https://github.com/ZhanYang-nwpu/SkyEyeGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1&quot;&gt;Yang Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zhitong Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuan Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09716">
<title>HCVP: Leveraging Hierarchical Contrastive Visual Prompt for Domain Generalization. (arXiv:2401.09716v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09716</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain Generalization (DG) endeavors to create machine learning models that
excel in unseen scenarios by learning invariant features. In DG, the prevalent
practice of constraining models to a fixed structure or uniform
parameterization to encapsulate invariant features can inadvertently blend
specific aspects. Such an approach struggles with nuanced differentiation of
inter-domain variations and may exhibit bias towards certain domains, hindering
the precise learning of domain-invariant features. Recognizing this, we
introduce a novel method designed to supplement the model with domain-level and
task-specific characteristics. This approach aims to guide the model in more
effectively separating invariant features from specific characteristics,
thereby boosting the generalization. Building on the emerging trend of visual
prompts in the DG paradigm, our work introduces the novel \textbf{H}ierarchical
\textbf{C}ontrastive \textbf{V}isual \textbf{P}rompt (HCVP) methodology. This
represents a significant advancement in the field, setting itself apart with a
unique generative approach to prompts, alongside an explicit model structure
and specialized loss functions. Differing from traditional visual prompts that
are often shared across entire datasets, HCVP utilizes a hierarchical prompt
generation network enhanced by prompt contrastive learning. These generative
prompts are instance-dependent, catering to the unique characteristics inherent
to different domains and tasks. Additionally, we devise a prompt modulation
network that serves as a bridge, effectively incorporating the generated visual
prompts into the vision transformer backbone. Experiments conducted on five DG
datasets demonstrate the effectiveness of HCVP, outperforming both established
DG algorithms and adaptation protocols.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Guanglin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhongyi Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shiming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Biwei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Liming Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tongliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1&quot;&gt;Lina Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09720">
<title>GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting. (arXiv:2401.09720v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09720</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a novel clothed human reconstruction method called
GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural
radiance based models, 3D Gaussian Splatting has recently demonstrated great
performance in terms of training time and rendering quality. However, applying
the static 3D Gaussian Splatting model to the dynamic human reconstruction
problem is non-trivial due to complicated non-rigid deformations and rich cloth
details. To address these challenges, our method considers explicit pose-guided
deformation to associate dynamic Gaussians across the canonical space and the
observation space, introducing a physically-based prior with regularized
transformations helps mitigate ambiguity between the two spaces. During the
training process, we further propose a pose refinement strategy to update the
pose regression for compensating the inaccurate initial estimation and a
split-with-scale mechanism to enhance the density of regressed point clouds.
The experiments validate that our method can achieve state-of-the-art
photorealistic novel-view rendering results with high-quality details for
dynamic clothed human bodies, along with explicit geometry reconstruction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mengtian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1&quot;&gt;Shengxiang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Keyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Gang Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09721">
<title>fast graph-based denoising for point cloud color information. (arXiv:2401.09721v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09721</link>
<description rdf:parseType="Literal">&lt;p&gt;Point clouds are utilized in various 3D applications such as cross-reality
(XR) and realistic 3D displays. In some applications, e.g., for live streaming
using a 3D point cloud, real-time point cloud denoising methods are required to
enhance the visual quality. However, conventional high-precision denoising
methods cannot be executed in real time for large-scale point clouds owing to
the complexity of graph constructions with K nearest neighbors and noise level
estimation. This paper proposes a fast graph-based denoising (FGBD) for a
large-scale point cloud. First, high-speed graph construction is achieved by
scanning a point cloud in various directions and searching adjacent
neighborhoods on the scanning lines. Second, we propose a fast noise level
estimation method using eigenvalues of the covariance matrix on a graph.
Finally, we also propose a new low-cost filter selection method to enhance
denoising accuracy to compensate for the degradation caused by the acceleration
algorithms. In our experiments, we succeeded in reducing the processing time
dramatically while maintaining accuracy relative to conventional denoising
methods. Denoising was performed at 30fps, with frames containing approximately
1 million points.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watanabe_R/0/1/0/all/0/1&quot;&gt;Ryosuke Watanabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nonaka_K/0/1/0/all/0/1&quot;&gt;Keisuke Nonaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavez_E/0/1/0/all/0/1&quot;&gt;Eduardo Pavez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobayashi_T/0/1/0/all/0/1&quot;&gt;Tatsuya Kobayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_A/0/1/0/all/0/1&quot;&gt;Antonio Ortega&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09732">
<title>Instance Brownian Bridge as Texts for Open-vocabulary Video Instance Segmentation. (arXiv:2401.09732v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09732</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporally locating objects with arbitrary class texts is the primary pursuit
of open-vocabulary Video Instance Segmentation (VIS). Because of the
insufficient vocabulary of video data, previous methods leverage image-text
pretraining model for recognizing object instances by separately aligning each
frame and class texts, ignoring the correlation between frames. As a result,
the separation breaks the instance movement context of videos, causing inferior
alignment between video and text. To tackle this issue, we propose to link
frame-level instance representations as a Brownian Bridge to model instance
dynamics and align bridge-level instance representation to class texts for more
precisely open-vocabulary VIS (BriVIS). Specifically, we build our system upon
a frozen video segmentor to generate frame-level instance queries, and design
Temporal Instance Resampler (TIR) to generate queries with temporal context
from frame queries. To mold instance queries to follow Brownian bridge and
accomplish alignment with class texts, we design Bridge-Text Alignment (BTA) to
learn discriminative bridge-level representations of instances via contrastive
objectives. Setting MinVIS as the basic video segmentor, BriVIS surpasses the
Open-vocabulary SOTA (OV2Seg) by a clear margin. For example, on the
challenging large-vocabulary VIS dataset (BURST), BriVIS achieves 7.43 mAP and
exhibits 49.49% improvement compared to OV2Seg (4.97 mAP).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Zesen Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kehan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1&quot;&gt;Peng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xiawu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09736">
<title>Measuring the Discrepancy between 3D Geometric Models using Directional Distance Fields. (arXiv:2401.09736v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09736</link>
<description rdf:parseType="Literal">&lt;p&gt;Qualifying the discrepancy between 3D geometric models, which could be
represented with either point clouds or triangle meshes, is a pivotal issue
with board applications. Existing methods mainly focus on directly establishing
the correspondence between two models and then aggregating point-wise distance
between corresponding points, resulting in them being either inefficient or
ineffective. In this paper, we propose DirDist, an efficient, effective,
robust, and differentiable distance metric for 3D geometry data. Specifically,
we construct DirDist based on the proposed implicit representation of 3D
models, namely directional distance field (DDF), which defines the directional
distances of 3D points to a model to capture its local surface geometry. We
then transfer the discrepancy between two 3D geometric models as the
discrepancy between their DDFs defined on an identical domain, naturally
establishing model correspondence. To demonstrate the advantage of our DirDist,
we explore various distance metric-driven 3D geometric modeling tasks,
including template surface fitting, rigid registration, non-rigid registration,
scene flow estimation and human pose optimization. Extensive experiments show
that our DirDist achieves significantly higher accuracy under all tasks. As a
generic distance metric, DirDist has the potential to advance the field of 3D
geometric modeling. The source code is available at
\url{https://github.com/rsy6318/DirDist}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Siyu Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Junhui Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaodong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hongkai Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09742">
<title>Image Translation as Diffusion Visual Programmers. (arXiv:2401.09742v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09742</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the novel Diffusion Visual Programmer (DVP), a neuro-symbolic
image translation framework. Our proposed DVP seamlessly embeds a
condition-flexible diffusion model within the GPT architecture, orchestrating a
coherent sequence of visual programs (i.e., computer vision models) for various
pro-symbolic steps, which span RoI identification, style transfer, and position
manipulation, facilitating transparent and controllable image translation
processes. Extensive experiments demonstrate DVP&apos;s remarkable performance,
surpassing concurrent arts. This success can be attributed to several key
features of DVP: First, DVP achieves condition-flexible translation via
instance normalization, enabling the model to eliminate sensitivity caused by
the manual guidance and optimally focus on textual descriptions for
high-quality content generation. Second, the framework enhances in-context
reasoning by deciphering intricate high-dimensional concepts in feature spaces
into more accessible low-dimensional symbols (e.g., [Prompt], [RoI object]),
allowing for localized, context-free editing while maintaining overall
coherence. Last but not least, DVP improves systemic controllability and
explainability by offering explicit symbolic representations at each
programming stage, empowering users to intuitively interpret and modify
results. Our research marks a substantial step towards harmonizing artificial
image translation processes with cognitive intelligence, promising broader
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Cheng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;James C. Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabbani_M/0/1/0/all/0/1&quot;&gt;Majid Rabbani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dianat_S/0/1/0/all/0/1&quot;&gt;Sohail Dianat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_R/0/1/0/all/0/1&quot;&gt;Raghuveer Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Ying Nian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dongfang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09759">
<title>SlideAVSR: A Dataset of Paper Explanation Videos for Audio-Visual Speech Recognition. (arXiv:2401.09759v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09759</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio-visual speech recognition (AVSR) is a multimodal extension of automatic
speech recognition (ASR), using video as a complement to audio. In AVSR,
considerable efforts have been directed at datasets for facial features such as
lip-readings, while they often fall short in evaluating the image comprehension
capabilities in broader contexts. In this paper, we construct SlideAVSR, an
AVSR dataset using scientific paper explanation videos. SlideAVSR provides a
new benchmark where models transcribe speech utterances with texts on the
slides on the presentation recordings. As technical terminologies that are
frequent in paper explanations are notoriously challenging to transcribe
without reference texts, our SlideAVSR dataset spotlights a new aspect of AVSR
problems. As a simple yet effective baseline, we propose DocWhisper, an AVSR
model that can refer to textual information from slides, and confirm its
effectiveness on SlideAVSR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurita_S/0/1/0/all/0/1&quot;&gt;Shuhei Kurita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shimizu_S/0/1/0/all/0/1&quot;&gt;Shuichiro Shimizu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawahara_D/0/1/0/all/0/1&quot;&gt;Daisuke Kawahara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09763">
<title>CLIP Model for Images to Textual Prompts Based on Top-k Neighbors. (arXiv:2401.09763v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09763</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image synthesis, a subfield of multimodal generation, has gained
significant attention in recent years. We propose a cost-effective approach for
image-to-prompt generation that leverages generative models to generate textual
prompts without the need for large amounts of annotated data. We divide our
method into two stages: online stage and offline stage. We use a combination of
the CLIP model and K-nearest neighbors (KNN) algorithm. The proposed system
consists of two main parts: an offline task and an online task. Our method owns
the highest metric 0.612 among these models, which is 0.013, 0.055, 0.011
higher than Clip, Clip + KNN(top 10) respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;YeMing Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_T/0/1/0/all/0/1&quot;&gt;Tianzhi Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09773">
<title>SEINE: Structure Encoding and Interaction Network for Nuclei Instance Segmentation. (arXiv:2401.09773v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09773</link>
<description rdf:parseType="Literal">&lt;p&gt;Nuclei instance segmentation in histopathological images is of great
importance for biological analysis and cancer diagnosis but remains challenging
for two reasons. (1) Similar visual presentation of intranuclear and
extranuclear regions of chromophobe nuclei often causes under-segmentation, and
(2) current methods lack the exploration of nuclei structure, resulting in
fragmented instance predictions. To address these problems, this paper proposes
a structure encoding and interaction network, termed SEINE, which develops the
structure modeling scheme of nuclei and exploits the structure similarity
between nuclei to improve the integrality of each segmented instance.
Concretely, SEINE introduces a contour-based structure encoding (SE) that
considers the correlation between nuclei structure and semantics, realizing a
reasonable representation of the nuclei structure. Based on the encoding, we
propose a structure-guided attention (SGA) that takes the clear nuclei as
prototypes to enhance the structure learning for the fuzzy nuclei. To
strengthen the structural learning ability, a semantic feature fusion (SFF) is
presented to boost the semantic consistency of semantic and structure branches.
Furthermore, a position enhancement (PE) method is applied to suppress
incorrect nuclei boundary predictions. Extensive experiments demonstrate the
superiority of our approaches, and SEINE achieves state-of-the-art (SOTA)
performance on four datasets. The code is available at
\href{https://github.com/zhangye-zoe/SEINE}{https://github.com/zhangye-zoe/SEINE}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ye Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1&quot;&gt;Linghan Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongbing Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09774">
<title>On the Audio Hallucinations in Large Audio-Video Language Models. (arXiv:2401.09774v1 [cs.MM])</title>
<link>http://arxiv.org/abs/2401.09774</link>
<description rdf:parseType="Literal">&lt;p&gt;Large audio-video language models can generate descriptions for both video
and audio. However, they sometimes ignore audio content, producing audio
descriptions solely reliant on visual information. This paper refers to this as
audio hallucinations and analyzes them in large audio-video language models. We
gather 1,000 sentences by inquiring about audio information and annotate them
whether they contain hallucinations. If a sentence is hallucinated, we also
categorize the type of hallucination. The results reveal that 332 sentences are
hallucinated with distinct trends observed in nouns and verbs for each
hallucination type. Based on this, we tackle a task of audio hallucination
classification using pre-trained audio-text models in the zero-shot and
fine-tuning settings. Our experimental results reveal that the zero-shot models
achieve higher performance (52.2% in F1) than the random (40.3%) and the
fine-tuning models achieve 87.9%, outperforming the zero-shot models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishimura_T/0/1/0/all/0/1&quot;&gt;Taichi Nishimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakada_S/0/1/0/all/0/1&quot;&gt;Shota Nakada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondo_M/0/1/0/all/0/1&quot;&gt;Masayoshi Kondo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09786">
<title>Adaptive Self-training Framework for Fine-grained Scene Graph Generation. (arXiv:2401.09786v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09786</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene graph generation (SGG) models have suffered from inherent problems
regarding the benchmark datasets such as the long-tailed predicate distribution
and missing annotation problems. In this work, we aim to alleviate the
long-tailed problem of SGG by utilizing unannotated triplets. To this end, we
introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels
for unannotated triplets based on which the SGG models are trained. While there
has been significant progress in self-training for image recognition, designing
a self-training framework for the SGG task is more challenging due to its
inherent nature such as the semantic ambiguity and the long-tailed distribution
of predicate classes. Hence, we propose a novel pseudo-labeling technique for
SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is
a model-agnostic framework that can be applied to any existing SGG models.
Furthermore, we devise a graph structure learner (GSL) that is beneficial when
adopting our proposed self-training framework to the state-of-the-art
message-passing neural network (MPNN)-based SGG models. Our extensive
experiments verify the effectiveness of ST-SGG on various SGG models,
particularly in enhancing the performance on fine-grained predicate classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kibum Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1&quot;&gt;Kanghoon Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+In_Y/0/1/0/all/0/1&quot;&gt;Yeonjun In&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1&quot;&gt;Jinyoung Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Donghyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Chanyoung Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09791">
<title>BreastRegNet: A Deep Learning Framework for Registration of Breast Faxitron and Histopathology Images. (arXiv:2401.09791v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.09791</link>
<description rdf:parseType="Literal">&lt;p&gt;A standard treatment protocol for breast cancer entails administering
neoadjuvant therapy followed by surgical removal of the tumor and surrounding
tissue. Pathologists typically rely on cabinet X-ray radiographs, known as
Faxitron, to examine the excised breast tissue and diagnose the extent of
residual disease. However, accurately determining the location, size, and
focality of residual cancer can be challenging, and incorrect assessments can
lead to clinical consequences. The utilization of automated methods can improve
the histopathology process, allowing pathologists to choose regions for
sampling more effectively and precisely. Despite the recognized necessity,
there are currently no such methods available. Training such automated
detection models require accurate ground truth labels on ex-vivo radiology
images, which can be acquired through registering Faxitron and histopathology
images and mapping the extent of cancer from histopathology to x-ray images.
This study introduces a deep learning-based image registration approach trained
on mono-modal synthetic image pairs. The models were trained using data from 50
women who received neoadjuvant chemotherapy and underwent surgery. The results
demonstrate that our method is faster and yields significantly lower average
landmark error ($2.1\pm1.96$ mm) over the state-of-the-art iterative
($4.43\pm4.1$ mm) and deep learning ($4.02\pm3.15$ mm) approaches. Improved
performance of our approach in integrating radiology and pathology information
facilitates generating large datasets, which allows training models for more
accurate breast cancer detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Golestani_N/0/1/0/all/0/1&quot;&gt;Negar Golestani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Aihui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bean_G/0/1/0/all/0/1&quot;&gt;Gregory R Bean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rusu_M/0/1/0/all/0/1&quot;&gt;Mirabela Rusu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09794">
<title>Wavelet-Guided Acceleration of Text Inversion in Diffusion-Based Image Editing. (arXiv:2401.09794v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09794</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of image editing, Null-text Inversion (NTI) enables fine-grained
editing while preserving the structure of the original image by optimizing null
embeddings during the DDIM sampling process. However, the NTI process is
time-consuming, taking more than two minutes per image. To address this, we
introduce an innovative method that maintains the principles of the NTI while
accelerating the image editing process. We propose the WaveOpt-Estimator, which
determines the text optimization endpoint based on frequency characteristics.
Utilizing wavelet transform analysis to identify the image&apos;s frequency
characteristics, we can limit text optimization to specific timesteps during
the DDIM sampling process. By adopting the Negative-Prompt Inversion (NPI)
concept, a target prompt representing the original image serves as the initial
text value for optimization. This approach maintains performance comparable to
NTI while reducing the average editing time by over 80% compared to the NTI
method. Our method presents a promising approach for efficient, high-quality
image editing based on diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koo_G/0/1/0/all/0/1&quot;&gt;Gwanhyeong Koo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sunjae Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1&quot;&gt;Chang D. Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09802">
<title>Multilingual Visual Speech Recognition with a Single Model by Learning with Discrete Visual Speech Units. (arXiv:2401.09802v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2401.09802</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores sentence-level Multilingual Visual Speech Recognition
with a single model for the first time. As the massive multilingual modeling of
visual data requires huge computational costs, we propose a novel strategy,
processing with visual speech units. Motivated by the recent success of the
audio speech unit, the proposed visual speech unit is obtained by discretizing
the visual speech features extracted from the self-supervised visual speech
model. To correctly capture multilingual visual speech, we first train the
self-supervised visual speech model on 5,512 hours of multilingual audio-visual
data. Through analysis, we verify that the visual speech units mainly contain
viseme information while suppressing non-linguistic information. By using the
visual speech units as the inputs of our system, we pre-train the model to
predict corresponding text outputs on massive multilingual data constructed by
merging several VSR databases. As both the inputs and outputs are discrete, we
can greatly improve the training efficiency compared to the standard VSR
training. Specifically, the input data size is reduced to 0.016% of the
original video inputs. In order to complement the insufficient visual
information in speech recognition, we apply curriculum learning where the
inputs of the system begin with audio-visual speech units and gradually change
to visual speech units. After pre-training, the model is finetuned on
continuous features. We set new state-of-the-art multilingual VSR performances
by achieving comparable performances to the previous language-specific VSR
models, with a single trained model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yeo_J/0/1/0/all/0/1&quot;&gt;Jeong Hun Yeo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jeongsoo Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Se Jin Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ro_Y/0/1/0/all/0/1&quot;&gt;Yong Man Ro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09823">
<title>Enhancing Small Object Encoding in Deep Neural Networks: Introducing Fast&amp;Focused-Net with Volume-wise Dot Product Layer. (arXiv:2401.09823v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09823</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce Fast&amp;amp;Focused-Net, a novel deep neural network
architecture tailored for efficiently encoding small objects into fixed-length
feature vectors. Contrary to conventional Convolutional Neural Networks (CNNs),
Fast&amp;amp;Focused-Net employs a series of our newly proposed layer, the Volume-wise
Dot Product (VDP) layer, designed to address several inherent limitations of
CNNs. Specifically, CNNs often exhibit a smaller effective receptive field than
their theoretical counterparts, limiting their vision span. Additionally, the
initial layers in CNNs produce low-dimensional feature vectors, presenting a
bottleneck for subsequent learning. Lastly, the computational overhead of CNNs,
particularly in capturing diverse image regions by parameter sharing, is
significantly high. The VDP layer, at the heart of Fast&amp;amp;Focused-Net, aims to
remedy these issues by efficiently covering the entire image patch information
with reduced computational demand. Experimental results demonstrate the prowess
of Fast&amp;amp;Focused-Net in a variety of applications. For small object
classification tasks, our network outperformed state-of-the-art methods on
datasets such as CIFAR-10, CIFAR-100, STL-10, SVHN-Cropped, and Fashion-MNIST.
In the context of larger image classification, when combined with a transformer
encoder (ViT), Fast&amp;amp;Focused-Net produced competitive results for OpenImages V6,
ImageNet-1K, and Places365 datasets. Moreover, the same combination showcased
unparalleled performance in text recognition tasks across SVT, IC15, SVTP, and
HOST datasets. This paper presents the architecture, the underlying motivation,
and extensive empirical evidence suggesting that Fast&amp;amp;Focused-Net is a
promising direction for efficient and focused deep learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tofik_A/0/1/0/all/0/1&quot;&gt;Ali Tofik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pratim_R/0/1/0/all/0/1&quot;&gt;Roy Partha Pratim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09826">
<title>Boosting Few-Shot Semantic Segmentation Via Segment Anything Model. (arXiv:2401.09826v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09826</link>
<description rdf:parseType="Literal">&lt;p&gt;In semantic segmentation, accurate prediction masks are crucial for
downstream tasks such as medical image analysis and image editing. Due to the
lack of annotated data, few-shot semantic segmentation (FSS) performs poorly in
predicting masks with precise contours. Recently, we have noticed that the
large foundation model segment anything model (SAM) performs well in processing
detailed features. Inspired by SAM, we propose FSS-SAM to boost FSS methods by
addressing the issue of inaccurate contour. The FSS-SAM is training-free. It
works as a post-processing tool for any FSS methods and can improve the
accuracy of predicted masks. Specifically, we use predicted masks from FSS
methods to generate prompts and then use SAM to predict new masks. To avoid
predicting wrong masks with SAM, we propose a prediction result selection (PRS)
algorithm. The algorithm can remarkably decrease wrong predictions. Experiment
results on public datasets show that our method is superior to base FSS methods
in both quantitative and qualitative aspects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1&quot;&gt;Chen-Bin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_Q/0/1/0/all/0/1&quot;&gt;Qi Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kangdao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Houcheng Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vong_C/0/1/0/all/0/1&quot;&gt;Chi-Man Vong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09828">
<title>Enhanced Automated Quality Assessment Network for Interactive Building Segmentation in High-Resolution Remote Sensing Imagery. (arXiv:2401.09828v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09828</link>
<description rdf:parseType="Literal">&lt;p&gt;In this research, we introduce the enhanced automated quality assessment
network (IBS-AQSNet), an innovative solution for assessing the quality of
interactive building segmentation within high-resolution remote sensing
imagery. This is a new challenge in segmentation quality assessment, and our
proposed IBS-AQSNet allievate this by identifying missed and mistaken segment
areas. First of all, to acquire robust image features, our method combines a
robust, pre-trained backbone with a lightweight counterpart for comprehensive
feature extraction from imagery and segmentation results. These features are
then fused through a simple combination of concatenation, convolution layers,
and residual connections. Additionally, ISR-AQSNet incorporates a multi-scale
differential quality assessment decoder, proficient in pinpointing areas where
segmentation result is either missed or mistaken. Experiments on a newly-built
EVLab-BGZ dataset, which includes over 39,198 buildings, demonstrate the
superiority of the proposed method in automating segmentation quality
assessment, thereby setting a new benchmark in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhili Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiangyun Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiabo Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09833">
<title>Slicer Networks. (arXiv:2401.09833v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.09833</link>
<description rdf:parseType="Literal">&lt;p&gt;In medical imaging, scans often reveal objects with varied contrasts but
consistent internal intensities or textures. This characteristic enables the
use of low-frequency approximations for tasks such as segmentation and
deformation field estimation. Yet, integrating this concept into neural network
architectures for medical image analysis remains underexplored. In this paper,
we propose the Slicer Network, a novel architecture designed to leverage these
traits. Comprising an encoder utilizing models like vision transformers for
feature extraction and a slicer employing a learnable bilateral grid, the
Slicer Network strategically refines and upsamples feature maps via a
splatting-blurring-slicing process. This introduces an edge-preserving
low-frequency approximation for the network outcome, effectively enlarging the
effective receptive field. The enhancement not only reduces computational
complexity but also boosts overall performance. Experiments across different
medical imaging applications, including unsupervised and keypoints-based image
registration and lesion segmentation, have verified the Slicer Network&apos;s
improved accuracy and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rongguang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Renjiu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dongdong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gaolei Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09836">
<title>Exploring Latent Cross-Channel Embedding for Accurate 3D Human Pose Reconstruction in a Diffusion Framework. (arXiv:2401.09836v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09836</link>
<description rdf:parseType="Literal">&lt;p&gt;Monocular 3D human pose estimation poses significant challenges due to the
inherent depth ambiguities that arise during the reprojection process from 2D
to 3D. Conventional approaches that rely on estimating an over-fit projection
matrix struggle to effectively address these challenges and often result in
noisy outputs. Recent advancements in diffusion models have shown promise in
incorporating structural priors to address reprojection ambiguities. However,
there is still ample room for improvement as these methods often overlook the
exploration of correlation between the 2D and 3D joint-level features. In this
study, we propose a novel cross-channel embedding framework that aims to fully
explore the correlation between joint-level features of 3D coordinates and
their 2D projections. In addition, we introduce a context guidance mechanism to
facilitate the propagation of joint graph attention across latent channels
during the iterative diffusion process. To evaluate the effectiveness of our
proposed method, we conduct experiments on two benchmark datasets, namely
Human3.6M and MPI-INF-3DHP. Our results demonstrate a significant improvement
in terms of reconstruction accuracy compared to state-of-the-art methods. The
code for our method will be made available online for further reference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Junkun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09852">
<title>Enhancing the Fairness and Performance of Edge Cameras with Explainable AI. (arXiv:2401.09852v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09852</link>
<description rdf:parseType="Literal">&lt;p&gt;The rising use of Artificial Intelligence (AI) in human detection on Edge
camera systems has led to accurate but complex models, challenging to interpret
and debug. Our research presents a diagnostic method using Explainable AI (XAI)
for model debugging, with expert-driven problem identification and solution
creation. Validated on the Bytetrack model in a real-world office Edge network,
we found the training dataset as the main bias source and suggested model
augmentation as a solution. Our approach helps identify model biases, essential
for achieving fair and trustworthy models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Truong Thanh Hung Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1&quot;&gt;Vo Thanh Khang Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1&quot;&gt;Quoc Hung Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truong_V/0/1/0/all/0/1&quot;&gt;Van Binh Truong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1&quot;&gt;Quoc Khanh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1&quot;&gt;Hung Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09861">
<title>Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models. (arXiv:2401.09861v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09861</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in Multimodal Large Language Models (MLLMs) have
significantly enhanced the comprehension of multimedia content, bringing
together diverse modalities such as text, images, and videos. However, a
critical challenge faced by these models, especially when processing video
inputs, is the occurrence of hallucinations - erroneous perceptions or
interpretations, particularly at the event level. This study introduces an
innovative method to address event-level hallucinations in MLLMs, focusing on
specific temporal understanding in video content. Our approach leverages a
novel framework that extracts and utilizes event-specific information from both
the event query and the provided video to refine MLLMs&apos; response. We propose a
unique mechanism that decomposes on-demand event queries into iconic actions.
Subsequently, we employ models like CLIP and BLIP2 to predict specific
timestamps for event occurrences. Our evaluation, conducted using the
Charades-STA dataset, demonstrates a significant reduction in temporal
hallucinations and an improvement in the quality of event-related responses.
This research not only provides a new perspective in addressing a critical
limitation of MLLMs but also contributes a quantitatively measurable method for
evaluating MLLMs in the context of temporal-related questions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Li Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jun Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1&quot;&gt;Takayuki Okatani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09865">
<title>Improving fine-grained understanding in image-text pre-training. (arXiv:2401.09865v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09865</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce SPARse Fine-grained Contrastive Alignment (SPARC), a simple
method for pretraining more fine-grained multimodal representations from
image-text pairs. Given that multiple image patches often correspond to single
words, we propose to learn a grouping of image patches for every token in the
caption. To achieve this, we use a sparse similarity metric between image
patches and language tokens and compute for each token a language-grouped
vision embedding as the weighted average of patches. The token and
language-grouped vision embeddings are then contrasted through a fine-grained
sequence-wise loss that only depends on individual samples and does not require
other batch samples as negatives. This enables more detailed information to be
learned in a computationally inexpensive manner. SPARC combines this
fine-grained loss with a contrastive loss between global image and text
embeddings to learn representations that simultaneously encode global and local
information. We thoroughly evaluate our proposed method and show improved
performance over competing approaches both on image-level tasks relying on
coarse-grained information, e.g. classification, as well as region-level tasks
relying on fine-grained information, e.g. retrieval, object detection, and
segmentation. Moreover, SPARC improves model faithfulness and captioning in
foundational vision-language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bica_I/0/1/0/all/0/1&quot;&gt;Ioana Bica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilic_A/0/1/0/all/0/1&quot;&gt;Anastasija Ili&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauer_M/0/1/0/all/0/1&quot;&gt;Matthias Bauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erdogan_G/0/1/0/all/0/1&quot;&gt;Goker Erdogan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosnjak_M/0/1/0/all/0/1&quot;&gt;Matko Bo&amp;#x161;njak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaplanis_C/0/1/0/all/0/1&quot;&gt;Christos Kaplanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1&quot;&gt;Alexey A. Gritsenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1&quot;&gt;Matthias Minderer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1&quot;&gt;Charles Blundell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitrovic_J/0/1/0/all/0/1&quot;&gt;Jovana Mitrovi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09866">
<title>Boosting Few-Shot Segmentation via Instance-Aware Data Augmentation and Local Consensus Guided Cross Attention. (arXiv:2401.09866v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09866</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot segmentation aims to train a segmentation model that can fast adapt
to a novel task for which only a few annotated images are provided. Most recent
models have adopted a prototype-based paradigm for few-shot inference. These
approaches may have limited generalization capacity beyond the standard 1- or
5-shot settings. In this paper, we closely examine and reevaluate the
fine-tuning based learning scheme that fine-tunes the classification layer of a
deep segmentation network pre-trained on diverse base classes. To improve the
generalizability of the classification layer optimized with sparsely annotated
samples, we introduce an instance-aware data augmentation (IDA) strategy that
augments the support images based on the relative sizes of the target objects.
The proposed IDA effectively increases the support set&apos;s diversity and promotes
the distribution consistency between support and query images. On the other
hand, the large visual difference between query and support images may hinder
knowledge transfer and cripple the segmentation performance. To cope with this
challenge, we introduce the local consensus guided cross attention (LCCA) to
align the query feature with support features based on their dense correlation,
further improving the model&apos;s generalizability to the query image. The
significant performance improvements on the standard few-shot segmentation
benchmarks PASCAL-$5^i$ and COCO-$20^i$ verify the efficacy of our proposed
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1&quot;&gt;Li Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haoming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chengyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiaochen Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09883">
<title>Question-Answer Cross Language Image Matching for Weakly Supervised Semantic Segmentation. (arXiv:2401.09883v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09883</link>
<description rdf:parseType="Literal">&lt;p&gt;Class Activation Map (CAM) has emerged as a popular tool for weakly
supervised semantic segmentation (WSSS), allowing the localization of object
regions in an image using only image-level labels. However, existing CAM
methods suffer from under-activation of target object regions and
false-activation of background regions due to the fact that a lack of detailed
supervision can hinder the model&apos;s ability to understand the image as a whole.
In this paper, we propose a novel Question-Answer Cross-Language-Image Matching
framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model
to maximize the text-based understanding of images and guide the generation of
activation maps. First, a series of carefully designed questions are posed to
the VQA (Visual Question Answering) model with Question-Answer Prompt
Engineering (QAPE) to generate a corpus of both foreground target objects and
backgrounds that are adaptive to query images. We then employ contrastive
learning in a Region Image Text Contrastive (RITC) network to compare the
obtained foreground and background regions with the generated corpus. Our
approach exploits the rich textual information from the open vocabulary as
additional supervision, enabling the model to generate high-quality CAMs with a
more complete object region and reduce false-activation of background regions.
We conduct extensive analysis to validate the proposed method and show that our
approach performs state-of-the-art on both PASCAL VOC 2012 and MS COCO
datasets. Code is available at: https://github.com/CVI-SZU/QA-CLIMS
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Songhe Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1&quot;&gt;Wei Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jinheng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Linlin Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09895">
<title>Skeleton-Guided Instance Separation for Fine-Grained Segmentation in Microscopy. (arXiv:2401.09895v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09895</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the fundamental challenges in microscopy (MS) image analysis is
instance segmentation (IS), particularly when segmenting cluster regions where
multiple objects of varying sizes and shapes may be connected or even
overlapped in arbitrary orientations. Existing IS methods usually fail in
handling such scenarios, as they rely on coarse instance representations such
as keypoints and horizontal bounding boxes (h-bboxes). In this paper, we
propose a novel one-stage framework named A2B-IS to address this challenge and
enhance the accuracy of IS in MS images. Our approach represents each instance
with a pixel-level mask map and a rotated bounding box (r-bbox). Unlike
two-stage methods that use box proposals for segmentations, our method
decouples mask and box predictions, enabling simultaneous processing to
streamline the model pipeline. Additionally, we introduce a Gaussian skeleton
map to aid the IS task in two key ways: (1) It guides anchor placement,
reducing computational costs while improving the model&apos;s capacity to learn
RoI-aware features by filtering out noise from background regions. (2) It
ensures accurate isolation of densely packed instances by rectifying erroneous
box predictions near instance boundaries. To further enhance the performance,
we integrate two modules into the framework: (1) An Atrous Attention Block
(A2B) designed to extract high-resolution feature maps with fine-grained
multiscale information, and (2) A Semi-Supervised Learning (SSL) strategy that
leverages both labeled and unlabeled images for model training. Our method has
been thoroughly validated on two large-scale MS datasets, demonstrating its
superiority over most state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chengfeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1&quot;&gt;Zhaoyan Ming&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1&quot;&gt;Lina Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xudong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_D/0/1/0/all/0/1&quot;&gt;Dahong Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09900">
<title>XAI-Enhanced Semantic Segmentation Models for Visual Quality Inspection. (arXiv:2401.09900v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09900</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual quality inspection systems, crucial in sectors like manufacturing and
logistics, employ computer vision and machine learning for precise, rapid
defect detection. However, their unexplained nature can hinder trust, error
identification, and system improvement. This paper presents a framework to
bolster visual quality inspection by using CAM-based explanations to refine
semantic segmentation models. Our approach consists of 1) Model Training, 2)
XAI-based Model Explanation, 3) XAI Evaluation, and 4) Annotation Augmentation
for Model Enhancement, informed by explanations and expert insights.
Evaluations show XAI-enhanced models surpass original DeepLabv3-ResNet101
models, especially in intricate object segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clement_T/0/1/0/all/0/1&quot;&gt;Tobias Clement&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Truong Thanh Hung Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelaal_M/0/1/0/all/0/1&quot;&gt;Mohamed Abdelaal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1&quot;&gt;Hung Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09921">
<title>BlenDA: Domain Adaptive Object Detection through diffusion-based blending. (arXiv:2401.09921v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09921</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain adaptation (UDA) aims to transfer a model learned using
labeled data from the source domain to unlabeled data in the target domain. To
address the large domain gap issue between the source and target domains, we
propose a novel regularization method for domain adaptive object detection,
BlenDA, by generating the pseudo samples of the intermediate domains and their
corresponding soft domain labels for adaptation training. The intermediate
samples are generated by dynamically blending the source images with their
corresponding translated images using an off-the-shelf pre-trained
text-to-image diffusion model which takes the text label of the target domain
as input and has demonstrated superior image-to-image translation quality.
Based on experimental results from two adaptation benchmarks, our proposed
approach can significantly enhance the performance of the state-of-the-art
domain adaptive object detector, Adversarial Query Transformer (AQT).
Particularly, in the Cityscapes to Foggy Cityscapes adaptation, we achieve an
impressive 53.4% mAP on the Foggy Cityscapes dataset, surpassing the previous
state-of-the-art by 1.5%. It is worth noting that our proposed method is also
applicable to various paradigms of domain adaptive object detection. The code
is available at:https://github.com/aiiu-lab/BlenDA
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tzuhsuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chen-Che Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ku_C/0/1/0/all/0/1&quot;&gt;Chung-Hao Ku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jun-Cheng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09923">
<title>MAMBA: Multi-level Aggregation via Memory Bank for Video Object Detection. (arXiv:2401.09923v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09923</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art video object detection methods maintain a memory structure,
either a sliding window or a memory queue, to enhance the current frame using
attention mechanisms. However, we argue that these memory structures are not
efficient or sufficient because of two implied operations: (1) concatenating
all features in memory for enhancement, leading to a heavy computational cost;
(2) frame-wise memory updating, preventing the memory from capturing more
temporal information. In this paper, we propose a multi-level aggregation
architecture via memory bank called MAMBA. Specifically, our memory bank
employs two novel operations to eliminate the disadvantages of existing
methods: (1) light-weight key-set construction which can significantly reduce
the computational cost; (2) fine-grained feature-wise updating strategy which
enables our method to utilize knowledge from the whole video. To better enhance
features from complementary levels, i.e., feature maps and proposals, we
further propose a generalized enhancement operation (GEO) to aggregate
multi-level features in a unified manner. We conduct extensive evaluations on
the challenging ImageNetVID dataset. Compared with existing state-of-the-art
methods, our method achieves superior performance in terms of both speed and
accuracy. More remarkably, MAMBA achieves mAP of 83.7/84.6% at 12.6/9.1 FPS
with ResNet-101. Code is available at
https://github.com/guanxiongsun/video_feature_enhancement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1&quot;&gt;Guanxiong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1&quot;&gt;Yang Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1&quot;&gt;Guosheng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robertson_N/0/1/0/all/0/1&quot;&gt;Neil Robertson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09939">
<title>ICGNet: A Unified Approach for Instance-Centric Grasping. (arXiv:2401.09939v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.09939</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate grasping is the key to several robotic tasks including assembly and
household robotics. Executing a successful grasp in a cluttered environment
requires multiple levels of scene understanding: First, the robot needs to
analyze the geometric properties of individual objects to find feasible grasps.
These grasps need to be compliant with the local object geometry. Second, for
each proposed grasp, the robot needs to reason about the interactions with
other objects in the scene. Finally, the robot must compute a collision-free
grasp trajectory while taking into account the geometry of the target object.
Most grasp detection algorithms directly predict grasp poses in a monolithic
fashion, which does not capture the composability of the environment. In this
paper, we introduce an end-to-end architecture for object-centric grasping. The
method uses pointcloud data from a single arbitrary viewing direction as an
input and generates an instance-centric representation for each partially
observed object in the scene. This representation is further used for object
reconstruction and grasp detection in cluttered table-top scenes. We show the
effectiveness of the proposed method by extensively evaluating it against
state-of-the-art methods on synthetic datasets, indicating superior performance
for grasping and reconstruction. Additionally, we demonstrate real-world
applicability by decluttering scenes with varying numbers of objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zurbrugg_R/0/1/0/all/0/1&quot;&gt;Ren&amp;#xe9; Zurbr&amp;#xfc;gg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yifan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engelmann_F/0/1/0/all/0/1&quot;&gt;Francis Engelmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Suryansh Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1&quot;&gt;Marco Hutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patil_V/0/1/0/all/0/1&quot;&gt;Vaishakh Patil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fisher Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09942">
<title>Multi-task Learning for Joint Re-identification, Team Affiliation, and Role Classification for Sports Visual Tracking. (arXiv:2401.09942v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09942</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective tracking and re-identification of players is essential for
analyzing soccer videos. But, it is a challenging task due to the non-linear
motion of players, the similarity in appearance of players from the same team,
and frequent occlusions. Therefore, the ability to extract meaningful
embeddings to represent players is crucial in developing an effective tracking
and re-identification system. In this paper, a multi-purpose part-based person
representation method, called PRTreID, is proposed that performs three tasks of
role classification, team affiliation, and re-identification, simultaneously.
In contrast to available literature, a single network is trained with
multi-task supervision to solve all three tasks, jointly. The proposed joint
method is computationally efficient due to the shared backbone. Also, the
multi-task learning leads to richer and more discriminative representations, as
demonstrated by both quantitative and qualitative results. To demonstrate the
effectiveness of PRTreID, it is integrated with a state-of-the-art tracking
method, using a part-based post-processing module to handle long-term tracking.
The proposed tracking method outperforms all existing tracking methods on the
challenging SoccerNet tracking dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansourian_A/0/1/0/all/0/1&quot;&gt;Amir M. Mansourian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Somers_V/0/1/0/all/0/1&quot;&gt;Vladimir Somers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vleeschouwer_C/0/1/0/all/0/1&quot;&gt;Christophe De Vleeschouwer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1&quot;&gt;Shohreh Kasaei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09962">
<title>CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects. (arXiv:2401.09962v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09962</link>
<description rdf:parseType="Literal">&lt;p&gt;Customized text-to-video generation aims to generate high-quality videos
guided by text prompts and subject references. Current approaches designed for
single subjects suffer from tackling multiple subjects, which is a more
challenging and practical scenario. In this work, we aim to promote
multi-subject guided text-to-video customization. We propose CustomVideo, a
novel framework that can generate identity-preserving videos with the guidance
of multiple subjects. To be specific, firstly, we encourage the co-occurrence
of multiple subjects via composing them in a single image. Further, upon a
basic text-to-video diffusion model, we design a simple yet effective attention
control strategy to disentangle different subjects in the latent space of
diffusion model. Moreover, to help the model focus on the specific object area,
we segment the object from given reference images and provide a corresponding
object mask for attention learning. Also, we collect a multi-subject
text-to-video generation dataset as a comprehensive benchmark, with 69
individual subjects and 57 meaningful pairs. Extensive qualitative,
quantitative, and user study results demonstrate the superiority of our method,
compared with the previous state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Aoxue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lingting Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1&quot;&gt;Qi Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09980">
<title>Ventricular Segmentation: A Brief Comparison of U-Net Derivatives. (arXiv:2401.09980v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.09980</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical imaging refers to the technologies and methods utilized to view the
human body and its inside, in order to diagnose, monitor, or even treat medical
disorders. This paper aims to explore the application of deep learning
techniques in the semantic segmentation of Cardiac short-axis MRI (Magnetic
Resonance Imaging) images, aiming to enhance the diagnosis, monitoring, and
treatment of medical disorders related to the heart. The focus centers on
implementing various architectures that are derivatives of U-Net, to
effectively isolate specific parts of the heart for comprehensive anatomical
and functional analysis. Through a combination of images, graphs, and
quantitative metrics, the efficacy of the models and their predictions are
showcased. Additionally, this paper addresses encountered challenges and
outline strategies for future improvements. This abstract provides a concise
overview of the efforts in utilizing deep learning for cardiac image
segmentation, emphasizing both the accomplishments and areas for further
refinement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saichandran_K/0/1/0/all/0/1&quot;&gt;Ketan Suhaas Saichandran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09985">
<title>WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens. (arXiv:2401.09985v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09985</link>
<description rdf:parseType="Literal">&lt;p&gt;World models play a crucial role in understanding and predicting the dynamics
of the world, which is essential for video generation. However, existing world
models are confined to specific scenarios such as gaming or driving, limiting
their ability to capture the complexity of general world dynamic environments.
Therefore, we introduce WorldDreamer, a pioneering world model to foster a
comprehensive comprehension of general world physics and motions, which
significantly enhances the capabilities of video generation. Drawing
inspiration from the success of large language models, WorldDreamer frames
world modeling as an unsupervised visual sequence modeling challenge. This is
achieved by mapping visual inputs to discrete tokens and predicting the masked
ones. During this process, we incorporate multi-modal prompts to facilitate
interaction within the world model. Our experiments show that WorldDreamer
excels in generating videos across different scenarios, including natural
scenes and driving environments. WorldDreamer showcases versatility in
executing tasks such as text-to-video conversion, image-tovideo synthesis, and
video editing. These results underscore WorldDreamer&apos;s effectiveness in
capturing dynamic elements within diverse general world environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Guan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Boyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinze Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09988">
<title>Developing an AI-based Integrated System for Bee Health Evaluation. (arXiv:2401.09988v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09988</link>
<description rdf:parseType="Literal">&lt;p&gt;Honey bees pollinate about one-third of the world&apos;s food supply, but bee
colonies have alarmingly declined by nearly 40% over the past decade due to
several factors, including pesticides and pests. Traditional methods for
monitoring beehives, such as human inspection, are subjective, disruptive, and
time-consuming. To overcome these limitations, artificial intelligence has been
used to assess beehive health. However, previous studies have lacked an
end-to-end solution and primarily relied on data from a single source, either
bee images or sounds. This study introduces a comprehensive system consisting
of bee object detection and health evaluation. Additionally, it utilized a
combination of visual and audio signals to analyze bee behaviors. An
Attention-based Multimodal Neural Network (AMNN) was developed to adaptively
focus on key features from each type of signal for accurate bee health
assessment. The AMNN achieved an overall accuracy of 92.61%, surpassing eight
existing single-signal Convolutional Neural Networks and Recurrent Neural
Networks. It outperformed the best image-based model by 32.51% and the top
sound-based model by 13.98% while maintaining efficient processing times.
Furthermore, it improved prediction robustness, attaining an F1-score higher
than 90% across all four evaluated health conditions. The study also shows that
audio signals are more reliable than images for assessing bee health. By
seamlessly integrating AMNN with image and sound data in a comprehensive bee
health monitoring system, this approach provides a more efficient and
non-invasive solution for the early detection of bee diseases and the
preservation of bee colonies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_A/0/1/0/all/0/1&quot;&gt;Andrew Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09997">
<title>BPDO:Boundary Points Dynamic Optimization for Arbitrary Shape Scene Text Detection. (arXiv:2401.09997v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09997</link>
<description rdf:parseType="Literal">&lt;p&gt;Arbitrary shape scene text detection is of great importance in scene
understanding tasks. Due to the complexity and diversity of text in natural
scenes, existing scene text algorithms have limited accuracy for detecting
arbitrary shape text. In this paper, we propose a novel arbitrary shape scene
text detector through boundary points dynamic optimization(BPDO). The proposed
model is designed with a text aware module (TAM) and a boundary point dynamic
optimization module (DOM). Specifically, the model designs a text aware module
based on segmentation to obtain boundary points describing the central region
of the text by extracting a priori information about the text region. Then,
based on the idea of deformable attention, it proposes a dynamic optimization
model for boundary points, which gradually optimizes the exact position of the
boundary points based on the information of the adjacent region of each
boundary point. Experiments on CTW-1500, Total-Text, and MSRA-TD500 datasets
show that the model proposed in this paper achieves a performance that is
better than or comparable to the state-of-the-art algorithm, proving the
effectiveness of the model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jinzhi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Libo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yanjun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chen Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10005">
<title>Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation. (arXiv:2401.10005v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10005</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing demand for intelligent systems capable of interpreting and
reasoning about visual content requires the development of Large Multi-Modal
Models (LMMs) that are not only accurate but also have explicit reasoning
capabilities. This paper presents a novel approach to imbue an LMM with the
ability to conduct explicit reasoning based on visual content and textual
instructions. We introduce a system that can ask a question to acquire
necessary knowledge, thereby enhancing the robustness and explicability of the
reasoning process. Our method comprises the development of a novel dataset
generated by a Large Language Model (LLM), designed to promote chain-of-thought
reasoning combined with a question-asking mechanism. We designed an LMM, which
has high capabilities on region awareness to address the intricate requirements
of image-text alignment. The model undergoes a three-stage training phase,
starting with large-scale image-text alignment using a large-scale datasets,
followed by instruction tuning, and fine-tuning with a focus on
chain-of-thought reasoning. The results demonstrate a stride toward a more
robust, accurate, and interpretable LMM, capable of reasoning explicitly and
seeking information proactively when confronted with ambiguous visual input.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uehara_K/0/1/0/all/0/1&quot;&gt;Kohei Uehara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goswami_N/0/1/0/all/0/1&quot;&gt;Nabarun Goswami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanqin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baba_T/0/1/0/all/0/1&quot;&gt;Toshiaki Baba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanaka_K/0/1/0/all/0/1&quot;&gt;Kohtaro Tanaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1&quot;&gt;Tomohiro Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ito_R/0/1/0/all/0/1&quot;&gt;Rei Ito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naoya_T/0/1/0/all/0/1&quot;&gt;Takagi Naoya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Umagami_R/0/1/0/all/0/1&quot;&gt;Ryo Umagami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yingyi Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anakewat_T/0/1/0/all/0/1&quot;&gt;Tanachai Anakewat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1&quot;&gt;Tatsuya Harada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10011">
<title>CPCL: Cross-Modal Prototypical Contrastive Learning for Weakly Supervised Text-based Person Re-Identification. (arXiv:2401.10011v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10011</link>
<description rdf:parseType="Literal">&lt;p&gt;Weakly supervised text-based person re-identification (TPRe-ID) seeks to
retrieve images of a target person using textual descriptions, without relying
on identity annotations and is more challenging and practical. The primary
challenge is the intra-class differences, encompassing intra-modal feature
variations and cross-modal semantic gaps. Prior works have focused on
instance-level samples and ignored prototypical features of each person which
are intrinsic and invariant. Toward this, we propose a Cross-Modal Prototypical
Contrastive Learning (CPCL) method. In practice, the CPCL introduces the CLIP
model to weakly supervised TPRe-ID for the first time, mapping visual and
textual instances into a shared latent space. Subsequently, the proposed
Prototypical Multi-modal Memory (PMM) module captures associations between
heterogeneous modalities of image-text pairs belonging to the same person
through the Hybrid Cross-modal Matching (HCM) module in a many-to-many mapping
fashion. Moreover, the Outlier Pseudo Label Mining (OPLM) module further
distinguishes valuable outlier samples from each modality, enhancing the
creation of more reliable clusters by mining implicit relationships between
image-text pairs. Experimental results demonstrate that our proposed CPCL
attains state-of-the-art performance on all three public datasets, with a
significant improvement of 11.58%, 8.77% and 5.25% in Rank@1 accuracy on
CUHK-PEDES, ICFG-PEDES and RSTPReid datasets, respectively. The code is
available at https://github.com/codeGallery24/CPCL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yanwei Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xinpeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1&quot;&gt;Chuanlin Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Bowen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jibin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dongxiao Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10017">
<title>Text Region Multiple Information Perception Network for Scene Text Detection. (arXiv:2401.10017v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10017</link>
<description rdf:parseType="Literal">&lt;p&gt;Segmentation-based scene text detection algorithms can handle arbitrary shape
scene texts and have strong robustness and adaptability, so it has attracted
wide attention. Existing segmentation-based scene text detection algorithms
usually only segment the pixels in the center region of the text, while
ignoring other information of the text region, such as edge information,
distance information, etc., thus limiting the detection accuracy of the
algorithm for scene text. This paper proposes a plug-and-play module called the
Region Multiple Information Perception Module (RMIPM) to enhance the detection
performance of segmentation-based algorithms. Specifically, we design an
improved module that can perceive various types of information about scene text
regions, such as text foreground classification maps, distance maps, direction
maps, etc. Experiments on MSRA-TD500 and TotalText datasets show that our
method achieves comparable performance with current state-of-the-art
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jinzhi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Libo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yanjun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chen Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10037">
<title>Depth Over RGB: Automatic Evaluation of Open Surgery Skills Using Depth Camera. (arXiv:2401.10037v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10037</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: In this paper, we present a novel approach to the automatic
evaluation of open surgery skills using depth cameras. This work is intended to
show that depth cameras achieve similar results to RGB cameras, which is the
common method in the automatic evaluation of open surgery skills. Moreover,
depth cameras offer advantages such as robustness to lighting variations,
camera positioning, simplified data compression, and enhanced privacy, making
them a promising alternative to RGB cameras.
&lt;/p&gt;
&lt;p&gt;Methods: Experts and novice surgeons completed two simulators of open
suturing. We focused on hand and tool detection, and action segmentation in
suturing procedures. YOLOv8 was used for tool detection in RGB and depth
videos. Furthermore, UVAST and MSTCN++ were used for action segmentation. Our
study includes the collection and annotation of a dataset recorded with Azure
Kinect.
&lt;/p&gt;
&lt;p&gt;Results: We demonstrated that using depth cameras in object detection and
action segmentation achieves comparable results to RGB cameras. Furthermore, we
analyzed 3D hand path length, revealing significant differences between experts
and novice surgeons, emphasizing the potential of depth cameras in capturing
surgical skills. We also investigated the influence of camera angles on
measurement accuracy, highlighting the advantages of 3D cameras in providing a
more accurate representation of hand movements.
&lt;/p&gt;
&lt;p&gt;Conclusion: Our research contributes to advancing the field of surgical skill
assessment by leveraging depth cameras for more reliable and privacy
evaluations. The findings suggest that depth cameras can be valuable in
assessing surgical skills and provide a foundation for future research in this
area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuckerman_I/0/1/0/all/0/1&quot;&gt;Ido Zuckerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werner_N/0/1/0/all/0/1&quot;&gt;Nicole Werner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kouchly_J/0/1/0/all/0/1&quot;&gt;Jonathan Kouchly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huston_E/0/1/0/all/0/1&quot;&gt;Emma Huston&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DiMarco_S/0/1/0/all/0/1&quot;&gt;Shannon DiMarco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DiMusto_P/0/1/0/all/0/1&quot;&gt;Paul DiMusto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laufer_S/0/1/0/all/0/1&quot;&gt;Shlomi Laufer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10039">
<title>GPT4Ego: Unleashing the Potential of Pre-trained Models for Zero-Shot Egocentric Action Recognition. (arXiv:2401.10039v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10039</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-Language Models (VLMs), pre-trained on large-scale datasets, have
shown impressive performance in various visual recognition tasks. This
advancement paves the way for notable performance in Zero-Shot Egocentric
Action Recognition (ZS-EAR). Typically, VLMs handle ZS-EAR as a global
video-text matching task, which often leads to suboptimal alignment of vision
and linguistic knowledge. We propose a refined approach for ZS-EAR using VLMs,
emphasizing fine-grained concept-description alignment that capitalizes on the
rich semantic and contextual details in egocentric videos. In this paper, we
introduce GPT4Ego, a straightforward yet remarkably potent VLM framework for
ZS-EAR, designed to enhance the fine-grained alignment of concept and
description between vision and language. Extensive experiments demonstrate
GPT4Ego significantly outperforms existing VLMs on three large-scale egocentric
video benchmarks, i.e., EPIC-KITCHENS-100 (33.2%, +9.4%), EGTEA (39.6%, +5.5%),
and CharadesEgo (31.5%, +2.6%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1&quot;&gt;Guangzhao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1&quot;&gt;Xiangbo Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenhao Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10041">
<title>CMFN: Cross-Modal Fusion Network for Irregular Scene Text Recognition. (arXiv:2401.10041v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10041</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene text recognition, as a cross-modal task involving vision and text, is
an important research topic in computer vision. Most existing methods use
language models to extract semantic information for optimizing visual
recognition. However, the guidance of visual cues is ignored in the process of
semantic mining, which limits the performance of the algorithm in recognizing
irregular scene text. To tackle this issue, we propose a novel cross-modal
fusion network (CMFN) for irregular scene text recognition, which incorporates
visual cues into the semantic mining process. Specifically, CMFN consists of a
position self-enhanced encoder, a visual recognition branch and an iterative
semantic recognition branch. The position self-enhanced encoder provides
character sequence position encoding for both the visual recognition branch and
the iterative semantic recognition branch. The visual recognition branch
carries out visual recognition based on the visual features extracted by CNN
and the position encoding information provided by the position self-enhanced
encoder. The iterative semantic recognition branch, which consists of a
language recognition module and a cross-modal fusion gate, simulates the way
that human recognizes scene text and integrates cross-modal visual cues for
text recognition. The experiments demonstrate that the proposed CMFN algorithm
achieves comparable performance to state-of-the-art algorithms, indicating its
effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jinzhi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Ruyi Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Libo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yanjun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chen Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10044">
<title>Deep spatial context: when attention-based models meet spatial regression. (arXiv:2401.10044v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10044</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose &apos;Deep spatial context&apos; (DSCon) method, which serves for
investigation of the attention-based vision models using the concept of spatial
context. It was inspired by histopathologists, however, the method can be
applied to various domains. The DSCon allows for a quantitative measure of the
spatial context&apos;s role using three Spatial Context Measures: $SCM_{features}$,
$SCM_{targets}$, $SCM_{residuals}$ to distinguish whether the spatial context
is observable within the features of neighboring regions, their target values
(attention scores) or residuals, respectively. It is achieved by integrating
spatial regression into the pipeline. The DSCon helps to verify research
questions. The experiments reveal that spatial relationships are much bigger in
the case of the classification of tumor lesions than normal tissues. Moreover,
it turns out that the larger the size of the neighborhood taken into account
within spatial regression, the less valuable contextual information is.
Furthermore, it is observed that the spatial context measure is the largest
when considered within the feature space as opposed to the targets and
residuals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomaszewska_P/0/1/0/all/0/1&quot;&gt;Paulina Tomaszewska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sienkiewicz_E/0/1/0/all/0/1&quot;&gt;El&amp;#x17c;bieta Sienkiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_M/0/1/0/all/0/1&quot;&gt;Mai P. Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biecek_P/0/1/0/all/0/1&quot;&gt;Przemys&amp;#x142;aw Biecek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10050">
<title>ContextMix: A context-aware data augmentation method for industrial visual inspection systems. (arXiv:2401.10050v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10050</link>
<description rdf:parseType="Literal">&lt;p&gt;While deep neural networks have achieved remarkable performance, data
augmentation has emerged as a crucial strategy to mitigate overfitting and
enhance network performance. These techniques hold particular significance in
industrial manufacturing contexts. Recently, image mixing-based methods have
been introduced, exhibiting improved performance on public benchmark datasets.
However, their application to industrial tasks remains challenging. The
manufacturing environment generates massive amounts of unlabeled data on a
daily basis, with only a few instances of abnormal data occurrences. This leads
to severe data imbalance. Thus, creating well-balanced datasets is not
straightforward due to the high costs associated with labeling. Nonetheless,
this is a crucial step for enhancing productivity. For this reason, we
introduce ContextMix, a method tailored for industrial applications and
benchmark datasets. ContextMix generates novel data by resizing entire images
and integrating them into other images within the batch. This approach enables
our method to learn discriminative features based on varying sizes from resized
images and train informative secondary features for object recognition using
occluded images. With the minimal additional computation cost of image
resizing, ContextMix enhances performance compared to existing augmentation
techniques. We evaluate its effectiveness across classification, detection, and
segmentation tasks using various network architectures on public benchmark
datasets. Our proposed method demonstrates improved results across a range of
robustness tasks. Its efficacy in real industrial environments is particularly
noteworthy, as demonstrated using the passive component dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyungmin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Donghun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_P/0/1/0/all/0/1&quot;&gt;Pyunghwan Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suh_S/0/1/0/all/0/1&quot;&gt;Sungho Suh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1&quot;&gt;Hansang Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junmo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10061">
<title>DiffusionGPT: LLM-Driven Text-to-Image Generation System. (arXiv:2401.10061v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10061</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have opened up new avenues for the field of image
generation, resulting in the proliferation of high-quality models shared on
open-source platforms. However, a major challenge persists in current
text-to-image systems are often unable to handle diverse inputs, or are limited
to single model results. Current unified attempts often fall into two
orthogonal aspects: i) parse Diverse Prompts in input stage; ii) activate
expert model to output. To combine the best of both worlds, we propose
DiffusionGPT, which leverages Large Language Models (LLM) to offer a unified
generation system capable of seamlessly accommodating various types of prompts
and integrating domain-expert models. DiffusionGPT constructs domain-specific
Trees for various generative models based on prior knowledge. When provided
with an input, the LLM parses the prompt and employs the Trees-of-Thought to
guide the selection of an appropriate model, thereby relaxing input constraints
and ensuring exceptional performance across diverse domains. Moreover, we
introduce Advantage Databases, where the Tree-of-Thought is enriched with human
feedback, aligning the model selection process with human preferences. Through
extensive experiments and comparisons, we demonstrate the effectiveness of
DiffusionGPT, showcasing its potential for pushing the boundaries of image
synthesis in diverse domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1&quot;&gt;Jie Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weifeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yuxi Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huixia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hefeng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xuefeng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1&quot;&gt;Shilei Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10083">
<title>A locally statistical active contour model for SAR image segmentation can be solved by denoising algorithms. (arXiv:2401.10083v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10083</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel locally statistical variational active
contour model based on I-divergence-TV denoising model, which hybrides geodesic
active contour (GAC) model with active contours without edges (ACWE) model, and
can be used to segment images corrupted by multiplicative gamma noise. By
adding a diffusion term into the level set evolution (LSE) equation of the
proposed model, we construct a reaction-diffusion (RD) equation, which can
gradually regularize the level set function (LSF) to be piecewise constant in
each segment domain and gain the stable solution. We further transform the
proposed model into classic ROF model by adding a proximity term. Inspired by a
fast denoising algorithm proposed by Jia-Zhao recently, we propose two fast
fixed point algorithms to solve SAR image segmentation question. Experimental
results for real SAR images show that the proposed image segmentation model can
efficiently stop the contours at weak or blurred edges, and can automatically
detect the exterior and interior boundaries of images with multiplicative gamma
noise. The proposed FPRD1/FPRD2 models are about 1/2 (or less than) of the time
required for the SBRD model based on the Split Bregman technique.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guangming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Quanying Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jing Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10090">
<title>Cross-Modality Perturbation Synergy Attack for Person Re-identification. (arXiv:2401.10090v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10090</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, there has been significant research focusing on addressing
security concerns in single-modal person re-identification (ReID) systems that
are based on RGB images. However, the safety of cross-modality scenarios, which
are more commonly encountered in practical applications involving images
captured by infrared cameras, has not received adequate attention. The main
challenge in cross-modality ReID lies in effectively dealing with visual
differences between different modalities. For instance, infrared images are
typically grayscale, unlike visible images that contain color information.
Existing attack methods have primarily focused on the characteristics of the
visible image modality, overlooking the features of other modalities and the
variations in data distribution among different modalities. This oversight can
potentially undermine the effectiveness of these methods in image retrieval
across diverse modalities. This study represents the first exploration into the
security of cross-modality ReID models and proposes a universal perturbation
attack specifically designed for cross-modality ReID. This attack optimizes
perturbations by leveraging gradients from diverse modality data, thereby
disrupting the discriminator and reinforcing the differences between
modalities. We conducted experiments on two widely used cross-modality
datasets, namely RegDB and SYSU, which not only demonstrated the effectiveness
of our method but also provided insights for future enhancements in the
robustness of cross-modality ReID systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yunpeng Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+others/0/1/0/all/0/1&quot;&gt;others&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10110">
<title>VIPTR: A Vision Permutable Extractor for Fast and Efficient Scene Text Recognition. (arXiv:2401.10110v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10110</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene Text Recognition (STR) is a challenging task that involves recognizing
text within images of natural scenes. Although current state-of-the-art models
for STR exhibit high performance, they typically suffer from low inference
efficiency due to their reliance on hybrid architectures comprised of visual
encoders and sequence decoders. In this work, we propose the VIsion Permutable
extractor for fast and efficient scene Text Recognition (VIPTR), which achieves
an impressive balance between high performance and rapid inference speeds in
the domain of STR. Specifically, VIPTR leverages a visual-semantic extractor
with a pyramid structure, characterized by multiple self-attention layers,
while eschewing the traditional sequence decoder. This design choice results in
a lightweight and efficient model capable of handling inputs of varying sizes.
Extensive experimental results on various standard datasets for both Chinese
and English scene text recognition validate the superiority of VIPTR. Notably,
the VIPTR-T (Tiny) variant delivers highly competitive accuracy on par with
other lightweight models and achieves SOTA inference speeds. Meanwhile, the
VIPTR-L (Large) variant attains greater recognition accuracy, while maintaining
a low parameter count and favorable inference speed. Our proposed method
provides a compelling solution for the STR challenge, which blends high
accuracy with efficiency and greatly benefits real-world applications requiring
fast and reliable text recognition. The code is publicly available at
https://github.com/cxfyxl/VIPTR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xianfu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Weixiao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tongliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhoujun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10113">
<title>Exposing Lip-syncing Deepfakes from Mouth Inconsistencies. (arXiv:2401.10113v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10113</link>
<description rdf:parseType="Literal">&lt;p&gt;A lip-syncing deepfake is a digitally manipulated video in which a person&apos;s
lip movements are created convincingly using AI models to match altered or
entirely new audio. Lip-syncing deepfakes are a dangerous type of deepfakes as
the artifacts are limited to the lip region and more difficult to discern. In
this paper, we describe a novel approach, LIP-syncing detection based on mouth
INConsistency (LIPINC), for lip-syncing deepfake detection by identifying
temporal inconsistencies in the mouth region. These inconsistencies are seen in
the adjacent frames and throughout the video. Our model can successfully
capture these irregularities and outperforms the state-of-the-art methods on
several benchmark deepfake datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Datta_S/0/1/0/all/0/1&quot;&gt;Soumyya Kanti Datta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1&quot;&gt;Shan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Siwei Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10128">
<title>Sub2Full: split spectrum to boost OCT despeckling without clean data. (arXiv:2401.10128v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.10128</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical coherence tomography (OCT) suffers from speckle noise, causing the
deterioration of image quality, especially in high-resolution modalities like
visible light OCT (vis-OCT). The potential of conventional supervised deep
learning denoising methods is limited by the difficulty of obtaining clean
data. Here, we proposed an innovative self-supervised strategy called Sub2Full
(S2F) for OCT despeckling without clean data. This approach works by acquiring
two repeated B-scans, splitting the spectrum of the first repeat as a
low-resolution input, and utilizing the full spectrum of the second repeat as
the high-resolution target. The proposed method was validated on vis-OCT
retinal images visualizing sublaminar structures in outer retina and
demonstrated superior performance over conventional Noise2Noise and Noise2Void
schemes. The code is available at
https://github.com/PittOCT/Sub2Full-OCT-Denoising.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lingyun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sahel_J/0/1/0/all/0/1&quot;&gt;Jose A Sahel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pi_S/0/1/0/all/0/1&quot;&gt;Shaohua Pi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10129">
<title>Few-shot learning for COVID-19 Chest X-Ray Classification with Imbalanced Data: An Inter vs. Intra Domain Study. (arXiv:2401.10129v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.10129</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image datasets are essential for training models used in
computer-aided diagnosis, treatment planning, and medical research. However,
some challenges are associated with these datasets, including variability in
data distribution, data scarcity, and transfer learning issues when using
models pre-trained from generic images. This work studies the effect of these
challenges at the intra- and inter-domain level in few-shot learning scenarios
with severe data imbalance. For this, we propose a methodology based on Siamese
neural networks in which a series of techniques are integrated to mitigate the
effects of data scarcity and distribution imbalance. Specifically, different
initialization and data augmentation methods are analyzed, and four adaptations
to Siamese networks of solutions to deal with imbalanced data are introduced,
including data balancing and weighted loss, both separately and combined, and
with a different balance of pairing ratios. Moreover, we also assess the
inference process considering four classifiers, namely Histogram, $k$NN, SVM,
and Random Forest. Evaluation is performed on three chest X-ray datasets with
annotated cases of both positive and negative COVID-19 diagnoses. The accuracy
of each technique proposed for the Siamese architecture is analyzed separately
and their results are compared to those obtained using equivalent methods on a
state-of-the-art CNN. We conclude that the introduced techniques offer
promising improvements over the baseline in almost all cases, and that the
selection of the technique may vary depending on the amount of data available
and the level of imbalance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Galan_Cuenca_A/0/1/0/all/0/1&quot;&gt;Alejandro Gal&amp;#xe1;n-Cuenca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gallego_A/0/1/0/all/0/1&quot;&gt;Antonio Javier Gallego&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saval_Calvo_M/0/1/0/all/0/1&quot;&gt;Marcelo Saval-Calvo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pertusa_A/0/1/0/all/0/1&quot;&gt;Antonio Pertusa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10139">
<title>Model Compression Techniques in Biometrics Applications: A Survey. (arXiv:2401.10139v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10139</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of deep learning algorithms has extensively empowered
humanity&apos;s task automatization capacity. However, the huge improvement in the
performance of these models is highly correlated with their increasing level of
complexity, limiting their usefulness in human-oriented applications, which are
usually deployed in resource-constrained devices. This led to the development
of compression techniques that drastically reduce the computational and memory
costs of deep learning models without significant performance degradation. This
paper aims to systematize the current literature on this topic by presenting a
comprehensive survey of model compression techniques in biometrics
applications, namely quantization, knowledge distillation and pruning. We
conduct a critical analysis of the comparative value of these techniques,
focusing on their advantages and disadvantages and presenting suggestions for
future work directions that can potentially improve the current methods.
Additionally, we discuss and analyze the link between model bias and model
compression, highlighting the need to direct compression research toward model
fairness in future works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caldeira_E/0/1/0/all/0/1&quot;&gt;Eduarda Caldeira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neto_P/0/1/0/all/0/1&quot;&gt;Pedro C. Neto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1&quot;&gt;Marco Huber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1&quot;&gt;Naser Damer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sequeira_A/0/1/0/all/0/1&quot;&gt;Ana F. Sequeira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10148">
<title>Explicitly Disentangled Representations in Object-Centric Learning. (arXiv:2401.10148v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10148</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracting structured representations from raw visual data is an important
and long-standing challenge in machine learning. Recently, techniques for
unsupervised learning of object-centric representations have raised growing
interest. In this context, enhancing the robustness of the latent features can
improve the efficiency and effectiveness of the training of downstream tasks. A
promising step in this direction is to disentangle the factors that cause
variation in the data. Previously, Invariant Slot Attention disentangled
position, scale, and orientation from the remaining features. Extending this
approach, we focus on separating the shape and texture components. In
particular, we propose a novel architecture that biases object-centric models
toward disentangling shape and texture components into two non-overlapping
subsets of the latent space dimensions. These subsets are known a priori, hence
before the training process. Experiments on a range of object-centric
benchmarks reveal that our approach achieves the desired disentanglement while
also numerically improving baseline performance in most cases. In addition, we
show that our method can generate novel textures for a specific object or
transfer textures between objects with distinct shapes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majellaro_R/0/1/0/all/0/1&quot;&gt;Riccardo Majellaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collu_J/0/1/0/all/0/1&quot;&gt;Jonathan Collu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1&quot;&gt;Aske Plaat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moerland_T/0/1/0/all/0/1&quot;&gt;Thomas M. Moerland&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10150">
<title>Motion-Zero: Zero-Shot Moving Object Control Framework for Diffusion-Based Video Generation. (arXiv:2401.10150v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10150</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent large-scale pre-trained diffusion models have demonstrated a powerful
generative ability to produce high-quality videos from detailed text
descriptions. However, exerting control over the motion of objects in videos
generated by any video diffusion model is a challenging problem. In this paper,
we propose a novel zero-shot moving object trajectory control framework,
Motion-Zero, to enable a bounding-box-trajectories-controlled text-to-video
diffusion model.To this end, an initial noise prior module is designed to
provide a position-based prior to improve the stability of the appearance of
the moving object and the accuracy of position. In addition, based on the
attention map of the U-net, spatial constraints are directly applied to the
denoising process of diffusion models, which further ensures the positional and
spatial consistency of moving objects during the inference. Furthermore,
temporal consistency is guaranteed with a proposed shift temporal attention
mechanism. Our method can be flexibly applied to various state-of-the-art video
diffusion models without any training process. Extensive experiments
demonstrate our proposed method can control the motion trajectories of objects
and generate high-quality videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changgu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_J/0/1/0/all/0/1&quot;&gt;Junwei Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lianggangxu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1&quot;&gt;Gaoqi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10153">
<title>Importance-Aware Image Segmentation-based Semantic Communication for Autonomous Driving. (arXiv:2401.10153v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2401.10153</link>
<description rdf:parseType="Literal">&lt;p&gt;This article studies the problem of image segmentation-based semantic
communication in autonomous driving. In real traffic scenes, detecting the key
objects (e.g., vehicles, pedestrians and obstacles) is more crucial than that
of other objects to guarantee driving safety. Therefore, we propose a vehicular
image segmentation-oriented semantic communication system, termed VIS-SemCom,
where image segmentation features of important objects are transmitted to
reduce transmission redundancy. First, to accurately extract image semantics,
we develop a semantic codec based on Swin Transformer architecture, which
expands the perceptual field thus improving the segmentation accuracy. Next, we
propose a multi-scale semantic extraction scheme via assigning the number of
Swin Transformer blocks for diverse resolution features, thus highlighting the
important objects&apos; accuracy. Furthermore, the importance-aware loss is invoked
to emphasize the important objects, and an online hard sample mining (OHEM)
strategy is proposed to handle small sample issues in the dataset. Experimental
results demonstrate that the proposed VIS-SemCom can achieve a coding gain of
nearly 6 dB with a 60% mean intersection over union (mIoU), reduce the
transmitted data amount by up to 70% with a 60% mIoU, and improve the
segmentation intersection over union (IoU) of important objects by 4%, compared
to traditional transmission scheme.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1&quot;&gt;Jie Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1&quot;&gt;Haonan Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Q/0/1/0/all/0/1&quot;&gt;Qiang Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhilong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xinxin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1&quot;&gt;Tao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1&quot;&gt;Changchuan Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10166">
<title>VMamba: Visual State Space Model. (arXiv:2401.10166v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10166</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) stand as
the two most popular foundation models for visual representation learning.
While CNNs exhibit remarkable scalability with linear complexity w.r.t. image
resolution, ViTs surpass them in fitting capabilities despite contending with
quadratic complexity. A closer inspection reveals that ViTs achieve superior
visual modeling performance through the incorporation of global receptive
fields and dynamic weights. This observation motivates us to propose a novel
architecture that inherits these components while enhancing computational
efficiency. To this end, we draw inspiration from the recently introduced state
space model and propose the Visual State Space Model (VMamba), which achieves
linear complexity without sacrificing global receptive fields. To address the
encountered direction-sensitive issue, we introduce the Cross-Scan Module (CSM)
to traverse the spatial domain and convert any non-causal visual image into
order patch sequences. Extensive experimental results substantiate that VMamba
not only demonstrates promising capabilities across various visual perception
tasks, but also exhibits more pronounced advantages over established benchmarks
as the image resolution increases. Source code has been available at
https://github.com/MzeroMiko/VMamba.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yunjie Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yuzhong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hongtian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Lingxi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qixiang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yunfan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10171">
<title>SHINOBI: Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild. (arXiv:2401.10171v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10171</link>
<description rdf:parseType="Literal">&lt;p&gt;We present SHINOBI, an end-to-end framework for the reconstruction of shape,
material, and illumination from object images captured with varying lighting,
pose, and background. Inverse rendering of an object based on unconstrained
image collections is a long-standing challenge in computer vision and graphics
and requires a joint optimization over shape, radiance, and pose. We show that
an implicit shape representation based on a multi-resolution hash encoding
enables faster and robust shape reconstruction with joint camera alignment
optimization that outperforms prior work. Further, to enable the editing of
illumination and object reflectance (i.e. material) we jointly optimize BRDF
and illumination together with the object&apos;s shape. Our method is class-agnostic
and works on in-the-wild image collections of objects to produce relightable 3D
assets for several use cases such as AR/VR, movies, games, etc. Project page:
https://shinobi.aengelhardt.com Video:
https://www.youtube.com/watch?v=iFENQ6AcYd8&amp;amp;feature=youtu.be
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engelhardt_A/0/1/0/all/0/1&quot;&gt;Andreas Engelhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raj_A/0/1/0/all/0/1&quot;&gt;Amit Raj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boss_M/0/1/0/all/0/1&quot;&gt;Mark Boss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunzhi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kar_A/0/1/0/all/0/1&quot;&gt;Abhishek Kar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1&quot;&gt;Deqing Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brualla_R/0/1/0/all/0/1&quot;&gt;Ricardo Martin Brualla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1&quot;&gt;Jonathan T. Barron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1&quot;&gt;Hendrik P. A. Lensch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1&quot;&gt;Varun Jampani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10176">
<title>Comprehensive OOD Detection Improvements. (arXiv:2401.10176v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10176</link>
<description rdf:parseType="Literal">&lt;p&gt;As machine learning becomes increasingly prevalent in impactful decisions,
recognizing when inference data is outside the model&apos;s expected input
distribution is paramount for giving context to predictions.
Out-of-distribution (OOD) detection methods have been created for this task.
Such methods can be split into representation-based or logit-based methods from
whether they respectively utilize the model&apos;s embeddings or predictions for OOD
detection. In contrast to most papers which solely focus on one such group, we
address both. We employ dimensionality reduction on feature embeddings in
representation-based methods for both time speedups and improved performance.
Additionally, we propose DICE-COL, a modification of the popular logit-based
method Directed Sparsification (DICE) that resolves an unnoticed flaw. We
demonstrate the effectiveness of our methods on the OpenOODv1.5 benchmark
framework, where they significantly improve performance and set
state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakkapragada_A/0/1/0/all/0/1&quot;&gt;Anish Lakkapragada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khanna_A/0/1/0/all/0/1&quot;&gt;Amol Khanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1&quot;&gt;Edward Raff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inkawhich_N/0/1/0/all/0/1&quot;&gt;Nathan Inkawhich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10178">
<title>Neural Echos: Depthwise Convolutional Filters Replicate Biological Receptive Fields. (arXiv:2401.10178v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10178</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we present evidence suggesting that depthwise convolutional
kernels are effectively replicating the structural intricacies of the
biological receptive fields observed in the mammalian retina. We provide
analytics of trained kernels from various state-of-the-art models
substantiating this evidence. Inspired by this intriguing discovery, we propose
an initialization scheme that draws inspiration from the biological receptive
fields. Experimental analysis of the ImageNet dataset with multiple CNN
architectures featuring depthwise convolutions reveals a marked enhancement in
the accuracy of the learned model when initialized with biologically derived
weights. This underlies the potential for biologically inspired computational
models to further our understanding of vision processing systems and to improve
the efficacy of convolutional networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babaiee_Z/0/1/0/all/0/1&quot;&gt;Zahra Babaiee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiasari_P/0/1/0/all/0/1&quot;&gt;Peyman M. Kiasari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1&quot;&gt;Daniela Rus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosu_R/0/1/0/all/0/1&quot;&gt;Radu Grosu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10191">
<title>Divide and not forget: Ensemble of selectively trained experts in Continual Learning. (arXiv:2401.10191v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10191</link>
<description rdf:parseType="Literal">&lt;p&gt;Class-incremental learning is becoming more popular as it helps models widen
their applicability while not forgetting what they already know. A trend in
this area is to use a mixture-of-expert technique, where different models work
together to solve the task. However, the experts are usually trained all at
once using whole task data, which makes them all prone to forgetting and
increasing computational burden. To address this limitation, we introduce a
novel approach named SEED. SEED selects only one, the most optimal expert for a
considered task, and uses data from this task to fine-tune only this expert.
For this purpose, each expert represents each class with a Gaussian
distribution, and the optimal expert is selected based on the similarity of
those distributions. Consequently, SEED increases diversity and heterogeneity
within the experts while maintaining the high stability of this ensemble
method. The extensive experiments demonstrate that SEED achieves
state-of-the-art performance in exemplar-free settings across various
scenarios, showing the potential of expert diversification through data in
continual learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rypesc_G/0/1/0/all/0/1&quot;&gt;Grzegorz Rype&amp;#x15b;&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cygert_S/0/1/0/all/0/1&quot;&gt;Sebastian Cygert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_V/0/1/0/all/0/1&quot;&gt;Valeriya Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1&quot;&gt;Tomasz Trzci&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zielinski_B/0/1/0/all/0/1&quot;&gt;Bartosz Zieli&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1&quot;&gt;Bart&amp;#x142;omiej Twardowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10208">
<title>MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer. (arXiv:2401.10208v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10208</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing generative models for interleaved image-text data has both
research and practical value. It requires models to understand the interleaved
sequences and subsequently generate images and text. However, existing attempts
are limited by the issue that the fixed number of visual tokens cannot
efficiently capture image details, which is particularly problematic in the
multi-image scenarios. To address this, this paper presents MM-Interleaved, an
end-to-end generative model for interleaved image-text data. It introduces a
multi-scale and multi-image feature synchronizer module, allowing direct access
to fine-grained image features in the previous context during the generation
process. MM-Interleaved is end-to-end pre-trained on both paired and
interleaved image-text corpora. It is further enhanced through a supervised
fine-tuning phase, wherein the model improves its ability to follow complex
multi-modal instructions. Experiments demonstrate the versatility of
MM-Interleaved in recognizing visual details following multi-modal instructions
and generating consistent images following both textual and visual conditions.
Code and models are available at
\url{https://github.com/OpenGVLab/MM-Interleaved}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1&quot;&gt;Changyao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xizhou Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yuwen Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weiyun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuntao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Lewei Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Tong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10213">
<title>Improving automatic detection of driver fatigue and distraction using machine learning. (arXiv:2401.10213v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10213</link>
<description rdf:parseType="Literal">&lt;p&gt;Changes and advances in information technology have played an important role
in the development of intelligent vehicle systems in recent years. Driver
fatigue and distracted driving are important factors in traffic accidents.
Thus, onboard monitoring of driving behavior has become a crucial component of
advanced driver assistance systems for intelligent vehicles. In this article,
we present techniques for simultaneously detecting fatigue and distracted
driving behaviors using vision-based and machine learning-based approaches. In
driving fatigue detection, we use facial alignment networks to identify facial
feature points in the images, and calculate the distance of the facial feature
points to detect the opening and closing of the eyes and mouth. Furthermore, we
use a convolutional neural network (CNN) based on the MobileNet architecture to
identify various distracted driving behaviors. Experiments are performed on a
PC based setup with a webcam and results are demonstrated using public datasets
as well as custom datasets created for training and testing. Compared to
previous approaches, we build our own datasets and provide better results in
terms of accuracy and computation time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Dongjiang Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10215">
<title>GPAvatar: Generalizable and Precise Head Avatar from Image(s). (arXiv:2401.10215v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10215</link>
<description rdf:parseType="Literal">&lt;p&gt;Head avatar reconstruction, crucial for applications in virtual reality,
online meetings, gaming, and film industries, has garnered substantial
attention within the computer vision community. The fundamental objective of
this field is to faithfully recreate the head avatar and precisely control
expressions and postures. Existing methods, categorized into 2D-based warping,
mesh-based, and neural rendering approaches, present challenges in maintaining
multi-view consistency, incorporating non-facial information, and generalizing
to new identities. In this paper, we propose a framework named GPAvatar that
reconstructs 3D head avatars from one or several images in a single forward
pass. The key idea of this work is to introduce a dynamic point-based
expression field driven by a point cloud to precisely and effectively capture
expressions. Furthermore, we use a Multi Tri-planes Attention (MTA) fusion
module in the tri-planes canonical field to leverage information from multiple
input images. The proposed method achieves faithful identity reconstruction,
precise expression control, and multi-view consistency, demonstrating promising
results for free-viewpoint rendering and novel view synthesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1&quot;&gt;Xuangeng Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Ailing Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tianyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Lijian Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yunfei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1&quot;&gt;Tatsuya Harada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10217">
<title>Explaining the Implicit Neural Canvas: Connecting Pixels to Neurons by Tracing their Contributions. (arXiv:2401.10217v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10217</link>
<description rdf:parseType="Literal">&lt;p&gt;The many variations of Implicit Neural Representations (INRs), where a neural
network is trained as a continuous representation of a signal, have tremendous
practical utility for downstream tasks including novel view synthesis, video
compression, and image superresolution. Unfortunately, the inner workings of
these networks are seriously under-studied. Our work, eXplaining the Implicit
Neural Canvas (XINC), is a unified framework for explaining properties of INRs
by examining the strength of each neuron&apos;s contribution to each output pixel.
We call the aggregate of these contribution maps the Implicit Neural Canvas and
we use this concept to demonstrate that the INRs which we study learn to
&apos;&apos;see&apos;&apos; the frames they represent in surprising ways. For example, INRs tend to
have highly distributed representations. While lacking high-level object
semantics, they have a significant bias for color and edges, and are almost
entirely space-agnostic. We arrive at our conclusions by examining how objects
are represented across time in video INRs, using clustering to visualize
similar neurons across layers and architectures, and show that this is
dominated by motion. These insights demonstrate the general usefulness of our
analysis framework. Our project page is available at
https://namithap10.github.io/xinc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padmanabhan_N/0/1/0/all/0/1&quot;&gt;Namitha Padmanabhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gwilliam_M/0/1/0/all/0/1&quot;&gt;Matthew Gwilliam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1&quot;&gt;Pulkit Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maiya_S/0/1/0/all/0/1&quot;&gt;Shishira R Maiya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehrlich_M/0/1/0/all/0/1&quot;&gt;Max Ehrlich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1&quot;&gt;Abhinav Shrivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10219">
<title>Edit One for All: Interactive Batch Image Editing. (arXiv:2401.10219v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10219</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, image editing has advanced remarkably. With increased human
control, it is now possible to edit an image in a plethora of ways; from
specifying in text what we want to change, to straight up dragging the contents
of the image in an interactive point-based manner. However, most of the focus
has remained on editing single images at a time. Whether and how we can
simultaneously edit large batches of images has remained understudied. With the
goal of minimizing human supervision in the editing process, this paper
presents a novel method for interactive batch image editing using StyleGAN as
the medium. Given an edit specified by users in an example image (e.g., make
the face frontal), our method can automatically transfer that edit to other
test images, so that regardless of their initial state (pose), they all arrive
at the same final state (e.g., all facing front). Extensive experiments
demonstrate that edits performed using our method have similar visual quality
to existing single-image-editing methods, while having more visual consistency
and saving significant time and human effort.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thao Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ojha_U/0/1/0/all/0/1&quot;&gt;Utkarsh Ojha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haotian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yong Jae Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10220">
<title>AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data. (arXiv:2401.10220v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10220</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation models encode rich representations that can be adapted to a
desired task by fine-tuning on task-specific data. However, fine-tuning a model
on one particular data distribution often compromises the model&apos;s original
performance on other distributions. Current methods for robust fine-tuning
utilize hand-crafted regularization techniques to constrain the fine-tuning
process towards the base foundation model. Yet, it is hard to precisely specify
what characteristics of the foundation model to retain during fine-tuning, as
this depends on how the pre-training, fine-tuning, and evaluation data
distributions relate to each other. We propose AutoFT, a data-driven approach
for guiding foundation model fine-tuning. AutoFT optimizes fine-tuning
hyperparameters to maximize performance on a small out-of-distribution (OOD)
validation set. To guide fine-tuning in a granular way, AutoFT searches a
highly expressive hyperparameter space that includes weight coefficients for
many different losses, in addition to learning rate and weight decay values. We
evaluate AutoFT on nine natural distribution shifts which include domain shifts
and subpopulation shifts. Our experiments show that AutoFT significantly
improves generalization to new OOD data, outperforming existing robust
fine-tuning methods. Notably, AutoFT achieves new state-of-the-art performance
on the WILDS-iWildCam and WILDS-FMoW benchmarks, outperforming the previous
best methods by $6.0\%$ and $1.5\%$, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1&quot;&gt;Caroline Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yoonho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Annie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Allan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1&quot;&gt;Aditi Raghunathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10222">
<title>Supervised Fine-tuning in turn Improves Visual Foundation Models. (arXiv:2401.10222v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10222</link>
<description rdf:parseType="Literal">&lt;p&gt;Image-text training like CLIP has dominated the pretraining of vision
foundation models in recent years. Subsequent efforts have been made to
introduce region-level visual learning into CLIP&apos;s pretraining but face
scalability challenges due to the lack of large-scale region-level datasets.
Drawing inspiration from supervised fine-tuning (SFT) in natural language
processing such as instruction tuning, we explore the potential of fine-grained
SFT in enhancing the generation of vision foundation models after their
pretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash
the fine-grained knowledge of vision foundation models. In ViSFT, the vision
foundation model is enhanced by performing visual joint learning on some
in-domain tasks and then tested on out-of-domain benchmarks. With updating
using ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over
4.4B parameters shows improvements across various out-of-domain benchmarks
including vision and vision-linguistic scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xiaohu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yixiao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yuying Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1&quot;&gt;Chun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10224">
<title>The Manga Whisperer: Automatically Generating Transcriptions for Comics. (arXiv:2401.10224v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10224</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past few decades, Japanese comics, commonly referred to as Manga, have
transcended both cultural and linguistic boundaries to become a true worldwide
sensation. Yet, the inherent reliance on visual cues and illustration within
manga renders it largely inaccessible to individuals with visual impairments.
In this work, we seek to address this substantial barrier, with the aim of
ensuring that manga can be appreciated and actively engaged by everyone.
Specifically, we tackle the problem of diarisation i.e. generating a
transcription of who said what and when, in a fully automatic way.
&lt;/p&gt;
&lt;p&gt;To this end, we make the following contributions: (1) we present a unified
model, Magi, that is able to (a) detect panels, text boxes and character boxes,
(b) cluster characters by identity (without knowing the number of clusters
apriori), and (c) associate dialogues to their speakers; (2) we propose a novel
approach that is able to sort the detected text boxes in their reading order
and generate a dialogue transcript; (3) we annotate an evaluation benchmark for
this task using publicly available [English] manga pages. The code, evaluation
datasets and the pre-trained model can be found at:
https://github.com/ragavsachdeva/magi.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachdeva_R/0/1/0/all/0/1&quot;&gt;Ragav Sachdeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1&quot;&gt;Andrew Zisserman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10226">
<title>Towards Language-Driven Video Inpainting via Multimodal Large Language Models. (arXiv:2401.10226v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10226</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new task -- language-driven video inpainting, which uses
natural language instructions to guide the inpainting process. This approach
overcomes the limitations of traditional video inpainting methods that depend
on manually labeled binary masks, a process often tedious and labor-intensive.
We present the Remove Objects from Videos by Instructions (ROVI) dataset,
containing 5,650 videos and 9,091 inpainting results, to support training and
evaluation for this task. We also propose a novel diffusion-based
language-driven video inpainting framework, the first end-to-end baseline for
this task, integrating Multimodal Large Language Models to understand and
execute complex language-based inpainting requests effectively. Our
comprehensive results showcase the dataset&apos;s versatility and the model&apos;s
effectiveness in various language-instructed inpainting scenarios. We will make
datasets, code, and models publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jianzong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangtai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1&quot;&gt;Chenyang Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shangchen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jingkang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiangning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yining Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1&quot;&gt;Yunhai Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1&quot;&gt;Chen Change Loy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10227">
<title>A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting. (arXiv:2401.10227v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10227</link>
<description rdf:parseType="Literal">&lt;p&gt;Panoptic and instance segmentation networks are often trained with
specialized object detection modules, complex loss functions, and ad-hoc
post-processing steps to handle the permutation-invariance of the instance
masks. This work builds upon Stable Diffusion and proposes a latent diffusion
approach for panoptic segmentation, resulting in a simple architecture which
omits these complexities. Our training process consists of two steps: (1)
training a shallow autoencoder to project the segmentation masks to latent
space; (2) training a diffusion model to allow image-conditioned sampling in
latent space. The use of a generative model unlocks the exploration of mask
completion or inpainting, which has applications in interactive segmentation.
The experimental validation yields promising results for both panoptic
segmentation and mask inpainting. While not setting a new state-of-the-art, our
model&apos;s simplicity, generality, and mask completion capability are desirable
properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gansbeke_W/0/1/0/all/0/1&quot;&gt;Wouter Van Gansbeke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brabandere_B/0/1/0/all/0/1&quot;&gt;Bert De Brabandere&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10228">
<title>RAP-SAM: Towards Real-Time All-Purpose Segment Anything. (arXiv:2401.10228v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10228</link>
<description rdf:parseType="Literal">&lt;p&gt;Advanced by transformer architecture, vision foundation models (VFMs) achieve
remarkable progress in performance and generalization ability. Segment Anything
Model (SAM) is one remarkable model that can achieve generalized segmentation.
However, most VFMs cannot run in realtime, which makes it difficult to transfer
them into several products. On the other hand, current real-time segmentation
mainly has one purpose, such as semantic segmentation on the driving scene. We
argue that diverse outputs are needed for real applications. Thus, this work
explores a new real-time segmentation setting, named all-purpose segmentation
in real-time, to transfer VFMs in real-time deployment. It contains three
different tasks, including interactive segmentation, panoptic segmentation, and
video segmentation. We aim to use one model to achieve the above tasks in
real-time. We first benchmark several strong baselines. Then, we present
Real-Time All Purpose SAM (RAP-SAM). It contains an efficient encoder and an
efficient decoupled decoder to perform prompt-driven decoding. Moreover, we
further explore different training strategies and tuning methods to boost
co-training performance further. Our code and model are available at
https://github.com/xushilin1/RAP-SAM/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shilin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Haobo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1&quot;&gt;Qingyu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1&quot;&gt;Lu Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yibo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yining Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1&quot;&gt;Yunhai Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1&quot;&gt;Bernard Ghanem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangtai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming-Hsuan Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10229">
<title>OMG-Seg: Is One Model Good Enough For All Segmentation?. (arXiv:2401.10229v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10229</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we address various segmentation tasks, each traditionally
tackled by distinct or partially unified models. We propose OMG-Seg, One Model
that is Good enough to efficiently and effectively handle all the segmentation
tasks, including image semantic, instance, and panoptic segmentation, as well
as their video counterparts, open vocabulary settings, prompt-driven,
interactive segmentation like SAM, and video object segmentation. To our
knowledge, this is the first model to handle all these tasks in one model and
achieve satisfactory performance. We show that OMG-Seg, a transformer-based
encoder-decoder architecture with task-specific queries and outputs, can
support over ten distinct segmentation tasks and yet significantly reduce
computational and parameter overhead across various tasks and datasets. We
rigorously evaluate the inter-task influences and correlations during
co-training. Code and models are available at https://github.com/lxtGH/OMG-Seg.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangtai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Haobo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Henghui Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Size Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yining Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1&quot;&gt;Chen Change Loy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10232">
<title>ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative Modeling of Human-Object Interactions. (arXiv:2401.10232v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10232</link>
<description rdf:parseType="Literal">&lt;p&gt;To enable machines to learn how humans interact with the physical world in
our daily activities, it is crucial to provide rich data that encompasses the
3D motion of humans as well as the motion of objects in a learnable 3D
representation. Ideally, this data should be collected in a natural setup,
capturing the authentic dynamic 3D signals during human-object interactions. To
address this challenge, we introduce the ParaHome system, designed to capture
and parameterize dynamic 3D movements of humans and objects within a common
home environment. Our system consists of a multi-view setup with 70
synchronized RGB cameras, as well as wearable motion capture devices equipped
with an IMU-based body suit and hand motion capture gloves. By leveraging the
ParaHome system, we collect a novel large-scale dataset of human-object
interaction. Notably, our dataset offers key advancement over existing datasets
in three main aspects: (1) capturing 3D body and dexterous hand manipulation
motion alongside 3D object movement within a contextual home environment during
natural activities; (2) encompassing human interaction with multiple objects in
various episodic scenarios with corresponding descriptions in texts; (3)
including articulated objects with multiple parts expressed with parameterized
articulations. Building upon our dataset, we introduce new research tasks aimed
at building a generative model for learning and synthesizing human-object
interactions in a real-world room setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jeonghwan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jisoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Na_J/0/1/0/all/0/1&quot;&gt;Jeonghyeon Na&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1&quot;&gt;Hanbyul Joo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.08219">
<title>HaGRID - HAnd Gesture Recognition Image Dataset. (arXiv:2206.08219v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.08219</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces an enormous dataset, HaGRID (HAnd Gesture Recognition
Image Dataset), to build a hand gesture recognition (HGR) system concentrating
on interaction with devices to manage them. That is why all 18 chosen gestures
are endowed with the semiotic function and can be interpreted as a specific
action. Although the gestures are static, they were picked up, especially for
the ability to design several dynamic gestures. It allows the trained model to
recognize not only static gestures such as &quot;like&quot; and &quot;stop&quot; but also &quot;swipes&quot;
and &quot;drag and drop&quot; dynamic gestures. The HaGRID contains 554,800 images and
bounding box annotations with gesture labels to solve hand detection and
gesture classification tasks. The low variability in context and subjects of
other datasets was the reason for creating the dataset without such
limitations. Utilizing crowdsourcing platforms allowed us to collect samples
recorded by 37,583 subjects in at least as many scenes with subject-to-camera
distances from 0.5 to 4 meters in various natural light conditions. The
influence of the diversity characteristics was assessed in ablation study
experiments. Also, we demonstrate the HaGRID ability to be used for pretraining
models in HGR tasks. The HaGRID and pretrained models are publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapitanov_A/0/1/0/all/0/1&quot;&gt;Alexander Kapitanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kvanchiani_K/0/1/0/all/0/1&quot;&gt;Karina Kvanchiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagaev_A/0/1/0/all/0/1&quot;&gt;Alexander Nagaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kraynov_R/0/1/0/all/0/1&quot;&gt;Roman Kraynov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makhliarchuk_A/0/1/0/all/0/1&quot;&gt;Andrei Makhliarchuk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.07929">
<title>Towards Lightweight Super-Resolution with Dual Regression Learning. (arXiv:2207.07929v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.07929</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have exhibited remarkable performance in image
super-resolution (SR) tasks by learning a mapping from low-resolution (LR)
images to high-resolution (HR) images. However, the SR problem is typically an
ill-posed problem and existing methods would come with several limitations.
First, the possible mapping space of SR can be extremely large since there may
exist many different HR images that can be super-resolved from the same LR
image. As a result, it is hard to directly learn a promising SR mapping from
such a large space. Second, it is often inevitable to develop very large models
with extremely high computational cost to yield promising SR performance. In
practice, one can use model compression techniques to obtain compact models by
reducing model redundancy. Nevertheless, it is hard for existing model
compression methods to accurately identify the redundant components due to the
extremely large SR mapping space. To alleviate the first challenge, we propose
a dual regression learning scheme to reduce the space of possible SR mappings.
Specifically, in addition to the mapping from LR to HR images, we learn an
additional dual regression mapping to estimate the downsampling kernel and
reconstruct LR images. In this way, the dual mapping acts as a constraint to
reduce the space of possible mappings. To address the second challenge, we
propose a dual regression compression (DRC) method to reduce model redundancy
in both layer-level and channel-level based on channel pruning. Specifically,
we first develop a channel number search method that minimizes the dual
regression loss to determine the redundancy of each layer. Given the searched
channel numbers, we further exploit the dual regression manner to evaluate the
importance of channels and prune the redundant ones. Extensive experiments show
the effectiveness of our method in obtaining accurate and efficient SR models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingdong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jiezhang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zeshuai Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yanwu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1&quot;&gt;Mingkui Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11086">
<title>An Embarrassingly Simple Baseline for Imbalanced Semi-Supervised Learning. (arXiv:2211.11086v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11086</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised learning (SSL) has shown great promise in leveraging
unlabeled data to improve model performance. While standard SSL assumes uniform
data distribution, we consider a more realistic and challenging setting called
imbalanced SSL, where imbalanced class distributions occur in both labeled and
unlabeled data. Although there are existing endeavors to tackle this challenge,
their performance degenerates when facing severe imbalance since they can not
reduce the class imbalance sufficiently and effectively. In this paper, we
study a simple yet overlooked baseline -- SimiS -- which tackles data imbalance
by simply supplementing labeled data with pseudo-labels, according to the
difference in class distribution from the most frequent class. Such a simple
baseline turns out to be highly effective in reducing class imbalance. It
outperforms existing methods by a significant margin, e.g., 12.8%, 13.6%, and
16.7% over previous SOTA on CIFAR100-LT, FOOD101-LT, and ImageNet127
respectively. The reduced imbalance results in faster convergence and better
pseudo-label accuracy of SimiS. The simplicity of our method also makes it
possible to be combined with other re-balancing techniques to improve the
performance further. Moreover, our method shows great robustness to a wide
range of data distributions, which holds enormous potential in practice. Code
will be publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yue Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yidong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1&quot;&gt;Bernt Schiele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savvides_M/0/1/0/all/0/1&quot;&gt;Marios Savvides&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1&quot;&gt;Bhiksha Raj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.08044">
<title>Benchmarking Robustness of Multimodal Image-Text Models under Distribution Shift. (arXiv:2212.08044v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.08044</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal image-text models have shown remarkable performance in the past
few years. However, evaluating robustness against distribution shifts is
crucial before adopting them in real-world applications. In this work, we
investigate the robustness of 12 popular open-sourced image-text models under
common perturbations on five tasks (image-text retrieval, visual reasoning,
visual entailment, image captioning, and text-to-image generation). In
particular, we propose several new multimodal robustness benchmarks by applying
17 image perturbation and 16 text perturbation techniques on top of existing
datasets. We observe that multimodal models are not robust to image and text
perturbations, especially to image perturbations. Among the tested perturbation
methods, character-level perturbations constitute the most severe distribution
shift for text, and zoom blur is the most severe shift for image data. We also
introduce two new robustness metrics (\textbf{MMI} for MultiModal Impact score
and \textbf{MOR} for Missing Object Rate) for proper evaluations of multimodal
models. We hope our extensive study sheds light on new directions for the
development of robust multimodal models. More details can be found on the
project webpage: \url{https://MMRobustness.github.io}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jielin Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xingjian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wenzel_F/0/1/0/all/0/1&quot;&gt;Florian Wenzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Ding Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09129">
<title>SUCRe: Leveraging Scene Structure for Underwater Color Restoration. (arXiv:2212.09129v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09129</link>
<description rdf:parseType="Literal">&lt;p&gt;Underwater images are altered by the physical characteristics of the medium
through which light rays pass before reaching the optical sensor. Scattering
and wavelength-dependent absorption significantly modify the captured colors
depending on the distance of observed elements to the image plane. In this
paper, we aim to recover an image of the scene as if the water had no effect on
light propagation. We introduce SUCRe, a novel method that exploits the scene&apos;s
3D structure for underwater color restoration. By following points in multiple
images and tracking their intensities at different distances to the sensor, we
constrain the optimization of the parameters in an underwater image formation
model and retrieve unattenuated pixel intensities. We conduct extensive
quantitative and qualitative analyses of our approach in a variety of scenarios
ranging from natural light to deep-sea environments using three underwater
datasets acquired from real-world scenarios and one synthetic dataset. We also
compare the performance of the proposed approach with that of a wide range of
existing state-of-the-art methods. The results demonstrate a consistent benefit
of exploiting multiple views across a spectrum of objective metrics. Our code
is publicly available at https://github.com/clementinboittiaux/sucre.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boittiaux_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;mentin Boittiaux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marxer_R/0/1/0/all/0/1&quot;&gt;Ricard Marxer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dune_C/0/1/0/all/0/1&quot;&gt;Claire Dune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arnaubec_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lien Arnaubec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrera_M/0/1/0/all/0/1&quot;&gt;Maxime Ferrera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hugel_V/0/1/0/all/0/1&quot;&gt;Vincent Hugel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.00114">
<title>Skeletal Video Anomaly Detection using Deep Learning: Survey, Challenges and Future Directions. (arXiv:2301.00114v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.00114</link>
<description rdf:parseType="Literal">&lt;p&gt;The existing methods for video anomaly detection mostly utilize videos
containing identifiable facial and appearance-based features. The use of videos
with identifiable faces raises privacy concerns, especially when used in a
hospital or community-based setting. Appearance-based features can also be
sensitive to pixel-based noise, straining the anomaly detection methods to
model the changes in the background and making it difficult to focus on the
actions of humans in the foreground. Structural information in the form of
skeletons describing the human motion in the videos is privacy-protecting and
can overcome some of the problems posed by appearance-based features. In this
paper, we present a survey of privacy-protecting deep learning anomaly
detection methods using skeletons extracted from videos. We present a novel
taxonomy of algorithms based on the various learning approaches. We conclude
that skeleton-based approaches for anomaly detection can be a plausible
privacy-protecting alternative for video anomaly detection. Lastly, we identify
major open research questions and provide guidelines to address them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1&quot;&gt;Pratik K. Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mihailidis_A/0/1/0/all/0/1&quot;&gt;Alex Mihailidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Shehroz S. Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.06657">
<title>Free Lunch for Generating Effective Outlier Supervision. (arXiv:2301.06657v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.06657</link>
<description rdf:parseType="Literal">&lt;p&gt;When deployed in practical applications, computer vision systems will
encounter numerous unexpected images (\emph{{i.e.}}, out-of-distribution data).
Due to the potentially raised safety risks, these aforementioned unseen data
should be carefully identified and handled. Generally, existing approaches in
dealing with out-of-distribution (OOD) detection mainly focus on the
statistical difference between the features of OOD and in-distribution (ID)
data extracted by the classifiers. Although many of these schemes have brought
considerable performance improvements, reducing the false positive rate (FPR)
when processing open-set images, they necessarily lack reliable theoretical
analysis and generalization guarantees. Unlike the observed ways, in this
paper, we investigate the OOD detection problem based on the Bayes rule and
present a convincing description of the reason for failures encountered by
conventional classifiers. Concretely, our analysis reveals that refining the
probability distribution yielded by the vanilla neural networks is necessary
for OOD detection, alleviating the issues of assigning high confidence to OOD
data. To achieve this effortlessly, we propose an ultra-effective method to
generate near-realistic outlier supervision. Extensive experiments on
large-scale benchmarks reveal that our proposed \texttt{BayesAug} significantly
reduces the FPR95 over 12.50\% compared with the previous schemes, boosting the
reliability of machine learning systems. The code will be made publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1&quot;&gt;Sen Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiaxi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Richard Yi Da Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1&quot;&gt;Bin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1&quot;&gt;Shiming Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1&quot;&gt;Gaofeng Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.02070">
<title>Semantic-Guided Generative Image Augmentation Method with Diffusion Models for Image Classification. (arXiv:2302.02070v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.02070</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing image augmentation methods consist of two categories:
perturbation-based methods and generative methods. Perturbation-based methods
apply pre-defined perturbations to augment an original image, but only locally
vary the image, thus lacking image diversity. In contrast, generative methods
bring more image diversity in the augmented images but may not preserve
semantic consistency, thus incorrectly changing the essential semantics of the
original image. To balance image diversity and semantic consistency in
augmented images, we propose SGID, a Semantic-guided Generative Image
augmentation method with Diffusion models for image classification.
Specifically, SGID employs diffusion models to generate augmented images with
good image diversity. More importantly, SGID takes image labels and captions as
guidance to maintain semantic consistency between the augmented and original
images. Experimental results show that SGID outperforms the best augmentation
baseline by 1.72% on ResNet-50 (from scratch), 0.33% on ViT (ImageNet-21k), and
0.14% on CLIP-ViT (LAION-2B). Moreover, SGID can be combined with other image
augmentation baselines and further improves the overall performance. We
demonstrate the semantic consistency and image diversity of SGID through
quantitative human and automated evaluations, as well as qualitative case
studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bohan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinghao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1&quot;&gt;Yutai Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yunlong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Feng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuanliang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qingfu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1&quot;&gt;Wanxiang Che&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.02472">
<title>ESD: Expected Squared Difference as a Tuning-Free Trainable Calibration Measure. (arXiv:2303.02472v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.02472</link>
<description rdf:parseType="Literal">&lt;p&gt;Studies have shown that modern neural networks tend to be poorly calibrated
due to over-confident predictions. Traditionally, post-processing methods have
been used to calibrate the model after training. In recent years, various
trainable calibration measures have been proposed to incorporate them directly
into the training process. However, these methods all incorporate internal
hyperparameters, and the performance of these calibration objectives relies on
tuning these hyperparameters, incurring more computational costs as the size of
neural networks and datasets become larger. As such, we present Expected
Squared Difference (ESD), a tuning-free (i.e., hyperparameter-free) trainable
calibration objective loss, where we view the calibration error from the
perspective of the squared difference between the two expectations. With
extensive experiments on several architectures (CNNs, Transformers) and
datasets, we demonstrate that (1) incorporating ESD into the training improves
model calibration in various batch size settings without the need for internal
hyperparameter tuning, (2) ESD yields the best-calibrated results compared with
previous approaches, and (3) ESD drastically improves the computational costs
required for calibration during training due to the absence of internal
hyperparameter. The code is publicly accessible at
https://github.com/hee-suk-yoon/ESD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1&quot;&gt;Hee Suk Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tee_J/0/1/0/all/0/1&quot;&gt;Joshua Tian Jin Tee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_E/0/1/0/all/0/1&quot;&gt;Eunseop Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sunjae Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1&quot;&gt;Gwangsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingzhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1&quot;&gt;Chang D. Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12307">
<title>Curvature-Balanced Feature Manifold Learning for Long-Tailed Classification. (arXiv:2303.12307v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12307</link>
<description rdf:parseType="Literal">&lt;p&gt;To address the challenges of long-tailed classification, researchers have
proposed several approaches to reduce model bias, most of which assume that
classes with few samples are weak classes. However, recent studies have shown
that tail classes are not always hard to learn, and model bias has been
observed on sample-balanced datasets, suggesting the existence of other factors
that affect model bias. In this work, we systematically propose a series of
geometric measurements for perceptual manifolds in deep neural networks, and
then explore the effect of the geometric characteristics of perceptual
manifolds on classification difficulty and how learning shapes the geometric
characteristics of perceptual manifolds. An unanticipated finding is that the
correlation between the class accuracy and the separation degree of perceptual
manifolds gradually decreases during training, while the negative correlation
with the curvature gradually increases, implying that curvature imbalance leads
to model bias. Therefore, we propose curvature regularization to facilitate the
model to learn curvature-balanced and flatter perceptual manifolds. Evaluations
on multiple long-tailed and non-long-tailed datasets show the excellent
performance and exciting generality of our approach, especially in achieving
significant performance improvements based on current state-of-the-art
techniques. Our work opens up a geometric analysis perspective on model bias
and reminds researchers to pay attention to model bias on non-long-tailed and
even sample-balanced datasets. The code and model will be made public.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yanbiao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1&quot;&gt;Licheng Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shuyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lingling Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.00429">
<title>Information Recovery-Driven Deep Incomplete Multiview Clustering Network. (arXiv:2304.00429v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.00429</link>
<description rdf:parseType="Literal">&lt;p&gt;Incomplete multi-view clustering is a hot and emerging topic. It is well
known that unavoidable data incompleteness greatly weakens the effective
information of multi-view data. To date, existing incomplete multi-view
clustering methods usually bypass unavailable views according to prior missing
information, which is considered as a second-best scheme based on evasion.
Other methods that attempt to recover missing information are mostly applicable
to specific two-view datasets. To handle these problems, in this paper, we
propose an information recovery-driven deep incomplete multi-view clustering
network, termed as RecFormer. Concretely, a two-stage autoencoder network with
the self-attention structure is built to synchronously extract high-level
semantic representations of multiple views and recover the missing data.
Besides, we develop a recurrent graph reconstruction mechanism that cleverly
leverages the restored views to promote the representation learning and the
further data reconstruction. Visualization of recovery results are given and
sufficient experimental results confirm that our RecFormer has obvious
advantages over other top methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chengliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Jie Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhihao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiaoling Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yong Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.05156">
<title>Accelerating Globally Optimal Consensus Maximization in Geometric Vision. (arXiv:2304.05156v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.05156</link>
<description rdf:parseType="Literal">&lt;p&gt;Branch-and-bound-based consensus maximization stands out due to its important
ability of retrieving the globally optimal solution to outlier-affected
geometric problems. However, while the discovery of such solutions caries high
scientific value, its application in practical scenarios is often prohibited by
its computational complexity growing exponentially as a function of the
dimensionality of the problem at hand. In this work, we convey a novel, general
technique that allows us to branch over an n-1 dimensional space for an
n-dimensional problem. The remaining degree of freedom can be solved globally
optimally within each bound calculation by applying the efficient interval
stabbing technique. While each individual bound derivation is harder to compute
owing to the additional need for solving a sorting problem, the reduced number
of intervals and tighter bounds in practice lead to a significant reduction in
the overall number of required iterations. Besides an abstract introduction of
the approach, we present applications to four fundamental geometric computer
vision problems: camera resectioning, relative camera pose estimation, point
set registration, and rotation and focal length estimation. Through our
exhaustive tests, we demonstrate significant speed-up factors at times
exceeding two orders of magnitude, thereby increasing the viability of globally
optimal consensus maximizers in online application scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1&quot;&gt;Liangzu Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wanting Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1&quot;&gt;Laurent Kneip&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09172">
<title>Hyperbolic Image-Text Representations. (arXiv:2304.09172v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09172</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual and linguistic concepts naturally organize themselves in a hierarchy,
where a textual concept &quot;dog&quot; entails all images that contain dogs. Despite
being intuitive, current large-scale vision and language models such as CLIP do
not explicitly capture such hierarchy. We propose MERU, a contrastive model
that yields hyperbolic representations of images and text. Hyperbolic spaces
have suitable geometric properties to embed tree-like data, so MERU can better
capture the underlying hierarchy in image-text datasets. Our results show that
MERU learns a highly interpretable and structured representation space while
being competitive with CLIP&apos;s performance on standard multi-modal tasks like
image classification and image-text retrieval. Our code and models are
available at https://www.github.com/facebookresearch/meru
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desai_K/0/1/0/all/0/1&quot;&gt;Karan Desai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nickel_M/0/1/0/all/0/1&quot;&gt;Maximilian Nickel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajpurohit_T/0/1/0/all/0/1&quot;&gt;Tanmay Rajpurohit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_J/0/1/0/all/0/1&quot;&gt;Justin Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vedantam_R/0/1/0/all/0/1&quot;&gt;Ramakrishna Vedantam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.00163">
<title>Enhancing Video Super-Resolution via Implicit Resampling-based Alignment. (arXiv:2305.00163v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.00163</link>
<description rdf:parseType="Literal">&lt;p&gt;In video super-resolution, it is common to use a frame-wise alignment to
support the propagation of information over time. The role of alignment is
well-studied for low-level enhancement in video, but existing works overlook a
critical step -- resampling. We show through extensive experiments that for
alignment to be effective, the resampling should preserve the reference
frequency spectrum while minimizing spatial distortions. However, most existing
works simply use a default choice of bilinear interpolation for resampling even
though bilinear interpolation has a smoothing effect and hinders
super-resolution. From these observations, we propose an implicit
resampling-based alignment. The sampling positions are encoded by a sinusoidal
positional encoding, while the value is estimated with a coordinate network and
a window-based cross-attention. We show that bilinear interpolation inherently
attenuates high-frequency information while an MLP-based coordinate network can
approximate more frequencies. Experiments on synthetic and real-world datasets
show that alignment with our proposed implicit resampling enhances the
performance of state-of-the-art frameworks with minimal impact on both compute
and parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_M/0/1/0/all/0/1&quot;&gt;Michael Bi Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1&quot;&gt;Angela Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07931">
<title>GSB: Group Superposition Binarization for Vision Transformer with Limited Training Samples. (arXiv:2305.07931v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07931</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformer (ViT) has performed remarkably in various computer vision
tasks. Nonetheless, affected by the massive amount of parameters, ViT usually
suffers from serious overfitting problems with a relatively limited number of
training samples. In addition, ViT generally demands heavy computing resources,
which limit its deployment on resource-constrained devices. As a type of
model-compression method, model binarization is potentially a good choice to
solve the above problems. Compared with the full-precision one, the model with
the binarization method replaces complex tensor multiplication with simple
bit-wise binary operations and represents full-precision model parameters and
activations with only 1-bit ones, which potentially solves the problem of model
size and computational complexity, respectively. In this paper, we investigate
a binarized ViT model. Empirically, we observe that the existing binarization
technology designed for Convolutional Neural Networks (CNN) cannot migrate well
to a ViT&apos;s binarization task. We also find that the decline of the accuracy of
the binary ViT model is mainly due to the information loss of the Attention
module and the Value vector. Therefore, we propose a novel model binarization
technique, called Group Superposition Binarization (GSB), to deal with these
issues. Furthermore, in order to further improve the performance of the
binarization model, we have investigated the gradient calculation procedure in
the binarization process and derived more proper gradient calculation equations
for GSB to reduce the influence of gradient mismatch. Then, the knowledge
distillation technique is introduced to alleviate the performance degradation
caused by model binarization. Analytically, model binarization can limit the
parameters search space during parameter updates while training a model....
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1&quot;&gt;Tian Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Cheng-Zhong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Le Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_H/0/1/0/all/0/1&quot;&gt;Hui Kong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00977">
<title>AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation. (arXiv:2306.00977v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00977</link>
<description rdf:parseType="Literal">&lt;p&gt;During interactive segmentation, a model and a user work together to
delineate objects of interest in a 3D point cloud. In an iterative process, the
model assigns each data point to an object (or the background), while the user
corrects errors in the resulting segmentation and feeds them back into the
model. The current best practice formulates the problem as binary
classification and segments objects one at a time. The model expects the user
to provide positive clicks to indicate regions wrongly assigned to the
background and negative clicks on regions wrongly assigned to the object.
Sequentially visiting objects is wasteful since it disregards synergies between
objects: a positive click for a given object can, by definition, serve as a
negative click for nearby objects. Moreover, a direct competition between
adjacent objects can speed up the identification of their common boundary. We
introduce AGILE3D, an efficient, attention-based model that (1) supports
simultaneous segmentation of multiple 3D objects, (2) yields more accurate
segmentation masks with fewer user clicks, and (3) offers faster inference. Our
core idea is to encode user clicks as spatial-temporal queries and enable
explicit interactions between click queries as well as between them and the 3D
scene through a click attention module. Every time new clicks are added, we
only need to run a lightweight decoder that produces updated segmentation
masks. In experiments with four different 3D point cloud datasets, AGILE3D sets
a new state-of-the-art. Moreover, we also verify its practicality in real-world
setups with real user studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yuanwen Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahadevan_S/0/1/0/all/0/1&quot;&gt;Sabarinath Mahadevan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schult_J/0/1/0/all/0/1&quot;&gt;Jonas Schult&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engelmann_F/0/1/0/all/0/1&quot;&gt;Francis Engelmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leibe_B/0/1/0/all/0/1&quot;&gt;Bastian Leibe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1&quot;&gt;Konrad Schindler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kontogianni_T/0/1/0/all/0/1&quot;&gt;Theodora Kontogianni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16003">
<title>Text-driven Talking Face Synthesis by Reprogramming Audio-driven Models. (arXiv:2306.16003v2 [cs.GR] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16003</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a method for reprogramming pre-trained audio-driven
talking face synthesis models to operate in a text-driven manner. Consequently,
we can easily generate face videos that articulate the provided textual
sentences, eliminating the necessity of recording speech for each inference, as
required in the audio-driven model. To this end, we propose to embed the input
text into the learned audio latent space of the pre-trained audio-driven model,
while preserving the face synthesis capability of the original pre-trained
model. Specifically, we devise a Text-to-Audio Embedding Module (TAEM) which
maps a given text input into the audio latent space by modeling pronunciation
and duration characteristics. Furthermore, to consider the speaker
characteristics in audio while using text inputs, TAEM is designed to accept a
visual speaker embedding. The visual speaker embedding is derived from a single
target face image and enables improved mapping of input text to the learned
audio latent space by incorporating the speaker characteristics inherent in the
audio. The main advantages of the proposed framework are that 1) it can be
applied to diverse audio-driven talking face synthesis models and 2) we can
generate talking face videos with either text inputs or audio inputs with high
flexibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jeongsoo Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Se Jin Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1&quot;&gt;Yong Man Ro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07142">
<title>Quantity-Aware Coarse-to-Fine Correspondence for Image-to-Point Cloud Registration. (arXiv:2307.07142v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07142</link>
<description rdf:parseType="Literal">&lt;p&gt;Image-to-point cloud registration aims to determine the relative camera pose
between an RGB image and a reference point cloud, serving as a general solution
for locating 3D objects from 2D observations. Matching individual points with
pixels can be inherently ambiguous due to modality gaps. To address this
challenge, we propose a framework to capture quantity-aware correspondences
between local point sets and pixel patches and refine the results at both the
point and pixel levels. This framework aligns the high-level semantics of point
sets and pixel patches to improve the matching accuracy. On a coarse scale, the
set-to-patch correspondence is expected to be influenced by the quantity of 3D
points. To achieve this, a novel supervision strategy is proposed to adaptively
quantify the degrees of correlation as continuous values. On a finer scale,
point-to-pixel correspondences are refined from a smaller search space through
a well-designed scheme, which incorporates both resampling and quantity-aware
priors. Particularly, a confidence sorting strategy is proposed to
proportionally select better correspondences at the final stage. Leveraging the
advantages of high-quality correspondences, the problem is successfully
resolved using an efficient Perspective-n-Point solver within the framework of
random sample consensus (RANSAC). Extensive experiments on the KITTI Odometry
and NuScenes datasets demonstrate the superiority of our method over the
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_G/0/1/0/all/0/1&quot;&gt;Gongxin Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuan_Y/0/1/0/all/0/1&quot;&gt;Yixin Xuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiwei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yu Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03200">
<title>Uncovering local aggregated air quality index with smartphone captured images leveraging efficient deep convolutional neural network. (arXiv:2308.03200v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03200</link>
<description rdf:parseType="Literal">&lt;p&gt;The prevalence and mobility of smartphones make these a widely used tool for
environmental health research. However, their potential for determining
aggregated air quality index (AQI) based on PM2.5 concentration in specific
locations remains largely unexplored in the existing literature. In this paper,
we thoroughly examine the challenges associated with predicting
location-specific PM2.5 concentration using images taken with smartphone
cameras. The focus of our study is on Dhaka, the capital of Bangladesh, due to
its significant air pollution levels and the large population exposed to it.
Our research involves the development of a Deep Convolutional Neural Network
(DCNN), which we train using over a thousand outdoor images taken and
annotated. These photos are captured at various locations in Dhaka, and their
labels are based on PM2.5 concentration data obtained from the local US
consulate, calculated using the NowCast algorithm. Through supervised learning,
our model establishes a correlation index during training, enhancing its
ability to function as a Picture-based Predictor of PM2.5 Concentration (PPPC).
This enables the algorithm to calculate an equivalent daily averaged AQI index
from a smartphone image. Unlike, popular overly parameterized models, our model
shows resource efficiency since it uses fewer parameters. Furthermore, test
results indicate that our model outperforms popular models like ViT and INN, as
well as popular CNN-based models such as VGG19, ResNet50, and MobileNetV2, in
predicting location-specific PM2.5 concentration. Our dataset is the first
publicly available collection that includes atmospheric images and
corresponding PM2.5 measurements from Dhaka. Our codes and dataset are
available at https://github.com/lepotatoguy/aqi.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondal_J/0/1/0/all/0/1&quot;&gt;Joyanta Jyoti Mondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Md. Farhadul Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_R/0/1/0/all/0/1&quot;&gt;Raima Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rhidi_N/0/1/0/all/0/1&quot;&gt;Nowsin Kabir Rhidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Newaz_S/0/1/0/all/0/1&quot;&gt;Sarfaraz Newaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manab_M/0/1/0/all/0/1&quot;&gt;Meem Arafat Manab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_A/0/1/0/all/0/1&quot;&gt;A. B. M. Alim Al Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noor_J/0/1/0/all/0/1&quot;&gt;Jannatun Noor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05021">
<title>On Error Propagation of Diffusion Models. (arXiv:2308.05021v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.05021</link>
<description rdf:parseType="Literal">&lt;p&gt;Although diffusion models (DMs) have shown promising performances in a number
of tasks (e.g., speech synthesis and image generation), they might suffer from
error propagation because of their sequential structure. However, this is not
certain because some sequential models, such as Conditional Random Field (CRF),
are free from this problem. To address this issue, we develop a theoretical
framework to mathematically formulate error propagation in the architecture of
DMs, The framework contains three elements, including modular error, cumulative
error, and propagation equation. The modular and cumulative errors are related
by the equation, which interprets that DMs are indeed affected by error
propagation. Our theoretical study also suggests that the cumulative error is
closely related to the generation quality of DMs. Based on this finding, we
apply the cumulative error as a regularization term to reduce error
propagation. Because the term is computationally intractable, we derive its
upper bound and design a bootstrap algorithm to efficiently estimate the bound
for optimization. We have conducted extensive experiments on multiple image
datasets, showing that our proposed regularization reduces error propagation,
significantly improves vanilla DMs, and outperforms previous baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yangming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela van der Schaar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11164">
<title>Decoupled Contrastive Multi-View Clustering with High-Order Random Walks. (arXiv:2308.11164v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11164</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent, some robust contrastive multi-view clustering (MvC) methods have
been proposed, which construct data pairs from neighborhoods to alleviate the
false negative issue, i.e., some intra-cluster samples are wrongly treated as
negative pairs. Although promising performance has been achieved by these
methods, the false negative issue is still far from addressed and the false
positive issue emerges because all in- and out-of-neighborhood samples are
simply treated as positive and negative, respectively. To address the issues,
we propose a novel robust method, dubbed decoupled contrastive multi-view
clustering with high-order random walks (DIVIDE). In brief, DIVIDE leverages
random walks to progressively identify data pairs in a global instead of local
manner. As a result, DIVIDE could identify in-neighborhood negatives and
out-of-neighborhood positives. Moreover, DIVIDE embraces a novel MvC
architecture to perform inter- and intra-view contrastive learning in different
embedding spaces, thus boosting clustering performance and embracing the
robustness against missing views. To verify the efficacy of DIVIDE, we carry
out extensive experiments on four benchmark datasets comparing with nine
state-of-the-art MvC methods in both complete and incomplete MvC settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yiding Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yijie Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mouxing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1&quot;&gt;Dezhong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1&quot;&gt;Peng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xi Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11918">
<title>AMSP-UOD: When Vortex Convolution and Stochastic Perturbation Meet Underwater Object Detection. (arXiv:2308.11918v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11918</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a novel Amplitude-Modulated Stochastic Perturbation
and Vortex Convolutional Network, AMSP-UOD, designed for underwater object
detection. AMSP-UOD specifically addresses the impact of non-ideal imaging
factors on detection accuracy in complex underwater environments. To mitigate
the influence of noise on object detection performance, we propose AMSP Vortex
Convolution (AMSP-VConv) to disrupt the noise distribution, enhance feature
extraction capabilities, effectively reduce parameters, and improve network
robustness. We design the Feature Association Decoupling Cross Stage Partial
(FAD-CSP) module, which strengthens the association of long and short range
features, improving the network performance in complex underwater environments.
Additionally, our sophisticated post-processing method, based on Non-Maximum
Suppression (NMS) with aspect-ratio similarity thresholds, optimizes detection
in dense scenes, such as waterweed and schools of fish, improving object
detection accuracy. Extensive experiments on the URPC and RUOD datasets
demonstrate that our method outperforms existing state-of-the-art methods in
terms of accuracy and noise immunity. AMSP-UOD proposes an innovative solution
with the potential for real-world applications. Our code is available at
https://github.com/zhoujingchun03/AMSP-UOD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingchun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zongxin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1&quot;&gt;Kin-Man Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yudong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weishi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;ChunLe Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongyi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11932">
<title>Synergistic Multiscale Detail Refinement via Intrinsic Supervision for Underwater Image Enhancement. (arXiv:2308.11932v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11932</link>
<description rdf:parseType="Literal">&lt;p&gt;Visually restoring underwater scenes primarily involves mitigating
interference from underwater media. Existing methods ignore the inherent
scale-related characteristics in underwater scenes. Therefore, we present the
synergistic multi-scale detail refinement via intrinsic supervision (SMDR-IS)
for enhancing underwater scene details, which contain multi-stages. The
low-degradation stage from the original images furnishes the original stage
with multi-scale details, achieved through feature propagation using the
Adaptive Selective Intrinsic Supervised Feature (ASISF) module. By using
intrinsic supervision, the ASISF module can precisely control and guide feature
transmission across multi-degradation stages, enhancing multi-scale detail
refinement and minimizing the interference from irrelevant information in the
low-degradation stage. In multi-degradation encoder-decoder framework of
SMDR-IS, we introduce the Bifocal Intrinsic-Context Attention Module (BICA).
Based on the intrinsic supervision principles, BICA efficiently exploits
multi-scale scene information in images. BICA directs higher-resolution spaces
by tapping into the insights of lower-resolution ones, underscoring the pivotal
role of spatial contextual relationships in underwater image restoration.
Throughout training, the inclusion of a multi-degradation loss function can
enhance the network, allowing it to adeptly extract information across diverse
scales. When benchmarked against state-of-the-art methods, SMDR-IS consistently
showcases superior performance. The code is publicly available at:
https://github.com/zhoujingchun03/SMDR-IS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dehuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingchun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;ChunLe Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weishi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongyi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16573">
<title>Dual-Decoder Consistency via Pseudo-Labels Guided Data Augmentation for Semi-Supervised Medical Image Segmentation. (arXiv:2308.16573v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16573</link>
<description rdf:parseType="Literal">&lt;p&gt;While supervised learning has achieved remarkable success, obtaining
large-scale labeled datasets in biomedical imaging is often impractical due to
high costs and the time-consuming annotations required from radiologists.
Semi-supervised learning emerges as an effective strategy to overcome this
limitation by leveraging useful information from unlabeled datasets. In this
paper, we present a novel semi-supervised learning method, Dual-Decoder
Consistency via Pseudo-Labels Guided Data Augmentation (DCPA), for medical
image segmentation. We devise a consistency regularization to promote
consistent representations during the training process. Specifically, we use
distinct decoders for student and teacher networks while maintain the same
encoder. Moreover, to learn from unlabeled data, we create pseudo-labels
generated by the teacher networks and augment the training data with the
pseudo-labels. Both techniques contribute to enhancing the performance of the
proposed method. The method is evaluated on three representative medical image
segmentation datasets. Comprehensive comparisons with state-of-the-art
semi-supervised medical image segmentation methods were conducted under typical
scenarios, utilizing 10% and 20% labeled data, as well as in the extreme
scenario of only 5% labeled data. The experimental results consistently
demonstrate the superior performance of our method compared to other methods
across the three semi-supervised settings. The source code is publicly
available at https://github.com/BinYCn/DCPA.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuanbin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hui Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Longxuan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zong_R/0/1/0/all/0/1&quot;&gt;Ruige Zong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tan_T/0/1/0/all/0/1&quot;&gt;Tao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinlin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tong_T/0/1/0/all/0/1&quot;&gt;Tong Tong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02119">
<title>Hierarchical Masked 3D Diffusion Model for Video Outpainting. (arXiv:2309.02119v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02119</link>
<description rdf:parseType="Literal">&lt;p&gt;Video outpainting aims to adequately complete missing areas at the edges of
video frames. Compared to image outpainting, it presents an additional
challenge as the model should maintain the temporal consistency of the filled
area. In this paper, we introduce a masked 3D diffusion model for video
outpainting. We use the technique of mask modeling to train the 3D diffusion
model. This allows us to use multiple guide frames to connect the results of
multiple video clip inferences, thus ensuring temporal consistency and reducing
jitter between adjacent frames. Meanwhile, we extract the global frames of the
video as prompts and guide the model to obtain information other than the
current video clip using cross-attention. We also introduce a hybrid
coarse-to-fine inference pipeline to alleviate the artifact accumulation
problem. The existing coarse-to-fine pipeline only uses the infilling strategy,
which brings degradation because the time interval of the sparse frames is too
large. Our pipeline benefits from bidirectional learning of the mask modeling
and thus can employ a hybrid strategy of infilling and interpolation when
generating sparse frames. Experiments show that our method achieves
state-of-the-art results in video outpainting tasks. More results and codes are
provided at our https://fanfanda.github.io/M3DDM/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1&quot;&gt;Fanda Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Chaoxu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_L/0/1/0/all/0/1&quot;&gt;Litong Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Biao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1&quot;&gt;Tiezheng Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yuning Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chunjie Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1&quot;&gt;Jianfeng Zhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.04780">
<title>Latent Degradation Representation Constraint for Single Image Deraining. (arXiv:2309.04780v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.04780</link>
<description rdf:parseType="Literal">&lt;p&gt;Since rain streaks show a variety of shapes and directions, learning the
degradation representation is extremely challenging for single image deraining.
Existing methods are mainly targeted at designing complicated modules to
implicitly learn latent degradation representation from coupled rainy images.
This way, it is hard to decouple the content-independent degradation
representation due to the lack of explicit constraint, resulting in over- or
under-enhancement problems. To tackle this issue, we propose a novel Latent
Degradation Representation Constraint Network (LDRCNet) that consists of
Direction-Aware Encoder (DAEncoder), UNet Deraining Network, and Multi-Scale
Interaction Block (MSIBlock). Specifically, the DAEncoder is proposed to
adaptively extract latent degradation representation by using the deformable
convolutions to exploit the direction consistency of rain streaks. Next, a
constraint loss is introduced to explicitly constraint the degradation
representation learning during training. Last, we propose an MSIBlock to fuse
with the learned degradation representation and decoder features of the
deraining network for adaptive information interaction, which enables the
deraining network to remove various complicated rainy patterns and reconstruct
image details. Experimental results on synthetic and real datasets demonstrate
that our method achieves new state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuhong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1&quot;&gt;Long Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jun Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05448">
<title>Panoptic Vision-Language Feature Fields. (arXiv:2309.05448v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05448</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, methods have been proposed for 3D open-vocabulary semantic
segmentation. Such methods are able to segment scenes into arbitrary classes
based on text descriptions provided during runtime. In this paper, we propose
to the best of our knowledge the first algorithm for open-vocabulary panoptic
segmentation in 3D scenes. Our algorithm, Panoptic Vision-Language Feature
Fields (PVLFF), learns a semantic feature field of the scene by distilling
vision-language features from a pretrained 2D model, and jointly fits an
instance feature field through contrastive learning using 2D instance segments
on input frames. Despite not being trained on the target classes, our method
achieves panoptic segmentation performance similar to the state-of-the-art
closed-set 3D systems on the HyperSim, ScanNet and Replica dataset and
additionally outperforms current 3D open-vocabulary systems in terms of
semantic segmentation. We ablate the components of our method to demonstrate
the effectiveness of our model architecture. Our code will be available at
https://github.com/ethz-asl/pvlff.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haoran Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blomqvist_K/0/1/0/all/0/1&quot;&gt;Kenneth Blomqvist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milano_F/0/1/0/all/0/1&quot;&gt;Francesco Milano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siegwart_R/0/1/0/all/0/1&quot;&gt;Roland Siegwart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07778">
<title>Virchow: A Million-Slide Digital Pathology Foundation Model. (arXiv:2309.07778v5 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07778</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of artificial intelligence to enable precision medicine and decision
support systems through the analysis of pathology images has the potential to
revolutionize the diagnosis and treatment of cancer. Such applications will
depend on models&apos; abilities to capture the diverse patterns observed in
pathology images. To address this challenge, we present Virchow, a foundation
model for computational pathology. Using self-supervised learning empowered by
the DINOv2 algorithm, Virchow is a vision transformer model with 632 million
parameters trained on 1.5 million hematoxylin and eosin stained whole slide
images from diverse tissue and specimen types, which is orders of magnitude
more data than previous works. The Virchow model enables the development of a
pan-cancer detection system with 0.949 overall specimen-level AUC across 17
different cancer types, while also achieving 0.937 AUC on 7 rare cancer types.
The Virchow model sets the state-of-the-art on the internal and external image
tile level benchmarks and slide level biomarker prediction tasks. The gains in
performance highlight the importance of training on massive pathology image
datasets, suggesting scaling up the data and network architecture can improve
the accuracy for many high-impact computational pathology applications where
limited amounts of training data are available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vorontsov_E/0/1/0/all/0/1&quot;&gt;Eugene Vorontsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bozkurt_A/0/1/0/all/0/1&quot;&gt;Alican Bozkurt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Casson_A/0/1/0/all/0/1&quot;&gt;Adam Casson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shaikovski_G/0/1/0/all/0/1&quot;&gt;George Shaikovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zelechowski_M/0/1/0/all/0/1&quot;&gt;Michal Zelechowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Siqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Severson_K/0/1/0/all/0/1&quot;&gt;Kristen Severson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zimmermann_E/0/1/0/all/0/1&quot;&gt;Eric Zimmermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hall_J/0/1/0/all/0/1&quot;&gt;James Hall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tenenholtz_N/0/1/0/all/0/1&quot;&gt;Neil Tenenholtz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fusi_N/0/1/0/all/0/1&quot;&gt;Nicolo Fusi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mathieu_P/0/1/0/all/0/1&quot;&gt;Philippe Mathieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eck_A/0/1/0/all/0/1&quot;&gt;Alexander van Eck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Donghun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Viret_J/0/1/0/all/0/1&quot;&gt;Julian Viret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Robert_E/0/1/0/all/0/1&quot;&gt;Eric Robert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yi Kan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kunz_J/0/1/0/all/0/1&quot;&gt;Jeremy D. Kunz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Matthew C. H. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bernhard_J/0/1/0/all/0/1&quot;&gt;Jan Bernhard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Godrich_R/0/1/0/all/0/1&quot;&gt;Ran A. Godrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oakley_G/0/1/0/all/0/1&quot;&gt;Gerard Oakley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Millar_E/0/1/0/all/0/1&quot;&gt;Ewan Millar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hanna_M/0/1/0/all/0/1&quot;&gt;Matthew Hanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Retamero_J/0/1/0/all/0/1&quot;&gt;Juan Retamero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moye_W/0/1/0/all/0/1&quot;&gt;William A. Moye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yousfi_R/0/1/0/all/0/1&quot;&gt;Razik Yousfi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kanan_C/0/1/0/all/0/1&quot;&gt;Christopher Kanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Klimstra_D/0/1/0/all/0/1&quot;&gt;David Klimstra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rothrock_B/0/1/0/all/0/1&quot;&gt;Brandon Rothrock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fuchs_T/0/1/0/all/0/1&quot;&gt;Thomas J. Fuchs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14068">
<title>Soft Mixture Denoising: Beyond the Expressive Bottleneck of Diffusion Models. (arXiv:2309.14068v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14068</link>
<description rdf:parseType="Literal">&lt;p&gt;Because diffusion models have shown impressive performances in a number of
tasks, such as image synthesis, there is a trend in recent works to prove (with
certain assumptions) that these models have strong approximation capabilities.
In this paper, we show that current diffusion models actually have an
expressive bottleneck in backward denoising and some assumption made by
existing theoretical guarantees is too strong. Based on this finding, we prove
that diffusion models have unbounded errors in both local and global denoising.
In light of our theoretical studies, we introduce soft mixture denoising (SMD),
an expressive and efficient model for backward denoising. SMD not only permits
diffusion models to well approximate any Gaussian mixture distributions in
theory, but also is simple and efficient for implementation. Our experiments on
multiple image datasets show that SMD significantly improves different types of
diffusion models (e.g., DDPM), espeically in the situation of few backward
iterations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yangming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breugel_B/0/1/0/all/0/1&quot;&gt;Boris van Breugel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela van der Schaar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17189">
<title>RTFS-Net: Recurrent time-frequency modelling for efficient audio-visual speech separation. (arXiv:2309.17189v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17189</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio-visual speech separation methods aim to integrate different modalities
to generate high-quality separated speech, thereby enhancing the performance of
downstream tasks such as speech recognition. Most existing state-of-the-art
(SOTA) models operate in the time domain. However, their overly simplistic
approach to modeling acoustic features often necessitates larger and more
computationally intensive models in order to achieve SOTA performance. In this
paper, we present a novel time-frequency domain audio-visual speech separation
method: Recurrent Time-Frequency Separation Network (RTFS-Net), which applies
its algorithms on the complex time-frequency bins yielded by the Short-Time
Fourier Transform. We model and capture the time and frequency dimensions of
the audio independently using a multi-layered RNN along each dimension.
Furthermore, we introduce a unique attention-based fusion technique for the
efficient integration of audio and visual information, and a new mask
separation approach that takes advantage of the intrinsic spectral nature of
the acoustic features for a clearer separation. RTFS-Net outperforms the
previous SOTA method using only 10% of the parameters and 18% of the MACs. This
is the first time-frequency domain audio-visual speech separation method to
outperform all contemporary time-domain counterparts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pegg_S/0/1/0/all/0/1&quot;&gt;Samuel Pegg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaolin Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08587">
<title>Pseudo-Generalized Dynamic View Synthesis from a Video. (arXiv:2310.08587v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08587</link>
<description rdf:parseType="Literal">&lt;p&gt;Rendering scenes observed in a monocular video from novel viewpoints is a
challenging problem. For static scenes the community has studied both
scene-specific optimization techniques, which optimize on every test scene, and
generalized techniques, which only run a deep net forward pass on a test scene.
In contrast, for dynamic scenes, scene-specific optimization techniques exist,
but, to our best knowledge, there is currently no generalized method for
dynamic novel view synthesis from a given monocular video. To answer whether
generalized dynamic novel view synthesis from monocular videos is possible
today, we establish an analysis framework based on existing techniques and work
toward the generalized approach. We find a pseudo-generalized process without
scene-specific appearance optimization is possible, but geometrically and
temporally consistent depth estimates are needed. Despite no scene-specific
appearance optimization, the pseudo-generalized approach improves upon some
scene-specific methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiaoming Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colburn_A/0/1/0/all/0/1&quot;&gt;Alex Colburn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1&quot;&gt;Fangchang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bautista_M/0/1/0/all/0/1&quot;&gt;Miguel Angel Bautista&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1&quot;&gt;Joshua M. Susskind&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1&quot;&gt;Alexander G. Schwing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09760">
<title>GPT-Prompt Controlled Diffusion for Weakly-Supervised Semantic Segmentation. (arXiv:2310.09760v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09760</link>
<description rdf:parseType="Literal">&lt;p&gt;Weakly supervised semantic segmentation (WSSS), aiming to train segmentation
models solely using image-level labels, has received significant attention.
Existing approaches mainly concentrate on creating high-quality pseudo labels
by utilizing existing images and their corresponding image-level labels.
However, the quality of pseudo labels degrades significantly when the size of
available dataset is limited. Thus, in this paper, we tackle this problem from
a different view by introducing a novel approach called GPT-Prompt Controlled
Diffusion (GPCD) for data augmentation. This approach enhances the current
labeled datasets by augmenting with a variety of images, achieved through
controlled diffusion guided by GPT prompts. In this process, the existing
images and image-level labels provide the necessary control information, where
GPT is employed to enrich the prompts, leading to the generation of diverse
backgrounds. Moreover, we integrate data source information as tokens into the
Vision Transformer (ViT) framework. These tokens are specifically designed to
improve the ability of downstream WSSS framework to recognize the origins of
augmented images. Our proposed GPCD approach clearly surpasses existing
state-of-the-art methods. This effect is more obvious when the amount of
available data is small, demonstrating the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wangyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1&quot;&gt;Tianhong Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1&quot;&gt;Fei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jimin Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10642">
<title>Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting. (arXiv:2310.10642v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10642</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing dynamic 3D scenes from 2D images and generating diverse views
over time is challenging due to scene complexity and temporal dynamics. Despite
advancements in neural implicit models, limitations persist: (i) Inadequate
Scene Structure: Existing methods struggle to reveal the spatial and temporal
structure of dynamic scenes from directly learning the complex 6D plenoptic
function. (ii) Scaling Deformation Modeling: Explicitly modeling scene element
deformation becomes impractical for complex dynamics. To address these issues,
we consider the spacetime as an entirety and propose to approximate the
underlying spatio-temporal 4D volume of a dynamic scene by optimizing a
collection of 4D primitives, with explicit geometry and appearance modeling.
Learning to optimize the 4D primitives enables us to synthesize novel views at
any desired time with our tailored rendering routine. Our model is conceptually
simple, consisting of a 4D Gaussian parameterized by anisotropic ellipses that
can rotate arbitrarily in space and time, as well as view-dependent and
time-evolved appearance represented by the coefficient of 4D spherindrical
harmonics. This approach offers simplicity, flexibility for variable-length
video and end-to-end training, and efficient real-time rendering, making it
suitable for capturing complex dynamic scene motions. Experiments across
various benchmarks, including monocular and multi-view scenarios, demonstrate
our 4DGS model&apos;s superior visual quality and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hongye Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zijie Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12086">
<title>FactCHD: Benchmarking Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12086</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite their impressive generative capabilities, LLMs are hindered by
fact-conflicting hallucinations in real-world applications. The accurate
identification of hallucinations in texts generated by LLMs, especially in
complex inferential scenarios, is a relatively unexplored area. To address this
gap, we present FactCHD, a dedicated benchmark designed for the detection of
fact-conflicting hallucinations from LLMs. FactCHD features a diverse dataset
that spans various factuality patterns, including vanilla, multi-hop,
comparison, and set operation. A distinctive element of FactCHD is its
integration of fact-based evidence chains, significantly enhancing the depth of
evaluating the detectors&apos; explanations. Experiments on different LLMs expose
the shortcomings of current approaches in detecting factual errors accurately.
Furthermore, we introduce Truth-Triangulator that synthesizes reflective
considerations by tool-enhanced ChatGPT and LoRA-tuning based on Llama2, aiming
to yield more credible detection through the amalgamation of predictive results
and evidence. The benchmark dataset is available at
https://github.com/zjunlp/FactCHD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Duanzheng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_H/0/1/0/all/0/1&quot;&gt;Honghao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chenxi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yong_J/0/1/0/all/0/1&quot;&gt;Jiang Yong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1&quot;&gt;Chengfei Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12474">
<title>Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping. (arXiv:2310.12474v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12474</link>
<description rdf:parseType="Literal">&lt;p&gt;High-resolution 3D object generation remains a challenging task primarily due
to the limited availability of comprehensive annotated training data. Recent
advancements have aimed to overcome this constraint by harnessing image
generative models, pretrained on extensive curated web datasets, using
knowledge transfer techniques like Score Distillation Sampling (SDS).
Efficiently addressing the requirements of high-resolution rendering often
necessitates the adoption of latent representation-based models, such as the
Latent Diffusion Model (LDM). In this framework, a significant challenge
arises: To compute gradients for individual image pixels, it is necessary to
backpropagate gradients from the designated latent space through the frozen
components of the image model, such as the VAE encoder used within LDM.
However, this gradient propagation pathway has never been optimized, remaining
uncontrolled during training. We find that the unregulated gradients adversely
affect the 3D model&apos;s capacity in acquiring texture-related information from
the image generative model, leading to poor quality appearance synthesis. To
address this overarching challenge, we propose an innovative operation termed
Pixel-wise Gradient Clipping (PGC) designed for seamless integration into
existing 3D generative models, thereby enhancing their synthesis quality.
Specifically, we control the magnitude of stochastic gradients by clipping the
pixel-wise gradients efficiently, while preserving crucial texture-related
gradient directions. Despite this simplicity and minimal extra cost, extensive
experiments demonstrate the efficacy of our PGC in enhancing the performance of
existing 3D generative models for high-resolution object rendering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zijie Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiachen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15646">
<title>Mean Teacher DETR with Masked Feature Alignment: A Robust Domain Adaptive Detection Transformer Framework. (arXiv:2310.15646v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15646</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain adaptation object detection (UDAOD) research on Detection
Transformer(DETR) mainly focuses on feature alignment and existing methods can
be divided into two kinds, each of which has its unresolved issues. One-stage
feature alignment methods can easily lead to performance fluctuation and
training stagnation. Two-stage feature alignment method based on mean teacher
comprises a pretraining stage followed by a self-training stage, each facing
problems in obtaining reliable pretrained model and achieving consistent
performance gains. Methods mentioned above have not yet explore how to utilize
the third related domain such as target-like domain to assist adaptation. To
address these issues, we propose a two-stage framework named MTM, i.e. Mean
Teacher-DETR with Masked Feature Alignment. In the pretraining stage, we
utilize labeled target-like images produced by image style transfer to avoid
performance fluctuation. In the self-training stage, we leverage unlabeled
target images by pseudo labels based on mean teacher and propose a module
called Object Queries Knowledge Transfer (OQKT) to ensure consistent
performance gains of the student model. Most importantly, we propose masked
feature alignment methods including Masked Domain Query-based Feature Alignment
(MDQFA) and Masked Token-wise Feature Alignment (MTWFA) to alleviate domain
shift in a more robust way, which not only prevent training stagnation and lead
to a robust pretrained model in the pretraining stage, but also enhance the
model&apos;s target performance in the self-training stage. Experiments on three
challenging scenarios and a theoretical analysis verify the effectiveness of
MTM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_W/0/1/0/all/0/1&quot;&gt;Weixi Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1&quot;&gt;Chun Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17212">
<title>Affective Video Content Analysis: Decade Review and New Perspectives. (arXiv:2310.17212v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17212</link>
<description rdf:parseType="Literal">&lt;p&gt;Video content is rich in semantics and has the ability to evoke various
emotions in viewers. In recent years, with the rapid development of affective
computing and the explosive growth of visual data, affective video content
analysis (AVCA) as an essential branch of affective computing has become a
widely researched topic. In this study, we comprehensively review the
development of AVCA over the past decade, particularly focusing on the most
advanced methods adopted to address the three major challenges of video feature
extraction, expression subjectivity, and multimodal feature fusion. We first
introduce the widely used emotion representation models in AVCA and describe
commonly used datasets. We summarize and compare representative methods in the
following aspects: (1) unimodal AVCA models, including facial expression
recognition and posture emotion recognition; (2) multimodal AVCA models,
including feature fusion, decision fusion, and attention-based multimodal
models; (3) model performance evaluation standards. Finally, we discuss future
challenges and promising research directions, such as emotion recognition and
public opinion analysis, human-computer interaction, and emotional
intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1&quot;&gt;Junxiao Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xuecheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qian Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19776">
<title>Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery. (arXiv:2310.19776v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19776</link>
<description rdf:parseType="Literal">&lt;p&gt;In the quest for unveiling novel categories at test time, we confront the
inherent limitations of traditional supervised recognition models that are
restricted by a predefined category set. While strides have been made in the
realms of self-supervised and open-world learning towards test-time category
discovery, a crucial yet often overlooked question persists: what exactly
delineates a category? In this paper, we conceptualize a category through the
lens of optimization, viewing it as an optimal solution to a well-defined
problem. Harnessing this unique conceptualization, we propose a novel,
efficient and self-supervised method capable of discovering previously unknown
categories at test time. A salient feature of our approach is the assignment of
minimum length category codes to individual data instances, which encapsulates
the implicit category hierarchy prevalent in real-world datasets. This
mechanism affords us enhanced control over category granularity, thereby
equipping our model to handle fine-grained categories adeptly. Experimental
evaluations, bolstered by state-of-the-art benchmark comparisons, testify to
the efficacy of our solution in managing unknown categories at test time.
Furthermore, we fortify our proposition with a theoretical foundation,
providing proof of its optimality. Our code is available at
https://github.com/SarahRastegar/InfoSieve.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastegar_S/0/1/0/all/0/1&quot;&gt;Sarah Rastegar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doughty_H/0/1/0/all/0/1&quot;&gt;Hazel Doughty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1&quot;&gt;Cees G. M. Snoek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04938">
<title>Improved DDIM Sampling with Moment Matching Gaussian Mixtures. (arXiv:2311.04938v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04938</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose using a Gaussian Mixture Model (GMM) as reverse transition
operator (kernel) within the Denoising Diffusion Implicit Models (DDIM)
framework, which is one of the most widely used approaches for accelerated
sampling from pre-trained Denoising Diffusion Probabilistic Models (DDPM).
Specifically we match the first and second order central moments of the DDPM
forward marginals by constraining the parameters of the GMM. We see that moment
matching is sufficient to obtain samples with equal or better quality than the
original DDIM with Gaussian kernels. We provide experimental results with
unconditional models trained on CelebAHQ and FFHQ and class-conditional models
trained on ImageNet datasets respectively. Our results suggest that using the
GMM kernel leads to significant improvements in the quality of the generated
samples when the number of sampling steps is small, as measured by FID and IS
metrics. For example on ImageNet 256x256, using 10 sampling steps, we achieve a
FID of 6.94 and IS of 207.85 with a GMM kernel compared to 10.15 and 196.73
respectively with a Gaussian kernel.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabbur_P/0/1/0/all/0/1&quot;&gt;Prasad Gabbur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15111">
<title>UAE: Universal Anatomical Embedding on Multi-modality Medical Images. (arXiv:2311.15111v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15111</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying specific anatomical structures (\textit{e.g.}, lesions or
landmarks) in medical images plays a fundamental role in medical image
analysis. Exemplar-based landmark detection methods are receiving increasing
attention since they can detect arbitrary anatomical points in inference while
do not need landmark annotations in training. They use self-supervised learning
to acquire a discriminative embedding for each voxel within the image. These
approaches can identify corresponding landmarks through nearest neighbor
matching and has demonstrated promising results across various tasks. However,
current methods still face challenges in: (1) differentiating voxels with
similar appearance but different semantic meanings (\textit{e.g.}, two adjacent
structures without clear borders); (2) matching voxels with similar semantics
but markedly different appearance (\textit{e.g.}, the same vessel before and
after contrast injection); and (3) cross-modality matching (\textit{e.g.},
CT-MRI landmark-based registration). To overcome these challenges, we propose
universal anatomical embedding (UAE), which is a unified framework designed to
learn appearance, semantic, and cross-modality anatomical embeddings.
Specifically, UAE incorporates three key innovations: (1) semantic embedding
learning with prototypical contrastive loss; (2) a fixed-point-based matching
strategy; and (3) an iterative approach for cross-modality embedding learning.
We thoroughly evaluated UAE across intra- and inter-modality tasks, including
one-shot landmark detection, lesion tracking on longitudinal CT scans, and
CT-MRI affine/rigid registration with varying field of view. Our results
suggest that UAE outperforms state-of-the-art methods, offering a robust and
versatile approach for landmark based medical image analysis tasks. Code and
trained models are available at: \href{https://shorturl.at/bgsB3}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_F/0/1/0/all/0/1&quot;&gt;Fan Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_X/0/1/0/all/0/1&quot;&gt;Xiaofei Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1&quot;&gt;Jia Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jingjing Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xianghua Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Ke Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yong Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18243">
<title>DKiS: Decay weight invertible image steganography with private key. (arXiv:2311.18243v2 [cs.MM] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18243</link>
<description rdf:parseType="Literal">&lt;p&gt;Image steganography, defined as the practice of concealing information within
another image, traditionally encounters security challenges when its methods
become publicly known or are under attack. To address this, a novel private
key-based image steganography technique has been introduced. This approach
ensures the security of the hidden information, as access requires a
corresponding private key, regardless of the public knowledge of the
steganography method. Experimental evidence has been presented, demonstrating
the effectiveness of our method and showcasing its real-world applicability.
Furthermore, a critical challenge in the invertible image steganography process
has been identified by us: the transfer of non-essential, or `garbage&apos;,
information from the secret to the host pipeline. To tackle this issue, the
decay weight has been introduced to control the information transfer,
effectively filtering out irrelevant data and enhancing the performance of
image steganography. The code for this technique is publicly accessible at
https://github.com/yanghangAI/DKiS, and a practical demonstration can be found
at &lt;a href=&quot;http://yanghang.site/hidekey.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yitian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuhua Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02116">
<title>GIVT: Generative Infinite-Vocabulary Transformers. (arXiv:2312.02116v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02116</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce generative infinite-vocabulary transformers (GIVT) which
generate vector sequences with real-valued entries, instead of discrete tokens
from a finite vocabulary. To this end, we propose two surprisingly simple
modifications to decoder-only transformers: 1) at the input, we replace the
finite-vocabulary lookup table with a linear projection of the input vectors;
and 2) at the output, we replace the logits prediction (usually mapped to a
categorical distribution) with the parameters of a multivariate Gaussian
mixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT,
where transformers are used to model the discrete latent sequences of a VQ-VAE,
we use GIVT to model the unquantized real-valued latent sequences of a VAE.
When applying GIVT to class-conditional image generation with iterative masked
modeling, we show competitive results with MaskGIT, while our approach
outperforms both VQ-GAN and MaskGIT when using it for causal modeling. Finally,
we obtain competitive results outside of image generation when applying our
approach to panoptic segmentation and depth estimation with a VAE-based variant
of the UViM framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tschannen_M/0/1/0/all/0/1&quot;&gt;Michael Tschannen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eastwood_C/0/1/0/all/0/1&quot;&gt;Cian Eastwood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mentzer_F/0/1/0/all/0/1&quot;&gt;Fabian Mentzer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12379">
<title>Mixture of Cluster-conditional LoRA Experts for Vision-language Instruction Tuning. (arXiv:2312.12379v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12379</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction tuning of the Large Vision-language Models (LVLMs) has
revolutionized the development of versatile models with zero-shot
generalization across a wide range of downstream vision-language tasks.
However, diversity of training tasks of different sources and formats would
lead to inevitable task conflicts, where different tasks conflicts for the same
set of model parameters, resulting in sub-optimal instruction-following
abilities. To address that, we propose the Mixture of Cluster-conditional LoRA
Experts (MoCLE), a novel Mixture of Experts (MoE) architecture designed to
activate the task-customized model parameters based on the instruction
clusters. A separate universal expert is further incorporated to improve the
generalization capabilities of MoCLE for novel instructions. Extensive
experiments on 10 zero-shot tasks demonstrate the effectiveness of MoCLE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gou_Y/0/1/0/all/0/1&quot;&gt;Yunhao Gou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhili Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1&quot;&gt;Lanqing Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Aoxue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1&quot;&gt;Dit-Yan Yeung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1&quot;&gt;James T. Kwok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12838">
<title>FedA3I: Annotation Quality-Aware Aggregation for Federated Medical Image Segmentation against Heterogeneous Annotation Noise. (arXiv:2312.12838v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12838</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) has emerged as a promising paradigm for training
segmentation models on decentralized medical data, owing to its
privacy-preserving property. However, existing research overlooks the prevalent
annotation noise encountered in real-world medical datasets, which limits the
performance ceilings of FL. In this paper, we, for the first time, identify and
tackle this problem. For problem formulation, we propose a contour evolution
for modeling non-independent and identically distributed (Non-IID) noise across
pixels within each client and then extend it to the case of multi-source data
to form a heterogeneous noise model (i.e., Non-IID annotation noise across
clients). For robust learning from annotations with such two-level Non-IID
noise, we emphasize the importance of data quality in model aggregation,
allowing high-quality clients to have a greater impact on FL. To achieve this,
we propose Federated learning with Annotation quAlity-aware AggregatIon, named
FedA3I, by introducing a quality factor based on client-wise noise estimation.
Specifically, noise estimation at each client is accomplished through the
Gaussian mixture model and then incorporated into model aggregation in a
layer-wise manner to up-weight high-quality clients. Extensive experiments on
two real-world medical image segmentation datasets demonstrate the superior
performance of FedA$^3$I against the state-of-the-art approaches in dealing
with cross-client annotation noise. The code is available at
https://github.com/wnn2000/FedAAAI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1&quot;&gt;Nannan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhaobin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zengqiang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Li Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13646">
<title>Weakly Supervised Semantic Segmentation for Driving Scenes. (arXiv:2312.13646v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13646</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art techniques in weakly-supervised semantic segmentation (WSSS)
using image-level labels exhibit severe performance degradation on driving
scene datasets such as Cityscapes. To address this challenge, we develop a new
WSSS framework tailored to driving scene datasets. Based on extensive analysis
of dataset characteristics, we employ Contrastive Language-Image Pre-training
(CLIP) as our baseline to obtain pseudo-masks. However, CLIP introduces two key
challenges: (1) pseudo-masks from CLIP lack in representing small object
classes, and (2) these masks contain notable noise. We propose solutions for
each issue as follows. (1) We devise Global-Local View Training that seamlessly
incorporates small-scale patches during model training, thereby enhancing the
model&apos;s capability to handle small-sized yet critical objects in driving scenes
(e.g., traffic light). (2) We introduce Consistency-Aware Region Balancing
(CARB), a novel technique that discerns reliable and noisy regions through
evaluating the consistency between CLIP masks and segmentation predictions. It
prioritizes reliable pixels over noisy pixels via adaptive loss weighting.
Notably, the proposed method achieves 51.8\% mIoU on the Cityscapes test
dataset, showcasing its potential as a strong WSSS baseline on driving scene
datasets. Experimental results on CamVid and WildDash2 demonstrate the
effectiveness of our method across diverse datasets, even with small-scale
datasets or visually challenging conditions. The code is available at
https://github.com/k0u-id/CARB.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dongseob Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seungho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1&quot;&gt;Junsuk Choe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1&quot;&gt;Hyunjung Shim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15144">
<title>Spatial-Temporal Decoupling Contrastive Learning for Skeleton-based Human Action Recognition. (arXiv:2312.15144v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15144</link>
<description rdf:parseType="Literal">&lt;p&gt;Skeleton-based action recognition is a central task in human-computer
interaction. However, most previous methods suffer from two issues: (i)
semantic ambiguity arising from spatial-temporal information mixture; and (ii)
overlooking the explicit exploitation of the latent data distributions (i.e.,
the intra-class variations and inter-class relations), thereby leading to
sub-optimum solutions of the skeleton encoders. To mitigate this, we propose a
spatial-temporal decoupling contrastive learning (STD-CL) framework to obtain
discriminative and semantically distinct representations from the sequences,
which can be incorporated into various previous skeleton encoders and can be
removed when testing. Specifically, we decouple the global features into
spatial-specific and temporal-specific features to reduce the spatial-temporal
coupling of features. Furthermore, to explicitly exploit the latent data
distributions, we employ the attentive features to contrastive learning, which
models the cross-sequence semantic relations by pulling together the features
from the positive pairs and pushing away the negative pairs. Extensive
experiments show that STD-CL with four various skeleton encoders (HCN, 2S-AGCN,
CTR-GCN, and Hyperformer) achieves solid improvements on NTU60, NTU120, and
NW-UCLA benchmarks. The code will be released soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaojie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jianqin Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_Y/0/1/0/all/0/1&quot;&gt;Yonghao Dang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.17670">
<title>TopCoW: Benchmarking Topology-Aware Anatomical Segmentation of the Circle of Willis (CoW) for CTA and MRA. (arXiv:2312.17670v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.17670</link>
<description rdf:parseType="Literal">&lt;p&gt;The Circle of Willis (CoW) is an important network of arteries connecting
major circulations of the brain. Its vascular architecture is believed to
affect the risk, severity, and clinical outcome of serious neuro-vascular
diseases. However, characterizing the highly variable CoW anatomy is still a
manual and time-consuming expert task. The CoW is usually imaged by two
angiographic imaging modalities, magnetic resonance angiography (MRA) and
computed tomography angiography (CTA), but there exist limited public datasets
with annotations on CoW anatomy, especially for CTA. Therefore we organized the
TopCoW Challenge in 2023 with the release of an annotated CoW dataset. The
TopCoW dataset was the first public dataset with voxel-level annotations for
thirteen possible CoW vessel components, enabled by virtual-reality (VR)
technology. It was also the first large dataset with paired MRA and CTA from
the same patients. TopCoW challenge formalized the CoW characterization problem
as a multiclass anatomical segmentation task with an emphasis on topological
metrics. We invited submissions worldwide for the CoW segmentation task, which
attracted over 140 registered participants from four continents. The top
performing teams managed to segment many CoW components to Dice scores around
90%, but with lower scores for communicating arteries and rare variants. There
were also topological mistakes for predictions with high Dice scores.
Additional topological analysis revealed further areas for improvement in
detecting certain CoW components and matching CoW variant topology accurately.
TopCoW represented a first attempt at benchmarking the CoW anatomical
segmentation task for MRA and CTA, both morphologically and topologically.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kaiyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musio_F/0/1/0/all/0/1&quot;&gt;Fabio Musio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yihui Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juchler_N/0/1/0/all/0/1&quot;&gt;Norman Juchler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paetzold_J/0/1/0/all/0/1&quot;&gt;Johannes C. Paetzold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Maskari_R/0/1/0/all/0/1&quot;&gt;Rami Al-Maskari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoher_L/0/1/0/all/0/1&quot;&gt;Luciano H&amp;#xf6;her&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongwei Bran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamamci_I/0/1/0/all/0/1&quot;&gt;Ibrahim Ethem Hamamci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sekuboyina_A/0/1/0/all/0/1&quot;&gt;Anjany Sekuboyina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shit_S/0/1/0/all/0/1&quot;&gt;Suprosanna Shit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Houjing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waldmannstetter_D/0/1/0/all/0/1&quot;&gt;Diana Waldmannstetter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kofler_F/0/1/0/all/0/1&quot;&gt;Florian Kofler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navarro_F/0/1/0/all/0/1&quot;&gt;Fernando Navarro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menten_M/0/1/0/all/0/1&quot;&gt;Martin Menten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ezhov_I/0/1/0/all/0/1&quot;&gt;Ivan Ezhov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vos_I/0/1/0/all/0/1&quot;&gt;Iris Vos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruigrok_Y/0/1/0/all/0/1&quot;&gt;Ynte Ruigrok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velthuis_B/0/1/0/all/0/1&quot;&gt;Birgitta Velthuis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuijf_H/0/1/0/all/0/1&quot;&gt;Hugo Kuijf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hammerli_J/0/1/0/all/0/1&quot;&gt;Julien H&amp;#xe4;mmerli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wurster_C/0/1/0/all/0/1&quot;&gt;Catherine Wurster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bijlenga_P/0/1/0/all/0/1&quot;&gt;Philippe Bijlenga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Westphal_L/0/1/0/all/0/1&quot;&gt;Laura Westphal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisschop_J/0/1/0/all/0/1&quot;&gt;Jeroen Bisschop&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colombo_E/0/1/0/all/0/1&quot;&gt;Elisa Colombo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baazaoui_H/0/1/0/all/0/1&quot;&gt;Hakim Baazaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makmur_A/0/1/0/all/0/1&quot;&gt;Andrew Makmur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hallinan_J/0/1/0/all/0/1&quot;&gt;James Hallinan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiestler_B/0/1/0/all/0/1&quot;&gt;Bene Wiestler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirschke_J/0/1/0/all/0/1&quot;&gt;Jan S. Kirschke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiest_R/0/1/0/all/0/1&quot;&gt;Roland Wiest&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montagnon_E/0/1/0/all/0/1&quot;&gt;Emmanuel Montagnon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Letourneau_Guillon_L/0/1/0/all/0/1&quot;&gt;Laurent Letourneau-Guillon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galdran_A/0/1/0/all/0/1&quot;&gt;Adrian Galdran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galati_F/0/1/0/all/0/1&quot;&gt;Francesco Galati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falcetta_D/0/1/0/all/0/1&quot;&gt;Daniele Falcetta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuluaga_M/0/1/0/all/0/1&quot;&gt;Maria A. Zuluaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chaolong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haoran Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zehan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ra_S/0/1/0/all/0/1&quot;&gt;Sinyoung Ra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jongyun Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1&quot;&gt;Hyunjin Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junqiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wodzinski_M/0/1/0/all/0/1&quot;&gt;Marek Wodzinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_H/0/1/0/all/0/1&quot;&gt;Henning M&amp;#xfc;ller&lt;/a&gt;, et al. (33 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05163">
<title>MISS: A Generative Pretraining and Finetuning Approach for Med-VQA. (arXiv:2401.05163v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05163</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical visual question answering (VQA) is a challenging multimodal task,
where Vision-Language Pre-training (VLP) models can effectively improve the
generalization performance. However, most methods in the medical field treat
VQA as an answer classification task which is difficult to transfer to
practical application scenarios. Additionally, due to the privacy of medical
images and the expensive annotation process, large-scale medical image-text
pairs datasets for pretraining are severely lacking. In this paper, we propose
a large-scale MultI-task Self-Supervised learning based framework (MISS) for
medical VQA tasks. Unlike existing methods, we treat medical VQA as a
generative task. We unify the text encoder and multimodal encoder and align
image-text features through multi-task learning. Furthermore, we propose a
Transfer-and-Caption method that extends the feature space of single-modal
image datasets using large language models (LLMs), enabling those traditional
medical vision field task data to be applied to VLP. Experiments show that our
method achieves excellent results with fewer multimodal datasets and
demonstrates the advantages of generative VQA models. The code and model
weights will be released upon the paper&apos;s acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiawei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dingkang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yue Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lihua Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06397">
<title>UMG-CLIP: A Unified Multi-Granularity Vision Generalist for Open-World Understanding. (arXiv:2401.06397v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.06397</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-language foundation models, represented by Contrastive language-image
pre-training (CLIP), have gained increasing attention for jointly understanding
both vision and textual tasks. However, existing approaches primarily focus on
training models to match global image representations with textual
descriptions, thereby overlooking the critical alignment between local regions
and corresponding text tokens. This paper extends CLIP with multi-granularity
alignment. Notably, we deliberately construct a new dataset comprising pseudo
annotations at various levels of granularities, encompassing image-level,
region-level, and pixel-level captions/tags. Accordingly, we develop a unified
multi-granularity learning framework, named UMG-CLIP, that simultaneously
empowers the model with versatile perception abilities across different levels
of detail. Equipped with parameter efficient tuning, UMG-CLIP surpasses current
widely used CLIP models and achieves state-of-the-art performance on diverse
image understanding benchmarks, including open-world recognition, retrieval,
semantic segmentation, and panoptic segmentation tasks. We hope UMG-CLIP can
serve as a valuable option for advancing vision-language foundation models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bowen Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Peisen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zichen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuhang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaoming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Wenrui Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;Junni Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hongkai Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qi Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaopeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07402">
<title>Improved Implicity Neural Representation with Fourier Bases Reparameterized Training. (arXiv:2401.07402v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07402</link>
<description rdf:parseType="Literal">&lt;p&gt;Implicit Neural Representation (INR) as a mighty representation paradigm has
achieved success in various computer vision tasks recently. Due to the
low-frequency bias issue of vanilla multi-layer perceptron (MLP), existing
methods have investigated advanced techniques, such as positional encoding and
periodic activation function, to improve the accuracy of INR. In this paper, we
connect the network training bias with the reparameterization technique and
theoretically prove that weight reparameterization could provide us a chance to
alleviate the spectral bias of MLP. Based on our theoretical analysis, we
propose a Fourier reparameterization method which learns coefficient matrix of
fixed Fourier bases to compose the weights of MLP. We evaluate the proposed
Fourier reparameterization method on different INR tasks with various MLP
architectures, including vanilla MLP, MLP with positional encoding and MLP with
advanced activation function, etc. The superiority approximation results on
different MLP architectures clearly validate the advantage of our proposed
method. Armed with our Fourier reparameterization method, better INR with more
textures and less artifacts can be learned from the training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1&quot;&gt;Kexuan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xingyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1&quot;&gt;Shuhang Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07450">
<title>Hierarchical Fashion Design with Multi-stage Diffusion Models. (arXiv:2401.07450v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07450</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-modal fashion synthesis and editing offer intelligent support to
fashion designers by enabling the automatic generation and local modification
of design drafts.While current diffusion models demonstrate commendable
stability and controllability in image synthesis,they still face significant
challenges in generating fashion design from abstract design elements and
fine-grained editing.Abstract sensory expressions, \eg office, business, and
party, form the high-level design concepts, while measurable aspects like
sleeve length, collar type, and pant length are considered the low-level
attributes of clothing.Controlling and editing fashion images using lengthy
text descriptions poses a difficulty.In this paper, we propose HieraFashDiff,a
novel fashion design method using the shared multi-stage diffusion model
encompassing high-level design concepts and low-level clothing attributes in a
hierarchical structure.Specifically, we categorized the input text into
different levels and fed them in different time step to the diffusion model
according to the criteria of professional clothing designers.HieraFashDiff
allows designers to add low-level attributes after high-level prompts for
interactive editing incrementally.In addition, we design a differentiable loss
function in the sampling process with a mask to keep non-edit
areas.Comprehensive experiments performed on our newly conducted Hierarchical
fashion dataset,demonstrate that our proposed method outperforms other
state-of-the-art competitors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+li_H/0/1/0/all/0/1&quot;&gt;Hao li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Huiming Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mengtian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Ying Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08140">
<title>ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process. (arXiv:2401.08140v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08140</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural radiance fields (NeRFs) have gained popularity across various
applications. However, they face challenges in the sparse view setting, lacking
sufficient constraints from volume rendering. Reconstructing and understanding
a 3D scene from sparse and unconstrained cameras is a long-standing problem in
classical computer vision with diverse applications. While recent works have
explored NeRFs in sparse, unconstrained view scenarios, their focus has been
primarily on enhancing reconstruction and novel view synthesis. Our approach
takes a broader perspective by posing the question: &quot;from where has each point
been seen?&quot; -- which gates how well we can understand and reconstruct it. In
other words, we aim to determine the origin or provenance of each 3D point and
its associated information under sparse, unconstrained views. We introduce
ProvNeRF, a model that enriches a traditional NeRF representation by
incorporating per-point provenance, modeling likely source locations for each
point. We achieve this by extending implicit maximum likelihood estimation
(IMLE) for stochastic processes. Notably, our method is compatible with any
pre-trained NeRF model and the associated training camera poses. We demonstrate
that modeling per-point provenance offers several advantages, including
uncertainty estimation, criteria-based view selection, and improved novel view
synthesis, compared to state-of-the-art methods. Please visit our project page
at https://provnerf.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakayama_K/0/1/0/all/0/1&quot;&gt;Kiyohiro Nakayama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uy_M/0/1/0/all/0/1&quot;&gt;Mikaela Angelina Uy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1&quot;&gt;Yang You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Ke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1&quot;&gt;Leonidas Guibas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08209">
<title>Transcending the Limit of Local Window: Advanced Super-Resolution Transformer with Adaptive Token Dictionary. (arXiv:2401.08209v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08209</link>
<description rdf:parseType="Literal">&lt;p&gt;Single Image Super-Resolution is a classic computer vision problem that
involves estimating high-resolution (HR) images from low-resolution (LR) ones.
Although deep neural networks (DNNs), especially Transformers for
super-resolution, have seen significant advancements in recent years,
challenges still remain, particularly in limited receptive field caused by
window-based self-attention. To address these issues, we introduce a group of
auxiliary Adaptive Token Dictionary to SR Transformer and establish an ATD-SR
method. The introduced token dictionary could learn prior information from
training data and adapt the learned prior to specific testing image through an
adaptive refinement step. The refinement strategy could not only provide global
information to all input tokens but also group image tokens into categories.
Based on category partitions, we further propose a category-based
self-attention mechanism designed to leverage distant but similar tokens for
enhancing input features. The experimental results show that our method
achieves the best performance on various single image super-resolution
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Leheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xingyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiaorui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1&quot;&gt;Shuhang Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08689">
<title>NODI: Out-Of-Distribution Detection with Noise from Diffusion. (arXiv:2401.08689v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08689</link>
<description rdf:parseType="Literal">&lt;p&gt;Out-of-distribution (OOD) detection is a crucial part of deploying machine
learning models safely. It has been extensively studied with a plethora of
methods developed in the literature. This problem is tackled with an OOD score
computation, however, previous methods compute the OOD scores with limited
usage of the in-distribution dataset. For instance, the OOD scores are computed
with information from a small portion of the in-distribution data. Furthermore,
these methods encode images with a neural image encoder. The robustness of
these methods is rarely checked with respect to image encoders of different
training methods and architectures. In this work, we introduce the diffusion
process into the OOD task. The diffusion model integrates information on the
whole training set into the predicted noise vectors. What&apos;s more, we deduce a
closed-form solution for the noise vector (stable point). Then the noise vector
is converted into our OOD score, we test both the deep model predicted noise
vector and the closed-form noise vector on the OOD benchmarks \cite{openood}.
Our method outperforms previous OOD methods across all types of image encoders
(Table. \ref{main}). A $3.5\%$ performance gain is achieved with the MAE-based
image encoder. Moreover, we studied the robustness of OOD methods by applying
different types of image encoders. Some OOD methods failed to generalize well
when switching image encoders from ResNet to Vision Transformers, our method
performs exhibits good robustness with all the image encoders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingqiu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Aojun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08868">
<title>B-Cos Aligned Transformers Learn Human-Interpretable Features. (arXiv:2401.08868v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08868</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) and Swin Transformers (Swin) are currently
state-of-the-art in computational pathology. However, domain experts are still
reluctant to use these models due to their lack of interpretability. This is
not surprising, as critical decisions need to be transparent and
understandable. The most common approach to understanding transformers is to
visualize their attention. However, attention maps of ViTs are often
fragmented, leading to unsatisfactory explanations. Here, we introduce a novel
architecture called the B-cos Vision Transformer (BvT) that is designed to be
more interpretable. It replaces all linear transformations with the B-cos
transform to promote weight-input alignment. In a blinded study, medical
experts clearly ranked BvTs above ViTs, suggesting that our network is better
at capturing biomedically relevant structures. This is also true for the B-cos
Swin Transformer (Bwin). Compared to the Swin Transformer, it even improves the
F1-score by up to 4.7% on two public datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1&quot;&gt;Manuel Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lahiani_A/0/1/0/all/0/1&quot;&gt;Amal Lahiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cid_Y/0/1/0/all/0/1&quot;&gt;Yashin Dicente Cid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boxberg_M/0/1/0/all/0/1&quot;&gt;Melanie Boxberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lienemann_P/0/1/0/all/0/1&quot;&gt;Peter Lienemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matek_C/0/1/0/all/0/1&quot;&gt;Christian Matek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_S/0/1/0/all/0/1&quot;&gt;Sophia J. Wagner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theis_F/0/1/0/all/0/1&quot;&gt;Fabian J. Theis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klaiman_E/0/1/0/all/0/1&quot;&gt;Eldad Klaiman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1&quot;&gt;Tingying Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09112">
<title>Stream Query Denoising for Vectorized HD Map Construction. (arXiv:2401.09112v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09112</link>
<description rdf:parseType="Literal">&lt;p&gt;To enhance perception performance in complex and extensive scenarios within
the realm of autonomous driving, there has been a noteworthy focus on temporal
modeling, with a particular emphasis on streaming methods. The prevailing trend
in streaming models involves the utilization of stream queries for the
propagation of temporal information. Despite the prevalence of this approach,
the direct application of the streaming paradigm to the construction of
vectorized high-definition maps (HD-maps) fails to fully harness the inherent
potential of temporal information. This paper introduces the Stream Query
Denoising (SQD) strategy as a novel approach for temporal modeling in
high-definition map (HD-map) construction. SQD is designed to facilitate the
learning of temporal consistency among map elements within the streaming model.
The methodology involves denoising the queries that have been perturbed by the
addition of noise to the ground-truth information from the preceding frame.
This denoising process aims to reconstruct the ground-truth information for the
current frame, thereby simulating the prediction process inherent in stream
queries. The SQD strategy can be applied to those streaming methods (e.g.,
StreamMapNet) to enhance the temporal modeling. The proposed SQD-MapNet is the
StreamMapNet equipped with SQD. Extensive experiments on nuScenes and
Argoverse2 show that our method is remarkably superior to other existing
methods across all settings of close range and long range. The code will be
available soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1&quot;&gt;Fan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yingfei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yucheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zehui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tiancai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1&quot;&gt;Feng Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09180">
<title>Unsupervised Multiple Domain Translation through Controlled Disentanglement in Variational Autoencoder. (arXiv:2401.09180v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09180</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised Multiple Domain Translation is the task of transforming data
from one domain to other domains without having paired data to train the
systems. Typically, methods based on Generative Adversarial Networks (GANs) are
used to address this task. However, our proposal exclusively relies on a
modified version of a Variational Autoencoder. This modification consists of
the use of two latent variables disentangled in a controlled way by design. One
of this latent variables is imposed to depend exclusively on the domain, while
the other one must depend on the rest of the variability factors of the data.
Additionally, the conditions imposed over the domain latent variable allow for
better control and understanding of the latent space. We empirically
demonstrate that our approach works on different vision datasets improving the
performance of other well known methods. Finally, we prove that, indeed, one of
the latent variables stores all the information related to the domain and the
other one hardly contains any domain information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almudevar_A/0/1/0/all/0/1&quot;&gt;Antonio Almud&amp;#xe9;var&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mariotte_T/0/1/0/all/0/1&quot;&gt;Th&amp;#xe9;o Mariotte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_A/0/1/0/all/0/1&quot;&gt;Alfonso Ortega&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tahon_M/0/1/0/all/0/1&quot;&gt;Marie Tahon&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>