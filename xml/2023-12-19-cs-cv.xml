<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-17T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09299" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09313" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09367" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09398" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09411" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09445" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09455" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09480" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09481" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09510" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09514" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09523" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09525" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09533" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09538" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09551" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09554" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09558" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09570" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09576" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09584" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09608" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09612" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09625" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09627" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09641" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09673" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09676" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09783" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09821" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09876" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09909" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09922" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09935" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09955" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09968" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10034" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10035" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2001.01258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2107.10984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.04968" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.09602" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01272" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.12381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.12417" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.07786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.06132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.00901" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.04998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07213" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10839" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10941" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08265" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13237" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04787" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13859" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03054" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13194" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16835" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05777" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08054" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08078" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08733" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08774" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09076" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09095" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09228" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.09299">
<title>Weight subcloning: direct initialization of transformers using larger pretrained ones. (arXiv:2312.09299v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09299</link>
<description rdf:parseType="Literal">&lt;p&gt;Training large transformer models from scratch for a target task requires
lots of data and is computationally demanding. The usual practice of transfer
learning overcomes this challenge by initializing the model with weights of a
pretrained model of the same size and specification to increase the convergence
and training speed. However, what if no pretrained model of the required size
is available? In this paper, we introduce a simple yet effective technique to
transfer the knowledge of a pretrained model to smaller variants. Our approach
called weight subcloning expedites the training of scaled-down transformers by
initializing their weights from larger pretrained models.
&lt;/p&gt;
&lt;p&gt;Weight subcloning involves an operation on the pretrained model to obtain the
equivalent initialized scaled-down model. It consists of two key steps: first,
we introduce neuron importance ranking to decrease the embedding dimension per
layer in the pretrained model. Then, we remove blocks from the transformer
model to match the number of layers in the scaled-down network. The result is a
network ready to undergo training, which gains significant improvements in
training speed compared to random initialization. For instance, we achieve 4x
faster training for vision transformers in image classification and language
models designed for next token prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samragh_M/0/1/0/all/0/1&quot;&gt;Mohammad Samragh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farajtabar_M/0/1/0/all/0/1&quot;&gt;Mehrdad Farajtabar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1&quot;&gt;Sachin Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vemulapalli_R/0/1/0/all/0/1&quot;&gt;Raviteja Vemulapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faghri_F/0/1/0/all/0/1&quot;&gt;Fartash Faghri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naik_D/0/1/0/all/0/1&quot;&gt;Devang Naik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuzel_O/0/1/0/all/0/1&quot;&gt;Oncel Tuzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1&quot;&gt;Mohammad Rastegari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09305">
<title>Stable Score Distillation for High-Quality 3D Generation. (arXiv:2312.09305v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09305</link>
<description rdf:parseType="Literal">&lt;p&gt;Score Distillation Sampling (SDS) has exhibited remarkable performance in
conditional 3D content generation. However, a comprehensive understanding of
the SDS formulation is still lacking, hindering the development of 3D
generation. In this work, we present an interpretation of SDS as a combination
of three functional components: mode-disengaging, mode-seeking and
variance-reducing terms, and analyze the properties of each. We show that
problems such as over-smoothness and color-saturation result from the intrinsic
deficiency of the supervision terms and reveal that the variance-reducing term
introduced by SDS is sub-optimal. Additionally, we shed light on the adoption
of large Classifier-Free Guidance (CFG) scale for 3D generation. Based on the
analysis, we propose a simple yet effective approach named Stable Score
Distillation (SSD) which strategically orchestrates each term for high-quality
3D generation. Extensive experiments validate the efficacy of our approach,
demonstrating its ability to generate high-fidelity 3D content without
succumbing to issues such as over-smoothness and over-saturation, even under
low CFG conditions with the most challenging NeRF representation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1&quot;&gt;Boshi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09313">
<title>LatentEditor: Text Driven Local Editing of 3D Scenes. (arXiv:2312.09313v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09313</link>
<description rdf:parseType="Literal">&lt;p&gt;While neural fields have made significant strides in view synthesis and scene
reconstruction, editing them poses a formidable challenge due to their implicit
encoding of geometry and texture information from multi-view inputs. In this
paper, we introduce \textsc{LatentEditor}, an innovative framework designed to
empower users with the ability to perform precise and locally controlled
editing of neural fields using text prompts. Leveraging denoising diffusion
models, we successfully embed real-world scenes into the latent space,
resulting in a faster and more adaptable NeRF backbone for editing compared to
traditional methods. To enhance editing precision, we introduce a delta score
to calculate the 2D mask in the latent space that serves as a guide for local
modifications while preserving irrelevant regions. Our novel pixel-level
scoring approach harnesses the power of InstructPix2Pix (IP2P) to discern the
disparity between IP2P conditional and unconditional noise predictions in the
latent space. The edited latents conditioned on the 2D masks are then
iteratively updated in the training set to achieve 3D local editing. Our
approach achieves faster editing speeds and superior output quality compared to
existing 3D editing models, bridging the gap between textual instructions and
high-quality 3D scene editing in latent space. We show the superiority of our
approach on four benchmark 3D datasets, LLFF, IN2N, NeRFStudio and NeRF-Art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalid_U/0/1/0/all/0/1&quot;&gt;Umar Khalid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iqbal_H/0/1/0/all/0/1&quot;&gt;Hasan Iqbal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1&quot;&gt;Nazmul Karim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_J/0/1/0/all/0/1&quot;&gt;Jing Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09337">
<title>Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences. (arXiv:2312.09337v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09337</link>
<description rdf:parseType="Literal">&lt;p&gt;Customizing robotic behaviors to be aligned with diverse human preferences is
an underexplored challenge in the field of embodied AI. In this paper, we
present Promptable Behaviors, a novel framework that facilitates efficient
personalization of robotic agents to diverse human preferences in complex
environments. We use multi-objective reinforcement learning to train a single
policy adaptable to a broad spectrum of preferences. We introduce three
distinct methods to infer human preferences by leveraging different types of
interactions: (1) human demonstrations, (2) preference feedback on trajectory
comparisons, and (3) language instructions. We evaluate the proposed method in
personalized object-goal navigation and flee navigation tasks in ProcTHOR and
RoboTHOR, demonstrating the ability to prompt agent behaviors to satisfy human
preferences in various scenarios. Project page:
https://promptable-behaviors.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_M/0/1/0/all/0/1&quot;&gt;Minyoung Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weihs_L/0/1/0/all/0/1&quot;&gt;Luca Weihs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Chanwoo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kimin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1&quot;&gt;Aniruddha Kembhavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehsani_K/0/1/0/all/0/1&quot;&gt;Kiana Ehsani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09360">
<title>The Expert Knowledge combined with AI outperforms AI Alone in Seizure Onset Zone Localization using resting state fMRI. (arXiv:2312.09360v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09360</link>
<description rdf:parseType="Literal">&lt;p&gt;We evaluated whether integration of expert guidance on seizure onset zone
(SOZ) identification from resting state functional MRI (rs-fMRI) connectomics
combined with deep learning (DL) techniques enhances the SOZ delineation in
patients with refractory epilepsy (RE), compared to utilizing DL alone. Rs-fMRI
were collected from 52 children with RE who had subsequently undergone ic-EEG
and then, if indicated, surgery for seizure control (n = 25). The resting state
functional connectomics data were previously independently classified by two
expert epileptologists, as indicative of measurement noise, typical resting
state network connectivity, or SOZ. An expert knowledge integrated deep network
was trained on functional connectomics data to identify SOZ. Expert knowledge
integrated with DL showed a SOZ localization accuracy of 84.8&amp;amp; and F1 score,
harmonic mean of positive predictive value and sensitivity, of 91.7%.
Conversely, a DL only model yielded an accuracy of less than 50% (F1 score
63%). Activations that initiate in gray matter, extend through white matter and
end in vascular regions are seen as the most discriminative expert identified
SOZ characteristics. Integration of expert knowledge of functional connectomics
can not only enhance the performance of DL in localizing SOZ in RE, but also
lead toward potentially useful explanations of prevalent co-activation patterns
in SOZ. RE with surgical outcomes and pre-operative rs-fMRI studies can yield
expert knowledge most salient for SOZ identification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamboj_P/0/1/0/all/0/1&quot;&gt;Payal Kamboj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1&quot;&gt;Ayan Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boerwinkle_V/0/1/0/all/0/1&quot;&gt;Varina L. Boerwinkle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Sandeep K.S. Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09361">
<title>RTRA: Rapid Training of Regularization-based Approaches in Continual Learning. (arXiv:2312.09361v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09361</link>
<description rdf:parseType="Literal">&lt;p&gt;Catastrophic forgetting(CF) is a significant challenge in continual learning
(CL). In regularization-based approaches to mitigate CF, modifications to
important training parameters are penalized in subsequent tasks using an
appropriate loss function. We propose the RTRA, a modification to the widely
used Elastic Weight Consolidation (EWC) regularization scheme, using the
Natural Gradient for loss function optimization. Our approach improves the
training of regularization-based methods without sacrificing test-data
performance. We compare the proposed RTRA approach against EWC using the
iFood251 dataset. We show that RTRA has a clear edge over the state-of-the-art
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nokhwal_S/0/1/0/all/0/1&quot;&gt;Sahil Nokhwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1&quot;&gt;Nirman Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09365">
<title>SAR image segmentation algorithms based on I-divergence-TV model. (arXiv:2312.09365v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09365</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel variational active contour model based on
I-divergence-TV model to segment Synthetic aperture radar (SAR) images with
multiplicative gamma noise, which hybrides edge-based model with region-based
model. The proposed model can efficiently stop the contours at weak or blurred
edges, and can automatically detect the exterior and interior boundaries of
images. We incorporate the global convex segmentation method and split Bregman
technique into the proposed model, and propose a fast fixed point algorithm to
solve the global convex segmentation question[25]. Experimental results for
synthetic images and real SAR images show that the proposed fast fixed point
algorithm is robust and efficient compared with the state-of-the-art approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guangming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Quanying Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+iu_Q/0/1/0/all/0/1&quot;&gt;Qi iu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09367">
<title>Text-Guided Face Recognition using Multi-Granularity Cross-Modal Contrastive Learning. (arXiv:2312.09367v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09367</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art face recognition (FR) models often experience a significant
performance drop when dealing with facial images in surveillance scenarios
where images are in low quality and often corrupted with noise. Leveraging
facial characteristics, such as freckles, scars, gender, and ethnicity, becomes
highly beneficial in improving FR performance in such scenarios. In this paper,
we introduce text-guided face recognition (TGFR) to analyze the impact of
integrating facial attributes in the form of natural language descriptions. We
hypothesize that adding semantic information into the loop can significantly
improve the image understanding capability of an FR algorithm compared to other
soft biometrics. However, learning a discriminative joint embedding within the
multimodal space poses a considerable challenge due to the semantic gap in the
unaligned image-text representations, along with the complexities arising from
ambiguous and incoherent textual descriptions of the face. To address these
challenges, we introduce a face-caption alignment module (FCAM), which
incorporates cross-modal contrastive losses across multiple granularities to
maximize the mutual information between local and global features of the
face-caption pair. Within FCAM, we refine both facial and textual features for
learning aligned and discriminative features. We also design a face-caption
fusion module (FCFM) that applies fine-grained interactions and coarse-grained
associations among cross-modal features. Through extensive experiments
conducted on three face-caption datasets, proposed TGFR demonstrates remarkable
improvements, particularly on low-quality images, over existing FR models and
outperforms other related methods and benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1&quot;&gt;Md Mahedi Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sami_S/0/1/0/all/0/1&quot;&gt;Shoaib Meraj Sami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1&quot;&gt;Nasser Nasrabadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09387">
<title>High-Resolution Maps of Left Atrial Displacements and Strains Estimated with 3D CINE MRI and Unsupervised Neural Networks. (arXiv:2312.09387v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09387</link>
<description rdf:parseType="Literal">&lt;p&gt;The functional analysis of the left atrium (LA) is important for evaluating
cardiac health and understanding diseases like atrial fibrillation. Cine MRI is
ideally placed for the detailed 3D characterisation of LA motion and
deformation, but it is lacking appropriate acquisition and analysis tools. In
this paper, we present Analysis for Left Atrial Displacements and Deformations
using unsupervIsed neural Networks, \textit{Aladdin}, to automatically and
reliably characterise regional LA deformations from high-resolution 3D Cine
MRI. The tool includes: an online few-shot segmentation network (Aladdin-S), an
online unsupervised image registration network (Aladdin-R), and a strain
calculations pipeline tailored to the LA. We create maps of LA Displacement
Vector Field (DVF) magnitude and LA principal strain values from images of 10
healthy volunteers and 8 patients with cardiovascular disease (CVD). We
additionally create an atlas of these biomarkers using the data from the
healthy volunteers. Aladdin is able to accurately track the LA wall across the
cardiac cycle and characterize its motion and deformation. The overall DVF
magnitude and principal strain values are significantly higher in the healthy
group vs CVD patients: $2.85 \pm 1.59~mm$ and $0.09 \pm 0.05$ vs $1.96 \pm
0.74~mm$ and $0.03 \pm 0.04$, respectively. The time course of these metrics is
also different in the two groups, with a more marked active contraction phase
observed in the healthy cohort. Finally, utilizing the LA atlas allows us to
identify regional deviations from the population distribution that may indicate
focal tissue abnormalities. The proposed tool for the quantification of novel
regional LA deformation biomarkers should have important clinical applications.
The source code, anonymized images, generated maps and atlas are publicly
available: https://github.com/cgalaz01/aladdin_cmr_la.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galazis_C/0/1/0/all/0/1&quot;&gt;Christoforos Galazis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shepperd_S/0/1/0/all/0/1&quot;&gt;Samuel Shepperd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brouwer_E/0/1/0/all/0/1&quot;&gt;Emma Brouwer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Queiros_S/0/1/0/all/0/1&quot;&gt;Sandro Queir&amp;#xf3;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alskaf_E/0/1/0/all/0/1&quot;&gt;Ebraham Alskaf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anjari_M/0/1/0/all/0/1&quot;&gt;Mustafa Anjari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiribiri_A/0/1/0/all/0/1&quot;&gt;Amedeo Chiribiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jack Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bharath_A/0/1/0/all/0/1&quot;&gt;Anil A. Bharath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varela_M/0/1/0/all/0/1&quot;&gt;Marta Varela&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09398">
<title>Relightable Neural Assets. (arXiv:2312.09398v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2312.09398</link>
<description rdf:parseType="Literal">&lt;p&gt;High-fidelity 3D assets with materials composed of fibers (including hair),
complex layered material shaders, or fine scattering geometry are ubiquitous in
high-end realistic rendering applications. Rendering such models is
computationally expensive due to heavy shaders and long scattering paths.
Moreover, implementing the shading and scattering models is non-trivial and has
to be done not only in the 3D content authoring software (which is necessarily
complex), but also in all downstream rendering solutions. For example, web and
mobile viewers for complex 3D assets are desirable, but frequently cannot
support the full shading complexity allowed by the authoring application. Our
goal is to design a neural representation for 3D assets with complex shading
that supports full relightability and full integration into existing renderers.
We provide an end-to-end shading solution at the first intersection of a ray
with the underlying geometry. All shading and scattering is precomputed and
included in the neural asset; no multiple scattering paths need to be traced,
and no complex shading models need to be implemented to render our assets,
beyond a single neural architecture. We combine an MLP decoder with a feature
grid. Shading consists of querying a feature vector, followed by an MLP
evaluation producing the final reflectance value. Our method provides
high-fidelity shading, close to the ground-truth Monte Carlo estimate even at
close-up views. We believe our neural assets could be used in practical
renderers, providing significant speed-ups and simplifying renderer
implementations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mullia_K/0/1/0/all/0/1&quot;&gt;Krishna Mullia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luan_F/0/1/0/all/0/1&quot;&gt;Fujun Luan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1&quot;&gt;Milo&amp;#x161; Ha&amp;#x161;an&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09411">
<title>OTOv3: Automatic Architecture-Agnostic Neural Network Training and Compression from Structured Pruning to Erasing Operators. (arXiv:2312.09411v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09411</link>
<description rdf:parseType="Literal">&lt;p&gt;Compressing a predefined deep neural network (DNN) into a compact sub-network
with competitive performance is crucial in the efficient machine learning
realm. This topic spans various techniques, from structured pruning to neural
architecture search, encompassing both pruning and erasing operators
perspectives. Despite advancements, existing methods suffers from complex,
multi-stage processes that demand substantial engineering and domain knowledge,
limiting their broader applications. We introduce the third-generation
Only-Train-Once (OTOv3), which first automatically trains and compresses a
general DNN through pruning and erasing operations, creating a compact and
competitive sub-network without the need of fine-tuning. OTOv3 simplifies and
automates the training and compression process, minimizes the engineering
efforts required from users. It offers key technological advancements: (i)
automatic search space construction for general DNNs based on dependency graph
analysis; (ii) Dual Half-Space Projected Gradient (DHSPG) and its enhanced
version with hierarchical search (H2SPG) to reliably solve (hierarchical)
structured sparsity problems and ensure sub-network validity; and (iii)
automated sub-network construction using solutions from DHSPG/H2SPG and
dependency graphs. Our empirical results demonstrate the efficacy of OTOv3
across various benchmarks in structured pruning and neural architecture search.
OTOv3 produces sub-networks that match or exceed the state-of-the-arts. The
source code will be available at https://github.com/tianyic/only_train_once.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1&quot;&gt;Tianyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhihui Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zeyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;HsiangTao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zharkov_I/0/1/0/all/0/1&quot;&gt;Ilya Zharkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Luming Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09445">
<title>IncepSE: Leveraging InceptionTime&apos;s performance with Squeeze and Excitation mechanism in ECG analysis. (arXiv:2312.09445v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2312.09445</link>
<description rdf:parseType="Literal">&lt;p&gt;Our study focuses on the potential for modifications of Inception-like
architecture within the electrocardiogram (ECG) domain. To this end, we
introduce IncepSE, a novel network characterized by strategic architectural
incorporation that leverages the strengths of both InceptionTime and channel
attention mechanisms. Furthermore, we propose a training setup that employs
stabilization techniques that are aimed at tackling the formidable challenges
of severe imbalance dataset PTB-XL and gradient corruption. By this means, we
manage to set a new height for deep learning model in a supervised learning
manner across the majority of tasks. Our model consistently surpasses
InceptionTime by substantial margins compared to other state-of-the-arts in
this domain, noticeably 0.013 AUROC score improvement in the &quot;all&quot; task, while
also mitigating the inherent dataset fluctuations during training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cao_T/0/1/0/all/0/1&quot;&gt;Tue Minh Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tran_N/0/1/0/all/0/1&quot;&gt;Nhat Hong Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_L/0/1/0/all/0/1&quot;&gt;Le Phi Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pham_H/0/1/0/all/0/1&quot;&gt;Hieu Huy Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hung Thanh Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09446">
<title>A Distributed Inference System for Detecting Task-wise Single Trial Event-Related Potential in Stream of Satellite Images. (arXiv:2312.09446v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2312.09446</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain-computer interface (BCI) has garnered the significant attention for
their potential in various applications, with event-related potential (ERP)
performing a considerable role in BCI systems. This paper introduces a novel
Distributed Inference System tailored for detecting task-wise single-trial ERPs
in a stream of satellite images. Unlike traditional methodologies that employ a
single model for target detection, our system utilizes multiple models, each
optimized for specific tasks, ensuring enhanced performance across varying
image transition times and target onset times. Our experiments, conducted on
four participants, employed two paradigms: the Normal paradigm and an AI
paradigm with bounding boxes. Results indicate that our proposed system
outperforms the conventional methods in both paradigms, achieving the highest
$F_{\beta}$ scores. Furthermore, including bounding boxes in the AI paradigm
significantly improved target recognition. This study underscores the potential
of our Distributed Inference System in advancing the field of ERP detection in
satellite image streams.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sung-Jin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kwak_H/0/1/0/all/0/1&quot;&gt;Heon-Gyu Kwak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_H/0/1/0/all/0/1&quot;&gt;Hyeon-Taek Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dae-Hyeok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jeong_J/0/1/0/all/0/1&quot;&gt;Ji-Hoon Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seong-Whan Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09455">
<title>Integration of Robotics, Computer Vision, and Algorithm Design: A Chinese Poker Self-Playing Robot. (arXiv:2312.09455v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.09455</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents Chinese Poker Self-Playing Robot, an integrated system
enabling a TM5-900 robotic arm to independently play the four-person card game
Chinese poker. The robot uses a custom sucker mechanism to pick up and play
cards. An object detection model based on YOLOv5 is utilized to recognize the
suit and number of 13 cards dealt to the robot. A greedy algorithm is developed
to divide the 13 cards into optimal hands of 3, 5, and 5 cards to play.
Experiments demonstrate that the robot can successfully obtain the cards,
identify them using computer vision, strategically select hands to play using
the algorithm, and physically play the selected cards in the game. The system
showcases effective integration of mechanical design, computer vision,
algorithm design, and robotic control to accomplish the complex task of
independently playing cards.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Kuan-Huang Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09480">
<title>TAB: Text-Align Anomaly Backbone Model for Industrial Inspection Tasks. (arXiv:2312.09480v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09480</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the focus on anomaly detection and localization in
industrial inspection tasks has intensified. While existing studies have
demonstrated impressive outcomes, they often rely heavily on extensive training
datasets or robust features extracted from pre-trained models trained on
diverse datasets like ImageNet. In this work, we propose a novel framework
leveraging the visual-linguistic CLIP model to adeptly train a backbone model
tailored to the manufacturing domain. Our approach concurrently considers
visual and text-aligned embedding spaces for normal and abnormal conditions.
The resulting pre-trained backbone markedly enhances performance in industrial
downstream tasks, particularly in anomaly detection and localization. Notably,
this improvement is substantiated through experiments conducted on multiple
datasets such as MVTecAD, BTAD, and KSDD2. Furthermore, using our pre-trained
backbone weights allows previous works to achieve superior performance in
few-shot scenarios with less training data. The proposed anomaly backbone
provides a foundation model for more precise anomaly detection and
localization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Ho-Weng Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1&quot;&gt;Shang-Hong Lai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09481">
<title>Continual Adversarial Defense. (arXiv:2312.09481v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09481</link>
<description rdf:parseType="Literal">&lt;p&gt;In response to the rapidly evolving nature of adversarial attacks on a
monthly basis, numerous defenses have been proposed to generalize against as
many known attacks as possible. However, designing a defense method that can
generalize to all types of attacks, including unseen ones, is not realistic
because the environment in which defense systems operate is dynamic and
comprises various unique attacks used by many attackers. The defense system
needs to upgrade itself by utilizing few-shot defense feedback and efficient
memory. Therefore, we propose the first continual adversarial defense (CAD)
framework that adapts to any attacks in a dynamic scenario, where various
attacks emerge stage by stage. In practice, CAD is modeled under four
principles: (1) continual adaptation to new attacks without catastrophic
forgetting, (2) few-shot adaptation, (3) memory-efficient adaptation, and (4)
high accuracy on both clean and adversarial images. We leverage cutting-edge
continual learning, few-shot learning, and ensemble learning techniques to
qualify the principles. Experiments conducted on CIFAR-10 and ImageNet-100
validate the effectiveness of our approach against multiple stages of 10 modern
adversarial attacks and significant improvements over 10 baseline methods. In
particular, CAD is capable of quickly adapting with minimal feedback and a low
cost of defense failure, while maintaining good performance against old
attacks. Our research sheds light on a brand-new paradigm for continual defense
adaptation against dynamic and evolving attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yaoyao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1&quot;&gt;Hefei Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qihao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Ping Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiazhong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1&quot;&gt;Alan Yuille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1&quot;&gt;Ning Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09486">
<title>Unraveling Batch Normalization for Realistic Test-Time Adaptation. (arXiv:2312.09486v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09486</link>
<description rdf:parseType="Literal">&lt;p&gt;While recent test-time adaptations exhibit efficacy by adjusting batch
normalization to narrow domain disparities, their effectiveness diminishes with
realistic mini-batches due to inaccurate target estimation. As previous
attempts merely introduce source statistics to mitigate this issue, the
fundamental problem of inaccurate target estimation still persists, leaving the
intrinsic test-time domain shifts unresolved. This paper delves into the
problem of mini-batch degradation. By unraveling batch normalization, we
discover that the inexact target statistics largely stem from the substantially
reduced class diversity in batch. Drawing upon this insight, we introduce a
straightforward tool, Test-time Exponential Moving Average (TEMA), to bridge
the class diversity gap between training and testing batches. Importantly, our
TEMA adaptively extends the scope of typical methods beyond the current batch
to incorporate a diverse set of class information, which in turn boosts an
accurate target estimation. Built upon this foundation, we further design a
novel layer-wise rectification strategy to consistently promote test-time
performance. Our proposed method enjoys a unique advantage as it requires
neither training nor tuning parameters, offering a truly hassle-free solution.
It significantly enhances model robustness against shifted domains and
maintains resilience in diverse real-world scenarios with various batch sizes,
achieving state-of-the-art performance on several major benchmarks. Code is
available at \url{https://github.com/kiwi12138/RealisticTTA}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1&quot;&gt;Zixian Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jingwei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1&quot;&gt;Kai Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qiufeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaizhu Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09496">
<title>Image Deblurring using GAN. (arXiv:2312.09496v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09496</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, deep generative models, such as Generative Adversarial
Network (GAN), has grabbed significant attention in the field of computer
vision. This project focuses on the application of GAN in image deblurring with
the aim of generating clearer images from blurry inputs caused by factors such
as motion blur. However, traditional image restoration techniques have
limitations in handling complex blurring patterns. Hence, a GAN-based framework
is proposed as a solution to generate high-quality deblurred images. The
project defines a GAN model in Tensorflow and trains it with GoPRO dataset. The
Generator will intake blur images directly to create fake images to convince
the Discriminator which will receive clear images at the same time and
distinguish between the real image and the fake image. After obtaining the
trained parameters, the model was used to deblur motion-blur images taken in
daily life as well as testing set for validation. The result shows that the
pretrained network of GAN can obtain sharper pixels in image, achieving an
average of 29.3 Peak Signal-to-Noise Ratio (PSNR) and 0.72 Structural
Similarity Assessment (SSIM). This help to effectively address the challenges
posed by image blurring, leading to the generation of visually pleasing and
sharp images. By exploiting the adversarial learning framework, the proposed
approach enhances the potential for real-world applications in image
restoration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhengdong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09501">
<title>EDA: Evolving and Distinct Anchors for Multimodal Motion Prediction. (arXiv:2312.09501v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09501</link>
<description rdf:parseType="Literal">&lt;p&gt;Motion prediction is a crucial task in autonomous driving, and one of its
major challenges lands in the multimodality of future behaviors. Many
successful works have utilized mixture models which require identification of
positive mixture components, and correspondingly fall into two main lines:
prediction-based and anchor-based matching. The prediction clustering
phenomenon in prediction-based matching makes it difficult to pick
representative trajectories for downstream tasks, while the anchor-based
matching suffers from a limited regression capability. In this paper, we
introduce a novel paradigm, named Evolving and Distinct Anchors (EDA), to
define the positive and negative components for multimodal motion prediction
based on mixture models. We enable anchors to evolve and redistribute
themselves under specific scenes for an enlarged regression capacity.
Furthermore, we select distinct anchors before matching them with the ground
truth, which results in impressive scoring performance. Our approach enhances
all metrics compared to the baseline MTR, particularly with a notable relative
reduction of 13.5% in Miss Rate, resulting in state-of-the-art performance on
the Waymo Open Motion Dataset. Code is available at
https://github.com/Longzhong-Lin/EDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Longzhong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xuewu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tianwei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lichao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1&quot;&gt;Rong Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09507">
<title>WAVER: Writing-style Agnostic Video Retrieval via Distilling Vision-Language Models Through Open-Vocabulary Knowledge. (arXiv:2312.09507v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09507</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-video retrieval, a prominent sub-field within the broader domain of
multimedia content management, has witnessed remarkable growth and innovation
over the past decade. However, existing methods assume the video scenes are
consistent and the description annotators are unbiased. These limitations fail
to align with fluid real-world scenarios, and descriptions can be influenced by
annotator biases, diverse writing styles, and varying textual perspectives. To
overcome the aforementioned problems, we introduce WAVER, a cross-domain
knowledge distillation mechanism designed to tackle the challenge of handling
writing-style agnostics. WAVER capitalizes on the open-vocabulary properties
inherent in pre-trained vision-language models and employs an implicit
knowledge distillation approach to transfer text-based knowledge from a teacher
model to a vision-based student. Empirical studies conducted across four
standard benchmark datasets, encompassing various settings, provide compelling
evidence that \WAVER can achieve state-of-the-art performance in text-video
retrieval tasks while handling writing-style variations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1&quot;&gt;Huy Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kieu_T/0/1/0/all/0/1&quot;&gt;Tung Kieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngan Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09510">
<title>Fast Sampling generative model for Ultrasound image reconstruction. (arXiv:2312.09510v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09510</link>
<description rdf:parseType="Literal">&lt;p&gt;Image reconstruction from radio-frequency data is pivotal in ultrafast plane
wave ultrasound imaging. Unlike the conventional delay-and-sum (DAS) technique,
which relies on somewhat imprecise assumptions, deep learning-based methods
perform image reconstruction by training on paired data, leading to a notable
enhancement in image quality. Nevertheless, these strategies often exhibit
limited generalization capabilities. Recently, denoising diffusion models have
become the preferred paradigm for image reconstruction tasks. However, their
reliance on an iterative sampling procedure results in prolonged generation
time. In this paper, we propose a novel sampling framework that concurrently
enforces data consistency of ultrasound signals and data-driven priors. By
leveraging the advanced diffusion model, the generation of high-quality images
is substantially expedited. Experimental evaluations on an in-vivo dataset
indicate that our approach with a single plane wave surpasses DAS with spatial
coherent compounding of 75 plane waves.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1&quot;&gt;Hengrong Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qiong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jianwen Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09514">
<title>Single PW takes a shortcut to compound PW in US imaging. (arXiv:2312.09514v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09514</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstruction of ultrasound (US) images from radio-frequency data can be
conceptualized as a linear inverse problem. Traditional deep learning
approaches, which aim to improve the quality of US images by directly learning
priors, often encounter challenges in generalization. Recently, diffusion-based
generative models have received significant attention within the research
community due to their robust performance in image reconstruction tasks.
However, a limitation of these models is their inherent low speed in generating
image samples from pure Gaussian noise progressively. In this study, we exploit
the inherent similarity between the US images reconstructed from a single plane
wave (PW) and PW compounding PWC). We hypothesize that a single PW can take a
shortcut to reach the diffusion trajectory of PWC, removing the need to begin
with Gaussian noise. By employing an advanced diffusion model, we demonstrate
its effectiveness in US image reconstruction, achieving a substantial reduction
in sampling steps. In-vivo experimental results indicate that our approach can
reduce sampling steps by 60%, while preserving comparable performance metrics
with the conventional diffusion model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1&quot;&gt;Hengrong Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lijie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qiong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jianwen Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09520">
<title>SlowTrack: Increasing the Latency of Camera-based Perception in Autonomous Driving Using Adversarial Examples. (arXiv:2312.09520v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09520</link>
<description rdf:parseType="Literal">&lt;p&gt;In Autonomous Driving (AD), real-time perception is a critical component
responsible for detecting surrounding objects to ensure safe driving. While
researchers have extensively explored the integrity of AD perception due to its
safety and security implications, the aspect of availability (real-time
performance) or latency has received limited attention. Existing works on
latency-based attack have focused mainly on object detection, i.e., a component
in camera-based AD perception, overlooking the entire camera-based AD
perception, which hinders them to achieve effective system-level effects, such
as vehicle crashes. In this paper, we propose SlowTrack, a novel framework for
generating adversarial attacks to increase the execution time of camera-based
AD perception. We propose a novel two-stage attack strategy along with the
three new loss function designs. Our evaluation is conducted on four popular
camera-based AD perception pipelines, and the results demonstrate that
SlowTrack significantly outperforms existing latency-based attacks while
maintaining comparable imperceptibility levels. Furthermore, we perform the
evaluation on Baidu Apollo, an industry-grade full-stack AD system, and LGSVL,
a production-grade AD simulator, with two scenarios to compare the system-level
effects of SlowTrack and existing attacks. Our evaluation results show that the
system-level effects can be significantly improved, i.e., the vehicle crash
rate of SlowTrack is around 95% on average while existing works only have
around 30%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chen Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Ningfei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qi Alfred Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chao Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09523">
<title>DriveTrack: A Benchmark for Long-Range Point Tracking in Real-World Videos. (arXiv:2312.09523v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09523</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents DriveTrack, a new benchmark and data generation framework
for long-range keypoint tracking in real-world videos. DriveTrack is motivated
by the observation that the accuracy of state-of-the-art trackers depends
strongly on visual attributes around the selected keypoints, such as texture
and lighting. The problem is that these artifacts are especially pronounced in
real-world videos, but these trackers are unable to train on such scenes due to
a dearth of annotations. DriveTrack bridges this gap by building a framework to
automatically annotate point tracks on autonomous driving datasets. We release
a dataset consisting of 1 billion point tracks across 24 hours of video, which
is seven orders of magnitude greater than prior real-world benchmarks and on
par with the scale of synthetic benchmarks. DriveTrack unlocks new use cases
for point tracking in real-world videos. First, we show that fine-tuning
keypoint trackers on DriveTrack improves accuracy on real-world scenes by up to
7%. Second, we analyze the sensitivity of trackers to visual artifacts in real
scenes and motivate the idea of running assistive keypoint selectors alongside
trackers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasingam_A/0/1/0/all/0/1&quot;&gt;Arjun Balasingam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandler_J/0/1/0/all/0/1&quot;&gt;Joseph Chandler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenning Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhoutong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balakrishnan_H/0/1/0/all/0/1&quot;&gt;Hari Balakrishnan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09525">
<title>Hierarchical Graph Pattern Understanding for Zero-Shot VOS. (arXiv:2312.09525v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09525</link>
<description rdf:parseType="Literal">&lt;p&gt;The optical flow guidance strategy is ideal for obtaining motion information
of objects in the video. It is widely utilized in video segmentation tasks.
However, existing optical flow-based methods have a significant dependency on
optical flow, which results in poor performance when the optical flow
estimation fails for a particular scene. The temporal consistency provided by
the optical flow could be effectively supplemented by modeling in a structural
form. This paper proposes a new hierarchical graph neural network (GNN)
architecture, dubbed hierarchical graph pattern understanding (HGPU), for
zero-shot video object segmentation (ZS-VOS). Inspired by the strong ability of
GNNs in capturing structural relations, HGPU innovatively leverages motion cues
(\ie, optical flow) to enhance the high-order representations from the
neighbors of target frames. Specifically, a hierarchical graph pattern encoder
with message aggregation is introduced to acquire different levels of motion
and appearance features in a sequential manner. Furthermore, a decoder is
designed for hierarchically parsing and understanding the transformed
multi-modal contexts to achieve more accurate and robust results. HGPU achieves
state-of-the-art performance on four publicly available benchmarks (DAVIS-16,
YouTube-Objects, Long-Videos and DAVIS-17). Code and pre-trained model can be
found at \url{https://github.com/NUST-Machine-Intelligence-Laboratory/HGPU}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_G/0/1/0/all/0/1&quot;&gt;Gensheng Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1&quot;&gt;Fumin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yazhou Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1&quot;&gt;Xian-Sheng Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Heng-Tao Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09527">
<title>TIFace: Improving Facial Reconstruction through Tensorial Radiance Fields and Implicit Surfaces. (arXiv:2312.09527v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09527</link>
<description rdf:parseType="Literal">&lt;p&gt;This report describes the solution that secured the first place in the &quot;View
Synthesis Challenge for Human Heads (VSCHH)&quot; at the ICCV 2023 workshop. Given
the sparse view images of human heads, the objective of this challenge is to
synthesize images from novel viewpoints. Due to the complexity of textures on
the face and the impact of lighting, the baseline method TensoRF yields results
with significant artifacts, seriously affecting facial reconstruction. To
address this issue, we propose TI-Face, which improves facial reconstruction
through tensorial radiance fields (T-Face) and implicit surfaces (I-Face),
respectively. Specifically, we employ an SAM-based approach to obtain the
foreground mask, thereby filtering out intense lighting in the background.
Additionally, we design mask-based constraints and sparsity constraints to
eliminate rendering artifacts effectively. The experimental results demonstrate
the effectiveness of the proposed improvements and superior performance of our
method on face reconstruction. The code will be available at
https://github.com/RuijieZhu94/TI-Face.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1&quot;&gt;Ruijie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1&quot;&gt;Jiahao Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Ziyang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jiahuan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianzhu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09529">
<title>Can Physician Judgment Enhance Model Trustworthiness? A Case Study on Predicting Pathological Lymph Nodes in Rectal Cancer. (arXiv:2312.09529v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.09529</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainability is key to enhancing artificial intelligence&apos;s trustworthiness
in medicine. However, several issues remain concerning the actual benefit of
explainable models for clinical decision-making. Firstly, there is a lack of
consensus on an evaluation framework for quantitatively assessing the practical
benefits that effective explainability should provide to practitioners.
Secondly, physician-centered evaluations of explainability are limited.
Thirdly, the utility of built-in attention mechanisms in transformer-based
models as an explainability technique is unclear. We hypothesize that superior
attention maps should align with the information that physicians focus on,
potentially reducing prediction uncertainty and increasing model reliability.
We employed a multimodal transformer to predict lymph node metastasis in rectal
cancer using clinical data and magnetic resonance imaging, exploring how well
attention maps, visualized through a state-of-the-art technique, can achieve
agreement with physician understanding. We estimated the model&apos;s uncertainty
using meta-level information like prediction probability variance and
quantified agreement. Our assessment of whether this agreement reduces
uncertainty found no significant effect. In conclusion, this case study did not
confirm the anticipated benefit of attention maps in enhancing model
reliability. Superficial explanations could do more harm than good by
misleading physicians into relying on uncertain predictions, suggesting that
the current state of attention mechanisms in explainability should not be
overestimated. Identifying explainability mechanisms truly beneficial for
clinical decision-making remains essential.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kobayashi_K/0/1/0/all/0/1&quot;&gt;Kazuma Kobayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Takamizawa_Y/0/1/0/all/0/1&quot;&gt;Yasuyuki Takamizawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Miyake_M/0/1/0/all/0/1&quot;&gt;Mototaka Miyake&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ito_S/0/1/0/all/0/1&quot;&gt;Sono Ito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gu_L/0/1/0/all/0/1&quot;&gt;Lin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nakatsuka_T/0/1/0/all/0/1&quot;&gt;Tatsuya Nakatsuka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Akagi_Y/0/1/0/all/0/1&quot;&gt;Yu Akagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Harada_T/0/1/0/all/0/1&quot;&gt;Tatsuya Harada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kanemitsu_Y/0/1/0/all/0/1&quot;&gt;Yukihide Kanemitsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hamamoto_R/0/1/0/all/0/1&quot;&gt;Ryuji Hamamoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09533">
<title>Adversarial Robustness on Image Classification with $k$-means. (arXiv:2312.09533v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09533</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we explore the challenges and strategies for enhancing the
robustness of $k$-means clustering algorithms against adversarial
manipulations. We evaluate the vulnerability of clustering algorithms to
adversarial attacks, emphasising the associated security risks. Our study
investigates the impact of incremental attack strength on training, introduces
the concept of transferability between supervised and unsupervised models, and
highlights the sensitivity of unsupervised models to sample distributions. We
additionally introduce and evaluate an adversarial training method that
improves testing performance in adversarial scenarios, and we highlight the
importance of various parameters in the proposed training method, such as
continuous learning, centroid initialisation, and adversarial step-count.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omari_R/0/1/0/all/0/1&quot;&gt;Rollin Omari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junae Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montague_P/0/1/0/all/0/1&quot;&gt;Paul Montague&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09534">
<title>WeatherProof: A Paired-Dataset Approach to Semantic Segmentation in Adverse Weather. (arXiv:2312.09534v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09534</link>
<description rdf:parseType="Literal">&lt;p&gt;The introduction of large, foundational models to computer vision has led to
drastically improved performance on the task of semantic segmentation. However,
these existing methods exhibit a large performance drop when testing on images
degraded by weather conditions such as rain, fog, or snow. We introduce a
general paired-training method that can be applied to all current foundational
model architectures that leads to improved performance on images in adverse
weather conditions. To this end, we create the WeatherProof Dataset, the first
semantic segmentation dataset with accurate clear and adverse weather image
pairs, which not only enables our new training paradigm, but also improves the
evaluation of the performance gap between clear and degraded segmentation. We
find that training on these paired clear and adverse weather frames which share
an underlying scene results in improved performance on adverse weather data.
With this knowledge, we propose a training pipeline which accentuates the
advantages of paired-data training using consistency losses and language
guidance, which leads to performance improvements by up to 18.4% as compared to
standard training procedures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gella_B/0/1/0/all/0/1&quot;&gt;Blake Gella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Howard Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Upadhyay_R/0/1/0/all/0/1&quot;&gt;Rishi Upadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1&quot;&gt;Tiffany Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waliman_M/0/1/0/all/0/1&quot;&gt;Matthew Waliman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ba_Y/0/1/0/all/0/1&quot;&gt;Yunhao Ba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alex Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadambi_A/0/1/0/all/0/1&quot;&gt;Achuta Kadambi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09538">
<title>AEGIS-Net: Attention-guided Multi-Level Feature Aggregation for Indoor Place Recognition. (arXiv:2312.09538v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09538</link>
<description rdf:parseType="Literal">&lt;p&gt;We present AEGIS-Net, a novel indoor place recognition model that takes in
RGB point clouds and generates global place descriptors by aggregating
lower-level color, geometry features and higher-level implicit semantic
features. However, rather than simple feature concatenation, self-attention
modules are employed to select the most important local features that best
describe an indoor place. Our AEGIS-Net is made of a semantic encoder, a
semantic decoder and an attention-guided feature embedding. The model is
trained in a 2-stage process with the first stage focusing on an auxiliary
semantic segmentation task and the second one on the place recognition task. We
evaluate our AEGIS-Net on the ScanNetPR dataset and compare its performance
with a pre-deep-learning feature-based method and five state-of-the-art
deep-learning-based methods. Our AEGIS-Net achieves exceptional performance and
outperforms all six methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1&quot;&gt;Yuhang Ming&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xingrui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Weichen Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1&quot;&gt;Wanzeng Kong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09551">
<title>Learning-based Axial Motion Magnification. (arXiv:2312.09551v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.09551</link>
<description rdf:parseType="Literal">&lt;p&gt;Video motion magnification amplifies invisible small motions to be
perceptible, which provides humans with spatially dense and holistic
understanding about small motions from the scene of interest. This is based on
the premise that magnifying small motions enhances the legibility of the
motion. In the real world, however, vibrating objects often possess complex
systems, having complex natural frequencies, modes, and directions. Existing
motion magnification often fails to improve the legibility since the intricate
motions still retain complex characteristics even when magnified, which
distracts us from analyzing them. In this work, we focus on improving the
legibility by proposing a new concept, axial motion magnification, which
magnifies decomposed motions along the user-specified direction. Axial motion
magnification can be applied to various applications where motions of specific
axes are critical, by providing simplified and easily readable motion
information. We propose a novel learning-based axial motion magnification
method with the Motion Separation Module that enables to disentangle and
magnify the motion representation along axes of interest. Further, we build a
new synthetic training dataset for the axial motion magnification task. Our
proposed method improves the legibility of resulting motions along certain
axes, while adding additional user controllability. Our method can be directly
adopted to the generic motion magnification and achieves favorable performance
against competing methods. Our project page is available at
https://axial-momag.github.io/axial-momag/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Byung_Ki_K/0/1/0/all/0/1&quot;&gt;Kwon Byung-Ki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hyun_Bin_O/0/1/0/all/0/1&quot;&gt;Oh Hyun-Bin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jun_Seong_K/0/1/0/all/0/1&quot;&gt;Kim Jun-Seong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oh_T/0/1/0/all/0/1&quot;&gt;Tae-Hyun Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09553">
<title>Prompt-based Distribution Alignment for Unsupervised Domain Adaptation. (arXiv:2312.09553v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09553</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, despite the unprecedented success of large pre-trained
visual-language models (VLMs) on a wide range of downstream tasks, the
real-world unsupervised domain adaptation (UDA) problem is still not well
explored. Therefore, in this paper, we first experimentally demonstrate that
the unsupervised-trained VLMs can significantly reduce the distribution
discrepancy between source and target domains, thereby improving the
performance of UDA. However, a major challenge for directly deploying such
models on downstream UDA tasks is prompt engineering, which requires aligning
the domain knowledge of source and target domains, since the performance of UDA
is severely influenced by a good domain-invariant representation. We further
propose a Prompt-based Distribution Alignment (PDA) method to incorporate the
domain knowledge into prompt learning. Specifically, PDA employs a two-branch
prompt-tuning paradigm, namely base branch and alignment branch. The base
branch focuses on integrating class-related representation into prompts,
ensuring discrimination among different classes. To further minimize domain
discrepancy, for the alignment branch, we construct feature banks for both the
source and target domains and propose image-guided feature tuning (IFT) to make
the input attend to feature banks, which effectively integrates self-enhanced
and cross-domain features into the model. In this way, these two branches can
be mutually promoted to enhance the adaptation of VLMs for UDA. We conduct
extensive experiments on three benchmarks to demonstrate that our proposed PDA
achieves state-of-the-art performance. The code is available at
https://github.com/BaiShuanghao/Prompt-based-Distribution-Alignment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1&quot;&gt;Shuanghao Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wanqi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Siteng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luan_Z/0/1/0/all/0/1&quot;&gt;Zhirong Luan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Donglin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Badong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09554">
<title>Embodied Adversarial Attack: A Dynamic Robust Physical Attack in Autonomous Driving. (arXiv:2312.09554v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09554</link>
<description rdf:parseType="Literal">&lt;p&gt;As physical adversarial attacks become extensively applied in unearthing the
potential risk of security-critical scenarios, especially in autonomous
driving, their vulnerability to environmental changes has also been brought to
light. The non-robust nature of physical adversarial attack methods brings
less-than-stable performance consequently. To enhance the robustness of
physical adversarial attacks in the real world, instead of statically
optimizing a robust adversarial example via an off-line training manner like
the existing methods, this paper proposes a brand new robust adversarial attack
framework: Embodied Adversarial Attack (EAA) from the perspective of dynamic
adaptation, which aims to employ the paradigm of embodied intelligence:
Perception-Decision-Control to dynamically adjust the optimal attack strategy
according to the current situations in real time. For the perception module,
given the challenge of needing simulation for the victim&apos;s viewpoint, EAA
innovatively devises a Perspective Transformation Network to estimate the
target&apos;s transformation from the attacker&apos;s perspective. For the decision and
control module, EAA adopts the laser-a highly manipulable medium to implement
physical attacks, and further trains an attack agent with reinforcement
learning to make it capable of instantaneously determining the best attack
strategy based on the perceived information. Finally, we apply our framework to
the autonomous driving scenario. A variety of experiments verify the high
effectiveness of our method under complex scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yitong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xingxing Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09558">
<title>Towards Transferable Targeted 3D Adversarial Attack in the Physical World. (arXiv:2312.09558v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09558</link>
<description rdf:parseType="Literal">&lt;p&gt;Compared with transferable untargeted attacks, transferable targeted
adversarial attacks could specify the misclassification categories of
adversarial samples, posing a greater threat to security-critical tasks. In the
meanwhile, 3D adversarial samples, due to their potential of multi-view
robustness, can more comprehensively identify weaknesses in existing deep
learning systems, possessing great application value. However, the field of
transferable targeted 3D adversarial attacks remains vacant. The goal of this
work is to develop a more effective technique that could generate transferable
targeted 3D adversarial examples, filling the gap in this field. To achieve
this goal, we design a novel framework named TT3D that could rapidly
reconstruct from few multi-view images into Transferable Targeted 3D textured
meshes. While existing mesh-based texture optimization methods compute
gradients in the high-dimensional mesh space and easily fall into local optima,
leading to unsatisfactory transferability and distinct distortions, TT3D
innovatively performs dual optimization towards both feature grid and
Multi-layer Perceptron (MLP) parameters in the grid-based NeRF space, which
significantly enhances black-box transferability while enjoying naturalness.
Experimental results show that TT3D not only exhibits superior cross-model
transferability but also maintains considerable adaptability across different
renders and vision tasks. More importantly, we produce 3D adversarial examples
with 3D printing techniques in the real world and verify their robust
performance under various scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_S/0/1/0/all/0/1&quot;&gt;Shouwei Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xingxing Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09570">
<title>CAGE: Controllable Articulation GEneration. (arXiv:2312.09570v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09570</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the challenge of generating 3D articulated objects in a
controllable fashion. Currently, modeling articulated 3D objects is either
achieved through laborious manual authoring, or using methods from prior work
that are hard to scale and control directly. We leverage the interplay between
part shape, connectivity, and motion using a denoising diffusion-based method
with attention modules designed to extract correlations between part
attributes. Our method takes an object category label and a part connectivity
graph as input and generates an object&apos;s geometry and motion parameters. The
generated objects conform to user-specified constraints on the object category,
part shape, and part articulation. Our experiments show that our method
outperforms the state-of-the-art in articulated object generation, producing
more realistic objects while conforming better to user constraints.
&lt;/p&gt;
&lt;p&gt;Video Summary at: &lt;a href=&quot;http://youtu.be/cH_rbKbyTpE&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiayi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tam_H/0/1/0/all/0/1&quot;&gt;Hou In Ivan Tam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1&quot;&gt;Ali Mahdavi-Amiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1&quot;&gt;Manolis Savva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09576">
<title>SegRap2023: A Benchmark of Organs-at-Risk and Gross Tumor Volume Segmentation for Radiotherapy Planning of Nasopharyngeal Carcinoma. (arXiv:2312.09576v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.09576</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiation therapy is a primary and effective NasoPharyngeal Carcinoma (NPC)
treatment strategy. The precise delineation of Gross Tumor Volumes (GTVs) and
Organs-At-Risk (OARs) is crucial in radiation treatment, directly impacting
patient prognosis. Previously, the delineation of GTVs and OARs was performed
by experienced radiation oncologists. Recently, deep learning has achieved
promising results in many medical image segmentation tasks. However, for NPC
OARs and GTVs segmentation, few public datasets are available for model
development and evaluation. To alleviate this problem, the SegRap2023 challenge
was organized in conjunction with MICCAI2023 and presented a large-scale
benchmark for OAR and GTV segmentation with 400 Computed Tomography (CT) scans
from 200 NPC patients, each with a pair of pre-aligned non-contrast and
contrast-enhanced CT scans. The challenge&apos;s goal was to segment 45 OARs and 2
GTVs from the paired CT scans. In this paper, we detail the challenge and
analyze the solutions of all participants. The average Dice similarity
coefficient scores for all submissions ranged from 76.68\% to 86.70\%, and
70.42\% to 73.44\% for OARs and GTVs, respectively. We conclude that the
segmentation of large-size OARs is well-addressed, and more efforts are needed
for GTVs and small-size or thin-structure OARs. The benchmark will remain
publicly available here: https://segrap2023.grand-challenge.org
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiangde Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jia Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yunxin Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shuolin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Bing Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Astaraki_M/0/1/0/all/0/1&quot;&gt;Mehdi Astaraki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bendazzoli_S/0/1/0/all/0/1&quot;&gt;Simone Bendazzoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Toma_Dasu_I/0/1/0/all/0/1&quot;&gt;Iuliana Toma-Dasu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yiwen Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Ziyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yong Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yanzhou Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jin Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junjun He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xing_Z/0/1/0/all/0/1&quot;&gt;Zhaohu Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongqiu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kaixiang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fang_X/0/1/0/all/0/1&quot;&gt;Xin Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chan Woong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sang Joon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chun_J/0/1/0/all/0/1&quot;&gt;Jaehee Chun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ulrich_C/0/1/0/all/0/1&quot;&gt;Constantin Ulrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maier_Hein_K/0/1/0/all/0/1&quot;&gt;Klaus H. Maier-Hein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ndipenoch_N/0/1/0/all/0/1&quot;&gt;Nchongmaje Ndipenoch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Miron_A/0/1/0/all/0/1&quot;&gt;Alina Miron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yongmin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yimeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Lu Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jinlong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+An_C/0/1/0/all/0/1&quot;&gt;Chengyang An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lisheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaiwen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yunqi Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shichuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liao_W/0/1/0/all/0/1&quot;&gt;Wenjun Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guotai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaoting Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09579">
<title>MobileSAMv2: Faster Segment Anything to Everything. (arXiv:2312.09579v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09579</link>
<description rdf:parseType="Literal">&lt;p&gt;Segment anything model (SAM) addresses two practical yet challenging
segmentation tasks: \textbf{segment anything (SegAny)}, which utilizes a
certain point to predict the mask for a single object of interest, and
\textbf{segment everything (SegEvery)}, which predicts the masks for all
objects on the image. What makes SegAny slow for SAM is its heavyweight image
encoder, which has been addressed by MobileSAM via decoupled knowledge
distillation. The efficiency bottleneck of SegEvery with SAM, however, lies in
its mask decoder because it needs to first generate numerous masks with
redundant grid-search prompts and then perform filtering to obtain the final
valid masks. We propose to improve its efficiency by directly generating the
final masks with only valid prompts, which can be obtained through object
discovery. Our proposed approach not only helps reduce the total time on the
mask decoder by at least 16 times but also achieves superior performance.
Specifically, our approach yields an average performance boost of 3.6\% (42.5\%
\textit{v.s.} 38.9\%) for zero-shot object proposal on the LVIS dataset with
the mask AR@$K$ metric. Qualitative results show that our approach generates
fine-grained masks while avoiding over-segmenting things. This project
targeting faster SegEvery than the original SAM is termed MobileSAMv2 to
differentiate from MobileSAM which targets faster SegAny. Moreover, we
demonstrate that our new prompt sampling is also compatible with the distilled
image encoders in MobileSAM, contributing to a unified framework for efficient
SegAny and SegEvery. The code is available at the same link as MobileSAM
Project
\href{https://github.com/ChaoningZhang/MobileSAM}{\textcolor{red}{https://github.com/ChaoningZhang/MobileSAM}}.
\end{abstract}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaoning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1&quot;&gt;Dongshen Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Sheng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jinwoo Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Tae-Ho Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1&quot;&gt;Choong Seon Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09584">
<title>Multiscale Vision Transformer With Deep Clustering-Guided Refinement for Weakly Supervised Object Localization. (arXiv:2312.09584v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09584</link>
<description rdf:parseType="Literal">&lt;p&gt;This work addresses the task of weakly-supervised object localization. The
goal is to learn object localization using only image-level class labels, which
are much easier to obtain compared to bounding box annotations. This task is
important because it reduces the need for labor-intensive ground-truth
annotations. However, methods for object localization trained using weak
supervision often suffer from limited accuracy in localization. To address this
challenge and enhance localization accuracy, we propose a multiscale object
localization transformer (MOLT). It comprises multiple object localization
transformers that extract patch embeddings across various scales. Moreover, we
introduce a deep clustering-guided refinement method that further enhances
localization accuracy by utilizing separately extracted image segments. These
segments are obtained by clustering pixels using convolutional neural networks.
Finally, we demonstrate the effectiveness of our proposed method by conducting
experiments on the publicly available ILSVRC-2012 dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;David Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1&quot;&gt;Sinhae Cha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1&quot;&gt;Byeongkeun Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09589">
<title>Improving Cross-domain Few-shot Classification with Multilayer Perceptron. (arXiv:2312.09589v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09589</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-domain few-shot classification (CDFSC) is a challenging and tough task
due to the significant distribution discrepancies across different domains. To
address this challenge, many approaches aim to learn transferable
representations. Multilayer perceptron (MLP) has shown its capability to learn
transferable representations in various downstream tasks, such as unsupervised
image classification and supervised concept generalization. However, its
potential in the few-shot settings has yet to be comprehensively explored. In
this study, we investigate the potential of MLP to assist in addressing the
challenges of CDFSC. Specifically, we introduce three distinct frameworks
incorporating MLP in accordance with three types of few-shot classification
methods to verify the effectiveness of MLP. We reveal that MLP can
significantly enhance discriminative capabilities and alleviate distribution
shifts, which can be supported by our expensive experiments involving 10
baseline models and 12 benchmark datasets. Furthermore, our method even
compares favorably against other state-of-the-art CDFSC algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1&quot;&gt;Shuanghao Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wanqi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luan_Z/0/1/0/all/0/1&quot;&gt;Zhirong Luan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Donglin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Badong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09595">
<title>Density Matters: Improved Core-set for Active Domain Adaptive Segmentation. (arXiv:2312.09595v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09595</link>
<description rdf:parseType="Literal">&lt;p&gt;Active domain adaptation has emerged as a solution to balance the expensive
annotation cost and the performance of trained models in semantic segmentation.
However, existing works usually ignore the correlation between selected samples
and its local context in feature space, which leads to inferior usage of
annotation budgets. In this work, we revisit the theoretical bound of the
classical Core-set method and identify that the performance is closely related
to the local sample distribution around selected samples. To estimate the
density of local samples efficiently, we introduce a local proxy estimator with
Dynamic Masked Convolution and develop a Density-aware Greedy algorithm to
optimize the bound. Extensive experiments demonstrate the superiority of our
approach. Moreover, with very few labels, our scheme achieves comparable
performance to the fully supervised counterpart.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shizhan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhengkai Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuxi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jinlong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yabiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weiyao Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09598">
<title>CLAF: Contrastive Learning with Augmented Features for Imbalanced Semi-Supervised Learning. (arXiv:2312.09598v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09598</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the advantages of leveraging unlabeled data and learning meaningful
representations, semi-supervised learning and contrastive learning have been
progressively combined to achieve better performances in popular applications
with few labeled data and abundant unlabeled data. One common manner is
assigning pseudo-labels to unlabeled samples and selecting positive and
negative samples from pseudo-labeled samples to apply contrastive learning.
However, the real-world data may be imbalanced, causing pseudo-labels to be
biased toward the majority classes and further undermining the effectiveness of
contrastive learning. To address the challenge, we propose Contrastive Learning
with Augmented Features (CLAF). We design a class-dependent feature
augmentation module to alleviate the scarcity of minority class samples in
contrastive learning. For each pseudo-labeled sample, we select positive and
negative samples from labeled data instead of unlabeled data to compute
contrastive loss. Comprehensive experiments on imbalanced image classification
datasets demonstrate the effectiveness of CLAF in the context of imbalanced
semi-supervised learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_B/0/1/0/all/0/1&quot;&gt;Bowen Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin-Chun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1&quot;&gt;De-Chuan Zhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09608">
<title>Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models. (arXiv:2312.09608v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09608</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the key components within diffusion models is the UNet for noise
prediction. While several works have explored basic properties of the UNet
decoder, its encoder largely remains unexplored. In this work, we conduct the
first comprehensive study of the UNet encoder. We empirically analyze the
encoder features and provide insights to important questions regarding their
changes at the inference process. In particular, we find that encoder features
change gently, whereas the decoder features exhibit substantial variations
across different time-steps. This finding inspired us to omit the encoder at
certain adjacent time-steps and reuse cyclically the encoder features in the
previous time-steps for the decoder. Further based on this observation, we
introduce a simple yet effective encoder propagation scheme to accelerate the
diffusion sampling for a diverse set of tasks. By benefiting from our
propagation scheme, we are able to perform in parallel the decoder at certain
adjacent time-steps. Additionally, we introduce a prior noise injection method
to improve the texture details in the generated image. Besides the standard
text-to-image task, we also validate our approach on other tasks:
text-to-video, personalized generation and reference-guided generation. Without
utilizing any knowledge distillation technique, our approach accelerates both
the Stable Diffusion (SD) and the DeepFloyd-IF models sampling by 41$\%$ and
24$\%$ respectively, while maintaining high-quality generation performance. Our
code is available in
\href{https://github.com/hutaiHang/Faster-Diffusion}{FasterDiffusion}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Senmao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1&quot;&gt;Taihang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shiqi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaxing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Ming-Ming Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09609">
<title>Semantic-Aware Transformation-Invariant RoI Align. (arXiv:2312.09609v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09609</link>
<description rdf:parseType="Literal">&lt;p&gt;Great progress has been made in learning-based object detection methods in
the last decade. Two-stage detectors often have higher detection accuracy than
one-stage detectors, due to the use of region of interest (RoI) feature
extractors which extract transformation-invariant RoI features for different
RoI proposals, making refinement of bounding boxes and prediction of object
categories more robust and accurate. However, previous RoI feature extractors
can only extract invariant features under limited transformations. In this
paper, we propose a novel RoI feature extractor, termed Semantic RoI Align
(SRA), which is capable of extracting invariant RoI features under a variety of
transformations for two-stage detectors. Specifically, we propose a semantic
attention module to adaptively determine different sampling areas by leveraging
the global and local semantic relationship within the RoI. We also propose a
Dynamic Feature Sampler which dynamically samples features based on the RoI
aspect ratio to enhance the efficiency of SRA, and a new position embedding,
\ie Area Embedding, to provide more accurate position information for SRA
through an improved sampling area representation. Experiments show that our
model significantly outperforms baseline models with slight computational
overhead. In addition, it shows excellent generalization ability and can be
used to improve performance with various state-of-the-art backbones and
detection methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guo-Ye Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakayama_G/0/1/0/all/0/1&quot;&gt;George Kiyohiro Nakayama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zi-Kai Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1&quot;&gt;Tai-Jiang Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaolei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shi-Min Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09612">
<title>TOP-ReID: Multi-spectral Object Re-Identification with Token Permutation. (arXiv:2312.09612v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09612</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-spectral object Re-identification (ReID) aims to retrieve specific
objects by leveraging complementary information from different image spectra.
It delivers great advantages over traditional single-spectral ReID in complex
visual environment. However, the significant distribution gap among different
image spectra poses great challenges for effective multi-spectral feature
representations. In addition, most of current Transformer-based ReID methods
only utilize the global feature of class tokens to achieve the holistic
retrieval, ignoring the local discriminative ones. To address the above issues,
we step further to utilize all the tokens of Transformers and propose a cyclic
token permutation framework for multi-spectral object ReID, dubbled TOP-ReID.
More specifically, we first deploy a multi-stream deep network based on vision
Transformers to preserve distinct information from different image spectra.
Then, we propose a Token Permutation Module (TPM) for cyclic multi-spectral
feature aggregation. It not only facilitates the spatial feature alignment
across different image spectra, but also allows the class token of each
spectrum to perceive the local details of other spectra. Meanwhile, we propose
a Complementary Reconstruction Module (CRM), which introduces dense token-level
reconstruction constraints to reduce the distribution gap across different
image spectra. With the above modules, our proposed framework can generate more
discriminative multi-spectral features for robust object ReID. Extensive
experiments on three ReID benchmarks (i.e., RGBNT201, RGBNT100 and MSVR310)
verify the effectiveness of our methods. The code is available at
https://github.&lt;a href=&quot;/abs/com/9249732&quot;&gt;com/9249732&lt;/a&gt;92/TOP-ReID.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuehu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pingping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhengzheng Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Huchuan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09625">
<title>Weakly-Supervised 3D Visual Grounding based on Visual Linguistic Alignment. (arXiv:2312.09625v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09625</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning to ground natural language queries to target objects or regions in
3D point clouds is quite essential for 3D scene understanding. Nevertheless,
existing 3D visual grounding approaches require a substantial number of
bounding box annotations for text queries, which is time-consuming and
labor-intensive to obtain. In this paper, we propose \textbf{3D-VLA}, a weakly
supervised approach for \textbf{3D} visual grounding based on \textbf{V}isual
\textbf{L}inguistic \textbf{A}lignment. Our 3D-VLA exploits the superior
ability of current large-scale vision-language models (VLMs) on aligning the
semantics between texts and 2D images, as well as the naturally existing
correspondences between 2D images and 3D point clouds, and thus implicitly
constructs correspondences between texts and 3D point clouds with no need for
fine-grained box annotations in the training procedure. During the inference
stage, the learned text-3D correspondence will help us ground the text queries
to the 3D target objects even without 2D images. To the best of our knowledge,
this is the first work to investigate 3D visual grounding in a weakly
supervised manner by involving large scale vision-language models, and
extensive experiments on ReferIt3D and ScanRefer datasets demonstrate that our
3D-VLA achieves comparable and even superior results over the fully supervised
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaoxu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yitian Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiudan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenhui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1&quot;&gt;Zequn Jie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09627">
<title>TF-CLIP: Learning Text-free CLIP for Video-based Person Re-Identification. (arXiv:2312.09627v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09627</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale language-image pre-trained models (e.g., CLIP) have shown
superior performances on many cross-modal retrieval tasks. However, the problem
of transferring the knowledge learned from such models to video-based person
re-identification (ReID) has barely been explored. In addition, there is a lack
of decent text descriptions in current ReID benchmarks. To address these
issues, in this work, we propose a novel one-stage text-free CLIP-based
learning framework named TF-CLIP for video-based person ReID. More
specifically, we extract the identity-specific sequence feature as the
CLIP-Memory to replace the text feature. Meanwhile, we design a
Sequence-Specific Prompt (SSP) module to update the CLIP-Memory online. To
capture temporal information, we further propose a Temporal Memory Diffusion
(TMD) module, which consists of two key components: Temporal Memory
Construction (TMC) and Memory Diffusion (MD). Technically, TMC allows the
frame-level memories in a sequence to communicate with each other, and to
extract temporal information based on the relations within the sequence. MD
further diffuses the temporal memories to each token in the original features
to obtain more robust sequence features. Extensive experiments demonstrate that
our proposed method shows much better results than other state-of-the-art
methods on MARS, LS-VID and iLIDS-VID. The code is available at
https://github.com/AsuradaYuci/TF-CLIP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chenyang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuehu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yingquan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pingping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Huchuan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09630">
<title>Pixel-Superpixel Contrastive Learning and Pseudo-Label Correction for Hyperspectral Image Clustering. (arXiv:2312.09630v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09630</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperspectral image (HSI) clustering is gaining considerable attention owing
to recent methods that overcome the inefficiency and misleading results from
the absence of supervised information. Contrastive learning methods excel at
existing pixel level and super pixel level HSI clustering tasks. The
pixel-level contrastive learning method can effectively improve the ability of
the model to capture fine features of HSI but requires a large time overhead.
The super pixel-level contrastive learning method utilizes the homogeneity of
HSI and reduces computing resources; however, it yields rough classification
results. To exploit the strengths of both methods, we present a pixel super
pixel contrastive learning and pseudo-label correction (PSCPC) method for the
HSI clustering. PSCPC can reasonably capture domain-specific and fine-grained
features through super pixels and the comparative learning of a small number of
pixels within the super pixels. To improve the clustering performance of super
pixels, this paper proposes a pseudo-label correction module that aligns the
clustering pseudo-labels of pixels and super-pixels. In addition, pixel-level
clustering results are used to supervise super pixel-level clustering,
improving the generalization ability of the model. Extensive experiments
demonstrate the effectiveness and efficiency of PSCPC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_R/0/1/0/all/0/1&quot;&gt;Renxiang Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zihao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xianju Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chang Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09641">
<title>Ins-HOI: Instance Aware Human-Object Interactions Recovery. (arXiv:2312.09641v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09641</link>
<description rdf:parseType="Literal">&lt;p&gt;Recovering detailed interactions between humans/hands and objects is an
appealing yet challenging task. Existing methods typically use template-based
representations to track human/hand and objects in interactions. Despite the
progress, they fail to handle the invisible contact surfaces. In this paper, we
propose Ins-HOI, an end-to-end solution to recover human/hand-object
reconstruction via instance-level implicit reconstruction. To this end, we
introduce an instance-level occupancy field to support simultaneous human/hand
and object representation, and a complementary training strategy to handle the
lack of instance-level ground truths. Such a representation enables learning a
contact prior implicitly from sparse observations. During the complementary
training, we augment the real-captured data with synthesized data by randomly
composing individual scans of humans/hands and objects and intentionally
allowing for penetration. In this way, our network learns to recover individual
shapes as completely as possible from the synthesized data, while being aware
of the contact constraints and overall reasonability based on real-captured
scans. As demonstrated in experiments, our method Ins-HOI can produce
reasonable and realistic non-visible contact surfaces even in cases of
extremely close interaction. To facilitate the research of this task, we
collect a large-scale, high-fidelity 3D scan dataset, including 5.2k
high-quality scans with real-world human-chair and hand-object interactions. We
will release our dataset and source codes. Data examples and the video results
of our method can be found on the project page.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiajun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongwen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Boyao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1&quot;&gt;Ruizhi Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zonghai Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yebin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09673">
<title>Style Generation in Robot Calligraphy with Deep Generative Adversarial Networks. (arXiv:2312.09673v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09673</link>
<description rdf:parseType="Literal">&lt;p&gt;Robot calligraphy is an emerging exploration of artificial intelligence in
the fields of art and education. Traditional calligraphy generation researches
mainly focus on methods such as tool-based image processing, generative models,
and style transfer. Unlike the English alphabet, the number of Chinese
characters is tens of thousands, which leads to difficulties in the generation
of a style consistent Chinese calligraphic font with over 6000 characters. Due
to the lack of high-quality data sets, formal definitions of calligraphy
knowledge, and scientific art evaluation methods, The results generated are
frequently of low quality and falls short of professional-level requirements.
To address the above problem, this paper proposes an automatic calligraphy
generation model based on deep generative adversarial networks (deepGAN) that
can generate style calligraphy fonts with professional standards. The key
highlights of the proposed method include: (1) The datasets use a
high-precision calligraphy synthesis method to ensure its high quality and
sufficient quantity; (2) Professional calligraphers are invited to conduct a
series of Turing tests to evaluate the gap between model generation results and
human artistic level; (3) Experimental results indicate that the proposed model
is the state-of-the-art among current calligraphy generation methods. The
Turing tests and similarity evaluations validate the effectiveness of the
proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1&quot;&gt;Zhiguo Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09676">
<title>nuScenes Knowledge Graph -- A comprehensive semantic representation of traffic scenes for trajectory prediction. (arXiv:2312.09676v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09676</link>
<description rdf:parseType="Literal">&lt;p&gt;Trajectory prediction in traffic scenes involves accurately forecasting the
behaviour of surrounding vehicles. To achieve this objective it is crucial to
consider contextual information, including the driving path of vehicles, road
topology, lane dividers, and traffic rules. Although studies demonstrated the
potential of leveraging heterogeneous context for improving trajectory
prediction, state-of-the-art deep learning approaches still rely on a limited
subset of this information. This is mainly due to the limited availability of
comprehensive representations. This paper presents an approach that utilizes
knowledge graphs to model the diverse entities and their semantic connections
within traffic scenes. Further, we present nuScenes Knowledge Graph (nSKG), a
knowledge graph for the nuScenes dataset, that models explicitly all scene
participants and road elements, as well as their semantic and spatial
relationships. To facilitate the usage of the nSKG via graph neural networks
for trajectory prediction, we provide the data in a format, ready-to-use by the
PyG library. All artefacts can be found here:
https://github.com/boschresearch/nuScenes_Knowledge_Graph
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mlodzian_L/0/1/0/all/0/1&quot;&gt;Leon Mlodzian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhigang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berkemeyer_H/0/1/0/all/0/1&quot;&gt;Hendrik Berkemeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monka_S/0/1/0/all/0/1&quot;&gt;Sebastian Monka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zixu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dietze_S/0/1/0/all/0/1&quot;&gt;Stefan Dietze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halilaj_L/0/1/0/all/0/1&quot;&gt;Lavdim Halilaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luettin_J/0/1/0/all/0/1&quot;&gt;Juergen Luettin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09681">
<title>Urban Region Embedding via Multi-View Contrastive Prediction. (arXiv:2312.09681v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09681</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, learning urban region representations utilizing multi-modal data
(information views) has become increasingly popular, for deep understanding of
the distributions of various socioeconomic features in cities. However,
previous methods usually blend multi-view information in a posteriors stage,
falling short in learning coherent and consistent representations across
different views. In this paper, we form a new pipeline to learn consistent
representations across varying views, and propose the multi-view Contrastive
Prediction model for urban Region embedding (ReCP), which leverages the
multiple information views from point-of-interest (POI) and human mobility
data. Specifically, ReCP comprises two major modules, namely an intra-view
learning module utilizing contrastive learning and feature reconstruction to
capture the unique information from each single view, and inter-view learning
module that perceives the consistency between the two views using a contrastive
prediction learning scheme. We conduct thorough experiments on two downstream
tasks to assess the proposed model, i.e., land use clustering and region
popularity prediction. The experimental results demonstrate that our model
outperforms state-of-the-art baseline methods significantly in urban region
representation learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zechen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weiming Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Kai Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Min Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yongshun Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Meng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09682">
<title>Exploring the Feasibility of Generating Realistic 3D Models of Endangered Species Using DreamGaussian: An Analysis of Elevation Angle&apos;s Impact on Model Generation. (arXiv:2312.09682v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09682</link>
<description rdf:parseType="Literal">&lt;p&gt;Many species face the threat of extinction. It&apos;s important to study these
species and gather information about them as much as possible to preserve
biodiversity. Due to the rarity of endangered species, there is a limited
amount of data available, making it difficult to apply data requiring
generative AI methods to this domain. We aim to study the feasibility of
generating consistent and real-like 3D models of endangered animals using
limited data. Such a phenomenon leads us to utilize zero-shot stable diffusion
models that can generate a 3D model out of a single image of the target
species. This paper investigates the intricate relationship between elevation
angle and the output quality of 3D model generation, focusing on the innovative
approach presented in DreamGaussian. DreamGaussian, a novel framework utilizing
Generative Gaussian Splatting along with novel mesh extraction and refinement
algorithms, serves as the focal point of our study. We conduct a comprehensive
analysis, analyzing the effect of varying elevation angles on DreamGaussian&apos;s
ability to reconstruct 3D scenes accurately. Through an empirical evaluation,
we demonstrate how changes in elevation angle impact the generated images&apos;
spatial coherence, structural integrity, and perceptual realism. We observed
that giving a correct elevation angle with the input image significantly
affects the result of the generated 3D model. We hope this study to be
influential for the usability of AI to preserve endangered animals; while the
penultimate aim is to obtain a model that can output biologically consistent 3D
models via small samples, the qualitative interpretation of an existing
state-of-the-art model such as DreamGaussian will be a step forward in our
goal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karatopak_S/0/1/0/all/0/1&quot;&gt;Selcuk Anil Karatopak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_D/0/1/0/all/0/1&quot;&gt;Deniz Sen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09709">
<title>ParsNets: A Parsimonious Orthogonal and Low-Rank Linear Networks for Zero-Shot Learning. (arXiv:2312.09709v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09709</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides a novel parsimonious yet efficient design for zero-shot
learning (ZSL), dubbed ParsNets, where we are interested in learning a
composition of on-device friendly linear networks, each with orthogonality and
low-rankness properties, to achieve equivalent or even better performance
against existing deep models. Concretely, we first refactor the core module of
ZSL, i.e., visual-semantics mapping function, into several base linear networks
that correspond to diverse components of the semantic space, where the complex
nonlinearity can be collapsed into simple local linearities. Then, to
facilitate the generalization of local linearities, we construct a maximal
margin geometry on the learned features by enforcing low-rank constraints on
intra-class samples and high-rank constraints on inter-class samples, resulting
in orthogonal subspaces for different classes and each subspace lies on a
compact manifold. To enhance the model&apos;s adaptability and counterbalance
over/under-fittings in ZSL, a set of sample-wise indicators is employed to
select a sparse subset from these base linear networks to form a composite
semantic predictor for each sample. Notably, maximal margin geometry can
guarantee the diversity of features, and meanwhile, local linearities guarantee
efficiency. Thus, our ParsNets can generalize better to unseen classes and can
be deployed flexibly on resource-constrained devices. Theoretical explanations
and extensive experiments are conducted to verify the effectiveness of the
proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jingcai Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qihua Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruibing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiaocheng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09716">
<title>Let All be Whitened: Multi-teacher Distillation for Efficient Visual Retrieval. (arXiv:2312.09716v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09716</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual retrieval aims to search for the most relevant visual items, e.g.,
images and videos, from a candidate gallery with a given query item. Accuracy
and efficiency are two competing objectives in retrieval tasks. Instead of
crafting a new method pursuing further improvement on accuracy, in this paper
we propose a multi-teacher distillation framework Whiten-MTD, which is able to
transfer knowledge from off-the-shelf pre-trained retrieval models to a
lightweight student model for efficient visual retrieval. Furthermore, we
discover that the similarities obtained by different retrieval models are
diversified and incommensurable, which makes it challenging to jointly distill
knowledge from multiple models. Therefore, we propose to whiten the output of
teacher models before fusion, which enables effective multi-teacher
distillation for retrieval models. Whiten-MTD is conceptually simple and
practically effective. Extensive experiments on two landmark image retrieval
datasets and one video retrieval dataset demonstrate the effectiveness of our
proposed method, and its good balance of retrieval performance and efficiency.
Our source code is released at https://github.com/Maryeon/whiten_mtd.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhe Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jianfeng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shouling Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhenguang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zonghui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Sifeng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_F/0/1/0/all/0/1&quot;&gt;Feng Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaobo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09719">
<title>On the calibration of neural networks for histological slide-level classification. (arXiv:2312.09719v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.09719</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks have shown promising classification performance when
predicting certain biomarkers from Whole Slide Images in digital pathology.
However, the calibration of the networks&apos; output probabilities is often not
evaluated. Communicating uncertainty by providing reliable confidence scores is
of high relevance in the medical context. In this work, we compare three neural
network architectures that combine feature representations on patch-level to a
slide-level prediction with respect to their classification performance and
evaluate their calibration. As slide-level classification task, we choose the
prediction of Microsatellite Instability from Colorectal Cancer tissue
sections. We observe that Transformers lead to good results in terms of
classification performance and calibration. When evaluating the classification
performance on a separate dataset, we observe that Transformers generalize
best. The investigation of reliability diagrams provides additional insights to
the Expected Calibration Error metric and we observe that especially
Transformers push the output probabilities to extreme values, which results in
overconfident predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kurz_A/0/1/0/all/0/1&quot;&gt;Alexander Kurz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mehrtens_H/0/1/0/all/0/1&quot;&gt;Hendrik A. Mehrtens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bucher_T/0/1/0/all/0/1&quot;&gt;Tabea-Clara Bucher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brinker_T/0/1/0/all/0/1&quot;&gt;Titus J. Brinker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09723">
<title>Tracking Skiers from the Top to the Bottom. (arXiv:2312.09723v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09723</link>
<description rdf:parseType="Literal">&lt;p&gt;Skiing is a popular winter sport discipline with a long history of
competitive events. In this domain, computer vision has the potential to
enhance the understanding of athletes&apos; performance, but its application lags
behind other sports due to limited studies and datasets. This paper makes a
step forward in filling such gaps. A thorough investigation is performed on the
task of skier tracking in a video capturing his/her complete performance.
Obtaining continuous and accurate skier localization is preemptive for further
higher-level performance analyses. To enable the study, the largest and most
annotated dataset for computer vision in skiing, SkiTB, is introduced. Several
visual object tracking algorithms, including both established methodologies and
a newly introduced skier-optimized baseline algorithm, are tested using the
dataset. The results provide valuable insights into the applicability of
different tracking methods for vision-based skiing analysis. SkiTB, code, and
results are available at https://machinelearning.uniud.it/datasets/skitb.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dunnhofer_M/0/1/0/all/0/1&quot;&gt;Matteo Dunnhofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sordi_L/0/1/0/all/0/1&quot;&gt;Luca Sordi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinel_N/0/1/0/all/0/1&quot;&gt;Niki Martinel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Micheloni_C/0/1/0/all/0/1&quot;&gt;Christian Micheloni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09727">
<title>LiteVSR: Efficient Visual Speech Recognition by Learning from Speech Representations of Unlabeled Data. (arXiv:2312.09727v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09727</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel, resource-efficient approach to Visual Speech
Recognition (VSR) leveraging speech representations produced by any trained
Automatic Speech Recognition (ASR) model. Moving away from the
resource-intensive trends prevalent in recent literature, our method distills
knowledge from a trained Conformer-based ASR model, achieving competitive
performance on standard VSR benchmarks with significantly less resource
utilization. Using unlabeled audio-visual data only, our baseline model
achieves a word error rate (WER) of 47.4% and 54.7% on the LRS2 and LRS3 test
benchmarks, respectively. After fine-tuning the model with limited labeled
data, the word error rate reduces to 35% (LRS2) and 45.7% (LRS3). Our model can
be trained on a single consumer-grade GPU within a few days and is capable of
performing real-time end-to-end VSR on dated hardware, suggesting a path
towards more accessible and resource-efficient VSR methodologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laux_H/0/1/0/all/0/1&quot;&gt;Hendrik Laux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mededovic_E/0/1/0/all/0/1&quot;&gt;Emil Mededovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hallawa_A/0/1/0/all/0/1&quot;&gt;Ahmed Hallawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1&quot;&gt;Lukas Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peine_A/0/1/0/all/0/1&quot;&gt;Arne Peine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmeink_A/0/1/0/all/0/1&quot;&gt;Anke Schmeink&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09743">
<title>SLS4D: Sparse Latent Space for 4D Novel View Synthesis. (arXiv:2312.09743v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09743</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural radiance field (NeRF) has achieved great success in novel view
synthesis and 3D representation for static scenarios. Existing dynamic NeRFs
usually exploit a locally dense grid to fit the deformation field; however,
they fail to capture the global dynamics and concomitantly yield models of
heavy parameters. We observe that the 4D space is inherently sparse. Firstly,
the deformation field is sparse in spatial but dense in temporal due to the
continuity of of motion. Secondly, the radiance field is only valid on the
surface of the underlying scene, usually occupying a small fraction of the
whole space. We thus propose to represent the 4D scene using a learnable sparse
latent space, a.k.a. SLS4D. Specifically, SLS4D first uses dense learnable time
slot features to depict the temporal space, from which the deformation field is
fitted with linear multi-layer perceptions (MLP) to predict the displacement of
a 3D position at any time. It then learns the spatial features of a 3D position
using another sparse latent space. This is achieved by learning the adaptive
weights of each latent code with the attention mechanism. Extensive experiments
demonstrate the effectiveness of our SLS4D: it achieves the best 4D novel view
synthesis using only about $6\%$ parameters of the most recent work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1&quot;&gt;Qi-Yuan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao-Xiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qun-Ce Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1&quot;&gt;Tai-Jiang Mu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09750">
<title>Attention-Based VR Facial Animation with Visual Mouth Camera Guidance for Immersive Telepresence Avatars. (arXiv:2312.09750v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09750</link>
<description rdf:parseType="Literal">&lt;p&gt;Facial animation in virtual reality environments is essential for
applications that necessitate clear visibility of the user&apos;s face and the
ability to convey emotional signals. In our scenario, we animate the face of an
operator who controls a robotic Avatar system. The use of facial animation is
particularly valuable when the perception of interacting with a specific
individual, rather than just a robot, is intended. Purely keypoint-driven
animation approaches struggle with the complexity of facial movements. We
present a hybrid method that uses both keypoints and direct visual guidance
from a mouth camera. Our method generalizes to unseen operators and requires
only a quick enrolment step with capture of two short videos. Multiple source
images are selected with the intention to cover different facial expressions.
Given a mouth camera frame from the HMD, we dynamically construct the target
keypoints and apply an attention mechanism to determine the importance of each
source image. To resolve keypoint ambiguities and animate a broader range of
mouth expressions, we propose to inject visual mouth camera information into
the latent space. We enable training on large-scale speaking head datasets by
simulating the mouth camera input with its perspective differences and facial
deformations. Our method outperforms a baseline in quality, capability, and
temporal consistency. In addition, we highlight how the facial animation
contributed to our victory at the ANA Avatar XPRIZE Finals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rochow_A/0/1/0/all/0/1&quot;&gt;Andre Rochow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwarz_M/0/1/0/all/0/1&quot;&gt;Max Schwarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1&quot;&gt;Sven Behnke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09754">
<title>PPFM: Image denoising in photon-counting CT using single-step posterior sampling Poisson flow generative models. (arXiv:2312.09754v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.09754</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion and Poisson flow models have shown impressive performance in a wide
range of generative tasks, including low-dose CT image denoising. However, one
limitation in general, and for clinical applications in particular, is slow
sampling. Due to their iterative nature, the number of function evaluations
(NFE) required is usually on the order of $10-10^3$, both for conditional and
unconditional generation. In this paper, we present posterior sampling Poisson
flow generative models (PPFM), a novel image denoising technique for low-dose
and photon-counting CT that produces excellent image quality whilst keeping
NFE=1. Updating the training and sampling processes of Poisson flow generative
models (PFGM)++, we learn a conditional generator which defines a trajectory
between the prior noise distribution and the posterior distribution of
interest. We additionally hijack and regularize the sampling process to achieve
NFE=1. Our results shed light on the benefits of the PFGM++ framework compared
to diffusion models. In addition, PPFM is shown to perform favorably compared
to current state-of-the-art diffusion-style models with NFE=1, consistency
models, as well as popular deep learning and non-deep learning-based image
denoising techniques, on clinical low-dose CT images and clinical images from a
prototype photon-counting CT system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hein_D/0/1/0/all/0/1&quot;&gt;Dennis Hein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Holmin_S/0/1/0/all/0/1&quot;&gt;Staffan Holmin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Szczykutowicz_T/0/1/0/all/0/1&quot;&gt;Timothy Szczykutowicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maltz_J/0/1/0/all/0/1&quot;&gt;Jonathan S Maltz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Danielsson_M/0/1/0/all/0/1&quot;&gt;Mats Danielsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Ge Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Persson_M/0/1/0/all/0/1&quot;&gt;Mats Persson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09767">
<title>DreamTalk: When Expressive Talking Head Generation Meets Diffusion Probabilistic Models. (arXiv:2312.09767v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09767</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have shown remarkable success in a variety of downstream
generative tasks, yet remain under-explored in the important and challenging
expressive talking head generation. In this work, we propose a DreamTalk
framework to fulfill this gap, which employs meticulous design to unlock the
potential of diffusion models in generating expressive talking heads.
Specifically, DreamTalk consists of three crucial components: a denoising
network, a style-aware lip expert, and a style predictor. The diffusion-based
denoising network is able to consistently synthesize high-quality audio-driven
face motions across diverse expressions. To enhance the expressiveness and
accuracy of lip motions, we introduce a style-aware lip expert that can guide
lip-sync while being mindful of the speaking styles. To eliminate the need for
expression reference video or text, an extra diffusion-based style predictor is
utilized to predict the target expression directly from the audio. By this
means, DreamTalk can harness powerful diffusion models to generate expressive
faces effectively and reduce the reliance on expensive style references.
Experimental results demonstrate that DreamTalk is capable of generating
photo-realistic talking faces with diverse speaking styles and achieving
accurate lip motions, surpassing existing state-of-the-art counterparts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yifeng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shiwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiayu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yingya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhidong Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09780">
<title>RANRAC: Robust Neural Scene Representations via Random Ray Consensus. (arXiv:2312.09780v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09780</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce RANRAC, a robust reconstruction algorithm for 3D objects
handling occluded and distracted images, which is a particularly challenging
scenario that prior robust reconstruction methods cannot deal with. Our
solution supports single-shot reconstruction by involving light-field networks,
and is also applicable to photo-realistic, robust, multi-view reconstruction
from real-world images based on neural radiance fields. While the algorithm
imposes certain limitations on the scene representation and, thereby, the
supported scene types, it reliably detects and excludes inconsistent
perspectives, resulting in clean images without floating artifacts. Our
solution is based on a fuzzy adaption of the random sample consensus paradigm,
enabling its application to large scale models. We interpret the minimal number
of samples to determine the model parameters as a tunable hyperparameter. This
is applicable, as a cleaner set of samples improves reconstruction quality.
Further, this procedure also handles outliers. Especially for conditioned
models, it can result in the same local minimum in the latent space as would be
obtained with a completely clean set. We report significant improvements for
novel-view synthesis in occluded scenarios, of up to 8dB PSNR compared to the
baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buschmann_B/0/1/0/all/0/1&quot;&gt;Benno Buschmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dogaru_A/0/1/0/all/0/1&quot;&gt;Andreea Dogaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisemann_E/0/1/0/all/0/1&quot;&gt;Elmar Eisemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinmann_M/0/1/0/all/0/1&quot;&gt;Michael Weinmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Egger_B/0/1/0/all/0/1&quot;&gt;Bernhard Egger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09783">
<title>Keep the Faith: Faithful Explanations in Convolutional Neural Networks for Case-Based Reasoning. (arXiv:2312.09783v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09783</link>
<description rdf:parseType="Literal">&lt;p&gt;Explaining predictions of black-box neural networks is crucial when applied
to decision-critical tasks. Thus, attribution maps are commonly used to
identify important image regions, despite prior work showing that humans prefer
explanations based on similar examples. To this end, ProtoPNet learns a set of
class-representative feature vectors (prototypes) for case-based reasoning.
During inference, similarities of latent features to prototypes are linearly
classified to form predictions and attribution maps are provided to explain the
similarity. In this work, we evaluate whether architectures for case-based
reasoning fulfill established axioms required for faithful explanations using
the example of ProtoPNet. We show that such architectures allow the extraction
of faithful explanations. However, we prove that the attribution maps used to
explain the similarities violate the axioms. We propose a new procedure to
extract explanations for trained ProtoPNets, named ProtoPFaith. Conceptually,
these explanations are Shapley values, calculated on the similarity scores of
each prototype. They allow to faithfully answer which prototypes are present in
an unseen image and quantify each pixel&apos;s contribution to that presence,
thereby complying with all axioms. The theoretical violations of ProtoPNet
manifest in our experiments on three datasets (CUB-200-2011, Stanford Dogs,
RSNA) and five architectures (ConvNet, ResNet, ResNet50, WideResNet50,
ResNeXt50). Our experiments show a qualitative difference between the
explanations given by ProtoPNet and ProtoPFaith. Additionally, we quantify the
explanations with the Area Over the Perturbation Curve, on which ProtoPFaith
outperforms ProtoPNet on all experiments by a factor $&amp;gt;10^3$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1&quot;&gt;Tom Nuno Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bongratz_F/0/1/0/all/0/1&quot;&gt;Fabian Bongratz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rickmann_A/0/1/0/all/0/1&quot;&gt;Anne-Marie Rickmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1&quot;&gt;Sebastian P&amp;#xf6;lsterl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1&quot;&gt;Christian Wachinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09788">
<title>Collaborating Foundation models for Domain Generalized Semantic Segmentation. (arXiv:2312.09788v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09788</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain Generalized Semantic Segmentation (DGSS) deals with training a model
on a labeled source domain with the aim of generalizing to unseen domains
during inference. Existing DGSS methods typically effectuate robust features by
means of Domain Randomization (DR). Such an approach is often limited as it can
only account for style diversification and not content. In this work, we take
an orthogonal approach to DGSS and propose to use an assembly of CoLlaborative
FOUndation models for Domain Generalized Semantic Segmentation (CLOUDS). In
detail, CLOUDS is a framework that integrates FMs of various kinds: (i) CLIP
backbone for its robust feature representation, (ii) generative models to
diversify the content, thereby covering various modes of the possible target
distribution, and (iii) Segment Anything Model (SAM) for iteratively refining
the predictions of the segmentation model. Extensive experiments show that our
CLOUDS excels in adapting from synthetic to real DGSS benchmarks and under
varying weather conditions, notably outperforming prior methods by 5.6% and
6.7% on averaged miou, respectively. The code is available at :
https://github.com/yasserben/CLOUDS
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benigmim_Y/0/1/0/all/0/1&quot;&gt;Yasser Benigmim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Subhankar Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Essid_S/0/1/0/all/0/1&quot;&gt;Slim Essid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalogeiton_V/0/1/0/all/0/1&quot;&gt;Vicky Kalogeiton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Lathuili&amp;#xe8;re&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09792">
<title>Latent Diffusion Models with Image-Derived Annotations for Enhanced AI-Assisted Cancer Diagnosis in Histopathology. (arXiv:2312.09792v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09792</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence (AI) based image analysis has an immense potential to
support diagnostic histopathology, including cancer diagnostics. However,
developing supervised AI methods requires large-scale annotated datasets. A
potentially powerful solution is to augment training data with synthetic data.
Latent diffusion models, which can generate high-quality, diverse synthetic
images, are promising. However, the most common implementations rely on
detailed textual descriptions, which are not generally available in this
domain. This work proposes a method that constructs structured textual prompts
from automatically extracted image features. We experiment with the PCam
dataset, composed of tissue patches only loosely annotated as healthy or
cancerous. We show that including image-derived features in the prompt, as
opposed to only healthy and cancerous labels, improves the Fr\&apos;echet Inception
Distance (FID) from 178.8 to 90.2. We also show that pathologists find it
challenging to detect synthetic images, with a median sensitivity/specificity
of 0.55/0.55. Finally, we show that synthetic data effectively trains AI
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osorio_P/0/1/0/all/0/1&quot;&gt;Pedro Osorio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jimenez_Perez_G/0/1/0/all/0/1&quot;&gt;Guillermo Jimenez-Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montalt_Tordera_J/0/1/0/all/0/1&quot;&gt;Javier Montalt-Tordera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hooge_J/0/1/0/all/0/1&quot;&gt;Jens Hooge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duran_Ballester_G/0/1/0/all/0/1&quot;&gt;Guillem Duran-Ballester&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Shivam Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radbruch_M/0/1/0/all/0/1&quot;&gt;Moritz Radbruch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bach_U/0/1/0/all/0/1&quot;&gt;Ute Bach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroeder_S/0/1/0/all/0/1&quot;&gt;Sabrina Schroeder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siudak_K/0/1/0/all/0/1&quot;&gt;Krystyna Siudak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vienenkoetter_J/0/1/0/all/0/1&quot;&gt;Julia Vienenkoetter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lawrenz_B/0/1/0/all/0/1&quot;&gt;Bettina Lawrenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1&quot;&gt;Sadegh Mohammadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09797">
<title>Part Representation Learning with Teacher-Student Decoder for Occluded Person Re-identification. (arXiv:2312.09797v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09797</link>
<description rdf:parseType="Literal">&lt;p&gt;Occluded person re-identification (ReID) is a very challenging task due to
the occlusion disturbance and incomplete target information. Leveraging
external cues such as human pose or parsing to locate and align part features
has been proven to be very effective in occluded person ReID. Meanwhile, recent
Transformer structures have a strong ability of long-range modeling.
Considering the above facts, we propose a Teacher-Student Decoder (TSD)
framework for occluded person ReID, which utilizes the Transformer decoder with
the help of human parsing. More specifically, our proposed TSD consists of a
Parsing-aware Teacher Decoder (PTD) and a Standard Student Decoder (SSD). PTD
employs human parsing cues to restrict Transformer&apos;s attention and imparts this
information to SSD through feature distillation. Thereby, SSD can learn from
PTD to aggregate information of body parts automatically. Moreover, a mask
generator is designed to provide discriminative regions for better ReID. In
addition, existing occluded person ReID benchmarks utilize occluded samples as
queries, which will amplify the role of alleviating occlusion interference and
underestimate the impact of the feature absence issue. Contrastively, we
propose a new benchmark with non-occluded queries, serving as a complement to
the existing benchmark. Extensive experiments demonstrate that our proposed
method is superior and the new benchmark is essential. The source codes are
available at https://github.com/hh23333/TSD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chenyang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pingping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Huchuan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09799">
<title>IQNet: Image Quality Assessment Guided Just Noticeable Difference Prefiltering For Versatile Video Coding. (arXiv:2312.09799v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.09799</link>
<description rdf:parseType="Literal">&lt;p&gt;Image prefiltering with just noticeable distortion (JND) improves coding
efficiency in a visual lossless way by filtering the perceptually redundant
information prior to compression. However, real JND cannot be well modeled with
inaccurate masking equations in traditional approaches or image-level subject
tests in deep learning approaches. Thus, this paper proposes a fine-grained JND
prefiltering dataset guided by image quality assessment for accurate
block-level JND modeling. The dataset is constructed from decoded images to
include coding effects and is also perceptually enhanced with block overlap and
edge preservation. Furthermore, based on this dataset, we propose a lightweight
JND prefiltering network, IQNet, which can be applied directly to different
quantization cases with the same model and only needs 3K parameters. The
experimental results show that the proposed approach to Versatile Video Coding
could yield maximum/average bitrate savings of 41\%/15\% and 53\%/19\% for
all-intra and low-delay P configurations, respectively, with negligible
subjective quality loss. Our method demonstrates higher perceptual quality and
a model size that is an order of magnitude smaller than previous deep learning
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yu-Han Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chiang Lo-Hsuan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_T/0/1/0/all/0/1&quot;&gt;Tian-Sheuan Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09800">
<title>Deep Event Visual Odometry. (arXiv:2312.09800v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09800</link>
<description rdf:parseType="Literal">&lt;p&gt;Event cameras offer the exciting possibility of tracking the camera&apos;s pose
during high-speed motion and in adverse lighting conditions. Despite this
promise, existing event-based monocular visual odometry (VO) approaches
demonstrate limited performance on recent benchmarks. To address this
limitation, some methods resort to additional sensors such as IMUs, stereo
event cameras, or frame-based cameras. Nonetheless, these additional sensors
limit the application of event cameras in real-world devices since they
increase cost and complicate system requirements. Moreover, relying on a
frame-based camera makes the system susceptible to motion blur and HDR. To
remove the dependency on additional sensors and to push the limits of using
only a single event camera, we present Deep Event VO (DEVO), the first
monocular event-only system with strong performance on a large number of
real-world benchmarks. DEVO sparsely tracks selected event patches over time. A
key component of DEVO is a novel deep patch selection mechanism tailored to
event data. We significantly decrease the pose tracking error on seven
real-world benchmarks by up to 97% compared to event-only methods and often
surpass or are close to stereo or inertial methods. Code is available at
https://github.com/tum-vision/DEVO
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klenk_S/0/1/0/all/0/1&quot;&gt;Simon Klenk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Motzet_M/0/1/0/all/0/1&quot;&gt;Marvin Motzet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koestler_L/0/1/0/all/0/1&quot;&gt;Lukas Koestler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1&quot;&gt;Daniel Cremers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09812">
<title>Structural Information Guided Multimodal Pre-training for Vehicle-centric Perception. (arXiv:2312.09812v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09812</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding vehicles in images is important for various applications such
as intelligent transportation and self-driving system. Existing vehicle-centric
works typically pre-train models on large-scale classification datasets and
then fine-tune them for specific downstream tasks. However, they neglect the
specific characteristics of vehicle perception in different tasks and might
thus lead to sub-optimal performance. To address this issue, we propose a novel
vehicle-centric pre-training framework called VehicleMAE, which incorporates
the structural information including the spatial structure from vehicle profile
information and the semantic structure from informative high-level natural
language descriptions for effective masked vehicle appearance reconstruction.
To be specific, we explicitly extract the sketch lines of vehicles as a form of
the spatial structure to guide vehicle reconstruction. The more comprehensive
knowledge distilled from the CLIP big model based on the similarity between the
paired/unpaired vehicle image-text sample is further taken into consideration
to help achieve a better understanding of vehicles. A large-scale dataset is
built to pre-train our model, termed Autobot1M, which contains about 1M vehicle
images and 12693 text information. Extensive experiments on four vehicle-based
downstream tasks fully validated the effectiveness of our VehicleMAE. The
source code and pre-trained models will be released at
https://github.com/Event-AHU/VehicleMAE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wentao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenglong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yukai Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jin Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09821">
<title>Fragility, Robustness and Antifragility in Deep Learning. (arXiv:2312.09821v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09821</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a systematic analysis of deep neural networks (DNNs) based on a
signal processing technique for network parameter removal, in the form of
synaptic filters that identifies the fragility, robustness and antifragility
characteristics of DNN parameters. Our proposed analysis investigates if the
DNN performance is impacted negatively, invariantly, or positively on both
clean and adversarially perturbed test datasets when the DNN undergoes synaptic
filtering. We define three \textit{filtering scores} for quantifying the
fragility, robustness and antifragility characteristics of DNN parameters based
on the performances for (i) clean dataset, (ii) adversarial dataset, and (iii)
the difference in performances of clean and adversarial datasets. We validate
the proposed systematic analysis on ResNet-18, ResNet-50, SqueezeNet-v1.1 and
ShuffleNet V2 x1.0 network architectures for MNIST, CIFAR10 and Tiny ImageNet
datasets. The filtering scores, for a given network architecture, identify
network parameters that are invariant in characteristics across different
datasets over learning epochs. Vice-versa, for a given dataset, the filtering
scores identify the parameters that are invariant in characteristics across
different network architectures. We show that our synaptic filtering method
improves the test accuracy of ResNet and ShuffleNet models on adversarial
datasets when only the robust and antifragile parameters are selectively
retrained at any given epoch, thus demonstrating applications of the proposed
strategy in improving model robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pravin_C/0/1/0/all/0/1&quot;&gt;Chandresh Pravin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martino_I/0/1/0/all/0/1&quot;&gt;Ivan Martino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicosia_G/0/1/0/all/0/1&quot;&gt;Giuseppe Nicosia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ojha_V/0/1/0/all/0/1&quot;&gt;Varun Ojha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09854">
<title>Q-Segment: Segmenting Images In-Sensor for Vessel-Based Medical Diagnosis. (arXiv:2312.09854v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.09854</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the growing interest in deploying deep learning models
directly in-sensor. We present &quot;Q-Segment&quot;, a quantized real-time segmentation
algorithm, and conduct a comprehensive evaluation on two low-power edge vision
platforms, namely Sony IMX500, which has an in-sensors processor, and Sony
Spresense, a low-power multi-core ARM Cortex-M microcontroller. One of the main
goals of the model is to achieve end-to-end image segmentation for vessel-based
medical diagnosis. Deployed on the IMX500 platform, Q-Segment achieves
ultra-low inference time in-sensor of only 1.9 ms and energy consumption of
only 5.7 mJ. We compare the proposed network with outperforming existing
networks on various platforms by a factor of 75x (compared to ERFNet). The
network architecture employs an encoder-decoder structure with skip
connections, and results in a binary accuracy of 97.25% and an Area Under the
Receiver Operating Characteristic Curve (AUC) of 96.97% on the CHASE dataset.
This research contributes valuable insights into edge-based image segmentation,
laying the foundation for efficient algorithms tailored to low-power
environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bonazzi_P/0/1/0/all/0/1&quot;&gt;Pietro Bonazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moosmann_J/0/1/0/all/0/1&quot;&gt;Julian Moosmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bian_S/0/1/0/all/0/1&quot;&gt;Sizhen Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Magno_M/0/1/0/all/0/1&quot;&gt;Michele Magno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09866">
<title>PLGSLAM: Progressive Neural Scene Represenation with Local to Global Bundle Adjustment. (arXiv:2312.09866v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09866</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural implicit scene representations have recently shown encouraging results
in dense visual SLAM. However, existing methods produce low-quality scene
reconstruction and low-accuracy localization performance when scaling up to
large indoor scenes and long sequences. These limitations are mainly due to
their single, global radiance field with finite capacity, which does not adapt
to large scenarios. Their end-to-end pose networks are also not robust enough
with the growth of cumulative errors in large scenes. To this end, we present
PLGSLAM, a neural visual SLAM system which performs high-fidelity surface
reconstruction and robust camera tracking in real time. To handle large-scale
indoor scenes, PLGSLAM proposes a progressive scene representation method which
dynamically allocates new local scene representation trained with frames within
a local sliding window. This allows us to scale up to larger indoor scenes and
improves robustness (even under pose drifts). In local scene representation,
PLGSLAM utilizes tri-planes for local high-frequency features. We also
incorporate multi-layer perceptron (MLP) networks for the low-frequency
feature, smoothness, and scene completion in unobserved areas. Moreover, we
propose local-to-global bundle adjustment method with a global keyframe
database to address the increased pose drifts on long sequences. Experimental
results demonstrate that PLGSLAM achieves state-of-the-art scene reconstruction
results and tracking performance across various datasets and scenarios (both in
small and large-scale indoor environments). The code will be open-sourced upon
paper acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_T/0/1/0/all/0/1&quot;&gt;Tianchen Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_G/0/1/0/all/0/1&quot;&gt;Guole Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1&quot;&gt;Tong Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wentao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingchuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Danwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weidong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09876">
<title>Automatic Image Colourizer. (arXiv:2312.09876v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09876</link>
<description rdf:parseType="Literal">&lt;p&gt;In this project we have designed and described a model which colourize a
gray-scale image, with no human intervention. We propose a fully automatic
process of colouring and re-colouring faded or gray-scale image with vibrant
and pragmatic colours. We have used Convolutional Neural Network to hallucinate
input images and feed-forwarded by training thousands of images. This approach
results in trailblazing results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1&quot;&gt;Aditya Parikh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09880">
<title>Information Extraction from Unstructured data using Augmented-AI and Computer Vision. (arXiv:2312.09880v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09880</link>
<description rdf:parseType="Literal">&lt;p&gt;Process of information extraction (IE) is often used to extract meaningful
information from unstructured and unlabeled data. Conventional methods of data
extraction including application of OCR and passing extraction engine, are
inefficient on large data and have their limitation. In this paper, a peculiar
technique of information extraction is proposed using A2I and computer vision
technologies, which also includes NLP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1&quot;&gt;Aditya Parikh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09894">
<title>PathoDuet: Foundation Models for Pathological Slide Analysis of H&amp;E and IHC Stains. (arXiv:2312.09894v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09894</link>
<description rdf:parseType="Literal">&lt;p&gt;Large amounts of digitized histopathological data display a promising future
for developing pathological foundation models via self-supervised learning
methods. Foundation models pretrained with these methods serve as a good basis
for downstream tasks. However, the gap between natural and histopathological
images hinders the direct application of existing methods. In this work, we
present PathoDuet, a series of pretrained models on histopathological images,
and a new self-supervised learning framework in histopathology. The framework
is featured by a newly-introduced pretext token and later task raisers to
explicitly utilize certain relations between images, like multiple
magnifications and multiple stains. Based on this, two pretext tasks,
cross-scale positioning and cross-stain transferring, are designed to pretrain
the model on Hematoxylin and Eosin (H\&amp;amp;E) images and transfer the model to
immunohistochemistry (IHC) images, respectively. To validate the efficacy of
our models, we evaluate the performance over a wide variety of downstream
tasks, including patch-level colorectal cancer subtyping and whole slide image
(WSI)-level classification in H\&amp;amp;E field, together with expression level
prediction of IHC marker and tumor identification in IHC field. The
experimental results show the superiority of our models over most tasks and the
efficacy of proposed pretext tasks. The codes and models are available at
https://github.com/openmedlab/PathoDuet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_S/0/1/0/all/0/1&quot;&gt;Shengyi Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_F/0/1/0/all/0/1&quot;&gt;Fang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1&quot;&gt;Tianle Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaofan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09899">
<title>SQA-SAM: Segmentation Quality Assessment for Medical Images Utilizing the Segment Anything Model. (arXiv:2312.09899v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.09899</link>
<description rdf:parseType="Literal">&lt;p&gt;Segmentation quality assessment (SQA) plays a critical role in the deployment
of a medical image based AI system. Users need to be informed/alerted whenever
an AI system generates unreliable/incorrect predictions. With the introduction
of the Segment Anything Model (SAM), a general foundation segmentation model,
new research opportunities emerged in how one can utilize SAM for medical image
segmentation. In this paper, we propose a novel SQA method, called SQA-SAM,
which exploits SAM to enhance the accuracy of quality assessment for medical
image segmentation. When a medical image segmentation model (MedSeg) produces
predictions for a test image, we generate visual prompts based on the
predictions, and SAM is utilized to generate segmentation maps corresponding to
the visual prompts. How well MedSeg&apos;s segmentation aligns with SAM&apos;s
segmentation indicates how well MedSeg&apos;s segmentation aligns with the general
perception of objectness and image region partition. We develop a score measure
for such alignment. In experiments, we find that the generated scores exhibit
moderate to strong positive correlation (in Pearson correlation and Spearman
correlation) with Dice coefficient scores reflecting the true segmentation
quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yizhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dou_Q/0/1/0/all/0/1&quot;&gt;Qi Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Danny Z. Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09909">
<title>TMP: Temporal Motion Propagation for Online Video Super-Resolution. (arXiv:2312.09909v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09909</link>
<description rdf:parseType="Literal">&lt;p&gt;Online video super-resolution (online-VSR) highly relies on an effective
alignment module to aggregate temporal information, while the strict latency
requirement makes accurate and efficient alignment very challenging. Though
much progress has been achieved, most of the existing online-VSR methods
estimate the motion fields of each frame separately to perform alignment, which
is computationally redundant and ignores the fact that the motion fields of
adjacent frames are correlated. In this work, we propose an efficient Temporal
Motion Propagation (TMP) method, which leverages the continuity of motion field
to achieve fast pixel-level alignment among consecutive frames. Specifically,
we first propagate the offsets from previous frames to the current frame, and
then refine them in the neighborhood, which significantly reduces the matching
space and speeds up the offset estimation process. Furthermore, to enhance the
robustness of alignment, we perform spatial-wise weighting on the warped
features, where the positions with more precise offsets are assigned higher
importance. Experiments on benchmark datasets demonstrate that the proposed TMP
method achieves leading online-VSR accuracy as well as inference speed. The
source code of TMP can be found at
\href{https://github.com/xtudbxk/TMP}{https://github.com/xtudbxk/TMP}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhengqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruihuang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Shi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09913">
<title>LAENeRF: Local Appearance Editing for Neural Radiance Fields. (arXiv:2312.09913v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09913</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the omnipresence of Neural Radiance Fields (NeRFs), the interest
towards editable implicit 3D representations has surged over the last years.
However, editing implicit or hybrid representations as used for NeRFs is
difficult due to the entanglement of appearance and geometry encoded in the
model parameters. Despite these challenges, recent research has shown first
promising steps towards photorealistic and non-photorealistic appearance edits.
The main open issues of related work include limited interactivity, a lack of
support for local edits and large memory requirements, rendering them less
useful in practice. We address these limitations with LAENeRF, a unified
framework for photorealistic and non-photorealistic appearance editing of
NeRFs. To tackle local editing, we leverage a voxel grid as starting point for
region selection. We learn a mapping from expected ray terminations to final
output color, which can optionally be supervised by a style loss, resulting in
a framework which can perform photorealistic and non-photorealistic appearance
editing of selected regions. Relying on a single point per ray for our mapping,
we limit memory requirements and enable fast optimization. To guarantee
interactivity, we compose the output color using a set of learned, modifiable
base colors, composed with additive layer mixing. Compared to concurrent work,
LAENeRF enables recoloring and stylization while keeping processing time low.
Furthermore, we demonstrate that our approach surpasses baseline methods both
quantitatively and qualitatively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radl_L/0/1/0/all/0/1&quot;&gt;Lukas Radl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steiner_M/0/1/0/all/0/1&quot;&gt;Michael Steiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurz_A/0/1/0/all/0/1&quot;&gt;Andreas Kurz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinberger_M/0/1/0/all/0/1&quot;&gt;Markus Steinberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09922">
<title>A Unifying Tensor View for Lightweight CNNs. (arXiv:2312.09922v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09922</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the decomposition of convolutional kernels for lightweight CNNs being
well studied, existing works that rely on tensor network diagrams or
hyperdimensional abstraction lack geometry intuition. This work devises a new
perspective by linking a 3D-reshaped kernel tensor to its various slice-wise
and rank-1 decompositions, permitting a straightforward connection between
various tensor approximations and efficient CNN modules. Specifically, it is
discovered that a pointwise-depthwise-pointwise (PDP) configuration constitutes
a viable construct for lightweight CNNs. Moreover, a novel link to the latest
ShiftNet is established, inspiring a first-ever shift layer pruning that
achieves nearly 50% compression with &amp;lt; 1% drop in accuracy for ShiftResNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jason Chun Lok Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1&quot;&gt;Rui Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiajun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_E/0/1/0/all/0/1&quot;&gt;Edmund Yin Mun Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_N/0/1/0/all/0/1&quot;&gt;Ngai Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09925">
<title>CNC-Net: Self-Supervised Learning for CNC Machining Operations. (arXiv:2312.09925v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09925</link>
<description rdf:parseType="Literal">&lt;p&gt;CNC manufacturing is a process that employs computer numerical control (CNC)
machines to govern the movements of various industrial tools and machinery,
encompassing equipment ranging from grinders and lathes to mills and CNC
routers. However, the reliance on manual CNC programming has become a
bottleneck, and the requirement for expert knowledge can result in significant
costs. Therefore, we introduce a pioneering approach named CNC-Net,
representing the use of deep neural networks (DNNs) to simulate CNC machines
and grasp intricate operations when supplied with raw materials. CNC-Net
constitutes a self-supervised framework that exclusively takes an input 3D
model and subsequently generates the essential operation parameters required by
the CNC machine to construct the object. Our method has the potential to
transformative automation in manufacturing by offering a cost-effective
alternative to the high costs of manual CNC programming while maintaining
exceptional precision in 3D object production. Our experiments underscore the
effectiveness of our CNC-Net in constructing the desired 3D objects through the
utilization of CNC operations. Notably, it excels in preserving finer local
details, exhibiting a marked enhancement in precision compared to the
state-of-the-art 3D CAD reconstruction approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yavartanoo_M/0/1/0/all/0/1&quot;&gt;Mohsen Yavartanoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Sangmin Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neshatavar_R/0/1/0/all/0/1&quot;&gt;Reyhaneh Neshatavar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kyoung Mu Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09935">
<title>LogoStyleFool: Vitiating Video Recognition Systems via Logo Style Transfer. (arXiv:2312.09935v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09935</link>
<description rdf:parseType="Literal">&lt;p&gt;Video recognition systems are vulnerable to adversarial examples. Recent
studies show that style transfer-based and patch-based unrestricted
perturbations can effectively improve attack efficiency. These attacks,
however, face two main challenges: 1) Adding large stylized perturbations to
all pixels reduces the naturalness of the video and such perturbations can be
easily detected. 2) Patch-based video attacks are not extensible to targeted
attacks due to the limited search space of reinforcement learning that has been
widely used in video attacks recently. In this paper, we focus on the video
black-box setting and propose a novel attack framework named LogoStyleFool by
adding a stylized logo to the clean video. We separate the attack into three
stages: style reference selection, reinforcement-learning-based logo style
transfer, and perturbation optimization. We solve the first challenge by
scaling down the perturbation range to a regional logo, while the second
challenge is addressed by complementing an optimization stage after
reinforcement learning. Experimental results substantiate the overall
superiority of LogoStyleFool over three state-of-the-art patch-based attacks in
terms of attack performance and semantic preservation. Meanwhile, LogoStyleFool
still maintains its performance against two existing patch-based defense
methods. We believe that our research is beneficial in increasing the attention
of the security community to such subregional style transfer attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yuxin Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Ziyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xi Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Derui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1&quot;&gt;Minhui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jin Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09955">
<title>DHFormer: A Vision Transformer-Based Attention Module for Image Dehazing. (arXiv:2312.09955v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09955</link>
<description rdf:parseType="Literal">&lt;p&gt;Images acquired in hazy conditions have degradations induced in them.
Dehazing such images is a vexed and ill-posed problem. Scores of prior-based
and learning-based approaches have been proposed to mitigate the effect of haze
and generate haze-free images. Many conventional methods are constrained by
their lack of awareness regarding scene depth and their incapacity to capture
long-range dependencies. In this paper, a method that uses residual learning
and vision transformers in an attention module is proposed. It essentially
comprises two networks: In the first one, the network takes the ratio of a hazy
image and the approximated transmission matrix to estimate a residual map. The
second network takes this residual image as input and passes it through
convolution layers before superposing it on the generated feature maps. It is
then passed through global context and depth-aware transformer encoders to
obtain channel attention. The attention module then infers the spatial
attention map before generating the final haze-free image. Experimental
results, including several quantitative metrics, demonstrate the efficiency and
scalability of the suggested methodology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasi_A/0/1/0/all/0/1&quot;&gt;Abdul Wasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shiney_O/0/1/0/all/0/1&quot;&gt;O. Jeba Shiney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09968">
<title>Human Perception-Inspired Grain Segmentation Refinement Using Conditional Random Fields. (arXiv:2312.09968v1 [cond-mat.mtrl-sci])</title>
<link>http://arxiv.org/abs/2312.09968</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate segmentation of interconnected line networks, such as grain
boundaries in polycrystalline material microstructures, poses a significant
challenge due to the fragmented masks produced by conventional computer vision
algorithms, including convolutional neural networks. These algorithms struggle
with thin masks, often necessitating intricate post-processing for effective
contour closure and continuity. Addressing this issue, this paper introduces a
fast, high-fidelity post-processing technique, leveraging domain knowledge
about grain boundary connectivity and employing conditional random fields and
perceptual grouping rules. This approach significantly enhances segmentation
mask accuracy, achieving a 79% segment identification accuracy in validation
with a U-Net model on electron microscopy images of a polycrystalline oxide.
Additionally, a novel grain alignment metric is introduced, showing a 51%
improvement in grain alignment, providing a more detailed assessment of
segmentation performance for complex microstructures. This method not only
enables rapid and accurate segmentation but also facilitates an unprecedented
level of data analysis, significantly improving the statistical representation
of grain boundary networks, making it suitable for a range of disciplines where
precise segmentation of interconnected line networks is essential.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Aksoy_D/0/1/0/all/0/1&quot;&gt;Doruk Aksoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Xin_H/0/1/0/all/0/1&quot;&gt;Huolin L. Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Rupert_T/0/1/0/all/0/1&quot;&gt;Timothy J. Rupert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Bowman_W/0/1/0/all/0/1&quot;&gt;William J. Bowman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09988">
<title>Towards Architecture-Insensitive Untrained Network Priors for Accelerated MRI Reconstruction. (arXiv:2312.09988v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.09988</link>
<description rdf:parseType="Literal">&lt;p&gt;Untrained neural networks pioneered by Deep Image Prior (DIP) have recently
enabled MRI reconstruction without requiring fully-sampled measurements for
training. Their success is widely attributed to the implicit regularization
induced by suitable network architectures. However, the lack of understanding
of such architectural priors results in superfluous design choices and
sub-optimal outcomes. This work aims to simplify the architectural design
decisions for DIP-MRI to facilitate its practical deployment. We observe that
certain architectural components are more prone to causing overfitting
regardless of the number of parameters, incurring severe reconstruction
artifacts by hindering accurate extrapolation on the un-acquired measurements.
We interpret this phenomenon from a frequency perspective and find that the
architectural characteristics favoring low frequencies, i.e., deep and narrow
with unlearnt upsampling, can lead to enhanced generalization and hence better
reconstruction. Building on this insight, we propose two architecture-agnostic
remedies: one to constrain the frequency range of the white-noise input and the
other to penalize the Lipschitz constants of the network. We demonstrate that
even with just one extra line of code on the input, the performance gap between
the ill-designed models and the high-performing ones can be closed. These
results signify that for the first time, architectural biases on untrained MRI
reconstruction can be mitigated without architectural modifications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yilin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pang_Y/0/1/0/all/0/1&quot;&gt;Yunkui Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yap_P/0/1/0/all/0/1&quot;&gt;Pew-Thian Yap&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09997">
<title>One Self-Configurable Model to Solve Many Abstract Visual Reasoning Problems. (arXiv:2312.09997v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09997</link>
<description rdf:parseType="Literal">&lt;p&gt;Abstract Visual Reasoning (AVR) comprises a wide selection of various
problems similar to those used in human IQ tests. Recent years have brought
dynamic progress in solving particular AVR tasks, however, in the contemporary
literature AVR problems are largely dealt with in isolation, leading to highly
specialized task-specific methods. With the aim of developing universal
learning systems in the AVR domain, we propose the unified model for solving
Single-Choice Abstract visual Reasoning tasks (SCAR), capable of solving
various single-choice AVR tasks, without making any a priori assumptions about
the task structure, in particular the number and location of panels. The
proposed model relies on a novel Structure-Aware dynamic Layer (SAL), which
adapts its weights to the structure of the considered AVR problem. Experiments
conducted on Raven&apos;s Progressive Matrices, Visual Analogy Problems, and Odd One
Out problems show that SCAR (SAL-based models, in general) effectively solves
diverse AVR tasks, and its performance is on par with the state-of-the-art
task-specific baselines. What is more, SCAR demonstrates effective knowledge
reuse in multi-task and transfer learning settings. To our knowledge, this work
is the first successful attempt to construct a general single-choice AVR solver
relying on self-configurable architecture and unified solving method. With this
work we aim to stimulate and foster progress on task-independent research paths
in the AVR domain, with the long-term goal of development of a general AVR
solver.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malkinski_M/0/1/0/all/0/1&quot;&gt;Miko&amp;#x142;aj Ma&amp;#x142;ki&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandziuk_J/0/1/0/all/0/1&quot;&gt;Jacek Ma&amp;#x144;dziuk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10032">
<title>Osprey: Pixel Understanding with Visual Instruction Tuning. (arXiv:2312.10032v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10032</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal large language models (MLLMs) have recently achieved impressive
general-purpose vision-language capabilities through visual instruction tuning.
However, current MLLMs primarily focus on image-level or box-level
understanding, falling short of achieving fine-grained vision-language
alignment at the pixel level. Besides, the lack of mask-based instruction data
limits their advancements. In this paper, we propose Osprey, a mask-text
instruction tuning approach, to extend MLLMs by incorporating fine-grained mask
regions into language instruction, aiming at achieving pixel-wise visual
understanding. To achieve this goal, we first meticulously curate a mask-based
region-text dataset with 724K samples, and then design a vision-language model
by injecting pixel-level representation into LLM. Especially, Osprey adopts a
convolutional CLIP backbone as the vision encoder and employs a mask-aware
visual extractor to extract precise visual mask features from high resolution
input. Experimental results demonstrate Osprey&apos;s superiority in various region
understanding tasks, showcasing its new capability for pixel-level instruction
tuning. In particular, Osprey can be integrated with Segment Anything Model
(SAM) seamlessly to obtain multi-granularity semantics. The source code,
dataset and demo can be found at https://github.com/CircleRadon/Osprey.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuqian Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wentong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1&quot;&gt;Dongqi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xinjie Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1&quot;&gt;Chi Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jianke Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10034">
<title>SlimmeRF: Slimmable Radiance Fields. (arXiv:2312.10034v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10034</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Field (NeRF) and its variants have recently emerged as
successful methods for novel view synthesis and 3D scene reconstruction.
However, most current NeRF models either achieve high accuracy using large
model sizes, or achieve high memory-efficiency by trading off accuracy. This
limits the applicable scope of any single model, since high-accuracy models
might not fit in low-memory devices, and memory-efficient models might not
satisfy high-quality requirements. To this end, we present SlimmeRF, a model
that allows for instant test-time trade-offs between model size and accuracy
through slimming, thus making the model simultaneously suitable for scenarios
with different computing budgets. We achieve this through a newly proposed
algorithm named Tensorial Rank Incrementation (TRaIn) which increases the rank
of the model&apos;s tensorial representation gradually during training. We also
observe that our model allows for more effective trade-offs in sparse-view
scenarios, at times even achieving higher accuracy after being slimmed. We
credit this to the fact that erroneous information such as floaters tend to be
stored in components corresponding to higher ranks. Our implementation is
available at https://github.com/Shiran-Yuan/SlimmeRF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1&quot;&gt;Shiran Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hao Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10035">
<title>Point Transformer V3: Simpler, Faster, Stronger. (arXiv:2312.10035v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10035</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper is not motivated to seek innovation within the attention
mechanism. Instead, it focuses on overcoming the existing trade-offs between
accuracy and efficiency within the context of point cloud processing,
leveraging the power of scale. Drawing inspiration from recent advances in 3D
large-scale representation learning, we recognize that model performance is
more influenced by scale than by intricate design. Therefore, we present Point
Transformer V3 (PTv3), which prioritizes simplicity and efficiency over the
accuracy of certain mechanisms that are minor to the overall performance after
scaling, such as replacing the precise neighbor search by KNN with an efficient
serialized neighbor mapping of point clouds organized with specific patterns.
This principle enables significant scaling, expanding the receptive field from
16 to 1024 points while remaining efficient (a 3x increase in processing speed
and a 10x improvement in memory efficiency compared with its predecessor,
PTv2). PTv3 attains state-of-the-art results on over 20 downstream tasks that
span both indoor and outdoor scenarios. Further enhanced with multi-dataset
joint training, PTv3 pushes these results to a higher level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Li Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng-Shuai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhijian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xihui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hengshuang Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2001.01258">
<title>The troublesome kernel -- On hallucinations, no free lunches and the accuracy-stability trade-off in inverse problems. (arXiv:2001.01258v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2001.01258</link>
<description rdf:parseType="Literal">&lt;p&gt;Methods inspired by Artificial Intelligence (AI) are starting to
fundamentally change computational science and engineering through breakthrough
performances on challenging problems. However, reliability and trustworthiness
of such techniques is becoming a major concern. In inverse problems in imaging,
the focus of this paper, there is increasing empirical evidence that methods
may suffer from hallucinations, i.e., false, but realistic-looking artifacts;
instability, i.e., sensitivity to perturbations in the data; and unpredictable
generalization, i.e., excellent performance on some images, but significant
deterioration on others. This paper presents a theoretical foundation for these
phenomena. We give a mathematical framework describing how and when such
effects arise in arbitrary reconstruction methods, not just AI-inspired
techniques. Several of our results take the form of `no free lunch&apos; theorems.
Specifically, we show that (i) methods that overperform on a single image can
wrongly transfer details from one image to another, creating a hallucination,
(ii) methods that overperform on two or more images can hallucinate or be
unstable, (iii) optimizing the accuracy-stability trade-off is generally
difficult, (iv) hallucinations and instabilities, if they occur, are not rare
events, and may be encouraged by standard training, (v) it may be impossible to
construct optimal reconstruction maps for certain problems. Our results trace
these effects to the kernel of the forward operator whenever it is nontrivial,
but also extend to the case when the forward operator is ill-conditioned. Based
on these insights, our work aims to spur research into new ways to develop
robust and reliable AI-inspired methods for inverse problems in imaging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottschling_N/0/1/0/all/0/1&quot;&gt;Nina M. Gottschling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antun_V/0/1/0/all/0/1&quot;&gt;Vegard Antun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hansen_A/0/1/0/all/0/1&quot;&gt;Anders C. Hansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adcock_B/0/1/0/all/0/1&quot;&gt;Ben Adcock&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2107.10984">
<title>Human Pose Transfer with Augmented Disentangled Feature Consistency. (arXiv:2107.10984v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2107.10984</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models have made great progress in synthesizing images with
arbitrary human poses and transferring poses of one person to others. Though
many different methods have been proposed to generate images with high visual
fidelity, the main challenge remains and comes from two fundamental issues:
pose ambiguity and appearance inconsistency. To alleviate the current
limitations and improve the quality of the synthesized images, we propose a
pose transfer network with augmented Disentangled Feature Consistency (DFC-Net)
to facilitate human pose transfer. Given a pair of images containing the source
and target person, DFC-Net extracts pose and static information from the source
and target respectively, then synthesizes an image of the target person with
the desired pose from the source. Moreover, DFC-Net leverages disentangled
feature consistency losses in the adversarial training to strengthen the
transfer coherence and integrates a keypoint amplifier to enhance the pose
feature extraction. With the help of the disentangled feature consistency
losses, we further propose a novel data augmentation scheme that introduces
unpaired support data with the augmented consistency constraints to improve the
generality and robustness of DFC-Net. Extensive experimental results on
Mixamo-Pose and EDN-10k have demonstrated DFC-Net achieves state-of-the-art
performance on pose transfer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1&quot;&gt;Kun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1&quot;&gt;Chengxiang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1&quot;&gt;Zhengping Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1&quot;&gt;Zheng Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1&quot;&gt;Gangyi Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.04968">
<title>Bimodal Camera Pose Prediction for Endoscopy. (arXiv:2204.04968v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.04968</link>
<description rdf:parseType="Literal">&lt;p&gt;Deducing the 3D structure of endoscopic scenes from images is exceedingly
challenging. In addition to deformation and view-dependent lighting, tubular
structures like the colon present problems stemming from their self-occluding
and repetitive anatomical structure. In this paper, we propose SimCol, a
synthetic dataset for camera pose estimation in colonoscopy, and a novel method
that explicitly learns a bimodal distribution to predict the endoscope pose.
Our dataset replicates real colonoscope motion and highlights the drawbacks of
existing methods. We publish 18k RGB images from simulated colonoscopy with
corresponding depth and camera poses and make our data generation environment
in Unity publicly available. We evaluate different camera pose prediction
methods and demonstrate that, when trained on our data, they generalize to real
colonoscopy sequences, and our bimodal approach outperforms prior unimodal
work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rau_A/0/1/0/all/0/1&quot;&gt;Anita Rau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattarai_B/0/1/0/all/0/1&quot;&gt;Binod Bhattarai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agapito_L/0/1/0/all/0/1&quot;&gt;Lourdes Agapito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1&quot;&gt;Danail Stoyanov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.09602">
<title>Exploring Adversarial Robustness of Vision Transformers in the Spectral Perspective. (arXiv:2208.09602v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.09602</link>
<description rdf:parseType="Literal">&lt;p&gt;The Vision Transformer has emerged as a powerful tool for image
classification tasks, surpassing the performance of convolutional neural
networks (CNNs). Recently, many researchers have attempted to understand the
robustness of Transformers against adversarial attacks. However, previous
researches have focused solely on perturbations in the spatial domain. This
paper proposes an additional perspective that explores the adversarial
robustness of Transformers against frequency-selective perturbations in the
spectral domain. To facilitate comparison between these two domains, an attack
framework is formulated as a flexible tool for implementing attacks on images
in the spatial and spectral domains. The experiments reveal that Transformers
rely more on phase and low frequency information, which can render them more
vulnerable to frequency-selective attacks than CNNs. This work offers new
insights into the properties and adversarial robustness of Transformers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1&quot;&gt;Gihyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Juyeop Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jong-Seok Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01272">
<title>A systematic review of the use of Deep Learning in Satellite Imagery for Agriculture. (arXiv:2210.01272v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01272</link>
<description rdf:parseType="Literal">&lt;p&gt;Agricultural research is essential for increasing food production to meet the
requirements of an increasing population in the coming decades. Recently,
satellite technology has been improving rapidly and deep learning has seen much
success in generic computer vision tasks and many application areas which
presents an important opportunity to improve analysis of agricultural land.
Here we present a systematic review of 150 studies to find the current uses of
deep learning on satellite imagery for agricultural research. Although we
identify 5 categories of agricultural monitoring tasks, the majority of the
research interest is in crop segmentation and yield prediction. We found that,
when used, modern deep learning methods consistently outperformed traditional
machine learning across most tasks; the only exception was that Long Short-Term
Memory (LSTM) Recurrent Neural Networks did not consistently outperform Random
Forests (RF) for yield prediction. The reviewed studies have largely adopted
methodologies from generic computer vision, except for one major omission:
benchmark datasets are not utilised to evaluate models across studies, making
it difficult to compare results. Additionally, some studies have specifically
utilised the extra spectral resolution available in satellite imagery, but
other divergent properties of satellite images - such as the hugely different
scales of spatial patterns - are not being taken advantage of in the reviewed
studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Victor_B/0/1/0/all/0/1&quot;&gt;Brandon Victor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhen He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nibali_A/0/1/0/all/0/1&quot;&gt;Aiden Nibali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.12381">
<title>S2WAT: Image Style Transfer via Hierarchical Vision Transformer using Strips Window Attention. (arXiv:2210.12381v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.12381</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer&apos;s recent integration into style transfer leverages its
proficiency in establishing long-range dependencies, albeit at the expense of
attenuated local modeling. This paper introduces Strips Window Attention
Transformer (S2WAT), a novel hierarchical vision transformer designed for style
transfer. S2WAT employs attention computation in diverse window shapes to
capture both short- and long-range dependencies. The merged dependencies
utilize the &quot;Attn Merge&quot; strategy, which adaptively determines spatial weights
based on their relevance to the target. Extensive experiments on representative
datasets show the proposed method&apos;s effectiveness compared to state-of-the-art
(SOTA) transformer-based and other approaches. The code and pre-trained models
are available at https://github.com/AlienZhang1996/S2WAT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chiyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaogang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1&quot;&gt;Zaiyan Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jun Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11727">
<title>Parametric Classification for Generalized Category Discovery: A Baseline Study. (arXiv:2211.11727v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11727</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalized Category Discovery (GCD) aims to discover novel categories in
unlabelled datasets using knowledge learned from labelled samples. Previous
studies argued that parametric classifiers are prone to overfitting to seen
categories, and endorsed using a non-parametric classifier formed with
semi-supervised k-means. However, in this study, we investigate the failure of
parametric classifiers, verify the effectiveness of previous design choices
when high-quality supervision is available, and identify unreliable
pseudo-labels as a key problem. We demonstrate that two prediction biases
exist: the classifier tends to predict seen classes more often, and produces an
imbalanced distribution across seen and novel categories. Based on these
findings, we propose a simple yet effective parametric classification method
that benefits from entropy regularisation, achieves state-of-the-art
performance on multiple GCD benchmarks and shows strong robustness to unknown
class numbers. We hope the investigation and proposed simple framework can
serve as a strong baseline to facilitate future studies in this field. Our code
is available at: https://github.com/CVMI-Lab/SimGCD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1&quot;&gt;Xin Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bingchen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xiaojuan Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.12417">
<title>ProCC: Progressive Cross-primitive Compatibility for Open-World Compositional Zero-Shot Learning. (arXiv:2211.12417v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.12417</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-World Compositional Zero-shot Learning (OW-CZSL) aims to recognize novel
compositions of state and object primitives in images with no priors on the
compositional space, which induces a tremendously large output space containing
all possible state-object compositions. Existing works either learn the joint
compositional state-object embedding or predict simple primitives with separate
classifiers. However, the former heavily relies on external word embedding
methods, and the latter ignores the interactions of interdependent primitives,
respectively. In this paper, we revisit the primitive prediction approach and
propose a novel method, termed Progressive Cross-primitive Compatibility
(ProCC), to mimic the human learning process for OW-CZSL tasks. Specifically,
the cross-primitive compatibility module explicitly learns to model the
interactions of state and object features with the trainable memory units,
which efficiently acquires cross-primitive visual attention to reason
high-feasibility compositions, without the aid of external knowledge. Moreover,
considering the partial-supervision setting (pCZSL) as well as the imbalance
issue of multiple task prediction, we design a progressive training paradigm to
enable the primitive classifiers to interact to obtain discriminative
information in an easy-to-hard manner. Extensive experiments on three widely
used benchmark datasets demonstrate that our method outperforms other
representative methods on both OW-CZSL and pCZSL settings by large margins.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_F/0/1/0/all/0/1&quot;&gt;Fushuo Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wenchao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Song Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jingcai Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haozhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiaocheng Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.07786">
<title>Convergent Data-driven Regularizations for CT Reconstruction. (arXiv:2212.07786v2 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/2212.07786</link>
<description rdf:parseType="Literal">&lt;p&gt;The reconstruction of images from their corresponding noisy Radon transform
is a typical example of an ill-posed linear inverse problem as arising in the
application of computerized tomography (CT). As the (naive) solution does not
depend on the measured data continuously, regularization is needed to
re-establish a continuous dependence. In this work, we investigate simple, but
yet still provably convergent approaches to learning linear regularization
methods from data. More specifically, we analyze two approaches: One generic
linear regularization that learns how to manipulate the singular values of the
linear operator in an extension of our previous work, and one tailored approach
in the Fourier domain that is specific to CT-reconstruction. We prove that such
approaches become convergent regularization methods as well as the fact that
the reconstructions they provide are typically much smoother than the training
data they were trained on. Finally, we compare the spectral as well as the
Fourier-based approaches for CT-reconstruction numerically, discuss their
advantages and disadvantages and investigate the effect of discretization
errors at different resolutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kabri_S/0/1/0/all/0/1&quot;&gt;Samira Kabri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Auras_A/0/1/0/all/0/1&quot;&gt;Alexander Auras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Riccio_D/0/1/0/all/0/1&quot;&gt;Danilo Riccio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bauermeister_H/0/1/0/all/0/1&quot;&gt;Hartmut Bauermeister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Benning_M/0/1/0/all/0/1&quot;&gt;Martin Benning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Moeller_M/0/1/0/all/0/1&quot;&gt;Michael Moeller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Burger_M/0/1/0/all/0/1&quot;&gt;Martin Burger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.06132">
<title>Deep Diversity-Enhanced Feature Representation of Hyperspectral Images. (arXiv:2301.06132v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.06132</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the problem of efficiently and effectively embedding
the high-dimensional spatio-spectral information of hyperspectral (HS) images,
guided by feature diversity. Specifically, based on the theoretical formulation
that feature diversity is correlated with the rank of the unfolded kernel
matrix, we rectify 3D convolution by modifying its topology to enhance the rank
upper-bound. This modification yields a rank-enhanced spatial-spectral
symmetrical convolution set (ReS$^3$-ConvSet), which not only learns diverse
and powerful feature representations but also saves network parameters.
Additionally, we also propose a novel diversity-aware regularization (DA-Reg)
term that directly acts on the feature maps to maximize independence among
elements. To demonstrate the superiority of the proposed ReS$^3$-ConvSet and
DA-Reg, we apply them to various HS image processing and analysis tasks,
including denoising, spatial super-resolution, and classification. Extensive
experiments show that the proposed approaches outperform state-of-the-art
methods both quantitatively and qualitatively to a significant extent. The code
is publicly available at https://github.com/jinnh/ReSSS-ConvSet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Jinhui Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhiyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Junhui Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1&quot;&gt;Huanqiang Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1&quot;&gt;Deyu Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.00901">
<title>Longformer: Longitudinal Transformer for Alzheimer&apos;s Disease Classification with Structural MRIs. (arXiv:2302.00901v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.00901</link>
<description rdf:parseType="Literal">&lt;p&gt;Structural magnetic resonance imaging (sMRI) is widely used for brain
neurological disease diagnosis; while longitudinal MRIs are often collected to
monitor and capture disease progression, as clinically used in diagnosing
Alzheimer&apos;s disease (AD). However, most current methods neglect AD&apos;s
progressive nature and only take a single sMRI for recognizing AD. In this
paper, we consider the problem of leveraging the longitudinal MRIs of a subject
for AD identification. To capture longitudinal changes in sMRIs, we propose a
novel model Longformer, a spatiotemporal transformer network that performs
attention mechanisms spatially on sMRIs at each time point and integrates brain
region features over time to obtain longitudinal embeddings for classification.
Our Longformer achieves state-of-the-art performance on two binary
classification tasks of separating different stages of AD using the ADNI
dataset. Our source code is available at https://github.com/Qybc/LongFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qiuhui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yi Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.04998">
<title>Rethinking Visual Prompt Learning as Masked Visual Token Modeling. (arXiv:2303.04998v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.04998</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt learning has achieved great success in efficiently exploiting
large-scale pre-trained models in natural language processing (NLP). It
reformulates the downstream tasks as the generative pre-training ones to
achieve consistency, thus improving the performance stably. However, when
transferring it to the vision area, current visual prompt learning methods are
almost designed on discriminative pre-trained models, and there is also a lack
of careful design to unify the forms of pre-training and downstream tasks. To
explore prompt learning on the generative pre-trained visual model, as well as
keeping the task consistency, we propose Visual Prompt learning as masked
visual Token Modeling (VPTM) to transform the downstream visual classification
into the pre-trained masked visual token prediction. In addition, we develop
the prototypical verbalizer for mapping the predicted visual token with
implicit semantics to explicit downstream labels. To our best knowledge, VPTM
is the first visual prompt method on the generative pre-trained visual model,
which achieves consistency between pre-training and downstream visual
classification by task reformulation. Experiments show that VPTM outperforms
other visual prompt methods and achieves excellent efficiency. Moreover, the
task consistency of VPTM contributes to the robustness against prompt location,
prompt length and prototype dimension, and could be deployed uniformly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_N/0/1/0/all/0/1&quot;&gt;Ning Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bowen Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaopeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1&quot;&gt;Min Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qi Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05754">
<title>Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse Problems. (arXiv:2303.05754v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05754</link>
<description rdf:parseType="Literal">&lt;p&gt;Krylov subspace, which is generated by multiplying a given vector by the
matrix of a linear transformation and its successive powers, has been
extensively studied in classical optimization literature to design algorithms
that converge quickly for large linear inverse problems. For example, the
conjugate gradient method (CG), one of the most popular Krylov subspace
methods, is based on the idea of minimizing the residual error in the Krylov
subspace. However, with the recent advancement of high-performance diffusion
solvers for inverse problems, it is not clear how classical wisdom can be
synergistically combined with modern diffusion models. In this study, we
propose a novel and efficient diffusion sampling strategy that synergistically
combine the diffusion sampling and Krylov subspace methods. Specifically, we
prove that if the tangent space at a denoised sample by Tweedie&apos;s formula forms
a Krylov subspace, then the CG initialized with the denoised data ensures the
data consistency update to remain in the tangent space. This negates the need
to compute the manifold-constrained gradient (MCG), leading to a more efficient
diffusion sampling method. Our method is applicable regardless of the
parametrization and setting (i.e., VE, VP). Notably, we achieve
state-of-the-art reconstruction quality on challenging real-world medical
inverse imaging problems, including multi-coil MRI reconstruction and 3D CT
reconstruction. Moreover, our proposed method achieves more than 80 times
faster inference time than the previous state-of-the-art method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1&quot;&gt;Hyungjin Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Suhyeon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jong Chul Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10891">
<title>Non-Exemplar Online Class-incremental Continual Learning via Dual-prototype Self-augment and Refinement. (arXiv:2303.10891v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10891</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates a new, practical, but challenging problem named
Non-exemplar Online Class-incremental continual Learning (NO-CL), which aims to
preserve the discernibility of base classes without buffering data examples and
efficiently learn novel classes continuously in a single-pass (i.e., online)
data stream. The challenges of this task are mainly two-fold: (1) Both base and
novel classes suffer from severe catastrophic forgetting as no previous samples
are available for replay. (2) As the online data can only be observed once,
there is no way to fully re-train the whole model, e.g., re-calibrate the
decision boundaries via prototype alignment or feature distillation. In this
paper, we propose a novel Dual-prototype Self-augment and Refinement method
(DSR) for NO-CL problem, which consists of two strategies: 1) Dual class
prototypes: vanilla and high-dimensional prototypes are exploited to utilize
the pre-trained information and obtain robust quasi-orthogonal representations
rather than example buffers for both privacy preservation and memory reduction.
2) Self-augment and refinement: Instead of updating the whole network, we
optimize high-dimensional prototypes alternatively with the extra projection
module based on self-augment vanilla prototypes, through a bi-level
optimization problem. Extensive experiments demonstrate the effectiveness and
superiority of the proposed DSR in NO-CL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_F/0/1/0/all/0/1&quot;&gt;Fushuo Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wenchao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jingcai Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haozhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yunfeng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Song Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07213">
<title>Very high resolution canopy height maps from RGB imagery using self-supervised vision transformer and convolutional decoder trained on Aerial Lidar. (arXiv:2304.07213v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07213</link>
<description rdf:parseType="Literal">&lt;p&gt;Vegetation structure mapping is critical for understanding the global carbon
cycle and monitoring nature-based approaches to climate adaptation and
mitigation. Repeated measurements of these data allow for the observation of
deforestation or degradation of existing forests, natural forest regeneration,
and the implementation of sustainable agricultural practices like agroforestry.
Assessments of tree canopy height and crown projected area at a high spatial
resolution are also important for monitoring carbon fluxes and assessing
tree-based land uses, since forest structures can be highly spatially
heterogeneous, especially in agroforestry systems. Very high resolution
satellite imagery (less than one meter (1m) Ground Sample Distance) makes it
possible to extract information at the tree level while allowing monitoring at
a very large scale. This paper presents the first high-resolution canopy height
map concurrently produced for multiple sub-national jurisdictions.
Specifically, we produce very high resolution canopy height maps for the states
of California and Sao Paulo, a significant improvement in resolution over the
ten meter (10m) resolution of previous Sentinel / GEDI based worldwide maps of
canopy height. The maps are generated by the extraction of features from a
self-supervised model trained on Maxar imagery from 2017 to 2020, and the
training of a dense prediction decoder against aerial lidar maps. We also
introduce a post-processing step using a convolutional network trained on GEDI
observations. We evaluate the proposed maps with set-aside validation lidar
data as well as by comparing with other remotely sensed maps and
field-collected data, and find our model produces an average Mean Absolute
Error (MAE) of 2.8 meters and Mean Error (ME) of 0.6 meters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolan_J/0/1/0/all/0/1&quot;&gt;Jamie Tolan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hung-I Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nosarzewski_B/0/1/0/all/0/1&quot;&gt;Ben Nosarzewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couairon_G/0/1/0/all/0/1&quot;&gt;Guillaume Couairon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1&quot;&gt;Huy Vo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brandt_J/0/1/0/all/0/1&quot;&gt;John Brandt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spore_J/0/1/0/all/0/1&quot;&gt;Justine Spore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1&quot;&gt;Sayantan Majumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haziza_D/0/1/0/all/0/1&quot;&gt;Daniel Haziza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vamaraju_J/0/1/0/all/0/1&quot;&gt;Janaki Vamaraju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moutakanni_T/0/1/0/all/0/1&quot;&gt;Theo Moutakanni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1&quot;&gt;Piotr Bojanowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johns_T/0/1/0/all/0/1&quot;&gt;Tracy Johns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+White_B/0/1/0/all/0/1&quot;&gt;Brian White&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiecke_T/0/1/0/all/0/1&quot;&gt;Tobias Tiecke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couprie_C/0/1/0/all/0/1&quot;&gt;Camille Couprie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10839">
<title>Multi-frame-based Cross-domain Denoising for Low-dose Spiral Computed Tomography. (arXiv:2304.10839v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10839</link>
<description rdf:parseType="Literal">&lt;p&gt;Computed tomography (CT) has been used worldwide as a non-invasive test in
assisting diagnosis. However, the ionizing nature of X-ray exposure raises
concerns about potential health risks such as cancer. The desire for lower
radiation doses has driven researchers to improve reconstruction quality.
Although previous studies on low-dose computed tomography (LDCT) denoising have
demonstrated the effectiveness of learning-based methods, most were developed
on the simulated data collected using the Radon transform. However, the
real-world scenario differs significantly from the simulation domain,
especially when using the multi-slice spiral scanner geometry. This paper
proposes a two-stage method for the commercially available third-generation
multi-slice spiral CT scanners that better exploits the complete reconstruction
pipeline for LDCT denoising across different domains. Our approach makes good
use of the high redundancy of the multi-slice projections and the volumetric
reconstructions while leveraging the over-smoothing of high-frequency
information in conventional cascaded frameworks due to aggressive denoising.
The dedicated design also provides a more explicit interpretation of the data
flow. Extensive experiments on various datasets showed that the proposed method
could remove up to 70% of noise without compromised spatial resolution, while
subjective evaluations by two radiologists further supported its superior
performance against state-of-the-art methods in clinical practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yucheng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhixin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Choi_M/0/1/0/all/0/1&quot;&gt;Moon Hyung Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jimin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jung_S/0/1/0/all/0/1&quot;&gt;Seung-Won Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05421">
<title>DC3DCD: unsupervised learning for multiclass 3D point cloud change detection. (arXiv:2305.05421v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05421</link>
<description rdf:parseType="Literal">&lt;p&gt;In a constant evolving world, change detection is of prime importance to keep
updated maps. To better sense areas with complex geometry (urban areas in
particular), considering 3D data appears to be an interesting alternative to
classical 2D images. In this context, 3D point clouds (PCs), whether obtained
through LiDAR or photogrammetric techniques, provide valuable information.
While recent studies showed the considerable benefit of using deep
learning-based methods to detect and characterize changes into raw 3D PCs,
these studies rely on large annotated training data to obtain accurate results.
The collection of these annotations are tricky and time-consuming. The
availability of unsupervised or weakly supervised approaches is then of prime
interest. In this paper, we propose an unsupervised method, called DeepCluster
3D Change Detection (DC3DCD), to detect and categorize multiclass changes at
point level. We classify our approach in the unsupervised family given the fact
that we extract in a completely unsupervised way a number of clusters
associated with potential changes. Let us precise that in the end of the
process, the user has only to assign a label to each of these clusters to
derive the final change map. Our method builds upon the DeepCluster approach,
originally designed for image classification, to handle complex raw 3D PCs and
perform change segmentation task. An assessment of the method on both simulated
and real public dataset is provided. The proposed method allows to outperform
fully-supervised traditional machine learning algorithm and to be competitive
with fully-supervised deep learning networks applied on rasterization of 3D PCs
with a mean of IoU over classes of change of 57.06\% and 66.69\% for the
simulated and the real datasets, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gelis_I/0/1/0/all/0/1&quot;&gt;Iris de G&amp;#xe9;lis&lt;/a&gt; (1 and 2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lefevre_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Lef&amp;#xe8;vre&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corpetti_T/0/1/0/all/0/1&quot;&gt;Thomas Corpetti&lt;/a&gt; (3) ((1) Magellium, (2) Institut de Recherche en Informatique et Syst&amp;#xe8;mes Al&amp;#xe9;atoires IRISA - UMR 6074 - Universit&amp;#xe9; Bretagne Sud, (3) Littoral - Environnement - T&amp;#xe9;l&amp;#xe9;d&amp;#xe9;tection - G&amp;#xe9;omatique LETG - UMR 6554 - Universit&amp;#xe9; Rennes 2)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08946">
<title>Image Matching by Bare Homography. (arXiv:2305.08946v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08946</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents Slime, a novel non-deep image matching framework which
models the scene as rough local overlapping planes. This intermediate
representation sits in-between the local affine approximation of the keypoint
patches and the global matching based on both spatial and similarity
constraints, providing a progressive pruning of the correspondences, as planes
are easier to handle with respect to general scenes.
&lt;/p&gt;
&lt;p&gt;Slime decomposes the images into overlapping regions at different scales and
computes loose planar homographies. Planes are mutually extended by compatible
matches and the images are split into fixed tiles, with only the best
homographies retained for each pair of tiles. Stable matches are identified
according to the consensus of the admissible stereo configurations provided by
pairwise homographies. Within tiles, the rough planes are then merged according
to their overlap in terms of matches and further consistent correspondences are
extracted.
&lt;/p&gt;
&lt;p&gt;The whole process only involves homography constraints. As a result, both the
coverage and the stability of correct matches over the scene are amplified,
together with the ability to spot matches in challenging scenes, allowing
traditional hybrid matching pipelines to make up lost ground against recent
end-to-end deep matching methods.
&lt;/p&gt;
&lt;p&gt;In addition, the paper gives a thorough comparative analysis of recent
state-of-the-art in image matching represented by end-to-end deep networks and
hybrid pipelines. The evaluation considers both planar and non-planar scenes,
taking into account critical and challenging scenarios including abrupt
temporal image changes and strong variations in relative image rotations.
According to this analysis, although the impressive progress done in this
field, there is still a wide room for improvements to be investigated in future
research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellavia_F/0/1/0/all/0/1&quot;&gt;Fabio Bellavia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10941">
<title>Synthetic optical coherence tomography angiographs for detailed retinal vessel segmentation without human annotations. (arXiv:2306.10941v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10941</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical coherence tomography angiography (OCTA) is a non-invasive imaging
modality that can acquire high-resolution volumes of the retinal vasculature
and aid the diagnosis of ocular, neurological and cardiac diseases. Segmenting
the visible blood vessels is a common first step when extracting quantitative
biomarkers from these images. Classical segmentation algorithms based on
thresholding are strongly affected by image artifacts and limited
signal-to-noise ratio. The use of modern, deep learning-based segmentation
methods has been inhibited by a lack of large datasets with detailed
annotations of the blood vessels. To address this issue, recent work has
employed transfer learning, where a segmentation network is trained on
synthetic OCTA images and is then applied to real data. However, the previously
proposed simulations fail to faithfully model the retinal vasculature and do
not provide effective domain adaptation. Because of this, current methods are
unable to fully segment the retinal vasculature, in particular the smallest
capillaries. In this work, we present a lightweight simulation of the retinal
vascular network based on space colonization for faster and more realistic OCTA
synthesis. We then introduce three contrast adaptation pipelines to decrease
the domain gap between real and artificial images. We demonstrate the superior
segmentation performance of our approach in extensive quantitative and
qualitative experiments on three public datasets that compare our method to
traditional computer vision algorithms and supervised training using human
annotations. Finally, we make our entire pipeline publicly available, including
the source code, pretrained models, and a large dataset of synthetic OCTA
images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kreitner_L/0/1/0/all/0/1&quot;&gt;Linus Kreitner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Paetzold_J/0/1/0/all/0/1&quot;&gt;Johannes C. Paetzold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rauch_N/0/1/0/all/0/1&quot;&gt;Nikolaus Rauch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hagag_A/0/1/0/all/0/1&quot;&gt;Ahmed M. Hagag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fayed_A/0/1/0/all/0/1&quot;&gt;Alaa E. Fayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sivaprasad_S/0/1/0/all/0/1&quot;&gt;Sobha Sivaprasad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rausch_S/0/1/0/all/0/1&quot;&gt;Sebastian Rausch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Weichsel_J/0/1/0/all/0/1&quot;&gt;Julian Weichsel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1&quot;&gt;Bjoern H. Menze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Harders_M/0/1/0/all/0/1&quot;&gt;Matthias Harders&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Knier_B/0/1/0/all/0/1&quot;&gt;Benjamin Knier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Menten_M/0/1/0/all/0/1&quot;&gt;Martin J. Menten&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14448">
<title>Progressive Energy-Based Cooperative Learning for Multi-Domain Image-to-Image Translation. (arXiv:2306.14448v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14448</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies a novel energy-based cooperative learning framework for
multi-domain image-to-image translation. The framework consists of four
components: descriptor, translator, style encoder, and style generator. The
descriptor is a multi-head energy-based model that represents a multi-domain
image distribution. The components of translator, style encoder, and style
generator constitute a diversified image generator. Specifically, given an
input image from a source domain, the translator turns it into a stylised
output image of the target domain according to a style code, which can be
inferred by the style encoder from a reference image or produced by the style
generator from a random noise. Since the style generator is represented as an
domain-specific distribution of style codes, the translator can provide a
one-to-many transformation (i.e., diversified generation) between source domain
and target domain. To train our framework, we propose a likelihood-based
multi-domain cooperative learning algorithm to jointly train the multi-domain
descriptor and the diversified image generator (including translator, style
encoder, and style generator modules) via multi-domain MCMC teaching, in which
the descriptor guides the diversified image generator to shift its probability
density toward the data distribution, while the diversified image generator
uses its randomly translated images to initialize the descriptor&apos;s Langevin
dynamics process for efficient sampling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1&quot;&gt;Weinan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yaxuan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Lei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yingnian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jianwen Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08265">
<title>Extreme Image Compression using Fine-tuned VQGANs. (arXiv:2307.08265v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08265</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in generative compression methods have demonstrated
remarkable progress in enhancing the perceptual quality of compressed data,
especially in scenarios with low bitrates. However, their efficacy and
applicability to achieve extreme compression ratios ($&amp;lt;0.05$ bpp) remain
constrained. In this work, we propose a simple yet effective coding framework
by introducing vector quantization (VQ)--based generative models into the image
compression domain. The main insight is that the codebook learned by the VQGAN
model yields a strong expressive capacity, facilitating efficient compression
of continuous information in the latent space while maintaining reconstruction
quality. Specifically, an image can be represented as VQ-indices by finding the
nearest codeword, which can be encoded using lossless compression methods into
bitstreams. We propose clustering a pre-trained large-scale codebook into
smaller codebooks through the K-means algorithm, yielding variable bitrates and
different levels of reconstruction quality within the coding framework.
Furthermore, we introduce a transformer to predict lost indices and restore
images in unstable environments. Extensive qualitative and quantitative
experiments on various benchmark datasets demonstrate that the proposed
framework outperforms state-of-the-art codecs in terms of perceptual
quality-oriented metrics and human perception at extremely low bitrates ($\le
0.04$ bpp). Remarkably, even with the loss of up to $20\%$ of indices, the
images can be effectively restored with minimal perceptual loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1&quot;&gt;Qi Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tinghan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yinuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zijian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Siwei Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01262">
<title>Incorporating Season and Solar Specificity into Renderings made by a NeRF Architecture using Satellite Images. (arXiv:2308.01262v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01262</link>
<description rdf:parseType="Literal">&lt;p&gt;As a result of Shadow NeRF and Sat-NeRF, it is possible to take the solar
angle into account in a NeRF-based framework for rendering a scene from a novel
viewpoint using satellite images for training. Our work extends those
contributions and shows how one can make the renderings season-specific. Our
main challenge was creating a Neural Radiance Field (NeRF) that could render
seasonal features independently of viewing angle and solar angle while still
being able to render shadows. We teach our network to render seasonal features
by introducing one more input variable -- time of the year. However, the small
training datasets typical of satellite imagery can introduce ambiguities in
cases where shadows are present in the same location for every image of a
particular season. We add additional terms to the loss function to discourage
the network from using seasonal features for accounting for shadows. We show
the performance of our network on eight Areas of Interest containing images
captured by the Maxar WorldView-3 satellite. This evaluation includes tests
measuring the ability of our framework to accurately render novel views,
generate height maps, predict shadows, and specify seasonal features
independently from shadows. Our ablation studies justify the choices made for
network design parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gableman_M/0/1/0/all/0/1&quot;&gt;Michael Gableman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kak_A/0/1/0/all/0/1&quot;&gt;Avinash Kak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02162">
<title>Learning Referring Video Object Segmentation from Weak Annotation. (arXiv:2308.02162v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02162</link>
<description rdf:parseType="Literal">&lt;p&gt;Referring video object segmentation (RVOS) is a task that aims to segment the
target object in all video frames based on a sentence describing the object.
Although existing RVOS methods have achieved significant performance, they
depend on densely-annotated datasets, which are expensive and time-consuming to
obtain. In this paper, we propose a new annotation scheme that reduces the
annotation effort by 8 times, while providing sufficient supervision for RVOS.
Our scheme only requires a mask for the frame where the object first appears
and bounding boxes for the rest of the frames. Based on this scheme, we develop
a novel RVOS method that exploits weak annotations effectively. Specifically,
we build a simple but effective baseline model, SimRVOS, for RVOS with weak
annotation. Then, we design a cross frame segmentation module, which uses the
language-guided dynamic filters from one frame to segment the target object in
other frames to thoroughly leverage the valuable mask annotation and bounding
boxes. Finally, we develop a bi-level contrastive learning method to enhance
the pixel-level discriminative representation of the model with weak
annotation. We conduct extensive experiments to show that our method achieves
comparable or even superior performance to fully-supervised methods, without
requiring dense mask annotations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wangbo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nan_K/0/1/0/all/0/1&quot;&gt;Kepan Nan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Songyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1&quot;&gt;Yang You&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11681">
<title>VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video Anomaly Detection. (arXiv:2308.11681v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11681</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent contrastive language-image pre-training (CLIP) model has shown
great success in a wide range of image-level tasks, revealing remarkable
ability for learning powerful visual representations with rich semantics. An
open and worthwhile problem is efficiently adapting such a strong model to the
video domain and designing a robust video anomaly detector. In this work, we
propose VadCLIP, a new paradigm for weakly supervised video anomaly detection
(WSVAD) by leveraging the frozen CLIP model directly without any pre-training
and fine-tuning process. Unlike current works that directly feed extracted
features into the weakly supervised classifier for frame-level binary
classification, VadCLIP makes full use of fine-grained associations between
vision and language on the strength of CLIP and involves dual branch. One
branch simply utilizes visual features for coarse-grained binary
classification, while the other fully leverages the fine-grained language-image
alignment. With the benefit of dual branch, VadCLIP achieves both
coarse-grained and fine-grained video anomaly detection by transferring
pre-trained knowledge from CLIP to WSVAD task. We conduct extensive experiments
on two commonly-used benchmarks, demonstrating that VadCLIP achieves the best
performance on both coarse-grained and fine-grained WSVAD, surpassing the
state-of-the-art methods by a large margin. Specifically, VadCLIP achieves
84.51% AP and 88.02% AUC on XD-Violence and UCF-Crime, respectively. Code and
features are released at https://github.com/nwpu-zxr/VadCLIP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1&quot;&gt;Peng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xuerong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1&quot;&gt;Guansong Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Lingru Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1&quot;&gt;Qingsen Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanning Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11932">
<title>Synergistic Multiscale Detail Refinement via Intrinsic Supervision for Underwater Image Enhancement. (arXiv:2308.11932v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11932</link>
<description rdf:parseType="Literal">&lt;p&gt;Visually restoring underwater scenes primarily involves mitigating
interference from underwater media. Existing methods ignore the inherent
scale-related characteristics in underwater scenes. Therefore, we present the
synergistic multi-scale detail refinement via intrinsic supervision (SMDR-IS)
for enhancing underwater scene details, which contain multi-stages. The
low-degradation stage from the original images furnishes the original stage
with multi-scale details, achieved through feature propagation using the
Adaptive Selective Intrinsic Supervised Feature (ASISF) module. By using
intrinsic supervision, the ASISF module can precisely control and guide feature
transmission across multi-degradation stages, enhancing multi-scale detail
refinement and minimizing the interference from irrelevant information in the
low-degradation stage. In multi-degradation encoder-decoder framework of
SMDR-IS, we introduce the Bifocal Intrinsic-Context Attention Module (BICA).
Based on the intrinsic supervision principles, BICA efficiently exploits
multi-scale scene information in images. BICA directs higher-resolution spaces
by tapping into the insights of lower-resolution ones, underscoring the pivotal
role of spatial contextual relationships in underwater image restoration.
Throughout training, the inclusion of a multi-degradation loss function can
enhance the network, allowing it to adeptly extract information across diverse
scales. When benchmarked against state-of-the-art methods, SMDR-IS consistently
showcases superior performance. The code is publicly available at:
https://github.com/zhoujingchun03/SMDR-IS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dehuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingchun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;ChunLe Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weishi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongyi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15316">
<title>3D-MuPPET: 3D Multi-Pigeon Pose Estimation and Tracking. (arXiv:2308.15316v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15316</link>
<description rdf:parseType="Literal">&lt;p&gt;Markerless methods for animal posture tracking have been rapidly developing
recently, but frameworks and benchmarks for tracking large animal groups in 3D
are still lacking. To overcome this gap in the literature, we present
3D-MuPPET, a framework to estimate and track 3D poses of up to 10 pigeons at
interactive speed using multiple camera views. We train a pose estimator to
infer 2D keypoints and bounding boxes of multiple pigeons, then triangulate the
keypoints to 3D. For identity matching of individuals in all views, we first
dynamically match 2D detections to global identities in the first frame, then
use a 2D tracker to maintain IDs across views in subsequent frames. We achieve
comparable accuracy to a state of the art 3D pose estimator in terms of median
error and Percentage of Correct Keypoints. Additionally, we benchmark the
inference speed of 3D-MuPPET, with up to 9.45 fps in 2D and 1.89 fps in 3D, and
perform quantitative tracking evaluation, which yields encouraging results.
Finally, we showcase two novel applications for 3D-MuPPET. First, we train a
model with data of single pigeons and achieve comparable results in 2D and 3D
posture estimation for up to 5 pigeons. Second, we show that 3D-MuPPET also
works in outdoors without additional annotations from natural environments.
Both use cases simplify the domain shift to new species and environments,
largely reducing annotation effort needed for 3D posture tracking. To the best
of our knowledge we are the first to present a framework for 2D/3D animal
posture and trajectory tracking that works in both indoor and outdoor
environments for up to 10 individuals. We hope that the framework can open up
new opportunities in studying animal collective behaviour and encourages
further developments in 3D multi-animal posture tracking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waldmann_U/0/1/0/all/0/1&quot;&gt;Urs Waldmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1&quot;&gt;Alex Hoi Hang Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naik_H/0/1/0/all/0/1&quot;&gt;Hemal Naik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagy_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe1;t&amp;#xe9; Nagy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couzin_I/0/1/0/all/0/1&quot;&gt;Iain D. Couzin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deussen_O/0/1/0/all/0/1&quot;&gt;Oliver Deussen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldluecke_B/0/1/0/all/0/1&quot;&gt;Bastian Goldluecke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kano_F/0/1/0/all/0/1&quot;&gt;Fumihiro Kano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13237">
<title>Spatial-Temporal Knowledge-Embedded Transformer for Video Scene Graph Generation. (arXiv:2309.13237v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13237</link>
<description rdf:parseType="Literal">&lt;p&gt;Video scene graph generation (VidSGG) aims to identify objects in visual
scenes and infer their relationships for a given video. It requires not only a
comprehensive understanding of each object scattered on the whole scene but
also a deep dive into their temporal motions and interactions. Inherently,
object pairs and their relationships enjoy spatial co-occurrence correlations
within each image and temporal consistency/transition correlations across
different images, which can serve as prior knowledge to facilitate VidSGG model
learning and inference. In this work, we propose a spatial-temporal
knowledge-embedded transformer (STKET) that incorporates the prior
spatial-temporal knowledge into the multi-head cross-attention mechanism to
learn more representative relationship representations. Specifically, we first
learn spatial co-occurrence and temporal transition correlations in a
statistical manner. Then, we design spatial and temporal knowledge-embedded
layers that introduce the multi-head cross-attention mechanism to fully explore
the interaction between visual representation and the knowledge to generate
spatial- and temporal-embedded representations, respectively. Finally, we
aggregate these representations for each subject-object pair to predict the
final semantic labels and their relationships. Extensive experiments show that
STKET outperforms current competing algorithms by a large margin, e.g.,
improving the mR@50 by 8.1%, 4.7%, and 2.1% on different settings over current
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_T/0/1/0/all/0/1&quot;&gt;Tao Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianshui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hefeng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yongyi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liang Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16137">
<title>Context-I2W: Mapping Images to Context-dependent Words for Accurate Zero-Shot Composed Image Retrieval. (arXiv:2309.16137v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16137</link>
<description rdf:parseType="Literal">&lt;p&gt;Different from Composed Image Retrieval task that requires expensive labels
for training task-specific models, Zero-Shot Composed Image Retrieval (ZS-CIR)
involves diverse tasks with a broad range of visual content manipulation intent
that could be related to domain, scene, object, and attribute. The key
challenge for ZS-CIR tasks is to learn a more accurate image representation
that has adaptive attention to the reference image for various manipulation
descriptions. In this paper, we propose a novel context-dependent mapping
network, named Context-I2W, for adaptively converting description-relevant
Image information into a pseudo-word token composed of the description for
accurate ZS-CIR. Specifically, an Intent View Selector first dynamically learns
a rotation rule to map the identical image to a task-specific manipulation
view. Then a Visual Target Extractor further captures local information
covering the main targets in ZS-CIR tasks under the guidance of multiple
learnable queries. The two complementary modules work together to map an image
to a context-dependent pseudo-word token without extra supervision. Our model
shows strong generalization ability on four ZS-CIR tasks, including domain
conversion, object composition, object manipulation, and attribute
manipulation. It obtains consistent and significant performance boosts ranging
from 1.88% to 3.60% over the best methods and achieves new state-of-the-art
results on ZS-CIR. Our code is available at
https://github.com/Pter61/context-i2w.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yuanmin Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1&quot;&gt;Keke Gai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1&quot;&gt;Jiamin Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_G/0/1/0/all/0/1&quot;&gt;Gang Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yue Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qi Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04687">
<title>Understanding and Improving Adversarial Attacks on Latent Diffusion Model. (arXiv:2310.04687v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04687</link>
<description rdf:parseType="Literal">&lt;p&gt;Latent Diffusion Model (LDM) achieves state-of-the-art performances in image
generation yet raising copyright and privacy concerns. Adversarial attacks on
LDM are then born to protect unauthorized images from being used in LDM-driven
few-shot generation. However, these attacks suffer from moderate performance
and excessive computational cost, especially in GPU memory. In this paper, we
propose an effective adversarial attack on LDM that shows superior performance
against state-of-the-art few-shot generation pipeline of LDM, for example,
LoRA. We implement the attack with memory efficiency by introducing several
mechanisms and decrease the memory cost of the attack to less than 6GB, which
allows individual users to run the attack on a majority of consumer GPUs. Our
proposed attack can be a practical tool for people facing the copyright and
privacy risk brought by LDM to protect themselves.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1&quot;&gt;Boyang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Chumeng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04787">
<title>HI-SLAM: Monocular Real-time Dense Mapping with Hybrid Implicit Fields. (arXiv:2310.04787v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04787</link>
<description rdf:parseType="Literal">&lt;p&gt;In this letter, we present a neural field-based real-time monocular mapping
framework for accurate and dense Simultaneous Localization and Mapping (SLAM).
Recent neural mapping frameworks show promising results, but rely on RGB-D or
pose inputs, or cannot run in real-time. To address these limitations, our
approach integrates dense-SLAM with neural implicit fields. Specifically, our
dense SLAM approach runs parallel tracking and global optimization, while a
neural field-based map is constructed incrementally based on the latest SLAM
estimates. For the efficient construction of neural fields, we employ
multi-resolution grid encoding and signed distance function (SDF)
representation. This allows us to keep the map always up-to-date and adapt
instantly to global updates via loop closing. For global consistency, we
propose an efficient Sim(3)-based pose graph bundle adjustment (PGBA) approach
to run online loop closing and mitigate the pose and scale drift. To enhance
depth accuracy further, we incorporate learned monocular depth priors. We
propose a novel joint depth and scale adjustment (JDSA) module to solve the
scale ambiguity inherent in depth priors. Extensive evaluations across
synthetic and real-world datasets validate that our approach outperforms
existing methods in accuracy and map completeness while preserving real-time
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tiecheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1&quot;&gt;Qing Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haala_N/0/1/0/all/0/1&quot;&gt;Norbert Haala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13859">
<title>Not all Fake News is Written: A Dataset and Analysis of Misleading Video Headlines. (arXiv:2310.13859v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13859</link>
<description rdf:parseType="Literal">&lt;p&gt;Polarization and the marketplace for impressions have conspired to make
navigating information online difficult for users, and while there has been a
significant effort to detect false or misleading text, multimodal datasets have
received considerably less attention. To complement existing resources, we
present multimodal Video Misleading Headline (VMH), a dataset that consists of
videos and whether annotators believe the headline is representative of the
video&apos;s contents. After collecting and annotating this dataset, we analyze
multimodal baselines for detecting misleading headlines. Our annotation process
also focuses on why annotators view a video as misleading, allowing us to
better understand the interplay of annotators&apos; background and the content of
the videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1&quot;&gt;Yoo Yeon Sung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1&quot;&gt;Jordan Boyd-Graber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassan_N/0/1/0/all/0/1&quot;&gt;Naeemul Hassan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20092">
<title>The Missing U for Efficient Diffusion Models. (arXiv:2310.20092v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20092</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion Probabilistic Models stand as a critical tool in generative
modelling, enabling the generation of complex data distributions. This family
of generative models yields record-breaking performance in tasks such as image
synthesis, video generation, and molecule design. Despite their capabilities,
their efficiency, especially in the reverse process, remains a challenge due to
slow convergence rates and high computational costs. In this paper, we
introduce an approach that leverages continuous dynamical systems to design a
novel denoising network for diffusion models that is more parameter-efficient,
exhibits faster convergence, and demonstrates increased noise robustness.
Experimenting with Denoising Diffusion Probabilistic Models (DDPMs), our
framework operates with approximately a quarter of the parameters, and $\sim$
30\% of the Floating Point Operations (FLOPs) compared to standard U-Nets in
DDPMs. Furthermore, our model is notably faster in inference than the baseline
when measured in fair and equal conditions. We also provide a mathematical
intuition as to why our proposed reverse process is faster as well as a
mathematical discussion of the empirical tradeoffs in the denoising downstream
task. Finally, we argue that our method is compatible with existing performance
enhancement techniques, enabling further improvements in efficiency, quality,
and speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calvo_Ordonez_S/0/1/0/all/0/1&quot;&gt;Sergio Calvo-Ordonez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Chun-Wun Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiahao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lipei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1&quot;&gt;Carola-Bibiane Schonlieb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1&quot;&gt;Angelica I Aviles-Rivero&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03054">
<title>AnyText: Multilingual Visual Text Generation And Editing. (arXiv:2311.03054v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03054</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion model based Text-to-Image has achieved impressive achievements
recently. Although current technology for synthesizing images is highly
advanced and capable of generating images with high fidelity, it is still
possible to give the show away when focusing on the text area in the generated
image. To address this issue, we introduce AnyText, a diffusion-based
multilingual visual text generation and editing model, that focuses on
rendering accurate and coherent text in the image. AnyText comprises a
diffusion pipeline with two primary elements: an auxiliary latent module and a
text embedding module. The former uses inputs like text glyph, position, and
masked image to generate latent features for text generation or editing. The
latter employs an OCR model for encoding stroke data as embeddings, which blend
with image caption embeddings from the tokenizer to generate texts that
seamlessly integrate with the background. We employed text-control diffusion
loss and text perceptual loss for training to further enhance writing accuracy.
AnyText can write characters in multiple languages, to the best of our
knowledge, this is the first work to address multilingual visual text
generation. It is worth mentioning that AnyText can be plugged into existing
diffusion models from the community for rendering or editing text accurately.
After conducting extensive evaluation experiments, our method has outperformed
all other approaches by a significant margin. Additionally, we contribute the
first large-scale multilingual text images dataset, AnyWord-3M, containing 3
million image-text pairs with OCR annotations in multiple languages. Based on
AnyWord-3M dataset, we propose AnyText-benchmark for the evaluation of visual
text generation accuracy and quality. Our project will be open-sourced on
https://github.com/tyxsspa/AnyText to improve and promote the development of
text generation technology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuo_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Tuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1&quot;&gt;Wangmeng Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jun-Yan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1&quot;&gt;Yifeng Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xuansong Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05836">
<title>UMedNeRF: Uncertainty-aware Single View Volumetric Rendering for Medical Neural Radiance Fields. (arXiv:2311.05836v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05836</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of clinical medicine, computed tomography (CT) is an effective
medical imaging modality for the diagnosis of various pathologies. Compared
with X-ray images, CT images can provide more information, including
multi-planar slices and three-dimensional structures for clinical diagnosis.
However, CT imaging requires patients to be exposed to large doses of ionizing
radiation for a long time, which may cause irreversible physical harm. In this
paper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on
generated radiation fields. The network can learn a continuous representation
of CT projections from 2D X-ray images by obtaining the internal structure and
depth information and using adaptive loss weights to ensure the quality of the
generated images. Our model is trained on publicly available knee and chest
datasets, and we show the results of CT projection rendering with a single
X-ray and compare our method with other methods based on generated radiation
fields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jing Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fan_Q/0/1/0/all/0/1&quot;&gt;Qinrui Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Siwei Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10329">
<title>High-fidelity Person-centric Subject-to-Image Synthesis. (arXiv:2311.10329v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10329</link>
<description rdf:parseType="Literal">&lt;p&gt;Current subject-driven image generation methods encounter significant
challenges in person-centric image generation. The reason is that they learn
the semantic scene and person generation by fine-tuning a common pre-trained
diffusion, which involves an irreconcilable training imbalance. Precisely, to
generate realistic persons, they need to sufficiently tune the pre-trained
model, which inevitably causes the model to forget the rich semantic scene
prior and makes scene generation over-fit to the training data. Moreover, even
with sufficient fine-tuning, these methods can still not generate high-fidelity
persons since joint learning of the scene and person generation also lead to
quality compromise. In this paper, we propose Face-diffuser, an effective
collaborative generation pipeline to eliminate the above training imbalance and
quality compromise. Specifically, we first develop two specialized pre-trained
diffusion models, i.e., Text-driven Diffusion Model (TDM) and Subject-augmented
Diffusion Model (SDM), for scene and person generation, respectively. The
sampling process is divided into three sequential stages, i.e., semantic scene
construction, subject-scene fusion, and subject enhancement. The first and last
stages are performed by TDM and SDM respectively. The subject-scene fusion
stage, that is the collaboration achieved through a novel and highly effective
mechanism, Saliency-adaptive Noise Fusion (SNF). Specifically, it is based on
our key observation that there exists a robust link between classifier-free
guidance responses and the saliency of generated images. In each time step, SNF
leverages the unique strengths of each model and allows for the spatial
blending of predicted noises from both models automatically in a saliency-aware
manner. Extensive experiments confirm the impressive effectiveness and
robustness of the Face-diffuser.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yibin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weizhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jianwei Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Cheng Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13194">
<title>Towards Improving Document Understanding: An Exploration on Text-Grounding via MLLMs. (arXiv:2311.13194v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13194</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of document understanding, significant advances have been made
in the fine-tuning of Multimodal Large Language Models (MLLMs) with
instruction-following data. Nevertheless, the potential of text-grounding
capability within text-rich scenarios remains underexplored. In this paper, we
present a text-grounding document understanding model, termed TGDoc, which
addresses this deficiency by enhancing MLLMs with the ability to discern the
spatial positioning of text within images. Empirical evidence suggests that
text-grounding improves the model&apos;s interpretation of textual content, thereby
elevating its proficiency in comprehending text-rich images. Specifically, we
compile a dataset containing 99K PowerPoint presentations sourced from the
internet. We formulate instruction tuning tasks including text detection,
recognition, and spotting to facilitate the cohesive alignment between the
visual encoder and large language model. Moreover, we curate a collection of
text-rich images and prompt the text-only GPT-4 to generate 12K high-quality
conversations, featuring textual locations within text-rich scenarios. By
integrating text location data into the instructions, TGDoc is adept at
discerning text locations during the visual question process. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
across multiple text-rich benchmarks, validating the effectiveness of our
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yonghui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wengang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Hao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Keyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Houqiang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16835">
<title>Unified-modal Salient Object Detection via Adaptive Prompt Learning. (arXiv:2311.16835v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16835</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing single-modal and multi-modal salient object detection (SOD) methods
focus on designing specific architectures tailored for their respective tasks.
However, developing completely different models for different tasks leads to
labor and time consumption, as well as high computational and practical
deployment costs. In this paper, we make the first attempt to address both
single-modal and multi-modal SOD in a unified framework called UniSOD.
Nevertheless, assigning appropriate strategies to modality variable inputs is
challenging. To this end, UniSOD learns modality-aware prompts with
task-specific hints through adaptive prompt learning, which are plugged into
the proposed pre-trained baseline SOD model to handle corresponding tasks,
while only requiring few learnable parameters compared to training the entire
model. Each modality-aware prompt is generated from a switchable prompt
generation block, which performs structural switching solely relied on
single-modal and multi-modal inputs. UniSOD achieves consistent performance
improvement on 14 benchmark datasets for RGB, RGB-D, and RGB-T SOD, which
demonstrates that our method effectively and efficiently unifies single-modal
and multi-modal SOD tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kunpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenglong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhengzheng Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Bin Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16973">
<title>DemoFusion: Democratising High-Resolution Image Generation With No $$$. (arXiv:2311.16973v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16973</link>
<description rdf:parseType="Literal">&lt;p&gt;High-resolution image generation with Generative Artificial Intelligence
(GenAI) has immense potential but, due to the enormous capital investment
required for training, it is increasingly centralised to a few large
corporations, and hidden behind paywalls. This paper aims to democratise
high-resolution GenAI by advancing the frontier of high-resolution generation
while remaining accessible to a broad audience. We demonstrate that existing
Latent Diffusion Models (LDMs) possess untapped potential for higher-resolution
image generation. Our novel DemoFusion framework seamlessly extends open-source
GenAI models, employing Progressive Upscaling, Skip Residual, and Dilated
Sampling mechanisms to achieve higher-resolution image generation. The
progressive nature of DemoFusion requires more passes, but the intermediate
results can serve as &quot;previews&quot;, facilitating rapid prompt iteration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1&quot;&gt;Ruoyi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1&quot;&gt;Dongliang Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1&quot;&gt;Timothy Hospedales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yi-Zhe Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhanyu Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04008">
<title>Natural-language-driven Simulation Benchmark and Copilot for Efficient Production of Object Interactions in Virtual Road Scenes. (arXiv:2312.04008v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04008</link>
<description rdf:parseType="Literal">&lt;p&gt;We advocate the idea of the natural-language-driven(NLD) simulation to
efficiently produce the object interactions between multiple objects in the
virtual road scenes, for teaching and testing the autonomous driving systems
that should take quick action to avoid collision with obstacles with
unpredictable motions. The NLD simulation allows the brief natural-language
description to control the object interactions, significantly reducing the
human efforts for creating a large amount of interaction data. To facilitate
the research of NLD simulation, we collect the Language-to-Interaction(L2I)
benchmark dataset with 120,000 natural-language descriptions of object
interactions in 6 common types of road topologies. Each description is
associated with the programming code, which the graphic render can use to
visually reconstruct the object interactions in the virtual scenes. As a
methodology contribution, we design SimCopilot to translate the interaction
descriptions to the renderable code. We use the L2I dataset to evaluate
SimCopilot&apos;s abilities to control the object motions, generate complex
interactions, and generalize interactions across road topologies. The L2I
dataset and the evaluation results motivate the relevant research of the NLD
simulation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kairui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zihao Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Gengjie Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Haotian Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_D/0/1/0/all/0/1&quot;&gt;Die Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jibin Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhecheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fupeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1&quot;&gt;Ziyun Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Di Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04233">
<title>Fine-tuning vision foundation model for crack segmentation in civil infrastructures. (arXiv:2312.04233v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04233</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale foundation models have become the mainstream deep learning
method, while in civil engineering, the scale of AI models is strictly limited.
In this work, a vision foundation model is introduced for crack segmentation.
Two parameter-efficient fine-tuning methods, adapter and low-rank adaptation,
are adopted to fine-tune the foundation model in semantic segmentation: the
Segment Anything Model (SAM). The fine-tuned CrackSAM model is much larger than
all the existing crack segmentation models but shows excellent performance. To
test the zero-shot performance of the proposed method, two unique datasets
related to road and exterior wall cracks are collected, annotated and
open-sourced, for a total of 810 images. Comparative experiments are conducted
with twelve mature semantic segmentation models. On datasets with artificial
noise and previously unseen datasets, the performance of CrackSAM far exceeds
that of all state-of-the-art models. CrackSAM exhibits remarkable superiority,
particularly under challenging conditions such as dim lighting, shadows, road
markings, construction joints, and other interference factors. These
cross-scenario results demonstrate the outstanding zero-shot capability of
foundation models and provide new ideas for developing vision models in civil
engineering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_K/0/1/0/all/0/1&quot;&gt;Kang Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yutao Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yansong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhenzhong Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05777">
<title>Negative Pre-aware for Noisy Cross-modal Matching. (arXiv:2312.05777v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05777</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-modal noise-robust learning is a challenging task since noisy
correspondence is hard to recognize and rectify. Due to the cumulative and
unavoidable negative impact of unresolved noise, existing methods cannot
maintain a stable performance when the noise increases. In this paper, we
present a novel Negative Pre-aware Cross-modal (NPC) matching solution for
large visual-language model fine-tuning on noisy downstream tasks. It is
featured in two aspects: (1) For noise recognition and resistance, previous
methods usually directly filter out a noise subset, we propose to estimate the
negative impact of each sample. It does not need additional correction
mechanisms that may predict unreliable correction results, leading to
self-reinforcing error. We assign a confidence weight to each sample according
to its negative impact in the training process. This adaptively adjusts the
contribution of each sample to avoid noisy accumulation. (2) For maintaining
stable performance with increasing noise, we utilize the memorization effect of
DNNs by maintaining a memory bank. Specifically, we apply GMM to select
high-confident clean samples as the memory entry, where the memory entry is
used to estimate the negative impact of each sample. Since clean samples are
easier distinguished by GMM with increasing noise, the memory bank can still
maintain high quality at a high noise ratio. Compared to the correction
mechanism focusing on noise samples, memory bank-based estimation is more
robust, which makes the model performance stable on noisy datasets. Extensive
experiments demonstrate that our method significantly improves matching
accuracy and performance stability at increasing noise ratio. Our approach also
surpasses the state-of-the-art methods by a large margin. The code is available
at: https://github.com/ZhangXu0963/NPC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1&quot;&gt;Mang Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07009">
<title>Vision-language Assisted Attribute Learning. (arXiv:2312.07009v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07009</link>
<description rdf:parseType="Literal">&lt;p&gt;Attribute labeling at large scale is typically incomplete and partial, posing
significant challenges to model optimization. Existing attribute learning
methods often treat the missing labels as negative or simply ignore them all
during training, either of which could hamper the model performance to a great
extent. To overcome these limitations, in this paper we leverage the available
vision-language knowledge to explicitly disclose the missing labels for
enhancing model learning. Given an image, we predict the likelihood of each
missing attribute label assisted by an off-the-shelf vision-language model, and
randomly select to ignore those with high scores in training. Our strategy
strikes a good balance between fully ignoring and negatifying the missing
labels, as these high scores are found to be informative on revealing label
ambiguity. Extensive experiments show that our proposed vision-language
assisted loss can achieve state-of-the-art performance on the newly cleaned VAW
dataset. Qualitative evaluation demonstrates the ability of the proposed method
in predicting more complete attributes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Kongming Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1&quot;&gt;Donghui Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1&quot;&gt;Ling Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weidong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhanyu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jun Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08054">
<title>Semantic Complete Scene Forecasting from a 4D Dynamic Point Cloud Sequence. (arXiv:2312.08054v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08054</link>
<description rdf:parseType="Literal">&lt;p&gt;We study a new problem of semantic complete scene forecasting (SCSF) in this
work. Given a 4D dynamic point cloud sequence, our goal is to forecast the
complete scene corresponding to the future next frame along with its semantic
labels. To tackle this challenging problem, we properly model the synergetic
relationship between future forecasting and semantic scene completion through a
novel network named SCSFNet. SCSFNet leverages a hybrid geometric
representation for high-resolution complete scene forecasting. To leverage
multi-frame observation as well as the understanding of scene dynamics to ease
the completion task, SCSFNet introduces an attention-based skip connection
scheme. To ease the need to model occlusion variations and to better focus on
the occluded part, SCSFNet utilizes auxiliary visibility grids to guide the
forecasting task. To evaluate the effectiveness of SCSFNet, we conduct
experiments on various benchmarks including two large-scale indoor benchmarks
we contributed and the outdoor SemanticKITTI benchmark. Extensive experiments
show SCSFNet outperforms baseline methods on multiple metrics by a large
margin, and also prove the synergy between future forecasting and semantic
scene completion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1&quot;&gt;Zhuorui Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haoran Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1&quot;&gt;Li Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08078">
<title>Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation. (arXiv:2312.08078v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08078</link>
<description rdf:parseType="Literal">&lt;p&gt;To address these issues, we propose a novel Adaptive patch-word Matching
(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in
medical reports and apply it to CXR-report generation to provide explainability
for the generation process. AdaMatch exploits the fine-grained relation between
adaptive patches and words to provide explanations of specific image regions
with corresponding words. To capture the abnormal regions of varying sizes and
positions, we introduce the Adaptive Patch extraction (AdaPatch) module to
acquire the adaptive patches for these regions adaptively. In order to provide
explicit explainability for CXR-report generation task, we propose an
AdaMatch-based bidirectional large language model for Cyclic CXR-report
generation (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords
for CXR images and `keypatches&apos; for medical reports as hints to guide
CXR-report generation. Extensive experiments on two publicly available CXR
datasets prove the effectiveness of our method and its superior performance to
existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Linlin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yixuan Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08733">
<title>VMT-Adapter: Parameter-Efficient Transfer Learning for Multi-Task Dense Scene Understanding. (arXiv:2312.08733v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08733</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale pre-trained models have achieved remarkable success in various
computer vision tasks. A standard approach to leverage these models is to
fine-tune all model parameters for downstream tasks, which poses challenges in
terms of computational and storage costs. Recently, inspired by Natural
Language Processing (NLP), parameter-efficient transfer learning has been
successfully applied to vision tasks. However, most existing techniques
primarily focus on single-task adaptation, and despite limited research on
multi-task adaptation, these methods often exhibit suboptimal training and
inference efficiency. In this paper, we first propose an once-for-all Vision
Multi-Task Adapter (VMT-Adapter), which strikes approximately O(1) training and
inference efficiency w.r.t task number. Concretely, VMT-Adapter shares the
knowledge from multiple tasks to enhance cross-task interaction while preserves
task-specific knowledge via independent knowledge extraction modules. Notably,
since task-specific modules require few parameters, VMT-Adapter can handle an
arbitrary number of tasks with a negligible increase of trainable parameters.
We also propose VMT-Adapter-Lite, which further reduces the trainable
parameters by learning shared parameters between down- and up-projections.
Extensive experiments on four dense scene understanding tasks demonstrate the
superiority of VMT-Adapter(-Lite), achieving a 3.96%(1.34%) relative
improvement compared to single-task full fine-tuning, while utilizing merely
~1% (0.36%) trainable parameters of the pre-trained model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_Y/0/1/0/all/0/1&quot;&gt;Yi Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;Junlong Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhiwen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Ke Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08774">
<title>VSFormer: Visual-Spatial Fusion Transformer for Correspondence Pruning. (arXiv:2312.08774v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08774</link>
<description rdf:parseType="Literal">&lt;p&gt;Correspondence pruning aims to find correct matches (inliers) from an initial
set of putative correspondences, which is a fundamental task for many
applications. The process of finding is challenging, given the varying inlier
ratios between scenes/image pairs due to significant visual differences.
However, the performance of the existing methods is usually limited by the
problem of lacking visual cues (\eg texture, illumination, structure) of
scenes. In this paper, we propose a Visual-Spatial Fusion Transformer
(VSFormer) to identify inliers and recover camera poses accurately. Firstly, we
obtain highly abstract visual cues of a scene with the cross attention between
local features of two-view images. Then, we model these visual cues and
correspondences by a joint visual-spatial fusion module, simultaneously
embedding visual cues into correspondences for pruning. Additionally, to mine
the consistency of correspondences, we also design a novel module that combines
the KNN-based graph and the transformer, effectively capturing both local and
global contexts. Extensive experiments have demonstrated that the proposed
VSFormer outperforms state-of-the-art methods on outdoor and indoor benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_T/0/1/0/all/0/1&quot;&gt;Tangfei Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoqin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Li Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1&quot;&gt;Guobao Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08782">
<title>Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis. (arXiv:2312.08782v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08782</link>
<description rdf:parseType="Literal">&lt;p&gt;Building general-purpose robots that can operate seamlessly, in any
environment, with any object, and utilizing various skills to complete diverse
tasks has been a long-standing goal in Artificial Intelligence. Unfortunately,
however, most existing robotic systems have been constrained - having been
designed for specific tasks, trained on specific datasets, and deployed within
specific environments. These systems usually require extensively-labeled data,
rely on task-specific models, have numerous generalization issues when deployed
in real-world scenarios, and struggle to remain robust to distribution shifts.
Motivated by the impressive open-set performance and content generation
capabilities of web-scale, large-capacity pre-trained models (i.e., foundation
models) in research fields such as Natural Language Processing (NLP) and
Computer Vision (CV), we devote this survey to exploring (i) how these existing
foundation models from NLP and CV can be applied to the field of robotics, and
also exploring (ii) what a robotics-specific foundation model would look like.
We begin by providing an overview of what constitutes a conventional robotic
system and the fundamental barriers to making it universally applicable. Next,
we establish a taxonomy to discuss current work exploring ways to leverage
existing foundation models for robotics and develop ones catered to robotics.
Finally, we discuss key challenges and promising future directions in using
foundation models for enabling general-purpose robotic systems. We encourage
readers to view our living GitHub repository of resources, including papers
reviewed in this survey as well as related projects and repositories for
developing foundation models for robotics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yafei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Quanting Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vidhi Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1&quot;&gt;Jonathan Francis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patrikar_J/0/1/0/all/0/1&quot;&gt;Jay Patrikar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keetha_N/0/1/0/all/0/1&quot;&gt;Nikhil Keetha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seungchan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yaqi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shibo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chong_Y/0/1/0/all/0/1&quot;&gt;Yu Quan Chong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1&quot;&gt;Katia Sycara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_Roberson_M/0/1/0/all/0/1&quot;&gt;Matthew Johnson-Roberson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1&quot;&gt;Dhruv Batra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1&quot;&gt;Sebastian Scherer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1&quot;&gt;Zsolt Kira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bisk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08887">
<title>SpeedUpNet: A Plug-and-Play Hyper-Network for Accelerating Text-to-Image Diffusion Models. (arXiv:2312.08887v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08887</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image diffusion models (SD) exhibit significant advancements while
requiring extensive computational resources. Though many acceleration methods
have been proposed, they suffer from generation quality degradation or extra
training cost generalizing to new fine-tuned models. To address these
limitations, we propose a novel and universal Stable-Diffusion (SD)
acceleration module called SpeedUpNet(SUN). SUN can be directly plugged into
various fine-tuned SD models without extra training. This technique utilizes
cross-attention layers to learn the relative offsets in the generated image
results between negative and positive prompts achieving classifier-free
guidance distillation with negative prompts controllable, and introduces a
Multi-Step Consistency (MSC) loss to ensure a harmonious balance between
reducing inference steps and maintaining consistency in the generated output.
Consequently, SUN significantly reduces the number of inference steps to just 4
steps and eliminates the need for classifier-free guidance. It leads to an
overall speedup of more than 10 times for SD models compared to the
state-of-the-art 25-step DPM-solver++, and offers two extra advantages: (1)
classifier-free guidance distillation with controllable negative prompts and
(2) seamless integration into various fine-tuned Stable-Diffusion models
without training. The effectiveness of the SUN has been verified through
extensive experimentation. Project Page:
https://williechai.github.io/speedup-plugin-for-stable-diffusions.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_W/0/1/0/all/0/1&quot;&gt;Weilong Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1&quot;&gt;DanDan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jiajiong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhiquan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changbao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chenguang Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09076">
<title>ProSGNeRF: Progressive Dynamic Neural Scene Graph with Frequency Modulated Auto-Encoder in Urban Scenes. (arXiv:2312.09076v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09076</link>
<description rdf:parseType="Literal">&lt;p&gt;Implicit neural representation has demonstrated promising results in view
synthesis for large and complex scenes. However, existing approaches either
fail to capture the fast-moving objects or need to build the scene graph
without camera ego-motions, leading to low-quality synthesized views of the
scene. We aim to jointly solve the view synthesis problem of large-scale urban
scenes and fast-moving vehicles, which is more practical and challenging. To
this end, we first leverage a graph structure to learn the local scene
representations of dynamic objects and the background. Then, we design a
progressive scheme that dynamically allocates a new local scene graph trained
with frames within a temporal window, allowing us to scale up the
representation to an arbitrarily large scene. Besides, the training views of
urban scenes are relatively sparse, which leads to a significant decline in
reconstruction accuracy for dynamic objects. Therefore, we design a frequency
auto-encoder network to encode the latent code and regularize the frequency
range of objects, which can enhance the representation of dynamic objects and
address the issue of sparse image inputs. Additionally, we employ lidar point
projection to maintain geometry consistency in large-scale urban scenes.
Experimental results demonstrate that our method achieves state-of-the-art view
synthesis accuracy, object manipulation, and scene roaming ability. The code
will be open-sourced upon paper acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_T/0/1/0/all/0/1&quot;&gt;Tianchen Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Siyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yejia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Danwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weidong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09093">
<title>Aleth-NeRF: Illumination Adaptive NeRF with Concealing Field Assumption. (arXiv:2312.09093v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09093</link>
<description rdf:parseType="Literal">&lt;p&gt;The standard Neural Radiance Fields (NeRF) paradigm employs a viewer-centered
methodology, entangling the aspects of illumination and material reflectance
into emission solely from 3D points. This simplified rendering approach
presents challenges in accurately modeling images captured under adverse
lighting conditions, such as low light or over-exposure. Motivated by the
ancient Greek emission theory that posits visual perception as a result of rays
emanating from the eyes, we slightly refine the conventional NeRF framework to
train NeRF under challenging light conditions and generate normal-light
condition novel views unsupervised. We introduce the concept of a &quot;Concealing
Field,&quot; which assigns transmittance values to the surrounding air to account
for illumination effects. In dark scenarios, we assume that object emissions
maintain a standard lighting level but are attenuated as they traverse the air
during the rendering process. Concealing Field thus compel NeRF to learn
reasonable density and colour estimations for objects even in dimly lit
situations. Similarly, the Concealing Field can mitigate over-exposed emissions
during the rendering stage. Furthermore, we present a comprehensive multi-view
dataset captured under challenging illumination conditions for evaluation. Our
code and dataset available at https://github.com/cuiziteng/Aleth-NeRF
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1&quot;&gt;Ziteng Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1&quot;&gt;Lin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xianzheng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1&quot;&gt;Tatsuya Harada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09095">
<title>ColNeRF: Collaboration for Generalizable Sparse Input Neural Radiance Field. (arXiv:2312.09095v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09095</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRF) have demonstrated impressive potential in
synthesizing novel views from dense input, however, their effectiveness is
challenged when dealing with sparse input. Existing approaches that incorporate
additional depth or semantic supervision can alleviate this issue to an extent.
However, the process of supervision collection is not only costly but also
potentially inaccurate, leading to poor performance and generalization ability
in diverse scenarios. In our work, we introduce a novel model: the
Collaborative Neural Radiance Fields (ColNeRF) designed to work with sparse
input. The collaboration in ColNeRF includes both the cooperation between
sparse input images and the cooperation between the output of the neural
radiation field. Through this, we construct a novel collaborative module that
aligns information from various views and meanwhile imposes self-supervised
constraints to ensure multi-view consistency in both geometry and appearance. A
Collaborative Cross-View Volume Integration module (CCVI) is proposed to
capture complex occlusions and implicitly infer the spatial location of
objects. Moreover, we introduce self-supervision of target rays projected in
multiple directions to ensure geometric and color consistency in adjacent
regions. Benefiting from the collaboration at the input and output ends,
ColNeRF is capable of capturing richer and more generalized scene
representation, thereby facilitating higher-quality results of the novel view
synthesis. Extensive experiments demonstrate that ColNeRF outperforms
state-of-the-art sparse input generalizable NeRF methods. Furthermore, our
approach exhibits superiority in fine-tuning towards adapting to new scenes,
achieving competitive performance compared to per-scene optimized NeRF-based
methods while significantly reducing computational costs. Our code is available
at: https://github.com/eezkni/ColNeRF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1&quot;&gt;Zhangkai Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1&quot;&gt;Peiqi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenhan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanli Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1&quot;&gt;Sam Kwong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09228">
<title>3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting. (arXiv:2312.09228v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09228</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce an approach that creates animatable human avatars from monocular
videos using 3D Gaussian Splatting (3DGS). Existing methods based on neural
radiance fields (NeRFs) achieve high-quality novel-view/novel-pose image
synthesis but often require days of training, and are extremely slow at
inference time. Recently, the community has explored fast grid structures for
efficient training of clothed avatars. Albeit being extremely fast at training,
these methods can barely achieve an interactive rendering frame rate with
around 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a
non-rigid deformation network to reconstruct animatable clothed human avatars
that can be trained within 30 minutes and rendered at real-time frame rates
(50+ FPS). Given the explicit nature of our representation, we further
introduce as-isometric-as-possible regularizations on both the Gaussian mean
vectors and the covariance matrices, enhancing the generalization of our model
on highly articulated unseen poses. Experimental results show that our method
achieves comparable and even better performance compared to state-of-the-art
approaches on animatable avatar creation from a monocular input, while being
400x and 250x faster in training and inference, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1&quot;&gt;Zhiyin Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shaofei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mihajlovic_M/0/1/0/all/0/1&quot;&gt;Marko Mihajlovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1&quot;&gt;Andreas Geiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Siyu Tang&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>