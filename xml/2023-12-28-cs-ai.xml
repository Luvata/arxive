<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-26T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14935" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14954" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14972" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14977" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14990" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14996" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15002" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15064" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15106" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2008.07007" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.01537" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.04941" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.01077" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.10789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.01515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.08143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.02442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.02646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.12723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.02758" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.01829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.07695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.04178" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.14691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.14496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.00670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.02711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06876" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.01461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14910" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18341" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05685" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05385" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16082" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10169" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14089" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09180" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14970" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01569" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.14403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15296" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18652" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01248" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11261" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12188" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01339" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01678" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04025" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07625" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08642" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08901" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10385" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10407" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11562" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11571" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11583" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14187" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14222" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14769" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14852" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10868" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.14934">
<title>aoip.ai: An Open-Source P2P SDK. (arXiv:2312.14934v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2312.14934</link>
<description rdf:parseType="Literal">&lt;p&gt;This white paper introduces aoip.ai, a groundbreaking open-source SDK
incorporating peer-to-peer technology and advanced AI integration to transform
VoIP and IoT applications. It addresses key market challenges by enhancing data
security, elevating communication quality, and providing greater flexibility
for developers and users. Developed in collaboration with Carnegie Mellon
University, aoip.ai sets a new standard for decentralized and democratized
communication solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konan_J/0/1/0/all/0/1&quot;&gt;Joseph Konan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agnihotri_S/0/1/0/all/0/1&quot;&gt;Shikhar Agnihotri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Chia-Chun Hsieh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14935">
<title>AS-XAI: Self-supervised Automatic Semantic Interpretation for CNN. (arXiv:2312.14935v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14935</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainable artificial intelligence (XAI) aims to develop transparent
explanatory approaches for &quot;black-box&quot; deep learning models. However,it remains
difficult for existing methods to achieve the trade-off of the three key
criteria in interpretability, namely, reliability, causality, and usability,
which hinder their practical applications. In this paper, we propose a
self-supervised automatic semantic interpretable explainable artificial
intelligence (AS-XAI) framework, which utilizes transparent orthogonal
embedding semantic extraction spaces and row-centered principal component
analysis (PCA) for global semantic interpretation of model decisions in the
absence of human interference, without additional computational costs. In
addition, the invariance of filter feature high-rank decomposition is used to
evaluate model sensitivity to different semantic concepts. Extensive
experiments demonstrate that robust and orthogonal semantic spaces can be
automatically extracted by AS-XAI, providing more effective global
interpretability for convolutional neural networks (CNNs) and generating
human-comprehensible explanations. The proposed approach offers broad
fine-grained extensible practical applications, including shared semantic
interpretation under out-of-distribution (OOD) categories, auxiliary
explanations for species that are challenging to distinguish, and
classification explanations from various perspectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Changqi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuntian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dongxiao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14936">
<title>PerCNet: Periodic Complete Representation for Crystal Graphs. (arXiv:2312.14936v1 [cond-mat.mtrl-sci])</title>
<link>http://arxiv.org/abs/2312.14936</link>
<description rdf:parseType="Literal">&lt;p&gt;Crystal material representation is the foundation of crystal material
research. Existing works consider crystal molecules as graph data with
different representation methods and leverage the advantages of techniques in
graph learning. A reasonable crystal representation method should capture the
local and global information. However, existing methods only consider the local
information of crystal molecules by modeling the bond distance and bond angle
of first-order neighbors of atoms, which leads to the issue that different
crystals will have the same representation. To solve this many-to-one issue, we
consider the global information by further considering dihedral angles, which
can guarantee that the proposed representation corresponds one-to-one with the
crystal material. We first propose a periodic complete representation and
calculation algorithm for infinite extended crystal materials. A theoretical
proof for the representation that satisfies the periodic completeness is
provided. Based on the proposed representation, we then propose a network for
predicting crystal material properties, PerCNet, with a specially designed
message passing mechanism. Extensive experiments are conducted on two
real-world material benchmark datasets. The PerCNet achieves the best
performance among baseline methods in terms of MAE. In addition, our results
demonstrate the importance of the periodic scheme and completeness for crystal
representation learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Xing_Q/0/1/0/all/0/1&quot;&gt;Qianli Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jinglong Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bo Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14942">
<title>Liquid State Genetic Programming. (arXiv:2312.14942v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2312.14942</link>
<description rdf:parseType="Literal">&lt;p&gt;A new Genetic Programming variant called Liquid State Genetic Programming
(LSGP) is proposed in this paper. LSGP is a hybrid method combining a dynamic
memory for storing the inputs (the liquid) and a Genetic Programming technique
used for the problem solving part. Several numerical experiments with LSGP are
performed by using several benchmarking problems. Numerical experiments show
that LSGP performs similarly and sometimes even better than standard Genetic
Programming for the considered test problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oltean_M/0/1/0/all/0/1&quot;&gt;Mihai Oltean&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14945">
<title>Empowering ChatGPT-Like Large-Scale Language Models with Local Knowledge Base for Industrial Prognostics and Health Management. (arXiv:2312.14945v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2312.14945</link>
<description rdf:parseType="Literal">&lt;p&gt;Prognostics and health management (PHM) is essential for industrial operation
and maintenance, focusing on predicting, diagnosing, and managing the health
status of industrial systems. The emergence of the ChatGPT-Like large-scale
language model (LLM) has begun to lead a new round of innovation in the AI
field. It has extensively promoted the level of intelligence in various fields.
Therefore, it is also expected further to change the application paradigm in
industrial PHM and promote PHM to become intelligent. Although ChatGPT-Like
LLMs have rich knowledge reserves and powerful language understanding and
generation capabilities, they lack domain-specific expertise, significantly
limiting their practicability in PHM applications. To this end, this study
explores the ChatGPT-Like LLM empowered by the local knowledge base (LKB) in
industrial PHM to solve the above limitations. In addition, we introduce the
method and steps of combining the LKB with LLMs, including LKB preparation, LKB
vectorization, prompt engineering, etc. Experimental analysis of real cases
shows that combining the LKB with ChatGPT-Like LLM can significantly improve
its performance and make ChatGPT-Like LLMs more accurate, relevant, and able to
provide more insightful information. This can promote the development of
ChatGPT-Like LLMs in industrial PHM and promote their efficiency and quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yan-Fu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1&quot;&gt;Min Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14948">
<title>An Evolving Population Approach to Data-Stream Classification with Extreme Verification Latency. (arXiv:2312.14948v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2312.14948</link>
<description rdf:parseType="Literal">&lt;p&gt;Recognising and reacting to change in non-stationary data-streams is a
challenging task. The majority of research in this area assumes that the true
class label of incoming points are available, either at each time step or
intermittently with some latency. In the worse case this latency approaches
infinity and we can assume that no labels are available beyond the initial
training set. When change is expected and no further training labels are
provided the challenge of maintaining a high classification accuracy is very
great. The challenge is to propagate the original training information through
several timesteps, possibly indefinitely, while adapting to underlying change
in the data-stream. In this paper we conduct an initial study into the
effectiveness of using an evolving, population-based approach as the mechanism
for adapting to change. An ensemble of one-class-classifiers is maintained for
each class. Each classifier is considered as an agent in the sub-population and
is subject to selection pressure to find interesting areas of the feature
space. This selection pressure forces the ensemble to adapt to the underlying
change in the data-stream.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fahy_C/0/1/0/all/0/1&quot;&gt;Conor Fahy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shengxiang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14949">
<title>LLM Interactive Optimization of Open Source Python Libraries -- Case Studies and Generalization. (arXiv:2312.14949v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2312.14949</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advent of large language models (LLMs) like GPT-3, a natural
question is the extent to which these models can be utilized for source code
optimization. This paper presents methodologically stringent case studies
applied to well-known open source python libraries pillow and numpy. We find
that contemporary LLM ChatGPT-4 (state September and October 2023) is
surprisingly adept at optimizing energy and compute efficiency. However, this
is only the case in interactive use, with a human expert in the loop. Aware of
experimenter bias, we document our qualitative approach in detail, and provide
transcript and source code. We start by providing a detailed description of our
approach in conversing with the LLM to optimize the _getextrema function in the
pillow library, and a quantitative evaluation of the performance improvement.
To demonstrate qualitative replicability, we report further attempts on another
locus in the pillow library, and one code locus in the numpy library, to
demonstrate generalization within and beyond a library. In all attempts, the
performance improvement is significant (factor up to 38). We have also not
omitted reporting of failed attempts (there were none). We conclude that LLMs
are a promising tool for code optimization in open source libraries, but that
the human expert in the loop is essential for success. Nonetheless, we were
surprised by how few iterations were required to achieve substantial
performance improvements that were not obvious to the expert in the loop. We
would like bring attention to the qualitative nature of this study, more robust
quantitative studies would need to introduce a layer of selecting experts in a
representative sample -- we invite the community to collaborate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Florath_A/0/1/0/all/0/1&quot;&gt;Andreas Florath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiraly_F/0/1/0/all/0/1&quot;&gt;Franz Kiraly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14950">
<title>TypeFly: Flying Drones with Large Language Model. (arXiv:2312.14950v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.14950</link>
<description rdf:parseType="Literal">&lt;p&gt;Commanding a drone with a natural language is not only user-friendly but also
opens the door for emerging language agents to control the drone. Emerging
large language models (LLMs) provide a previously impossible opportunity to
automatically translate a task description in a natural language to a program
that can be executed by the drone. However, powerful LLMs and their vision
counterparts are limited in three important ways. First, they are only
available as cloud-based services. Sending images to the cloud raises privacy
concerns. Second, they are expensive, costing proportionally to the request
size. Finally, without expensive fine-tuning, existing LLMs are quite limited
in their capability of writing a program for specialized systems like drones.
&lt;/p&gt;
&lt;p&gt;In this paper, we present a system called TypeFly that tackles the above
three problems using a combination of edge-based vision intelligence, novel
programming language design, and prompt engineering. Instead of the familiar
Python, TypeFly gets a cloud-based LLM service to write a program in a small,
custom language called MiniSpec, based on task and scene descriptions in
English. Such MiniSpec programs are not only succinct (and therefore efficient)
but also able to consult the LLM during their execution using a special skill
called query. Using a set of increasingly challenging drone tasks, we show that
design choices made by TypeFly can reduce both the cost of LLM service and the
task execution time by more than 2x. More importantly, query and prompt
engineering techniques contributed by TypeFly significantly improve the chance
of success of complex tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guojun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xiaojing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1&quot;&gt;Lin Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14954">
<title>Neuromorphic Co-Design as a Game. (arXiv:2312.14954v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2312.14954</link>
<description rdf:parseType="Literal">&lt;p&gt;Co-design is a prominent topic presently in computing, speaking to the mutual
benefit of coordinating design choices of several layers in the technology
stack. For example, this may be designing algorithms which can most efficiently
take advantage of the acceleration properties of a given architecture, while
simultaneously designing the hardware to support the structural needs of a
class of computation. The implications of these design decisions are
influential enough to be deemed a lottery, enabling an idea to win out over
others irrespective of the individual merits. Coordination is a well studied
topic in the mathematics of game theory, where in many cases without a
coordination mechanism the outcome is sub-optimal. Here we consider what
insights game theoretic analysis can offer for computer architecture co-design.
In particular, we consider the interplay between algorithm and architecture
advances in the field of neuromorphic computing. Analyzing developments of
spiking neural network algorithms and neuromorphic hardware as a co-design game
we use the Stag Hunt model to illustrate challenges for spiking algorithms or
architectures to advance the field independently and advocate for a strategic
pursuit to advance neuromorphic computing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vineyard_C/0/1/0/all/0/1&quot;&gt;Craig M. Vineyard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Severa_W/0/1/0/all/0/1&quot;&gt;William M. Severa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aimone_J/0/1/0/all/0/1&quot;&gt;James B. Aimone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14963">
<title>Optimizing Mario Adventures in a Constrained Environment. (arXiv:2312.14963v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2312.14963</link>
<description rdf:parseType="Literal">&lt;p&gt;This project proposes and compares a new way to optimise Super Mario Bros.
(SMB) environment where the control is in hand of two approaches, namely,
Genetic Algorithm (MarioGA) and NeuroEvolution (MarioNE). Not only we learn
playing SMB using these techniques, but also optimise it with constrains of
collection of coins and finishing levels. Firstly, we formalise the SMB agent
to maximize the total value of collected coins (reward) and maximising the
total distance traveled (reward) in order to finish the level faster (time
penalty) for both the algorithms. Secondly, we study MarioGA and its evaluation
function (fitness criteria) including its representation methods, crossover
used, mutation operator formalism, selection method used, MarioGA loop, and few
other parameters. Thirdly, MarioNE is applied on SMB where a population of ANNs
with random weights is generated, and these networks control Marios actions in
the game. Fourth, SMB is further constrained to complete the task within the
specified time, rebirths (deaths) within the limit, and performs actions or
moves within the maximum allowed moves, while seeking to maximize the total
coin value collected. This ensures an efficient way of finishing SMB levels.
Finally, we provide a fivefold comparative analysis by plotting fitness plots,
ability to finish different levels of world 1, and domain adaptation (transfer
learning) of the trained models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Sanyam Jain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14972">
<title>Scaling Down to Scale Up: A Cost-Benefit Analysis of Replacing OpenAI&apos;s GPT-4 with Self-Hosted Open Source SLMs in Production. (arXiv:2312.14972v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2312.14972</link>
<description rdf:parseType="Literal">&lt;p&gt;Many companies rely on APIs of managed AI models such as OpenAI&apos;s GPT-4 to
create AI-enabled experiences in their products. Along with the benefits of
ease of use and shortened time to production, this reliance on proprietary APIs
has downsides in terms of model control, performance reliability, up-time
predictability, and cost. At the same time, there has been a flurry of open
source small language models (SLMs) that have been made available for
commercial use. However, their readiness to replace existing capabilities
remains unclear, and a systematic approach to test these models is not readily
available. In this paper, we present a systematic evaluation methodology for,
and characterization of, modern open source SLMs and their trade-offs when
replacing a proprietary LLM APIs for a real-world product feature. We have
designed SLaM, an automated analysis tool that enables the quantitative and
qualitative testing of product features utilizing arbitrary SLMs. Using SLaM,
we examine both the quality and the performance characteristics of modern SLMs
relative to an existing customer-facing OpenAI-based implementation. We find
that across 9 SLMs and 29 variants, we observe competitive quality-of-results
for our use case, significant performance consistency improvement, and a cost
reduction of 5x-29x when compared to OpenAI GPT-4.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irugalbandara_C/0/1/0/all/0/1&quot;&gt;Chandra Irugalbandara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahendra_A/0/1/0/all/0/1&quot;&gt;Ashish Mahendra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daynauth_R/0/1/0/all/0/1&quot;&gt;Roland Daynauth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arachchige_T/0/1/0/all/0/1&quot;&gt;Tharuka Kasthuri Arachchige&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flautner_K/0/1/0/all/0/1&quot;&gt;Krisztian Flautner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Lingjia Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yiping Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mars_J/0/1/0/all/0/1&quot;&gt;Jason Mars&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14977">
<title>Diffusion Models for Generative Artificial Intelligence: An Introduction for Applied Mathematicians. (arXiv:2312.14977v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.14977</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative artificial intelligence (AI) refers to algorithms that create
synthetic but realistic output. Diffusion models currently offer state of the
art performance in generative AI for images. They also form a key component in
more general tools, including text-to-image generators and large language
models. Diffusion models work by adding noise to the available training data
and then learning how to reverse the process. The reverse operation may then be
applied to new random data in order to produce new outputs. We provide a brief
introduction to diffusion models for applied mathematicians and statisticians.
Our key aims are (a) to present illustrative computational examples, (b) to
give a careful derivation of the underlying mathematical formulas involved, and
(c) to draw a connection with partial differential equation (PDE) diffusion
models. We provide code for the computational experiments. We hope that this
topic will be of interest to advanced undergraduate students and postgraduate
students. Portions of the material may also provide useful motivational
examples for those who teach courses in stochastic processes, inference,
machine learning, PDEs or scientific computing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Higham_C/0/1/0/all/0/1&quot;&gt;Catherine F. Higham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Higham_D/0/1/0/all/0/1&quot;&gt;Desmond J. Higham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grindrod_P/0/1/0/all/0/1&quot;&gt;Peter Grindrod&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14978">
<title>On Quantifying Sentiments of Financial News -- Are We Doing the Right Things?. (arXiv:2312.14978v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2312.14978</link>
<description rdf:parseType="Literal">&lt;p&gt;Typical investors start off the day by going through the daily news to get an
intuition about the performance of the market. The speculations based on the
tone of the news ultimately shape their responses towards the market. Today,
computers are being trained to compute the news sentiment so that it can be
used as a variable to predict stock market movements and returns. Some
researchers have even developed news-based market indices to forecast stock
market returns. Majority of the research in the field of news sentiment
analysis has focussed on using libraries like Vader, Loughran-McDonald (LM),
Harvard IV and Pattern. However, are the popular approaches for measuring
financial news sentiment really approaching the problem of sentiment analysis
correctly? Our experiments suggest that measuring sentiments using these
libraries, especially for financial news, fails to depict the true picture and
hence may not be very reliable. Therefore, the question remains: What is the
most effective and accurate approach to measure financial news sentiment? Our
paper explores these questions and attempts to answer them through SENTInews: a
one-of-its-kind financial news sentiment analyzer customized to the Indian
context
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nath_G/0/1/0/all/0/1&quot;&gt;Gourab Nath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sood_A/0/1/0/all/0/1&quot;&gt;Arav Sood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khanna_A/0/1/0/all/0/1&quot;&gt;Aanchal Khanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1&quot;&gt;Savi Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manot_K/0/1/0/all/0/1&quot;&gt;Karan Manot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durbaka_S/0/1/0/all/0/1&quot;&gt;Sree Kavya Durbaka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14990">
<title>Learning to Prompt Knowledge Transfer for Open-World Continual Learning. (arXiv:2312.14990v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.14990</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the problem of continual learning in an open-world
scenario, referred to as Open-world Continual Learning (OwCL). OwCL is
increasingly rising while it is highly challenging in two-fold: i) learning a
sequence of tasks without forgetting knowns in the past, and ii) identifying
unknowns (novel objects/classes) in the future. Existing OwCL methods suffer
from the adaptability of task-aware boundaries between knowns and unknowns, and
do not consider the mechanism of knowledge transfer. In this work, we propose
Pro-KT, a novel prompt-enhanced knowledge transfer model for OwCL. Pro-KT
includes two key components: (1) a prompt bank to encode and transfer both
task-generic and task-specific knowledge, and (2) a task-aware open-set
boundary to identify unknowns in the new tasks. Experimental results using two
real-world datasets demonstrate that the proposed Pro-KT outperforms the
state-of-the-art counterparts in both the detection of unknowns and the
classification of knowns markedly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yujie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiangkun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianrui Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14996">
<title>Bridging AI and Clinical Practice: Integrating Automated Sleep Scoring Algorithm with Uncertainty-Guided Physician Review. (arXiv:2312.14996v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.14996</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: This study aims to enhance the clinical use of automated
sleep-scoring algorithms by incorporating an uncertainty estimation approach to
efficiently assist clinicians in the manual review of predicted hypnograms, a
necessity due to the notable inter-scorer variability inherent in
polysomnography (PSG) databases. Our efforts target the extent of review
required to achieve predefined agreement levels, examining both in-domain and
out-of-domain data, and considering subjects diagnoses. Patients and methods:
Total of 19578 PSGs from 13 open-access databases were used to train U-Sleep, a
state-of-the-art sleep-scoring algorithm. We leveraged a comprehensive clinical
database of additional 8832 PSGs, covering a full spectrum of ages and
sleep-disorders, to refine the U-Sleep, and to evaluate different
uncertainty-quantification approaches, including our novel confidence network.
The ID data consisted of PSGs scored by over 50 physicians, and the two OOD
sets comprised recordings each scored by a unique senior physician. Results:
U-Sleep demonstrated robust performance, with Cohen&apos;s kappa (K) at 76.2% on ID
and 73.8-78.8% on OOD data. The confidence network excelled at identifying
uncertain predictions, achieving AUROC scores of 85.7% on ID and 82.5-85.6% on
OOD data. Independently of sleep-disorder status, statistical evaluations
revealed significant differences in confidence scores between aligning vs
discording predictions, and significant correlations of confidence scores with
classification performance metrics. To achieve K of at least 90% with physician
intervention, examining less than 29.0% of uncertain epochs was required,
substantially reducing physicians workload, and facilitating near-perfect
agreement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bechny_M/0/1/0/all/0/1&quot;&gt;Michal Bechny&lt;/a&gt; (1 and 2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monachino_G/0/1/0/all/0/1&quot;&gt;Giuliana Monachino&lt;/a&gt; (1 and 2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiorillo_L/0/1/0/all/0/1&quot;&gt;Luigi Fiorillo&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meer_J/0/1/0/all/0/1&quot;&gt;Julia van der Meer&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_M/0/1/0/all/0/1&quot;&gt;Markus H. Schmidt&lt;/a&gt; (3 and 4), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bassetti_C/0/1/0/all/0/1&quot;&gt;Claudio L. A. Bassetti&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzovara_A/0/1/0/all/0/1&quot;&gt;Athina Tzovara&lt;/a&gt; (1 and 5), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faraci_F/0/1/0/all/0/1&quot;&gt;Francesca D. Faraci&lt;/a&gt; (2) ((1) Institute of Computer Science, University of Bern, Bern, Switzerland (2) Institute of Digital Technologies for Personalized Healthcare (MeDiTech), University of Applied Sciences and Arts of Southern Switzerland, Lugano, Switzerland (3) Department of Neurology, Inselspital, Bern University Hospital, University of Bern, Bern, Switzerland (4) Ohio Sleep Medicine Institute, Dublin, United States (5) Center for Experimental Neurology, Department of Neurology, Inselspital, Bern University Hospital, University of Bern, Bern, Switzerland)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14998">
<title>Synthetic images aid the recognition of human-made art forgeries. (arXiv:2312.14998v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14998</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous research has shown that Artificial Intelligence is capable of
distinguishing between authentic paintings by a given artist and human-made
forgeries with remarkable accuracy, provided sufficient training. However, with
the limited amount of existing known forgeries, augmentation methods for
forgery detection are highly desirable. In this work, we examine the potential
of incorporating synthetic artworks into training datasets to enhance the
performance of forgery detection. Our investigation focuses on paintings by
Vincent van Gogh, for which we release the first dataset specialized for
forgery detection. To reinforce our results, we conduct the same analyses on
the artists Amedeo Modigliani and Raphael. We train a classifier to distinguish
original artworks from forgeries. For this, we use human-made forgeries and
imitations in the style of well-known artists and augment our training sets
with images in a similar style generated by Stable Diffusion and StyleGAN. We
find that the additional synthetic forgeries consistently improve the detection
of human-made forgeries. In addition, we find that, in line with previous
research, the inclusion of synthetic forgeries in the training also enables the
detection of AI-generated forgeries, especially if created using a similar
generator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ostmeyer_J/0/1/0/all/0/1&quot;&gt;Johann Ostmeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaerf_L/0/1/0/all/0/1&quot;&gt;Ludovica Schaerf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buividovich_P/0/1/0/all/0/1&quot;&gt;Pavel Buividovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charles_T/0/1/0/all/0/1&quot;&gt;Tessa Charles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Postma_E/0/1/0/all/0/1&quot;&gt;Eric Postma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popovici_C/0/1/0/all/0/1&quot;&gt;Carina Popovici&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15002">
<title>C2FAR: Coarse-to-Fine Autoregressive Networks for Precise Probabilistic Forecasting. (arXiv:2312.15002v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.15002</link>
<description rdf:parseType="Literal">&lt;p&gt;We present coarse-to-fine autoregressive networks (C2FAR), a method for
modeling the probability distribution of univariate, numeric random variables.
C2FAR generates a hierarchical, coarse-to-fine discretization of a variable
autoregressively; progressively finer intervals of support are generated from a
sequence of binned distributions, where each distribution is conditioned on
previously-generated coarser intervals. Unlike prior (flat) binned
distributions, C2FAR can represent values with exponentially higher precision,
for only a linear increase in complexity. We use C2FAR for probabilistic
forecasting via a recurrent neural network, thus modeling time series
autoregressively in both space and time. C2FAR is the first method to
simultaneously handle discrete and continuous series of arbitrary scale and
distribution shape. This flexibility enables a variety of time series use
cases, including anomaly detection, interpolation, and compression. C2FAR
achieves improvements over the state-of-the-art on several benchmark
forecasting datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bergsma_S/0/1/0/all/0/1&quot;&gt;Shane Bergsma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeyl_T/0/1/0/all/0/1&quot;&gt;Timothy Zeyl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anaraki_J/0/1/0/all/0/1&quot;&gt;Javad Rahimipour Anaraki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1&quot;&gt;Lei Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15006">
<title>Assessing the Impact of Prompting, Persona, and Chain of Thought Methods on ChatGPT&apos;s Arithmetic Capabilities. (arXiv:2312.15006v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.15006</link>
<description rdf:parseType="Literal">&lt;p&gt;This study critically evaluates the mathematical proficiency of OpenAI&apos;s
language model, ChatGPT, by juxtaposing its default computational capabilities
against the efficiency of three prescriptive methods: strategic prompting,
persona implementation, and the Chain of Thought approach. The evaluation
harnessed the diverse and extensive problem sets from the MATH, GSM8K, and MMLU
data-sets, which encompassing a broad spectrum of mathematical conundrums and
levels of complexity. A sophisticated grading script was designed to determine
the efficacy of these interventions in enhancing the model&apos;s mathematical
precision. Contrary to expectations, our empirical analysis revealed that none
of the trialed methods substantially improved ChatGPT&apos;s baseline performance.
In some cases, these interventions inadvertently disrupted the model&apos;s response
generation. This investigation concluded that while the pursuit of innovative
strategies for augmenting language model performance remains crucial, the
specific methods examined within this study did not induce significant
improvements in ChatGPT&apos;s computational aptitude. These findings underscore the
importance of further comprehensive research and exploration of novel
techniques to enhance the precision and dependability of such models across
diverse domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1&quot;&gt;Chloe Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hanwen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aguenza_J/0/1/0/all/0/1&quot;&gt;Juan Aguenza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhujangari_S/0/1/0/all/0/1&quot;&gt;Sai Bhujangari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_B/0/1/0/all/0/1&quot;&gt;Benthan Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1&quot;&gt;Xun Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1&quot;&gt;Amisha Prasad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fluss_M/0/1/0/all/0/1&quot;&gt;Manny Fluss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phuong_E/0/1/0/all/0/1&quot;&gt;Eric Phuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Minghao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1&quot;&gt;James Davis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15020">
<title>Detecting Technical Debt Using Natural Language Processing Approaches -- A Systematic Literature Review. (arXiv:2312.15020v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2312.15020</link>
<description rdf:parseType="Literal">&lt;p&gt;Context: Technical debt (TD) is a well-known metaphor for the long-term
effects of architectural decisions in software development and the trade-off
between producing high-quality, effective, and efficient code and meeting a
release schedule. Thus, the code degrades and needs refactoring. A lack of
resources, time, knowledge, or experience on the development team might cause
TD in any software development project. Objective: In the context of TD
detection, NLP has been utilized to identify the presence of TD automatically
and even recognize specific types of TD. However, the enormous variety of
feature extraction approaches and ML/DL algorithms employed in the literature
often hinders researchers from trying to improve their performance. Method: In
light of this, this SLR proposes a taxonomy of feature extraction techniques
and algorithms used in technical debt detection: its objective is to compare
and benchmark their performance in the examined studies. Results: We selected
55 articles that passed the quality evaluation of this SLR. We then
investigated which feature extractions and algorithms were employed to identify
TD in each SDLC phase. All approaches proposed in the analyzed studies were
grouped into NLP, NLP+ML, and NLP+DL. This allows us to discuss the performance
in three different ways. Conclusion: Overall, the NLP+DL group consistently
outperforms in precision and F1-score for all projects, and in all but one
project for the recall metric. Regarding the feature extraction techniques, the
PTWE consistently achieves higher precision, recall, and F1-score for each
project analyzed. Furthermore, TD types have been mapped, when possible, to
SDLC phases: this served to determine the best-performing feature extractions
and algorithms for each SDLC phase. Finally, based on the SLR results, we also
identify implications that could be of concern to researchers and
practitioners.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutoyo_E/0/1/0/all/0/1&quot;&gt;Edi Sutoyo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Capiluppi_A/0/1/0/all/0/1&quot;&gt;Andrea Capiluppi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15033">
<title>Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time Intervention. (arXiv:2312.15033v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.15033</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have achieved unprecedented breakthroughs in
various natural language processing domains. However, the enigmatic
``black-box&apos;&apos; nature of LLMs remains a significant challenge for
interpretability, hampering transparent and accountable applications. While
past approaches, such as attention visualization, pivotal subnetwork
extraction, and concept-based analyses, offer some insight, they often focus on
either local or global explanations within a single dimension, occasionally
falling short in providing comprehensive clarity. In response, we propose a
novel methodology anchored in sparsity-guided techniques, aiming to provide a
holistic interpretation of LLMs. Our framework, termed SparseCBM, innovatively
integrates sparsity to elucidate three intertwined layers of interpretation:
input, subnetwork, and concept levels. In addition, the newly introduced
dimension of interpretable inference-time intervention facilitates dynamic
adjustments to the model during deployment. Through rigorous empirical
evaluations on real-world datasets, we demonstrate that SparseCBM delivers a
profound understanding of LLM behaviors, setting it apart in both interpreting
and ameliorating model inaccuracies. Codes are provided in supplements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zhen Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianlong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15040">
<title>Towards Detecting Cascades of Biased Medical Claims on Twitter. (arXiv:2312.15040v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2312.15040</link>
<description rdf:parseType="Literal">&lt;p&gt;Social media may disseminate medical claims that highlight misleading
correlations between social identifiers and diseases due to not accounting for
structural determinants of health. Our research aims to identify biased medical
claims on Twitter and measure their spread. We propose a machine learning
framework that uses two models in tandem: RoBERTa to detect medical claims and
DistilBERT to classify bias. After identifying original biased medical claims,
we conducted a retweet cascade analysis, computing their individual reach and
rate of spread. Tweets containing biased claims were found to circulate faster
and further than unbiased claims.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiderman_L/0/1/0/all/0/1&quot;&gt;Libby Tiderman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mercedes_J/0/1/0/all/0/1&quot;&gt;Juan Sanchez Mercedes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romanoschi_F/0/1/0/all/0/1&quot;&gt;Fiona Romanoschi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murai_F/0/1/0/all/0/1&quot;&gt;Fabricio Murai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15059">
<title>Deformable 3D Gaussian Splatting for Animatable Human Avatars. (arXiv:2312.15059v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.15059</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in neural radiance fields enable novel view synthesis of
photo-realistic images in dynamic settings, which can be applied to scenarios
with human animation. Commonly used implicit backbones to establish accurate
models, however, require many input views and additional annotations such as
human masks, UV maps and depth maps. In this work, we propose ParDy-Human
(Parameterized Dynamic Human Avatar), a fully explicit approach to construct a
digital avatar from as little as a single monocular sequence. ParDy-Human
introduces parameter-driven dynamics into 3D Gaussian Splatting where 3D
Gaussians are deformed by a human pose model to animate the avatar. Our method
is composed of two parts: A first module that deforms canonical 3D Gaussians
according to SMPL vertices and a consecutive module that further takes their
designed joint encodings and predicts per Gaussian deformations to deal with
dynamics beyond SMPL vertex deformations. Images are then synthesized by a
rasterizer. ParDy-Human constitutes an explicit model for realistic dynamic
human avatars which requires significantly fewer training views and images. Our
avatars learning is free of additional annotations such as masks and can be
trained with variable backgrounds while inferring full-resolution images
efficiently even on consumer hardware. We provide experimental evidence to show
that ParDy-Human outperforms state-of-the-art methods on ZJU-MoCap and
THUman4.0 datasets both quantitatively and visually.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1&quot;&gt;HyunJun Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brasch_N/0/1/0/all/0/1&quot;&gt;Nikolas Brasch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jifei Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_Pellitero_E/0/1/0/all/0/1&quot;&gt;Eduardo Perez-Pellitero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yiren Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhihao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1&quot;&gt;Benjamin Busam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15064">
<title>Joint Self-Supervised and Supervised Contrastive Learning for Multimodal MRI Data: Towards Predicting Abnormal Neurodevelopment. (arXiv:2312.15064v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.15064</link>
<description rdf:parseType="Literal">&lt;p&gt;The integration of different imaging modalities, such as structural,
diffusion tensor, and functional magnetic resonance imaging, with deep learning
models has yielded promising outcomes in discerning phenotypic characteristics
and enhancing disease diagnosis. The development of such a technique hinges on
the efficient fusion of heterogeneous multimodal features, which initially
reside within distinct representation spaces. Naively fusing the multimodal
features does not adequately capture the complementary information and could
even produce redundancy. In this work, we present a novel joint self-supervised
and supervised contrastive learning method to learn the robust latent feature
representation from multimodal MRI data, allowing the projection of
heterogeneous features into a shared common space, and thereby amalgamating
both complementary and analogous information across various modalities and
among similar subjects. We performed a comparative analysis between our
proposed method and alternative deep multimodal learning approaches. Through
extensive experiments on two independent datasets, the results demonstrated
that our method is significantly superior to several other deep multimodal
learning methods in predicting abnormal neurodevelopment. Our method has the
capability to facilitate computer-aided diagnosis within clinical practice,
harnessing the power of multimodal data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hailong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ralescu_A/0/1/0/all/0/1&quot;&gt;Anca L. Ralescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dillman_J/0/1/0/all/0/1&quot;&gt;Jonathan R. Dillman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Altaye_M/0/1/0/all/0/1&quot;&gt;Mekibib Altaye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cecil_K/0/1/0/all/0/1&quot;&gt;Kim M. Cecil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Parikh_N/0/1/0/all/0/1&quot;&gt;Nehal A. Parikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Lili He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15097">
<title>Recourse under Model Multiplicity via Argumentative Ensembling. (arXiv:2312.15097v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.15097</link>
<description rdf:parseType="Literal">&lt;p&gt;Model Multiplicity (MM) arises when multiple, equally performing machine
learning models can be trained to solve the same prediction task. Recent
studies show that models obtained under MM may produce inconsistent predictions
for the same input. When this occurs, it becomes challenging to provide
counterfactual explanations (CEs), a common means for offering recourse
recommendations to individuals negatively affected by models&apos; predictions. In
this paper, we formalise this problem, which we name recourse-aware ensembling,
and identify several desirable properties which methods for solving it should
satisfy. We show that existing ensembling methods, naturally extended in
different ways to provide CEs, fail to satisfy these properties. We then
introduce argumentative ensembling, deploying computational argumentation to
guarantee robustness of CEs to MM, while also accommodating customisable user
preferences. We show theoretically and experimentally that argumentative
ensembling satisfies properties which the existing methods lack, and that the
trade-offs are minimal wrt accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Junqi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rago_A/0/1/0/all/0/1&quot;&gt;Antonio Rago&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leofante_F/0/1/0/all/0/1&quot;&gt;Francesco Leofante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1&quot;&gt;Francesca Toni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15101">
<title>Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model Conversions. (arXiv:2312.15101v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2312.15101</link>
<description rdf:parseType="Literal">&lt;p&gt;Converting deep learning models between frameworks is a common step to
maximize model compatibility across devices and leverage optimization features
that may be exclusively provided in one deep learning framework. However, this
conversion process may be riddled with bugs, making the converted models either
undeployable or problematic, considerably degrading their prediction
correctness.
&lt;/p&gt;
&lt;p&gt;We propose an automated approach for fault localization and repair, Fix-Con,
during model conversion between deep learning frameworks. Fix-Con is capable of
detecting and fixing faults introduced in model input, parameters,
hyperparameters, and the model graph during conversion.
&lt;/p&gt;
&lt;p&gt;Fix-Con uses a set of fault types mined from surveying conversion issues
raised to localize potential conversion faults in the converted target model,
and then repairs them appropriately, e.g. replacing the parameters of the
target model with those from the source model. This is done iteratively for
every image in the dataset with output label differences between the source
model and the converted target model until all differences are resolved. We
evaluate the effectiveness of Fix-Con in fixing model conversion bugs of three
widely used image recognition models converted across four different deep
learning frameworks. Overall, Fix-Con was able to either completely repair, or
significantly improve the performance of 14 out of the 15 erroneous conversion
cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Louloudakis_N/0/1/0/all/0/1&quot;&gt;Nikolaos Louloudakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gibson_P/0/1/0/all/0/1&quot;&gt;Perry Gibson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cano_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Cano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajan_A/0/1/0/all/0/1&quot;&gt;Ajitha Rajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15102">
<title>Robust Sclera Segmentation for Skin-tone Agnostic Face Image Quality Assessment. (arXiv:2312.15102v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.15102</link>
<description rdf:parseType="Literal">&lt;p&gt;Face image quality assessment (FIQA) is crucial for obtaining good face
recognition performance. FIQA algorithms should be robust and insensitive to
demographic factors. The eye sclera has a consistent whitish color in all
humans regardless of their age, ethnicity and skin-tone. This work proposes a
robust sclera segmentation method that is suitable for face images in the
enrolment and the border control face recognition scenarios. It shows how the
statistical analysis of the sclera pixels produces features that are invariant
to skin-tone, age and ethnicity and thus can be incorporated into FIQA
algorithms to make them agnostic to demographic factors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kabbani_W/0/1/0/all/0/1&quot;&gt;Wassim Kabbani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1&quot;&gt;Christoph Busch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raja_K/0/1/0/all/0/1&quot;&gt;Kiran Raja&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15106">
<title>Generative AI and the History of Architecture. (arXiv:2312.15106v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.15106</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent generative AI platforms are able to create texts or impressive images
from simple text prompts. This makes them powerful tools for summarizing
knowledge about architectural history or deriving new creative work in early
design tasks like ideation, sketching and modelling. But, how good is the
understanding of the generative AI models of the history of architecture? Has
it learned to properly distinguish styles, or is it hallucinating information?
In this chapter, we investigate this question for generative AI platforms for
text and image generation for different architectural styles, to understand the
capabilities and boundaries of knowledge of those tools. We also analyze how
they are already being used by analyzing a data set of 101 million Midjourney
queries to see if and how practitioners are already querying for specific
architectural concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ploennigs_J/0/1/0/all/0/1&quot;&gt;Joern Ploennigs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berger_M/0/1/0/all/0/1&quot;&gt;Markus Berger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15109">
<title>UAS-based Automated Structural Inspection Path Planning via Visual Data Analytics and Optimization. (arXiv:2312.15109v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.15109</link>
<description rdf:parseType="Literal">&lt;p&gt;Unmanned Aerial Systems (UAS) have gained significant traction for their
application in infrastructure inspections. However, considering the enormous
scale and complex nature of infrastructure, automation is essential for
improving the efficiency and quality of inspection operations. One of the core
problems in this regard is electing an optimal automated flight path that can
achieve the mission objectives while minimizing flight time. This paper
presents an effective formulation for the path planning problem in the context
of structural inspections. Coverage is guaranteed as a constraint to ensure
damage detectability and path length is minimized as an objective, thus
maximizing efficiency while ensuring inspection quality. A two-stage algorithm
is then devised to solve the path planning problem, composed of a genetic
algorithm for determining the positions of viewpoints and a greedy algorithm
for calculating the poses. A comprehensive sensitivity analysis is conducted to
demonstrate the proposed algorithm&apos;s effectiveness and range of applicability.
Applied examples of the algorithm, including partial space inspection with
no-fly zones and focused inspection, are also presented, demonstrating the
flexibility of the proposed method to meet real-world structural inspection
requirements. In conclusion, the results of this study highlight the
feasibility of the proposed approach and establish the groundwork for
incorporating automation into UAS-based structural inspection mission planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1&quot;&gt;Benhao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alipour_M/0/1/0/all/0/1&quot;&gt;Mohamad Alipour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15116">
<title>EGAIN: Extended GAn INversion. (arXiv:2312.15116v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.15116</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) have witnessed significant advances in
recent years, generating increasingly higher quality images, which are
non-distinguishable from real ones. Recent GANs have proven to encode features
in a disentangled latent space, enabling precise control over various semantic
attributes of the generated facial images such as pose, illumination, or
gender. GAN inversion, which is projecting images into the latent space of a
GAN, opens the door for the manipulation of facial semantics of real face
images. This is useful for numerous applications such as evaluating the
performance of face recognition systems. In this work, EGAIN, an architecture
for constructing GAN inversion models, is presented. This architecture
explicitly addresses some of the shortcomings in previous GAN inversion models.
A specific model with the same name, egain, based on this architecture is also
proposed, demonstrating superior reconstruction quality over state-of-the-art
models, and illustrating the validity of the EGAIN architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kabbani_W/0/1/0/all/0/1&quot;&gt;Wassim Kabbani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grimmer_M/0/1/0/all/0/1&quot;&gt;Marcel Grimmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1&quot;&gt;Christoph Busch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15122">
<title>Scaling Is All You Need: Training Strong Policies for Autonomous Driving with JAX-Accelerated Reinforcement Learning. (arXiv:2312.15122v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.15122</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning has been used to train policies that outperform even
the best human players in various games. However, a large amount of data is
needed to achieve good performance, which in turn requires building large-scale
frameworks and simulators. In this paper, we study how large-scale
reinforcement learning can be applied to autonomous driving, analyze how the
resulting policies perform as the experiment size is scaled, and what the most
important factors contributing to policy performance are. To do this, we first
introduce a hardware-accelerated autonomous driving simulator, which allows us
to efficiently collect experience from billions of agent steps. This simulator
is paired with a large-scale, multi-GPU reinforcement learning framework. We
demonstrate that simultaneous scaling of dataset size, model size, and agent
steps trained provides increasingly strong driving policies in regard to
collision, traffic rule violations, and progress. In particular, our best
policy reduces the failure rate by 57% while improving progress by 23% compared
to the current state-of-the-art machine learning policies for autonomous
driving.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harmel_M/0/1/0/all/0/1&quot;&gt;Moritz Harmel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paras_A/0/1/0/all/0/1&quot;&gt;Anubhav Paras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pasternak_A/0/1/0/all/0/1&quot;&gt;Andreas Pasternak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linscott_G/0/1/0/all/0/1&quot;&gt;Gary Linscott&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15136">
<title>Towards End-to-End Structure Solutions from Information-Compromised Diffraction Data via Generative Deep Learning. (arXiv:2312.15136v1 [physics.comp-ph])</title>
<link>http://arxiv.org/abs/2312.15136</link>
<description rdf:parseType="Literal">&lt;p&gt;The revolution in materials in the past century was built on a knowledge of
the atomic arrangements and the structure-property relationship. The sine qua
non for obtaining quantitative structural information is single crystal
crystallography. However, increasingly we need to solve structures in cases
where the information content in our input signal is significantly degraded,
for example, due to orientational averaging of grains, finite size effects due
to nanostructure, and mixed signals due to sample heterogeneity. Understanding
the structure property relationships in such situations is, if anything, more
important and insightful, yet we do not have robust approaches for
accomplishing it. In principle, machine learning (ML) and deep learning (DL)
are promising approaches since they augment information in the degraded input
signal with prior knowledge learned from large databases of already known
structures. Here we present a novel ML approach, a variational query-based
multi-branch deep neural network that has the promise to be a robust but
general tool to address this problem end-to-end. We demonstrate the approach on
computed powder x-ray diffraction (PXRD), along with partial chemical
composition information, as input. We choose as a structural representation a
modified electron density we call the Cartesian mapped electron density (CMED),
that straightforwardly allows our ML model to learn material structures across
different chemistries, symmetries and crystal systems. When evaluated on
theoretically simulated data for the cubic and trigonal crystal systems, the
system achieves up to $93.4\%$ average similarity with the ground truth on
unseen materials, both with known and partially-known chemical composition
information, showing great promise for successful structure solution even from
degraded and incomplete input data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Guo_G/0/1/0/all/0/1&quot;&gt;Gabe Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Goldfeder_J/0/1/0/all/0/1&quot;&gt;Judah Goldfeder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lan_L/0/1/0/all/0/1&quot;&gt;Ling Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ray_A/0/1/0/all/0/1&quot;&gt;Aniv Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yang_A/0/1/0/all/0/1&quot;&gt;Albert Hanming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Boyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Billinge_S/0/1/0/all/0/1&quot;&gt;Simon JL Billinge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lipson_H/0/1/0/all/0/1&quot;&gt;Hod Lipson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2008.07007">
<title>Interpretable Representations in Explainable AI: From Theory to Practice. (arXiv:2008.07007v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2008.07007</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretable representations are the backbone of many explainers that target
black-box predictive systems based on artificial intelligence and machine
learning algorithms. They translate the low-level data representation necessary
for good predictive performance into high-level human-intelligible concepts
used to convey the explanatory insights. Notably, the explanation type and its
cognitive complexity are directly controlled by the interpretable
representation, tweaking which allows to target a particular audience and use
case. However, many explainers built upon interpretable representations
overlook their merit and fall back on default solutions that often carry
implicit assumptions, thereby degrading the explanatory power and reliability
of such techniques. To address this problem, we study properties of
interpretable representations that encode presence and absence of
human-comprehensible concepts. We demonstrate how they are operationalised for
tabular, image and text data; discuss their assumptions, strengths and
weaknesses; identify their core building blocks; and scrutinise their
configuration and parameterisation. In particular, this in-depth analysis
allows us to pinpoint their explanatory properties, desiderata and scope for
(malicious) manipulation in the context of tabular data where a linear model is
used to quantify the influence of interpretable concepts on a black-box
prediction. Our findings lead to a range of recommendations for designing
trustworthy interpretable representations; specifically, the benefits of
class-aware (supervised) discretisation of tabular data, e.g., with decision
trees, and sensitivity of image interpretable representations to segmentation
granularity and occlusion colour.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sokol_K/0/1/0/all/0/1&quot;&gt;Kacper Sokol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flach_P/0/1/0/all/0/1&quot;&gt;Peter Flach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.01537">
<title>A Longitudinal Multi-modal Dataset for Dementia Monitoring and Diagnosis. (arXiv:2109.01537v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2109.01537</link>
<description rdf:parseType="Literal">&lt;p&gt;Dementia affects cognitive functions of adults, including memory, language,
and behaviour. Standard diagnostic biomarkers such as MRI are costly, whilst
neuropsychological tests suffer from sensitivity issues in detecting dementia
onset. The analysis of speech and language has emerged as a promising and
non-intrusive technology to diagnose and monitor dementia. Currently, most work
in this direction ignores the multi-modal nature of human communication and
interactive aspects of everyday conversational interaction. Moreover, most
studies ignore changes in cognitive status over time due to the lack of
consistent longitudinal data. Here we introduce a novel fine-grained
longitudinal multi-modal corpus collected in a natural setting from healthy
controls and people with dementia over two phases, each spanning 28 sessions.
The corpus consists of spoken conversations, a subset of which are transcribed,
as well as typed and written thoughts and associated extra-linguistic
information such as pen strokes and keystrokes. We present the data collection
process and describe the corpus in detail. Furthermore, we establish baselines
for capturing longitudinal changes in language across different modalities for
two cohorts, healthy controls and people with dementia, outlining future
research directions enabled by the corpus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gkoumas_D/0/1/0/all/0/1&quot;&gt;Dimitris Gkoumas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsakalidis_A/0/1/0/all/0/1&quot;&gt;Adam Tsakalidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolters_M/0/1/0/all/0/1&quot;&gt;Maria Wolters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1&quot;&gt;Arkaitz Zubiaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1&quot;&gt;Matthew Purver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liakata_M/0/1/0/all/0/1&quot;&gt;Maria Liakata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.04941">
<title>Solving PDE-constrained Control Problems Using Operator Learning. (arXiv:2111.04941v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2111.04941</link>
<description rdf:parseType="Literal">&lt;p&gt;The modeling and control of complex physical systems are essential in
real-world problems. We propose a novel framework that is generally applicable
to solving PDE-constrained optimal control problems by introducing surrogate
models for PDE solution operators with special regularizers. The procedure of
the proposed framework is divided into two phases: solution operator learning
for PDE constraints (Phase 1) and searching for optimal control (Phase 2). Once
the surrogate model is trained in Phase 1, the optimal control can be inferred
in Phase 2 without intensive computations. Our framework can be applied to both
data-driven and data-free cases. We demonstrate the successful application of
our method to various optimal control problems for different control variables
with diverse PDE constraints from the Poisson equation to Burgers&apos; equation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hwang_R/0/1/0/all/0/1&quot;&gt;Rakhoon Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jae Yong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jin Young Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hwang_H/0/1/0/all/0/1&quot;&gt;Hyung Ju Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.01077">
<title>Addressing Gap between Training Data and Deployed Environment by On-Device Learning. (arXiv:2203.01077v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2203.01077</link>
<description rdf:parseType="Literal">&lt;p&gt;The accuracy of tinyML applications is often affected by various
environmental factors, such as noises, location/calibration of sensors, and
time-related changes. This article introduces a neural network based on-device
learning (ODL) approach to address this issue by retraining in deployed
environments. Our approach relies on semi-supervised sequential training of
multiple neural networks tailored for low-end edge devices. This article
introduces its algorithm and implementation on wireless sensor nodes consisting
of a Raspberry Pi Pico and low-power wireless module. Experiments using
vibration patterns of rotating machines demonstrate that retraining by ODL
improves anomaly detection accuracy compared with a prediction-only deep neural
network in a noisy environment. The results also show that the ODL approach can
save communication cost and energy consumption for battery-powered Internet of
Things devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunaga_K/0/1/0/all/0/1&quot;&gt;Kazuki Sunaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondo_M/0/1/0/all/0/1&quot;&gt;Masaaki Kondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsutani_H/0/1/0/all/0/1&quot;&gt;Hiroki Matsutani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.10789">
<title>Verification of Locally Tight Programs. (arXiv:2204.10789v3 [cs.LO] UPDATED)</title>
<link>http://arxiv.org/abs/2204.10789</link>
<description rdf:parseType="Literal">&lt;p&gt;Program completion is a translation from the language of logic programs into
the language of first-order theories. Its original definition has been extended
to programs that include integer arithmetic, accept input, and distinguish
between output predicates and auxiliary predicates. For tight programs, that
generalization of completion is known to match the stable model semantics,
which is the basis of answer set programming. We show that the tightness
condition in this theorem can be replaced by a less restrictive &quot;local
tightness&quot; requirement. From this fact we conclude that the proof assistant
anthem-p2p can be used to verify equivalence between locally tight programs.
Under consideration for publication in Theory and Practice of Logic Programming
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fandinno_J/0/1/0/all/0/1&quot;&gt;Jorge Fandinno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lifschitz_V/0/1/0/all/0/1&quot;&gt;Vladimir Lifschitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Temple_N/0/1/0/all/0/1&quot;&gt;Nathan Temple&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.01515">
<title>Understanding Deep Learning via Decision Boundary. (arXiv:2206.01515v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.01515</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper discovers that the neural network with lower decision boundary
(DB) variability has better generalizability. Two new notions, algorithm DB
variability and $(\epsilon, \eta)$-data DB variability, are proposed to measure
the decision boundary variability from the algorithm and data perspectives.
Extensive experiments show significant negative correlations between the
decision boundary variability and the generalizability. From the theoretical
view, two lower bounds based on algorithm DB variability are proposed and do
not explicitly depend on the sample size. We also prove an upper bound of order
$\mathcal{O}\left(\frac{1}{\sqrt{m}}+\epsilon+\eta\log\frac{1}{\eta}\right)$
based on data DB variability. The bound is convenient to estimate without the
requirement of labels, and does not explicitly depend on the network size which
is usually prohibitively large in deep learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1&quot;&gt;Shiye Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1&quot;&gt;Fengxiang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yancheng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.08143">
<title>Can large language models reason about medical questions?. (arXiv:2207.08143v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2207.08143</link>
<description rdf:parseType="Literal">&lt;p&gt;Although large language models (LLMs) often produce impressive outputs, it
remains unclear how they perform in real-world scenarios requiring strong
reasoning skills and expert domain knowledge. We set out to investigate whether
close- and open-source models (GPT-3.5, LLama-2, etc.) can be applied to answer
and reason about difficult real-world-based questions. We focus on three
popular medical benchmarks (MedQA-USMLE, MedMCQA, and PubMedQA) and multiple
prompting scenarios: Chain-of-Thought (CoT, think step-by-step), few-shot and
retrieval augmentation. Based on an expert annotation of the generated CoTs, we
found that InstructGPT can often read, reason and recall expert knowledge.
Last, by leveraging advances in prompt engineering (few-shot and ensemble
methods), we demonstrated that GPT-3.5 not only yields calibrated predictive
distributions, but also reaches the passing score on three datasets:
MedQA-USMLE 60.2%, MedMCQA 62.7% and PubMedQA 78.2%. Open-source models are
closing the gap: Llama-2 70B also passed the MedQA-USMLE with 62.5% accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lievin_V/0/1/0/all/0/1&quot;&gt;Valentin Li&amp;#xe9;vin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hother_C/0/1/0/all/0/1&quot;&gt;Christoffer Egeberg Hother&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Motzfeldt_A/0/1/0/all/0/1&quot;&gt;Andreas Geert Motzfeldt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1&quot;&gt;Ole Winther&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.02442">
<title>SimCLF: A Simple Contrastive Learning Framework for Function-level Binary Embeddings. (arXiv:2209.02442v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2209.02442</link>
<description rdf:parseType="Literal">&lt;p&gt;Function-level binary code similarity detection is a crucial aspect of
cybersecurity. It enables the detection of bugs and patent infringements in
released software and plays a pivotal role in preventing supply chain attacks.
A practical embedding learning framework relies on the robustness of the
assembly code representation and the accuracy of function-pair annotation,
which is traditionally accomplished using supervised learning-based frameworks.
However, annotating different function pairs with accurate labels poses
considerable challenges. These supervised learning methods can be easily
overtrained and suffer from representation robustness problems. To address
these challenges, we propose SimCLF: A Simple Contrastive Learning Framework
for Function-level Binary Embeddings. We take an unsupervised learning approach
and formulate binary code similarity detection as instance discrimination.
SimCLF directly operates on disassembled binary functions and could be
implemented with any encoder. It does not require manually annotated
information but only augmented data. Augmented data is generated using compiler
optimization options and code obfuscation techniques. The experimental results
demonstrate that SimCLF surpasses the state-of-the-art in accuracy and has a
significant advantage in few-shot settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+RuiJin_S/0/1/0/all/0/1&quot;&gt;Sun RuiJin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shize_G/0/1/0/all/0/1&quot;&gt;Guo Shize&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jinhong_G/0/1/0/all/0/1&quot;&gt;Guo Jinhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1&quot;&gt;Li Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dazhi_Z/0/1/0/all/0/1&quot;&gt;Zhan Dazhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_S/0/1/0/all/0/1&quot;&gt;Sun Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhisong_P/0/1/0/all/0/1&quot;&gt;Pan Zhisong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.02646">
<title>A Survey on Generative Diffusion Model. (arXiv:2209.02646v10 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2209.02646</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models have unlocked another profound realm of human
creativity. By capturing and generalizing patterns within data, we have entered
the epoch of all-encompassing Artificial Intelligence for General Creativity
(AIGC). Notably, diffusion models, recognized as one of the paramount
generative models, materialize human ideation into tangible instances across
diverse domains, encompassing imagery, text, speech, biology, and healthcare.
To provide advanced and comprehensive insights into diffusion, this survey
comprehensively elucidates its developmental trajectory and future directions
from three distinct angles: the fundamental formulation of diffusion,
algorithmic enhancements, and the manifold applications of diffusion. Each
layer is meticulously explored to offer a profound comprehension of its
evolution. Structured and summarized approaches are presented in
https://github.com/chq1155/A-Survey-on-Generative-Diffusion-Model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1&quot;&gt;Hanqun Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Cheng Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yilun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guangyong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1&quot;&gt;Pheng-Ann Heng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Stan Z. Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.12723">
<title>A Faithful Deep Sensitivity Estimation for Accelerated Magnetic Resonance Imaging. (arXiv:2210.12723v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.12723</link>
<description rdf:parseType="Literal">&lt;p&gt;Magnetic resonance imaging (MRI) is an essential diagnostic tool that suffers
from prolonged scan time. To alleviate this limitation, advanced fast MRI
technology attracts extensive research interests. Recent deep learning has
shown its great potential in improving image quality and reconstruction speed.
Faithful coil sensitivity estimation is vital for MRI reconstruction. However,
most deep learning methods still rely on pre-estimated sensitivity maps and
ignore their inaccuracy, resulting in the significant quality degradation of
reconstructed images. In this work, we propose a Joint Deep Sensitivity
estimation and Image reconstruction network, called JDSI. During the image
artifacts removal, it gradually provides more faithful sensitivity maps with
high-frequency information, leading to improved image reconstructions. To
understand the behavior of the network, the mutual promotion of sensitivity
estimation and image reconstruction is revealed through the visualization of
network intermediate results. Results on in vivo datasets and radiologist
reader study demonstrate that, for both calibration-based and calibrationless
reconstruction, the proposed JDSI achieves the state-of-the-art performance
visually and quantitatively, especially when the acceleration factor is high.
Additionally, JDSI owns nice robustness to patients and autocalibration
signals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fang_H/0/1/0/all/0/1&quot;&gt;Haoming Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Boxuan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bao_L/0/1/0/all/0/1&quot;&gt;Lijun Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Liuhong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jianjun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wenping Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jianzhong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_D/0/1/0/all/0/1&quot;&gt;Di Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xiaobo Qu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.02758">
<title>Tackling Data Heterogeneity in Federated Learning with Class Prototypes. (arXiv:2212.02758v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.02758</link>
<description rdf:parseType="Literal">&lt;p&gt;Data heterogeneity across clients in federated learning (FL) settings is a
widely acknowledged challenge. In response, personalized federated learning
(PFL) emerged as a framework to curate local models for clients&apos; tasks. In PFL,
a common strategy is to develop local and global models jointly - the global
model (for generalization) informs the local models, and the local models (for
personalization) are aggregated to update the global model. A key observation
is that if we can improve the generalization ability of local models, then we
can improve the generalization of global models, which in turn builds better
personalized models. In this work, we consider class imbalance, an overlooked
type of data heterogeneity, in the classification setting. We propose FedNH, a
novel method that improves the local models&apos; performance for both
personalization and generalization by combining the uniformity and semantics of
class prototypes. FedNH initially distributes class prototypes uniformly in the
latent space and smoothly infuses the class semantics into class prototypes. We
show that imposing uniformity helps to combat prototype collapse while infusing
class semantics improves local models. Extensive experiments were conducted on
popular classification datasets under the cross-device setting. Our results
demonstrate the effectiveness and stability of our method over recent works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yutong Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zeyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junnan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinecke_S/0/1/0/all/0/1&quot;&gt;Shelby Heinecke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lichao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Ran Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.01829">
<title>t-SMILES: A Scalable Fragment-based Molecular Representation Framework for De Novo Molecule Generation. (arXiv:2301.01829v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.01829</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective representation of molecules is a crucial factor affecting the
performance of artificial intelligence models. This study introduces a
flexible, fragment-based, multiscale molecular representation framework called
t-SMILES (tree-based SMILES) with three code algorithms: TSSA (t-SMILES with
Shared Atom), TSDY (t-SMILES with Dummy Atom) and TSID (t-SMILES with ID). It
describes molecules using SMILES-type strings obtained by performing a
breadth-first search on a full binary tree formed from a fragmented molecular
graph. Systematic evaluations using JTVAE, BRICS, MMPA, and Scaffold show the
feasibility to construct a multilingual molecular description system, where
various descriptions complement each other, enhancing the overall performance.
Additionally, it exhibits impressive performance on low-resource datasets,
whether the model is original, data augmented, or pre-training fine-tuned. It
significantly outperforms classical SMILES, DeepSMILES, SELFIES and baseline
models in goal-directed tasks. Furthermore, it surpasses start-of-the-art
fragment, graph and SMILES based approaches on ChEMBL, Zinc, and QM9.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Juan-Ni Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yue Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Li-Juan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hai-Long Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1&quot;&gt;Ru-Qin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.07695">
<title>EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records. (arXiv:2301.07695v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2301.07695</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new text-to-SQL dataset for electronic health records (EHRs).
The utterances were collected from 222 hospital staff members, including
physicians, nurses, and insurance review and health records teams. To construct
the QA dataset on structured EHR data, we conducted a poll at a university
hospital and used the responses to create seed questions. We then manually
linked these questions to two open-source EHR databases, MIMIC-III and eICU,
and included various time expressions and held-out unanswerable questions in
the dataset, which were also collected from the poll. Our dataset poses a
unique set of challenges: the model needs to 1) generate SQL queries that
reflect a wide range of needs in the hospital, including simple retrieval and
complex operations such as calculating survival rate, 2) understand various
time expressions to answer time-sensitive questions in healthcare, and 3)
distinguish whether a given question is answerable or unanswerable. We believe
our dataset, EHRSQL, can serve as a practical benchmark for developing and
assessing QA models on structured EHR data and take a step further towards
bridging the gap between text-to-SQL research and its real-life deployment in
healthcare. EHRSQL is available at https://github.com/glee4810/EHRSQL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gyubok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1&quot;&gt;Hyeonji Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1&quot;&gt;Seongsu Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1&quot;&gt;Yeonsu Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1&quot;&gt;Woncheol Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Seongjun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1&quot;&gt;Minjoon Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jong-Yeup Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1&quot;&gt;Edward Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10923">
<title>Trust Region-Based Safe Distributional Reinforcement Learning for Multiple Constraints. (arXiv:2301.10923v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10923</link>
<description rdf:parseType="Literal">&lt;p&gt;In safety-critical robotic tasks, potential failures must be reduced, and
multiple constraints must be met, such as avoiding collisions, limiting energy
consumption, and maintaining balance. Thus, applying safe reinforcement
learning (RL) in such robotic tasks requires to handle multiple constraints and
use risk-averse constraints rather than risk-neutral constraints. To this end,
we propose a trust region-based safe RL algorithm for multiple constraints
called a safe distributional actor-critic (SDAC). Our main contributions are as
follows: 1) introducing a gradient integration method to manage infeasibility
issues in multi-constrained problems, ensuring theoretical convergence, and 2)
developing a TD($\lambda$) target distribution to estimate risk-averse
constraints with low biases. We evaluate SDAC through extensive experiments
involving multi- and single-constrained robotic tasks. While maintaining high
scores, SDAC shows 1.93 times fewer steps to satisfy all constraints in
multi-constrained tasks and 1.78 times fewer constraint violations in
single-constrained tasks compared to safe RL baselines. Code is available at:
https://github.com/rllab-snu/Safe-Distributional-Actor-Critic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dohyeong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kyungjae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Songhwai Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.04178">
<title>DynGFN: Towards Bayesian Inference of Gene Regulatory Networks with GFlowNets. (arXiv:2302.04178v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.04178</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the grand challenges of cell biology is inferring the gene regulatory
network (GRN) which describes interactions between genes and their products
that control gene expression and cellular function. We can treat this as a
causal discovery problem but with two non-standard challenges: (1) regulatory
networks are inherently cyclic so we should not model a GRN as a directed
acyclic graph (DAG), and (2) observations have significant measurement noise,
so for typical sample sizes there will always be a large equivalence class of
graphs that are likely given the data, and we want methods that capture this
uncertainty. Existing methods either focus on challenge (1), identifying cyclic
structure from dynamics, or on challenge (2) learning complex Bayesian
posteriors over DAGs, but not both. In this paper we leverage the fact that it
is possible to estimate the &quot;velocity&quot; of gene expression with RNA velocity
techniques to develop an approach that addresses both challenges. Because we
have access to velocity information, we can treat the Bayesian structure
learning problem as a problem of sparse identification of a dynamical system,
capturing cyclic feedback loops through time. Since our objective is to model
uncertainty over discrete structures, we leverage Generative Flow Networks
(GFlowNets) to estimate the posterior distribution over the combinatorial space
of possible sparse dependencies. Our results indicate that our method learns
posteriors that better encapsulate the distributions of cyclic structures
compared to counterpart state-of-the-art Bayesian structure learning
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atanackovic_L/0/1/0/all/0/1&quot;&gt;Lazar Atanackovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_A/0/1/0/all/0/1&quot;&gt;Alexander Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1&quot;&gt;Leo J. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartford_J/0/1/0/all/0/1&quot;&gt;Jason Hartford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06961">
<title>DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical Awareness for Robust Fovea Localization. (arXiv:2302.06961v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06961</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate fovea localization is essential for analyzing retinal diseases to
prevent irreversible vision loss. While current deep learning-based methods
outperform traditional ones, they still face challenges such as the lack of
local anatomical landmarks around the fovea, the inability to robustly handle
diseased retinal images, and the variations in image conditions. In this paper,
we propose a novel transformer-based architecture called DualStreamFoveaNet
(DSFN) for multi-cue fusion. This architecture explicitly incorporates
long-range connections and global features using retina and vessel
distributions for robust fovea localization. We introduce a spatial attention
mechanism in the dual-stream encoder to extract and fuse self-learned
anatomical information, focusing more on features distributed along blood
vessels and significantly reducing computational costs by decreasing token
numbers. Our extensive experiments show that the proposed architecture achieves
state-of-the-art performance on two public datasets and one large-scale private
dataset. Furthermore, we demonstrate that the DSFN is more robust on both
normal and diseased retina images and has better generalization capacity in
cross-dataset experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Sifan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zilong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jionglong Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xiaowei Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_K/0/1/0/all/0/1&quot;&gt;Kang Dang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.14691">
<title>Investigating the Effectiveness of Task-Agnostic Prefix Prompt for Instruction Following. (arXiv:2302.14691v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2302.14691</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present our finding that prepending a Task-Agnostic Prefix
Prompt (TAPP) to the input improves the instruction-following ability of
various Large Language Models (LLMs) during inference. TAPP is different from
canonical prompts for LLMs in that it is a fixed prompt prepended to the
beginning of every input regardless of the target task for zero-shot
generalization. We observe that both base LLMs (i.e. not fine-tuned to follow
instructions) and instruction-tuned models benefit from TAPP, resulting in
34.58% and 12.26% improvement on average, respectively. This implies that the
instruction-following ability of LLMs can be improved during inference time
with a fixed prompt constructed with simple heuristics. We hypothesize that
TAPP assists language models to better estimate the output distribution by
focusing more on the instruction of the target task during inference. In other
words, such ability does not seem to be sufficiently activated in not only base
LLMs but also many instruction-fine-tuned LLMs. All experiments are
reproducible from https://github.com/seonghyeonye/TAPP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1&quot;&gt;Seonghyeon Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1&quot;&gt;Hyeonbin Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Sohee Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_H/0/1/0/all/0/1&quot;&gt;Hyeongu Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yireun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1&quot;&gt;Minjoon Seo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.14496">
<title>Learning with Explanation Constraints. (arXiv:2303.14496v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.14496</link>
<description rdf:parseType="Literal">&lt;p&gt;As larger deep learning models are hard to interpret, there has been a recent
focus on generating explanations of these black-box models. In contrast, we may
have apriori explanations of how models should behave. In this paper, we
formalize this notion as learning from explanation constraints and provide a
learning theoretic framework to analyze how such explanations can improve the
learning of our models. One may naturally ask, &quot;When would these explanations
be helpful?&quot; Our first key contribution addresses this question via a class of
models that satisfies these explanation constraints in expectation over new
data. We provide a characterization of the benefits of these models (in terms
of the reduction of their Rademacher complexities) for a canonical class of
explanations given by gradient information in the settings of both linear
models and two layer neural networks. In addition, we provide an algorithmic
solution for our framework, via a variational approximation that achieves
better performance and satisfies these constraints more frequently, when
compared to simpler augmented Lagrangian methods to incorporate these
explanations. We demonstrate the benefits of our approach over a large array of
synthetic and real-world experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pukdee_R/0/1/0/all/0/1&quot;&gt;Rattana Pukdee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sam_D/0/1/0/all/0/1&quot;&gt;Dylan Sam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1&quot;&gt;J. Zico Kolter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balcan_M/0/1/0/all/0/1&quot;&gt;Maria-Florina Balcan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1&quot;&gt;Pradeep Ravikumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.00670">
<title>CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception. (arXiv:2304.00670v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.00670</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous driving requires an accurate and fast 3D perception system that
includes 3D object detection, tracking, and segmentation. Although recent
low-cost camera-based approaches have shown promising results, they are
susceptible to poor illumination or bad weather conditions and have a large
localization error. Hence, fusing camera with low-cost radar, which provides
precise long-range measurement and operates reliably in all environments, is
promising but has not yet been thoroughly investigated. In this paper, we
propose Camera Radar Net (CRN), a novel camera-radar fusion framework that
generates a semantically rich and spatially accurate bird&apos;s-eye-view (BEV)
feature map for various tasks. To overcome the lack of spatial information in
an image, we transform perspective view image features to BEV with the help of
sparse but accurate radar points. We further aggregate image and radar feature
maps in BEV using multi-modal deformable attention designed to tackle the
spatial misalignment between inputs. CRN with real-time setting operates at 20
FPS while achieving comparable performance to LiDAR detectors on nuScenes, and
even outperforms at a far distance on 100m setting. Moreover, CRN with offline
setting yields 62.4% NDS, 57.5% mAP on nuScenes test set and ranks first among
all camera and camera-radar 3D object detectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Youngseok Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Juyeb Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sanmin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;In-Jae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jun Won Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kum_D/0/1/0/all/0/1&quot;&gt;Dongsuk Kum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01046">
<title>Deep Manifold Learning for Reading Comprehension and Logical Reasoning Tasks with Polytuplet Loss. (arXiv:2304.01046v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01046</link>
<description rdf:parseType="Literal">&lt;p&gt;The current trend in developing machine learning models for reading
comprehension and logical reasoning tasks is focused on improving the models&apos;
abilities to understand and utilize logical rules. This work focuses on
providing a novel loss function and accompanying model architecture that has
more interpretable components than some other models by representing a common
strategy employed by humans when given reading comprehension and logical
reasoning tasks. Our strategy involves emphasizing relative accuracy over
absolute accuracy and can theoretically produce the correct answer with
incomplete knowledge. We examine the effectiveness of this strategy to solve
reading comprehension and logical reasoning questions. The models were
evaluated on the ReClor dataset, a challenging reading comprehension and
logical reasoning benchmark. We propose the polytuplet loss function, which
forces prioritization of learning the relative correctness of answer choices
over learning the true accuracy of each choice. Our results indicate that
models employing polytuplet loss outperform existing baseline models, though
further research is required to quantify the benefits it may present.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jeffrey Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_I/0/1/0/all/0/1&quot;&gt;Ivan Rodriguez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.02711">
<title>Structured prompt interrogation and recursive extraction of semantics (SPIRES): A method for populating knowledge bases using zero-shot learning. (arXiv:2304.02711v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2304.02711</link>
<description rdf:parseType="Literal">&lt;p&gt;Creating knowledge bases and ontologies is a time consuming task that relies
on a manual curation. AI/NLP approaches can assist expert curators in
populating these knowledge bases, but current approaches rely on extensive
training data, and are not able to populate arbitrary complex nested knowledge
schemas.
&lt;/p&gt;
&lt;p&gt;Here we present Structured Prompt Interrogation and Recursive Extraction of
Semantics (SPIRES), a Knowledge Extraction approach that relies on the ability
of Large Language Models (LLMs) to perform zero-shot learning (ZSL) and
general-purpose query answering from flexible prompts and return information
conforming to a specified schema. Given a detailed, user-defined knowledge
schema and an input text, SPIRES recursively performs prompt interrogation
against GPT-3+ to obtain a set of responses matching the provided schema.
SPIRES uses existing ontologies and vocabularies to provide identifiers for all
matched elements.
&lt;/p&gt;
&lt;p&gt;We present examples of use of SPIRES in different domains, including
extraction of food recipes, multi-species cellular signaling pathways, disease
treatments, multi-step drug mechanisms, and chemical to disease causation
graphs. Current SPIRES accuracy is comparable to the mid-range of existing
Relation Extraction (RE) methods, but has the advantage of easy customization,
flexibility, and, crucially, the ability to perform new tasks in the absence of
any training data. This method supports a general strategy of leveraging the
language interpreting capabilities of LLMs to assemble knowledge bases,
assisting manual knowledge curation and acquisition while supporting validation
with publicly-available databases and ontologies external to the LLM.
&lt;/p&gt;
&lt;p&gt;SPIRES is available as part of the open source OntoGPT package:
https://github.com/ monarch-initiative/ontogpt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caufield_J/0/1/0/all/0/1&quot;&gt;J. Harry Caufield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hegde_H/0/1/0/all/0/1&quot;&gt;Harshad Hegde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emonet_V/0/1/0/all/0/1&quot;&gt;Vincent Emonet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harris_N/0/1/0/all/0/1&quot;&gt;Nomi L. Harris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joachimiak_M/0/1/0/all/0/1&quot;&gt;Marcin P. Joachimiak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matentzoglu_N/0/1/0/all/0/1&quot;&gt;Nicolas Matentzoglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;HyeongSik Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moxon_S/0/1/0/all/0/1&quot;&gt;Sierra A.T. Moxon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reese_J/0/1/0/all/0/1&quot;&gt;Justin T. Reese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haendel_M/0/1/0/all/0/1&quot;&gt;Melissa A. Haendel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robinson_P/0/1/0/all/0/1&quot;&gt;Peter N. Robinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mungall_C/0/1/0/all/0/1&quot;&gt;Christopher J. Mungall&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06876">
<title>Sampling-based Reactive Synthesis for Nondeterministic Hybrid Systems. (arXiv:2304.06876v3 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06876</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a sampling-based strategy synthesis algorithm for
nondeterministic hybrid systems with complex continuous dynamics under temporal
and reachability constraints. We model the evolution of the hybrid system as a
two-player game, where the nondeterminism is an adversarial player whose
objective is to prevent achieving temporal and reachability goals. The aim is
to synthesize a winning strategy -- a reactive (robust) strategy that
guarantees the satisfaction of the goals under all possible moves of the
adversarial player. Our proposed approach involves growing a (search) game-tree
in the hybrid space by combining sampling-based motion planning with a novel
bandit-based technique to select and improve on partial strategies. We show
that the algorithm is probabilistically complete, i.e., the algorithm will
asymptotically almost surely find a winning strategy, if one exists. The case
studies and benchmark results show that our algorithm is general and effective,
and consistently outperforms state of the art algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ho_Q/0/1/0/all/0/1&quot;&gt;Qi Heng Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sunberg_Z/0/1/0/all/0/1&quot;&gt;Zachary N. Sunberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lahijanian_M/0/1/0/all/0/1&quot;&gt;Morteza Lahijanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.01461">
<title>Mixed-Integer Optimal Control via Reinforcement Learning: A Case Study on Hybrid Vehicle Energy Management. (arXiv:2305.01461v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2305.01461</link>
<description rdf:parseType="Literal">&lt;p&gt;Many optimal control problems require the simultaneous output of continuous
and discrete control variables. Such problems are usually formulated as
mixed-integer optimal control (MIOC) problems, which are challenging to solve
due to the complexity of the solution space. Numerical methods such as
branch-and-bound are computationally expensive and unsuitable for real-time
control. This brief proposes a novel continuous-discrete reinforcement learning
(CDRL) algorithm, twin delayed deep deterministic actor-Q (TD3AQ), for MIOC
problems. TD3AQ combines the advantages of both actor-critic and Q-learning
methods, and can handle the continuous and discrete action spaces
simultaneously. The proposed algorithm is evaluated on a plug-in hybrid
electric vehicle (PHEV) energy management problem, where real-time control of
the continuous variable, engine torque, and discrete variables, gear shift and
clutch engagement/disengagement is essential to maximize fuel economy while
satisfying driving constraints. Simulation results on different drive cycles
show that TD3AQ achieves near-optimal control compared to dynamic programming
(DP) and outperforms baseline reinforcement learning algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jinming Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yuan Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02437">
<title>Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory. (arXiv:2305.02437v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02437</link>
<description rdf:parseType="Literal">&lt;p&gt;With direct access to human-written reference as memory, retrieval-augmented
generation has achieved much progress in a wide range of text generation tasks.
Since better memory would typically prompt better generation~(we define this as
primal problem). The traditional approach for memory retrieval involves
selecting memory that exhibits the highest similarity to the input. However,
this method is constrained by the quality of the fixed corpus from which memory
is retrieved. In this paper, by exploring the duality of the primal problem:
better generation also prompts better memory, we propose a novel framework,
selfmem, which addresses this limitation by iteratively employing a
retrieval-augmented generator to create an unbounded memory pool and using a
memory selector to choose one output as memory for the subsequent generation
round. This enables the model to leverage its own output, referred to as
self-memory, for improved generation. We evaluate the effectiveness of selfmem
on three distinct text generation tasks: neural machine translation,
abstractive text summarization, and dialogue generation, under two generation
paradigms: fine-tuned small model and few-shot LLM. Our approach achieves
state-of-the-art results in four directions in JRC-Acquis, XSum (50.3 ROUGE-1),
and BigPatent (62.9 ROUGE-1), demonstrating the potential of self-memory in
enhancing retrieval-augmented generation models. Furthermore, we conduct
thorough analyses of each component in the selfmem framework to identify
bottlenecks and provide insights for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xin Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1&quot;&gt;Di Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiuying Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lemao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Dongyan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1&quot;&gt;Rui Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14910">
<title>From Shortcuts to Triggers: Backdoor Defense with Denoised PoE. (arXiv:2305.14910v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14910</link>
<description rdf:parseType="Literal">&lt;p&gt;Language models are often at risk of diverse backdoor attacks, especially
data poisoning. Thus, it is important to investigate defense solutions for
addressing them. Existing backdoor defense methods mainly focus on backdoor
attacks with explicit triggers, leaving a universal defense against various
backdoor attacks with diverse triggers largely unexplored. In this paper, we
propose an end-to-end ensemble-based backdoor defense framework, DPoE (Denoised
Product-of-Experts), which is inspired by the shortcut nature of backdoor
attacks, to defend various backdoor attacks. DPoE consists of two models: a
shallow model that captures the backdoor shortcuts and a main model that is
prevented from learning the backdoor shortcuts. To address the label flip
caused by backdoor attackers, DPoE incorporates a denoising design. Experiments
on SST-2 dataset show that DPoE significantly improves the defense performance
against various types of backdoor triggers including word-level,
sentence-level, and syntactic triggers. Furthermore, DPoE is also effective
under a more challenging but practical setting that mixes multiple types of
trigger.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Muhao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18341">
<title>Coarse-Tuning Models of Code with Reinforcement Learning Feedback. (arXiv:2305.18341v2 [cs.PL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18341</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) pre-trained on code have recently emerged as the
dominant approach to program synthesis. However, these models are trained using
next-token prediction, which ignores the syntax and semantics of code. We
propose RLCF, that further trains a pre-trained LLM via reinforcement learning,
using feedback from a grounding function that scores the quality of the code.
The grounding function uses (i) compiler-derived feedback on whether the code
it generates passes a set of correctness checks; and (ii) feedback from a
different LLM that compares the generated code to a reference code. RLCF is
model- and language-agnostic. We empirically evaluate it on the MBJP and MathQA
tasks for Java. Our experiments show that RLCF raises the odds that an
LLM-generated program compiles, is executable, and produces the right output on
tests, often allowing LLMs to match the performance of 2x-8x larger LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Abhinav Jain&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adiole_C/0/1/0/all/0/1&quot;&gt;Chima Adiole&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1&quot;&gt;Swarat Chaudhuri&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reps_T/0/1/0/all/0/1&quot;&gt;Thomas Reps&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jermaine_C/0/1/0/all/0/1&quot;&gt;Chris Jermaine&lt;/a&gt; (1) ((1) Rice University, (2) UT Austin, (3) University of Wisconsin)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05685">
<title>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. (arXiv:2306.05685v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05685</link>
<description rdf:parseType="Literal">&lt;p&gt;Evaluating large language model (LLM) based chat assistants is challenging
due to their broad capabilities and the inadequacy of existing benchmarks in
measuring human preferences. To address this, we explore using strong LLMs as
judges to evaluate these models on more open-ended questions. We examine the
usage and limitations of LLM-as-a-judge, including position, verbosity, and
self-enhancement biases, as well as limited reasoning ability, and propose
solutions to mitigate some of them. We then verify the agreement between LLM
judges and human preferences by introducing two benchmarks: MT-bench, a
multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our
results reveal that strong LLM judges like GPT-4 can match both controlled and
crowdsourced human preferences well, achieving over 80% agreement, the same
level of agreement between humans. Hence, LLM-as-a-judge is a scalable and
explainable way to approximate human preferences, which are otherwise very
expensive to obtain. Additionally, we show our benchmark and traditional
benchmarks complement each other by evaluating several variants of LLaMA and
Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with
human preferences are publicly available at
https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Lianmin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_W/0/1/0/all/0/1&quot;&gt;Wei-Lin Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1&quot;&gt;Ying Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1&quot;&gt;Siyuan Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhanghao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1&quot;&gt;Yonghao Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhuohan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dacheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1&quot;&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1&quot;&gt;Ion Stoica&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10249">
<title>Large Generative AI Models for Telecom: The Next Big Thing?. (arXiv:2306.10249v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10249</link>
<description rdf:parseType="Literal">&lt;p&gt;The evolution of generative artificial intelligence (GenAI) constitutes a
turning point in reshaping the future of technology in different aspects.
Wireless networks in particular, with the blooming of self-evolving networks,
represent a rich field for exploiting GenAI and reaping several benefits that
can fundamentally change the way how wireless networks are designed and
operated nowadays. To be specific, large GenAI models are envisioned to open up
a new era of autonomous wireless networks, in which multi-modal GenAI models
trained over various Telecom data, can be fine-tuned to perform several
downstream tasks, eliminating the need for building and training dedicated AI
models for each specific task and paving the way for the realization of
artificial general intelligence (AGI)-empowered wireless networks. In this
article, we aim to unfold the opportunities that can be reaped from integrating
large GenAI models into the Telecom domain. In particular, we first highlight
the applications of large GenAI models in future wireless networks, defining
potential use-cases and revealing insights on the associated theoretical and
practical challenges. Furthermore, we unveil how 6G can open up new
opportunities through connecting multiple on-device large GenAI models, and
hence, paves the way to the collective intelligence paradigm. Finally, we put a
forward-looking vision on how large GenAI models will be the key to realize
self-evolving networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bariah_L/0/1/0/all/0/1&quot;&gt;Lina Bariah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qiyang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1&quot;&gt;Hang Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bader_F/0/1/0/all/0/1&quot;&gt;Faouzi Bader&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Debbah_M/0/1/0/all/0/1&quot;&gt;Merouane Debbah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14535">
<title>About the Cost of Central Privacy in Density Estimation. (arXiv:2306.14535v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14535</link>
<description rdf:parseType="Literal">&lt;p&gt;We study non-parametric density estimation for densities in Lipschitz and
Sobolev spaces, and under central privacy. In particular, we investigate
regimes where the privacy budget is not supposed to be constant. We consider
the classical definition of central differential privacy, but also the more
recent notion of central concentrated differential privacy. We recover the
result of Barber \&amp;amp; Duchi (2014) stating that histogram estimators are optimal
against Lipschitz distributions for the L2 risk, and under regular differential
privacy, and we extend it to other norms and notions of privacy. Then, we
investigate higher degrees of smoothness, drawing two conclusions: First, and
contrary to what happens with constant privacy budget (Wasserman \&amp;amp; Zhou,
2010), there are regimes where imposing privacy degrades the regular minimax
risk of estimation on Sobolev densities. Second, so-called projection
estimators are near-optimal against the same classes of densities in this new
setup with pure differential privacy, but contrary to the constant privacy
budget case, it comes at the cost of relaxation. With zero concentrated
differential privacy, there is no need for relaxation, and we prove that the
estimation is optimal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lalanne_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Lalanne&lt;/a&gt; (ENS de Lyon, OCKHAM), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garivier_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lien Garivier&lt;/a&gt; (UMPA-ENSL, MC2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gribonval_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Gribonval&lt;/a&gt; (OCKHAM)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02588">
<title>TransformerG2G: Adaptive time-stepping for learning temporal graph embeddings using transformers. (arXiv:2307.02588v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02588</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic graph embedding has emerged as a very effective technique for
addressing diverse temporal graph analytic tasks (i.e., link prediction, node
classification, recommender systems, anomaly detection, and graph generation)
in various applications. Such temporal graphs exhibit heterogeneous transient
dynamics, varying time intervals, and highly evolving node features throughout
their evolution. Hence, incorporating long-range dependencies from the
historical graph context plays a crucial role in accurately learning their
temporal dynamics. In this paper, we develop a graph embedding model with
uncertainty quantification, TransformerG2G, by exploiting the advanced
transformer encoder to first learn intermediate node representations from its
current state ($t$) and previous context (over timestamps [$t-1, t-l$], $l$ is
the length of context). Moreover, we employ two projection layers to generate
lower-dimensional multivariate Gaussian distributions as each node&apos;s latent
embedding at timestamp $t$. We consider diverse benchmarks with varying levels
of ``novelty&quot; as measured by the TEA (Temporal Edge Appearance) plots. Our
experiments demonstrate that the proposed TransformerG2G model outperforms
conventional multi-step methods and our prior work (DynG2G) in terms of both
link prediction accuracy and computational efficiency, especially for high
degree of novelty. Furthermore, the learned time-dependent attention weights
across multiple graph snapshots reveal the development of an automatic adaptive
time stepping enabled by the transformer. Importantly, by examining the
attention weights, we can uncover temporal dependencies, identify influential
elements, and gain insights into the complex interactions within the graph
structure. For example, we identified a strong correlation between attention
weights and node degree at the various stages of the graph topology evolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varghese_A/0/1/0/all/0/1&quot;&gt;Alan John Varghese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bora_A/0/1/0/all/0/1&quot;&gt;Aniruddha Bora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mengjia Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1&quot;&gt;George Em Karniadakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05385">
<title>Learned Kernels for Interpretable and Efficient Medical Time Series Processing. (arXiv:2307.05385v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05385</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: Signal processing methods are the foundation for clinical
interpretation across a wide variety of medical applications. The advent of
deep learning allowed for an explosion of new models that offered unprecedented
performance but at a cost: deep learning models are often compute-intensive and
lack interpretability.
&lt;/p&gt;
&lt;p&gt;Methods: We propose a sparse, interpretable architecture for medical time
series processing. The method learns a set of lightweight flexible kernels to
construct a single-layer neural network, providing a new efficient, robust, and
interpretable approach. We introduce novel parameter reduction techniques to
further reduce the size of our network. We demonstrate the power of our
architecture on the important task of photoplethysmography artifact detection,
where our approach has performance similar to the state-of-the-art deep neural
networks with several orders of magnitude fewer parameters, allowing for the
integration of deep neural network level performance into extremely low-power
wearable devices.
&lt;/p&gt;
&lt;p&gt;Results: Our interpretable method achieves greater than 99\% of the
performance of the state-of-the-art methods on the artifact detection task, and
even outperforms the state-of-the-art on a challenging out-of-distribution test
set, while using dramatically fewer parameters (2\% of the parameters of
Segade, and about half of the parameters of Tiny-PPG).
&lt;/p&gt;
&lt;p&gt;Conclusions: Learned kernels are competitive with deep neural networks for
medical time series processing with dramatically fewer parameters. Our method
is particularly suited for real-time applications and low-power devices, and it
maintains interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sully F. Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Cheng Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rudin_C/0/1/0/all/0/1&quot;&gt;Cynthia Rudin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05722">
<title>Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations. (arXiv:2307.05722v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05722</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have revolutionized natural language processing
tasks, demonstrating their exceptional capabilities in various domains.
However, their potential for behavior graph understanding in job
recommendations remains largely unexplored. This paper focuses on unveiling the
capability of large language models in understanding behavior graphs and
leveraging this understanding to enhance recommendations in online recruitment,
including the promotion of out-of-distribution (OOD) application. We present a
novel framework that harnesses the rich contextual information and semantic
representations provided by large language models to analyze behavior graphs
and uncover underlying patterns and relationships. Specifically, we propose a
meta-path prompt constructor that leverages LLM recommender to understand
behavior graphs for the first time and design a corresponding path augmentation
module to alleviate the prompt bias introduced by path-based sequence input. By
leveraging this capability, our framework enables personalized and accurate job
recommendations for individual users. We evaluate the effectiveness of our
approach on a comprehensive dataset and demonstrate its ability to improve the
relevance and quality of recommended quality. This research not only sheds
light on the untapped potential of large language models but also provides
valuable insights for developing advanced recommendation systems in the
recruitment market. The findings contribute to the growing field of natural
language processing and offer practical implications for enhancing job search
experiences. We release the code at https://github.com/WLiK/GLRec.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Likang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1&quot;&gt;Zhaopeng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zhi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hengshu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1&quot;&gt;Enhong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12267">
<title>Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v6 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12267</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent large language models (LLMs), e.g., ChatGPT, have been able to
generate human-like and fluent responses when provided with specific
instructions. While admitting the convenience brought by technological
advancement, educators also have concerns that students might leverage LLMs to
complete their writing assignments and pass them off as their original work.
Although many AI content detection studies have been conducted as a result of
such concerns, most of these prior studies modeled AI content detection as a
classification problem, assuming that a text is either entirely human-written
or entirely AI-generated. In this study, we investigated AI content detection
in a rarely explored yet realistic setting where the text to be detected is
collaboratively written by human and generative LLMs (i.e., hybrid text). We
first formalized the detection task as identifying the transition points
between human-written content and AI-generated content from a given hybrid text
(boundary detection). Then we proposed a two-step approach where we (1)
separated AI-generated content from human-written content during the encoder
training process; and (2) calculated the distances between every two adjacent
prototypes and assumed that the boundaries exist between the two adjacent
prototypes that have the furthest distance from each other. Through extensive
experiments, we observed the following main findings: (1) the proposed approach
consistently outperformed the baseline methods across different experiment
settings; (2) the encoder training process can significantly boost the
performance of the proposed approach; (3) when detecting boundaries for
single-boundary hybrid essays, the proposed approach could be enhanced by
adopting a relatively large prototype size, leading to a 22% improvement in the
In-Domain evaluation and an 18% improvement in the Out-of-Domain evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zijie Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1&quot;&gt;Lele Sha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kaixun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gasevic_D/0/1/0/all/0/1&quot;&gt;Dragan Ga&amp;#x161;evi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guanliang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16082">
<title>EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction. (arXiv:2307.16082v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16082</link>
<description rdf:parseType="Literal">&lt;p&gt;Social platforms have emerged as crucial platforms for disseminating
information and discussing real-life social events, offering researchers an
excellent opportunity to design and implement novel event detection frameworks.
However, most existing approaches only exploit keyword burstiness or network
structures to detect unspecified events. Thus, they often need help identifying
unknown events regarding the challenging nature of events and social data.
Social data, e.g., tweets, is characterized by misspellings, incompleteness,
word sense ambiguation, irregular language, and variation in aspects of
opinions. Moreover, extracting discriminative features and patterns for
evolving events by exploiting the limited structural knowledge is almost
infeasible. To address these challenges, in this paper, we propose a novel
framework, namely EnrichEvent, that leverages the linguistic and contextual
representations of streaming social data. In particular, we leverage contextual
and linguistic knowledge to detect semantically related tweets and enhance the
effectiveness of the event detection approaches. Eventually, our proposed
framework produces cluster chains for each event to show the evolving variation
of the event through time. We conducted extensive experiments to evaluate our
framework, validating its high performance and effectiveness in detecting and
distinguishing unspecified social events.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esfahani_M/0/1/0/all/0/1&quot;&gt;Mohammadali Sefidi Esfahani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akbari_M/0/1/0/all/0/1&quot;&gt;Mohammad Akbari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03314">
<title>GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis. (arXiv:2308.03314v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03314</link>
<description rdf:parseType="Literal">&lt;p&gt;Smart contracts are prone to various vulnerabilities, leading to substantial
financial losses over time. Current analysis tools mainly target
vulnerabilities with fixed control or data-flow patterns, such as re-entrancy
and integer overflow. However, a recent study on Web3 security bugs revealed
that about 80% of these bugs cannot be audited by existing tools due to the
lack of domain-specific property description and checking. Given recent
advances in Large Language Models (LLMs), it is worth exploring how Generative
Pre-training Transformer (GPT) could aid in detecting logicc vulnerabilities.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose GPTScan, the first tool combining GPT with static
analysis for smart contract logic vulnerability detection. Instead of relying
solely on GPT to identify vulnerabilities, which can lead to high false
positives and is limited by GPT&apos;s pre-trained knowledge, we utilize GPT as a
versatile code understanding tool. By breaking down each logic vulnerability
type into scenarios and properties, GPTScan matches candidate vulnerabilities
with GPT. To enhance accuracy, GPTScan further instructs GPT to intelligently
recognize key variables and statements, which are then validated by static
confirmation. Evaluation on diverse datasets with around 400 contract projects
and 3K Solidity files shows that GPTScan achieves high precision (over 90%) for
token contracts and acceptable precision (57.14%) for large projects like
Web3Bugs. It effectively detects ground-truth logic vulnerabilities with a
recall of over 70%, including 9 new vulnerabilities missed by human auditors.
GPTScan is fast and cost-effective, taking an average of 14.39 seconds and 0.01
USD to scan per thousand lines of Solidity code. Moreover, static confirmation
helps GPTScan reduce two-thirds of false positives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuqiang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Daoyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yue Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haijun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhengzi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaofei Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06595">
<title>VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use. (arXiv:2308.06595v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06595</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for
evaluation of instruction-following vision-language models for real-world use.
Our starting point is curating 70 &apos;instruction families&apos; that we envision
instruction tuned vision-language models should be able to address. Extending
beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to
game playing and creative generation. Following curation, our dataset comprises
592 test queries, each with a human-authored instruction-conditioned caption.
These descriptions surface instruction-specific factors, e.g., for an
instruction asking about the accessibility of a storefront for wheelchair
users, the instruction-conditioned caption describes ramps/potential obstacles.
These descriptions enable 1) collecting human-verified reference outputs for
each instance; and 2) automatic evaluation of candidate multimodal generations
using a text-only LLM, aligning with human judgment. We quantify quality gaps
between models and references using both human and automatic evaluations; e.g.,
the top-performing instruction-following model wins against the GPT-4 reference
in just 27% of the comparison. VisIT-Bench is dynamic to participate,
practitioners simply submit their model&apos;s response on the project website;
Data, code and leaderboard is available at visit-bench.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1&quot;&gt;Hritik Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1&quot;&gt;Jack Hessel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1&quot;&gt;Rulin Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wanrong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awadalla_A/0/1/0/all/0/1&quot;&gt;Anas Awadalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1&quot;&gt;Josh Gardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1&quot;&gt;Rohan Taori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07948">
<title>Leveraging Symmetries in Pick and Place. (arXiv:2308.07948v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07948</link>
<description rdf:parseType="Literal">&lt;p&gt;Robotic pick and place tasks are symmetric under translations and rotations
of both the object to be picked and the desired place pose. For example, if the
pick object is rotated or translated, then the optimal pick action should also
rotate or translate. The same is true for the place pose; if the desired place
pose changes, then the place action should also transform accordingly. A
recently proposed pick and place framework known as Transporter Net captures
some of these symmetries, but not all. This paper analytically studies the
symmetries present in planar robotic pick and place and proposes a method of
incorporating equivariant neural models into Transporter Net in a way that
captures all symmetries. The new model, which we call Equivariant Transporter
Net, is equivariant to both pick and place symmetries and can immediately
generalize pick and place knowledge to different pick and place poses. We
evaluate the new model empirically and show that it is much more sample
efficient than the non-symmetric version, resulting in a system that can
imitate demonstrated pick and place behavior using very few human
demonstrations on a variety of imitation learning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haojie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tangri_A/0/1/0/all/0/1&quot;&gt;Arsh Tangri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walters_R/0/1/0/all/0/1&quot;&gt;Robin Walters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Platt_R/0/1/0/all/0/1&quot;&gt;Robert Platt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09891">
<title>SwinLSTM:Improving Spatiotemporal Prediction Accuracy using Swin Transformer and LSTM. (arXiv:2308.09891v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09891</link>
<description rdf:parseType="Literal">&lt;p&gt;Integrating CNNs and RNNs to capture spatiotemporal dependencies is a
prevalent strategy for spatiotemporal prediction tasks. However, the property
of CNNs to learn local spatial information decreases their efficiency in
capturing spatiotemporal dependencies, thereby limiting their prediction
accuracy. In this paper, we propose a new recurrent cell, SwinLSTM, which
integrates Swin Transformer blocks and the simplified LSTM, an extension that
replaces the convolutional structure in ConvLSTM with the self-attention
mechanism. Furthermore, we construct a network with SwinLSTM cell as the core
for spatiotemporal prediction. Without using unique tricks, SwinLSTM
outperforms state-of-the-art methods on Moving MNIST, Human3.6m, TaxiBJ, and
KTH datasets. In particular, it exhibits a significant improvement in
prediction accuracy compared to ConvLSTM. Our competitive experimental results
demonstrate that learning global spatial dependencies is more advantageous for
models to capture spatiotemporal dependencies. We hope that SwinLSTM can serve
as a solid baseline to promote the advancement of spatiotemporal prediction
accuracy. The codes are publicly available at
https://github.com/SongTang-x/SwinLSTM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Song Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chuang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1&quot;&gt;RongNian Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10169">
<title>Efficient Real-time Path Planning with Self-evolving Particle Swarm Optimization in Dynamic Scenarios. (arXiv:2308.10169v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10169</link>
<description rdf:parseType="Literal">&lt;p&gt;Particle Swarm Optimization (PSO) has demonstrated efficacy in addressing
static path planning problems. Nevertheless, such application on dynamic
scenarios has been severely precluded by PSO&apos;s low computational efficiency and
premature convergence downsides. To address these limitations, we proposed a
Tensor Operation Form (TOF) that converts particle-wise manipulations to tensor
operations, thereby enhancing computational efficiency. Harnessing the
computational advantage of TOF, a variant of PSO, designated as Self-Evolving
Particle Swarm Optimization (SEPSO) was developed. The SEPSO is underpinned by
a novel Hierarchical Self-Evolving Framework (HSEF) that enables autonomous
optimization of its own hyper-parameters to evade premature convergence.
Additionally, a Priori Initialization (PI) mechanism and an Auto Truncation
(AT) mechanism that substantially elevates the real-time performance of SEPSO
on dynamic path planning problems were introduced. Comprehensive experiments on
four widely used benchmark optimization functions have been initially conducted
to corroborate the validity of SEPSO. Following this, a dynamic simulation
environment that encompasses moving start/target points and dynamic/static
obstacles was employed to assess the effectiveness of SEPSO on the dynamic path
planning problem. Simulation results exhibit that the proposed SEPSO is capable
of generating superior paths with considerably better real-time performance (67
path planning computations per second in a regular desktop computer) in
contrast to alternative methods. The code and video of this paper can be
accessed here.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1&quot;&gt;Jinghao Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1&quot;&gt;Ning Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10800">
<title>Fact-checking information generated by a large language model can decrease news discernment. (arXiv:2308.10800v3 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10800</link>
<description rdf:parseType="Literal">&lt;p&gt;Fact checking can be an effective strategy against misinformation, but its
implementation at scale is impeded by the overwhelming volume of information
online. Recent artificial intelligence (AI) language models have shown
impressive ability in fact-checking tasks, but how humans interact with
fact-checking information provided by these models is unclear. Here, we
investigate the impact of fact-checking information generated by a popular
large language model (LLM) on belief in, and sharing intent of, political news
in a preregistered randomized control experiment. Although the LLM performs
reasonably well in debunking false headlines, we find that it does not
significantly affect participants&apos; ability to discern headline accuracy or
share accurate news. Subsequent analysis reveals that the AI fact-checker is
harmful in specific cases: it decreases beliefs in true headlines that it
mislabels as false and increases beliefs in false headlines that it is unsure
about. On the positive side, the AI fact-checking information increases sharing
intents for correctly labeled true headlines. When participants are given the
option to view LLM fact checks and choose to do so, they are significantly more
likely to share both true and false news but only more likely to believe false
news. Our findings highlight an important source of potential harm stemming
from AI applications and underscore the critical need for policies to prevent
or mitigate such unintended consequences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeVerna_M/0/1/0/all/0/1&quot;&gt;Matthew R. DeVerna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Harry Yaojun Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kai-Cheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menczer_F/0/1/0/all/0/1&quot;&gt;Filippo Menczer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11730">
<title>Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11730</link>
<description rdf:parseType="Literal">&lt;p&gt;The `pre-train, prompt, predict&apos; paradigm of large language models (LLMs) has
achieved remarkable success in open-domain question answering (OD-QA). However,
few works explore this paradigm in the scenario of multi-document question
answering (MD-QA), a task demanding a thorough understanding of the logical
associations among the contents and structures of different documents. To fill
this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to
formulate the right context in prompting LLMs for MD-QA, which consists of a
graph construction module and a graph traversal module. For graph construction,
we create a knowledge graph (KG) over multiple documents with nodes symbolizing
passages or document structures (e.g., pages/tables), and edges denoting the
semantic/lexical similarity between passages or intra-document structural
relations. For graph traversal, we design an LLM-based graph traversal agent
that navigates across nodes and gathers supporting passages assisting LLMs in
MD-QA. The constructed graph serves as the global ruler that regulates the
transitional space among passages and reduces retrieval latency. Concurrently,
the graph traversal agent acts as a local navigator that gathers pertinent
context to progressively approach the question and guarantee retrieval quality.
Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the
potential of leveraging graphs in enhancing the prompt design for LLMs. Our
code: https://github.com/YuWVandy/KG-LLM-MDQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1&quot;&gt;Nedim Lipka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1&quot;&gt;Ryan A. Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siu_A/0/1/0/all/0/1&quot;&gt;Alexa Siu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruiyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derr_T/0/1/0/all/0/1&quot;&gt;Tyler Derr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14089">
<title>MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records. (arXiv:2308.14089v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14089</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability of large language models (LLMs) to follow natural language
instructions with human-level fluency suggests many opportunities in healthcare
to reduce administrative burden and improve quality of care. However,
evaluating LLMs on realistic text generation tasks for healthcare remains
challenging. Existing question answering datasets for electronic health record
(EHR) data fail to capture the complexity of information needs and
documentation burdens experienced by clinicians. To address these challenges,
we introduce MedAlign, a benchmark dataset of 983 natural language instructions
for EHR data. MedAlign is curated by 15 clinicians (7 specialities), includes
clinician-written reference responses for 303 instructions, and provides 276
longitudinal EHRs for grounding instruction-response pairs. We used MedAlign to
evaluate 6 general domain LLMs, having clinicians rank the accuracy and quality
of each LLM response. We found high error rates, ranging from 35% (GPT-4) to
68% (MPT-7B-Instruct), and an 8.3% drop in accuracy moving from 32k to 2k
context lengths for GPT-4. Finally, we report correlations between clinician
rankings and automated natural language generation metrics as a way to rank
LLMs without human review. We make MedAlign available under a research data use
agreement to enable LLM evaluations on tasks aligned with clinician needs and
preferences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fleming_S/0/1/0/all/0/1&quot;&gt;Scott L. Fleming&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lozano_A/0/1/0/all/0/1&quot;&gt;Alejandro Lozano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haberkorn_W/0/1/0/all/0/1&quot;&gt;William J. Haberkorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jindal_J/0/1/0/all/0/1&quot;&gt;Jenelle A. Jindal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reis_E/0/1/0/all/0/1&quot;&gt;Eduardo P. Reis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thapa_R/0/1/0/all/0/1&quot;&gt;Rahul Thapa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blankemeier_L/0/1/0/all/0/1&quot;&gt;Louis Blankemeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Genkins_J/0/1/0/all/0/1&quot;&gt;Julian Z. Genkins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinberg_E/0/1/0/all/0/1&quot;&gt;Ethan Steinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nayak_A/0/1/0/all/0/1&quot;&gt;Ashwin Nayak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_B/0/1/0/all/0/1&quot;&gt;Birju S. Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_C/0/1/0/all/0/1&quot;&gt;Chia-Chun Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Callahan_A/0/1/0/all/0/1&quot;&gt;Alison Callahan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1&quot;&gt;Zepeng Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gatidis_S/0/1/0/all/0/1&quot;&gt;Sergios Gatidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adams_S/0/1/0/all/0/1&quot;&gt;Scott J. Adams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fayanju_O/0/1/0/all/0/1&quot;&gt;Oluseyi Fayanju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1&quot;&gt;Shreya J. Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savage_T/0/1/0/all/0/1&quot;&gt;Thomas Savage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goh_E/0/1/0/all/0/1&quot;&gt;Ethan Goh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhari_A/0/1/0/all/0/1&quot;&gt;Akshay S. Chaudhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aghaeepour_N/0/1/0/all/0/1&quot;&gt;Nima Aghaeepour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharp_C/0/1/0/all/0/1&quot;&gt;Christopher Sharp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfeffer_M/0/1/0/all/0/1&quot;&gt;Michael A. Pfeffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Percy Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jonathan H. Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morse_K/0/1/0/all/0/1&quot;&gt;Keith E. Morse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1&quot;&gt;Emma P. Brunskill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fries_J/0/1/0/all/0/1&quot;&gt;Jason A. Fries&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Nigam H. Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07867">
<title>Beta Diffusion. (arXiv:2309.07867v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07867</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce beta diffusion, a novel generative modeling method that
integrates demasking and denoising to generate data within bounded ranges.
Using scaled and shifted beta distributions, beta diffusion utilizes
multiplicative transitions over time to create both forward and reverse
diffusion processes, maintaining beta distributions in both the forward
marginals and the reverse conditionals, given the data at any point in time.
Unlike traditional diffusion-based generative models relying on additive
Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is
multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived
from the convexity of the KL divergence. We demonstrate that the proposed KLUBs
are more effective for optimizing beta diffusion compared to negative ELBOs,
which can also be derived as the KLUBs of the same KL divergence with its two
arguments swapped. The loss function of beta diffusion, expressed in terms of
Bregman divergence, further supports the efficacy of KLUBs for optimization.
Experimental results on both synthetic data and natural images demonstrate the
unique capabilities of beta diffusion in generative modeling of range-bounded
data and validate the effectiveness of KLUBs in optimizing diffusion models,
thereby making them valuable additions to the family of diffusion-based
generative models and the optimization techniques used to train them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingyuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhendong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Huangjie Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09180">
<title>Neural Speaker Diarization Using Memory-Aware Multi-Speaker Embedding with Sequence-to-Sequence Architecture. (arXiv:2309.09180v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09180</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel neural speaker diarization system using memory-aware
multi-speaker embedding with sequence-to-sequence architecture (NSD-MS2S),
which integrates the strengths of memory-aware multi-speaker embedding (MA-MSE)
and sequence-to-sequence (Seq2Seq) architecture, leading to improvement in both
efficiency and performance. Next, we further decrease the memory occupation of
decoding by incorporating input features fusion and then employ a multi-head
attention mechanism to capture features at different levels. NSD-MS2S achieved
a macro diarization error rate (DER) of 15.9% on the CHiME-7 EVAL set, which
signifies a relative improvement of 49% over the official baseline system, and
is the key technique for us to achieve the best performance for the main track
of CHiME-7 DASR Challenge. Additionally, we introduce a deep interactive module
(DIM) in MA-MSE module to better retrieve a cleaner and more discriminative
multi-speaker embedding, enabling the current model to outperform the system we
used in the CHiME-7 DASR Challenge. Our code will be available at
https://github.com/liyunlongaaa/NSD-MS2S.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Gaobin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_M/0/1/0/all/0/1&quot;&gt;Maokui He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Niu_S/0/1/0/all/0/1&quot;&gt;Shutong Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruoyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yanyan Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qian_S/0/1/0/all/0/1&quot;&gt;Shuangqing Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shilong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;Jun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chin-Hui Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14316">
<title>Physics of Language Models: Part 3.1, Knowledge Storage and Extraction. (arXiv:2309.14316v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14316</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) can store a vast amount of world knowledge,
often extractable via question-answering (e.g., &quot;What is Abraham Lincoln&apos;s
birthday?&quot;). However, do they answer such questions based on exposure to
similar questions during training (i.e., cheating), or by genuinely learning to
extract knowledge from sources like Wikipedia?
&lt;/p&gt;
&lt;p&gt;In this paper, we investigate this issue using a controlled biography
dataset. We find a strong correlation between the model&apos;s ability to extract
knowledge and various diversity measures of the training data.
$\textbf{Essentially}$, for knowledge to be reliably extracted, it must be
sufficiently augmented (e.g., through paraphrasing, sentence shuffling)
$\textit{during pretraining}$. Without such augmentation, knowledge may be
memorized but not extractable, leading to 0% accuracy, regardless of subsequent
instruction fine-tuning.
&lt;/p&gt;
&lt;p&gt;To understand why this occurs, we employ (nearly) linear probing to
demonstrate a strong connection between the observed correlation and how the
model internally encodes knowledge -- whether it is linearly encoded in the
hidden embeddings of entity names or distributed across other token embeddings
in the training text.
&lt;/p&gt;
&lt;p&gt;This paper provides $\textbf{several key recommendations for LLM pretraining
in the industry}$: (1) rewrite the pretraining data -- using small, auxiliary
models -- to provide knowledge augmentation, and (2) incorporate more
instruction-finetuning data into the pretraining stage before it becomes too
late.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1&quot;&gt;Zeyuan Allen-Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14970">
<title>Recurrent Hypernetworks are Surprisingly Strong in Meta-RL. (arXiv:2309.14970v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14970</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning (RL) is notoriously impractical to deploy due to
sample inefficiency. Meta-RL directly addresses this sample inefficiency by
learning to perform few-shot learning when a distribution of related tasks is
available for meta-training. While many specialized meta-RL methods have been
proposed, recent work suggests that end-to-end learning in conjunction with an
off-the-shelf sequential model, such as a recurrent network, is a surprisingly
strong baseline. However, such claims have been controversial due to limited
supporting evidence, particularly in the face of prior work establishing
precisely the opposite. In this paper, we conduct an empirical investigation.
While we likewise find that a recurrent network can achieve strong performance,
we demonstrate that the use of hypernetworks is crucial to maximizing their
potential. Surprisingly, when combined with hypernetworks, the recurrent
baselines that are far simpler than existing specialized methods actually
achieve the strongest performance of all methods evaluated. We provide code at
https://github.com/jacooba/hyper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beck_J/0/1/0/all/0/1&quot;&gt;Jacob Beck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuorio_R/0/1/0/all/0/1&quot;&gt;Risto Vuorio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zheng Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1&quot;&gt;Shimon Whiteson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15074">
<title>Natural Language based Context Modeling and Reasoning for Ubiquitous Computing with Large Language Models: A Tutorial. (arXiv:2309.15074v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15074</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have become phenomenally surging, since
2018--two decades after introducing context-awareness into computing systems.
Through taking into account the situations of ubiquitous devices, users and the
societies, context-aware computing has enabled a wide spectrum of innovative
applications, such as assisted living, location-based social network services
and so on. To recognize contexts and make decisions for actions accordingly,
various artificial intelligence technologies, such as Ontology and OWL, have
been adopted as representations for context modeling and reasoning. Recently,
with the rise of LLMs and their improved natural language understanding and
reasoning capabilities, it has become feasible to model contexts using natural
language and perform context reasoning by interacting with LLMs such as ChatGPT
and GPT-4. In this tutorial, we demonstrate the use of texts, prompts, and
autonomous agents (AutoAgents) that enable LLMs to perform context modeling and
reasoning without requiring fine-tuning of the model. We organize and introduce
works in the related field, and name this computing paradigm as the LLM-driven
Context-aware Computing (LCaC). In the LCaC paradigm, users&apos; requests, sensors
reading data, and the command to actuators are supposed to be represented as
texts. Given the text of users&apos; request and sensor data, the AutoAgent models
the context by prompting and sends to the LLM for context reasoning. LLM
generates a plan of actions and responds to the AutoAgent, which later follows
the action plan to foster context-awareness. To prove the concepts, we use two
showcases--(1) operating a mobile z-arm in an apartment for assisted living,
and (2) planning a trip and scheduling the itinerary in a context-aware and
personalized manner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Haoyi Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1&quot;&gt;Jiang Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Sijia Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaofei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1&quot;&gt;Linghe Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Daqing Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01569">
<title>Iterative Option Discovery for Planning, by Planning. (arXiv:2310.01569v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01569</link>
<description rdf:parseType="Literal">&lt;p&gt;Discovering useful temporal abstractions, in the form of options, is widely
thought to be key to applying reinforcement learning and planning to
increasingly complex domains. Building on the empirical success of the Expert
Iteration approach to policy learning used in AlphaZero, we propose Option
Iteration, an analogous approach to option discovery. Rather than learning a
single strong policy that is trained to match the search results everywhere,
Option Iteration learns a set of option policies trained such that for each
state encountered, at least one policy in the set matches the search results
for some horizon into the future. Intuitively, this may be significantly easier
as it allows the algorithm to hedge its bets compared to learning a single
globally strong policy, which may have complex dependencies on the details of
the current state. Having learned such a set of locally strong policies, we can
use them to guide the search algorithm resulting in a virtuous cycle where
better options lead to better search results which allows for training of
better options. We demonstrate experimentally that planning using options
learned with Option Iteration leads to a significant benefit in challenging
planning environments compared to an analogous planning algorithm operating in
the space of primitive actions and learning a single rollout policy with Expert
Iteration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Young_K/0/1/0/all/0/1&quot;&gt;Kenny Young&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1&quot;&gt;Richard S. Sutton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07937">
<title>Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using Large Language Models. (arXiv:2310.07937v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07937</link>
<description rdf:parseType="Literal">&lt;p&gt;In advanced human-robot interaction tasks, visual target navigation is
crucial for autonomous robots navigating unknown environments. While numerous
approaches have been developed in the past, most are designed for single-robot
operations, which often suffer from reduced efficiency and robustness due to
environmental complexities. Furthermore, learning policies for multi-robot
collaboration are resource-intensive. To address these challenges, we propose
Co-NavGPT, an innovative framework that integrates Large Language Models (LLMs)
as a global planner for multi-robot cooperative visual target navigation.
Co-NavGPT encodes the explored environment data into prompts, enhancing LLMs&apos;
scene comprehension. It then assigns exploration frontiers to each robot for
efficient target search. Experimental results on Habitat-Matterport 3D (HM3D)
demonstrate that Co-NavGPT surpasses existing models in success rates and
efficiency without any learning process, demonstrating the vast potential of
LLMs in multi-robot collaboration domains. The supplementary video, prompts,
and code can be accessed via the following link:
https://sites.google.com/view/co-navgpt
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bangguo Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasaei_H/0/1/0/all/0/1&quot;&gt;Hamidreza Kasaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1&quot;&gt;Ming Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08475">
<title>Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08475</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we focus on editing Multimodal Large Language Models (MLLMs).
Compared to editing single-modal LLMs, multimodal model editing is more
challenging, which demands a higher level of scrutiny and careful consideration
in the editing process. To facilitate research in this area, we construct a new
benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite
of innovative metrics for evaluation. We conduct comprehensive experiments
involving various model editing baselines and analyze the impact of editing
different components for multimodal LLMs. Empirically, we notice that previous
baselines can implement editing multimodal LLMs to some extent, but the effect
is still barely satisfactory, indicating the potential difficulty of this task.
We hope that our work can provide the NLP community with insights. Code and
dataset are available in https://github.com/zjunlp/EasyEdit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Siyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1&quot;&gt;Bozhong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qingbin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yongheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10072">
<title>Fine-tuning ChatGPT for Automatic Scoring. (arXiv:2310.10072v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10072</link>
<description rdf:parseType="Literal">&lt;p&gt;This study highlights the potential of fine-tuned ChatGPT (GPT-3.5) for
automatically scoring student written constructed responses using example
assessment tasks in science education. Recent studies on OpenAI&apos;s generative
model GPT-3.5 proved its superiority in predicting the natural language with
high accuracy and human-like responses. GPT-3.5 has been trained over enormous
online language materials such as journals and Wikipedia; therefore, more than
direct usage of pre-trained GPT-3.5 is required for automatic scoring as
students utilize a different language than trained material. These imply that a
domain-specific model, fine-tuned over data for specific tasks, can enhance
model performance. In this study, we fine-tuned GPT-3.5 on six assessment tasks
with a diverse dataset of middle-school and high-school student responses and
expert scoring. The six tasks comprise two multi-label and four multi-class
assessment tasks. We compare the performance of fine-tuned GPT-3.5 with the
fine-tuned state-of-the-art Google&apos;s generated language model, BERT. The
results show that in-domain training corpora constructed from science questions
and responses for BERT achieved average accuracy = 0.838, SD = 0.069. GPT-3.5
shows a remarkable average increase (9.1%) in automatic scoring accuracy (mean
= 9.15, SD = 0.042) for the six tasks, p =0.001 &amp;lt; 0.05. Specifically, for
multi-label tasks (item 1 with 5 labels; item 2 with 10 labels), GPT-3.5
achieved significantly higher scoring accuracy than BERT across all the labels,
with the second item achieving a 7.1% increase. The average scoring increase
for the four multi-class items for GPT-3.5 was 10.6% compared to BERT. Our
study confirmed the effectiveness of fine-tuned GPT-3.5 for automatic scoring
of student responses on domain-specific data in education with high accuracy.
We have released fine-tuned models for public use and community engagement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latif_E/0/1/0/all/0/1&quot;&gt;Ehsan Latif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1&quot;&gt;Xiaoming Zhai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11971">
<title>Improving Generalization of Alignment with Human Preferences through Group Invariant Learning. (arXiv:2310.11971v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11971</link>
<description rdf:parseType="Literal">&lt;p&gt;The success of AI assistants based on language models (LLMs) hinges crucially
on Reinforcement Learning from Human Feedback (RLHF), which enables the
generation of responses more aligned with human preferences. As universal AI
assistants, there&apos;s a growing expectation for them to perform consistently
across various domains. However, previous work shows that Reinforcement
Learning (RL) often exploits shortcuts to attain high rewards and overlooks
challenging samples. This focus on quick reward gains undermines both the
stability in training and the model&apos;s ability to generalize to new, unseen
data. In this work, we propose a novel approach that can learn a consistent
policy via RL across various data groups or domains. Given the challenges
associated with acquiring group annotations, our method automatically
classifies data into different groups, deliberately maximizing performance
variance. Then, we optimize the policy to perform well on challenging groups.
Lastly, leveraging the established groups, our approach adaptively adjusts the
exploration space, allocating more learning capacity to more challenging data
and preventing the model from over-optimizing on simpler data. Experimental
results indicate that our approach significantly enhances training stability
and model generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1&quot;&gt;Rui Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;Wei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1&quot;&gt;Yuan Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1&quot;&gt;Wenbin Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1&quot;&gt;Shihan Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1&quot;&gt;Zhiheng Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haoran Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1&quot;&gt;Tao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.14403">
<title>O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models. (arXiv:2310.14403v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.14403</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in large language models (LLMs) have exhibited promising
performance in solving sequential decision-making problems. By imitating
few-shot examples provided in the prompts (i.e., in-context learning), an LLM
agent can interact with an external environment and complete given tasks
without additional training. However, such few-shot examples are often
insufficient to generate high-quality solutions for complex and long-horizon
tasks, while the limited context length cannot consume larger-scale
demonstrations. To this end, we propose an offline learning framework that
utilizes offline data at scale (e.g, logs of human interactions) to facilitate
the in-context learning performance of LLM agents. We formally define
LLM-powered policies with both text-based approaches and code-based approaches.
We then introduce an Offline Data-driven Discovery and Distillation (O3D)
framework to improve LLM-powered policies without finetuning. O3D automatically
discovers reusable skills and distills generalizable knowledge across multiple
tasks based on offline interaction data, advancing the capability of solving
downstream tasks. Empirical results under two interactive decision-making
benchmarks (ALFWorld and WebShop) demonstrate that O3D can notably enhance the
decision-making capabilities of LLMs through the offline discovery and
distillation process, and consistently outperform baselines across various LLMs
with both text-based-policy and code-based-policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yuchen Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yanchao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mengda Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madhushani_U/0/1/0/all/0/1&quot;&gt;Udari Madhushani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vann_J/0/1/0/all/0/1&quot;&gt;Jared Vann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_D/0/1/0/all/0/1&quot;&gt;Deepeka Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganesh_S/0/1/0/all/0/1&quot;&gt;Sumitra Ganesh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15296">
<title>DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM. (arXiv:2310.15296v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15296</link>
<description rdf:parseType="Literal">&lt;p&gt;In the burgeoning field of natural language processing (NLP), Neural Topic
Models (NTMs) , Large Language Models (LLMs) and Diffusion model have emerged
as areas of significant research interest. Despite this, NTMs primarily utilize
contextual embeddings from LLMs, which are not optimal for clustering or
capable for topic based text generation. NTMs have never been combined with
diffusion model for text generation. Our study addresses these gaps by
introducing a novel framework named Diffusion-Enhanced Topic Modeling using
Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages Encoder-Decoder-based
LLMs to produce highly clusterable embeddings that could generate topics that
exhibit both superior clusterability and enhanced semantic coherence compared
to existing methods. Additionally, by exploiting the power of diffusion model,
our framework also provides the capability to do topic based text generation.
This dual functionality allows users to efficiently produce highly clustered
topics and topic based text generation simultaneously. DeTiME&apos;s potential
extends to generating clustered embeddings as well. Notably, our proposed
framework(both encoder-decoder based LLM and diffusion model) proves to be
efficient to train and exhibits high adaptability to other LLMs and diffusion
model, demonstrating its potential for a wide array of applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weijie Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenxiang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fanyou Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengamedu_S/0/1/0/all/0/1&quot;&gt;Srinivasan Sengamedu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18378">
<title>Ontology Revision based on Pre-trained Language Models. (arXiv:2310.18378v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18378</link>
<description rdf:parseType="Literal">&lt;p&gt;Ontology revision aims to seamlessly incorporate a new ontology into an
existing ontology and plays a crucial role in tasks such as ontology evolution,
ontology maintenance, and ontology alignment. Similar to repair single
ontologies, resolving logical incoherence in the task of ontology revision is
also important and meaningful, because incoherence is a main potential factor
to cause inconsistency and reasoning with an inconsistent ontology will obtain
meaningless answers.To deal with this problem, various ontology revision
approaches have been proposed to define revision operators and design ranking
strategies for axioms in an ontology. However, they rarely consider axiom
semantics which provides important information to differentiate axioms. In
addition, pre-trained models can be utilized to encode axiom semantics, and
have been widely applied in many natural language processing tasks and
ontology-related ones in recent years.Therefore, in this paper, we study how to
apply pre-trained models to revise ontologies. We first define four scoring
functions to rank axioms based on a pre-trained model by considering various
information from an ontology. Based on the functions, an ontology revision
algorithm is then proposed to deal with unsatisfiable concepts at once. To
improve efficiency, an adapted revision algorithm is designed to deal with
unsatisfiable concepts group by group. We conduct experiments over 19 ontology
pairs and compare our algorithms and scoring functions with existing ones.
According to the experiments, our algorithms could achieve promising
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Q/0/1/0/all/0/1&quot;&gt;Qiu Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1&quot;&gt;Guilin Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yuxin Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiaye Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Site Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jianjie Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Songtao Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18652">
<title>EHRXQA: A Multi-Modal Question Answering Dataset for Electronic Health Records with Chest X-ray Images. (arXiv:2310.18652v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18652</link>
<description rdf:parseType="Literal">&lt;p&gt;Electronic Health Records (EHRs), which contain patients&apos; medical histories
in various multi-modal formats, often overlook the potential for joint
reasoning across imaging and table modalities underexplored in current EHR
Question Answering (QA) systems. In this paper, we introduce EHRXQA, a novel
multi-modal question answering dataset combining structured EHRs and chest
X-ray images. To develop our dataset, we first construct two uni-modal
resources: 1) The MIMIC-CXR-VQA dataset, our newly created medical visual
question answering (VQA) benchmark, specifically designed to augment the
imaging modality in EHR QA, and 2) EHRSQL (MIMIC-IV), a refashioned version of
a previously established table-based EHR QA dataset. By integrating these two
uni-modal resources, we successfully construct a multi-modal EHR QA dataset
that necessitates both uni-modal and cross-modal reasoning. To address the
unique challenges of multi-modal questions within EHRs, we propose a
NeuralSQL-based strategy equipped with an external VQA API. This pioneering
endeavor enhances engagement with multi-modal EHR sources and we believe that
our dataset can catalyze advances in real-world medical scenarios such as
clinical decision-making and research. EHRXQA is available at
https://github.com/baeseongsu/ehrxqa.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1&quot;&gt;Seongsu Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kyung_D/0/1/0/all/0/1&quot;&gt;Daeun Kyung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_J/0/1/0/all/0/1&quot;&gt;Jaehee Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_E/0/1/0/all/0/1&quot;&gt;Eunbyeol Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gyubok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kweon_S/0/1/0/all/0/1&quot;&gt;Sunjun Kweon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Jungwoo Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1&quot;&gt;Lei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1&quot;&gt;Eric I-Chao Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Tackeun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1&quot;&gt;Edward Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01212">
<title>Multi-level Relation Learning for Cross-domain Few-shot Hyperspectral Image Classification. (arXiv:2311.01212v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01212</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-domain few-shot hyperspectral image classification focuses on learning
prior knowledge from a large number of labeled samples from source domains and
then transferring the knowledge to the tasks which contain few labeled samples
in target domains. Following the metric-based manner, many current methods
first extract the features of the query and support samples, and then directly
predict the classes of query samples according to their distance to the support
samples or prototypes. The relations between samples have not been fully
explored and utilized. Different from current works, this paper proposes to
learn sample relations on different levels and take them into the model
learning process, to improve the cross-domain few-shot hyperspectral image
classification. Building on current method of &quot;Deep Cross-Domain Few-Shot
Learning for Hyperspectral Image Classification&quot; which adopts a domain
discriminator to deal with domain-level distribution difference, the proposed
method applies contrastive learning to learn the class-level sample relations
to obtain more discriminable sample features. In addition, it adopts a
transformer based cross-attention learning module to learn the set-level sample
relations and acquire the attention from query samples to support samples. Our
experimental results have demonstrated the contribution of the multi-level
relation learning mechanism for few-shot hyperspectral image classification
when compared with the state of the art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Longwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhigang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jianzhong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Junyong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01248">
<title>Multimodal and Force-Matched Imitation Learning with a See-Through Visuotactile Sensor. (arXiv:2311.01248v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01248</link>
<description rdf:parseType="Literal">&lt;p&gt;Kinesthetic Teaching is a popular approach to collecting expert robotic
demonstrations of contact-rich tasks for imitation learning (IL), but it
typically only measures motion, ignoring the force placed on the environment by
the robot. Furthermore, contact-rich tasks require accurate sensing of both
reaching and touching, which can be difficult to provide with conventional
sensing modalities. We address these challenges with a See-Through-your-Skin
(STS) visuotactile sensor, using the sensor both (i) as a measurement tool to
improve kinesthetic teaching, and (ii) as a policy input in contact-rich door
manipulation tasks. An STS sensor can be switched between visual and tactile
modes by leveraging a semi-transparent surface and controllable lighting,
allowing for both pre-contact visual sensing and during-contact tactile sensing
with a single sensor. First, we propose tactile force matching, a methodology
that enables a robot to match forces read during kinesthetic teaching using
tactile signals. Second, we develop a policy that controls STS mode switching,
allowing a policy to learn the appropriate moment to switch an STS from its
visual to its tactile mode. Finally, we study multiple observation
configurations to compare and contrast the value of visual and tactile data
from an STS with visual data from a wrist-mounted eye-in-hand camera. With over
3,000 test episodes from real-world manipulation experiments, we find that the
inclusion of force matching raises average policy success rates by 62.5%, STS
mode switching by 30.3%, and STS data as a policy input by 42.5%. Our results
highlight the utility of see-through tactile sensing for IL, both for data
collection to allow force matching, and for policy execution to allow accurate
task feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ablett_T/0/1/0/all/0/1&quot;&gt;Trevor Ablett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Limoyo_O/0/1/0/all/0/1&quot;&gt;Oliver Limoyo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sigal_A/0/1/0/all/0/1&quot;&gt;Adam Sigal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jilani_A/0/1/0/all/0/1&quot;&gt;Affan Jilani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1&quot;&gt;Jonathan Kelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siddiqi_K/0/1/0/all/0/1&quot;&gt;Kaleem Siddiqi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hogan_F/0/1/0/all/0/1&quot;&gt;Francois Hogan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1&quot;&gt;Gregory Dudek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04945">
<title>Auto deep learning for bioacoustic signals. (arXiv:2311.04945v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04945</link>
<description rdf:parseType="Literal">&lt;p&gt;This study investigates the potential of automated deep learning to enhance
the accuracy and efficiency of multi-class classification of bird
vocalizations, compared against traditional manually-designed deep learning
models. Using the Western Mediterranean Wetland Birds dataset, we investigated
the use of AutoKeras, an automated machine learning framework, to automate
neural architecture search and hyperparameter tuning. Comparative analysis
validates our hypothesis that the AutoKeras-derived model consistently
outperforms traditional models like MobileNet, ResNet50 and VGG16. Our approach
and findings underscore the transformative potential of automated deep learning
for advancing bioacoustics research and models. In fact, the automated
techniques eliminate the need for manual feature engineering and model design
while improving performance. This study illuminates best practices in sampling,
evaluation and reporting to enhance reproducibility in this nascent field. All
the code used is available at https:
//github.com/giuliotosato/AutoKeras-bioacustic
&lt;/p&gt;
&lt;p&gt;Keywords: AutoKeras; automated deep learning; audio classification; Wetlands
Bird dataset; comparative analysis; bioacoustics; validation dataset;
multi-class classification; spectrograms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tosato_G/0/1/0/all/0/1&quot;&gt;Giulio Tosato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shehata_A/0/1/0/all/0/1&quot;&gt;Abdelrahman Shehata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janssen_J/0/1/0/all/0/1&quot;&gt;Joshua Janssen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamp_K/0/1/0/all/0/1&quot;&gt;Kees Kamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jati_P/0/1/0/all/0/1&quot;&gt;Pramatya Jati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stowell_D/0/1/0/all/0/1&quot;&gt;Dan Stowell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05197">
<title>Deep Learning in Computed Tomography Pulmonary Angiography Imaging: A Dual-Pronged Approach for Pulmonary Embolism Detection. (arXiv:2311.05197v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05197</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing reliance on Computed Tomography Pulmonary Angiography for
Pulmonary Embolism (PE) diagnosis presents challenges and a pressing need for
improved diagnostic solutions. The primary objective of this study is to
leverage deep learning techniques to enhance the Computer Assisted Diagnosis of
PE. With this aim, we propose a classifier-guided detection approach that
effectively leverages the classifier&apos;s probabilistic inference to direct the
detection predictions, marking a novel contribution in the domain of automated
PE diagnosis. Our end-to-end classification framework introduces an
Attention-Guided Convolutional Neural Network (AG-CNN) that leverages local
context by utilizing an attention mechanism. This approach emulates a human
expert&apos;s attention by looking at both global appearances and local lesion
regions before forming a conclusive decision. The classifier demonstrates
strong performance on the FUMPE dataset, achieving AUROC, sensitivity,
specificity, and F1-score of 0.927, 0.862, 0.879, and 0.805 respectively with
Inception-v3 backbone architecture. Moreover, AG-CNN outperforms the baseline
DenseNet-121 model, achieving an 8.1% AUROC gain. While prior studies have
primarily focused on PE detection in main arteries, our utilization of
cutting-edge object detection models and ensembling techniques greatly improves
the accuracy of finding small embolisms in the peripheral arteries. Finally,
our proposed classifier-guided detection approach further refines the detection
metrics contributing new state-of-the-art to the community: mAP$_{50}$,
sensitivity and F1-score of 0.846, 0.901 and 0.779 respectively outperforming
the former benchmark with a significant 3.7% improvement in mAP$_{50}$. Our
research aims to elevate PE patient care by integrating AI solutions into
clinical workflows, highlighting the potential of human-AI collaboration in
medical diagnostics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bushra_F/0/1/0/all/0/1&quot;&gt;Fabiha Bushra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1&quot;&gt;Muhammad E. H. Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarmun_R/0/1/0/all/0/1&quot;&gt;Rusab Sarmun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kabir_S/0/1/0/all/0/1&quot;&gt;Saidul Kabir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Said_M/0/1/0/all/0/1&quot;&gt;Menatalla Said&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoghoul_S/0/1/0/all/0/1&quot;&gt;Sohaib Bassam Zoghoul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mushtak_A/0/1/0/all/0/1&quot;&gt;Adam Mushtak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Hashimi_I/0/1/0/all/0/1&quot;&gt;Israa Al-Hashimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alqahtani_A/0/1/0/all/0/1&quot;&gt;Abdulrahman Alqahtani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1&quot;&gt;Anwarul Hasan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11261">
<title>Adversarial Prompt Tuning for Vision-Language Models. (arXiv:2311.11261v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11261</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid advancement of multimodal learning, pre-trained
Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable
capacities in bridging the gap between visual and language modalities. However,
these models remain vulnerable to adversarial attacks, particularly in the
image modality, presenting considerable security risks. This paper introduces
Adversarial Prompt Tuning (AdvPT), a novel technique to enhance the adversarial
robustness of image encoders in VLMs. AdvPT innovatively leverages learnable
text prompts and aligns them with adversarial image embeddings, to address the
vulnerabilities inherent in VLMs without the need for extensive parameter
training or modification of the model architecture. We demonstrate that AdvPT
improves resistance against white-box and black-box adversarial attacks and
exhibits a synergistic effect when combined with existing
image-processing-based defense techniques, further boosting defensive
capabilities. Comprehensive experimental analyses provide insights into
adversarial prompt tuning, a novel paradigm devoted to improving resistance to
adversarial images through textual input modifications, paving the way for
future robust multimodal learning research. These findings open up new
possibilities for enhancing the security of VLMs. Our code is available at
https://github.com/jiamingzhang94/Adversarial-Prompt-Tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xingjun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Lingyu Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Gang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1&quot;&gt;Jitao Sang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12188">
<title>ChatGPT and post-test probability. (arXiv:2311.12188v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12188</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning-based large language models, such as ChatGPT, are
believed to have potential to aid human experts in many domains, including
healthcare. There is, however, little work on ChatGPT&apos;s ability to perform a
key task in healthcare: formal, probabilistic medical diagnostic reasoning.
This type of reasoning is used, for example, to update a pre-test probability
to a post-test probability. In this work, we probe ChatGPT&apos;s ability to perform
this task. In particular, we ask ChatGPT to give examples of how to use Bayes
rule for medical diagnosis. Our prompts range from queries that use terminology
from pure probability (e.g., requests for a posterior of A given B and C) to
queries that use terminology from medical diagnosis (e.g., requests for a
posterior probability of Covid given a test result and cough). We show how the
introduction of medical variable names leads to an increase in the number of
errors that ChatGPT makes. Given our results, we also show how one can use
prompt engineering to facilitate ChatGPT&apos;s partial avoidance of these errors.
We discuss our results in light of recent commentaries on sensitivity and
specificity. We also discuss how our results might inform new research
directions for large language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weisenthal_S/0/1/0/all/0/1&quot;&gt;Samuel J. Weisenthal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12875">
<title>Nav-Q: Quantum Deep Reinforcement Learning for Collision-Free Navigation of Self-Driving Cars. (arXiv:2311.12875v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12875</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of collision-free navigation (CFN) of self-driving cars is an
NP-hard problem usually tackled using Deep Reinforcement Learning (DRL). While
DRL methods have proven to be effective, their implementation requires
substantial computing resources and extended training periods to develop a
robust agent. On the other hand, quantum reinforcement learning has recently
demonstrated faster convergence and improved stability in simple,
non-real-world environments.
&lt;/p&gt;
&lt;p&gt;In this work, we propose Nav-Q, the first quantum-supported DRL algorithm for
CFN of self-driving cars, that leverages quantum computation for improving the
training performance without the requirement for onboard quantum hardware.
Nav-Q is based on the actor-critic approach, where the critic is implemented
using a hybrid quantum-classical algorithm suitable for near-term quantum
devices. We assess the performance of Nav-Q using the CARLA driving simulator,
a de facto standard benchmark for evaluating state-of-the-art DRL methods. Our
empirical evaluations showcase that Nav-Q surpasses its classical counterpart
in terms of training stability and, in certain instances, with respect to the
convergence rate. Furthermore, we assess Nav-Q in relation to effective
dimension, unveiling that the incorporation of a quantum component results in a
model with greater descriptive power compared to classical baselines. Finally,
we evaluate the performance of Nav-Q using noisy quantum simulation, observing
that the quantum noise deteriorates the training performances but enhances the
exploratory tendencies of the agent during training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Sinha_A/0/1/0/all/0/1&quot;&gt;Akash Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Macaluso_A/0/1/0/all/0/1&quot;&gt;Antonio Macaluso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Klusch_M/0/1/0/all/0/1&quot;&gt;Matthias Klusch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13892">
<title>General Phrase Debiaser: Debiasing Masked Language Models at a Multi-Token Level. (arXiv:2311.13892v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13892</link>
<description rdf:parseType="Literal">&lt;p&gt;The social biases and unwelcome stereotypes revealed by pretrained language
models are becoming obstacles to their application. Compared to numerous
debiasing methods targeting word level, there has been relatively less
attention on biases present at phrase level, limiting the performance of
debiasing in discipline domains. In this paper, we propose an automatic
multi-token debiasing pipeline called \textbf{General Phrase Debiaser}, which
is capable of mitigating phrase-level biases in masked language models.
Specifically, our method consists of a \textit{phrase filter stage} that
generates stereotypical phrases from Wikipedia pages as well as a \textit{model
debias stage} that can debias models at the multi-token level to tackle bias
challenges on phrases. The latter searches for prompts that trigger model&apos;s
bias, and then uses them for debiasing. State-of-the-art results on standard
datasets and metrics show that our approach can significantly reduce gender
biases on both career and multiple disciplines, across models with varying
parameter sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bingkang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1&quot;&gt;Dehan Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yulei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zongzhen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1&quot;&gt;Honglei Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Longtao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16918">
<title>RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness in Text-to-3D. (arXiv:2311.16918v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16918</link>
<description rdf:parseType="Literal">&lt;p&gt;Lifting 2D diffusion for 3D generation is a challenging problem due to the
lack of geometric prior and the complex entanglement of materials and lighting
in natural images. Existing methods have shown promise by first creating the
geometry through score-distillation sampling (SDS) applied to rendered surface
normals, followed by appearance modeling. However, relying on a 2D RGB
diffusion model to optimize surface normals is suboptimal due to the
distribution discrepancy between natural images and normals maps, leading to
instability in optimization. In this paper, recognizing that the normal and
depth information effectively describe scene geometry and be automatically
estimated from images, we propose to learn a generalizable Normal-Depth
diffusion model for 3D generation. We achieve this by training on the
large-scale LAION dataset together with the generalizable image-to-depth and
normal prior models. In an attempt to alleviate the mixed illumination effects
in the generated materials, we introduce an albedo diffusion model to impose
data-driven constraints on the albedo component. Our experiments show that when
integrated into existing text-to-3D pipelines, our models significantly enhance
the detail richness, achieving state-of-the-art results. Our project page is
https://aigc3d.github.io/richdreamer/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Lingteng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guanying Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xiaodong Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_Q/0/1/0/all/0/1&quot;&gt;Qi Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mutian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yushuang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Weihao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zilong Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1&quot;&gt;Liefeng Bo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiaoguang Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17842">
<title>Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning. (arXiv:2311.17842v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17842</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we are interested in imbuing robots with the capability of
physically-grounded task planning. Recent advancements have shown that large
language models (LLMs) possess extensive knowledge useful in robotic tasks,
especially in reasoning and planning. However, LLMs are constrained by their
lack of world grounding and dependence on external affordance models to
perceive environmental information, which cannot jointly reason with LLMs. We
argue that a task planner should be an inherently grounded, unified multimodal
system. To this end, we introduce Robotic Vision-Language Planning (ViLa), a
novel approach for long-horizon robotic planning that leverages vision-language
models (VLMs) to generate a sequence of actionable steps. ViLa directly
integrates perceptual data into its reasoning and planning process, enabling a
profound understanding of commonsense knowledge in the visual world, including
spatial layouts and object attributes. It also supports flexible multimodal
goal specification and naturally incorporates visual feedback. Our extensive
evaluation, conducted in both real-robot and simulated environments,
demonstrates ViLa&apos;s superiority over existing LLM-based planners, highlighting
its effectiveness in a wide array of open-world manipulation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yingdong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1&quot;&gt;Fanqi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1&quot;&gt;Li Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01339">
<title>ArabIcros: AI-Powered Arabic Crossword Puzzle Generation for Educational Applications. (arXiv:2312.01339v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01339</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents the first Arabic crossword puzzle generator driven by
advanced AI technology. Leveraging cutting-edge large language models including
GPT4, GPT3-Davinci, GPT3-Curie, GPT3-Babbage, GPT3-Ada, and BERT, the system
generates distinctive and challenging clues. Based on a dataset comprising over
50,000 clue-answer pairs, the generator employs fine-tuning, few/zero-shot
learning strategies, and rigorous quality-checking protocols to enforce the
generation of high-quality clue-answer pairs. Importantly, educational
crosswords contribute to enhancing memory, expanding vocabulary, and promoting
problem-solving skills, thereby augmenting the learning experience through a
fun and engaging approach, reshaping the landscape of traditional learning
methods. The overall system can be exploited as a powerful educational tool
that amalgamates AI and innovative learning techniques, heralding a
transformative era for Arabic crossword puzzles and the intersection of
technology and education.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeinalipour_K/0/1/0/all/0/1&quot;&gt;Kamyar Zeinalipour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saad_M/0/1/0/all/0/1&quot;&gt;Mohamed Zaky Saad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggini_M/0/1/0/all/0/1&quot;&gt;Marco Maggini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1&quot;&gt;Marco Gori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01678">
<title>Jellyfish: A Large Language Model for Data Preprocessing. (arXiv:2312.01678v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01678</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present Jellyfish, an open-source LLM as a universal task
solver for DP. Built on the Llama 2 13B model, Jellyfish is instruction-tuned
with the datasets of several typical DP tasks including error detection, data
imputation, schema matching, and entity matching, and delivers generalizability
to other tasks. Remarkably, Jellyfish can operate on a local, single, and
low-priced GPU with its 13 billion parameters, ensuring data security and
enabling further tuning. Its proficiency in understanding natural language
allows users to manually craft instructions for DP tasks. Unlike many existing
methods that heavily rely on prior knowledge, Jellyfish acquires domain
knowledge during its tuning process and integrates optional knowledge injection
during inference. A distinctive feature of Jellyfish is its interpreter, which
elucidates its output decisions. To construct Jellyfish, we develop a series of
pre-tuning and DP-tuning techniques. Jellyfish is equipped with an instance
serializer, which automatically translates raw data into model prompts, and a
knowledge injector, which optionally introduces task- and dataset-specific
knowledge to enhance DP performance. Our evaluation of Jellyfish, using a range
of real datasets, shows its competitiveness compared to state-of-the-art
methods and its strong generalizability to unseen tasks. Jellyfish&apos;s
performance rivals that of GPT series models, and its interpreter offers
enhanced reasoning capabilities compared to GPT-3.5. Furthermore, our
evaluation highlights the effectiveness of the techniques employed in
constructing Jellyfish. Our model is available at Hugging Face:
https://huggingface.co/NECOUDBFM/Jellyfish .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haochen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yuyang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chuan Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oyamada_M/0/1/0/all/0/1&quot;&gt;Masafumi Oyamada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01700">
<title>Data Management For Large Language Models: A Survey. (arXiv:2312.01700v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01700</link>
<description rdf:parseType="Literal">&lt;p&gt;Data plays a fundamental role in the training of Large Language Models
(LLMs). Effective data management, particularly in the formulation of a
well-suited training dataset, holds significance for enhancing model
performance and improving training efficiency during pretraining and supervised
fine-tuning phases. Despite the considerable importance of data management, the
current research community still falls short in providing a systematic analysis
of the rationale behind management strategy selection, its consequential
effects, methodologies for evaluating curated datasets, and the ongoing pursuit
of improved strategies. Consequently, the exploration of data management has
attracted more and more attention among the research community. This survey
provides a comprehensive overview of current research in data management within
both the pretraining and supervised fine-tuning stages of LLMs, covering
various noteworthy aspects of data management strategy design: data quantity,
data quality, domain/task composition, etc. Looking toward the future, we
extrapolate existing challenges and outline promising directions for
development in this field. Therefore, this survey serves as a guiding resource
for practitioners aspiring to construct powerful LLMs through effective data
management practices. The collection of the latest papers is available at
https://github.com/ZigeW/data_management_LLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zige Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1&quot;&gt;Wanjun Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yufei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1&quot;&gt;Fei Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Baojun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1&quot;&gt;Lifeng Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qun Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02317">
<title>GNN2R: Weakly-Supervised Rationale-Providing Question Answering over Knowledge Graphs. (arXiv:2312.02317v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02317</link>
<description rdf:parseType="Literal">&lt;p&gt;Most current methods for multi-hop question answering (QA) over knowledge
graphs (KGs) only provide final conclusive answers without explanations, such
as a set of KG entities that is difficult for normal users to review and
comprehend. This issue severely limits the application of KG-based QA in
real-world scenarios. However, it is non-trivial to solve due to two
challenges: First, annotations of reasoning chains of multi-hop questions,
which could serve as supervision for explanation generation, are usually
lacking. Second, it is difficult to maintain high efficiency when explicit KG
triples need to be retrieved to generate explanations. In this paper, we
propose a novel Graph Neural Network-based Two-Step Reasoning model (GNN2R) to
solve this issue. GNN2R can provide both final answers and reasoning subgraphs
as a rationale behind final answers efficiently with only weak supervision that
is available through question-final answer pairs. We extensively evaluated
GNN2R with detailed analyses in experiments. The results demonstrate that, in
terms of effectiveness, efficiency, and quality of generated explanations,
GNN2R outperforms existing state-of-the-art methods that are applicable to this
task. Our code and pre-trained models are available at
https://github.com/ruijie-wang-uzh/GNN2R.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossetto_L/0/1/0/all/0/1&quot;&gt;Luca Rossetto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cochez_M/0/1/0/all/0/1&quot;&gt;Michael Cochez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernstein_A/0/1/0/all/0/1&quot;&gt;Abraham Bernstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02646">
<title>SAMSGL: Series-Aligned Multi-Scale Graph Learning for Spatio-Temporal Forecasting. (arXiv:2312.02646v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02646</link>
<description rdf:parseType="Literal">&lt;p&gt;Spatio-temporal forecasting in various domains, like traffic prediction and
weather forecasting, is a challenging endeavor, primarily due to the
difficulties in modeling propagation dynamics and capturing high-dimensional
interactions among nodes. Despite the significant strides made by graph-based
networks in spatio-temporal forecasting, there remain two pivotal factors
closely related to forecasting performance that need further consideration:
time delays in propagation dynamics and multi-scale high-dimensional
interactions. In this work, we present a Series-Aligned Multi-Scale Graph
Learning (SAMSGL) framework, aiming to enhance forecasting performance. In
order to handle time delays in spatial interactions, we propose a
series-aligned graph convolution layer to facilitate the aggregation of
non-delayed graph signals, thereby mitigating the influence of time delays for
the improvement in accuracy. To understand global and local spatio-temporal
interactions, we develop a spatio-temporal architecture via multi-scale graph
learning, which encompasses two essential components: multi-scale graph
structure learning and graph-fully connected (Graph-FC) blocks. The multi-scale
graph structure learning includes a global graph structure to learn both
delayed and non-delayed node embeddings, as well as a local one to learn node
variations influenced by neighboring factors. The Graph-FC blocks
synergistically fuse spatial and temporal information to boost prediction
accuracy. To evaluate the performance of SAMSGL, we conduct experiments on
meteorological and traffic forecasting datasets, which demonstrate its
effectiveness and superiority.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xiaobei Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1&quot;&gt;Luolin Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurths_J/0/1/0/all/0/1&quot;&gt;Jurgen Kurths&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03863">
<title>Efficient Large Language Models: A Survey. (arXiv:2312.03863v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03863</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have demonstrated remarkable capabilities in
important tasks such as natural language understanding, language generation,
and complex reasoning and have the potential to make a substantial impact on
our society. Such capabilities, however, come with the considerable resources
they demand, highlighting the strong need to develop effective techniques for
addressing their efficiency challenges. In this survey, we provide a systematic
and comprehensive review of efficient LLMs research. We organize the literature
in a taxonomy consisting of three main categories, covering distinct yet
interconnected efficient LLMs topics from model-centric, data-centric, and
framework-centric perspective, respectively. We have also created a GitHub
repository where we compile the papers featured in this survey at
https://github.com/AIoT-MLSys-Lab/EfficientLLMs, and will actively maintain
this repository and incorporate new research as it emerges. We hope our survey
can serve as a valuable resource to help researchers and practitioners gain a
systematic understanding of the research developments in efficient LLMs and
inspire them to contribute to this important and exciting field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1&quot;&gt;Zhongwei Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Che Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1&quot;&gt;Samiul Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiachen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_Z/0/1/0/all/0/1&quot;&gt;Zhongnan Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shen Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Quanlu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1&quot;&gt;Mosharaf Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04025">
<title>Moirai: Towards Optimal Placement for Distributed Inference on Heterogeneous Devices. (arXiv:2312.04025v3 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04025</link>
<description rdf:parseType="Literal">&lt;p&gt;The escalating size of Deep Neural Networks (DNNs) has spurred a growing
research interest in hosting and serving DNN models across multiple devices. A
number of studies have been reported to partition a DNN model across devices,
providing device placement solutions. The methods appeared in the literature,
however, either suffer from poor placement performance due to the exponential
search space or miss an optimal placement as a consequence of the reduced
search space with limited heuristics. Moreover, these methods have ignored the
runtime inter-operator optimization of a computation graph when coarsening the
graph, which degrades the end-to-end inference performance. This paper presents
Moirai that better exploits runtime inter-operator fusion in a model to render
a coarsened computation graph, reducing the search space while maintaining the
inter-operator optimization provided by inference backends. Moirai also
generalizes the device placement algorithm from multiple perspectives by
considering inference constraints and device heterogeneity.Extensive
experimental evaluation with 11 large DNNs demonstrates that Moirai outperforms
the state-of-the-art counterparts, i.e., Placeto, m-SCT, and GETF, up to
4.28$\times$ in reduction of the end-to-end inference latency. Moirai code is
anonymously released at \url{https://github.com/moirai-placement/moirai}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Beibei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hongwei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Feng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhihui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sean Xiaoyang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06718">
<title>Large Scale Foundation Models for Intelligent Manufacturing Applications: A Survey. (arXiv:2312.06718v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06718</link>
<description rdf:parseType="Literal">&lt;p&gt;Although the applications of artificial intelligence especially deep learning
had greatly improved various aspects of intelligent manufacturing, they still
face challenges for wide employment due to the poor generalization ability,
difficulties to establish high-quality training datasets, and unsatisfactory
performance of deep learning methods. The emergence of large scale foundational
models(LSFMs) had triggered a wave in the field of artificial intelligence,
shifting deep learning models from single-task, single-modal, limited data
patterns to a paradigm encompassing diverse tasks, multimodal, and pre-training
on massive datasets. Although LSFMs had demonstrated powerful generalization
capabilities, automatic high-quality training dataset generation and superior
performance across various domains, applications of LSFMs on intelligent
manufacturing were still in their nascent stage. A systematic overview of this
topic was lacking, especially regarding which challenges of deep learning can
be addressed by LSFMs and how these challenges can be systematically tackled.
To fill this gap, this paper systematically expounded current statue of LSFMs
and their advantages in the context of intelligent manufacturing. and compared
comprehensively with the challenges faced by current deep learning models in
various intelligent manufacturing applications. We also outlined the roadmaps
for utilizing LSFMs to address these challenges. Finally, case studies of
applications of LSFMs in real-world intelligent manufacturing scenarios were
presented to illustrate how LSFMs could help industries, improve their
efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haotian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dereck_S/0/1/0/all/0/1&quot;&gt;Semujju Stuart Dereck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1&quot;&gt;Xianwei Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Liang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Ye Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jing Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_Z/0/1/0/all/0/1&quot;&gt;Zhuo Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Wensheng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;X.G. Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_R/0/1/0/all/0/1&quot;&gt;Ruiyan Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07401">
<title>On Diversified Preferences of Large Language Model Alignment. (arXiv:2312.07401v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07401</link>
<description rdf:parseType="Literal">&lt;p&gt;Aligning large language models (LLMs) with human preferences has been
recognized as the key to improving LLMs&apos; interaction quality. However, in this
pluralistic world, human preferences can be diversified by people&apos;s different
tastes, which hinders the effectiveness of LLM alignment methods. In this
paper, we provide the first quantitative analysis to verify the existence of
diversified preferences in commonly used human feedback datasets. To mitigate
the alignment ineffectiveness caused by diversified preferences, we propose a
novel \textbf{M}ulti-\textbf{O}bjective \textbf{Re}ward learning method (MORE),
which can automatically adjust the learning gradients across different
preference data sources. In experiments, we evaluate MORE with the Pythia-1.4B
model on five mixed human preference datasets, on which our method achieves
superior performance compared with other baselines in terms of preference
accuracy and prediction calibration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1&quot;&gt;Dun Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yong Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1&quot;&gt;Pengyu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1&quot;&gt;Tianhao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wanshun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1&quot;&gt;Nan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zenglin Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07625">
<title>Astrocyte-Enabled Advancements in Spiking Neural Networks for Large Language Modeling. (arXiv:2312.07625v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07625</link>
<description rdf:parseType="Literal">&lt;p&gt;Within the complex neuroarchitecture of the brain, astrocytes play crucial
roles in development, structure, and metabolism. These cells regulate neural
activity through tripartite synapses, directly impacting cognitive processes
such as learning and memory. Despite the growing recognition of astrocytes&apos;
significance, traditional Spiking Neural Network (SNN) models remain
predominantly neuron-centric, overlooking the profound influence of astrocytes
on neural dynamics. Inspired by these biological insights, we have developed an
Astrocyte-Modulated Spiking Unit (AM-SU), an innovative framework that
integrates neuron-astrocyte interactions into the computational paradigm,
demonstrating wide applicability across various hardware platforms. Our
Astrocyte-Modulated Spiking Neural Network (AstroSNN) exhibits exceptional
performance in tasks involving memory retention and natural language
generation, particularly in handling long-term dependencies and complex
linguistic structures. The design of AstroSNN not only enhances its biological
authenticity but also introduces novel computational dynamics, enabling more
effective processing of complex temporal dependencies. Furthermore, AstroSNN
shows low latency, high throughput, and reduced memory usage in practical
applications, making it highly suitable for resource-constrained environments.
By successfully integrating astrocytic dynamics into intelligent neural
networks, our work narrows the gap between biological plausibility and neural
modeling, laying the groundwork for future biologically-inspired neural
computing research that includes both neurons and astrocytes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_G/0/1/0/all/0/1&quot;&gt;Guobin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Dongcheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yiting Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jindong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1&quot;&gt;Kang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yi Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08550">
<title>Harmonics of Learning: Universal Fourier Features Emerge in Invariant Networks. (arXiv:2312.08550v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08550</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we formally prove that, under certain conditions, if a neural
network is invariant to a finite group then its weights recover the Fourier
transform on that group. This provides a mathematical explanation for the
emergence of Fourier features -- a ubiquitous phenomenon in both biological and
artificial learning systems. The results hold even for non-commutative groups,
in which case the Fourier transform encodes all the irreducible unitary group
representations. Our findings have consequences for the problem of symmetry
discovery. Specifically, we demonstrate that the algebraic structure of an
unknown group can be recovered from the weights of a network that is at least
approximately invariant within certain bounds. Overall, this work contributes
to a foundation for an algebraic learning theory of invariant neural network
representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchetti_G/0/1/0/all/0/1&quot;&gt;Giovanni Luca Marchetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hillar_C/0/1/0/all/0/1&quot;&gt;Christopher Hillar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kragic_D/0/1/0/all/0/1&quot;&gt;Danica Kragic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanborn_S/0/1/0/all/0/1&quot;&gt;Sophia Sanborn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08642">
<title>Metacognition-Enhanced Few-Shot Prompting With Positive Reinforcement. (arXiv:2312.08642v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08642</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot prompting elicits the remarkable abilities of large language models
by equipping them with a few demonstration examples in the input. However, the
traditional method of providing large language models with all demonstration
input-output pairs at once may not effectively guide large language models to
learn the specific input-output mapping relationship. In this paper, inspired
by the regulatory and supportive role of metacognition in students&apos; learning,
we propose a novel metacognition-enhanced few-shot prompting, which guides
large language models to reflect on their thought processes to comprehensively
learn the given demonstration examples. Furthermore, considering that positive
reinforcement can improve students&apos; learning motivation, we introduce positive
reinforcement into our metacognition-enhanced few-shot prompting to promote the
few-shot learning of large language models by providing response-based positive
feedback. The experimental results on two real-world datasets show that our
metacognition-enhanced few-shot prompting with positive reinforcement surpasses
traditional few-shot prompting in classification accuracy and macro F1.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1&quot;&gt;Yu Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Liang He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08901">
<title>Boosting LLM Reasoning: Push the Limits of Few-shot Learning with Reinforced In-Context Pruning. (arXiv:2312.08901v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08901</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have shown impressive capabilities in various
tasks, yet they still struggle with math reasoning. Despite efforts to optimize
Chain-of-Thoughts (CoT) prompts and fine-tune LLMs, the potential of few-shot
learning remains unexplored. In this work, we propose CoT-Influx, a novel
approach pushing the boundaries of few-shot CoT learning to improve LLM math
reasoning capabilities. CoT-Influx addresses the challenges of the selection of
useful examples and limited number of examples due to restricted context window
length. Inspired by our observation that natural language inputs contain many
redundancy, we propose a coarse-to-fine pruner as a plug-and-play module for
LLMs, which first identifies as many crucial CoT examples as possible and then
further prunes unimportant tokens within the context window. To train the
pruner, we collect a math reasoning dataset with diverse difficulty and steps,
introduce a reward to measure both the input&apos;s effectiveness for math reasoning
and token length constraints, and propose a novel training approach with
reinforcement learning. As a result, CoT-Influx significantly outperforms CoT
and few-shot prompting baselines across various LLMs (LLaMA2-7B, 13B, 70B) and
5 mathematical datasets, achieving up to 4.55% absolute improvements.
Remarkably, without any fine-tuning, LLaMA2-70B with CoT-Influx surpasses
GPT-3.5 and a wide range of larger LLMs (PaLM, Minerva, etc.) on the GSM8K.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xijie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Lyna Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1&quot;&gt;Kwang-Ting Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09781">
<title>GSQA: An End-to-End Model for Generative Spoken Question Answering. (arXiv:2312.09781v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09781</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent advancements in spoken question answering (QA), end-to-end models
have made significant strides. However, previous research has primarily focused
on extractive span selection. While this extractive-based approach is effective
when answers are present directly within the input, it falls short in
addressing abstractive questions, where answers are not directly extracted but
inferred from the given information. To bridge this gap, we introduce the first
end-to-end Generative Spoken Question Answering (GSQA) model that empowers the
system to engage in abstractive reasoning. The challenge in training our GSQA
model lies in the absence of a spoken abstractive QA dataset. We propose using
text models for initialization and leveraging the extractive QA dataset to
transfer knowledge from the text generative model to the spoken generative
model. Experimental results indicate that our model surpasses the previous
extractive model by 3% on extractive QA datasets. Furthermore, the GSQA model
has only been fine-tuned on the spoken extractive QA dataset. Despite not
having seen any spoken abstractive QA data, it can still closely match the
performance of the cascade model. In conclusion, our GSQA model shows the
potential to generalize to a broad spectrum of questions, thus further
expanding the spoken question answering capabilities of abstractive QA. Our
code is available at https://voidful.github.io/GSQA
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shih_M/0/1/0/all/0/1&quot;&gt;Min-Han Shih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1&quot;&gt;Ho-Lam Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pai_Y/0/1/0/all/0/1&quot;&gt;Yu-Chi Pai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_M/0/1/0/all/0/1&quot;&gt;Ming-Hao Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guan-Ting Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shang-Wen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hung-yi Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10385">
<title>Imitate the Good and Avoid the Bad: An Incremental Approach to Safe Reinforcement Learning. (arXiv:2312.10385v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10385</link>
<description rdf:parseType="Literal">&lt;p&gt;A popular framework for enforcing safe actions in Reinforcement Learning (RL)
is Constrained RL, where trajectory based constraints on expected cost (or
other cost measures) are employed to enforce safety and more importantly these
constraints are enforced while maximizing expected reward. Most recent
approaches for solving Constrained RL convert the trajectory based cost
constraint into a surrogate problem that can be solved using minor
modifications to RL methods. A key drawback with such approaches is an over or
underestimation of the cost constraint at each state. Therefore, we provide an
approach that does not modify the trajectory based cost constraint and instead
imitates ``good&apos;&apos; trajectories and avoids ``bad&apos;&apos; trajectories generated from
incrementally improving policies. We employ an oracle that utilizes a reward
threshold (which is varied with learning) and the overall cost constraint to
label trajectories as ``good&apos;&apos; or ``bad&apos;&apos;. A key advantage of our approach is
that we are able to work from any starting policy or set of trajectories and
improve on it. In an exhaustive set of experiments, we demonstrate that our
approach is able to outperform top benchmark approaches for solving Constrained
RL problems, with respect to expected cost, CVaR cost, or even unknown cost
constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_H/0/1/0/all/0/1&quot;&gt;Huy Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1&quot;&gt;Tien Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varakantham_P/0/1/0/all/0/1&quot;&gt;Pradeep Varakantham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10407">
<title>DeepArt: A Benchmark to Advance Fidelity Research in AI-Generated Content. (arXiv:2312.10407v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10407</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the image synthesis capabilities of GPT-4, a leading
multi-modal large language model. We establish a benchmark for evaluating the
fidelity of texture features in images generated by GPT-4, comprising manually
painted pictures and their AI-generated counterparts. The contributions of this
study are threefold: First, we provide an in-depth analysis of the fidelity of
image synthesis features based on GPT-4, marking the first such study on this
state-of-the-art model. Second, the quantitative and qualitative experiments
fully reveals the limitations of the GPT-4 model in image synthesis. Third, we
have compiled a unique benchmark of manual drawings and corresponding
GPT-4-generated images, introducing a new task to advance fidelity research in
AI-generated content (AIGC). The dataset is available at:
\url{https://github.com/rickwang28574/DeepArt}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wentao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanyao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Swalpa Kumar Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11152">
<title>Prompt Based Tri-Channel Graph Convolution Neural Network for Aspect Sentiment Triplet Extraction. (arXiv:2312.11152v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11152</link>
<description rdf:parseType="Literal">&lt;p&gt;Aspect Sentiment Triplet Extraction (ASTE) is an emerging task to extract a
given sentence&apos;s triplets, which consist of aspects, opinions, and sentiments.
Recent studies tend to address this task with a table-filling paradigm, wherein
word relations are encoded in a two-dimensional table, and the process involves
clarifying all the individual cells to extract triples. However, these studies
ignore the deep interaction between neighbor cells, which we find quite helpful
for accurate extraction. To this end, we propose a novel model for the ASTE
task, called Prompt-based Tri-Channel Graph Convolution Neural Network
(PT-GCN), which converts the relation table into a graph to explore more
comprehensive relational information. Specifically, we treat the original table
cells as nodes and utilize a prompt attention score computation module to
determine the edges&apos; weights. This enables us to construct a target-aware
grid-like graph to enhance the overall extraction process. After that, a
triple-channel convolution module is conducted to extract precise sentiment
knowledge. Extensive experiments on the benchmark datasets show that our model
achieves state-of-the-art performance. The code is available at
https://github.com/KunPunCN/PT-GCN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1&quot;&gt;Kun Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Lei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Hao Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Rui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhengtao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jiaqian Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S.Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11193">
<title>&quot;Paraphrasing The Original Text&quot; Makes High Accuracy Long-Context QA. (arXiv:2312.11193v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11193</link>
<description rdf:parseType="Literal">&lt;p&gt;Although LLMs continue to iterate and improve, most open-source models still
have a context window of no more than 4k, limiting their ability to handle
long-context problems. Most existing open-source models for long-context chat
still lack satisfactory accuracy. To address this issue, I approach it from the
perspective of training data and theoretically prove that training the
capability to handle long contexts requires &quot;effective&quot; rather than &quot;long&quot;
data. Based on this, I propose using the &quot;original text paraphrase&quot; task, and
successfully extend the context window of the existing model to 32k by a
low-cost and effective method, achieving extremely high accuracy in
multi-document-QA and surpassing all existing open-source models of the same
scale. The model and training data have been open-sourced on
HuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) and
WiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yijiong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11444">
<title>An In-depth Look at Gemini&apos;s Language Abilities. (arXiv:2312.11444v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11444</link>
<description rdf:parseType="Literal">&lt;p&gt;The recently released Google Gemini class of models are the first to
comprehensively report results that rival the OpenAI GPT series across a wide
variety of tasks. In this paper, we do an in-depth exploration of Gemini&apos;s
language abilities, making two contributions. First, we provide a third-party,
objective comparison of the abilities of the OpenAI GPT and Google Gemini
models with reproducible code and fully transparent results. Second, we take a
closer look at the results, identifying areas where one of the two model
classes excels. We perform this analysis over 10 datasets testing a variety of
language abilities, including reasoning, answering knowledge-based questions,
solving math problems, translating between languages, generating code, and
acting as instruction-following agents. From this analysis, we find that Gemini
Pro achieves accuracy that is close but slightly inferior to the corresponding
GPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations
for some of this under-performance, including failures in mathematical
reasoning with many digits, sensitivity to multiple-choice answer ordering,
aggressive content filtering, and others. We also identify areas where Gemini
demonstrates comparably high performance, including generation into non-English
languages, and handling longer and more complex reasoning chains. Code and data
for reproduction can be found at https://github.com/neulab/gemini-benchmark
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akter_S/0/1/0/all/0/1&quot;&gt;Syeda Nahida Akter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zichun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muhamed_A/0/1/0/all/0/1&quot;&gt;Aashiq Muhamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ou_T/0/1/0/all/0/1&quot;&gt;Tianyue Ou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauerle_A/0/1/0/all/0/1&quot;&gt;Alex B&amp;#xe4;uerle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cabrera_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;ngel Alexander Cabrera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dholakia_K/0/1/0/all/0/1&quot;&gt;Krish Dholakia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Chenyan Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1&quot;&gt;Graham Neubig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11518">
<title>User Modeling in the Era of Large Language Models: Current Research and Future Directions. (arXiv:2312.11518v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11518</link>
<description rdf:parseType="Literal">&lt;p&gt;User modeling (UM) aims to discover patterns or learn representations from
user data about the characteristics of a specific user, such as profile,
preference, and personality. The user models enable personalization and
suspiciousness detection in many online applications such as recommendation,
education, and healthcare. Two common types of user data are text and graph, as
the data usually contain a large amount of user-generated content (UGC) and
online interactions. The research of text and graph mining is developing
rapidly, contributing many notable solutions in the past two decades. Recently,
large language models (LLMs) have shown superior performance on generating,
understanding, and even reasoning over text data. The approaches of user
modeling have been equipped with LLMs and soon become outstanding. This article
summarizes existing research about how and why LLMs are great tools of modeling
and understanding UGC. Then it reviews a few categories of large language
models for user modeling (LLM-UM) approaches that integrate the LLMs with text
and graph-based methods in different ways. Then it introduces specific LLM-UM
techniques for a variety of UM applications. Finally, it presents remaining
challenges and future directions in the LLM-UM research. We maintain the
reading list at: https://github.com/TamSiuhin/LLM-UM-Reading
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zhaoxuan Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Meng Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11562">
<title>A Survey of Reasoning with Foundation Models. (arXiv:2312.11562v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11562</link>
<description rdf:parseType="Literal">&lt;p&gt;Reasoning, a crucial ability for complex problem-solving, plays a pivotal
role in various real-world settings such as negotiation, medical diagnosis, and
criminal investigation. It serves as a fundamental methodology in the field of
Artificial General Intelligence (AGI). With the ongoing development of
foundation models, there is a growing interest in exploring their abilities in
reasoning tasks. In this paper, we introduce seminal foundation models proposed
or adaptable for reasoning, highlighting the latest advancements in various
reasoning tasks, methods, and benchmarks. We then delve into the potential
future directions behind the emergence of reasoning abilities within foundation
models. We also discuss the relevance of multimodal learning, autonomous
agents, and super alignment in the context of reasoning. By discussing these
future research directions, we hope to inspire researchers in their exploration
of this field, stimulate further advancements in reasoning with foundation
models, and contribute to the development of AGI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiankai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chuanyang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1&quot;&gt;Ruihang Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jianing Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Mingyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_M/0/1/0/all/0/1&quot;&gt;Mengzhe Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhangyue Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiaozhe Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junxian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Wu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xihui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1&quot;&gt;Pheng Ann Heng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingdong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yike Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11571">
<title>Model Stealing Attack against Recommender System. (arXiv:2312.11571v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11571</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies have demonstrated the vulnerability of recommender systems to
data privacy attacks. However, research on the threat to model privacy in
recommender systems, such as model stealing attacks, is still in its infancy.
Some adversarial attacks have achieved model stealing attacks against
recommender systems, to some extent, by collecting abundant training data of
the target model (target data) or making a mass of queries. In this paper, we
constrain the volume of available target data and queries and utilize auxiliary
data, which shares the item set with the target data, to promote model stealing
attacks. Although the target model treats target and auxiliary data
differently, their similar behavior patterns allow them to be fused using an
attention mechanism to assist attacks. Besides, we design stealing functions to
effectively extract the recommendation list obtained by querying the target
model. Experimental results show that the proposed methods are applicable to
most recommender systems and various scenarios and exhibit excellent attack
performance on multiple datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhihao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1&quot;&gt;Rui Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chenwang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1&quot;&gt;Defu Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1&quot;&gt;Enhong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11583">
<title>AI-Based Energy Transportation Safety: Pipeline Radial Threat Estimation Using Intelligent Sensing System. (arXiv:2312.11583v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11583</link>
<description rdf:parseType="Literal">&lt;p&gt;The application of artificial intelligence technology has greatly enhanced
and fortified the safety of energy pipelines, particularly in safeguarding
against external threats. The predominant methods involve the integration of
intelligent sensors to detect external vibration, enabling the identification
of event types and locations, thereby replacing manual detection methods.
However, practical implementation has exposed a limitation in current methods -
their constrained ability to accurately discern the spatial dimensions of
external signals, which complicates the authentication of threat events. Our
research endeavors to overcome the above issues by harnessing deep learning
techniques to achieve a more fine-grained recognition and localization process.
This refinement is crucial in effectively identifying genuine threats to
pipelines, thus enhancing the safety of energy transportation. This paper
proposes a radial threat estimation method for energy pipelines based on
distributed optical fiber sensing technology. Specifically, we introduce a
continuous multi-view and multi-domain feature fusion methodology to extract
comprehensive signal features and construct a threat estimation and recognition
network. The utilization of collected acoustic signal data is optimized, and
the underlying principle is elucidated. Moreover, we incorporate the concept of
transfer learning through a pre-trained model, enhancing both recognition
accuracy and training efficiency. Empirical evidence gathered from real-world
scenarios underscores the efficacy of our method, notably in its substantial
reduction of false alarms and remarkable gains in recognition accuracy. More
generally, our method exhibits versatility and can be extrapolated to a broader
spectrum of recognition tasks and scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chengyuan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yiyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kaixiang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haifeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qinmin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;C. L. Philip Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12021">
<title>Synergistic Anchored Contrastive Pre-training for Few-Shot Relation Extraction. (arXiv:2312.12021v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12021</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot Relation Extraction (FSRE) aims to extract relational facts from a
sparse set of labeled corpora. Recent studies have shown promising results in
FSRE by employing Pre-trained Language Models (PLMs) within the framework of
supervised contrastive learning, which considers both instances and label
facts. However, how to effectively harness massive instance-label pairs to
encompass the learned representation with semantic richness in this learning
paradigm is not fully explored. To address this gap, we introduce a novel
synergistic anchored contrastive pre-training framework. This framework is
motivated by the insight that the diverse viewpoints conveyed through
instance-label pairs capture incomplete yet complementary intrinsic textual
semantics. Specifically, our framework involves a symmetrical contrastive
objective that encompasses both sentence-anchored and label-anchored
contrastive losses. By combining these two losses, the model establishes a
robust and uniform representation space. This space effectively captures the
reciprocal alignment of feature distributions among instances and relational
facts, simultaneously enhancing the maximization of mutual information across
diverse perspectives within the same relation. Experimental results demonstrate
that our framework achieves significant performance enhancements compared to
baseline models in downstream FSRE tasks. Furthermore, our approach exhibits
superior adaptability to handle the challenges of domain shift and zero-shot
relation extraction. Our code is available online at
https://github.com/AONE-NLP/FSRE-SaCon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1&quot;&gt;Da Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1&quot;&gt;Yanglei Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1&quot;&gt;Rui Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1&quot;&gt;Run Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wannian Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13680">
<title>HGE: Embedding Temporal Knowledge Graphs in a Product Space of Heterogeneous Geometric Subspaces. (arXiv:2312.13680v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13680</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal knowledge graphs represent temporal facts $(s,p,o,\tau)$ relating a
subject $s$ and an object $o$ via a relation label $p$ at time $\tau$, where
$\tau$ could be a time point or time interval. Temporal knowledge graphs may
exhibit static temporal patterns at distinct points in time and dynamic
temporal patterns between different timestamps. In order to learn a rich set of
static and dynamic temporal patterns and apply them for inference, several
embedding approaches have been suggested in the literature. However, as most of
them resort to single underlying embedding spaces, their capability to model
all kinds of temporal patterns was severely limited by having to adhere to the
geometric property of their one embedding space. We lift this limitation by an
embedding approach that maps temporal facts into a product space of several
heterogeneous geometric subspaces with distinct geometric properties, i.e.\
Complex, Dual, and Split-complex spaces. In addition, we propose a
temporal-geometric attention mechanism to integrate information from different
geometric subspaces conveniently according to the captured relational and
temporal information. Experimental results on standard temporal benchmark
datasets favorably evaluate our approach against state-of-the-art models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jiaxin Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nayyeri_M/0/1/0/all/0/1&quot;&gt;Mojtaba Nayyeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yinan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Staab_S/0/1/0/all/0/1&quot;&gt;Steffen Staab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13772">
<title>On Task Performance and Model Calibration with Supervised and Self-Ensembled In-Context Learning. (arXiv:2312.13772v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13772</link>
<description rdf:parseType="Literal">&lt;p&gt;Following the standard supervised fine-tuning (SFT) paradigm, in-context
learning (ICL) has become an efficient approach propelled by the recent
advancements in large language models (LLMs), yielding promising performance
across various tasks in few-shot data setups. However, both paradigms are prone
to suffer from the critical problem of overconfidence (i.e., miscalibration),
especially in such limited data setups. In this work, we deliver an in-depth
analysis of the behavior across different choices of learning methods from the
perspective of both performance and calibration, as well as their interplay.
Through extensive controlled experiments, we find that simultaneous gains for
both task performance and calibration are difficult to achieve, and the problem
of miscalibration exists across all learning methods in low-resource scenarios.
To address this challenging trade-off between performance and calibration, we
then investigate the potential of self-ensembling techniques applied at
different modeling stages (e.g., variations of in-context examples or
variations in prompts or different ensembling strategies). We justify the
feasibility of self-ensembling on SFT in addition to ICL, to make the
predictions more calibrated and have comparable or even better performance. Our
work sheds light on which learning paradigm to choose and how to enhance both
task performance and calibration of LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chengzu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Han Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1&quot;&gt;Goran Glava&amp;#x161;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1&quot;&gt;Anna Korhonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1&quot;&gt;Ivan Vuli&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13923">
<title>Fed-CO2: Cooperation of Online and Offline Models for Severe Data Heterogeneity in Federated Learning. (arXiv:2312.13923v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13923</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) has emerged as a promising distributed learning
paradigm that enables multiple clients to learn a global model collaboratively
without sharing their private data. However, the effectiveness of FL is highly
dependent on the quality of the data that is being used for training. In
particular, data heterogeneity issues, such as label distribution skew and
feature skew, can significantly impact the performance of FL. Previous studies
in FL have primarily focused on addressing label distribution skew data
heterogeneity, while only a few recent works have made initial progress in
tackling feature skew issues. Notably, these two forms of data heterogeneity
have been studied separately and have not been well explored within a unified
FL framework. To address this gap, we propose Fed-CO$_{2}$, a universal FL
framework that handles both label distribution skew and feature skew within a
\textbf{C}ooperation mechanism between the \textbf{O}nline and \textbf{O}ffline
models. Specifically, the online model learns general knowledge that is shared
among all clients, while the offline model is trained locally to learn the
specialized knowledge of each individual client. To further enhance model
cooperation in the presence of feature shifts, we design an intra-client
knowledge transfer mechanism that reinforces mutual learning between the online
and offline models, and an inter-client knowledge transfer mechanism to
increase the models&apos; domain generalization ability. Extensive experiments show
that our Fed-CO$_{2}$ outperforms a wide range of existing personalized
federated learning algorithms in terms of handling label distribution skew and
feature skew, both individually and collectively. The empirical results are
supported by our convergence analyses in a simplified setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zhongyi Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Ye Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingya Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14187">
<title>WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation. (arXiv:2312.14187v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14187</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work demonstrates that, after being fine-tuned on a high-quality
instruction dataset, the resulting model can obtain impressive capabilities to
address a wide range of tasks. However, existing methods for instruction data
generation often produce duplicate data and are not controllable enough on data
quality. In this paper, we extend the generalization of instruction tuning by
classifying the instruction data to 4 code-related tasks and propose a
LLM-based Generator-Discriminator data process framework to generate diverse,
high-quality instruction data from open source code. Hence, we introduce
CodeOcean, a dataset comprising 20,000 instruction instances across 4 universal
code-related tasks,which is aimed at augmenting the effectiveness of
instruction tuning and improving the generalization ability of fine-tuned
model. Subsequently, we present WaveCoder, a fine-tuned Code LLM with
Widespread And Versatile Enhanced instruction tuning. This model is
specifically designed for enhancing instruction tuning of Code Language Models
(LLMs). Our experiments demonstrate that Wavecoder models outperform other
open-source models in terms of generalization ability across different
code-related tasks at the same level of fine-tuning scale. Moreover, Wavecoder
exhibits high efficiency in previous code generation tasks. This paper thus
offers a significant contribution to the field of instruction data generation
and fine-tuning models, providing new insights and tools for enhancing
performance in code-related tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhaojian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_N/0/1/0/all/0/1&quot;&gt;Ning Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yangyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Can Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yishujie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenxiang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1&quot;&gt;Qiufeng Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14222">
<title>Hierarchical Topology Isomorphism Expertise Embedded Graph Contrastive Learning. (arXiv:2312.14222v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14222</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph contrastive learning (GCL) aims to align the positive features while
differentiating the negative features in the latent space by minimizing a
pair-wise contrastive loss. As the embodiment of an outstanding discriminative
unsupervised graph representation learning approach, GCL achieves impressive
successes in various graph benchmarks. However, such an approach falls short of
recognizing the topology isomorphism of graphs, resulting in that graphs with
relatively homogeneous node features cannot be sufficiently discriminated. By
revisiting classic graph topology recognition works, we disclose that the
corresponding expertise intuitively complements GCL methods. To this end, we
propose a novel hierarchical topology isomorphism expertise embedded graph
contrastive learning, which introduces knowledge distillations to empower GCL
models to learn the hierarchical topology isomorphism expertise, including the
graph-tier and subgraph-tier. On top of this, the proposed method holds the
feature of plug-and-play, and we empirically demonstrate that the proposed
method is universal to multiple state-of-the-art GCL models. The solid
theoretical analyses are further provided to prove that compared with
conventional GCL methods, our method acquires the tighter upper bound of Bayes
classification error. We conduct extensive experiments on real-world benchmarks
to exhibit the performance superiority of our method over candidate GCL
methods, e.g., for the real-world graph representation learning experiments,
the proposed method beats the state-of-the-art method by 0.23% on unsupervised
representation learning setting, 0.43% on transfer learning setting. Our code
is available at https://github.com/jyf123/HTML.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiangmeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yifan Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1&quot;&gt;Hang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1&quot;&gt;Wenwen Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Changwen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1&quot;&gt;Fuchun Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14769">
<title>Large Language Model (LLM) Bias Index -- LLMBI. (arXiv:2312.14769v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14769</link>
<description rdf:parseType="Literal">&lt;p&gt;The Large Language Model Bias Index (LLMBI) is a pioneering approach designed
to quantify and address biases inherent in large language models (LLMs), such
as GPT-4. We recognise the increasing prevalence and impact of LLMs across
diverse sectors. This research introduces a novel metric, LLMBI, to
systematically measure and mitigate biases potentially skewing model responses.
We formulated LLMBI using a composite scoring system incorporating multiple
dimensions of bias, including but not limited to age, gender, and racial
biases.
&lt;/p&gt;
&lt;p&gt;To operationalise this metric, we engaged in a multi-step process involving
collecting and annotating LLM responses, applying sophisticated Natural
Language Processing (NLP) techniques for bias detection, and computing the
LLMBI score through a specially crafted mathematical formula. The formula
integrates weighted averages of various bias dimensions, a penalty for dataset
diversity deficiencies, and a correction for sentiment biases. Our empirical
analysis, conducted using responses from OpenAI&apos;s API, employs advanced
sentiment analysis as a representative method for bias detection.
&lt;/p&gt;
&lt;p&gt;The research reveals LLMs, whilst demonstrating impressive capabilities in
text generation, exhibit varying degrees of bias across different dimensions.
LLMBI provides a quantifiable measure to compare biases across models and over
time, offering a vital tool for systems engineers, researchers and regulators
in enhancing the fairness and reliability of LLMs. It highlights the potential
of LLMs in mimicking unbiased human-like responses. Additionally, it
underscores the necessity of continuously monitoring and recalibrating such
models to align with evolving societal norms and ethical standards.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oketunji_A/0/1/0/all/0/1&quot;&gt;Abiodun Finbarrs Oketunji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anas_M/0/1/0/all/0/1&quot;&gt;Muhammad Anas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saina_D/0/1/0/all/0/1&quot;&gt;Deepthi Saina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14852">
<title>TACO: Topics in Algorithmic COde generation dataset. (arXiv:2312.14852v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14852</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce TACO, an open-source, large-scale code generation dataset, with
a focus on the optics of algorithms, designed to provide a more challenging
training dataset and evaluation benchmark in the field of code generation
models. TACO includes competition-level programming questions that are more
challenging, to enhance or evaluate problem understanding and reasoning
abilities in real-world programming scenarios. There are 25433 and 1000 coding
problems in training and test set, as well as up to 1.55 million diverse
solution answers. Moreover, each TACO problem includes several fine-grained
labels such as task topics, algorithms, programming skills, and difficulty
levels, providing a more precise reference for the training and evaluation of
code generation models. The dataset and evaluation scripts are available on
Hugging Face Hub (https://huggingface.co/datasets/BAAI/TACO) and Github
(https://github.com/FlagOpen/TACO).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Rongao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo-Wen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhihong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1&quot;&gt;Chen Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zhi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Ge Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14890">
<title>NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes. (arXiv:2312.14890v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14890</link>
<description rdf:parseType="Literal">&lt;p&gt;Complex reasoning ability is one of the most important features of current
LLMs, which has also been leveraged to play an integral role in complex
decision-making tasks. Therefore, the investigation into the reasoning
capabilities of Large Language Models (LLMs) is critical: numerous benchmarks
have been established to assess the reasoning abilities of LLMs. However,
current benchmarks are inadequate in offering a rigorous evaluation of the full
extent of reasoning abilities that LLMs are capable of achieving. They are also
prone to the risk of overfitting, as these benchmarks, being publicly
accessible and static, allow models to potentially tailor their responses to
specific benchmark metrics, thereby inflating their performance. Addressing
these limitations, our research introduces a new benchmark, named NPHardEval.
This benchmark is designed to evaluate the reasoning abilities of LLMs across a
broad spectrum of 900 algorithmic questions, extending up to the NP-Hard
complexity class. These questions are meticulously chosen to represent a wide
range of complexity class below the NP-hard complexity class, offering a
rigorous measure of the reasoning ability of LLMs. Through this study, we shed
light on the current state of reasoning in LLMs, providing an objective and
rigorous perspective through the comparison of LLMs&apos; performance across complex
classes. Moreover, this benchmark is designed with a dynamic update mechanism,
where the datapoints are refreshed on a monthly basis. Such regular updates
play a crucial role in mitigating the risk of LLMs overfitting to the
benchmark, promoting a more accurate and reliable assessment of their reasoning
capabilities. The benchmark dataset and code of NPHardEval are available at
https://github.com/casmlab/NPHardEval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Lizhou Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1&quot;&gt;Wenyue Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lingyao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1&quot;&gt;Haoyang Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemphill_L/0/1/0/all/0/1&quot;&gt;Libby Hemphill&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14924">
<title>Training Convolutional Neural Networks with the Forward-Forward algorithm. (arXiv:2312.14924v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14924</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent successes in analyzing images with deep neural networks are almost
exclusively achieved with Convolutional Neural Networks (CNNs). The training of
these CNNs, and in fact of all deep neural network architectures, uses the
backpropagation algorithm where the output of the network is compared with the
desired result and the difference is then used to tune the weights of the
network towards the desired outcome. In a 2022 preprint, Geoffrey Hinton
suggested an alternative way of training which passes the desired results
together with the images at the input of the network. This so called Forward
Forward (FF) algorithm has up to now only been used in fully connected
networks. In this paper, we show how the FF paradigm can be extended to CNNs.
Our FF-trained CNN, featuring a novel spatially-extended labeling technique,
achieves a classification accuracy of 99.0% on the MNIST hand-written digits
dataset. We show how different hyperparameters affect the performance of the
proposed algorithm and compare the results with CNN trained with the standard
backpropagation approach. Furthermore, we use Class Activation Maps to
investigate which type of features are learnt by the FF algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scodellaro_R/0/1/0/all/0/1&quot;&gt;Riccardo Scodellaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1&quot;&gt;Ajinkya Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alves_F/0/1/0/all/0/1&quot;&gt;Frauke Alves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroter_M/0/1/0/all/0/1&quot;&gt;Matthias Schr&amp;#xf6;ter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10868">
<title>From Google Gemini to OpenAI Q* (Q-Star): A Survey of Reshaping the Generative Artificial Intelligence (AI) Research Landscape. (arXiv:2312.10868v1 [cs.AI] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2312.10868</link>
<description rdf:parseType="Literal">&lt;p&gt;This comprehensive survey explored the evolving landscape of generative
Artificial Intelligence (AI), with a specific focus on the transformative
impacts of Mixture of Experts (MoE), multimodal learning, and the speculated
advancements towards Artificial General Intelligence (AGI). It critically
examined the current state and future trajectory of generative Artificial
Intelligence (AI), exploring how innovations like Google&apos;s Gemini and the
anticipated OpenAI Q* project are reshaping research priorities and
applications across various domains, including an impact analysis on the
generative AI research taxonomy. It assessed the computational challenges,
scalability, and real-world implications of these technologies while
highlighting their potential in driving significant progress in fields like
healthcare, finance, and education. It also addressed the emerging academic
challenges posed by the proliferation of both AI-themed and AI-generated
preprints, examining their impact on the peer-review process and scholarly
communication. The study highlighted the importance of incorporating ethical
and human-centric methods in AI development, ensuring alignment with societal
norms and welfare, and outlined a strategy for future AI research that focuses
on a balanced and conscientious use of MoE, multimodality, and AGI in
generative AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McIntosh_T/0/1/0/all/0/1&quot;&gt;Timothy R. McIntosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Susnjak_T/0/1/0/all/0/1&quot;&gt;Teo Susnjak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watters_P/0/1/0/all/0/1&quot;&gt;Paul Watters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halgamuge_M/0/1/0/all/0/1&quot;&gt;Malka N. Halgamuge&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>