<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-09T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03266" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03270" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03275" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03293" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03339" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03353" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03376" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03398" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03407" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03416" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03423" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03427" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03441" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03449" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03465" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03476" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03505" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03536" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03538" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03575" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03592" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03601" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03602" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03623" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03662" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03712" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2007.10665" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.01708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.09706" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.13682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.08340" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.12817" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.00313" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10289" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02150" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03177" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.03211">
<title>PseudoCell: Hard Negative Mining as Pseudo Labeling for Deep Learning-Based Centroblast Cell Detection. (arXiv:2307.03211v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2307.03211</link>
<description rdf:parseType="Literal">&lt;p&gt;Patch classification models based on deep learning have been utilized in
whole-slide images (WSI) of H&amp;amp;E-stained tissue samples to assist pathologists
in grading follicular lymphoma patients. However, these approaches still
require pathologists to manually identify centroblast cells and provide refined
labels for optimal performance. To address this, we propose PseudoCell, an
object detection framework to automate centroblast detection in WSI (source
code is available at https://github.com/IoBT-VISTEC/PseudoCell.git). This
framework incorporates centroblast labels from pathologists and combines them
with pseudo-negative labels obtained from undersampled false-positive
predictions using the cell&apos;s morphological features. By employing PseudoCell,
pathologists&apos; workload can be reduced as it accurately narrows down the areas
requiring their attention during examining tissue. Depending on the confidence
threshold, PseudoCell can eliminate 58.18-99.35% of non-centroblasts tissue
areas on WSI. This study presents a practical centroblast prescreening method
that does not require pathologists&apos; refined labels for improvement. Detailed
guidance on the practical implementation of PseudoCell is provided in the
discussion section.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Seesawad_N/0/1/0/all/0/1&quot;&gt;Narongrid Seesawad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ittichaiwong_P/0/1/0/all/0/1&quot;&gt;Piyalitt Ittichaiwong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sudhawiyangkul_T/0/1/0/all/0/1&quot;&gt;Thapanun Sudhawiyangkul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sawangjai_P/0/1/0/all/0/1&quot;&gt;Phattarapong Sawangjai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Thuwajit_P/0/1/0/all/0/1&quot;&gt;Peti Thuwajit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Boonsakan_P/0/1/0/all/0/1&quot;&gt;Paisarn Boonsakan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sripodok_S/0/1/0/all/0/1&quot;&gt;Supasan Sripodok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Veerakanjana_K/0/1/0/all/0/1&quot;&gt;Kanyakorn Veerakanjana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Luenam_P/0/1/0/all/0/1&quot;&gt;Phoomraphee Luenam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Charngkaew_K/0/1/0/all/0/1&quot;&gt;Komgrid Charngkaew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Pongpaibul_A/0/1/0/all/0/1&quot;&gt;Ananya Pongpaibul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Angkathunyakul_N/0/1/0/all/0/1&quot;&gt;Napat Angkathunyakul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hnoohom_N/0/1/0/all/0/1&quot;&gt;Narit Hnoohom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yuenyong_S/0/1/0/all/0/1&quot;&gt;Sumeth Yuenyong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Thuwajit_C/0/1/0/all/0/1&quot;&gt;Chanitra Thuwajit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wilaiprasitporn_T/0/1/0/all/0/1&quot;&gt;Theerawit Wilaiprasitporn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03212">
<title>Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings. (arXiv:2307.03212v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03212</link>
<description rdf:parseType="Literal">&lt;p&gt;Urban region embedding is an important and yet highly challenging issue due
to the complexity and constantly changing nature of urban data. To address the
challenges, we propose a Region-Wise Multi-View Representation Learning (ROMER)
to capture multi-view dependencies and learn expressive representations of
urban regions without the constraints of rigid neighbourhood region conditions.
Our model focus on learn urban region representation from multi-source urban
data. First, we capture the multi-view correlations from mobility flow
patterns, POI semantics and check-in dynamics. Then, we adopt global graph
attention networks to learn similarity of any two vertices in graphs. To
comprehensively consider and share features of multiple views, a two-stage
fusion module is further proposed to learn weights with external attention to
fuse multi-view embeddings. Extensive experiments for two downstream tasks on
real-world datasets demonstrate that our model outperforms state-of-the-art
methods by up to 17\% improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1&quot;&gt;Weiliang Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1&quot;&gt;Qianqian Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03240">
<title>Adaptive Generation of Privileged Intermediate Information for Visible-Infrared Person Re-Identification. (arXiv:2307.03240v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03240</link>
<description rdf:parseType="Literal">&lt;p&gt;Visible-infrared person re-identification seeks to retrieve images of the
same individual captured over a distributed network of RGB and IR sensors.
Several V-I ReID approaches directly integrate both V and I modalities to
discriminate persons within a shared representation space. However, given the
significant gap in data distributions between V and I modalities, cross-modal
V-I ReID remains challenging. Some recent approaches improve generalization by
leveraging intermediate spaces that can bridge V and I modalities, yet
effective methods are required to select or generate data for such informative
domains. In this paper, the Adaptive Generation of Privileged Intermediate
Information training approach is introduced to adapt and generate a virtual
domain that bridges discriminant information between the V and I modalities.
The key motivation behind AGPI^2 is to enhance the training of a deep V-I ReID
backbone by generating privileged images that provide additional information.
These privileged images capture shared discriminative features that are not
easily accessible within the original V or I modalities alone. Towards this
goal, a non-linear generative module is trained with an adversarial objective,
translating V images into intermediate spaces with a smaller domain shift
w.r.t. the I domain. Meanwhile, the embedding module within AGPI^2 aims to
produce similar features for both V and generated images, encouraging the
extraction of features that are common to all modalities. In addition to these
contributions, AGPI^2 employs adversarial objectives for adapting the
intermediate images, which play a crucial role in creating a
non-modality-specific space to address the large domain shifts between V and I
domains. Experimental results conducted on challenging V-I ReID datasets
indicate that AGPI^2 increases matching accuracy without extra computational
resources during inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alehdaghi_M/0/1/0/all/0/1&quot;&gt;Mahdi Alehdaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Josi_A/0/1/0/all/0/1&quot;&gt;Arthur Josi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shamsolmoali_P/0/1/0/all/0/1&quot;&gt;Pourya Shamsolmoali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_R/0/1/0/all/0/1&quot;&gt;Rafael M. O. Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1&quot;&gt;Eric Granger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03243">
<title>That&apos;s BAD: Blind Anomaly Detection by Implicit Local Feature Clustering. (arXiv:2307.03243v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03243</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies on visual anomaly detection (AD) of industrial
objects/textures have achieved quite good performance. They consider an
unsupervised setting, specifically the one-class setting, in which we assume
the availability of a set of normal (\textit{i.e.}, anomaly-free) images for
training. In this paper, we consider a more challenging scenario of
unsupervised AD, in which we detect anomalies in a given set of images that
might contain both normal and anomalous samples. The setting does not assume
the availability of known normal data and thus is completely free from human
annotation, which differs from the standard AD considered in recent studies.
For clarity, we call the setting blind anomaly detection (BAD). We show that
BAD can be converted into a local outlier detection problem and propose a novel
method named PatchCluster that can accurately detect image- and pixel-level
anomalies. Experimental results show that PatchCluster shows a promising
performance without the knowledge of normal data, even comparable to the SOTA
methods applied in the one-class setting needing it.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1&quot;&gt;Masanori Suganuma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1&quot;&gt;Takayuki Okatani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03244">
<title>PSDR-Room: Single Photo to Scene using Differentiable Rendering. (arXiv:2307.03244v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03244</link>
<description rdf:parseType="Literal">&lt;p&gt;A 3D digital scene contains many components: lights, materials and
geometries, interacting to reach the desired appearance. Staging such a scene
is time-consuming and requires both artistic and technical skills. In this
work, we propose PSDR-Room, a system allowing to optimize lighting as well as
the pose and materials of individual objects to match a target image of a room
scene, with minimal user input. To this end, we leverage a recent path-space
differentiable rendering approach that provides unbiased gradients of the
rendering with respect to geometry, lighting, and procedural materials,
allowing us to optimize all of these components using gradient descent to
visually match the input photo appearance. We use recent single-image scene
understanding methods to initialize the optimization and search for appropriate
3D models and materials. We evaluate our method on real photographs of indoor
scenes and demonstrate the editability of the resulting scene components.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Kai Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luan_F/0/1/0/all/0/1&quot;&gt;Fujun Luan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+HaSAn_M/0/1/0/all/0/1&quot;&gt;Milo&amp;#x160; Ha&amp;#x160;An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groueix_T/0/1/0/all/0/1&quot;&gt;Thibault Groueix&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deschaintre_V/0/1/0/all/0/1&quot;&gt;Valentin Deschaintre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shuang Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03254">
<title>Vision Language Transformers: A Survey. (arXiv:2307.03254v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03254</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision language tasks, such as answering questions about or generating
captions that describe an image, are difficult tasks for computers to perform.
A relatively recent body of research has adapted the pretrained transformer
architecture introduced in \citet{vaswani2017attention} to vision language
modeling. Transformer models have greatly improved performance and versatility
over previous vision language models. They do so by pretraining models on a
large generic datasets and transferring their learning to new tasks with minor
changes in architecture and parameter values. This type of transfer learning
has become the standard modeling practice in both natural language processing
and computer vision. Vision language transformers offer the promise of
producing similar advancements in tasks which require both vision and language.
In this paper, we provide a broad synthesis of the currently available research
on vision language transformer models and offer some analysis of their
strengths, limitations and some open questions that remain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fields_C/0/1/0/all/0/1&quot;&gt;Clayton Fields&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kennington_C/0/1/0/all/0/1&quot;&gt;Casey Kennington&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03266">
<title>Empirical Analysis of a Segmentation Foundation Model in Prostate Imaging. (arXiv:2307.03266v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.03266</link>
<description rdf:parseType="Literal">&lt;p&gt;Most state-of-the-art techniques for medical image segmentation rely on
deep-learning models. These models, however, are often trained on
narrowly-defined tasks in a supervised fashion, which requires expensive
labeled datasets. Recent advances in several machine learning domains, such as
natural language generation have demonstrated the feasibility and utility of
building foundation models that can be customized for various downstream tasks
with little to no labeled data. This likely represents a paradigm shift for
medical imaging, where we expect that foundation models may shape the future of
the field. In this paper, we consider a recently developed foundation model for
medical image segmentation, UniverSeg. We conduct an empirical evaluation study
in the context of prostate imaging and compare it against the conventional
approach of training a task-specific segmentation model. Our results and
discussion highlight several important factors that will likely be important in
the development and adoption of foundation models for medical image
segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Heejong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Butoi_V/0/1/0/all/0/1&quot;&gt;Victor Ion Butoi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1&quot;&gt;Adrian V. Dalca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sabuncu_M/0/1/0/all/0/1&quot;&gt;Mert R. Sabuncu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03270">
<title>A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony in Talking Head Generation. (arXiv:2307.03270v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2307.03270</link>
<description rdf:parseType="Literal">&lt;p&gt;Animating still face images with deep generative models using a speech input
signal is an active research topic and has seen important recent progress.
However, much of the effort has been put into lip syncing and rendering quality
while the generation of natural head motion, let alone the audio-visual
correlation between head motion and speech, has often been neglected. In this
work, we propose a multi-scale audio-visual synchrony loss and a multi-scale
autoregressive GAN to better handle short and long-term correlation between
speech and the dynamics of the head and lips. In particular, we train a stack
of syncer models on multimodal input pyramids and use these models as guidance
in a multi-scale generator network to produce audio-aligned motion unfolding
over diverse time scales. Our generator operates in the facial landmark domain,
which is a standard low-dimensional head representation. The experiments show
significant improvements over the state of the art in head motion dynamics
quality and in multi-scale audio-visual synchrony both in the landmark domain
and in the image domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Airale_L/0/1/0/all/0/1&quot;&gt;Louis Airale&lt;/a&gt; (UGA, LIG), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaufreydaz_D/0/1/0/all/0/1&quot;&gt;Dominique Vaufreydaz&lt;/a&gt; (LIG), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1&quot;&gt;Xavier Alameda-Pineda&lt;/a&gt; (UGA)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03273">
<title>ADASSM: Adversarial Data Augmentation in Statistical Shape Models From Images. (arXiv:2307.03273v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03273</link>
<description rdf:parseType="Literal">&lt;p&gt;Statistical shape models (SSM) have been well-established as an excellent
tool for identifying variations in the morphology of anatomy across the
underlying population. Shape models use consistent shape representation across
all the samples in a given cohort, which helps to compare shapes and identify
the variations that can detect pathologies and help in formulating treatment
plans. In medical imaging, computing these shape representations from CT/MRI
scans requires time-intensive preprocessing operations, including but not
limited to anatomy segmentation annotations, registration, and texture
denoising. Deep learning models have demonstrated exceptional capabilities in
learning shape representations directly from volumetric images, giving rise to
highly effective and efficient Image-to-SSM. Nevertheless, these models are
data-hungry and due to the limited availability of medical data, deep learning
models tend to overfit. Offline data augmentation techniques, that use kernel
density estimation based (KDE) methods for generating shape-augmented samples,
have successfully aided Image-to-SSM networks in achieving comparable accuracy
to traditional SSM methods. However, these augmentation methods focus on shape
augmentation, whereas deep learning models exhibit texture bias results in
sub-optimal models. This paper introduces a novel strategy for on-the-fly data
augmentation for the Image-to-SSM framework by leveraging data-dependent noise
generation or texture augmentation. The proposed framework is trained as an
adversary to the Image-to-SSM network, augmenting diverse and challenging noisy
samples. Our approach achieves improved accuracy by encouraging the model to
focus on the underlying geometry rather than relying solely on pixel values.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karanam_M/0/1/0/all/0/1&quot;&gt;Mokshagna Sai Teja Karanam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kataria_T/0/1/0/all/0/1&quot;&gt;Tushar Kataria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhabian_S/0/1/0/all/0/1&quot;&gt;Shireen Elhabian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03274">
<title>It is not Sexually Suggestive, It is Educative. Separating Sex Education from Suggestive Content on TikTok Videos. (arXiv:2307.03274v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03274</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce SexTok, a multi-modal dataset composed of TikTok videos labeled
as sexually suggestive (from the annotator&apos;s point of view), sex-educational
content, or neither. Such a dataset is necessary to address the challenge of
distinguishing between sexually suggestive content and virtual sex education
videos on TikTok. Children&apos;s exposure to sexually suggestive videos has been
shown to have adversarial effects on their development. Meanwhile, virtual sex
education, especially on subjects that are more relevant to the LGBTQIA+
community, is very valuable. The platform&apos;s current system removes or penalizes
some of both types of videos, even though they serve different purposes. Our
dataset contains video URLs, and it is also audio transcribed. To validate its
importance, we explore two transformer-based models for classifying the videos.
Our preliminary results suggest that the task of distinguishing between these
types of videos is learnable but challenging. These experiments suggest that
this dataset is meaningful and invites further study on the subject.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+George_E/0/1/0/all/0/1&quot;&gt;Enfa George&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1&quot;&gt;Mihai Surdeanu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03275">
<title>To pretrain or not to pretrain? A case study of domain-specific pretraining for semantic segmentation in histopathology. (arXiv:2307.03275v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03275</link>
<description rdf:parseType="Literal">&lt;p&gt;Annotating medical imaging datasets is costly, so fine-tuning (or transfer
learning) is the most effective method for digital pathology vision
applications such as disease classification and semantic segmentation. However,
due to texture bias in models trained on real-world images, transfer learning
for histopathology applications might result in underperforming models, which
necessitates the need for using unlabeled histopathology data and
self-supervised methods to discover domain-specific characteristics. Here, we
tested the premise that histopathology-specific pretrained models provide
better initializations for pathology vision tasks, i.e., gland and cell
segmentation. In this study, we compare the performance of gland and cell
segmentation tasks with domain-specific and non-domain-specific pretrained
weights. Moreover, we investigate the data size at which domain-specific
pretraining produces a statistically significant difference in performance. In
addition, we investigated whether domain-specific initialization improves the
effectiveness of out-of-domain testing on distinct datasets but the same task.
The results indicate that performance gain using domain-specific pretraining
depends on both the task and the size of the training dataset. In instances
with limited dataset sizes, a significant improvement in gland segmentation
performance was also observed, whereas models trained on cell segmentation
datasets exhibit no improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kataria_T/0/1/0/all/0/1&quot;&gt;Tushar Kataria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knudsen_B/0/1/0/all/0/1&quot;&gt;Beatrice Knudsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhabian_S/0/1/0/all/0/1&quot;&gt;Shireen Elhabian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03293">
<title>CheXmask: a large-scale dataset of anatomical segmentation masks for multi-center chest x-ray images. (arXiv:2307.03293v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.03293</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of successful artificial intelligence models for chest X-ray
analysis relies on large, diverse datasets with high-quality annotations. While
several databases of chest X-ray images have been released, most include
disease diagnosis labels but lack detailed pixel-level anatomical segmentation
labels. To address this gap, we introduce an extensive chest X-ray multi-center
segmentation dataset with uniform and fine-grain anatomical annotations for
images coming from six well-known publicly available databases: CANDID-PTX,
ChestX-ray8, Chexpert, MIMIC-CXR-JPG, Padchest, and VinDr-CXR, resulting in
676,803 segmentation masks. Our methodology utilizes the HybridGNet model to
ensure consistent and high-quality segmentations across all datasets. Rigorous
validation, including expert physician evaluation and automatic quality
control, was conducted to validate the resulting masks. Additionally, we
provide individualized quality indices per mask and an overall quality
estimation per dataset. This dataset serves as a valuable resource for the
broader scientific community, streamlining the development and assessment of
innovative methodologies in chest X-ray analysis. The CheXmask dataset is
publicly available at:
\url{https://physionet.org/content/chexmask-cxr-segmentation-data/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gaggion_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xe1;s Gaggion&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mosquera_C/0/1/0/all/0/1&quot;&gt;Candelaria Mosquera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mansilla_L/0/1/0/all/0/1&quot;&gt;Lucas Mansilla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aineseder_M/0/1/0/all/0/1&quot;&gt;Martina Aineseder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Milone_D/0/1/0/all/0/1&quot;&gt;Diego H. Milone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ferrante_E/0/1/0/all/0/1&quot;&gt;Enzo Ferrante&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03329">
<title>Facial Landmark Detection Evaluation on MOBIO Database. (arXiv:2307.03329v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03329</link>
<description rdf:parseType="Literal">&lt;p&gt;MOBIO is a bi-modal database that was captured almost exclusively on mobile
phones. It aims to improve research into deploying biometric techniques to
mobile devices. Research has been shown that face and speaker recognition can
be performed in a mobile environment. Facial landmark localization aims at
finding the coordinates of a set of pre-defined key points for 2D face images.
A facial landmark usually has specific semantic meaning, e.g. nose tip or eye
centre, which provides rich geometric information for other face analysis tasks
such as face recognition, emotion estimation and 3D face reconstruction. Pretty
much facial landmark detection methods adopt still face databases, such as
300W, AFW, AFLW, or COFW, for evaluation, but seldomly use mobile data. Our
work is first to perform facial landmark detection evaluation on the mobile
still data, i.e., face images from MOBIO database. About 20,600 face images
have been extracted from this audio-visual database and manually labeled with
22 landmarks as the groundtruth. Several state-of-the-art facial landmark
detection methods are adopted to evaluate their performance on these data. The
result shows that the data from MOBIO database is pretty challenging. This
database can be a new challenging one for facial landmark detection evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Na Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03339">
<title>Open-Vocabulary Object Detection via Scene Graph Discovery. (arXiv:2307.03339v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03339</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, open-vocabulary (OV) object detection has attracted
increasing research attention. Unlike traditional detection, which only
recognizes fixed-category objects, OV detection aims to detect objects in an
open category set. Previous works often leverage vision-language (VL) training
data (e.g., referring grounding data) to recognize OV objects. However, they
only use pairs of nouns and individual objects in VL data, while these data
usually contain much more information, such as scene graphs, which are also
crucial for OV detection. In this paper, we propose a novel Scene-Graph-Based
Discovery Network (SGDN) that exploits scene graph cues for OV detection.
Firstly, a scene-graph-based decoder (SGDecoder) including sparse
scene-graph-guided attention (SSGA) is presented. It captures scene graphs and
leverages them to discover OV objects. Secondly, we propose scene-graph-based
prediction (SGPred), where we build a scene-graph-based offset regression
(SGOR) mechanism to enable mutual enhancement between scene graph extraction
and object localization. Thirdly, we design a cross-modal learning mechanism in
SGPred. It takes scene graphs as bridges to improve the consistency between
cross-modal embeddings for OV object classification. Experiments on COCO and
LVIS demonstrate the effectiveness of our approach. Moreover, we show the
ability of our model for OV scene graph detection, while previous OV scene
graph generation methods cannot tackle this task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Hengcan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1&quot;&gt;Munawar Hayat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jianfei Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03353">
<title>A Survey of Deep Learning in Sports Applications: Perception, Comprehension, and Decision. (arXiv:2307.03353v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03353</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has the potential to revolutionize sports performance, with
applications ranging from perception and comprehension to decision. This paper
presents a comprehensive survey of deep learning in sports performance,
focusing on three main aspects: algorithms, datasets and virtual environments,
and challenges. Firstly, we discuss the hierarchical structure of deep learning
algorithms in sports performance which includes perception, comprehension and
decision while comparing their strengths and weaknesses. Secondly, we list
widely used existing datasets in sports and highlight their characteristics and
limitations. Finally, we summarize current challenges and point out future
trends of deep learning in sports. Our survey provides valuable reference
material for researchers interested in deep learning in sports applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhonghan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_W/0/1/0/all/0/1&quot;&gt;Wenhao Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1&quot;&gt;Shengyu Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenhao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guanhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1&quot;&gt;Shidong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1&quot;&gt;Mingli Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jenq-Neng Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Gaoang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03373">
<title>All in One: Exploring Unified Vision-Language Tracking with Multi-Modal Alignment. (arXiv:2307.03373v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03373</link>
<description rdf:parseType="Literal">&lt;p&gt;Current mainstream vision-language (VL) tracking framework consists of three
parts, \ie a visual feature extractor, a language feature extractor, and a
fusion model. To pursue better performance, a natural modus operandi for VL
tracking is employing customized and heavier unimodal encoders, and multi-modal
fusion models. Albeit effective, existing VL trackers separate feature
extraction and feature integration, resulting in extracted features that lack
semantic guidance and have limited target-aware capability in complex
scenarios, \eg similar distractors and extreme illumination. In this work,
inspired by the recent success of exploring foundation models with unified
architecture for both natural language and computer vision tasks, we propose an
All-in-One framework, which learns joint feature extraction and interaction by
adopting a unified transformer backbone. Specifically, we mix raw vision and
language signals to generate language-injected vision tokens, which we then
concatenate before feeding into the unified backbone architecture. This
approach achieves feature integration in a unified backbone, removing the need
for carefully-designed fusion modules and resulting in a more effective and
efficient VL tracking framework. To further improve the learning efficiency, we
introduce a multi-modal alignment module based on cross-modal and intra-modal
contrastive objectives, providing more reasonable representations for the
unified All-in-One transformer backbone. Extensive experiments on five
benchmarks, \ie OTB99-L, TNL2K, LaSOT, LaSOT$_{\rm Ext}$ and WebUAV-3M,
demonstrate the superiority of the proposed tracker against existing
state-of-the-arts on VL tracking. Codes will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chunhui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Li Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yiqian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanfeng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03376">
<title>Weakly-supervised Contrastive Learning for Unsupervised Object Discovery. (arXiv:2307.03376v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03376</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised object discovery (UOD) refers to the task of discriminating the
whole region of objects from the background within a scene without relying on
labeled datasets, which benefits the task of bounding-box-level localization
and pixel-level segmentation. This task is promising due to its ability to
discover objects in a generic manner. We roughly categorise existing techniques
into two main directions, namely the generative solutions based on image
resynthesis, and the clustering methods based on self-supervised models. We
have observed that the former heavily relies on the quality of image
reconstruction, while the latter shows limitations in effectively modeling
semantic correlations. To directly target at object discovery, we focus on the
latter approach and propose a novel solution by incorporating weakly-supervised
contrastive learning (WCL) to enhance semantic information exploration. We
design a semantic-guided self-supervised learning model to extract high-level
semantic features from images, which is achieved by fine-tuning the feature
encoder of a self-supervised model, namely DINO, via WCL. Subsequently, we
introduce Principal Component Analysis (PCA) to localize object regions. The
principal projection direction, corresponding to the maximal eigenvalue, serves
as an indicator of the object region(s). Extensive experiments on benchmark
unsupervised object discovery datasets demonstrate the effectiveness of our
proposed solution. The source code and experimental results are publicly
available via our project page at https://github.com/npucvr/WSCUOD.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1&quot;&gt;Yunqiu Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1&quot;&gt;Nick Barnes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yuchao Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03388">
<title>General-Purpose Multimodal Transformer meets Remote Sensing Semantic Segmentation. (arXiv:2307.03388v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03388</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of high-resolution multispectral/hyperspectral sensors, LiDAR DSM
(Digital Surface Model) information and many others has provided us with an
unprecedented wealth of data for Earth Observation. Multimodal AI seeks to
exploit those complementary data sources, particularly for complex tasks like
semantic segmentation. While specialized architectures have been developed,
they are highly complicated via significant effort in model design, and require
considerable re-engineering whenever a new modality emerges. Recent trends in
general-purpose multimodal networks have shown great potential to achieve
state-of-the-art performance across multiple multimodal tasks with one unified
architecture. In this work, we investigate the performance of PerceiverIO, one
in the general-purpose multimodal family, in the remote sensing semantic
segmentation domain. Our experiments reveal that this ostensibly universal
network struggles with object scale variation in remote sensing images and
fails to detect the presence of cars from a top-down view. To address these
issues, even with extreme class imbalance issues, we propose a spatial and
volumetric learning component. Specifically, we design a UNet-inspired module
that employs 3D convolution to encode vital local information and learn
cross-modal features simultaneously, while reducing network computational
burden via the cross-attention mechanism of PerceiverIO. The effectiveness of
the proposed component is validated through extensive experiments comparing it
with other methods such as 2D convolution, and dual local module (\ie the
combination of Conv2D 1x1 and Conv2D 3x3 inspired by UNetFormer). The proposed
method achieves competitive results with specialized architectures like
UNetFormer and SwinUNet, showing its potential to minimize network architecture
engineering with a minimal compromise on the performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kieu_N/0/1/0/all/0/1&quot;&gt;Nhi Kieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Kien Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1&quot;&gt;Sridha Sridharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1&quot;&gt;Clinton Fookes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03398">
<title>Beyond Geo-localization: Fine-grained Orientation of Street-view Images by Cross-view Matching with Satellite Imagery. (arXiv:2307.03398v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03398</link>
<description rdf:parseType="Literal">&lt;p&gt;Street-view imagery provides us with novel experiences to explore different
places remotely. Carefully calibrated street-view images (e.g. Google Street
View) can be used for different downstream tasks, e.g. navigation, map features
extraction. As personal high-quality cameras have become much more affordable
and portable, an enormous amount of crowdsourced street-view images are
uploaded to the internet, but commonly with missing or noisy sensor
information. To prepare this hidden treasure for &quot;ready-to-use&quot; status,
determining missing location information and camera orientation angles are two
equally important tasks. Recent methods have achieved high performance on
geo-localization of street-view images by cross-view matching with a pool of
geo-referenced satellite imagery. However, most of the existing works focus
more on geo-localization than estimating the image orientation. In this work,
we re-state the importance of finding fine-grained orientation for street-view
images, formally define the problem and provide a set of evaluation metrics to
assess the quality of the orientation estimation. We propose two methods to
improve the granularity of the orientation estimation, achieving 82.4% and
72.3% accuracy for images with estimated angle errors below 2 degrees for CVUSA
and CVACT datasets, corresponding to 34.9% and 28.2% absolute improvement
compared to previous works. Integrating fine-grained orientation estimation in
training also improves the performance on geo-localization, giving top 1 recall
95.5%/85.5% and 86.8%/80.4% for orientation known/unknown tests on the two
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenmiao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yichen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1&quot;&gt;Yifang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Georgescu_A/0/1/0/all/0/1&quot;&gt;Andrei Georgescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1&quot;&gt;An Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kruppa_H/0/1/0/all/0/1&quot;&gt;Hannes Kruppa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1&quot;&gt;See-Kiong Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1&quot;&gt;Roger Zimmermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03404">
<title>RGB-D Mapping and Tracking in a Plenoxel Radiance Field. (arXiv:2307.03404v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03404</link>
<description rdf:parseType="Literal">&lt;p&gt;Building on the success of Neural Radiance Fields (NeRFs), recent years have
seen significant advances in the domain of novel view synthesis. These models
capture the scene&apos;s volumetric radiance field, creating highly convincing dense
photorealistic models through the use of simple, differentiable rendering
equations. Despite their popularity, these algorithms suffer from severe
ambiguities in visual data inherent to the RGB sensor, which means that
although images generated with view synthesis can visually appear very
believable, the underlying 3D model will often be wrong. This considerably
limits the usefulness of these models in practical applications like Robotics
and Extended Reality (XR), where an accurate dense 3D reconstruction otherwise
would be of significant value. In this technical report, we present the vital
differences between view synthesis models and 3D reconstruction models. We also
comment on why a depth sensor is essential for modeling accurate geometry in
general outward-facing scenes using the current paradigm of novel view
synthesis methods. Focusing on the structure-from-motion task, we practically
demonstrate this need by extending the Plenoxel radiance field model:
Presenting an analytical differential approach for dense mapping and tracking
with radiance fields based on RGB-D data without a neural network. Our method
achieves state-of-the-art results in both the mapping and tracking tasks while
also being faster than competing neural network-based approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teigen_A/0/1/0/all/0/1&quot;&gt;Andreas L. Teigen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1&quot;&gt;Yeonsoo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stahl_A/0/1/0/all/0/1&quot;&gt;Annette Stahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mester_R/0/1/0/all/0/1&quot;&gt;Rudolf Mester&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03407">
<title>Distilling Self-Supervised Vision Transformers for Weakly-Supervised Few-Shot Classification &amp; Segmentation. (arXiv:2307.03407v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03407</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the task of weakly-supervised few-shot image classification and
segmentation, by leveraging a Vision Transformer (ViT) pretrained with
self-supervision. Our proposed method takes token representations from the
self-supervised ViT and leverages their correlations, via self-attention, to
produce classification and segmentation predictions through separate task
heads. Our model is able to effectively learn to perform classification and
segmentation in the absence of pixel-level labels during training, using only
image-level labels. To do this it uses attention maps, created from tokens
generated by the self-supervised ViT backbone, as pixel-level pseudo-labels. We
also explore a practical setup with ``mixed&quot; supervision, where a small number
of training images contains ground-truth pixel-level labels and the remaining
images have only image-level labels. For this mixed setup, we propose to
improve the pseudo-labels using a pseudo-label enhancer that was trained using
the available ground-truth pixel-level labels. Experiments on Pascal-5i and
COCO-20i demonstrate significant performance gains in a variety of supervision
settings, and in particular when little-to-no pixel-level labels are available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1&quot;&gt;Dahyun Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1&quot;&gt;Piotr Koniusz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1&quot;&gt;Minsu Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murray_N/0/1/0/all/0/1&quot;&gt;Naila Murray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03413">
<title>Unsupervised Hyperspectral and Multispectral Images Fusion Based on the Cycle Consistency. (arXiv:2307.03413v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03413</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperspectral images (HSI) with abundant spectral information reflected
materials property usually perform low spatial resolution due to the hardware
limits. Meanwhile, multispectral images (MSI), e.g., RGB images, have a high
spatial resolution but deficient spectral signatures. Hyperspectral and
multispectral image fusion can be cost-effective and efficient for acquiring
both high spatial resolution and high spectral resolution images. Many of the
conventional HSI and MSI fusion algorithms rely on known spatial degradation
parameters, i.e., point spread function, spectral degradation parameters,
spectral response function, or both of them. Another class of deep
learning-based models relies on the ground truth of high spatial resolution HSI
and needs large amounts of paired training images when working in a supervised
manner. Both of these models are limited in practical fusion scenarios. In this
paper, we propose an unsupervised HSI and MSI fusion model based on the cycle
consistency, called CycFusion. The CycFusion learns the domain transformation
between low spatial resolution HSI (LrHSI) and high spatial resolution MSI
(HrMSI), and the desired high spatial resolution HSI (HrHSI) are considered to
be intermediate feature maps in the transformation networks. The CycFusion can
be trained with the objective functions of marginal matching in single
transform and cycle consistency in double transforms. Moreover, the estimated
PSF and SRF are embedded in the model as the pre-training weights, which
further enhances the practicality of our proposed model. Experiments conducted
on several datasets show that our proposed model outperforms all compared
unsupervised fusion methods. The codes of this paper will be available at this
address: https: //github.com/shuaikaishi/CycFusion for reproducibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1&quot;&gt;Shuaikai Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lijun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altmann_Y/0/1/0/all/0/1&quot;&gt;Yoann Altmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03416">
<title>Learning Adversarial Semantic Embeddings for Zero-Shot Recognition in Open Worlds. (arXiv:2307.03416v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03416</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-Shot Learning (ZSL) focuses on classifying samples of unseen classes
with only their side semantic information presented during training. It cannot
handle real-life, open-world scenarios where there are test samples of unknown
classes for which neither samples (e.g., images) nor their side semantic
information is known during training. Open-Set Recognition (OSR) is dedicated
to addressing the unknown class issue, but existing OSR methods are not
designed to model the semantic information of the unseen classes. To tackle
this combined ZSL and OSR problem, we consider the case of &quot;Zero-Shot Open-Set
Recognition&quot; (ZS-OSR), where a model is trained under the ZSL setting but it is
required to accurately classify samples from the unseen classes while being
able to reject samples from the unknown classes during inference. We perform
large experiments on combining existing state-of-the-art ZSL and OSR models for
the ZS-OSR task on four widely used datasets adapted from the ZSL task, and
reveal that ZS-OSR is a non-trivial task as the simply combined solutions
perform badly in distinguishing the unseen-class and unknown-class samples. We
further introduce a novel approach specifically designed for ZS-OSR, in which
our model learns to generate adversarial semantic embeddings of the unknown
classes to train an unknowns-informed ZS-OSR classifier. Extensive empirical
results show that our method 1) substantially outperforms the combined
solutions in detecting the unknown classes while retaining the classification
accuracy on the unseen classes and 2) achieves similar superiority under
generalized ZS-OSR settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1&quot;&gt;Guansong Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xiao Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Lei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1&quot;&gt;Xin Ning&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03421">
<title>Non-iterative Coarse-to-fine Transformer Networks for Joint Affine and Deformable Image Registration. (arXiv:2307.03421v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03421</link>
<description rdf:parseType="Literal">&lt;p&gt;Image registration is a fundamental requirement for medical image analysis.
Deep registration methods based on deep learning have been widely recognized
for their capabilities to perform fast end-to-end registration. Many deep
registration methods achieved state-of-the-art performance by performing
coarse-to-fine registration, where multiple registration steps were iterated
with cascaded networks. Recently, Non-Iterative Coarse-to-finE (NICE)
registration methods have been proposed to perform coarse-to-fine registration
in a single network and showed advantages in both registration accuracy and
runtime. However, existing NICE registration methods mainly focus on deformable
registration, while affine registration, a common prerequisite, is still
reliant on time-consuming traditional optimization-based methods or extra
affine registration networks. In addition, existing NICE registration methods
are limited by the intrinsic locality of convolution operations. Transformers
may address this limitation for their capabilities to capture long-range
dependency, but the benefits of using transformers for NICE registration have
not been explored. In this study, we propose a Non-Iterative Coarse-to-finE
Transformer network (NICE-Trans) for image registration. Our NICE-Trans is the
first deep registration method that (i) performs joint affine and deformable
coarse-to-fine registration within a single network, and (ii) embeds
transformers into a NICE registration framework to model long-range relevance
between images. Extensive experiments with seven public datasets show that our
NICE-Trans outperforms state-of-the-art registration methods on both
registration accuracy and runtime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1&quot;&gt;Mingyuan Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_L/0/1/0/all/0/1&quot;&gt;Lei Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fulham_M/0/1/0/all/0/1&quot;&gt;Michael Fulham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1&quot;&gt;Dagan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jinman Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03423">
<title>Hyperspectral and Multispectral Image Fusion Using the Conditional Denoising Diffusion Probabilistic Model. (arXiv:2307.03423v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.03423</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperspectral images (HSI) have a large amount of spectral information
reflecting the characteristics of matter, while their spatial resolution is low
due to the limitations of imaging technology. Complementary to this are
multispectral images (MSI), e.g., RGB images, with high spatial resolution but
insufficient spectral bands. Hyperspectral and multispectral image fusion is a
technique for acquiring ideal images that have both high spatial and high
spectral resolution cost-effectively. Many existing HSI and MSI fusion
algorithms rely on known imaging degradation models, which are often not
available in practice. In this paper, we propose a deep fusion method based on
the conditional denoising diffusion probabilistic model, called DDPM-Fus.
Specifically, the DDPM-Fus contains the forward diffusion process which
gradually adds Gaussian noise to the high spatial resolution HSI (HrHSI) and
another reverse denoising process which learns to predict the desired HrHSI
from its noisy version conditioning on the corresponding high spatial
resolution MSI (HrMSI) and low spatial resolution HSI (LrHSI). Once the
training is completes, the proposed DDPM-Fus implements the reverse process on
the test HrMSI and LrHSI to generate the fused HrHSI. Experiments conducted on
one indoor and two remote sensing datasets show the superiority of the proposed
model when compared with other advanced deep learningbased fusion methods. The
codes of this work will be opensourced at this address:
https://github.com/shuaikaishi/DDPMFus for reproducibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_S/0/1/0/all/0/1&quot;&gt;Shuaikai Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lijun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03425">
<title>Registration-Free Hybrid Learning Empowers Simple Multimodal Imaging System for High-quality Fusion Detection. (arXiv:2307.03425v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03425</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal fusion detection always places high demands on the imaging system
and image pre-processing, while either a high-quality pre-registration system
or image registration processing is costly. Unfortunately, the existing fusion
methods are designed for registered source images, and the fusion of
inhomogeneous features, which denotes a pair of features at the same spatial
location that expresses different semantic information, cannot achieve
satisfactory performance via these methods. As a result, we propose IA-VFDnet,
a CNN-Transformer hybrid learning framework with a unified high-quality
multimodal feature matching module (AKM) and a fusion module (WDAF), in which
AKM and DWDAF work in synergy to perform high-quality infrared-aware visible
fusion detection, which can be applied to smoke and wildfire detection.
Furthermore, experiments on the M3FD dataset validate the superiority of the
proposed method, with IA-VFDnet achieving the best detection performance than
other state-of-the-art methods under conventional registered conditions. In
addition, the first unregistered multimodal smoke and wildfire detection
benchmark is openly available in this letter.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1&quot;&gt;Yinghan Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Haoran Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zekuan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shouyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yuanjie Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03427">
<title>Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer. (arXiv:2307.03427v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.03427</link>
<description rdf:parseType="Literal">&lt;p&gt;Survival prediction is crucial for cancer patients as it provides early
prognostic information for treatment planning. Recently, deep survival models
based on deep learning and medical images have shown promising performance for
survival prediction. However, existing deep survival models are not well
developed in utilizing multi-modality images (e.g., PET-CT) and in extracting
region-specific information (e.g., the prognostic information in Primary Tumor
(PT) and Metastatic Lymph Node (MLN) regions). In view of this, we propose a
merging-diverging learning framework for survival prediction from
multi-modality images. This framework has a merging encoder to fuse
multi-modality information and a diverging decoder to extract region-specific
information. In the merging encoder, we propose a Hybrid Parallel
Cross-Attention (HPCA) block to effectively fuse multi-modality features via
parallel convolutional layers and cross-attention transformers. In the
diverging decoder, we propose a Region-specific Attention Gate (RAG) block to
screen out the features related to lesion regions. Our framework is
demonstrated on survival prediction from PET-CT images in Head and Neck (H&amp;amp;N)
cancer, by designing an X-shape merging-diverging hybrid transformer network
(named XSurv). Our XSurv combines the complementary information in PET and CT
images and extracts the region-specific prognostic information in PT and MLN
regions. Extensive experiments on the public dataset of HEad and neCK TumOR
segmentation and outcome prediction challenge (HECKTOR 2022) demonstrate that
our XSurv outperforms state-of-the-art survival prediction methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meng_M/0/1/0/all/0/1&quot;&gt;Mingyuan Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bi_L/0/1/0/all/0/1&quot;&gt;Lei Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fulham_M/0/1/0/all/0/1&quot;&gt;Michael Fulham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_D/0/1/0/all/0/1&quot;&gt;Dagan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jinman Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03441">
<title>NOFA: NeRF-based One-shot Facial Avatar Reconstruction. (arXiv:2307.03441v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03441</link>
<description rdf:parseType="Literal">&lt;p&gt;3D facial avatar reconstruction has been a significant research topic in
computer graphics and computer vision, where photo-realistic rendering and
flexible controls over poses and expressions are necessary for many related
applications. Recently, its performance has been greatly improved with the
development of neural radiance fields (NeRF). However, most existing NeRF-based
facial avatars focus on subject-specific reconstruction and reenactment,
requiring multi-shot images containing different views of the specific subject
for training, and the learned model cannot generalize to new identities,
limiting its further applications. In this work, we propose a one-shot 3D
facial avatar reconstruction framework that only requires a single source image
to reconstruct a high-fidelity 3D facial avatar. For the challenges of lacking
generalization ability and missing multi-view information, we leverage the
generative prior of 3D GAN and develop an efficient encoder-decoder network to
reconstruct the canonical neural volume of the source image, and further
propose a compensation network to complement facial details. To enable
fine-grained control over facial dynamics, we propose a deformation field to
warp the canonical volume into driven expressions. Through extensive
experimental comparisons, we achieve superior synthesis results compared to
several state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wangbo Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yanbo Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1&quot;&gt;Fei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yunpeng Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yan-Pei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhongqian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03449">
<title>Universal Semi-supervised Model Adaptation via Collaborative Consistency Training. (arXiv:2307.03449v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03449</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a realistic and challenging domain adaptation
problem called Universal Semi-supervised Model Adaptation (USMA), which i)
requires only a pre-trained source model, ii) allows the source and target
domain to have different label sets, i.e., they share a common label set and
hold their own private label set, and iii) requires only a few labeled samples
in each class of the target domain. To address USMA, we propose a collaborative
consistency training framework that regularizes the prediction consistency
between two models, i.e., a pre-trained source model and its variant
pre-trained with target data only, and combines their complementary strengths
to learn a more powerful model. The rationale of our framework stems from the
observation that the source model performs better on common categories than the
target-only model, while on target-private categories, the target-only model
performs better. We also propose a two-perspective, i.e., sample-wise and
class-wise, consistency regularization to improve the training. Experimental
results demonstrate the effectiveness of our method on several benchmark
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zizheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yushuang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yipeng Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiaoguang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1&quot;&gt;Shuguang Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanbin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03461">
<title>A Deep Active Contour Model for Delineating Glacier Calving Fronts. (arXiv:2307.03461v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03461</link>
<description rdf:parseType="Literal">&lt;p&gt;Choosing how to encode a real-world problem as a machine learning task is an
important design decision in machine learning. The task of glacier calving
front modeling has often been approached as a semantic segmentation task.
Recent studies have shown that combining segmentation with edge detection can
improve the accuracy of calving front detectors. Building on this observation,
we completely rephrase the task as a contour tracing problem and propose a
model for explicit contour detection that does not incorporate any dense
predictions as intermediate steps. The proposed approach, called ``Charting
Outlines by Recurrent Adaptation&apos;&apos; (COBRA), combines Convolutional Neural
Networks (CNNs) for feature extraction and active contour models for the
delineation. By training and evaluating on several large-scale datasets of
Greenland&apos;s outlet glaciers, we show that this approach indeed outperforms the
aforementioned methods based on segmentation and edge-detection. Finally, we
demonstrate that explicit contour detection has benefits over pixel-wise
methods when quantifying the models&apos; prediction uncertainties. The project page
containing the code and animated model predictions can be found at
\url{https://khdlr.github.io/COBRA/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heidler_K/0/1/0/all/0/1&quot;&gt;Konrad Heidler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1&quot;&gt;Lichao Mou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loebel_E/0/1/0/all/0/1&quot;&gt;Erik Loebel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scheinert_M/0/1/0/all/0/1&quot;&gt;Mirko Scheinert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lefevre_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Lef&amp;#xe8;vre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiao Xiang Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03465">
<title>TBGC: Task-level Backbone-Oriented Gradient Clip for Multi-Task Foundation Model Learning. (arXiv:2307.03465v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03465</link>
<description rdf:parseType="Literal">&lt;p&gt;The AllInOne training paradigm squeezes a wide range of tasks into a unified
model in a multi-task learning manner. However, optimization in multi-task
learning is more challenge than single-task learning, as the gradient norm from
different tasks may vary greatly, making the backbone overly biased towards one
specific task. To address this issue, we propose the task-level
backbone-oriented gradient clip paradigm, compared with the vanilla gradient
clip method, it has two points of emphasis:1) gradient clip is performed
independently for each task. 2) backbone gradients generated from each task are
rescaled to the same norm scale. Based on the experimental results, we argue
that the task-level backbone-oriented gradient clip paradigm can relieve the
gradient bias problem to some extent. We also propose a novel multi-branch data
augmentation strategy where conflict augmentations are placed in different
branches. Our approach has been shown to be effective and finally achieve 1st
place in the Leaderboard A and 2nd place in the Leaderboard B of the CVPR2023
Foundation Model Challenge. It&apos;s worth noting that instead of evaluating all
three tasks(detection, segmentation and fine-grained classification) in
Leaderboard A, the segmentation task is not evaluated in Leaderboard B, in
which our team has a huge advantage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zelun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xue Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03475">
<title>Freezing of Gait Prediction From Accelerometer Data Using a Simple 1D-Convolutional Neural Network -- 8th Place Solution for Kaggle&apos;s Parkinson&apos;s Freezing of Gait Prediction Competition. (arXiv:2307.03475v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03475</link>
<description rdf:parseType="Literal">&lt;p&gt;Freezing of Gait (FOG) is a common motor symptom in patients with Parkinson&apos;s
disease (PD). During episodes of FOG, patients suddenly lose their ability to
stride as intended. Patient-worn accelerometers can capture information on the
patient&apos;s movement during these episodes and machine learning algorithms can
potentially classify this data. The combination therefore holds the potential
to detect FOG in real-time. In this work I present a simple 1-D convolutional
neural network that was trained to detect FOG events in accelerometer data.
Model performance was assessed by measuring the success of the model to
discriminate normal movement from FOG episodes and resulted in a mean average
precision of 0.356 on the private leaderboard on Kaggle. Ultimately, the model
ranked 8th out of 1379 teams in the Parkinson&apos;s Freezing of Gait Prediction
competition. The results underscore the potential of Deep Learning-based
solutions in advancing the field of FOG detection, contributing to improved
interventions and management strategies for PD patients.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brederecke_J/0/1/0/all/0/1&quot;&gt;Jan Brederecke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03476">
<title>Unpaired Multi-View Graph Clustering with Cross-View Structure Matching. (arXiv:2307.03476v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03476</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-view clustering (MVC), which effectively fuses information from
multiple views for better performance, has received increasing attention. Most
existing MVC methods assume that multi-view data are fully paired, which means
that the mappings of all corresponding samples between views are pre-defined or
given in advance. However, the data correspondence is often incomplete in
real-world applications due to data corruption or sensor differences, referred
as the data-unpaired problem (DUP) in multi-view literature. Although several
attempts have been made to address the DUP issue, they suffer from the
following drawbacks: 1) Most methods focus on the feature representation while
ignoring the structural information of multi-view data, which is essential for
clustering tasks; 2) Existing methods for partially unpaired problems rely on
pre-given cross-view alignment information, resulting in their inability to
handle fully unpaired problems; 3) Their inevitable parameters degrade the
efficiency and applicability of the models. To tackle these issues, we propose
a novel parameter-free graph clustering framework termed Unpaired Multi-view
Graph Clustering framework with Cross-View Structure Matching (UPMGC-SM).
Specifically, unlike the existing methods, UPMGC-SM effectively utilizes the
structural information from each view to refine cross-view correspondences.
Besides, our UPMGC-SM is a unified framework for both the fully and partially
unpaired multi-view graph clustering. Moreover, existing graph clustering
methods can adopt our UPMGC-SM to enhance their ability for unpaired scenarios.
Extensive experiments demonstrate the effectiveness and generalization of our
proposed framework for both paired and unpaired datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yi Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1&quot;&gt;Qing Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Weixuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Ke Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xinhang Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinwang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03494">
<title>HoughLaneNet: Lane Detection with Deep Hough Transform and Dynamic Convolution. (arXiv:2307.03494v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03494</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of lane detection has garnered considerable attention in the field
of autonomous driving due to its complexity. Lanes can present difficulties for
detection, as they can be narrow, fragmented, and often obscured by heavy
traffic. However, it has been observed that the lanes have a geometrical
structure that resembles a straight line, leading to improved lane detection
results when utilizing this characteristic. To address this challenge, we
propose a hierarchical Deep Hough Transform (DHT) approach that combines all
lane features in an image into the Hough parameter space. Additionally, we
refine the point selection method and incorporate a Dynamic Convolution Module
to effectively differentiate between lanes in the original image. Our network
architecture comprises a backbone network, either a ResNet or Pyramid Vision
Transformer, a Feature Pyramid Network as the neck to extract multi-scale
features, and a hierarchical DHT-based feature aggregation head to accurately
segment each lane. By utilizing the lane features in the Hough parameter space,
the network learns dynamic convolution kernel parameters corresponding to each
lane, allowing the Dynamic Convolution Module to effectively differentiate
between lane features. Subsequently, the lane features are fed into the feature
decoder, which predicts the final position of the lane. Our proposed network
structure demonstrates improved performance in detecting heavily occluded or
worn lane images, as evidenced by our extensive experimental results, which
show that our method outperforms or is on par with state-of-the-art techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jia-Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1&quot;&gt;Hao-Bin Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jun-Long Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1&quot;&gt;Ariel Shamir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Miao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03505">
<title>RCDN -- Robust X-Corner Detection Algorithm based on Advanced CNN Model. (arXiv:2307.03505v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03505</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate detection and localization of X-corner on both planar and non-planar
patterns is a core step in robotics and machine vision. However, previous works
could not make a good balance between accuracy and robustness, which are both
crucial criteria to evaluate the detectors performance. To address this
problem, in this paper we present a novel detection algorithm which can
maintain high sub-pixel precision on inputs under multiple interference, such
as lens distortion, extreme poses and noise. The whole algorithm, adopting a
coarse-to-fine strategy, contains a X-corner detection network and three
post-processing techniques to distinguish the correct corner candidates, as
well as a mixed sub-pixel refinement technique and an improved region growth
strategy to recover the checkerboard pattern partially visible or occluded
automatically. Evaluations on real and synthetic images indicate that the
presented algorithm has the higher detection rate, sub-pixel accuracy and
robustness than other commonly used methods. Finally, experiments of camera
calibration and pose estimation verify it can also get smaller re-projection
error in quantitative comparisons to the state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Ben Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caihua Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Quanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1&quot;&gt;Zhonghua Wan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03512">
<title>Tranfer Learning of Semantic Segmentation Methods for Identifying Buried Archaeological Structures on LiDAR Data. (arXiv:2307.03512v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03512</link>
<description rdf:parseType="Literal">&lt;p&gt;When applying deep learning to remote sensing data in archaeological
research, a notable obstacle is the limited availability of suitable datasets
for training models. The application of transfer learning is frequently
employed to mitigate this drawback. However, there is still a need to explore
its effectiveness when applied across different archaeological datasets. This
paper compares the performance of various transfer learning configurations
using two semantic segmentation deep neural networks on two LiDAR datasets. The
experimental results indicate that transfer learning-based approaches in
archaeology can lead to performance improvements, although a systematic
enhancement has not yet been observed. We provide specific insights about the
validity of such techniques that can serve as a baseline for future works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soleni_P/0/1/0/all/0/1&quot;&gt;Paolo Soleni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaart_W/0/1/0/all/0/1&quot;&gt;Wouter B. Verschoof-van der Vaart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kokalj_Z/0/1/0/all/0/1&quot;&gt;&amp;#x17d;iga Kokalj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Traviglia_A/0/1/0/all/0/1&quot;&gt;Arianna Traviglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiorucci_M/0/1/0/all/0/1&quot;&gt;Marco Fiorucci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03535">
<title>Matching in the Wild: Learning Anatomical Embeddings for Multi-Modality Images. (arXiv:2307.03535v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03535</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiotherapists require accurate registration of MR/CT images to effectively
use information from both modalities. In a typical registration pipeline, rigid
or affine transformations are applied to roughly align the fixed and moving
images before proceeding with the deformation step. While recent learning-based
methods have shown promising results in the rigid/affine step, these methods
often require images with similar field-of-view (FOV) for successful alignment.
As a result, aligning images with different FOVs remains a challenging task.
Self-supervised landmark detection methods like self-supervised Anatomical
eMbedding (SAM) have emerged as a useful tool for mapping and cropping images
to similar FOVs. However, these methods are currently limited to intra-modality
use only. To address this limitation and enable cross-modality matching, we
propose a new approach called Cross-SAM. Our approach utilizes a novel
iterative process that alternates between embedding learning and CT-MRI
registration. We start by applying aggressive contrast augmentation on both CT
and MRI images to train a SAM model. We then use this SAM to identify
corresponding regions on paired images using robust grid-points matching,
followed by a point-set based affine/rigid registration, and a deformable
fine-tuning step to produce registered paired images. We use these registered
pairs to enhance the matching ability of SAM, which is then processed
iteratively. We use the final model for cross-modality matching tasks. We
evaluated our approach on two CT-MRI affine registration datasets and found
that Cross-SAM achieved robust affine registration on both datasets,
significantly outperforming other methods and achieving state-of-the-art
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_F/0/1/0/all/0/1&quot;&gt;Fan Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_X/0/1/0/all/0/1&quot;&gt;Xiaofei Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1&quot;&gt;Jia Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mok_T/0/1/0/all/0/1&quot;&gt;Tony C. W. Mok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Minfeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Le Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1&quot;&gt;Dakai Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xianghua Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jingjing Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Ke Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03536">
<title>Joint Perceptual Learning for Enhancement and Object Detection in Underwater Scenarios. (arXiv:2307.03536v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03536</link>
<description rdf:parseType="Literal">&lt;p&gt;Underwater degraded images greatly challenge existing algorithms to detect
objects of interest. Recently, researchers attempt to adopt attention
mechanisms or composite connections for improving the feature representation of
detectors. However, this solution does \textit{not} eliminate the impact of
degradation on image content such as color and texture, achieving minimal
improvements. Another feasible solution for underwater object detection is to
develop sophisticated deep architectures in order to enhance image quality or
features. Nevertheless, the visually appealing output of these enhancement
modules do \textit{not} necessarily generate high accuracy for deep detectors.
More recently, some multi-task learning methods jointly learn underwater
detection and image enhancement, accessing promising improvements. Typically,
these methods invoke huge architecture and expensive computations, rendering
inefficient inference. Definitely, underwater object detection and image
enhancement are two interrelated tasks. Leveraging information coming from the
two tasks can benefit each task. Based on these factual opinions, we propose a
bilevel optimization formulation for jointly learning underwater object
detection and image enhancement, and then unroll to a dual perception network
(DPNet) for the two tasks. DPNet with one shared module and two task subnets
learns from the two different tasks, seeking a shared representation. The
shared representation provides more structural details for image enhancement
and rich content information for object detection. Finally, we derive a
cooperative training strategy to optimize parameters for DPNet. Extensive
experiments on real-world and synthetic underwater datasets demonstrate that
our method outputs visually favoring images and higher detection accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1&quot;&gt;Chenping Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Wanqi Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jiewen Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Risheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xin Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03538">
<title>Language-free Compositional Action Generation via Decoupling Refinement. (arXiv:2307.03538v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03538</link>
<description rdf:parseType="Literal">&lt;p&gt;Composing simple elements into complex concepts is crucial yet challenging,
especially for 3D action generation. Existing methods largely rely on extensive
neural language annotations to discern composable latent semantics, a process
that is often costly and labor-intensive. In this study, we introduce a novel
framework to generate compositional actions without reliance on language
auxiliaries. Our approach consists of three main components: Action Coupling,
Conditional Action Generation, and Decoupling Refinement. Action Coupling
utilizes an energy model to extract the attention masks of each sub-action,
subsequently integrating two actions using these attentions to generate
pseudo-training examples. Then, we employ a conditional generative model, CVAE,
to learn a latent space, facilitating the diverse generation. Finally, we
propose Decoupling Refinement, which leverages a self-supervised pre-trained
model MAE to ensure semantic consistency between the sub-actions and
compositional actions. This refinement process involves rendering generated 3D
actions into 2D space, decoupling these images into two sub-segments, using the
MAE model to restore the complete image from sub-segments, and constraining the
recovered images to match images rendered from raw sub-actions. Due to the lack
of existing datasets containing both sub-actions and compositional actions, we
created two new datasets, named HumanAct-C and UESTC-C, and present a
corresponding evaluation metric. Both qualitative and quantitative assessments
are conducted to show our efficacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guangyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yansong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guangrun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Ser-Nam Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03553">
<title>VariGrad: A Novel Feature Vector Architecture for Geometric Deep Learning on Unregistered Data. (arXiv:2307.03553v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03553</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel geometric deep learning layer that leverages the varifold
gradient (VariGrad) to compute feature vector representations of 3D geometric
data. These feature vectors can be used in a variety of downstream learning
tasks such as classification, registration, and shape reconstruction. Our
model&apos;s use of parameterization independent varifold representations of
geometric data allows our model to be both trained and tested on data
independent of the given sampling or parameterization. We demonstrate the
efficiency, generalizability, and robustness to resampling demonstrated by the
proposed VariGrad layer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartman_E/0/1/0/all/0/1&quot;&gt;Emmanuel Hartman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pierson_E/0/1/0/all/0/1&quot;&gt;Emery Pierson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03567">
<title>SpawnNet: Learning Generalizable Visuomotor Skills from Pre-trained Networks. (arXiv:2307.03567v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.03567</link>
<description rdf:parseType="Literal">&lt;p&gt;The existing internet-scale image and video datasets cover a wide range of
everyday objects and tasks, bringing the potential of learning policies that
have broad generalization. Prior works have explored visual pre-training with
different self-supervised objectives, but the generalization capabilities of
the learned policies remain relatively unknown. In this work, we take the first
step towards this challenge, focusing on how pre-trained representations can
help the generalization of the learned policies. We first identify the key
bottleneck in using a frozen pre-trained visual backbone for policy learning.
We then propose SpawnNet, a novel two-stream architecture that learns to fuse
pre-trained multi-layer representations into a separate network to learn a
robust policy. Through extensive simulated and real experiments, we demonstrate
significantly better categorical generalization compared to prior approaches in
imitation learning settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xingyu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+So_J/0/1/0/all/0/1&quot;&gt;John So&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahalingam_S/0/1/0/all/0/1&quot;&gt;Sashwat Mahalingam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fangchen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03575">
<title>Multimodal Deep Learning for Personalized Renal Cell Carcinoma Prognosis: Integrating CT Imaging and Clinical Data. (arXiv:2307.03575v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03575</link>
<description rdf:parseType="Literal">&lt;p&gt;Renal cell carcinoma represents a significant global health challenge with a
low survival rate. This research aimed to devise a comprehensive deep-learning
model capable of predicting survival probabilities in patients with renal cell
carcinoma by integrating CT imaging and clinical data and addressing the
limitations observed in prior studies. The aim is to facilitate the
identification of patients requiring urgent treatment. The proposed framework
comprises three modules: a 3D image feature extractor, clinical variable
selection, and survival prediction. The feature extractor module, based on the
3D CNN architecture, predicts the ISUP grade of renal cell carcinoma tumors
linked to mortality rates from CT images. A selection of clinical variables is
systematically chosen using the Spearman score and random forest importance
score as criteria. A deep learning-based network, trained with discrete
LogisticHazard-based loss, performs the survival prediction. Nine distinct
experiments are performed, with varying numbers of clinical variables
determined by different thresholds of the Spearman and importance scores. Our
findings demonstrate that the proposed strategy surpasses the current
literature on renal cancer prognosis based on CT scans and clinical factors.
The best-performing experiment yielded a concordance index of 0.84 and an area
under the curve value of 0.8 on the test cohort, which suggests strong
predictive power. The multimodal deep-learning approach developed in this study
shows promising results in estimating survival probabilities for renal cell
carcinoma patients using CT imaging and clinical data. This may have potential
implications in identifying patients who require urgent treatment, potentially
improving patient outcomes. The code created for this project is available for
the public on:
\href{https://github.com/Balasingham-AI-Group/Survival_CTplusClinical}{GitHub}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahootiha_M/0/1/0/all/0/1&quot;&gt;Maryamalsadat Mahootiha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qadir_H/0/1/0/all/0/1&quot;&gt;Hemin Ali Qadir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bergsland_J/0/1/0/all/0/1&quot;&gt;Jacob Bergsland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasingham_I/0/1/0/all/0/1&quot;&gt;Ilangko Balasingham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03579">
<title>Unsupervised Segmentation of Fetal Brain MRI using Deep Learning Cascaded Registration. (arXiv:2307.03579v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03579</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate segmentation of fetal brain magnetic resonance images is crucial for
analyzing fetal brain development and detecting potential neurodevelopmental
abnormalities. Traditional deep learning-based automatic segmentation, although
effective, requires extensive training data with ground-truth labels, typically
produced by clinicians through a time-consuming annotation process. To overcome
this challenge, we propose a novel unsupervised segmentation method based on
multi-atlas segmentation, that accurately segments multiple tissues without
relying on labeled data for training. Our method employs a cascaded deep
learning network for 3D image registration, which computes small, incremental
deformations to the moving image to align it precisely with the fixed image.
This cascaded network can then be used to register multiple annotated images
with the image to be segmented, and combine the propagated labels to form a
refined segmentation. Our experiments demonstrate that the proposed cascaded
architecture outperforms the state-of-the-art registration methods that were
tested. Furthermore, the derived segmentation method achieves similar
performance and inference time to nnU-Net while only using a small subset of
annotated data for the multi-atlas segmentation task and none for training the
network. Our pipeline for registration and multi-atlas segmentation is publicly
available at https://github.com/ValBcn/CasReg.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Comte_V/0/1/0/all/0/1&quot;&gt;Valentin Comte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alenya_M/0/1/0/all/0/1&quot;&gt;Mireia Alenya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urru_A/0/1/0/all/0/1&quot;&gt;Andrea Urru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Recober_J/0/1/0/all/0/1&quot;&gt;Judith Recober&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakaki_A/0/1/0/all/0/1&quot;&gt;Ayako Nakaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crovetto_F/0/1/0/all/0/1&quot;&gt;Francesca Crovetto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camara_O/0/1/0/all/0/1&quot;&gt;Oscar Camara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gratacos_E/0/1/0/all/0/1&quot;&gt;Eduard Gratac&amp;#xf3;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eixarch_E/0/1/0/all/0/1&quot;&gt;Elisenda Eixarch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crispi_F/0/1/0/all/0/1&quot;&gt;F&amp;#xe0;tima Crispi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piella_G/0/1/0/all/0/1&quot;&gt;Gemma Piella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ceresa_M/0/1/0/all/0/1&quot;&gt;Mario Ceresa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ballester_M/0/1/0/all/0/1&quot;&gt;Miguel A. Gonz&amp;#xe1;lez Ballester&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03592">
<title>VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis. (arXiv:2307.03592v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03592</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a data-driven generative framework for synthesizing blood vessel
3D geometry. This is a challenging task due to the complexity of vascular
systems, which are highly variating in shape, size, and structure. Existing
model-based methods provide some degree of control and variation in the
structures produced, but fail to capture the diversity of actual anatomical
data. We developed VesselVAE, a recursive variational Neural Network that fully
exploits the hierarchical organization of the vessel and learns a
low-dimensional manifold encoding branch connectivity along with geometry
features describing the target surface. After training, the VesselVAE latent
space can be sampled to generate new vessel geometries. To the best of our
knowledge, this work is the first to utilize this technique for synthesizing
blood vessels. We achieve similarities of synthetic and real data for radius
(.97), length (.95), and tortuosity (.96). By leveraging the power of deep
neural networks, we generate 3D models of blood vessels that are both accurate
and diverse, which is crucial for medical and surgical training, hemodynamic
simulations, and many other purposes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldman_P/0/1/0/all/0/1&quot;&gt;Paula Feldman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fainstein_M/0/1/0/all/0/1&quot;&gt;Miguel Fainstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siless_V/0/1/0/all/0/1&quot;&gt;Viviana Siless&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delrieux_C/0/1/0/all/0/1&quot;&gt;Claudio Delrieux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iarussi_E/0/1/0/all/0/1&quot;&gt;Emmanuel Iarussi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03601">
<title>GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest. (arXiv:2307.03601v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03601</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction tuning large language model (LLM) on image-text pairs has
achieved unprecedented vision-language multimodal abilities. However, their
vision-language alignments are only built on image-level, the lack of
region-level alignment limits their advancements to fine-grained multimodal
understanding. In this paper, we propose instruction tuning on
region-of-interest. The key design is to reformulate the bounding box as the
format of spatial instruction. The interleaved sequences of visual features
extracted by the spatial instruction and the language embedding are input to
LLM, and trained on the transformed region-text data in instruction tuning
format. Our region-level vision-language model, termed as GPT4RoI, brings brand
new conversational and interactive experience beyond image-level understanding.
(1) Controllability: Users can interact with our model by both language and
spatial instructions to flexibly adjust the detail level of the question. (2)
Capacities: Our model supports not only single-region spatial instruction but
also multi-region. This unlocks more region-level multimodal capacities such as
detailed region caption and complex region reasoning. (3) Composition: Any
off-the-shelf object detector can be a spatial instruction provider so as to
mine informative object attributes from our model, like color, shape, material,
action, relation to other objects, etc. The code, data, and demo can be found
at https://github.com/jshilong/GPT4RoI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shilong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1&quot;&gt;Peize Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shoufa Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1&quot;&gt;Min Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1&quot;&gt;Wenqi Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03602">
<title>Depth Estimation Analysis of Orthogonally Divergent Fisheye Cameras with Distortion Removal. (arXiv:2307.03602v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03602</link>
<description rdf:parseType="Literal">&lt;p&gt;Stereo vision systems have become popular in computer vision applications,
such as 3D reconstruction, object tracking, and autonomous navigation. However,
traditional stereo vision systems that use rectilinear lenses may not be
suitable for certain scenarios due to their limited field of view. This has led
to the popularity of vision systems based on one or multiple fisheye cameras in
different orientations, which can provide a field of view of 180x180 degrees or
more. However, fisheye cameras introduce significant distortion at the edges
that affects the accuracy of stereo matching and depth estimation. To overcome
these limitations, this paper proposes a method for distortion-removal and
depth estimation analysis for stereovision system using orthogonally divergent
fisheye cameras (ODFC). The proposed method uses two virtual pinhole cameras
(VPC), each VPC captures a small portion of the original view and presents it
without any lens distortions, emulating the behavior of a pinhole camera. By
carefully selecting the captured regions, it is possible to create a stereo
pair using two VPCs. The performance of the proposed method is evaluated in
both simulation using virtual environment and experiments using real cameras
and their results compared to stereo cameras with parallel optical axes. The
results demonstrate the effectiveness of the proposed method in terms of
distortion removal and depth estimation accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panteleev_M/0/1/0/all/0/1&quot;&gt;Matvei Panteleev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bettahar_H/0/1/0/all/0/1&quot;&gt;Houari Bettahar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03623">
<title>Robust Human Detection under Visual Degradation via Thermal and mmWave Radar Fusion. (arXiv:2307.03623v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03623</link>
<description rdf:parseType="Literal">&lt;p&gt;The majority of human detection methods rely on the sensor using visible
lights (e.g., RGB cameras) but such sensors are limited in scenarios with
degraded vision conditions. In this paper, we present a multimodal human
detection system that combines portable thermal cameras and single-chip mmWave
radars. To mitigate the noisy detection features caused by the low contrast of
thermal cameras and the multi-path noise of radar point clouds, we propose a
Bayesian feature extractor and a novel uncertainty-guided fusion method that
surpasses a variety of competing methods, either single-modal or multi-modal.
We evaluate the proposed method on real-world data collection and demonstrate
that our approach outperforms the state-of-the-art methods by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_K/0/1/0/all/0/1&quot;&gt;Kaiwen Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1&quot;&gt;Qiyue Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peize Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stankovic_J/0/1/0/all/0/1&quot;&gt;John Stankovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chris Xiaoxuan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03662">
<title>Detecting the Sensing Area of A Laparoscopic Probe in Minimally Invasive Cancer Surgery. (arXiv:2307.03662v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.03662</link>
<description rdf:parseType="Literal">&lt;p&gt;In surgical oncology, it is challenging for surgeons to identify lymph nodes
and completely resect cancer even with pre-operative imaging systems like PET
and CT, because of the lack of reliable intraoperative visualization tools.
Endoscopic radio-guided cancer detection and resection has recently been
evaluated whereby a novel tethered laparoscopic gamma detector is used to
localize a preoperatively injected radiotracer. This can both enhance the
endoscopic imaging and complement preoperative nuclear imaging data. However,
gamma activity visualization is challenging to present to the operator because
the probe is non-imaging and it does not visibly indicate the activity
origination on the tissue surface. Initial failed attempts used segmentation or
geometric methods, but led to the discovery that it could be resolved by
leveraging high-dimensional image features and probe position information. To
demonstrate the effectiveness of this solution, we designed and implemented a
simple regression network that successfully addressed the problem. To further
validate the proposed solution, we acquired and publicly released two datasets
captured using a custom-designed, portable stereo laparoscope system. Through
intensive experimentation, we demonstrated that our method can successfully and
effectively detect the sensing area, establishing a new performance benchmark.
Code and data are available at
https://github.com/br0202/Sensing_area_detection.git
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Baoru Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yicheng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Giannarou_S/0/1/0/all/0/1&quot;&gt;Stamatia Giannarou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Elson_D/0/1/0/all/0/1&quot;&gt;Daniel S. Elson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03698">
<title>Motion Magnification in Robotic Sonography: Enabling Pulsation-Aware Artery Segmentation. (arXiv:2307.03698v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.03698</link>
<description rdf:parseType="Literal">&lt;p&gt;Ultrasound (US) imaging is widely used for diagnosing and monitoring arterial
diseases, mainly due to the advantages of being non-invasive, radiation-free,
and real-time. In order to provide additional information to assist clinicians
in diagnosis, the tubular structures are often segmented from US images. To
improve the artery segmentation accuracy and stability during scans, this work
presents a novel pulsation-assisted segmentation neural network (PAS-NN) by
explicitly taking advantage of the cardiac-induced motions. Motion
magnification techniques are employed to amplify the subtle motion within the
frequency band of interest to extract the pulsation signals from sequential US
images. The extracted real-time pulsation information can help to locate the
arteries on cross-section US images; therefore, we explicitly integrated the
pulsation into the proposed PAS-NN as attention guidance. Notably, a robotic
arm is necessary to provide stable movement during US imaging since magnifying
the target motions from the US images captured along a scan path is not
manually feasible due to the hand tremor. To validate the proposed robotic US
system for imaging arteries, experiments are carried out on volunteers&apos; carotid
and radial arteries. The results demonstrated that the PAS-NN could achieve
comparable results as state-of-the-art on carotid and can effectively improve
the segmentation performance for small vessels (radial artery).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Dianye Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bi_Y/0/1/0/all/0/1&quot;&gt;Yuan Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhongliang Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03704">
<title>Equivariant Single View Pose Prediction Via Induced and Restricted Representations. (arXiv:2307.03704v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03704</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning about the three-dimensional world from two-dimensional images is a
fundamental problem in computer vision. An ideal neural network architecture
for such tasks would leverage the fact that objects can be rotated and
translated in three dimensions to make predictions about novel images. However,
imposing SO(3)-equivariance on two-dimensional inputs is difficult because the
group of three-dimensional rotations does not have a natural action on the
two-dimensional plane. Specifically, it is possible that an element of SO(3)
will rotate an image out of plane. We show that an algorithm that learns a
three-dimensional representation of the world from two dimensional images must
satisfy certain geometric consistency properties which we formulate as
SO(2)-equivariance constraints. We use the induced and restricted
representations of SO(2) on SO(3) to construct and classify architectures which
satisfy these geometric consistency constraints. We prove that any architecture
which respects said consistency constraints can be realized as an instance of
our construction. We show that three previously proposed neural architectures
for 3D pose prediction are special cases of our construction. We propose a new
algorithm that is a learnable generalization of previously considered methods.
We test our architecture on three pose predictions task and achieve SOTA
results on both the PASCAL3D+ and SYMSOL pose estimation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Howell_O/0/1/0/all/0/1&quot;&gt;Owen Howell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klee_D/0/1/0/all/0/1&quot;&gt;David Klee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biza_O/0/1/0/all/0/1&quot;&gt;Ondrej Biza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Linfeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walters_R/0/1/0/all/0/1&quot;&gt;Robin Walters&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03712">
<title>INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers. (arXiv:2307.03712v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03712</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent rise of large language models (LLMs) has resulted in increased
efforts towards running LLMs at reduced precision. Running LLMs at lower
precision supports resource constraints and furthers their democratization,
enabling users to run billion-parameter LLMs on their personal devices. To
supplement this ongoing effort, we propose INT-FP-QSim: an open-source
simulator that enables flexible evaluation of LLMs and vision transformers at
various numerical precisions and formats. INT-FP-QSim leverages existing
open-source repositories such as TensorRT, QPytorch and AIMET for a combined
simulator that supports various floating point and integer formats. With the
help of our simulator, we survey the impact of different numerical formats on
the performance of LLMs and vision transformers at 4-bit weights and 4-bit or
8-bit activations. We also compare recently proposed methods like Adaptive
Block Floating Point, SmoothQuant, GPTQ and RPTQ on the model performances. We
hope INT-FP-QSim will enable researchers to flexibly simulate models at various
precisions to support further research in quantization of LLMs and vision
transformers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_L/0/1/0/all/0/1&quot;&gt;Lakshmi Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernadskiy_M/0/1/0/all/0/1&quot;&gt;Mikhail Bernadskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madhavan_A/0/1/0/all/0/1&quot;&gt;Arulselvan Madhavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1&quot;&gt;Craig Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basumallik_A/0/1/0/all/0/1&quot;&gt;Ayon Basumallik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bunandar_D/0/1/0/all/0/1&quot;&gt;Darius Bunandar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03741">
<title>Training Ensembles with Inliers and Outliers for Semi-supervised Active Learning. (arXiv:2307.03741v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03741</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep active learning in the presence of outlier examples poses a realistic
yet challenging scenario. Acquiring unlabeled data for annotation requires a
delicate balance between avoiding outliers to conserve the annotation budget
and prioritizing useful inlier examples for effective training. In this work,
we present an approach that leverages three highly synergistic components,
which are identified as key ingredients: joint classifier training with inliers
and outliers, semi-supervised learning through pseudo-labeling, and model
ensembling. Our work demonstrates that ensembling significantly enhances the
accuracy of pseudo-labeling and improves the quality of data acquisition. By
enabling semi-supervision through the joint training process, where outliers
are properly handled, we observe a substantial boost in classifier accuracy
through the use of all available unlabeled examples. Notably, we reveal that
the integration of joint training renders explicit outlier detection
unnecessary; a conventional component for acquisition in prior work. The three
key components align seamlessly with numerous existing approaches. Through
empirical evaluations, we showcase that their combined use leads to a
performance increase. Remarkably, despite its simplicity, our proposed approach
outperforms all other methods in terms of performance. Code:
https://github.com/vladan-stojnic/active-outliers
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stojnic_V/0/1/0/all/0/1&quot;&gt;Vladan Stojni&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laskar_Z/0/1/0/all/0/1&quot;&gt;Zakaria Laskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolias_G/0/1/0/all/0/1&quot;&gt;Giorgos Tolias&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2007.10665">
<title>Segmentation of the Left Ventricle by SDD double threshold selection and CHT. (arXiv:2007.10665v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2007.10665</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic and robust segmentation of the left ventricle (LV) in magnetic
resonance images (MRI) has remained challenging for many decades. With the
great success of deep learning in object detection and classification, the
research focus of LV segmentation has changed to convolutional neural network
(CNN) in recent years. However, LV segmentation is a pixel-level classification
problem and its categories are intractable compared to object detection and
classification. In this paper, we proposed a robust LV segmentation method
based on slope difference distribution (SDD) double threshold selection and
circular Hough transform (CHT). The proposed method achieved 96.51% DICE score
on the test set of automated cardiac diagnosis challenge (ACDC) which is higher
than the best accuracy reported in recently published literatures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;ZiHao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;ZhenZhou Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.01708">
<title>MRI-based Multi-task Decoupling Learning for Alzheimer&apos;s Disease Detection and MMSE Score Prediction: A Multi-site Validation. (arXiv:2204.01708v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.01708</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately detecting Alzheimer&apos;s disease (AD) and predicting mini-mental
state examination (MMSE) score are important tasks in elderly health by
magnetic resonance imaging (MRI). Most of the previous methods on these two
tasks are based on single-task learning and rarely consider the correlation
between them. Since the MMSE score, which is an important basis for AD
diagnosis, can also reflect the progress of cognitive impairment, some studies
have begun to apply multi-task learning methods to these two tasks. However,
how to exploit feature correlation remains a challenging problem for these
methods. To comprehensively address this challenge, we propose a MRI-based
multi-task decoupled learning method for AD detection and MMSE score
prediction. First, a multi-task learning network is proposed to implement AD
detection and MMSE score prediction, which exploits feature correlation by
adding three multi-task interaction layers between the backbones of the two
tasks. Each multi-task interaction layer contains two feature decoupling
modules and one feature interaction module. Furthermore, to enhance the
generalization between tasks of the features selected by the feature decoupling
module, we propose the feature consistency loss constrained feature decoupling
module. Finally, in order to exploit the specific distribution information of
MMSE score in different groups, a distribution loss is proposed to further
enhance the model performance. We evaluate our proposed method on multi-site
datasets. Experimental results show that our proposed multi-task decoupled
representation learning method achieves good performance, outperforming
single-task learning and other existing state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tian_X/0/1/0/all/0/1&quot;&gt;Xu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kuang_H/0/1/0/all/0/1&quot;&gt;Hulin Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sheng_Y/0/1/0/all/0/1&quot;&gt;Yu Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianxin Wang&lt;/a&gt;, The &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Initiative_A/0/1/0/all/0/1&quot;&gt;Alzheimer&amp;#x27;s Disease Neuroimaging Initiative&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.09706">
<title>k-strip: A novel segmentation algorithm in k-space for the application of skull stripping. (arXiv:2205.09706v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.09706</link>
<description rdf:parseType="Literal">&lt;p&gt;Objectives: Present a novel deep learning-based skull stripping algorithm for
magnetic resonance imaging (MRI) that works directly in the information rich
k-space.
&lt;/p&gt;
&lt;p&gt;Materials and Methods: Using two datasets from different institutions with a
total of 36,900 MRI slices, we trained a deep learning-based model to work
directly with the complex raw k-space data. Skull stripping performed by HD-BET
(Brain Extraction Tool) in the image domain were used as the ground truth.
&lt;/p&gt;
&lt;p&gt;Results: Both datasets were very similar to the ground truth (DICE scores of
92\%-98\% and Hausdorff distances of under 5.5 mm). Results on slices above the
eye-region reach DICE scores of up to 99\%, while the accuracy drops in regions
around the eyes and below, with partially blurred output. The output of k-strip
often smoothed edges at the demarcation to the skull. Binary masks are created
with an appropriate threshold.
&lt;/p&gt;
&lt;p&gt;Conclusion: With this proof-of-concept study, we were able to show the
feasibility of working in the k-space frequency domain, preserving phase
information, with consistent results. Future research should be dedicated to
discovering additional ways the k-space can be used for innovative image
analysis and further workflows.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rempe_M/0/1/0/all/0/1&quot;&gt;Moritz Rempe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mentzel_F/0/1/0/all/0/1&quot;&gt;Florian Mentzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pomykala_K/0/1/0/all/0/1&quot;&gt;Kelsey L. Pomykala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Haubold_J/0/1/0/all/0/1&quot;&gt;Johannes Haubold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nensa_F/0/1/0/all/0/1&quot;&gt;Felix Nensa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kroninger_K/0/1/0/all/0/1&quot;&gt;Kevin Kr&amp;#xf6;ninger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Egger_J/0/1/0/all/0/1&quot;&gt;Jan Egger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kleesiek_J/0/1/0/all/0/1&quot;&gt;Jens Kleesiek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.13682">
<title>ANISE: Assembly-based Neural Implicit Surface rEconstruction. (arXiv:2205.13682v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.13682</link>
<description rdf:parseType="Literal">&lt;p&gt;We present ANISE, a method that reconstructs a 3D~shape from partial
observations (images or sparse point clouds) using a part-aware neural implicit
shape representation. The shape is formulated as an assembly of neural implicit
functions, each representing a different part instance. In contrast to previous
approaches, the prediction of this representation proceeds in a coarse-to-fine
manner. Our model first reconstructs a structural arrangement of the shape in
the form of geometric transformations of its part instances. Conditioned on
them, the model predicts part latent codes encoding their surface geometry.
Reconstructions can be obtained in two ways: (i) by directly decoding the part
latent codes to part implicit functions, then combining them into the final
shape; or (ii) by using part latents to retrieve similar part instances in a
part database and assembling them in a single shape. We demonstrate that, when
performing reconstruction by decoding part representations into implicit
functions, our method achieves state-of-the-art part-aware reconstruction
results from both images and sparse point clouds.When reconstructing shapes by
assembling parts retrieved from a dataset, our approach significantly
outperforms traditional shape retrieval methods even when significantly
restricting the database size. We present our results in well-known sparse
point cloud reconstruction and single-view reconstruction benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrov_D/0/1/0/all/0/1&quot;&gt;Dmitry Petrov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gadelha_M/0/1/0/all/0/1&quot;&gt;Matheus Gadelha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mech_R/0/1/0/all/0/1&quot;&gt;Radomir Mech&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalogerakis_E/0/1/0/all/0/1&quot;&gt;Evangelos Kalogerakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.08340">
<title>Dual Modality Prompt Tuning for Vision-Language Pre-Trained Model. (arXiv:2208.08340v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.08340</link>
<description rdf:parseType="Literal">&lt;p&gt;With the emergence of large pre-trained vison-language model like CLIP,
transferable representations can be adapted to a wide range of downstream tasks
via prompt tuning. Prompt tuning tries to probe the beneficial information for
downstream tasks from the general knowledge stored in the pre-trained model. A
recently proposed method named Context Optimization (CoOp) introduces a set of
learnable vectors as text prompt from the language side. However, tuning the
text prompt alone can only adjust the synthesized &quot;classifier&quot;, while the
computed visual features of the image encoder can not be affected , thus
leading to sub-optimal solutions. In this paper, we propose a novel
Dual-modality Prompt Tuning (DPT) paradigm through learning text and visual
prompts simultaneously. To make the final image feature concentrate more on the
target visual concept, a Class-Aware Visual Prompt Tuning (CAVPT) scheme is
further proposed in our DPT, where the class-aware visual prompt is generated
dynamically by performing the cross attention between text prompts features and
image patch token embeddings to encode both the downstream task-related
information and visual instance information. Extensive experimental results on
11 datasets demonstrate the effectiveness and generalization ability of the
proposed method. Our code is available in https://github.com/fanrena/DPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1&quot;&gt;Yinghui Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qirui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1&quot;&gt;De Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shizhou Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1&quot;&gt;Guoqiang Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanning Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.12817">
<title>Word to Sentence Visual Semantic Similarity for Caption Generation: Lessons Learned. (arXiv:2209.12817v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2209.12817</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper focuses on enhancing the captions generated by image-caption
generation systems. We propose an approach for improving caption generation
systems by choosing the most closely related output to the image rather than
the most likely output produced by the model. Our model revises the language
generation output beam search from a visual context perspective. We employ a
visual semantic measure in a word and sentence level manner to match the proper
caption to the related information in the image. The proposed approach can be
applied to any caption system as a post-processing based method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabir_A/0/1/0/all/0/1&quot;&gt;Ahmed Sabir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02404">
<title>Tensor Robust PCA with Nonconvex and Nonlocal Regularization. (arXiv:2211.02404v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.02404</link>
<description rdf:parseType="Literal">&lt;p&gt;Tensor robust principal component analysis (TRPCA) is a classical way for
low-rank tensor recovery, which minimizes the convex surrogate of tensor rank
by shrinking each tensor singular value equally. However, for real-world visual
data, large singular values represent more significant information than small
singular values. In this paper, we propose a nonconvex TRPCA (N-TRPCA) model
based on the tensor adjustable logarithmic norm. Unlike TRPCA, our N-TRPCA can
adaptively shrink small singular values more and shrink large singular values
less. In addition, TRPCA assumes that the whole data tensor is of low rank.
This assumption is hardly satisfied in practice for natural visual data,
restricting the capability of TRPCA to recover the edges and texture details
from noisy images and videos. To this end, we integrate nonlocal
self-similarity into N-TRPCA, and further develop a nonconvex and nonlocal
TRPCA (NN-TRPCA) model. Specifically, similar nonlocal patches are grouped as a
tensor and then each group tensor is recovered by our N-TRPCA. Since the
patches in one group are highly correlated, all group tensors have strong
low-rank property, leading to an improvement of recovery performance.
Experimental results demonstrate that the proposed NN-TRPCA outperforms
existing TRPCA methods in visual data recovery. The demo code is available at
https://github.com/qguo2010/NN-TRPCA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qiang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_S/0/1/0/all/0/1&quot;&gt;Shuaixiong Hui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Caiming Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.00313">
<title>Concealed Object Detection for Passive Millimeter-Wave Security Imaging Based on Task-Aligned Detection Transformer. (arXiv:2212.00313v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.00313</link>
<description rdf:parseType="Literal">&lt;p&gt;Passive millimeter-wave (PMMW) is a significant potential technique for human
security screening. Several popular object detection networks have been used
for PMMW images. However, restricted by the low resolution and high noise of
PMMW images, PMMW hidden object detection based on deep learning usually
suffers from low accuracy and low classification confidence. To tackle the
above problems, this paper proposes a Task-Aligned Detection Transformer
network, named PMMW-DETR. In the first stage, a Denoising Coarse-to-Fine
Transformer (DCFT) backbone is designed to extract long- and short-range
features in the different scales. In the second stage, we propose the Query
Selection module to introduce learned spatial features into the network as
prior knowledge, which enhances the semantic perception capability of the
network. In the third stage, aiming to improve the classification performance,
we perform a Task-Aligned Dual-Head block to decouple the classification and
regression tasks. Based on our self-developed PMMW security screening dataset,
experimental results including comparison with State-Of-The-Art (SOTA) methods
and ablation study demonstrate that the PMMW-DETR obtains higher accuracy and
classification confidence than previous works, and exhibits robustness to the
PMMW images of low quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Cheng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1&quot;&gt;Fei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yan Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10289">
<title>Tackling Shortcut Learning in Deep Neural Networks: An Iterative Approach with Interpretable Models. (arXiv:2302.10289v9 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10289</link>
<description rdf:parseType="Literal">&lt;p&gt;We use concept-based interpretable models to mitigate shortcut learning.
Existing methods lack interpretability. Beginning with a Blackbox, we
iteratively carve out a mixture of interpretable experts (MoIE) and a residual
network. Each expert explains a subset of data using First Order Logic (FOL).
While explaining a sample, the FOL from biased BB-derived MoIE detects the
shortcut effectively. Finetuning the BB with Metadata Normalization (MDN)
eliminates the shortcut. The FOLs from the finetuned-BB-derived MoIE verify the
elimination of the shortcut. Our experiments show that MoIE does not hurt the
accuracy of the original BB and eliminates shortcuts effectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Shantanu Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Ke Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arabshahi_F/0/1/0/all/0/1&quot;&gt;Forough Arabshahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1&quot;&gt;Kayhan Batmanghelich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12589">
<title>Revisiting Modality Imbalance In Multimodal Pedestrian Detection. (arXiv:2302.12589v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12589</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal learning, particularly for pedestrian detection, has recently
received emphasis due to its capability to function equally well in several
critical autonomous driving scenarios such as low-light, night-time, and
adverse weather conditions. However, in most cases, the training distribution
largely emphasizes the contribution of one specific input that makes the
network biased towards one modality. Hence, the generalization of such models
becomes a significant problem where the non-dominant input modality during
training could be contributing more to the course of inference. Here, we
introduce a novel training setup with regularizer in the multimodal
architecture to resolve the problem of this disparity between the modalities.
Specifically, our regularizer term helps to make the feature fusion method more
robust by considering both the feature extractors equivalently important during
the training to extract the multimodal distribution which is referred to as
removing the imbalance problem. Furthermore, our decoupling concept of output
stream helps the detection task by sharing the spatial sensitive information
mutually. Extensive experiments of the proposed method on KAIST and UTokyo
datasets shows improvement of the respective state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Arindam Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1&quot;&gt;Sudip Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1&quot;&gt;Ganesh Sistu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horgan_J/0/1/0/all/0/1&quot;&gt;Jonathan Horgan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1&quot;&gt;Ujjwal Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_E/0/1/0/all/0/1&quot;&gt;Edward Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glavin_M/0/1/0/all/0/1&quot;&gt;Martin Glavin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eising_C/0/1/0/all/0/1&quot;&gt;Ciar&amp;#xe1;n Eising&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09789">
<title>Automatic Interaction and Activity Recognition from Videos of Human Manual Demonstrations with Application to Anomaly Detection. (arXiv:2304.09789v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09789</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new method to describe spatio-temporal relations
between objects and hands, to recognize both interactions and activities within
video demonstrations of manual tasks. The approach exploits Scene Graphs to
extract key interaction features from image sequences while simultaneously
encoding motion patterns and context. Additionally, the method introduces
event-based automatic video segmentation and clustering, which allow for the
grouping of similar events and detect if a monitored activity is executed
correctly. The effectiveness of the approach was demonstrated in two
multi-subject experiments, showing the ability to recognize and cluster
hand-object and object-object interactions without prior knowledge of the
activity, as well as matching the same activity performed by different
subjects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merlo_E/0/1/0/all/0/1&quot;&gt;Elena Merlo&lt;/a&gt; (1, 2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lagomarsino_M/0/1/0/all/0/1&quot;&gt;Marta Lagomarsino&lt;/a&gt; (1, 3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamon_E/0/1/0/all/0/1&quot;&gt;Edoardo Lamon&lt;/a&gt; (1, 4), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ajoudani_A/0/1/0/all/0/1&quot;&gt;Arash Ajoudani&lt;/a&gt; (1) ((1) Human-Robot Interfaces and Interaction Laboratory, Istituto Italiano di Tecnologia, Genoa, Italy, (2) Dept. of Informatics, Bioengineering, Robotics, and Systems Engineering, University of Genoa, Genoa, Italy, (3) Dept. of Electronics, Information and Bioengineering, Politecnico di Milano, Milan, Italy, (4) Dept. of Information Engineering and Computer Science, University of Trento, Trento, Italy)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05789">
<title>Unsupervised Domain Adaptation for Medical Image Segmentation via Feature-space Density Matching. (arXiv:2305.05789v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05789</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation is a critical step in automated image interpretation
and analysis where pixels are classified into one or more predefined
semantically meaningful classes. Deep learning approaches for semantic
segmentation rely on harnessing the power of annotated images to learn features
indicative of these semantic classes. Nonetheless, they often fail to
generalize when there is a significant domain (i.e., distributional) shift
between the training (i.e., source) data and the dataset(s) encountered when
deployed (i.e., target), necessitating manual annotations for the target data
to achieve acceptable performance. This is especially important in medical
imaging because different image modalities have significant intra- and
inter-site variations due to protocol and vendor variability. Current
techniques are sensitive to hyperparameter tuning and target dataset size. This
paper presents an unsupervised domain adaptation approach for semantic
segmentation that alleviates the need for annotating target data. Using kernel
density estimation, we match the target data distribution to the source in the
feature space, particularly when the number of target samples is limited (3% of
the target dataset size). We demonstrate the efficacy of our proposed approach
on 2 datasets, multisite prostate MRI and histopathology images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kataria_T/0/1/0/all/0/1&quot;&gt;Tushar Kataria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knudsen_B/0/1/0/all/0/1&quot;&gt;Beatrice Knudsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhabian_S/0/1/0/all/0/1&quot;&gt;Shireen Elhabian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15964">
<title>ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs. (arXiv:2305.15964v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15964</link>
<description rdf:parseType="Literal">&lt;p&gt;The integration of Computer-Assisted Diagnosis (CAD) with Large Language
Models (LLMs) holds great potential in clinical applications, specifically in
the roles of virtual family doctors and clinic assistants. However, current
works in this field are plagued by limitations, specifically a restricted scope
of applicable image domains and the provision of unreliable medical advice.
This restricts their overall processing capabilities. Furthermore, the mismatch
in writing style between LLMs and radiologists undermines their practical
usefulness. To tackle these challenges, we introduce ChatCAD+, which is
designed to be universal and reliable. It is capable of handling medical images
from diverse domains and leveraging up-to-date information from reputable
medical websites to provide reliable medical advice. Additionally, it
incorporates a template retrieval system that improves report generation
performance via exemplar reports. This approach ensures greater consistency
with the expertise of human professionals. The source code is available at
https://github.com/zhaozh10/ChatCAD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zihao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jinchen Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yitao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_L/0/1/0/all/0/1&quot;&gt;Lanzhuju Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1&quot;&gt;Zixu Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1&quot;&gt;Zhiming Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dinggang Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19066">
<title>Nested Diffusion Processes for Anytime Image Generation. (arXiv:2305.19066v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19066</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models are the current state-of-the-art in image generation,
synthesizing high-quality images by breaking down the generation process into
many fine-grained denoising steps. Despite their good performance, diffusion
models are computationally expensive, requiring many neural function
evaluations (NFEs). In this work, we propose an anytime diffusion-based method
that can generate viable images when stopped at arbitrary times before
completion. Using existing pretrained diffusion models, we show that the
generation scheme can be recomposed as two nested diffusion processes, enabling
fast iterative refinement of a generated image. In experiments on ImageNet and
Stable Diffusion-based text-to-image generation, we show, both qualitatively
and quantitatively, that our method&apos;s intermediate generation quality greatly
exceeds that of the original diffusion model, while the final generation result
remains comparable. We illustrate the applicability of Nested Diffusion in
several settings, including for solving inverse problems, and for rapid
text-based content creation by allowing user intervention throughout the
sampling process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elata_N/0/1/0/all/0/1&quot;&gt;Noam Elata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawar_B/0/1/0/all/0/1&quot;&gt;Bahjat Kawar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michaeli_T/0/1/0/all/0/1&quot;&gt;Tomer Michaeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1&quot;&gt;Michael Elad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04719">
<title>Don&apos;t trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04719</link>
<description rdf:parseType="Literal">&lt;p&gt;How do neural networks extract patterns from pixels? Feature visualizations
attempt to answer this important question by visualizing highly activating
patterns through optimization. Today, visualization methods form the foundation
of our knowledge about the internal workings of neural networks, as a type of
mechanistic interpretability. Here we ask: How reliable are feature
visualizations? We start our investigation by developing network circuits that
trick feature visualizations into showing arbitrary patterns that are
completely disconnected from normal network behavior on natural input. We then
provide evidence for a similar phenomenon occurring in standard, unmanipulated
networks: feature visualizations are processed very differently from standard
input, casting doubt on their ability to &quot;explain&quot; how neural networks process
natural images. We underpin this empirical finding by theory proving that the
set of functions that can be reliably understood by feature visualization is
extremely small and does not include general black-box neural networks.
Therefore, a promising way forward could be the development of networks that
enforce certain structures in order to ensure more reliable feature
visualizations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1&quot;&gt;Robert Geirhos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1&quot;&gt;Roland S. Zimmermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilodeau_B/0/1/0/all/0/1&quot;&gt;Blair Bilodeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1&quot;&gt;Wieland Brendel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Been Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06722">
<title>$E(2)$-Equivariant Vision Transformer. (arXiv:2306.06722v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06722</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformer (ViT) has achieved remarkable performance in computer
vision. However, positional encoding in ViT makes it substantially difficult to
learn the intrinsic equivariance in data. Initial attempts have been made on
designing equivariant ViT but are proved defective in some cases in this paper.
To address this issue, we design a Group Equivariant Vision Transformer
(GE-ViT) via a novel, effective positional encoding operator. We prove that
GE-ViT meets all the theoretical requirements of an equivariant neural network.
Comprehensive experiments are conducted on standard benchmark datasets,
demonstrating that GE-ViT significantly outperforms non-equivariant
self-attention networks. The code is available at
https://github.com/ZJUCDSYangKaifan/GEVit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Renjun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kaifan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Ke Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1&quot;&gt;Fengxiang He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07520">
<title>Retrieve Anyone: A General-purpose Person Re-identification Task with Instructions. (arXiv:2306.07520v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07520</link>
<description rdf:parseType="Literal">&lt;p&gt;Human intelligence can retrieve any person according to both visual and
language descriptions. However, the current computer vision community studies
specific person re-identification (ReID) tasks in different scenarios
separately, which limits the applications in the real world. This paper strives
to resolve this problem by proposing a new instruct-ReID task that requires the
model to retrieve images according to the given image or language
instructions.Our instruct-ReID is a more general ReID setting, where existing
ReID tasks can be viewed as special cases by designing different instructions.
We propose a large-scale OmniReID benchmark and an adaptive triplet loss as a
baseline method to facilitate research in this new setting. Experimental
results show that the baseline model trained on our OmniReID benchmark can
improve +0.6%, +1.4%, 0.2% mAP on Market1501, CUHK03, MSMT17 for traditional
ReID, +0.8%, +2.0%, +13.4% mAP on PRCC, VC-Clothes, LTCC for clothes-changing
ReID, +11.7% mAP on COCAS+ real2 for clothestemplate based clothes-changing
ReID when using only RGB images, +25.4% mAP on COCAS+ real2 for our newly
defined language-instructed ReID. The dataset, model, and code will be
available at https://github.com/hwz-zju/Instruct-ReID.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Weizhen He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shixiang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yiheng Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qihao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Qingsong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhou Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Lei Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Feng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_D/0/1/0/all/0/1&quot;&gt;Donglian Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yunfeng Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10274">
<title>Benchmarking Deep Learning Architectures for Urban Vegetation Points Segmentation. (arXiv:2306.10274v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10274</link>
<description rdf:parseType="Literal">&lt;p&gt;Vegetation is crucial for sustainable and resilient cities providing various
ecosystem services and well-being of humans. However, vegetation is under
critical stress with rapid urbanization and expanding infrastructure
footprints. Consequently, mapping of this vegetation is essential in the urban
environment. Recently, deep learning for point cloud semantic segmentation has
shown significant progress. Advanced models attempt to obtain state-of-the-art
performance on benchmark datasets, comprising multiple classes and representing
real world scenarios. However, class specific segmentation with respect to
vegetation points has not been explored. Therefore, selection of a deep
learning model for vegetation points segmentation is ambiguous. To address this
problem, we provide a comprehensive assessment of point-based deep learning
models for semantic segmentation of vegetation class. We have selected four
representative point-based models, namely PointCNN, KPConv (omni-supervised),
RandLANet and SCFNet. These models are investigated on three different
datasets, specifically Chandigarh, Toronto3D and Kerala, which are
characterized by diverse nature of vegetation, varying scene complexity and
changing per-point features. PointCNN achieves the highest mIoU on the
Chandigarh (93.32%) and Kerala datasets (85.68%) while KPConv (omni-supervised)
provides the highest mIoU on the Toronto3D dataset (91.26%). The paper develops
a deeper insight, hitherto not reported, into the working of these models for
vegetation segmentation and outlines the ingredients that should be included in
a model specifically for vegetation segmentation. This paper is a step towards
the development of a novel architecture for vegetation points segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aditya_A/0/1/0/all/0/1&quot;&gt;Aditya Aditya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lohani_B/0/1/0/all/0/1&quot;&gt;Bharat Lohani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aryal_J/0/1/0/all/0/1&quot;&gt;Jagannath Aryal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winter_S/0/1/0/all/0/1&quot;&gt;Stephan Winter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12681">
<title>One at A Time: Multi-step Volumetric Probability Distribution Diffusion for Depth Estimation. (arXiv:2306.12681v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12681</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent works have explored the fundamental role of depth estimation in
multi-view stereo (MVS) and semantic scene completion (SSC). They generally
construct 3D cost volumes to explore geometric correspondence in depth, and
estimate such volumes in a single step relying directly on the ground truth
approximation. However, such problem cannot be thoroughly handled in one step
due to complex empirical distributions, especially in challenging regions like
occlusions, reflections, etc. In this paper, we formulate the depth estimation
task as a multi-step distribution approximation process, and introduce a new
paradigm of modeling the Volumetric Probability Distribution progressively
(step-by-step) following a Markov chain with Diffusion models (VPDD).
Specifically, to constrain the multi-step generation of volume in VPDD, we
construct a meta volume guidance and a confidence-aware contextual guidance as
conditional geometry priors to facilitate the distribution approximation. For
the sampling process, we further investigate an online filtering strategy to
maintain consistency in volume representations for stable training. Experiments
demonstrate that our plug-and-play VPDD outperforms the state-of-the-arts for
tasks of MVS and SSC, and can also be easily extended to different baselines to
get improvement. It is worth mentioning that we are the first camera-based work
that surpasses LiDAR-based methods on the SemanticKITTI dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bohan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jingxin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunnan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1&quot;&gt;Lianying Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xin Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1&quot;&gt;Wenjun Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16050">
<title>Evaluating Similitude and Robustness of Deep Image Denoising Models via Adversarial Attack. (arXiv:2306.16050v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16050</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have shown superior performance comparing to
traditional image denoising algorithms. However, DNNs are inevitably vulnerable
while facing adversarial attacks. In this paper, we propose an adversarial
attack method named denoising-PGD which can successfully attack all the current
deep denoising models while keep the noise distribution almost unchanged. We
surprisingly find that the current mainstream non-blind denoising models
(DnCNN, FFDNet, ECNDNet, BRDNet), blind denoising models (DnCNN-B, Noise2Noise,
RDDCNN-B, FAN), plug-and-play (DPIR, CurvPnP) and unfolding denoising models
(DeamNet) almost share the same adversarial sample set on both grayscale and
color images, respectively. Shared adversarial sample set indicates that all
these models are similar in term of local behaviors at the neighborhood of all
the test samples. Thus, we further propose an indicator to measure the local
similarity of models, called robustness similitude. Non-blind denoising models
are found to have high robustness similitude across each other, while
hybrid-driven models are also found to have high robustness similitude with
pure data-driven non-blind denoising models. According to our robustness
assessment, data-driven non-blind denoising models are the most robust. We use
adversarial training to complement the vulnerability to adversarial attacks.
Moreover, the model-driven image denoising BM3D shows resistance on adversarial
attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_J/0/1/0/all/0/1&quot;&gt;Jie Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiebao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhichang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wangmeng Zuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16873">
<title>Understanding the Overfitting of the Episodic Meta-training. (arXiv:2306.16873v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16873</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the success of two-stage few-shot classification methods, in the
episodic meta-training stage, the model suffers severe overfitting. We
hypothesize that it is caused by over-discrimination, i.e., the model learns to
over-rely on the superficial features that fit for base class discrimination
while suppressing the novel class generalization. To penalize
over-discrimination, we introduce knowledge distillation techniques to keep
novel generalization knowledge from the teacher model during training.
Specifically, we select the teacher model as the one with the best validation
accuracy during meta-training and restrict the symmetric Kullback-Leibler (SKL)
divergence between the output distribution of the linear classifier of the
teacher model and that of the student model. This simple approach outperforms
the standard meta-training process. We further propose the Nearest Neighbor
Symmetric Kullback-Leibler (NNSKL) divergence for meta-training to push the
limits of knowledge distillation techniques. NNSKL takes few-shot tasks as
input and penalizes the output of the nearest neighbor classifier, which
possesses an impact on the relationships between query embedding and support
centers. By combining SKL and NNSKL in meta-training, the model achieves even
better performance and surpasses state-of-the-art results on several
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_S/0/1/0/all/0/1&quot;&gt;Siqi Hui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Sanping Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+deng_Y/0/1/0/all/0/1&quot;&gt;Ye deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinjun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00724">
<title>LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion. (arXiv:2307.00724v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00724</link>
<description rdf:parseType="Literal">&lt;p&gt;As an emerging technology and a relatively affordable device, the 4D imaging
radar has already been confirmed effective in performing 3D object detection in
autonomous driving. Nevertheless, the sparsity and noisiness of 4D radar point
clouds hinder further performance improvement, and in-depth studies about its
fusion with other modalities are lacking. On the other hand, most of the
camera-based perception methods transform the extracted image perspective view
features into the bird&apos;s-eye view geometrically via &quot;depth-based splatting&quot;
proposed in Lift-Splat-Shoot (LSS), and some researchers exploit other modals
such as LiDARs or ordinary automotive radars for enhancement. Recently, a few
works have applied the &quot;sampling&quot; strategy for image view transformation,
showing that it outperforms &quot;splatting&quot; even without image depth prediction.
However, the potential of &quot;sampling&quot; is not fully unleashed. In this paper, we
investigate the &quot;sampling&quot; view transformation strategy on the camera and 4D
imaging radar fusion-based 3D object detection. In the proposed model, LXL,
predicted image depth distribution maps and radar 3D occupancy grids are
utilized to aid image view transformation, called &quot;radar occupancy-assisted
depth-based sampling&quot;. Experiments on VoD and TJ4DRadSet datasets show that the
proposed method outperforms existing 3D object detection methods by a
significant margin without bells and whistles. Ablation studies demonstrate
that our method performs the best among different enhancement settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1&quot;&gt;Weiyi Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1&quot;&gt;Qing-Long Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Bing Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02150">
<title>Harmonizing Feature Attributions Across Deep Learning Architectures: Enhancing Interpretability and Consistency. (arXiv:2307.02150v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02150</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensuring the trustworthiness and interpretability of machine learning models
is critical to their deployment in real-world applications. Feature attribution
methods have gained significant attention, which provide local explanations of
model predictions by attributing importance to individual input features. This
study examines the generalization of feature attributions across various deep
learning architectures, such as convolutional neural networks (CNNs) and vision
transformers. We aim to assess the feasibility of utilizing a feature
attribution method as a future detector and examine how these features can be
harmonized across multiple models employing distinct architectures but trained
on the same data distribution. By exploring this harmonization, we aim to
develop a more coherent and optimistic understanding of feature attributions,
enhancing the consistency of local explanations across diverse deep-learning
models. Our findings highlight the potential for harmonized feature attribution
methods to improve interpretability and foster trust in machine learning
applications, regardless of the underlying architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadir_M/0/1/0/all/0/1&quot;&gt;Md Abdul Kadir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Addluri_G/0/1/0/all/0/1&quot;&gt;Gowtham Krishna Addluri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1&quot;&gt;Daniel Sonntag&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02462">
<title>Expert-Agnostic Ultrasound Image Quality Assessment using Deep Variational Clustering. (arXiv:2307.02462v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02462</link>
<description rdf:parseType="Literal">&lt;p&gt;Ultrasound imaging is a commonly used modality for several diagnostic and
therapeutic procedures. However, the diagnosis by ultrasound relies heavily on
the quality of images assessed manually by sonographers, which diminishes the
objectivity of the diagnosis and makes it operator-dependent. The supervised
learning-based methods for automated quality assessment require manually
annotated datasets, which are highly labour-intensive to acquire. These
ultrasound images are low in quality and suffer from noisy annotations caused
by inter-observer perceptual variations, which hampers learning efficiency. We
propose an UnSupervised UltraSound image Quality assessment Network, US2QNet,
that eliminates the burden and uncertainty of manual annotations. US2QNet uses
the variational autoencoder embedded with the three modules, pre-processing,
clustering and post-processing, to jointly enhance, extract, cluster and
visualize the quality feature representation of ultrasound images. The
pre-processing module uses filtering of images to point the network&apos;s attention
towards salient quality features, rather than getting distracted by noise.
Post-processing is proposed for visualizing the clusters of feature
representations in 2D space. We validated the proposed framework for quality
assessment of the urinary bladder ultrasound images. The proposed framework
achieved 78% accuracy and superior performance to state-of-the-art clustering
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raina_D/0/1/0/all/0/1&quot;&gt;Deepak Raina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ntentia_D/0/1/0/all/0/1&quot;&gt;Dimitrios Ntentia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chandrashekhara_S/0/1/0/all/0/1&quot;&gt;SH Chandrashekhara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Voyles_R/0/1/0/all/0/1&quot;&gt;Richard Voyles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saha_S/0/1/0/all/0/1&quot;&gt;Subir Kumar Saha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02736">
<title>An Uncertainty Aided Framework for Learning based Liver $T_1\rho$ Mapping and Analysis. (arXiv:2307.02736v2 [physics.med-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02736</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: Quantitative $T_1\rho$ imaging has potential for assessment of
biochemical alterations of liver pathologies. Deep learning methods have been
employed to accelerate quantitative $T_1\rho$ imaging. To employ artificial
intelligence-based quantitative imaging methods in complicated clinical
environment, it is valuable to estimate the uncertainty of the predicated
$T_1\rho$ values to provide the confidence level of the quantification results.
The uncertainty should also be utilized to aid the post-hoc quantitative
analysis and model learning tasks. Approach: To address this need, we propose a
parametric map refinement approach for learning-based $T_1\rho$ mapping and
train the model in a probabilistic way to model the uncertainty. We also
propose to utilize the uncertainty map to spatially weight the training of an
improved $T_1\rho$ mapping network to further improve the mapping performance
and to remove pixels with unreliable $T_1\rho$ values in the region of
interest. The framework was tested on a dataset of 51 patients with different
liver fibrosis stages. Main results: Our results indicate that the
learning-based map refinement method leads to a relative mapping error of less
than 3% and provides uncertainty estimation simultaneously. The estimated
uncertainty reflects the actual error level, and it can be used to further
reduce relative $T_1\rho$ mapping error to 2.60% as well as removing unreliable
pixels in the region of interest effectively. Significance: Our studies
demonstrate the proposed approach has potential to provide a learning-based
quantitative MRI system for trustworthy $T_1\rho$ mapping of the liver.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chaoxing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wong_V/0/1/0/all/0/1&quot;&gt;Vincent Wai Sun Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chan_Q/0/1/0/all/0/1&quot;&gt;Queenie Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chu_W/0/1/0/all/0/1&quot;&gt;Winnie Chiu Wing Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weitian Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03177">
<title>IPO-LDM: Depth-aided 360-degree Indoor RGB Panorama Outpainting via Latent Diffusion Model. (arXiv:2307.03177v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03177</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating complete 360-degree panoramas from narrow field of view images is
ongoing research as omnidirectional RGB data is not readily available. Existing
GAN-based approaches face some barriers to achieving higher quality output, and
have poor generalization performance over different mask types. In this paper,
we present our 360-degree indoor RGB panorama outpainting model using latent
diffusion models (LDM), called IPO-LDM. We introduce a new bi-modal latent
diffusion structure that utilizes both RGB and depth panoramic data during
training, but works surprisingly well to outpaint normal depth-free RGB images
during inference. We further propose a novel technique of introducing
progressive camera rotations during each diffusion denoising step, which leads
to substantial improvement in achieving panorama wraparound consistency.
Results show that our IPO-LDM not only significantly outperforms
state-of-the-art methods on RGB panorama outpainting, but can also produce
multiple and diverse well-structured results for different types of masks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tianhao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chuanxia Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cham_T/0/1/0/all/0/1&quot;&gt;Tat-Jen Cham&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>