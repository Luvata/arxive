<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-14T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07593" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07594" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07600" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07603" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07616" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07622" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07623" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07734" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07761" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07765" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07784" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07840" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07864" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07871" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07885" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07912" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07928" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07955" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07956" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07967" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07993" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08007" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08013" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08024" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08075" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08077" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08080" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08083" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08094" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08100" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08148" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08151" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08159" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08172" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08199" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08213" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08223" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08236" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08265" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08278" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.10859" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.03359" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.14191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.08355" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.03461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02914" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.08253" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.09702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06015" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.02867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.02192" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03174" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.05071" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13277" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06210" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12540" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14277" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04156" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07977" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10557" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17105" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01523" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12570" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03402" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01994" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06794" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.07577">
<title>Algorithms for Object Detection in Substations. (arXiv:2311.07577v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07577</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspection of high-voltage power equipment is an effective way to ensure
power supply reliability. Object recognition, one of the key technologies in
automatic power equipment inspection, attracts attention of many researchers
and engineers. Although quite a few existing models have some their own
advantages, object relationship between equipment which is very important in
this task is scarcely considered. This paper combining object relationship
modeling and Transformer Model proposes a Relation Transformer Model. It has
four parts -- backbone, encoder, decoder and prediction heads. With this
structure, the proposed method shows in experiments a much better performance
than other three commonly used models in object recognition in substation,
largely promoting the development of automatic power equipment inspection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1&quot;&gt;Bingying Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yadong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1&quot;&gt;Qinlin Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07578">
<title>A Metacognitive Approach to Out-of-Distribution Detection for Segmentation. (arXiv:2311.07578v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07578</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite outstanding semantic scene segmentation in closed-worlds, deep neural
networks segment novel instances poorly, which is required for autonomous
agents acting in an open world. To improve out-of-distribution (OOD) detection
for segmentation, we introduce a metacognitive approach in the form of a
lightweight module that leverages entropy measures, segmentation predictions,
and spatial context to characterize the segmentation model&apos;s uncertainty and
detect pixel-wise OOD data in real-time. Additionally, our approach
incorporates a novel method of generating synthetic OOD data in context with
in-distribution data, which we use to fine-tune existing segmentation models
with maximum entropy training. This further improves the metacognitive module&apos;s
performance without requiring access to OOD data while enabling compatibility
with established pre-trained models. Our resulting approach can reliably detect
OOD instances in a scene, as shown by state-of-the-art performance on OOD
detection for semantic segmentation benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gummadi_M/0/1/0/all/0/1&quot;&gt;Meghna Gummadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kent_C/0/1/0/all/0/1&quot;&gt;Cassandra Kent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmeckpeper_K/0/1/0/all/0/1&quot;&gt;Karl Schmeckpeper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eaton_E/0/1/0/all/0/1&quot;&gt;Eric Eaton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07593">
<title>Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification. (arXiv:2311.07593v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.07593</link>
<description rdf:parseType="Literal">&lt;p&gt;A promising approach for improving the performance of vision-language models
like CLIP for image classification is to extend the class descriptions (i.e.,
prompts) with related attributes, e.g., using brown sparrow instead of sparrow.
However, current zero-shot methods select a subset of attributes regardless of
commonalities between the target classes, potentially providing no useful
information that would have helped to distinguish between them. For instance,
they may use color instead of bill shape to distinguish between sparrows and
wrens, which are both brown. We propose Follow-up Differential Descriptions
(FuDD), a zero-shot approach that tailors the class descriptions to each
dataset and leads to additional attributes that better differentiate the target
classes. FuDD first identifies the ambiguous classes for each image, and then
uses a Large Language Model (LLM) to generate new class descriptions that
differentiate between them. The new class descriptions resolve the initial
ambiguity and help predict the correct label. In our experiments, FuDD
consistently outperforms generic description ensembles and naive LLM-generated
descriptions on 12 datasets. We show that differential descriptions are an
effective tool to resolve class ambiguities, which otherwise significantly
degrade the performance. We also show that high quality natural language class
descriptions produced by FuDD result in comparable performance to few-shot
adaptation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esfandiarpoor_R/0/1/0/all/0/1&quot;&gt;Reza Esfandiarpoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1&quot;&gt;Stephen H. Bach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07594">
<title>How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model. (arXiv:2311.07594v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.07594</link>
<description rdf:parseType="Literal">&lt;p&gt;This review paper explores Multimodal Large Language Models (MLLMs), which
integrate Large Language Models (LLMs) like GPT-4 to handle multimodal data
such as text and vision. MLLMs demonstrate capabilities like generating image
narratives and answering image-based questions, bridging the gap towards
real-world human-computer interactions and hinting at a potential pathway to
artificial general intelligence. However, MLLMs still face challenges in
processing the semantic gap in multimodality, which may lead to erroneous
generation, posing potential risks to society. Choosing the appropriate
modality alignment method is crucial, as improper methods might require more
parameters with limited performance improvement. This paper aims to explore
modality alignment methods for LLMs and their existing capabilities.
Implementing modality alignment allows LLMs to address environmental issues and
enhance accessibility. The study surveys existing modal alignment methods in
MLLMs into four groups: (1) Multimodal Converters that change data into
something LLMs can understand; (2) Multimodal Perceivers to improve how LLMs
perceive different types of data; (3) Tools Assistance for changing data into
one common format, usually text; and (4) Data-Driven methods that teach LLMs to
understand specific types of data in a dataset. This field is still in a phase
of exploration and experimentation, and we will organize and update various
existing research methods for multimodal information alignment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shezheng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaopeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shasha Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07600">
<title>Polarimetric PatchMatch Multi-View Stereo. (arXiv:2311.07600v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07600</link>
<description rdf:parseType="Literal">&lt;p&gt;PatchMatch Multi-View Stereo (PatchMatch MVS) is one of the popular MVS
approaches, owing to its balanced accuracy and efficiency. In this paper, we
propose Polarimetric PatchMatch multi-view Stereo (PolarPMS), which is the
first method exploiting polarization cues to PatchMatch MVS. The key of
PatchMatch MVS is to generate depth and normal hypotheses, which form local 3D
planes and slanted stereo matching windows, and efficiently search for the best
hypothesis based on the consistency among multi-view images. In addition to
standard photometric consistency, our PolarPMS evaluates polarimetric
consistency to assess the validness of a depth and normal hypothesis, motivated
by the physical property that the polarimetric information is related to the
object&apos;s surface normal. Experimental results demonstrate that our PolarPMS can
improve the accuracy and the completeness of reconstructed 3D models,
especially for texture-less surfaces, compared with state-of-the-art PatchMatch
MVS methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jinyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oishi_J/0/1/0/all/0/1&quot;&gt;Jumpei Oishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monno_Y/0/1/0/all/0/1&quot;&gt;Yusuke Monno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okutomi_M/0/1/0/all/0/1&quot;&gt;Masatoshi Okutomi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07603">
<title>PECoP: Parameter Efficient Continual Pretraining for Action Quality Assessment. (arXiv:2311.07603v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07603</link>
<description rdf:parseType="Literal">&lt;p&gt;The limited availability of labelled data in Action Quality Assessment (AQA),
has forced previous works to fine-tune their models pretrained on large-scale
domain-general datasets. This common approach results in weak generalisation,
particularly when there is a significant domain shift. We propose a novel,
parameter efficient, continual pretraining framework, PECoP, to reduce such
domain shift via an additional pretraining stage. In PECoP, we introduce
3D-Adapters, inserted into the pretrained model, to learn spatiotemporal,
in-domain information via self-supervised learning where only the adapter
modules&apos; parameters are updated. We demonstrate PECoP&apos;s ability to enhance the
performance of recent state-of-the-art methods (MUSDL, CoRe, and TSA) applied
to AQA, leading to considerable improvements on benchmark datasets, JIGSAWS
($\uparrow6.0\%$), MTL-AQA ($\uparrow0.99\%$), and FineDiving
($\uparrow2.54\%$). We also present a new Parkinson&apos;s Disease dataset, PD4T, of
real patients performing four various actions, where we surpass
($\uparrow3.56\%$) the state-of-the-art in comparison. Our code, pretrained
models, and the PD4T dataset are available at https://github.com/Plrbear/PECoP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dadashzadeh_A/0/1/0/all/0/1&quot;&gt;Amirhossein Dadashzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1&quot;&gt;Shuchao Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whone_A/0/1/0/all/0/1&quot;&gt;Alan Whone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirmehdi_M/0/1/0/all/0/1&quot;&gt;Majid Mirmehdi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07604">
<title>Finetuning Text-to-Image Diffusion Models for Fairness. (arXiv:2311.07604v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.07604</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid adoption of text-to-image diffusion models in society underscores
an urgent need to address their biases. Without interventions, these biases
could propagate a distorted worldview and limit opportunities for minority
groups. In this work, we frame fairness as a distributional alignment problem.
Our solution consists of two main technical contributions: (1) a distributional
alignment loss that steers specific characteristics of the generated images
towards a user-defined target distribution, and (2) biased direct finetuning of
diffusion model&apos;s sampling process, which leverages a biased gradient to more
effectively optimize losses defined on the generated images. Empirically, our
method markedly reduces gender, racial, and their intersectional biases for
occupational prompts. Gender bias is significantly reduced even when finetuning
just five soft tokens. Crucially, our method supports diverse perspectives of
fairness beyond absolute equality, which is demonstrated by controlling age to
a $75\%$ young and $25\%$ old distribution while simultaneously debiasing
gender and race. Finally, our method is scalable: it can debias multiple
concepts at once by simply including these prompts in the finetuning data. We
hope our work facilitates the social alignment of T2I generative AI. We will
share code and various debiased diffusion model adaptors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xudong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1&quot;&gt;Chao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1&quot;&gt;Tianyu Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Min Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1&quot;&gt;Yongkang Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1&quot;&gt;Mohan Kankanhalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07609">
<title>Artificial Intelligence in Assessing Cardiovascular Diseases and Risk Factors via Retinal Fundus Images: A Review of the Last Decade. (arXiv:2311.07609v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2311.07609</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: Cardiovascular diseases (CVDs) continue to be the leading cause
of mortality on a global scale. In recent years, the application of artificial
intelligence (AI) techniques, particularly deep learning (DL), has gained
considerable popularity for evaluating the various aspects of CVDs. Moreover,
using fundus images and optical coherence tomography angiography (OCTA) to
diagnose retinal diseases has been extensively studied. To better understand
heart function and anticipate changes based on microvascular characteristics
and function, researchers are currently exploring the integration of AI with
non-invasive retinal scanning. Leveraging AI-assisted early detection and
prediction of cardiovascular diseases on a large scale holds excellent
potential to mitigate cardiovascular events and alleviate the economic burden
on healthcare systems. Method: A comprehensive search was conducted across
various databases, including PubMed, Medline, Google Scholar, Scopus, Web of
Sciences, IEEE Xplore, and ACM Digital Library, using specific keywords related
to cardiovascular diseases and artificial intelligence. Results: A total of 87
English-language publications, selected for relevance were included in the
study, and additional references were considered. This study presents an
overview of the current advancements and challenges in employing retinal
imaging and artificial intelligence to identify cardiovascular disorders and
provides insights for further exploration in this field. Conclusion:
Researchers aim to develop precise disease prognosis patterns as the aging
population and global CVD burden increase. AI and deep learning are
transforming healthcare, offering the potential for single retinal image-based
diagnosis of various CVDs, albeit with the need for accelerated adoption in
healthcare systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Abdollahi_M/0/1/0/all/0/1&quot;&gt;Mirsaeed Abdollahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jafarizadeh_A/0/1/0/all/0/1&quot;&gt;Ali Jafarizadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Asbagh_A/0/1/0/all/0/1&quot;&gt;Amirhosein Ghafouri Asbagh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sobhi_N/0/1/0/all/0/1&quot;&gt;Navid Sobhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Pourmoghtader_K/0/1/0/all/0/1&quot;&gt;Keysan Pourmoghtader&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Pedrammehr_S/0/1/0/all/0/1&quot;&gt;Siamak Pedrammehr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Asadi_H/0/1/0/all/0/1&quot;&gt;Houshyar Asadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Alizadehsani_R/0/1/0/all/0/1&quot;&gt;Roohallah Alizadehsani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tan_R/0/1/0/all/0/1&quot;&gt;Ru-San Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Acharya_U/0/1/0/all/0/1&quot;&gt;U. Rajendra Acharya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07616">
<title>ReIDTracker Sea: the technical report of BoaTrack and SeaDronesSee-MOT challenge at MaCVi of WACV24. (arXiv:2311.07616v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07616</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Object Tracking is one of the most important technologies in maritime
computer vision. Our solution tries to explore Multi-Object Tracking in
maritime Unmanned Aerial vehicles (UAVs) and Unmanned Surface Vehicles (USVs)
usage scenarios. Most of the current Multi-Object Tracking algorithms require
complex association strategies and association information (2D location and
motion, 3D motion, 3D depth, 2D appearance) to achieve better performance,
which makes the entire tracking system extremely complex and heavy. At the same
time, most of the current Multi-Object Tracking algorithms still require video
annotation data which is costly to obtain for training. Our solution tries to
explore Multi-Object Tracking in a completely unsupervised way. The scheme
accomplishes instance representation learning by using self-supervision on
ImageNet. Then, by cooperating with high-quality detectors, the multi-target
tracking task can be completed simply and efficiently. The scheme achieved top
3 performance on both UAV-based Multi-Object Tracking with Reidentification and
USV-based Multi-Object Tracking benchmarks and the solution won the
championship in many multiple Multi-Object Tracking competitions. such as
BDD100K MOT,MOTS, Waymo 2D MOT
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaer Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chong_W/0/1/0/all/0/1&quot;&gt;Weitu Chong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07622">
<title>Pretrain like You Inference: Masked Tuning Improves Zero-Shot Composed Image Retrieval. (arXiv:2311.07622v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07622</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-shot composed image retrieval (ZS-CIR), which aims to retrieve a target
image based on textual modifications to a reference image without triplet
labeling, has gained more and more attention. Current ZS-CIR research mainly
relies on two unlabeled pre-trained models: the vision-language model, e.g.,
CLIP, and the Pic2Word/textual inversion model. However, the pre-trained models
and CIR tasks have substantial discrepancies, where the pre-trained models
learn the similarities between vision and language but CIR aims to learn the
modifications of the image guided by text. In this paper, we introduce a novel
unlabeled and pre-trained masked tuning approach to reduce the gap between the
pre-trained model and the downstream CIR task. We first reformulate the
pre-trained vision-language contrastive learning as the CIR task, where we
randomly mask input image patches to generate $\langle$masked image, text,
image$\rangle$ triple from an image-text pair. Then, we propose a masked
tuning, which uses the text and the masked image to learn the modifications of
the original image. With such a simple design, it can learn to capture
fine-grained text-guided modifications. Extensive experimental results
demonstrate the significant superiority of our approach over the baseline
models on three ZS-CIR datasets, including FashionIQ, CIRR, and CIRCO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1&quot;&gt;Hanjiang Lai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07623">
<title>PadChannel: Improving CNN Performance through Explicit Padding Encoding. (arXiv:2311.07623v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07623</link>
<description rdf:parseType="Literal">&lt;p&gt;In convolutional neural networks (CNNs), padding plays a pivotal role in
preserving spatial dimensions throughout the layers. Traditional padding
techniques do not explicitly distinguish between the actual image content and
the padded regions, potentially causing CNNs to incorrectly interpret the
boundary pixels or regions that resemble boundaries. This ambiguity can lead to
suboptimal feature extraction. To address this, we propose PadChannel, a novel
padding method that encodes padding statuses as an additional input channel,
enabling CNNs to easily distinguish genuine pixels from padded ones. By
incorporating PadChannel into several prominent CNN architectures, we observed
small performance improvements and notable reductions in the variances on the
ImageNet-1K image classification task at marginal increases in the
computational cost. The source code is available at
https://github.com/AussieSeaweed/pad-channel
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Juho Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07630">
<title>Cross-modal Generative Model for Visual-Guided Binaural Stereo Generation. (arXiv:2311.07630v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2311.07630</link>
<description rdf:parseType="Literal">&lt;p&gt;Binaural stereo audio is recorded by imitating the way the human ear receives
sound, which provides people with an immersive listening experience. Existing
approaches leverage autoencoders and directly exploit visual spatial
information to synthesize binaural stereo, resulting in a limited
representation of visual guidance. For the first time, we propose a visually
guided generative adversarial approach for generating binaural stereo audio
from mono audio. Specifically, we develop a Stereo Audio Generation Model
(SAGM), which utilizes shared spatio-temporal visual information to guide the
generator and the discriminator to work separately. The shared visual
information is updated alternately in the generative adversarial stage,
allowing the generator and discriminator to deliver their respective guided
knowledge while visually sharing. The proposed method learns bidirectional
complementary visual information, which facilitates the expression of visual
guidance in generation. In addition, spatial perception is a crucial attribute
of binaural stereo audio, and thus the evaluation of stereo spatial perception
is essential. However, previous metrics failed to measure the spatial
perception of audio. To this end, a metric to measure the spatial perception of
audio is proposed for the first time. The proposed metric is capable of
measuring the magnitude and direction of spatial perception in the temporal
dimension. Further, considering its function, it is feasible to utilize it
instead of demanding user studies to some extent. The proposed method achieves
state-of-the-art performance on 2 datasets and 5 evaluation metrics.
Qualitative experiments and user studies demonstrate that the method generates
space-realistic stereo audio.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhaojian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuan Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07634">
<title>ActiveDC: Distribution Calibration for Active Finetuning. (arXiv:2311.07634v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07634</link>
<description rdf:parseType="Literal">&lt;p&gt;The pretraining-finetuning paradigm has gained popularity in various computer
vision tasks. In this paradigm, the emergence of active finetuning arises due
to the abundance of large-scale data and costly annotation requirements. Active
finetuning involves selecting a subset of data from an unlabeled pool for
annotation, facilitating subsequent finetuning. However, the use of a limited
number of training samples can lead to a biased distribution, potentially
resulting in model overfitting. In this paper, we propose a new method called
ActiveDC for the active finetuning tasks. Firstly, we select samples for
annotation by optimizing the distribution similarity between the subset to be
selected and the entire unlabeled pool in continuous space. Secondly, we
calibrate the distribution of the selected samples by exploiting implicit
category information in the unlabeled pool. The feature visualization provides
an intuitive sense of the effectiveness of our approach to distribution
calibration. We conducted extensive experiments on three image classification
datasets with different sampling ratios. The results indicate that ActiveDC
consistently outperforms the baseline performance in all image classification
tasks. The improvement is particularly significant when the sampling ratio is
low, with performance gains of up to 10%. Our code will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wenshuai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhenhui Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_J/0/1/0/all/0/1&quot;&gt;Jinzhou Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qingjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07711">
<title>Histopathologic Cancer Detection. (arXiv:2311.07711v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07711</link>
<description rdf:parseType="Literal">&lt;p&gt;Early diagnosis of the cancer cells is necessary for making an effective
treatment plan and for the health and safety of a patient. Nowadays, doctors
usually use a histological grade that pathologists determine by performing a
semi-quantitative analysis of the histopathological and cytological features of
hematoxylin-eosin (HE) stained histopathological images. This research
contributes a potential classification model for cancer prognosis to
efficiently utilize the valuable information underlying the HE-stained
histopathological images. This work uses the PatchCamelyon benchmark datasets
and trains them in a multi-layer perceptron and convolution model to observe
the model&apos;s performance in terms of precision, Recall, F1 Score, Accuracy, and
AUC Score. The evaluation result shows that the baseline convolution model
outperforms the baseline MLP model. Also, this paper introduced ResNet50 and
InceptionNet models with data augmentation, where ResNet50 is able to beat the
state-of-the-art model. Furthermore, the majority vote and concatenation
ensemble were evaluated and provided the future direction of using transfer
learning and segmentation to understand the specific features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rohila_V/0/1/0/all/0/1&quot;&gt;Varan Singh Rohila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lalwani_N/0/1/0/all/0/1&quot;&gt;Neeraj Lalwani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basyal_L/0/1/0/all/0/1&quot;&gt;Lochan Basyal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07734">
<title>Quality-Aware Prototype Memory for Face Representation Learning. (arXiv:2311.07734v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07734</link>
<description rdf:parseType="Literal">&lt;p&gt;Prototype Memory is a powerful model for face representation learning. It
enables the training of face recognition models using datasets of any size,
with on-the-fly generation of prototypes (classifier weights) and efficient
ways of their utilization. Prototype Memory demonstrated strong results in many
face recognition benchmarks. However, the algorithm of prototype generation,
used in it, is prone to the problems of imperfectly calculated prototypes in
case of low-quality or poorly recognizable faces in the images, selected for
the prototype creation. All images of the same person, presented in the
mini-batch, used with equal weights, and the resulting averaged prototype could
be contaminated with imperfect embeddings of such face images. It can lead to
misdirected training signals and impair the performance of the trained face
recognition models. In this paper, we propose a simple and effective way to
improve Prototype Memory with quality-aware prototype generation. Quality-Aware
Prototype Memory uses different weights for images of different quality in the
process of prototype generation. With this improvement, prototypes get more
valuable information from high-quality images and less hurt by low-quality
ones. We propose and compare several methods of quality estimation and usage,
perform extensive experiments on the different face recognition benchmarks and
demonstrate the advantages of the proposed model compared to the basic version
of Prototype Memory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smirnov_E/0/1/0/all/0/1&quot;&gt;Evgeny Smirnov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galyuk_V/0/1/0/all/0/1&quot;&gt;Vasiliy Galyuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lukyanets_E/0/1/0/all/0/1&quot;&gt;Evgeny Lukyanets&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07750">
<title>SynthEnsemble: A Fusion of CNN, Vision Transformer, and Hybrid Models for Multi-Label Chest X-Ray Classification. (arXiv:2311.07750v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07750</link>
<description rdf:parseType="Literal">&lt;p&gt;Chest X-rays are widely used to diagnose thoracic diseases, but the lack of
detailed information about these abnormalities makes it challenging to develop
accurate automated diagnosis systems, which is crucial for early detection and
effective treatment. To address this challenge, we employed deep learning
techniques to identify patterns in chest X-rays that correspond to different
diseases. We conducted experiments on the &quot;ChestX-ray14&quot; dataset using various
pre-trained CNNs, transformers, hybrid(CNN+Transformer) models and classical
models. The best individual model was the CoAtNet, which achieved an area under
the receiver operating characteristic curve (AUROC) of 84.2%. By combining the
predictions of all trained models using a weighted average ensemble where the
weight of each model was determined using differential evolution, we further
improved the AUROC to 85.4%, outperforming other state-of-the-art methods in
this field. Our findings demonstrate the potential of deep learning techniques,
particularly ensemble deep learning, for improving the accuracy of automatic
diagnosis of thoracic diseases from chest X-rays.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashraf_S/0/1/0/all/0/1&quot;&gt;S.M. Nabil Ashraf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mamun_M/0/1/0/all/0/1&quot;&gt;Md. Adyelullahil Mamun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdullah_H/0/1/0/all/0/1&quot;&gt;Hasnat Md. Abdullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Md. Golam Rabiul Alam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07761">
<title>Amodal Optical Flow. (arXiv:2311.07761v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07761</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical flow estimation is very challenging in situations with transparent or
occluded objects. In this work, we address these challenges at the task level
by introducing Amodal Optical Flow, which integrates optical flow with amodal
perception. Instead of only representing the visible regions, we define amodal
optical flow as a multi-layered pixel-level motion field that encompasses both
visible and occluded regions of the scene. To facilitate research on this new
task, we extend the AmodalSynthDrive dataset to include pixel-level labels for
amodal optical flow estimation. We present several strong baselines, along with
the Amodal Flow Quality metric to quantify the performance in an interpretable
manner. Furthermore, we propose the novel AmodalFlowNet as an initial step
toward addressing this task. AmodalFlowNet consists of a transformer-based
cost-volume encoder paired with a recurrent transformer decoder which
facilitates recurrent hierarchical feature propagation and amodal semantic
grounding. We demonstrate the tractability of amodal optical flow in extensive
experiments and show its utility for downstream tasks such as panoptic
tracking. We make the dataset, code, and trained models publicly available at
&lt;a href=&quot;http://amodal-flow.cs.uni-freiburg.de.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luz_M/0/1/0/all/0/1&quot;&gt;Maximilian Luz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohan_R/0/1/0/all/0/1&quot;&gt;Rohit Mohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sekkat_A/0/1/0/all/0/1&quot;&gt;Ahmed Rida Sekkat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawade_O/0/1/0/all/0/1&quot;&gt;Oliver Sawade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matthes_E/0/1/0/all/0/1&quot;&gt;Elmar Matthes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1&quot;&gt;Thomas Brox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1&quot;&gt;Abhinav Valada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07765">
<title>FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based Human Activity Recognition. (arXiv:2311.07765v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.07765</link>
<description rdf:parseType="Literal">&lt;p&gt;Motion sensors integrated into wearable and mobile devices provide valuable
information about the device users. Machine learning and, recently, deep
learning techniques have been used to characterize sensor data. Mostly, a
single task, such as recognition of activities, is targeted, and the data is
processed centrally at a server or in a cloud environment. However, the same
sensor data can be utilized for multiple tasks and distributed machine-learning
techniques can be used without the requirement of the transmission of data to a
centre. This paper explores Federated Transfer Learning in a Multi-Task manner
for both sensor-based human activity recognition and device position
identification tasks. The OpenHAR framework is used to train the models, which
contains ten smaller datasets. The aim is to obtain model(s) applicable for
both tasks in different datasets, which may include only some label types.
Multiple experiments are carried in the Flower federated learning environment
using the DeepConvLSTM architecture. Results are presented for federated and
centralized versions under different parameters and restrictions. By utilizing
transfer learning and training a task-specific and personalized federated
model, we obtained a similar accuracy with training each client individually
and higher accuracy than a fully centralized approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isguder_E/0/1/0/all/0/1&quot;&gt;Egemen &amp;#x130;&amp;#x15f;g&amp;#xfc;der&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Incel_O/0/1/0/all/0/1&quot;&gt;&amp;#xd6;zlem Durmaz &amp;#x130;ncel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07766">
<title>Vision-Language Integration in Multimodal Video Transformers (Partially) Aligns with the Brain. (arXiv:2311.07766v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07766</link>
<description rdf:parseType="Literal">&lt;p&gt;Integrating information from multiple modalities is arguably one of the
essential prerequisites for grounding artificial intelligence systems with an
understanding of the real world. Recent advances in video transformers that
jointly learn from vision, text, and sound over time have made some progress
toward this goal, but the degree to which these models integrate information
from modalities still remains unclear. In this work, we present a promising
approach for probing a pre-trained multimodal video transformer model by
leveraging neuroscientific evidence of multimodal information processing in the
brain. Using brain recordings of participants watching a popular TV show, we
analyze the effects of multi-modal connections and interactions in a
pre-trained multi-modal video transformer on the alignment with uni- and
multi-modal brain regions. We find evidence that vision enhances masked
prediction performance during language processing, providing support that
cross-modal representations in models can benefit individual modalities.
However, we don&apos;t find evidence of brain-relevant information captured by the
joint multi-modal transformer representations beyond that captured by all of
the individual modalities. We finally show that the brain alignment of the
pre-trained joint representation can be improved by fine-tuning using a task
that requires vision-language inferences. Overall, our results paint an
optimistic picture of the ability of multi-modal transformers to integrate
vision and language in partially brain-relevant ways but also show that
improving the brain alignment of these models may require new approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1&quot;&gt;Dota Tianai Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toneva_M/0/1/0/all/0/1&quot;&gt;Mariya Toneva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07784">
<title>A Data-Free Approach to Mitigate Catastrophic Forgetting in Federated Class Incremental Learning for Vision Tasks. (arXiv:2311.07784v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.07784</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models often suffer from forgetting previously learned
information when trained on new data. This problem is exacerbated in federated
learning (FL), where the data is distributed and can change independently for
each user. Many solutions are proposed to resolve this catastrophic forgetting
in a centralized setting. However, they do not apply directly to FL because of
its unique complexities, such as privacy concerns and resource limitations. To
overcome these challenges, this paper presents a framework for
\textbf{federated class incremental learning} that utilizes a generative model
to synthesize samples from past distributions. This data can be later exploited
alongside the training data to mitigate catastrophic forgetting. To preserve
privacy, the generative model is trained on the server using data-free methods
at the end of each task without requesting data from clients. Moreover, our
solution does not demand the users to store old data or models, which gives
them the freedom to join/leave the training at any time. Additionally, we
introduce SuperImageNet, a new regrouping of the ImageNet dataset specifically
tailored for federated continual learning. We demonstrate significant
improvements compared to existing baselines through extensive experiments on
multiple datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babakniya_S/0/1/0/all/0/1&quot;&gt;Sara Babakniya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fabian_Z/0/1/0/all/0/1&quot;&gt;Zalan Fabian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1&quot;&gt;Chaoyang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1&quot;&gt;Mahdi Soltanolkotabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1&quot;&gt;Salman Avestimehr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07788">
<title>CSLP-AE: A Contrastive Split-Latent Permutation Autoencoder Framework for Zero-Shot Electroencephalography Signal Conversion. (arXiv:2311.07788v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.07788</link>
<description rdf:parseType="Literal">&lt;p&gt;Electroencephalography (EEG) is a prominent non-invasive neuroimaging
technique providing insights into brain function. Unfortunately, EEG data
exhibit a high degree of noise and variability across subjects hampering
generalizable signal extraction. Therefore, a key aim in EEG analysis is to
extract the underlying neural activation (content) as well as to account for
the individual subject variability (style). We hypothesize that the ability to
convert EEG signals between tasks and subjects requires the extraction of
latent representations accounting for content and style. Inspired by recent
advancements in voice conversion technologies, we propose a novel contrastive
split-latent permutation autoencoder (CSLP-AE) framework that directly
optimizes for EEG conversion. Importantly, the latent representations are
guided using contrastive learning to promote the latent splits to explicitly
represent subject (style) and task (content). We contrast CSLP-AE to
conventional supervised, unsupervised (AE), and self-supervised (contrastive
learning) training and find that the proposed approach provides favorable
generalizable characterizations of subject and task. Importantly, the procedure
also enables zero-shot conversion between unseen subjects. While the present
work only considers conversion of EEG, the proposed CSLP-AE provides a general
framework for signal conversion and extraction of content (task activation) and
style (subject variability) components of general interest for the modeling and
analysis of biological signals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norskov_A/0/1/0/all/0/1&quot;&gt;Anders Vestergaard N&amp;#xf8;rskov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zahid_A/0/1/0/all/0/1&quot;&gt;Alexander Neergaard Zahid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morup_M/0/1/0/all/0/1&quot;&gt;Morten M&amp;#xf8;rup&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07806">
<title>Assessing Test-time Variability for Interactive 3D Medical Image Segmentation with Diverse Point Prompts. (arXiv:2311.07806v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07806</link>
<description rdf:parseType="Literal">&lt;p&gt;Interactive segmentation model leverages prompts from users to produce robust
segmentation. This advancement is facilitated by prompt engineering, where
interactive prompts serve as strong priors during test-time. However, this is
an inherently subjective and hard-to-reproduce process. The variability in user
expertise and inherently ambiguous boundaries in medical images can lead to
inconsistent prompt selections, potentially affecting segmentation accuracy.
This issue has not yet been extensively explored for medical imaging. In this
paper, we assess the test-time variability for interactive medical image
segmentation with diverse point prompts. For a given target region, the point
is classified into three sub-regions: boundary, margin, and center. Our goal is
to identify a straightforward and efficient approach for optimal prompt
selection during test-time based on three considerations: (1) benefits of
additional prompts, (2) effects of prompt placement, and (3) strategies for
optimal prompt selection. We conduct extensive experiments on the public
Medical Segmentation Decathlon dataset for challenging colon tumor segmentation
task. We suggest an optimal strategy for prompt selection during test-time,
supported by comprehensive results. The code is publicly available at
https://github.com/MedICL-VU/variability
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dewei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiacheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oguz_I/0/1/0/all/0/1&quot;&gt;Ipek Oguz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07840">
<title>Enabling Decision-Support Systems through Automated Cell Tower Detection. (arXiv:2311.07840v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07840</link>
<description rdf:parseType="Literal">&lt;p&gt;Cell phone coverage and high-speed service gaps persist in rural areas in
sub-Saharan Africa, impacting public access to mobile-based financial,
educational, and humanitarian services. Improving maps of telecommunications
infrastructure can help inform strategies to eliminate gaps in mobile coverage.
Deep neural networks, paired with remote sensing images, can be used for object
detection of cell towers and eliminate the need for inefficient and burdensome
manual mapping to find objects over large geographic regions. In this study, we
demonstrate a partially automated workflow to train an object detection model
to locate cell towers using OpenStreetMap (OSM) features and high-resolution
Maxar imagery. For model fine-tuning and evaluation, we curated a diverse
dataset of over 6,000 unique images of cell towers in 26 countries in eastern,
southern, and central Africa using automatically generated annotations from OSM
points. Our model achieves an average precision at 50% Intersection over Union
(IoU) (AP@50) of 81.2 with good performance across different geographies and
out-of-sample testing. Accurate localization of cell towers can yield more
accurate cell coverage maps, in turn enabling improved delivery of digital
services for decision-support applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krell_N/0/1/0/all/0/1&quot;&gt;Natasha Krell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gleave_W/0/1/0/all/0/1&quot;&gt;Will Gleave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakada_D/0/1/0/all/0/1&quot;&gt;Daniel Nakada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Downes_J/0/1/0/all/0/1&quot;&gt;Justin Downes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willet_A/0/1/0/all/0/1&quot;&gt;Amanda Willet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baran_M/0/1/0/all/0/1&quot;&gt;Matthew Baran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07864">
<title>Probing clustering in neural network representations. (arXiv:2311.07864v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.07864</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural network representations contain structure beyond what was present in
the training labels. For instance, representations of images that are visually
or semantically similar tend to lie closer to each other than to dissimilar
images, regardless of their labels. Clustering these representations can thus
provide insights into dataset properties as well as the network internals. In
this work, we study how the many design choices involved in neural network
training affect the clusters formed in the hidden representations. To do so, we
establish an evaluation setup based on the BREEDS hierarchy, for the task of
subclass clustering after training models with only superclass information. We
isolate the training dataset and architecture as important factors affecting
clusterability. Datasets with labeled classes consisting of unrelated
subclasses yield much better clusterability than those following a natural
hierarchy. When using pretrained models to cluster representations on
downstream datasets, models pretrained on subclass labels provide better
clusterability than models pretrained on superclass labels, but only when there
is a high degree of domain overlap between the pretraining and downstream data.
Architecturally, we find that normalization strategies affect which layers
yield the best clustering performance, and, surprisingly, Vision Transformers
attain lower subclass clusterability than ResNets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thao Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1&quot;&gt;Simon Kornblith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07871">
<title>Dual-channel Prototype Network for few-shot Classification of Pathological Images. (arXiv:2311.07871v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07871</link>
<description rdf:parseType="Literal">&lt;p&gt;In pathology, the rarity of certain diseases and the complexity in annotating
pathological images significantly hinder the creation of extensive,
high-quality datasets. This limitation impedes the progress of deep
learning-assisted diagnostic systems in pathology. Consequently, it becomes
imperative to devise a technology that can discern new disease categories from
a minimal number of annotated examples. Such a technology would substantially
advance deep learning models for rare diseases. Addressing this need, we
introduce the Dual-channel Prototype Network (DCPN), rooted in the few-shot
learning paradigm, to tackle the challenge of classifying pathological images
with limited samples. DCPN augments the Pyramid Vision Transformer (PVT)
framework for few-shot classification via self-supervised learning and
integrates it with convolutional neural networks. This combination forms a
dual-channel architecture that extracts multi-scale, highly precise
pathological features. The approach enhances the versatility of prototype
representations and elevates the efficacy of prototype networks in few-shot
pathological image classification tasks. We evaluated DCPN using three publicly
available pathological datasets, configuring small-sample classification tasks
that mirror varying degrees of clinical scenario domain shifts. Our
experimental findings robustly affirm DCPN&apos;s superiority in few-shot
pathological image classification, particularly in tasks within the same
domain, where it achieves the benchmarks of supervised learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quan_H/0/1/0/all/0/1&quot;&gt;Hao Quan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinjia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dayu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nan_T/0/1/0/all/0/1&quot;&gt;Tianhang Nan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07877">
<title>Test-Time Training for Semantic Segmentation with Output Contrastive Loss. (arXiv:2311.07877v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07877</link>
<description rdf:parseType="Literal">&lt;p&gt;Although deep learning-based segmentation models have achieved impressive
performance on public benchmarks, generalizing well to unseen environments
remains a major challenge. To improve the model&apos;s generalization ability to the
new domain during evaluation, the test-time training (TTT) is a challenging
paradigm that adapts the source-pretrained model in an online fashion. Early
efforts on TTT mainly focus on the image classification task. Directly
extending these methods to semantic segmentation easily experiences unstable
adaption due to segmentation&apos;s inherent characteristics, such as extreme class
imbalance and complex decision spaces. To stabilize the adaptation process, we
introduce contrastive loss (CL), known for its capability to learn robust and
generalized representations. Nevertheless, the traditional CL operates in the
representation space and cannot directly enhance predictions. In this paper, we
resolve this limitation by adapting the CL to the output space, employing a
high temperature, and simplifying the formulation, resulting in a
straightforward yet effective loss function called Output Contrastive Loss
(OCL). Our comprehensive experiments validate the efficacy of our approach
across diverse evaluation scenarios. Notably, our method excels even when
applied to models initially pre-trained using domain adaptation methods on test
domain data, showcasing its resilience and adaptability.\footnote{Code and more
information could be found at~ \url{https://github.com/dazhangyu123/OCL}}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunlong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Sunyi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shui_Z/0/1/0/all/0/1&quot;&gt;Zhongyi Shui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chenglu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07880">
<title>VegaEdge: Edge AI Confluence Anomaly Detection for Real-Time Highway IoT-Applications. (arXiv:2311.07880v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07880</link>
<description rdf:parseType="Literal">&lt;p&gt;Vehicle anomaly detection plays a vital role in highway safety applications
such as accident prevention, rapid response, traffic flow optimization, and
work zone safety. With the surge of the Internet of Things (IoT) in recent
years, there has arisen a pressing demand for Artificial Intelligence (AI)
based anomaly detection methods designed to meet the requirements of IoT
devices. Catering to this futuristic vision, we introduce a lightweight
approach to vehicle anomaly detection by utilizing the power of trajectory
prediction. Our proposed design identifies vehicles deviating from expected
paths, indicating highway risks from different camera-viewing angles from
real-world highway datasets. On top of that, we present VegaEdge - a
sophisticated AI confluence designed for real-time security and surveillance
applications in modern highway settings through edge-centric IoT-embedded
platforms equipped with our anomaly detection approach. Extensive testing
across multiple platforms and traffic scenarios showcases the versatility and
effectiveness of VegaEdge. This work also presents the Carolinas Anomaly
Dataset (CAD), to bridge the existing gap in datasets tailored for highway
anomalies. In real-world scenarios, our anomaly detection approach achieves an
AUC-ROC of 0.94, and our proposed VegaEdge design, on an embedded IoT platform,
processes 738 trajectories per second in a typical highway setting. The dataset
is available at
https://github.com/TeCSAR-UNCC/Carolinas_Dataset#chd-anomaly-test-set .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katariya_V/0/1/0/all/0/1&quot;&gt;Vinit Katariya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jannat_F/0/1/0/all/0/1&quot;&gt;Fatema-E- Jannat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pazho_A/0/1/0/all/0/1&quot;&gt;Armin Danesh Pazho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noghre_G/0/1/0/all/0/1&quot;&gt;Ghazal Alinezhad Noghre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1&quot;&gt;Hamed Tabkhi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07885">
<title>One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion. (arXiv:2311.07885v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07885</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in open-world 3D object generation have been remarkable,
with image-to-3D methods offering superior fine-grained control over their
text-to-3D counterparts. However, most existing models fall short in
simultaneously providing rapid generation speeds and high fidelity to input
images - two features essential for practical applications. In this paper, we
present One-2-3-45++, an innovative method that transforms a single image into
a detailed 3D textured mesh in approximately one minute. Our approach aims to
fully harness the extensive knowledge embedded in 2D diffusion models and
priors from valuable yet limited 3D data. This is achieved by initially
finetuning a 2D diffusion model for consistent multi-view image generation,
followed by elevating these images to 3D with the aid of multi-view conditioned
3D native diffusion models. Extensive experimental evaluations demonstrate that
our method can produce high-quality, diverse 3D assets that closely mirror the
original input image. Our project webpage:
https://sudo-ai-3d.github.io/One2345plus_page.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Minghua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1&quot;&gt;Ruoxi Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Linghao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhuoyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xinyue Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hansheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1&quot;&gt;Chong Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiayuan Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07912">
<title>Detection of Small Targets in Sea Clutter Based on RepVGG and Continuous Wavelet Transform. (arXiv:2311.07912v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07912</link>
<description rdf:parseType="Literal">&lt;p&gt;Constructing a high-performance target detector under the background of sea
clutter is always necessary and important. In this work, we propose a
RepVGGA0-CWT detector, where RepVGG is a residual network that gains a high
detection accuracy. Different from traditional residual networks, RepVGG keeps
an acceptable calculation speed. Giving consideration to both accuracy and
speed, the RepVGGA0 is selected among all the variants of RepVGG. Also,
continuous wavelet transform (CWT) is employed to extract the radar echoes&apos;
time-frequency feature effectively. In the tests, other networks (ResNet50,
ResNet18 and AlexNet) and feature extraction methods (short-time Fourier
transform (STFT), CWT) are combined to build detectors for comparison. The
result of different datasets shows that the RepVGGA0-CWT detector performs
better than those detectors in terms of low controllable false alarm rate, high
training speed, high inference speed and low memory usage. This RepVGGA0-CWT
detector is hardware-friendly and can be applied in real-time scenes for its
high inference speed in detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1&quot;&gt;Jingchen Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoru Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lilin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jing Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07928">
<title>Towards Improving Robustness Against Common Corruptions in Object Detectors Using Adversarial Contrastive Learning. (arXiv:2311.07928v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07928</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks have revolutionized various domains, exhibiting remarkable
accuracy in tasks like natural language processing and computer vision.
However, their vulnerability to slight alterations in input samples poses
challenges, particularly in safety-critical applications like autonomous
driving. Current approaches, such as introducing distortions during training,
fall short in addressing unforeseen corruptions. This paper proposes an
innovative adversarial contrastive learning framework to enhance neural network
robustness simultaneously against adversarial attacks and common corruptions.
By generating instance-wise adversarial examples and optimizing contrastive
loss, our method fosters representations that resist adversarial perturbations
and remain robust in real-world scenarios. Subsequent contrastive learning then
strengthens the similarity between clean samples and their adversarial
counterparts, fostering representations resistant to both adversarial attacks
and common distortions. By focusing on improving performance under adversarial
and real-world conditions, our approach aims to bolster the robustness of
neural networks in safety-critical applications, such as autonomous vehicles
navigating unpredictable weather conditions. We anticipate that this framework
will contribute to advancing the reliability of neural networks in challenging
environments, facilitating their widespread adoption in mission-critical
scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotyan_S/0/1/0/all/0/1&quot;&gt;Shashank Kotyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vargas_D/0/1/0/all/0/1&quot;&gt;Danilo Vasconcellos Vargas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07955">
<title>Deep Learning-Based Object Detection in Maritime Unmanned Aerial Vehicle Imagery: Review and Experimental Comparisons. (arXiv:2311.07955v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07955</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advancement of maritime unmanned aerial vehicles (UAVs) and deep
learning technologies, the application of UAV-based object detection has become
increasingly significant in the fields of maritime industry and ocean
engineering. Endowed with intelligent sensing capabilities, the maritime UAVs
enable effective and efficient maritime surveillance. To further promote the
development of maritime UAV-based object detection, this paper provides a
comprehensive review of challenges, relative methods, and UAV aerial datasets.
Specifically, in this work, we first briefly summarize four challenges for
object detection on maritime UAVs, i.e., object feature diversity, device
limitation, maritime environment variability, and dataset scarcity. We then
focus on computational methods to improve maritime UAV-based object detection
performance in terms of scale-aware, small object detection, view-aware,
rotated object detection, lightweight methods, and others. Next, we review the
UAV aerial image/video datasets and propose a maritime UAV aerial dataset named
MS2ship for ship detection. Furthermore, we conduct a series of experiments to
present the performance evaluation and robustness analysis of object detection
methods on maritime datasets. Eventually, we give the discussion and outlook on
future works for maritime UAV-based object detection. The MS2ship dataset is
available at
\href{https://github.com/zcj234/MS2ship}{https://github.com/zcj234/MS2ship}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chenjie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ryan Wen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1&quot;&gt;Jingxiang Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Ruobin Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07956">
<title>Robust Learning Based Condition Diagnosis Method for Distribution Network Switchgear. (arXiv:2311.07956v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2311.07956</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a robust, learning-based method for diagnosing the
state of distribution network switchgear, which is crucial for maintaining the
power quality for end users. Traditional diagnostic models often rely heavily
on expert knowledge and lack robustness. To address this, our method
incorporates an expanded feature vector that includes environmental data,
temperature readings, switch position, motor operation, insulation conditions,
and local discharge information. We tackle the issue of high dimensionality
through feature mapping. The method introduces a decision radius to categorize
unlabeled samples and updates the model parameters using a combination of
supervised and unsupervised loss, along with a consistency regularization
function. This approach ensures robust learning even with a limited number of
labeled samples. Comparative analysis demonstrates that this method
significantly outperforms existing models in both accuracy and robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenxi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weixi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Weisi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sizhe Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07967">
<title>Comparison of two data fusion approaches for land use classification. (arXiv:2311.07967v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.07967</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate land use maps, describing the territory from an anthropic
utilisation point of view, are useful tools for land management and planning.
To produce them, the use of optical images alone remains limited. It is
therefore necessary to make use of several heterogeneous sources, each carrying
complementary or contradictory information due to their imperfections or their
different specifications. This study compares two different approaches i.e. a
pre-classification and a post-classification fusion approach for combining
several sources of spatial data in the context of land use classification. The
approaches are applied on authoritative land use data located in the Gers
department in the southwest of France. Pre-classification fusion, while not
explicitly modeling imperfections, has the best final results, reaching an
overall accuracy of 97% and a macro-mean F1 score of 88%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cubaud_M/0/1/0/all/0/1&quot;&gt;Martin Cubaud&lt;/a&gt; (LaSTIG), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bris_A/0/1/0/all/0/1&quot;&gt;Arnaud Le Bris&lt;/a&gt; (LaSTIG), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jolivet_L/0/1/0/all/0/1&quot;&gt;Laurence Jolivet&lt;/a&gt; (LaSTIG), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olteanu_Raimond_A/0/1/0/all/0/1&quot;&gt;Ana-Maria Olteanu-Raimond&lt;/a&gt; (LaSTIG)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07981">
<title>Benchmarking Individual Tree Mapping with Sub-meter Imagery. (arXiv:2311.07981v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07981</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a rising interest in mapping trees using satellite or aerial
imagery, but there is no standardized evaluation protocol for comparing and
enhancing methods. In dense canopy areas, the high variability of tree sizes
and their spatial proximity makes it arduous to define the quality of the
predictions. Concurrently, object-centric approaches such as bounding box
detection usuallyperform poorly on small and dense objects. It thus remains
unclear what is the ideal framework for individual tree mapping, in regards to
detection and segmentation approaches, convolutional neural networks and
transformers. In this paper, we introduce an evaluation framework suited for
individual tree mapping in any physical environment, with annotation costs and
applicative goals in mind. We review and compare different approaches and deep
architectures, and introduce a new method that we experimentally prove to be a
good compromise between segmentation and detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gominski_D/0/1/0/all/0/1&quot;&gt;Dimitri Gominski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kariryaa_A/0/1/0/all/0/1&quot;&gt;Ankit Kariryaa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brandt_M/0/1/0/all/0/1&quot;&gt;Martin Brandt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Igel_C/0/1/0/all/0/1&quot;&gt;Christian Igel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sizhuo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mugabowindekwe_M/0/1/0/all/0/1&quot;&gt;Maurice Mugabowindekwe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fensholt_R/0/1/0/all/0/1&quot;&gt;Rasmus Fensholt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07993">
<title>Explicit Change Relation Learning for Change Detection in VHR Remote Sensing Images. (arXiv:2311.07993v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.07993</link>
<description rdf:parseType="Literal">&lt;p&gt;Change detection has always been a concerned task in the interpretation of
remote sensing images. It is essentially a unique binary classification task
with two inputs, and there is a change relationship between these two inputs.
At present, the mining of change relationship features is usually implicit in
the network architectures that contain single-branch or two-branch encoders.
However, due to the lack of artificial prior design for change relationship
features, these networks cannot learn enough change semantic information and
lose more accurate change detection performance. So we propose a network
architecture NAME for the explicit mining of change relation features. In our
opinion, the change features of change detection should be divided into
pre-changed image features, post-changed image features and change relation
features. In order to fully mine these three kinds of change features, we
propose the triple branch network combining the transformer and convolutional
neural network (CNN) to extract and fuse these change features from two
perspectives of global information and local information, respectively. In
addition, we design the continuous change relation (CCR) branch to further
obtain the continuous and detail change relation features to improve the change
discrimination capability of the model. The experimental results show that our
network performs better, in terms of F1, IoU, and OA, than those of the
existing advanced networks for change detection on four public very
high-resolution (VHR) remote sensing datasets. Our source code is available at
https://github.com/DalongZ/NAME.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1&quot;&gt;Dalong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zebin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hung_C/0/1/0/all/0/1&quot;&gt;Chih-Cheng Hung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhihui Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08007">
<title>Clearer Frames, Anytime: Resolving Velocity Ambiguity in Video Frame Interpolation. (arXiv:2311.08007v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08007</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing video frame interpolation (VFI) methods blindly predict where each
object is at a specific timestep t (&quot;time indexing&quot;), which struggles to
predict precise object movements. Given two images of a baseball, there are
infinitely many possible trajectories: accelerating or decelerating, straight
or curved. This often results in blurry frames as the method averages out these
possibilities. Instead of forcing the network to learn this complicated
time-to-location mapping implicitly together with predicting the frames, we
provide the network with an explicit hint on how far the object has traveled
between start and end frames, a novel approach termed &quot;distance indexing&quot;. This
method offers a clearer learning goal for models, reducing the uncertainty tied
to object speeds. We further observed that, even with this extra guidance,
objects can still be blurry especially when they are equally far from both
input frames (i.e., halfway in-between), due to the directional ambiguity in
long-range motion. To solve this, we propose an iterative reference-based
estimation strategy that breaks down a long-range prediction into several
short-range steps. When integrating our plug-and-play strategies into
state-of-the-art learning-based models, they exhibit markedly sharper outputs
and superior perceptual quality in arbitrary time interpolations, using a
uniform distance indexing map in the same format as time indexing.
Additionally, distance indexing can be specified pixel-wise, which enables
temporal manipulation of each object independently, offering a novel tool for
video editing tasks like re-timing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1&quot;&gt;Zhihang Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnan_G/0/1/0/all/0/1&quot;&gt;Gurunandan Krishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Sizhuo Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jian Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08013">
<title>CP-SLAM: Collaborative Neural Point-based SLAM System. (arXiv:2311.08013v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08013</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a collaborative implicit neural simultaneous localization
and mapping (SLAM) system with RGB-D image sequences, which consists of
complete front-end and back-end modules including odometry, loop detection,
sub-map fusion, and global refinement. In order to enable all these modules in
a unified framework, we propose a novel neural point based 3D scene
representation in which each point maintains a learnable neural feature for
scene encoding and is associated with a certain keyframe. Moreover, a
distributed-to-centralized learning strategy is proposed for the collaborative
implicit SLAM to improve consistency and cooperation. A novel global
optimization framework is also proposed to improve the system accuracy like
traditional bundle adjustment. Experiments on various datasets demonstrate the
superiority of the proposed method in both camera tracking and mapping.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jiarui Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_M/0/1/0/all/0/1&quot;&gt;Mao Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1&quot;&gt;Hujun Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guofeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1&quot;&gt;Zhaopeng Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08024">
<title>MD-IQA: Learning Multi-scale Distributed Image Quality Assessment with Semi Supervised Learning for Low Dose CT. (arXiv:2311.08024v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.08024</link>
<description rdf:parseType="Literal">&lt;p&gt;Image quality assessment (IQA) plays a critical role in optimizing radiation
dose and developing novel medical imaging techniques in computed tomography
(CT). Traditional IQA methods relying on hand-crafted features have limitations
in summarizing the subjective perceptual experience of image quality. Recent
deep learning-based approaches have demonstrated strong modeling capabilities
and potential for medical IQA, but challenges remain regarding model
generalization and perceptual accuracy. In this work, we propose a multi-scale
distributions regression approach to predict quality scores by constraining the
output distribution, thereby improving model generalization. Furthermore, we
design a dual-branch alignment network to enhance feature extraction
capabilities. Additionally, semi-supervised learning is introduced by utilizing
pseudo-labels for unlabeled data to guide model training. Extensive qualitative
experiments demonstrate the effectiveness of our proposed method for advancing
the state-of-the-art in deep learning-based medical IQA. Code is available at:
https://github.com/zunzhumu/MD-IQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_T/0/1/0/all/0/1&quot;&gt;Tao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hou_R/0/1/0/all/0/1&quot;&gt;Ruizhi Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dai_L/0/1/0/all/0/1&quot;&gt;Lisong Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xiang_L/0/1/0/all/0/1&quot;&gt;Lei Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08032">
<title>ELF: An End-to-end Local and Global Multimodal Fusion Framework for Glaucoma Grading. (arXiv:2311.08032v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.08032</link>
<description rdf:parseType="Literal">&lt;p&gt;Glaucoma is a chronic neurodegenerative condition that can lead to blindness.
Early detection and curing are very important in stopping the disease from
getting worse for glaucoma patients. The 2D fundus images and optical coherence
tomography(OCT) are useful for ophthalmologists in diagnosing glaucoma. There
are many methods based on the fundus images or 3D OCT volumes; however, the
mining for multi-modality, including both fundus images and data, is less
studied. In this work, we propose an end-to-end local and global multi-modal
fusion framework for glaucoma grading, named ELF for short. ELF can fully
utilize the complementary information between fundus and OCT. In addition,
unlike previous methods that concatenate the multi-modal features together,
which lack exploring the mutual information between different modalities, ELF
can take advantage of local-wise and global-wise mutual information. The
extensive experiment conducted on the multi-modal glaucoma grading GAMMA
dataset can prove the effiectness of ELF when compared with other
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenyun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pun_C/0/1/0/all/0/1&quot;&gt;Chi-Man Pun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08043">
<title>Contrastive Learning for Multi-Object Tracking with Transformers. (arXiv:2311.08043v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08043</link>
<description rdf:parseType="Literal">&lt;p&gt;The DEtection TRansformer (DETR) opened new possibilities for object
detection by modeling it as a translation task: converting image features into
object-level representations. Previous works typically add expensive modules to
DETR to perform Multi-Object Tracking (MOT), resulting in more complicated
architectures. We instead show how DETR can be turned into a MOT model by
employing an instance-level contrastive loss, a revised sampling strategy and a
lightweight assignment method. Our training scheme learns object appearances
while preserving detection capabilities and with little overhead. Its
performance surpasses the previous state-of-the-art by +2.6 mMOTA on the
challenging BDD100K dataset and is comparable to existing transformer-based
methods on the MOT17 dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plaen_P/0/1/0/all/0/1&quot;&gt;Pierre-Fran&amp;#xe7;ois De Plaen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marinello_N/0/1/0/all/0/1&quot;&gt;Nicola Marinello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Proesmans_M/0/1/0/all/0/1&quot;&gt;Marc Proesmans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1&quot;&gt;Tinne Tuytelaars&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08046">
<title>Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding. (arXiv:2311.08046v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08046</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models have demonstrated impressive universal capabilities
across a wide range of open-ended tasks and have extended their utility to
encompass multimodal conversations. However, existing methods encounter
challenges in effectively handling both image and video understanding,
particularly with limited visual tokens. In this work, we introduce Chat-UniVi,
a unified vision-language model capable of comprehending and engaging in
conversations involving images and videos through a unified visual
representation. Specifically, we employ a set of dynamic visual tokens to
uniformly represent images and videos. This representation framework empowers
the model to efficiently utilize a limited number of visual tokens to
simultaneously capture the spatial details necessary for images and the
comprehensive temporal relationship required for videos. Moreover, we leverage
a multi-scale representation, enabling the model to perceive both high-level
semantic concepts and low-level visual details. Notably, Chat-UniVi is trained
on a mixed dataset containing both images and videos, allowing direct
application to tasks involving both mediums without requiring any
modifications. Extensive experimental results demonstrate that Chat-UniVi, as a
unified model, consistently outperforms even existing methods exclusively
designed for either images or videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1&quot;&gt;Peng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takanobu_R/0/1/0/all/0/1&quot;&gt;Ryuichi Takanobu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Caiwan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xiaochun Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Li Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08059">
<title>FS-Net: Full Scale Network and Adaptive Threshold for Improving Extraction of Micro-Retinal Vessel Structures. (arXiv:2311.08059v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.08059</link>
<description rdf:parseType="Literal">&lt;p&gt;Retinal vascular segmentation, is a widely researched subject in biomedical
image processing, aims to relieve ophthalmologists&apos; workload when treating and
detecting retinal disorders. However, segmenting retinal vessels has its own
set of challenges, with prior techniques failing to generate adequate results
when segmenting branches and microvascular structures. The neural network
approaches used recently are characterized by the inability to keep local and
global properties together and the failure to capture tiny end vessels make it
challenging to attain the desired result. To reduce this retinal vessel
segmentation problem, we propose a full-scale micro-vessel extraction mechanism
based on an encoder-decoder neural network architecture, sigmoid smoothing, and
an adaptive threshold method. The network consists of of residual, encoder
booster, bottleneck enhancement, squeeze, and excitation building blocks. All
of these blocks together help to improve the feature extraction and prediction
of the segmentation map. The proposed solution has been evaluated using the
DRIVE, CHASE-DB1, and STARE datasets, and competitive results are obtained when
compared with previous studies. The AUC and accuracy on the DRIVE dataset are
0.9884 and 0.9702, respectively. On the CHASE-DB1 dataset, the scores are
0.9903 and 0.9755, respectively. On the STARE dataset, the scores are 0.9916
and 0.9750, respectively. The performance achieved is one step ahead of what
has been done in previous studies, and this results in a higher chance of
having this solution in real-life diagnostic centers that seek ophthalmologists
attention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Getahun_M/0/1/0/all/0/1&quot;&gt;Melaku N. Getahun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rogov_O/0/1/0/all/0/1&quot;&gt;Oleg Y. Rogov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dylov_D/0/1/0/all/0/1&quot;&gt;Dmitry V. Dylov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Somov_A/0/1/0/all/0/1&quot;&gt;Andrey Somov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bouridane_A/0/1/0/all/0/1&quot;&gt;Ahmed Bouridane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hamoudi_R/0/1/0/all/0/1&quot;&gt;Rifat Hamoudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08075">
<title>GlanceSeg: Real-time microaneurysm lesion segmentation with gaze-map-guided foundation model for early detection of diabetic retinopathy. (arXiv:2311.08075v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.08075</link>
<description rdf:parseType="Literal">&lt;p&gt;Early-stage diabetic retinopathy (DR) presents challenges in clinical
diagnosis due to inconspicuous and minute microangioma lesions, resulting in
limited research in this area. Additionally, the potential of emerging
foundation models, such as the segment anything model (SAM), in medical
scenarios remains rarely explored. In this work, we propose a
human-in-the-loop, label-free early DR diagnosis framework called GlanceSeg,
based on SAM. GlanceSeg enables real-time segmentation of microangioma lesions
as ophthalmologists review fundus images. Our human-in-the-loop framework
integrates the ophthalmologist&apos;s gaze map, allowing for rough localization of
minute lesions in fundus images. Subsequently, a saliency map is generated
based on the located region of interest, which provides prompt points to assist
the foundation model in efficiently segmenting microangioma lesions. Finally, a
domain knowledge filter refines the segmentation of minute lesions. We
conducted experiments on two newly-built public datasets, i.e., IDRiD and
Retinal-Lesions, and validated the feasibility and superiority of GlanceSeg
through visualized illustrations and quantitative measures. Additionally, we
demonstrated that GlanceSeg improves annotation efficiency for clinicians and
enhances segmentation performance through fine-tuning using annotations. This
study highlights the potential of GlanceSeg-based annotations for self-model
optimization, leading to enduring performance advancements through continual
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hongyang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_M/0/1/0/all/0/1&quot;&gt;Mengdi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zirong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chen Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Shuai Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Wu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08077">
<title>Zero-Shot Segmentation of Eye Features Using the Segment Anything Model (SAM). (arXiv:2311.08077v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08077</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of foundation models signals a new era in artificial intelligence.
The Segment Anything Model (SAM) is the first foundation model for image
segmentation. In this study, we evaluate SAM&apos;s ability to segment features from
eye images recorded in virtual reality setups. The increasing requirement for
annotated eye-image datasets presents a significant opportunity for SAM to
redefine the landscape of data annotation in gaze estimation. Our investigation
centers on SAM&apos;s zero-shot learning abilities and the effectiveness of prompts
like bounding boxes or point clicks. Our results are consistent with studies in
other domains, demonstrating that SAM&apos;s segmentation effectiveness can be
on-par with specialized models depending on the feature, with prompts improving
its performance, evidenced by an IoU of 93.34% for pupil segmentation in one
dataset. Foundation models like SAM could revolutionize gaze estimation by
enabling quick and easy image segmentation, reducing reliance on specialized
models and extensive manual annotation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maquiling_V/0/1/0/all/0/1&quot;&gt;Virmarie Maquiling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byrne_S/0/1/0/all/0/1&quot;&gt;Sean Anthony Byrne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niehorster_D/0/1/0/all/0/1&quot;&gt;Diederick C. Niehorster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nystrom_M/0/1/0/all/0/1&quot;&gt;Marcus Nystr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1&quot;&gt;Enkelejda Kasneci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08080">
<title>Identifying Light-curve Signals with a Deep Learning Based Object Detection Algorithm. II. A General Light Curve Classification Framework. (arXiv:2311.08080v1 [astro-ph.IM])</title>
<link>http://arxiv.org/abs/2311.08080</link>
<description rdf:parseType="Literal">&lt;p&gt;Vast amounts of astronomical photometric data are generated from various
projects, requiring significant efforts to identify variable stars and other
object classes. In light of this, a general, widely applicable classification
framework would simplify the task of designing custom classifiers. We present a
novel deep learning framework for classifying light curves using a weakly
supervised object detection model. Our framework identifies the optimal windows
for both light curves and power spectra automatically, and zooms in on their
corresponding data. This allows for automatic feature extraction from both time
and frequency domains, enabling our model to handle data across different
scales and sampling intervals. We train our model on datasets obtained from
both space-based and ground-based multi-band observations of variable stars and
transients. We achieve an accuracy of 87% for combined variables and transient
events, which is comparable to the performance of previous feature-based
models. Our trained model can be utilized directly to other missions, such as
ASAS-SN, without requiring any retraining or fine-tuning. To address known
issues with miscalibrated predictive probabilities, we apply conformal
prediction to generate robust predictive sets that guarantee true label
coverage with a given probability. Additionally, we incorporate various anomaly
detection algorithms to empower our model with the ability to identify
out-of-distribution objects. Our framework is implemented in the Deep-LC
toolkit, which is an open-source Python package hosted on Github and PyPI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Cui_K/0/1/0/all/0/1&quot;&gt;Kaiming Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Armstrong_D/0/1/0/all/0/1&quot;&gt;D. J. Armstrong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Feng_F/0/1/0/all/0/1&quot;&gt;Fabo Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08083">
<title>Solving ARC visual analogies with neural embeddings and vector arithmetic: A generalized method. (arXiv:2311.08083v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.08083</link>
<description rdf:parseType="Literal">&lt;p&gt;Analogical reasoning derives information from known relations and generalizes
this information to similar yet unfamiliar situations. One of the first
generalized ways in which deep learning models were able to solve verbal
analogies was through vector arithmetic of word embeddings, essentially
relating words that were mapped to a vector space (e.g., king - man + woman =
__?). In comparison, most attempts to solve visual analogies are still
predominantly task-specific and less generalizable. This project focuses on
visual analogical reasoning and applies the initial generalized mechanism used
to solve verbal analogies to the visual realm. Taking the Abstraction and
Reasoning Corpus (ARC) as an example to investigate visual analogy solving, we
use a variational autoencoder (VAE) to transform ARC items into low-dimensional
latent vectors, analogous to the word embeddings used in the verbal approaches.
Through simple vector arithmetic, underlying rules of ARC items are discovered
and used to solve them. Results indicate that the approach works well on simple
items with fewer dimensions (i.e., few colors used, uniform shapes), similar
input-to-output examples, and high reconstruction accuracy on the VAE.
Predictions on more complex items showed stronger deviations from expected
outputs, although, predictions still often approximated parts of the item&apos;s
rule set. Error patterns indicated that the model works as intended. On the
official ARC paradigm, the model achieved a score of 2% (cf. current world
record is 21%) and on ConceptARC it scored 8.8%. Although the methodology
proposed involves basic dimensionality reduction techniques and standard vector
arithmetic, this approach demonstrates promising outcomes on ARC and can easily
be generalized to other abstract visual reasoning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thoms_L/0/1/0/all/0/1&quot;&gt;Luca H. Thoms&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veldkamp_K/0/1/0/all/0/1&quot;&gt;Karel A. Veldkamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenbusch_H/0/1/0/all/0/1&quot;&gt;Hannes Rosenbusch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stevenson_C/0/1/0/all/0/1&quot;&gt;Claire E. Stevenson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08094">
<title>Act-VIT: A Representationally Robust Attention Architecture for Skeleton Based Action Recognition Using Vision Transformer. (arXiv:2311.08094v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08094</link>
<description rdf:parseType="Literal">&lt;p&gt;Skeleton-based action recognition receives the attention of many researchers
as it is robust to viewpoint and illumination changes, and its processing is
much more efficient than video frames. With the emergence of deep learning
models, it has become very popular to represent the skeleton data in
pseudo-image form and apply Convolutional Neural Networks for action
recognition. Thereafter, studies concentrated on finding effective methods for
forming pseudo-images. Recently, attention networks, more specifically
transformers have provided promising results in various vision problems. In
this study, the effectiveness of vision transformers for skeleton-based action
recognition is examined and its robustness on the pseudo-image representation
scheme is investigated. To this end, a three-level architecture, Act-VIT is
proposed, which forms a set of pseudo images apply a classifier on each of the
representation and combine their results to find the final action class. The
classifiers of Act-VIT are first realized by CNNs and then by VITs and their
performances are compared. Experimental studies reveal that the vision
transformer is less sensitive to the initial pseudo-image representation
compared to CNN. Nevertheless, even with the vision transformer, the
recognition performance can be further improved by consensus of classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karadag_O/0/1/0/all/0/1&quot;&gt;Ozge Oztimur Karadag&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08100">
<title>DeepEMplanner: An EM Motion Planner with Iterative Interactions. (arXiv:2311.08100v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08100</link>
<description rdf:parseType="Literal">&lt;p&gt;Motion planning is a computational problem that finds a sequence of valid
trajectories, often based on surrounding agents&apos; forecasting, environmental
understanding, and historical and future contexts. It can also be viewed as a
game in which agents continuously plan their next move according to other
agents&apos; intentions and the encountering environment, further achieving their
ultimate goals through incremental actions. To model the dynamic planning and
interaction process, we propose a novel framework, DeepEMplanner, which takes
the stepwise interaction into account for fine-grained behavior learning. The
ego vehicle maximizes each step motion to reach its eventual driving outcome
based on the stepwise expectation from agents and its upcoming road conditions.
On the other hand, the agents also follow the same philosophy to maximize their
stepwise behavior under the encountering environment and the expectations from
ego and other agents. Our DeepEMplanner models the interactions among ego,
agents, and the dynamic environment in an autoregressive manner by interleaving
the Expectation and Maximization processes. Further, we design ego-to-agents,
ego-to-map, and ego-to-BEV interaction mechanisms with hierarchical dynamic key
objects attention to better model the interactions. Experiments on the nuScenes
benchmark show that our approach achieves state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhili Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1&quot;&gt;Maosheng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuangjie Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1&quot;&gt;Tongyi Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qifeng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08110">
<title>Improving hateful memes detection via learning hatefulness-aware embedding space through retrieval-guided contrastive learning. (arXiv:2311.08110v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.08110</link>
<description rdf:parseType="Literal">&lt;p&gt;Hateful memes have emerged as a significant concern on the Internet. These
memes, which are a combination of image and text, often convey messages vastly
different from their individual meanings. Thus, detecting hateful memes
requires the system to jointly understand the visual and textual modalities.
However, our investigation reveals that the embedding space of existing
CLIP-based systems lacks sensitivity to subtle differences in memes that are
vital for correct hatefulness classification. To address this issue, we propose
constructing a hatefulness-aware embedding space through retrieval-guided
contrastive training. Specifically, we add an auxiliary loss that utilizes hard
negative and pseudo-gold samples to train the embedding space. Our approach
achieves state-of-the-art performance on the HatefulMemes dataset with an AUROC
of 86.7. Notably, our approach outperforms much larger fine-tuned Large
Multimodal Models like Flamingo and LLaVA. Finally, we demonstrate a
retrieval-based hateful memes detection system, which is capable of making
hatefulness classification based on data unseen in training from a database.
This allows developers to update the hateful memes detection system by simply
adding new data without retraining, a desirable feature for real services in
the constantly-evolving landscape of hateful memes on the Internet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1&quot;&gt;Jingbiao Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jinghong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weizhe Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byrne_B/0/1/0/all/0/1&quot;&gt;Bill Byrne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomalin_M/0/1/0/all/0/1&quot;&gt;Marcus Tomalin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08129">
<title>Learning based Deep Disentangling Light Field Reconstruction and Disparity Estimation Application. (arXiv:2311.08129v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08129</link>
<description rdf:parseType="Literal">&lt;p&gt;Light field cameras have a wide range of uses due to their ability to
simultaneously record light intensity and direction. The angular resolution of
light fields is important for downstream tasks such as depth estimation, yet is
often difficult to improve due to hardware limitations. Conventional methods
tend to perform poorly against the challenge of large disparity in sparse light
fields, while general CNNs have difficulty extracting spatial and angular
features coupled together in 4D light fields. The light field disentangling
mechanism transforms the 4D light field into 2D image format, which is more
favorable for CNN for feature extraction. In this paper, we propose a Deep
Disentangling Mechanism, which inherits the principle of the light field
disentangling mechanism and further develops the design of the feature
extractor and adds advanced network structure. We design a light-field
reconstruction network (i.e., DDASR) on the basis of the Deep Disentangling
Mechanism, and achieve SOTA performance in the experiments. In addition, we
design a Block Traversal Angular Super-Resolution Strategy for the practical
application of depth estimation enhancement where the input views is often
higher than 2x2 in the experiments resulting in a high memory usage, which can
reduce the memory usage while having a better reconstruction performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1&quot;&gt;Langqing Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Ping Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08141">
<title>GMTR: Graph Matching Transformers. (arXiv:2311.08141v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08141</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers (ViTs) have recently been used for visual matching beyond
object detection and segmentation. However, the original grid dividing strategy
of ViTs neglects the spatial information of the keypoints, limiting the
sensitivity to local information. Therefore, we propose \textbf{QueryTrans}
(Query Transformer), which adopts a cross-attention module and keypoints-based
center crop strategy for better spatial information extraction. We further
integrate the graph attention module and devise a transformer-based graph
matching approach \textbf{GMTR} (Graph Matching TRansformers) whereby the
combinatorial nature of GM is addressed by a graph transformer neural GM
solver. On standard GM benchmarks, GMTR shows competitive performance against
the SOTA frameworks. Specifically, on Pascal VOC, GMTR achieves
$\mathbf{83.6\%}$ accuracy, $\mathbf{0.9\%}$ higher than the SOTA framework. On
Spair-71k, GMTR shows great potential and outperforms most of the previous
works. Meanwhile, on Pascal VOC, QueryTrans improves the accuracy of NGMv2 from
$80.1\%$ to $\mathbf{83.3\%}$, and BBGM from $79.0\%$ to $\mathbf{84.5\%}$. On
Spair-71k, QueryTrans improves NGMv2 from $80.6\%$ to $\mathbf{82.5\%}$, and
BBGM from $82.1\%$ to $\mathbf{83.9\%}$. Source code will be made publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jinpei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaofeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Runzhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08148">
<title>Cattle Identification Using Muzzle Images and Deep Learning Techniques. (arXiv:2311.08148v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08148</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional animal identification methods such as ear-tagging, ear notching,
and branding have been effective but pose risks to the animal and have
scalability issues. Electrical methods offer better tracking and monitoring but
require specialized equipment and are susceptible to attacks. Biometric
identification using time-immutable dermatoglyphic features such as muzzle
prints and iris patterns is a promising solution. This project explores cattle
identification using 4923 muzzle images collected from 268 beef cattle. Two
deep learning classification models are implemented - wide ResNet50 and
VGG16\_BN and image compression is done to lower the image quality and adapt
the models to work for the African context. From the experiments run, a maximum
accuracy of 99.5\% is achieved while using the wide ResNet50 model with a
compression retaining 25\% of the original image. From the study, it is noted
that the time required by the models to train and converge as well as
recognition time are dependent on the machine used to run the model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kimani_G/0/1/0/all/0/1&quot;&gt;G. N. Kimani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oluwadara_P/0/1/0/all/0/1&quot;&gt;P. Oluwadara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fashingabo_P/0/1/0/all/0/1&quot;&gt;P. Fashingabo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busogi_M/0/1/0/all/0/1&quot;&gt;M. Busogi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luhanga_E/0/1/0/all/0/1&quot;&gt;E. Luhanga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sowon_K/0/1/0/all/0/1&quot;&gt;K. Sowon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chacha_L/0/1/0/all/0/1&quot;&gt;L. Chacha&lt;/a&gt; ((1) CyLab-Africa / Upanzi Network, (2) Carnegie Mellon University Africa and (3) Carnegie Mellon University Pittsburgh)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08151">
<title>Rethink Cross-Modal Fusion in Weakly-Supervised Audio-Visual Video Parsing. (arXiv:2311.08151v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08151</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing works on weakly-supervised audio-visual video parsing adopt hybrid
attention network (HAN) as the multi-modal embedding to capture the cross-modal
context. It embeds the audio and visual modalities with a shared network, where
the cross-attention is performed at the input. However, such an early fusion
method highly entangles the two non-fully correlated modalities and leads to
sub-optimal performance in detecting single-modality events. To deal with this
problem, we propose the messenger-guided mid-fusion transformer to reduce the
uncorrelated cross-modal context in the fusion. The messengers condense the
full cross-modal context into a compact representation to only preserve useful
cross-modal information. Furthermore, due to the fact that microphones capture
audio events from all directions, while cameras only record visual events
within a restricted field of view, there is a more frequent occurrence of
unaligned cross-modal context from audio for visual event predictions. We thus
propose cross-audio prediction consistency to suppress the impact of irrelevant
audio information on visual event prediction. Experiments consistently
illustrate the superior performance of our framework compared to existing
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yating Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1&quot;&gt;Conghui Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gim Hee Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08159">
<title>DynamicSurf: Dynamic Neural RGB-D Surface Reconstruction with an Optimizable Feature Grid. (arXiv:2311.08159v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08159</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose DynamicSurf, a model-free neural implicit surface reconstruction
method for high-fidelity 3D modelling of non-rigid surfaces from monocular
RGB-D video. To cope with the lack of multi-view cues in monocular sequences of
deforming surfaces, one of the most challenging settings for 3D reconstruction,
DynamicSurf exploits depth, surface normals, and RGB losses to improve
reconstruction fidelity and optimisation time. DynamicSurf learns a neural
deformation field that maps a canonical representation of the surface geometry
to the current frame. We depart from current neural non-rigid surface
reconstruction models by designing the canonical representation as a learned
feature grid which leads to faster and more accurate surface reconstruction
than competing approaches that use a single MLP. We demonstrate DynamicSurf on
public datasets and show that it can optimize sequences of varying frames with
$6\times$ speedup over pure MLP-based approaches while achieving comparable
results to the state-of-the-art methods. Project is available at
https://mirgahney.github.io//DynamicSurf.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1&quot;&gt;Mirgahney Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agapito_L/0/1/0/all/0/1&quot;&gt;Lourdes Agapito&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08172">
<title>Vision-Language Instruction Tuning: A Review and Analysis. (arXiv:2311.08172v1 [cs.MM])</title>
<link>http://arxiv.org/abs/2311.08172</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction tuning is an essential supervised training phase for Large
Language Models (LLMs), with the goal of enhancing LLMs&apos; capacity to generalize
instruction execution and adapt to user preferences. With the growing
incorporation of multi-modal data into LLMs, there is an increasing interest in
the performance of vision-language instruction tuning which presents more
complex features in comparison to pure text instructions. In this paper, we
systematically review the latest vision-language instruction tuning settings
and datasets in multi-modal LLMs and summarize the characteristics that
high-quality vision-language tuning data should have. We consider these
characteristics as the foundational principles for constructing vision-language
instruction data and propose a complete construction pipeline consisting of
data collection, instruction generation, and quality control modules that
incorporate meticulously designed instruction property evaluation indicators.
We perform vision-language instruction tuning on three widely used multi-modal
LLMs based on the instruction data we constructed and conduct extensive
experiments on the corresponding metrics to demonstrate the rationality of the
construction principles proposed in this paper. The code and dataset related to
this paper have been open-sourced at
\url{https://github.com/palchenli/VL-Instruction-Tuning}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yixiao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08176">
<title>A deformation-based morphometry framework for disentangling Alzheimer&apos;s disease from normal aging using learned normal aging templates. (arXiv:2311.08176v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08176</link>
<description rdf:parseType="Literal">&lt;p&gt;Alzheimer&apos;s Disease and normal aging are both characterized by brain atrophy.
The question of whether AD-related brain atrophy represents accelerated aging
or a neurodegeneration process distinct from that in normal aging remains
unresolved. Moreover, precisely disentangling AD-related brain atrophy from
normal aging in a clinical context is complex. In this study, we propose a
deformation-based morphometry framework to estimate normal aging and
AD-specific atrophy patterns of subjects from morphological MRI scans. We first
leverage deep-learning-based methods to create age-dependent templates of
cognitively normal (CN) subjects. These templates model the normal aging
atrophy patterns in a CN population. Then, we use the learned diffeomorphic
registration to estimate the one-year normal aging pattern at the voxel level.
We register the testing image to the 60-year-old CN template in the second
step. Finally, normal aging and AD-specific scores are estimated by measuring
the alignment of this registration with the one-year normal aging pattern. The
methodology was developed and evaluated on the OASIS3 dataset with 1,014
T1-weighted MRI scans. Of these, 326 scans were from CN subjects, and 688 scans
were from individuals clinically diagnosed with AD at different stages of
clinical severity defined by clinical dementia rating (CDR) scores. The results
show that ventricles predominantly follow an accelerated normal aging pattern
in subjects with AD. In turn, hippocampi and amygdala regions were affected by
both normal aging and AD-specific factors. Interestingly, hippocampi and
amygdala regions showed more of an accelerated normal aging pattern for
subjects during the early clinical stages of the disease, while the AD-specific
score increases in later clinical stages. Our code is freely available at
https://github.com/Fjr9516/DBM_with_DL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jingru Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_D/0/1/0/all/0/1&quot;&gt;Daniel Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smedby_O/0/1/0/all/0/1&quot;&gt;&amp;#xd6;rjan Smedby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreno_R/0/1/0/all/0/1&quot;&gt;Rodrigo Moreno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08190">
<title>SAMIHS: Adaptation of Segment Anything Model for Intracranial Hemorrhage Segmentation. (arXiv:2311.08190v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.08190</link>
<description rdf:parseType="Literal">&lt;p&gt;Segment Anything Model (SAM), a vision foundation model trained on
large-scale annotations, has recently continued raising awareness within
medical image segmentation. Despite the impressive capabilities of SAM on
natural scenes, it struggles with performance decline when confronted with
medical images, especially those involving blurry boundaries and highly
irregular regions of low contrast. In this paper, a SAM-based
parameter-efficient fine-tuning method, called SAMIHS, is proposed for
intracranial hemorrhage segmentation, which is a crucial and challenging step
in stroke diagnosis and surgical planning. Distinguished from previous SAM and
SAM-based methods, SAMIHS incorporates parameter-refactoring adapters into
SAM&apos;s image encoder and considers the efficient and flexible utilization of
adapters&apos; parameters. Additionally, we employ a combo loss that combines binary
cross-entropy loss and boundary-sensitive loss to enhance SAMIHS&apos;s ability to
recognize the boundary regions. Our experimental results on two public datasets
demonstrate the effectiveness of our proposed method. Code is available at
https://github.com/mileswyn/SAMIHS .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yinuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Weimin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meng_C/0/1/0/all/0/1&quot;&gt;Cai Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;XiangZhi Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08199">
<title>Diffusion-based generation of Histopathological Whole Slide Images at a Gigapixel scale. (arXiv:2311.08199v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.08199</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel diffusion-based approach to generate synthetic
histopathological Whole Slide Images (WSIs) at an unprecedented gigapixel
scale. Synthetic WSIs have many potential applications: They can augment
training datasets to enhance the performance of many computational pathology
applications. They allow the creation of synthesized copies of datasets that
can be shared without violating privacy regulations. Or they can facilitate
learning representations of WSIs without requiring data annotations. Despite
this variety of applications, no existing deep-learning-based method generates
WSIs at their typically high resolutions. Mainly due to the high computational
complexity. Therefore, we propose a novel coarse-to-fine sampling scheme to
tackle image generation of high-resolution WSIs. In this scheme, we increase
the resolution of an initial low-resolution image to a high-resolution WSI.
Particularly, a diffusion model sequentially adds fine details to images and
increases their resolution. In our experiments, we train our method with WSIs
from the TCGA-BRCA dataset. Additionally to quantitative evaluations, we also
performed a user study with pathologists. The study results suggest that our
generated WSIs resemble the structure of real WSIs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Harb_R/0/1/0/all/0/1&quot;&gt;Robert Harb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pock_T/0/1/0/all/0/1&quot;&gt;Thomas Pock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Muller_H/0/1/0/all/0/1&quot;&gt;Heimo M&amp;#xfc;ller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08213">
<title>Unlock the Power: Competitive Distillation for Multi-Modal Large Language Models. (arXiv:2311.08213v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08213</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, multi-modal content generation has attracted lots of attention from
researchers by investigating the utilization of visual instruction tuning based
on large language models (LLMs). To enhance the performance and generalization
ability of such LLMs, the practice of distilling knowledge from pretrained
multi-modal models (a.k.a. teachers) to more compact multi-modal LLMs
(students) has gained considerable interest. However, the prevailing paradigm
of instructiontuning in multi-modal LLMs knowledge distillation is
resource-intensive and unidirectional, neglecting the potential for mutual
feedback between the student and teacher models. Thus, we propose an innovative
Competitive Multi-modal Distillation framework (CoMD), which captures
bidirectional feedback between teacher and student models and continually
updates the multi-modal capabilities that the student model has learned. It
comprises two stages: multi-modal pre-training and multi-modal competitive
distillation. The first stage pre-trains the student model on a large number of
filtered multi-modal datasets. The second stage facilitates a bidirectional
knowledge transfer between the student and teacher models. Our experimental
analysis of diverse datasets shows that our knowledge transfer method
consistently improves the capabilities of the student model. Finally, the
7B-sized student model after four distillations surpassed the current
state-of-the-art model LLaVA-13B on the ScienceQA and LLaVA Test dataset, also
outperforms other strong baselines in the zero-shot setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Li Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chen Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08217">
<title>Peer is Your Pillar: A Data-unbalanced Conditional GANs for Few-shot Image Generation. (arXiv:2311.08217v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08217</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot image generation aims to train generative models using a small
number of training images. When there are few images available for training
(e.g. 10 images), Learning From Scratch (LFS) methods often generate images
that closely resemble the training data while Transfer Learning (TL) methods
try to improve performance by leveraging prior knowledge from GANs pre-trained
on large-scale datasets. However, current TL methods may not allow for
sufficient control over the degree of knowledge preservation from the source
model, making them unsuitable for setups where the source and target domains
are not closely related. To address this, we propose a novel pipeline called
Peer is your Pillar (PIP), which combines a target few-shot dataset with a peer
dataset to create a data-unbalanced conditional generation. Our approach
includes a class embedding method that separates the class space from the
latent space, and we use a direction loss based on pre-trained CLIP to improve
image diversity. Experiments on various few-shot datasets demonstrate the
advancement of the proposed PIP, especially reduces the training requirements
of few-shot image generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaoyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rui_X/0/1/0/all/0/1&quot;&gt;Xue Rui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1&quot;&gt;Chao Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1&quot;&gt;Jiaxu Leng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08223">
<title>Improving Image Captioning via Predicting Structured Concepts. (arXiv:2311.08223v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08223</link>
<description rdf:parseType="Literal">&lt;p&gt;Having the difficulty of solving the semantic gap between images and texts
for the image captioning task, conventional studies in this area paid some
attention to treating semantic concepts as a bridge between the two modalities
and improved captioning performance accordingly. Although promising results on
concept prediction were obtained, the aforementioned studies normally ignore
the relationship among concepts, which relies on not only objects in the image,
but also word dependencies in the text, so that offers a considerable potential
for improving the process of generating good descriptions. In this paper, we
propose a structured concept predictor (SCP) to predict concepts and their
structures, then we integrate them into captioning, so as to enhance the
contribution of visual signals in this task via concepts and further use their
relations to distinguish cross-modal semantics for better description
generation. Particularly, we design weighted graph convolutional networks
(W-GCN) to depict concept relations driven by word dependencies, and then
learns differentiated contributions from these concepts for following decoding
process. Therefore, our approach captures potential relations among concepts
and discriminatively learns different concepts, so that effectively facilitates
image captioning with inherited information across modalities. Extensive
experiments and their results demonstrate the effectiveness of our approach as
well as each proposed module in this work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Ting Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weidong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yuanhe Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1&quot;&gt;Zhendong Mao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08225">
<title>Uni-COAL: A Unified Framework for Cross-Modality Synthesis and Super-Resolution of MR Images. (arXiv:2311.08225v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.08225</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-modality synthesis (CMS), super-resolution (SR), and their combination
(CMSR) have been extensively studied for magnetic resonance imaging (MRI).
Their primary goals are to enhance the imaging quality by synthesizing the
desired modality and reducing the slice thickness. Despite the promising
synthetic results, these techniques are often tailored to specific tasks,
thereby limiting their adaptability to complex clinical scenarios. Therefore,
it is crucial to build a unified network that can handle various image
synthesis tasks with arbitrary requirements of modality and resolution
settings, so that the resources for training and deploying the models can be
greatly reduced. However, none of the previous works is capable of performing
CMS, SR, and CMSR using a unified network. Moreover, these MRI reconstruction
methods often treat alias frequencies improperly, resulting in suboptimal
detail restoration. In this paper, we propose a Unified Co-Modulated Alias-free
framework (Uni-COAL) to accomplish the aforementioned tasks with a single
network. The co-modulation design of the image-conditioned and stochastic
attribute representations ensures the consistency between CMS and SR, while
simultaneously accommodating arbitrary combinations of input/output modalities
and thickness. The generator of Uni-COAL is also designed to be alias-free
based on the Shannon-Nyquist signal processing framework, ensuring effective
suppression of alias frequencies. Additionally, we leverage the semantic prior
of Segment Anything Model (SAM) to guide Uni-COAL, ensuring a more authentic
preservation of anatomical structures during synthesis. Experiments on three
datasets demonstrate that Uni-COAL outperforms the alternatives in CMS, SR, and
CMSR tasks for MR images, which highlights its generalizability to wide-range
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhiyun Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qi_Z/0/1/0/all/0/1&quot;&gt;Zengxin Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhenrong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fei_M/0/1/0/all/0/1&quot;&gt;Manman Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zang_D/0/1/0/all/0/1&quot;&gt;Di Zang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dongdong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yao_L/0/1/0/all/0/1&quot;&gt;Linlin Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xuehai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lichi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08236">
<title>MeLo: Low-rank Adaptation is Better than Fine-tuning for Medical Image Diagnosis. (arXiv:2311.08236v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08236</link>
<description rdf:parseType="Literal">&lt;p&gt;The common practice in developing computer-aided diagnosis (CAD) models based
on transformer architectures usually involves fine-tuning from ImageNet
pre-trained weights. However, with recent advances in large-scale pre-training
and the practice of scaling laws, Vision Transformers (ViT) have become much
larger and less accessible to medical imaging communities. Additionally, in
real-world scenarios, the deployments of multiple CAD models can be troublesome
due to problems such as limited storage space and time-consuming model
switching. To address these challenges, we propose a new method MeLo (Medical
image Low-rank adaptation), which enables the development of a single CAD model
for multiple clinical tasks in a lightweight manner. It adopts low-rank
adaptation instead of resource-demanding fine-tuning. By fixing the weight of
ViT models and only adding small low-rank plug-ins, we achieve competitive
results on various diagnosis tasks across different imaging modalities using
only a few trainable parameters. Specifically, our proposed method achieves
comparable performance to fully fine-tuned ViT models on four distinct medical
imaging datasets using about 0.17% trainable parameters. Moreover, MeLo adds
only about 0.5MB of storage space and allows for extremely fast model switching
in deployment and inference. Our source code and pre-trained weights are
available on our website (https://absterzhu.github.io/melo.github.io/).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yitao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhenrong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zihao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dinggang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qian Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08239">
<title>Learning Physics-Inspired Regularization for Medical Image Registration with Hypernetworks. (arXiv:2311.08239v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.08239</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image registration aims at identifying the spatial deformation
between images of the same anatomical region and is fundamental to image-based
diagnostics and therapy. To date, the majority of the deep learning-based
registration methods employ regularizers that enforce global spatial
smoothness, e.g., the diffusion regularizer. However, such regularizers are not
tailored to the data and might not be capable of reflecting the complex
underlying deformation. In contrast, physics-inspired regularizers promote
physically plausible deformations. One such regularizer is the linear elastic
regularizer which models the deformation of elastic material. These
regularizers are driven by parameters that define the material&apos;s physical
properties. For biological tissue, a wide range of estimations of such
parameters can be found in the literature and it remains an open challenge to
identify suitable parameter values for successful registration. To overcome
this problem and to incorporate physical properties into learning-based
registration, we propose to use a hypernetwork that learns the effect of the
physical parameters of a physics-inspired regularizer on the resulting spatial
deformation field. In particular, we adapt the HyperMorph framework to learn
the effect of the two elasticity parameters of the linear elastic regularizer.
Our approach enables the efficient discovery of suitable, data-specific
physical parameters at test time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Reithmeir_A/0/1/0/all/0/1&quot;&gt;Anna Reithmeir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schnabel_J/0/1/0/all/0/1&quot;&gt;Julia A. Schnabel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zimmer_V/0/1/0/all/0/1&quot;&gt;Veronika A. Zimmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08245">
<title>TENT: Connect Language Models with IoT Sensors for Zero-Shot Activity Recognition. (arXiv:2311.08245v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08245</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent achievements in language models have showcased their extraordinary
capabilities in bridging visual information with semantic language
understanding. This leads us to a novel question: can language models connect
textual semantics with IoT sensory signals to perform recognition tasks, e.g.,
Human Activity Recognition (HAR)? If so, an intelligent HAR system with
human-like cognition can be built, capable of adapting to new environments and
unseen categories. This paper explores its feasibility with an innovative
approach, IoT-sEnsors-language alignmEnt pre-Training (TENT), which jointly
aligns textual embeddings with IoT sensor signals, including camera video,
LiDAR, and mmWave. Through the IoT-language contrastive learning, we derive a
unified semantic feature space that aligns multi-modal features with language
embeddings, so that the IoT data corresponds to specific words that describe
the IoT data. To enhance the connection between textual categories and their
IoT data, we propose supplementary descriptions and learnable prompts that
bring more semantic information into the joint feature space. TENT can not only
recognize actions that have been seen but also ``guess&apos;&apos; the unseen action by
the closest textual words from the feature space. We demonstrate TENT achieves
state-of-the-art performance on zero-shot HAR tasks using different modalities,
improving the best vision-language models by over 12%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yunjiao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianfei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1&quot;&gt;Han Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Lihua Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08265">
<title>On The Relationship Between Universal Adversarial Attacks And Sparse Representations. (arXiv:2311.08265v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08265</link>
<description rdf:parseType="Literal">&lt;p&gt;The prominent success of neural networks, mainly in computer vision tasks, is
increasingly shadowed by their sensitivity to small, barely perceivable
adversarial perturbations in image input.
&lt;/p&gt;
&lt;p&gt;In this work, we aim at explaining this vulnerability through the framework
of sparsity.
&lt;/p&gt;
&lt;p&gt;We show the connection between adversarial attacks and sparse
representations, with a focus on explaining the universality and
transferability of adversarial examples in neural networks.
&lt;/p&gt;
&lt;p&gt;To this end, we show that sparse coding algorithms, and the neural
network-based learned iterative shrinkage thresholding algorithm (LISTA) among
them, suffer from this sensitivity, and that common attacks on neural networks
can be expressed as attacks on the sparse representation of the input image.
The phenomenon that we observe holds true also when the network is agnostic to
the sparse representation and dictionary, and thus can provide a possible
explanation for the universality and transferability of adversarial attacks.
&lt;/p&gt;
&lt;p&gt;The code is available at
https://github.com/danawr/adversarial_attacks_and_sparse_representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weitzner_D/0/1/0/all/0/1&quot;&gt;Dana Weitzner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1&quot;&gt;Raja Giryes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08269">
<title>Defining the boundaries: challenges and advances in identifying cells in microscopy images. (arXiv:2311.08269v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2311.08269</link>
<description rdf:parseType="Literal">&lt;p&gt;Segmentation, or the outlining of objects within images, is a critical step
in the measurement and analysis of cells within microscopy images. While
improvements continue to be made in tools that rely on classical methods for
segmentation, deep learning-based tools increasingly dominate advances in the
technology. Specialist models such as Cellpose continue to improve in accuracy
and user-friendliness, and segmentation challenges such as the Multi-Modality
Cell Segmentation Challenge continue to push innovation in accuracy across
widely-varying test data as well as efficiency and usability. Increased
attention on documentation, sharing, and evaluation standards are leading to
increased user-friendliness and acceleration towards the goal of a truly
universal method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gogoberidze_N/0/1/0/all/0/1&quot;&gt;Nodar Gogoberidze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cimini_B/0/1/0/all/0/1&quot;&gt;Beth A. Cimini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08278">
<title>ARTEMIS: Using GANs with Multiple Discriminators to Generate Art. (arXiv:2311.08278v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08278</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel method for generating abstract art. First an autoencoder
is trained to encode and decode the style representations of images, which are
extracted from source images with a pretrained VGG network. Then, the decoder
component of the autoencoder is extracted and used as a generator in a GAN. The
generator works with an ensemble of discriminators. Each discriminator takes
different style representations of the same images, and the generator is
trained to create images that create convincing style representations in order
to deceive all of the generators. The generator is also trained to maximize a
diversity term. The resulting images had a surreal, geometric quality. We call
our approach ARTEMIS (ARTistic Encoder- Multi- Discriminators Including
Self-Attention), as it uses the self-attention layers and an encoder-decoder
architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baker_J/0/1/0/all/0/1&quot;&gt;James Baker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.10859">
<title>Visualizing the Diversity of Representations Learned by Bayesian Neural Networks. (arXiv:2201.10859v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2201.10859</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainable Artificial Intelligence (XAI) aims to make learning machines less
opaque, and offers researchers and practitioners various tools to reveal the
decision-making strategies of neural networks. In this work, we investigate how
XAI methods can be used for exploring and visualizing the diversity of feature
representations learned by Bayesian Neural Networks (BNNs). Our goal is to
provide a global understanding of BNNs by making their decision-making
strategies a) visible and tangible through feature visualizations and b)
quantitatively measurable with a distance measure learned by contrastive
learning. Our work provides new insights into the \emph{posterior} distribution
in terms of human-understandable feature information with regard to the
underlying decision making strategies. The main findings of our work are the
following: 1) global XAI methods can be applied to explain the diversity of
decision-making strategies of BNN instances, 2) Monte Carlo dropout with
commonly used Dropout rates exhibit increased diversity in feature
representations compared to the multimodal posterior approximation of
MultiSWAG, 3) the diversity of learned feature representations highly
correlates with the uncertainty estimate for the output and 4) the inter-mode
diversity of the multimodal posterior decreases as the network width increases,
while the intra mode diversity increases. These findings are consistent with
the recent Deep Neural Networks theory, providing additional intuitions about
what the theory implies in terms of humanly understandable concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grinwald_D/0/1/0/all/0/1&quot;&gt;Dennis Grinwald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bykov_K/0/1/0/all/0/1&quot;&gt;Kirill Bykov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1&quot;&gt;Shinichi Nakajima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1&quot;&gt;Marina M.-C. H&amp;#xf6;hne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.03359">
<title>An efficient semi-supervised quality control system trained using physics-based MRI-artefact generators and adversarial training. (arXiv:2206.03359v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.03359</link>
<description rdf:parseType="Literal">&lt;p&gt;Large medical imaging data sets are becoming increasingly available, but
ensuring sample quality without significant artefacts is challenging. Existing
methods for identifying imperfections in medical imaging rely on data-intensive
approaches, compounded by a scarcity of artefact-rich scans for training
machine learning models in clinical research. To tackle this problem, we
propose a framework with four main components: 1) artefact generators inspired
by magnetic resonance physics to corrupt brain MRI scans and augment a training
dataset, 2) abstract and engineered features to represent images compactly, 3)
a feature selection process depending on the artefact class to improve
classification, and 4) SVM classifiers to identify artefacts. Our contributions
are threefold: first, physics-based artefact generators produce synthetic brain
MRI scans with controlled artefacts for data augmentation. This will avoid the
labour-intensive collection and labelling process of scans with rare artefacts.
Second, we propose a pool of abstract and engineered image features to identify
9 different artefacts for structural MRI. Finally, we use an artefact-based
feature selection block that, for each class of artefacts, finds the set of
features providing the best classification performance. We performed validation
experiments on a large data set of scans with artificially-generated artefacts,
and in a multiple sclerosis clinical trial where real artefacts were identified
by experts, showing that the proposed pipeline outperforms traditional methods.
In particular, our data augmentation increases performance by up to 12.5
percentage points on accuracy, precision, and recall. The computational
efficiency of our pipeline enables potential real-time deployment, promising
high-throughput clinical applications through automated image-processing
pipelines driven by quality control systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ravi_D/0/1/0/all/0/1&quot;&gt;Daniele Ravi&lt;/a&gt; (for the Alzheimer&amp;#x27;s Disease Neuroimaging Initiative), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Barkhof_F/0/1/0/all/0/1&quot;&gt;Frederik Barkhof&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alexander_D/0/1/0/all/0/1&quot;&gt;Daniel C. Alexander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Puglisi_L/0/1/0/all/0/1&quot;&gt;Lemuel Puglisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Parker_G/0/1/0/all/0/1&quot;&gt;Geoffrey JM Parker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eshaghi_A/0/1/0/all/0/1&quot;&gt;Arman Eshaghi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.14191">
<title>Learning with Limited Annotations: A Survey on Deep Semi-Supervised Learning for Medical Image Segmentation. (arXiv:2207.14191v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.14191</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image segmentation is a fundamental and critical step in many
image-guided clinical approaches. Recent success of deep learning-based
segmentation methods usually relies on a large amount of labeled data, which is
particularly difficult and costly to obtain especially in the medical imaging
domain where only experts can provide reliable and accurate annotations.
Semi-supervised learning has emerged as an appealing strategy and been widely
applied to medical image segmentation tasks to train deep models with limited
annotations. In this paper, we present a comprehensive review of recently
proposed semi-supervised learning methods for medical image segmentation and
summarized both the technical novelties and empirical results. Furthermore, we
analyze and discuss the limitations and several unsolved problems of existing
approaches. We hope this review could inspire the research community to explore
solutions for this challenge and further promote the developments in medical
image segmentation field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_R/0/1/0/all/0/1&quot;&gt;Rushi Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yichi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Le Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1&quot;&gt;Rong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jicong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.08355">
<title>Differentiable Topology-Preserved Distance Transform for Pulmonary Airway Segmentation. (arXiv:2209.08355v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.08355</link>
<description rdf:parseType="Literal">&lt;p&gt;Detailed pulmonary airway segmentation is a clinically important task for
endobronchial intervention and treatment of peripheral located lung cancer
lesions. Convolutional Neural Networks (CNNs) are promising tools for medical
image analysis but have been performing poorly for cases when existing a
significant imbalanced feature distribution, which is true for the airway data
as the trachea and principal bronchi dominate most of the voxels whereas the
lobar bronchi and distal segmental bronchi occupy a small proportion. In this
paper, we propose a Differentiable Topology-Preserved Distance Transform
(DTPDT) framework to improve the performance of airway segmentation. A
Topology-Preserved Surrogate (TPS) learning strategy is first proposed to
balance the training progress within-class distribution. Furthermore, a
Convolutional Distance Transform (CDT) is designed to identify the breakage
phenomenon with superior sensitivity and minimize the variation of the distance
map between the predictionand ground-truth. The proposed method is validated
with the publically available reference airway segmentation datasets. The
detected rate of branch and length on public EXACT&apos;09 and BAS datasets are
82.1%/79.6% and 96.5%/91.5% respectively, demonstrating the reliability and
efficiency of the method in terms of improving the topology completeness of the
segmentation performance while maintaining the overall topology accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Minghui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guang-Zhong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yun Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.03461">
<title>FastCLIPstyler: Optimisation-free Text-based Image Style Transfer Using Style Representations. (arXiv:2210.03461v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.03461</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, language-driven artistic style transfer has emerged as a new
type of style transfer technique, eliminating the need for a reference style
image by using natural language descriptions of the style. The first model to
achieve this, called CLIPstyler, has demonstrated impressive stylisation
results. However, its lengthy optimisation procedure at runtime for each query
limits its suitability for many practical applications. In this work, we
present FastCLIPstyler, a generalised text-based image style transfer model
capable of stylising images in a single forward pass for arbitrary text inputs.
Furthermore, we introduce EdgeCLIPstyler, a lightweight model designed for
compatibility with resource-constrained devices. Through quantitative and
qualitative comparisons with state-of-the-art approaches, we demonstrate that
our models achieve superior stylisation quality based on measurable metrics
while offering significantly improved runtime efficiency, particularly on edge
devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1&quot;&gt;Ananda Padhmanabhan Suresh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Sanjana Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noinongyao_P/0/1/0/all/0/1&quot;&gt;Pavit Noinongyao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1&quot;&gt;Ankush Ganguly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watchareeruetai_U/0/1/0/all/0/1&quot;&gt;Ukrit Watchareeruetai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samacoits_A/0/1/0/all/0/1&quot;&gt;Aubin Samacoits&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02914">
<title>Robust Reflection Removal with Flash-only Cues in the Wild. (arXiv:2211.02914v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.02914</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a simple yet effective reflection-free cue for robust reflection
removal from a pair of flash and ambient (no-flash) images. The reflection-free
cue exploits a flash-only image obtained by subtracting the ambient image from
the corresponding flash image in raw data space. The flash-only image is
equivalent to an image taken in a dark environment with only a flash on. This
flash-only image is visually reflection-free and thus can provide robust cues
to infer the reflection in the ambient image. Since the flash-only image
usually has artifacts, we further propose a dedicated model that not only
utilizes the reflection-free cue but also avoids introducing artifacts, which
helps accurately estimate reflection and transmission. Our experiments on
real-world images with various types of reflection demonstrate the
effectiveness of our model with reflection-free flash-only cues: our model
outperforms state-of-the-art reflection removal approaches by more than 5.23dB
in PSNR. We extend our approach to handheld photography to address the
misalignment between the flash and no-flash pair. With misaligned training data
and the alignment module, our aligned model outperforms our previous version by
more than 3.19dB in PSNR on a misaligned dataset. We also study using linear
RGB images as training data. Our source code and dataset are publicly available
at https://github.com/ChenyangLEI/flash-reflection-removal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1&quot;&gt;Chenyang Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xudong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qifeng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.08253">
<title>HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization. (arXiv:2211.08253v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.08253</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to domain shifts, machine learning systems typically struggle to
generalize well to new domains that differ from those of training data, which
is what domain generalization (DG) aims to address. Although a variety of DG
methods have been proposed, most of them fall short in interpretability and
require domain labels, which are not available in many real-world scenarios.
This paper presents a novel DG method, called HMOE: Hypernetwork-based Mixture
of Experts (MoE), which does not rely on domain labels and is more
interpretable. MoE proves effective in identifying heterogeneous patterns in
data. For the DG problem, heterogeneity arises exactly from domain shifts. HMOE
employs hypernetworks taking vectors as input to generate the weights of
experts, which promotes knowledge sharing among experts and enables the
exploration of their similarities in a low-dimensional vector space. We
benchmark HMOE against other DG methods under a fair evaluation framework --
DomainBed. Our extensive experiments show that HMOE can effectively separate
mixed-domain data into distinct clusters that are surprisingly more consistent
with human intuition than original domain labels. Using self-learned domain
information, HMOE achieves state-of-the-art results on most datasets and
significantly surpasses other DG methods in average accuracy across all
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1&quot;&gt;Jingang Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faney_T/0/1/0/all/0/1&quot;&gt;Thibault Faney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1&quot;&gt;Patrick Gallinari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yousef_S/0/1/0/all/0/1&quot;&gt;Soleiman Yousef&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemptinne_J/0/1/0/all/0/1&quot;&gt;Jean-Charles de Hemptinne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13854">
<title>ComCLIP: Training-Free Compositional Image and Text Matching. (arXiv:2211.13854v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13854</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive Language-Image Pretraining (CLIP) has demonstrated great
zero-shot performance for matching images and text. However, it is still
challenging to adapt vision-lanaguage pretrained models like CLIP to
compositional image and text matching -- a more challenging image and text
matching task requiring the model understanding of compositional word concepts
and visual components. Towards better compositional generalization in zero-shot
image and text matching, in this paper, we study the problem from a causal
perspective: the erroneous semantics of individual entities are essentially
confounders that cause the matching failure. Therefore, we propose a novel
\textbf{\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP
disentangles input images into subjects, objects, and action sub-images and
composes CLIP&apos;s vision encoder and text encoder to perform evolving matching
over compositional text embedding and sub-image embeddings. In this way,
ComCLIP can mitigate spurious correlations introduced by the pretrained CLIP
models and dynamically evaluate the importance of each component. Experiments
on four compositional image-text matching datasets: SVO, ComVG, Winoground, and
VL-checklist, and two general image-text retrieval datasets: Flick30K, and
MSCOCO demonstrate the effectiveness of our plug-and-play method, which boosts
the \textbf{\textit{zero-shot}} inference ability of CLIP, SLIP, and BLIP2 even
without further training or fine-tuning. Our codes can be found at
https://github.com/eric-ai-lab/ComCLIP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1&quot;&gt;Kenan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xuehai He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Ruize Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Eric Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.09702">
<title>Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification. (arXiv:2301.09702v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.09702</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain adaptive (UDA) person re-identification (re-ID) aims to
learn identity information from labeled images in source domains and apply it
to unlabeled images in a target domain. One major issue with many unsupervised
re-identification methods is that they do not perform well relative to large
domain variations such as illumination, viewpoint, and occlusions. In this
paper, we propose a Synthesis Model Bank (SMB) to deal with illumination
variation in unsupervised person re-ID. The proposed SMB consists of several
convolutional neural networks (CNN) for feature extraction and Mahalanobis
matrices for distance metrics. They are trained using synthetic data with
different illumination conditions such that their synergistic effect makes the
SMB robust against illumination variation. To better quantify the illumination
intensity and improve the quality of synthetic images, we introduce a new 3D
virtual-human dataset for GAN-based image synthesis. From our experiments, the
proposed SMB outperforms other synthesis methods on several re-ID benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jiaqi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Reibman_A/0/1/0/all/0/1&quot;&gt;Amy R. Reibman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Delp_E/0/1/0/all/0/1&quot;&gt;Edward J. Delp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13591">
<title>Zero3D: Semantic-Driven Multi-Category 3D Shape Generation. (arXiv:2301.13591v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13591</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic-driven 3D shape generation aims to generate 3D objects conditioned
on text. Previous works face problems with single-category generation,
low-frequency 3D details, and requiring a large number of paired datasets for
training. To tackle these challenges, we propose a multi-category conditional
diffusion model. Specifically, 1) to alleviate the problem of lack of
large-scale paired data, we bridge the text, 2D image and 3D shape based on the
pre-trained CLIP model, and 2) to obtain the multi-category 3D shape feature,
we apply the conditional flow model to generate 3D shape vector conditioned on
CLIP embedding. 3) to generate multi-category 3D shape, we employ the
hidden-layer diffusion model conditioned on the multi-category shape vector,
which greatly reduces the training time and memory consumption.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Bo Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yitong Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yixuan Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06015">
<title>A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity. (arXiv:2302.06015v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06015</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) with self-attention modules have recently achieved
great empirical success in many vision tasks. Due to non-convex interactions
across layers, however, theoretical learning and generalization analysis is
mostly elusive. Based on a data model characterizing both label-relevant and
label-irrelevant tokens, this paper provides the first theoretical analysis of
training a shallow ViT, i.e., one self-attention layer followed by a two-layer
perceptron, for a classification task. We characterize the sample complexity to
achieve a zero generalization error. Our sample complexity bound is positively
correlated with the inverse of the fraction of label-relevant tokens, the token
noise level, and the initial model error. We also prove that a training process
using stochastic gradient descent (SGD) leads to a sparse attention map, which
is a formal verification of the general intuition about the success of
attention. Moreover, this paper indicates that a proper token sparsification
can improve the test performance by removing label-irrelevant and/or noisy
tokens, including spurious correlations. Empirical experiments on synthetic
data and CIFAR-10 dataset justify our theoretical results and generalize to
deeper ViTs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongkang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sijia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-yu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.02867">
<title>Boundary-semantic collaborative guidance network with dual-stream feedback mechanism for salient object detection in optical remote sensing imagery. (arXiv:2303.02867v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.02867</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing application of deep learning in various domains, salient
object detection in optical remote sensing images (ORSI-SOD) has attracted
significant attention. However, most existing ORSI-SOD methods predominantly
rely on local information from low-level features to infer salient boundary
cues and supervise them using boundary ground truth, but fail to sufficiently
optimize and protect the local information, and almost all approaches ignore
the potential advantages offered by the last layer of the decoder to maintain
the integrity of saliency maps. To address these issues, we propose a novel
method named boundary-semantic collaborative guidance network (BSCGNet) with
dual-stream feedback mechanism. First, we propose a boundary protection
calibration (BPC) module, which effectively reduces the loss of edge position
information during forward propagation and suppresses noise in low-level
features without relying on boundary ground truth. Second, based on the BPC
module, a dual feature feedback complementary (DFFC) module is proposed, which
aggregates boundary-semantic dual features and provides effective feedback to
coordinate features across different layers, thereby enhancing cross-scale
knowledge communication. Finally, to obtain more complete saliency maps, we
consider the uniqueness of the last layer of the decoder for the first time and
propose the adaptive feedback refinement (AFR) module, which further refines
feature representation and eliminates differences between features through a
unique feedback mechanism. Extensive experiments on three benchmark datasets
demonstrate that BSCGNet exhibits distinct advantages in challenging scenarios
and outperforms the 17 state-of-the-art (SOTA) approaches proposed in recent
years. Codes and results have been released on GitHub:
https://github.com/YUHsss/BSCGNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1&quot;&gt;Dejun Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hongyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Suning Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1&quot;&gt;Ziyang Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xingyu Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yakun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.02192">
<title>A Diffusion-based Method for Multi-turn Compositional Image Generation. (arXiv:2304.02192v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.02192</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-turn compositional image generation (M-CIG) is a challenging task that
aims to iteratively manipulate a reference image given a modification text.
While most of the existing methods for M-CIG are based on generative
adversarial networks (GANs), recent advances in image generation have
demonstrated the superiority of diffusion models over GANs. In this paper, we
propose a diffusion-based method for M-CIG named conditional denoising
diffusion with image compositional matching (CDD-ICM). We leverage CLIP as the
backbone of image and text encoders, and incorporate a gated fusion mechanism,
originally proposed for question answering, to compositionally fuse the
reference image and the modification text at each turn of M-CIG. We introduce a
conditioning scheme to generate the target image based on the fusion results.
To prioritize the semantic quality of the generated target image, we learn an
auxiliary image compositional match (ICM) objective, along with the conditional
denoising diffusion (CDD) objective in a multi-task learning framework.
Additionally, we also perform ICM guidance and classifier-free guidance to
improve performance. Experimental results show that CDD-ICM achieves
state-of-the-art results on two benchmark datasets for M-CIG, i.e., CoDraw and
i-CLEVR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03174">
<title>SketchFFusion: Sketch-guided image editing with diffusion model. (arXiv:2304.03174v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03174</link>
<description rdf:parseType="Literal">&lt;p&gt;Sketch-guided image editing aims to achieve local fine-tuning of the image
based on the sketch information provided by the user, while maintaining the
original status of the unedited areas. Due to the high cost of acquiring human
sketches, previous works mostly relied on edge maps as a substitute for
sketches, but sketches possess more rich structural information. In this paper,
we propose a sketch generation scheme that can preserve the main contours of an
image and closely adhere to the actual sketch style drawn by the user.
Simultaneously, current image editing methods often face challenges such as
image distortion, training cost, and loss of fine details in the sketch. To
address these limitations, We propose a conditional diffusion model
(SketchFFusion) based on the sketch structure vector. We evaluate the
generative performance of our model and demonstrate that it outperforms
existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1&quot;&gt;Weihang Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Bo Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.05071">
<title>Fracture Detection in Pediatric Wrist Trauma X-ray Images Using YOLOv8 Algorithm. (arXiv:2304.05071v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.05071</link>
<description rdf:parseType="Literal">&lt;p&gt;Hospital emergency departments frequently receive lots of bone fracture
cases, with pediatric wrist trauma fracture accounting for the majority of
them. Before pediatric surgeons perform surgery, they need to ask patients how
the fracture occurred and analyze the fracture situation by interpreting X-ray
images. The interpretation of X-ray images often requires a combination of
techniques from radiologists and surgeons, which requires time-consuming
specialized training. With the rise of deep learning in the field of computer
vision, network models applying for fracture detection has become an important
research topic. In this paper, we use data augmentation to improve the model
performance of YOLOv8 algorithm (the latest version of You Only Look Once) on a
pediatric wrist trauma X-ray dataset (GRAZPEDWRI-DX), which is a public
dataset. The experimental results show that our model has reached the
state-of-the-art (SOTA) mean average precision (mAP 50). Specifically, mAP 50
of our model is 0.638, which is significantly higher than the 0.634 and 0.636
of the improved YOLOv7 and original YOLOv8 models. To enable surgeons to use
our model for fracture detection on pediatric wrist trauma X-ray images, we
have designed the application &quot;Fracture Detection Using YOLOv8 App&quot; to assist
surgeons in diagnosing fractures, reducing the probability of error analysis,
and providing more useful information for surgery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_R/0/1/0/all/0/1&quot;&gt;Rui-Yang Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Weiming Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02143">
<title>GANonymization: A GAN-based Face Anonymization Framework for Preserving Emotional Expressions. (arXiv:2305.02143v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02143</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the increasing availability of personal data has raised
concerns regarding privacy and security. One of the critical processes to
address these concerns is data anonymization, which aims to protect individual
privacy and prevent the release of sensitive information. This research focuses
on the importance of face anonymization. Therefore, we introduce
GANonymization, a novel face anonymization framework with facial
expression-preserving abilities. Our approach is based on a high-level
representation of a face, which is synthesized into an anonymized version based
on a generative adversarial network (GAN). The effectiveness of the approach
was assessed by evaluating its performance in removing identifiable facial
attributes to increase the anonymity of the given individual face.
Additionally, the performance of preserving facial expressions was evaluated on
several affect recognition datasets and outperformed the state-of-the-art
methods in most categories. Finally, our approach was analyzed for its ability
to remove various facial traits, such as jewelry, hair color, and multiple
others. Here, it demonstrated reliable performance in removing these
attributes. Our results suggest that GANonymization is a promising approach for
anonymizing faces while preserving facial expressions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hellmann_F/0/1/0/all/0/1&quot;&gt;Fabio Hellmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mertes_S/0/1/0/all/0/1&quot;&gt;Silvan Mertes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benouis_M/0/1/0/all/0/1&quot;&gt;Mohamed Benouis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hustinx_A/0/1/0/all/0/1&quot;&gt;Alexander Hustinx&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_T/0/1/0/all/0/1&quot;&gt;Tzung-Chien Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conati_C/0/1/0/all/0/1&quot;&gt;Cristina Conati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krawitz_P/0/1/0/all/0/1&quot;&gt;Peter Krawitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1&quot;&gt;Elisabeth Andr&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13277">
<title>U-TILISE: A Sequence-to-sequence Model for Cloud Removal in Optical Satellite Time Series. (arXiv:2305.13277v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13277</link>
<description rdf:parseType="Literal">&lt;p&gt;Satellite image time series in the optical and infrared spectrum suffer from
frequent data gaps due to cloud cover, cloud shadows, and temporary sensor
outages. It has been a long-standing problem of remote sensing research how to
best reconstruct the missing pixel values and obtain complete, cloud-free image
sequences. We approach that problem from the perspective of representation
learning and develop U-TILISE, an efficient neural model that is able to
implicitly capture spatio-temporal patterns of the spectral intensities, and
that can therefore be trained to map a cloud-masked input sequence to a
cloud-free output sequence. The model consists of a convolutional spatial
encoder that maps each individual frame of the input sequence to a latent
encoding; an attention-based temporal encoder that captures dependencies
between those per-frame encodings and lets them exchange information along the
time dimension; and a convolutional spatial decoder that decodes the latent
embeddings back into multi-spectral images. We experimentally evaluate the
proposed model on EarthNet2021, a dataset of Sentinel-2 time series acquired
all over Europe, and demonstrate its superior ability to reconstruct the
missing pixels. Compared to a standard interpolation baseline, it increases the
PSNR by 1.8 dB at previously seen locations and by 1.3 dB at unseen locations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stucker_C/0/1/0/all/0/1&quot;&gt;Corinne Stucker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garnot_V/0/1/0/all/0/1&quot;&gt;Vivien Sainte Fare Garnot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1&quot;&gt;Konrad Schindler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06210">
<title>Single-Model Attribution of Generative Models Through Final-Layer Inversion. (arXiv:2306.06210v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06210</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent breakthroughs in generative modeling have sparked interest in
practical single-model attribution. Such methods predict whether a sample was
generated by a specific generator or not, for instance, to prove intellectual
property theft. However, previous works are either limited to the closed-world
setting or require undesirable changes to the generative model. We address
these shortcomings by, first, viewing single-model attribution through the lens
of anomaly detection. Arising from this change of perspective, we propose
FLIPAD, a new approach for single-model attribution in the open-world setting
based on final-layer inversion and anomaly detection. We show that the utilized
final-layer inversion can be reduced to a convex lasso optimization problem,
making our approach theoretically sound and computationally efficient. The
theoretical findings are accompanied by an experimental study demonstrating the
effectiveness of our approach and its flexibility to various domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laszkiewicz_M/0/1/0/all/0/1&quot;&gt;Mike Laszkiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ricker_J/0/1/0/all/0/1&quot;&gt;Jonas Ricker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lederer_J/0/1/0/all/0/1&quot;&gt;Johannes Lederer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1&quot;&gt;Asja Fischer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09126">
<title>STARSS23: An Audio-Visual Dataset of Spatial Recordings of Real Scenes with Spatiotemporal Annotations of Sound Events. (arXiv:2306.09126v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09126</link>
<description rdf:parseType="Literal">&lt;p&gt;While direction of arrival (DOA) of sound events is generally estimated from
multichannel audio data recorded in a microphone array, sound events usually
derive from visually perceptible source objects, e.g., sounds of footsteps come
from the feet of a walker. This paper proposes an audio-visual sound event
localization and detection (SELD) task, which uses multichannel audio and video
information to estimate the temporal activation and DOA of target sound events.
Audio-visual SELD systems can detect and localize sound events using signals
from a microphone array and audio-visual correspondence. We also introduce an
audio-visual dataset, Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23),
which consists of multichannel audio data recorded with a microphone array,
video data, and spatiotemporal annotation of sound events. Sound scenes in
STARSS23 are recorded with instructions, which guide recording participants to
ensure adequate activity and occurrences of sound events. STARSS23 also serves
human-annotated temporal activation labels and human-confirmed DOA labels,
which are based on tracking results of a motion capture system. Our benchmark
results demonstrate the benefits of using visual object positions in
audio-visual SELD tasks. The data is available at
https://zenodo.org/record/7880637.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shimada_K/0/1/0/all/0/1&quot;&gt;Kazuki Shimada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Politis_A/0/1/0/all/0/1&quot;&gt;Archontis Politis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sudarsanam_P/0/1/0/all/0/1&quot;&gt;Parthasaarathy Sudarsanam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krause_D/0/1/0/all/0/1&quot;&gt;Daniel Krause&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uchida_K/0/1/0/all/0/1&quot;&gt;Kengo Uchida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adavanne_S/0/1/0/all/0/1&quot;&gt;Sharath Adavanne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hakala_A/0/1/0/all/0/1&quot;&gt;Aapo Hakala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyama_Y/0/1/0/all/0/1&quot;&gt;Yuichiro Koyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takahashi_N/0/1/0/all/0/1&quot;&gt;Naoya Takahashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takahashi_S/0/1/0/all/0/1&quot;&gt;Shusuke Takahashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Virtanen_T/0/1/0/all/0/1&quot;&gt;Tuomas Virtanen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitsufuji_Y/0/1/0/all/0/1&quot;&gt;Yuki Mitsufuji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12540">
<title>UniFormaly: Towards Task-Agnostic Unified Framework for Visual Anomaly Detection. (arXiv:2307.12540v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12540</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual anomaly detection aims to learn normality from normal images, but
existing approaches are fragmented across various tasks: defect detection,
semantic anomaly detection, multi-class anomaly detection, and anomaly
clustering. This one-task-one-model approach is resource-intensive and incurs
high maintenance costs as the number of tasks increases. We present UniFormaly,
a universal and powerful anomaly detection framework. We emphasize the
necessity of our off-the-shelf approach by pointing out a suboptimal issue in
online encoder-based methods. We introduce Back Patch Masking (BPM) and top
k-ratio feature matching to achieve unified anomaly detection. BPM eliminates
irrelevant background regions using a self-attention map from self-supervised
ViTs. This operates in a task-agnostic manner and alleviates memory storage
consumption, scaling to tasks with large-scale datasets. Top k-ratio feature
matching unifies anomaly levels and tasks by casting anomaly scoring into
multiple instance learning. Finally, UniFormaly achieves outstanding results on
various tasks and datasets. Codes are available at
https://github.com/YoojLee/Uniformaly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yujin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1&quot;&gt;Harin Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1&quot;&gt;Seoyoon Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1&quot;&gt;Hyunsoo Yoon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14277">
<title>G2L: Semantically Aligned and Uniform Video Grounding via Geodesic and Game Theory. (arXiv:2307.14277v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14277</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent video grounding works attempt to introduce vanilla contrastive
learning into video grounding. However, we claim that this naive solution is
suboptimal. Contrastive learning requires two key properties: (1)
\emph{alignment} of features of similar samples, and (2) \emph{uniformity} of
the induced distribution of the normalized features on the hypersphere. Due to
two annoying issues in video grounding: (1) the co-existence of some visual
entities in both ground truth and other moments, \ie semantic overlapping; (2)
only a few moments in the video are annotated, \ie sparse annotation dilemma,
vanilla contrastive learning is unable to model the correlations between
temporally distant moments and learned inconsistent video representations. Both
characteristics lead to vanilla contrastive learning being unsuitable for video
grounding. In this paper, we introduce Geodesic and Game Localization (G2L), a
semantically aligned and uniform video grounding framework via geodesic and
game theory. We quantify the correlations among moments leveraging the geodesic
distance that guides the model to learn the correct cross-modal
representations. Furthermore, from the novel perspective of game theory, we
propose semantic Shapley interaction based on geodesic distance sampling to
learn fine-grained semantic alignment in similar moments. Experiments on three
benchmarks demonstrate the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongxiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1&quot;&gt;Meng Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xuxin Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yaowei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhihong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1&quot;&gt;Yuexian Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16694">
<title>Investigating and Improving Latent Density Segmentation Models for Aleatoric Uncertainty Quantification in Medical Imaging. (arXiv:2307.16694v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16694</link>
<description rdf:parseType="Literal">&lt;p&gt;Data uncertainties, such as sensor noise or occlusions, can introduce
irreducible ambiguities in images, which result in varying, yet plausible,
semantic hypotheses. In Machine Learning, this ambiguity is commonly referred
to as aleatoric uncertainty. Latent density models can be utilized to address
this problem in image segmentation. The most popular approach is the
Probabilistic U-Net (PU-Net), which uses latent Normal densities to optimize
the conditional data log-likelihood Evidence Lower Bound. In this work, we
demonstrate that the PU- Net latent space is severely inhomogenous. As a
result, the effectiveness of gradient descent is inhibited and the model
becomes extremely sensitive to the localization of the latent space samples,
resulting in defective predictions. To address this, we present the Sinkhorn
PU-Net (SPU-Net), which uses the Sinkhorn Divergence to promote homogeneity
across all latent dimensions, effectively improving gradient-descent updates
and model robustness. Our results show that by applying this on public datasets
of various clinical segmentation problems, the SPU-Net receives up to 11%
performance gains compared against preceding latent variable models for
probabilistic segmentation on the Hungarian-Matched metric. The results
indicate that by encouraging a homogeneous latent space, one can significantly
improve latent density modeling for medical image segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valiuddin_M/0/1/0/all/0/1&quot;&gt;M. M. Amaan Valiuddin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viviers_C/0/1/0/all/0/1&quot;&gt;Christiaan G. A. Viviers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sloun_R/0/1/0/all/0/1&quot;&gt;Ruud J. G. van Sloun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+With_P/0/1/0/all/0/1&quot;&gt;Peter H. N. de With&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sommen_F/0/1/0/all/0/1&quot;&gt;Fons van der Sommen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04156">
<title>Towards Top-Down Stereo Image Quality Assessment via Stereo Attention. (arXiv:2308.04156v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04156</link>
<description rdf:parseType="Literal">&lt;p&gt;Stereo image quality assessment (SIQA) plays a crucial role in evaluating and
improving the visual experience of 3D content. Existing visual properties-based
methods for SIQA have achieved promising performance. However, these approaches
ignore the top-down philosophy, leading to a lack of a comprehensive grasp of
the human visual system (HVS) and SIQA. This paper presents a novel Stereo
AttenTion Network (SATNet), which employs a top-down perspective to guide the
quality assessment process. Specifically, our generalized Stereo AttenTion
(SAT) structure adapts components and input/output for stereo scenarios. It
leverages the fusion-generated attention map as a higher-level binocular
modulator to influence two lower-level monocular features, allowing progressive
recalibration of both throughout the pipeline. Additionally, we introduce an
Energy Coefficient (EC) to flexibly tune the magnitude of binocular response,
accounting for the fact that binocular responses in the primate primary visual
cortex are less than the sum of monocular responses. To extract the most
discriminative quality information from the summation and subtraction of the
two branches of monocular features, we utilize a dual-pooling strategy that
applies min-pooling and max-pooling operations to the respective branches.
Experimental results highlight the superiority of our top-down method in
advancing the state-of-the-art in the SIQA field. The code is available at
https://github.com/Fanning-Zhang/SATNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huilin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sumei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Haoxiang Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1&quot;&gt;Peiming Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07977">
<title>YODA: You Only Diffuse Areas. An Area-Masked Diffusion Approach For Image Super-Resolution. (arXiv:2308.07977v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07977</link>
<description rdf:parseType="Literal">&lt;p&gt;This work introduces &quot;You Only Diffuse Areas&quot; (YODA), a novel method for
partial diffusion in Single-Image Super-Resolution (SISR). The core idea is to
utilize diffusion selectively on spatial regions based on attention maps
derived from the low-resolution image and the current time step in the
diffusion process. This time-dependent targeting enables a more effective
conversion to high-resolution outputs by focusing on areas that benefit the
most from the iterative refinement process, i.e., detail-rich objects. We
empirically validate YODA by extending leading diffusion-based SISR methods SR3
and SRDiff. Our experiments demonstrate new state-of-the-art performance gains
in face and general SR across PSNR, SSIM, and LPIPS metrics. A notable finding
is YODA&apos;s stabilization effect on training by reducing color shifts, especially
when induced by small batch sizes, potentially contributing to
resource-constrained scenarios. The proposed spatial and temporal adaptive
diffusion mechanism opens promising research directions, including developing
enhanced attention map extraction techniques and optimizing inference latency
based on sparser diffusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moser_B/0/1/0/all/0/1&quot;&gt;Brian B. Moser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frolov_S/0/1/0/all/0/1&quot;&gt;Stanislav Frolov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1&quot;&gt;Federico Raue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palacio_S/0/1/0/all/0/1&quot;&gt;Sebastian Palacio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1&quot;&gt;Andreas Dengel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09345">
<title>Denoising diffusion-based MRI to CT image translation enables automated spinal segmentation. (arXiv:2308.09345v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09345</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: Automated segmentation of spinal MR images plays a vital role
both scientifically and clinically. However, accurately delineating posterior
spine structures presents challenges.
&lt;/p&gt;
&lt;p&gt;Methods: This retrospective study, approved by the ethical committee,
involved translating T1w and T2w MR image series into CT images in a total of
n=263 pairs of CT/MR series. Landmark-based registration was performed to align
image pairs. We compared 2D paired (Pix2Pix, denoising diffusion implicit
models (DDIM) image mode, DDIM noise mode) and unpaired (contrastive unpaired
translation, SynDiff) image-to-image translation using &quot;peak signal to noise
ratio&quot; (PSNR) as quality measure. A publicly available segmentation network
segmented the synthesized CT datasets, and Dice scores were evaluated on
in-house test sets and the &quot;MRSpineSeg Challenge&quot; volumes. The 2D findings were
extended to 3D Pix2Pix and DDIM.
&lt;/p&gt;
&lt;p&gt;Results: 2D paired methods and SynDiff exhibited similar translation
performance and Dice scores on paired data. DDIM image mode achieved the
highest image quality. SynDiff, Pix2Pix, and DDIM image mode demonstrated
similar Dice scores (0.77). For craniocaudal axis rotations, at least two
landmarks per vertebra were required for registration. The 3D translation
outperformed the 2D approach, resulting in improved Dice scores (0.80) and
anatomically accurate segmentations in a higher resolution than the original MR
image.
&lt;/p&gt;
&lt;p&gt;Conclusion: Two landmarks per vertebra registration enabled paired
image-to-image translation from MR to CT and outperformed all unpaired
approaches. The 3D techniques provided anatomically correct segmentations,
avoiding underprediction of small structures like the spinous process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Graf_R/0/1/0/all/0/1&quot;&gt;Robert Graf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schmitt_J/0/1/0/all/0/1&quot;&gt;Joachim Schmitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schlaeger_S/0/1/0/all/0/1&quot;&gt;Sarah Schlaeger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moller_H/0/1/0/all/0/1&quot;&gt;Hendrik Kristian M&amp;#xf6;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sideri_Lampretsa_V/0/1/0/all/0/1&quot;&gt;Vasiliki Sideri-Lampretsa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sekuboyina_A/0/1/0/all/0/1&quot;&gt;Anjany Sekuboyina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Krieg_S/0/1/0/all/0/1&quot;&gt;Sandro Manuel Krieg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wiestler_B/0/1/0/all/0/1&quot;&gt;Benedikt Wiestler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1&quot;&gt;Bjoern Menze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kirschke_J/0/1/0/all/0/1&quot;&gt;Jan Stefan Kirschke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10557">
<title>Local Spherical Harmonics Improve Skeleton-Based Hand Action Recognition. (arXiv:2308.10557v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10557</link>
<description rdf:parseType="Literal">&lt;p&gt;Hand action recognition is essential. Communication, human-robot
interactions, and gesture control are dependent on it. Skeleton-based action
recognition traditionally includes hands, which belong to the classes which
remain challenging to correctly recognize to date. We propose a method
specifically designed for hand action recognition which uses relative angular
embeddings and local Spherical Harmonics to create novel hand representations.
The use of Spherical Harmonics creates rotation-invariant representations which
make hand action recognition even more robust against inter-subject differences
and viewpoint changes. We conduct extensive experiments on the hand joints in
the First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose
Annotations, and on the NTU RGB+D 120 dataset, demonstrating the benefit of
using Local Spherical Harmonics Representations. Our code is available at
https://github.com/KathPra/LSHR_LSHT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasse_K/0/1/0/all/0/1&quot;&gt;Katharina Prasse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1&quot;&gt;Steffen Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1&quot;&gt;Margret Keuper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00305">
<title>Efficient Surrogate Models for Materials Science Simulations: Machine Learning-based Prediction of Microstructure Properties. (arXiv:2309.00305v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00305</link>
<description rdf:parseType="Literal">&lt;p&gt;Determining, understanding, and predicting the so-called structure-property
relation is an important task in many scientific disciplines, such as
chemistry, biology, meteorology, physics, engineering, and materials science.
Structure refers to the spatial distribution of, e.g., substances, material, or
matter in general, while property is a resulting characteristic that usually
depends in a non-trivial way on spatial details of the structure.
Traditionally, forward simulations models have been used for such tasks.
Recently, several machine learning algorithms have been applied in these
scientific fields to enhance and accelerate simulation models or as surrogate
models. In this work, we develop and investigate the applications of six
machine learning techniques based on two different datasets from the domain of
materials science: data from a two-dimensional Ising model for predicting the
formation of magnetic domains and data representing the evolution of dual-phase
microstructures from the Cahn-Hilliard model. We analyze the accuracy and
robustness of all models and elucidate the reasons for the differences in their
performances. The impact of including domain knowledge through tailored
features is studied, and general recommendations based on the availability and
quality of training data are derived from this.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1&quot;&gt;Binh Duong Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potapenko_P/0/1/0/all/0/1&quot;&gt;Pavlo Potapenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dermici_A/0/1/0/all/0/1&quot;&gt;Aytekin Dermici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Govind_K/0/1/0/all/0/1&quot;&gt;Kishan Govind&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bompas_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Bompas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sandfeld_S/0/1/0/all/0/1&quot;&gt;Stefan Sandfeld&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13475">
<title>Detecting and Mitigating System-Level Anomalies of Vision-Based Controllers. (arXiv:2309.13475v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13475</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous systems, such as self-driving cars and drones, have made
significant strides in recent years by leveraging visual inputs and machine
learning for decision-making and control. Despite their impressive performance,
these vision-based controllers can make erroneous predictions when faced with
novel or out-of-distribution inputs. Such errors can cascade to catastrophic
system failures and compromise system safety. In this work, we introduce a
run-time anomaly monitor to detect and mitigate such closed-loop, system-level
failures. Specifically, we leverage a reachability-based framework to
stress-test the vision-based controller offline and mine its system-level
failures. This data is then used to train a classifier that is leveraged online
to flag inputs that might cause system breakdowns. The anomaly detector
highlights issues that transcend individual modules and pertain to the safety
of the overall system. We also design a fallback controller that robustly
handles these detected anomalies to preserve system safety. We validate the
proposed approach on an autonomous aircraft taxiing system that uses a
vision-based controller for taxiing. Our results show the efficacy of the
proposed approach in identifying and handling system-level anomalies,
outperforming methods such as prediction error-based detection, and ensembling,
thereby enhancing the overall safety and robustness of autonomous systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Aryaman Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_K/0/1/0/all/0/1&quot;&gt;Kaustav Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1&quot;&gt;Somil Bansal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17105">
<title>Continual Action Assessment via Task-Consistent Score-Discriminative Feature Distribution Modeling. (arXiv:2309.17105v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17105</link>
<description rdf:parseType="Literal">&lt;p&gt;Action Quality Assessment (AQA) is a task that tries to answer how well an
action is carried out. While remarkable progress has been achieved, existing
works on AQA assume that all the training data are visible for training in one
time, but do not enable continual learning on assessing new technical actions.
In this work, we address such a Continual Learning problem in AQA
(Continual-AQA), which urges a unified model to learn AQA tasks sequentially
without forgetting. Our idea for modeling Continual-AQA is to sequentially
learn a task-consistent score-discriminative feature distribution, in which the
latent features express a strong correlation with the score labels regardless
of the task or action types. From this perspective, we aim to mitigate the
forgetting in Continual-AQA from two aspects. Firstly, to fuse the features of
new and previous data into a score-discriminative distribution, a novel
Feature-Score Correlation-Aware Rehearsal is proposed to store and reuse data
from previous tasks with limited memory size. Secondly, an Action
General-Specific Graph is developed to learn and decouple the action-general
and action-specific knowledge so that the task-consistent score-discriminative
features can be better extracted across various tasks. Extensive experiments
are conducted to evaluate the contributions of proposed components. The
comparisons with the existing continual learning methods additionally verify
the effectiveness and versatility of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuan-Ming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1&quot;&gt;Ling-An Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_J/0/1/0/all/0/1&quot;&gt;Jing-Ke Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wei-Shi Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01523">
<title>Fetal-BET: Brain Extraction Tool for Fetal MRI. (arXiv:2310.01523v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01523</link>
<description rdf:parseType="Literal">&lt;p&gt;Fetal brain extraction is a necessary first step in most computational fetal
brain MRI pipelines. However, it has been a very challenging task due to
non-standard fetal head pose, fetal movements during examination, and vastly
heterogeneous appearance of the developing fetal brain and the neighboring
fetal and maternal anatomy across various sequences and scanning conditions.
Development of a machine learning method to effectively address this task
requires a large and rich labeled dataset that has not been previously
available. As a result, there is currently no method for accurate fetal brain
extraction on various fetal MRI sequences. In this work, we first built a large
annotated dataset of approximately 72,000 2D fetal brain MRI images. Our
dataset covers the three common MRI sequences including T2-weighted,
diffusion-weighted, and functional MRI acquired with different scanners.
Moreover, it includes normal and pathological brains. Using this dataset, we
developed and validated deep learning methods, by exploiting the power of the
U-Net style architectures, the attention mechanism, multi-contrast feature
learning, and data augmentation for fast, accurate, and generalizable automatic
fetal brain extraction. Our approach leverages the rich information from
multi-contrast (multi-sequence) fetal MRI data, enabling precise delineation of
the fetal brain structures. Evaluations on independent test data show that our
method achieves accurate brain extraction on heterogeneous test data acquired
with different scanners, on pathological brains, and at various gestational
stages. This robustness underscores the potential utility of our deep learning
model for fetal brain imaging and image analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Faghihpirayesh_R/0/1/0/all/0/1&quot;&gt;Razieh Faghihpirayesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Karimi_D/0/1/0/all/0/1&quot;&gt;Davood Karimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Erdogmus_D/0/1/0/all/0/1&quot;&gt;Deniz Erdo&amp;#x11f;mu&amp;#x15f;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gholipour_A/0/1/0/all/0/1&quot;&gt;Ali Gholipour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02251">
<title>Talk2BEV: Language-enhanced Bird&apos;s-eye View Maps for Autonomous Driving. (arXiv:2310.02251v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02251</link>
<description rdf:parseType="Literal">&lt;p&gt;Talk2BEV is a large vision-language model (LVLM) interface for bird&apos;s-eye
view (BEV) maps in autonomous driving contexts. While existing perception
systems for autonomous driving scenarios have largely focused on a pre-defined
(closed) set of object categories and driving scenarios, Talk2BEV blends recent
advances in general-purpose language and vision models with BEV-structured map
representations, eliminating the need for task-specific models. This enables a
single system to cater to a variety of autonomous driving tasks encompassing
visual and spatial reasoning, predicting the intents of traffic actors, and
decision-making based on visual cues. We extensively evaluate Talk2BEV on a
large number of scene understanding tasks that rely on both the ability to
interpret free-form natural language queries, and in grounding these queries to
the visual context embedded into the language-enhanced BEV map. To enable
further research in LVLMs for autonomous driving scenarios, we develop and
release Talk2BEV-Bench, a benchmark encompassing 1000 human-annotated BEV
scenarios, with more than 20,000 questions and ground-truth responses from the
NuScenes dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choudhary_T/0/1/0/all/0/1&quot;&gt;Tushar Choudhary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dewangan_V/0/1/0/all/0/1&quot;&gt;Vikrant Dewangan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandhok_S/0/1/0/all/0/1&quot;&gt;Shivam Chandhok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Priyadarshan_S/0/1/0/all/0/1&quot;&gt;Shubham Priyadarshan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Anushka Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Arun K. Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1&quot;&gt;Siddharth Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jatavallabhula_K/0/1/0/all/0/1&quot;&gt;Krishna Murthy Jatavallabhula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1&quot;&gt;K. Madhava Krishna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11755">
<title>RGM: A Robust Generalist Matching Model. (arXiv:2310.11755v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11755</link>
<description rdf:parseType="Literal">&lt;p&gt;Finding corresponding pixels within a pair of images is a fundamental
computer vision task with various applications. Due to the specific
requirements of different tasks like optical flow estimation and local feature
matching, previous works are primarily categorized into dense matching and
sparse feature matching focusing on specialized architectures along with
task-specific datasets, which may somewhat hinder the generalization
performance of specialized models. In this paper, we propose a deep model for
sparse and dense matching, termed RGM (Robust Generalist Matching). In
particular, we elaborately design a cascaded GRU module for refinement by
exploring the geometric similarity iteratively at multiple scales following an
additional uncertainty estimation module for sparsification. To narrow the gap
between synthetic training samples and real-world scenarios, we build a new,
large-scale dataset with sparse correspondence ground truth by generating
optical flow supervision with greater intervals. As such, we are able to mix up
various dense and sparse matching datasets, significantly improving the
training diversity. The generalization capacity of our proposed RGM is greatly
improved by learning the matching and uncertainty estimation in a two-stage
manner on the large, mixed data. Superior performance is achieved for zero-shot
matching and downstream geometry estimation across multiple datasets,
outperforming the previous methods by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Songyan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xinyu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chunhua Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12334">
<title>Improving Representation Learning for Histopathologic Images with Cluster Constraints. (arXiv:2310.12334v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12334</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in whole-slide image (WSI) scanners and computational
capabilities have significantly propelled the application of artificial
intelligence in histopathology slide analysis. While these strides are
promising, current supervised learning approaches for WSI analysis come with
the challenge of exhaustively labeling high-resolution slides - a process that
is both labor-intensive and time-consuming. In contrast, self-supervised
learning (SSL) pretraining strategies are emerging as a viable alternative,
given that they don&apos;t rely on explicit data annotations. These SSL strategies
are quickly bridging the performance disparity with their supervised
counterparts. In this context, we introduce an SSL framework. This framework
aims for transferable representation learning and semantically meaningful
clustering by synergizing invariance loss and clustering loss in WSI analysis.
Notably, our approach outperforms common SSL methods in downstream
classification and clustering tasks, as evidenced by tests on the Camelyon16
and a pancreatic cancer dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Weiyi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chongyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DiPalma_J/0/1/0/all/0/1&quot;&gt;Joseph DiPalma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1&quot;&gt;Soroush Vosoughi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassanpour_S/0/1/0/all/0/1&quot;&gt;Saeed Hassanpour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12570">
<title>DA-TransUNet: Integrating Spatial and Channel Dual Attention with Transformer U-Net for Medical Image Segmentation. (arXiv:2310.12570v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12570</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate medical image segmentation is critical for disease quantification
and treatment evaluation. While traditional Unet architectures and their
transformer-integrated variants excel in automated segmentation tasks. However,
they lack the ability to harness the intrinsic position and channel features of
image. Existing models also struggle with parameter efficiency and
computational complexity, often due to the extensive use of Transformers. To
address these issues, this study proposes a novel deep medical image
segmentation framework, called DA-TransUNet, aiming to integrate the
Transformer and dual attention block(DA-Block) into the traditional U-shaped
architecture. Unlike earlier transformer-based U-net models, DA-TransUNet
utilizes Transformers and DA-Block to integrate not only global and local
features, but also image-specific positional and channel features, improving
the performance of medical image segmentation. By incorporating a DA-Block at
the embedding layer and within each skip connection layer, we substantially
enhance feature extraction capabilities and improve the efficiency of the
encoder-decoder structure. DA-TransUNet demonstrates superior performance in
medical image segmentation tasks, consistently outperforming state-of-the-art
techniques across multiple datasets. In summary, DA-TransUNet offers a
significant advancement in medical image segmentation, providing an effective
and powerful alternative to existing techniques. Our architecture stands out
for its ability to improve segmentation accuracy, thereby advancing the field
of automated medical image diagnostics. The codes and parameters of our model
will be publicly available at https://github.com/SUN-1024/DA-TransUnet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_G/0/1/0/all/0/1&quot;&gt;Guanqun Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yizhi Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kong_W/0/1/0/all/0/1&quot;&gt;Weikun Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zichang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jianhua Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Racharak_T/0/1/0/all/0/1&quot;&gt;Teeradaj Racharak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_L/0/1/0/all/0/1&quot;&gt;Le-Minh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xin_J/0/1/0/all/0/1&quot;&gt;Junyi Xin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19522">
<title>Are Natural Domain Foundation Models Useful for Medical Image Classification?. (arXiv:2310.19522v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19522</link>
<description rdf:parseType="Literal">&lt;p&gt;The deep learning field is converging towards the use of general foundation
models that can be easily adapted for diverse tasks. While this paradigm shift
has become common practice within the field of natural language processing,
progress has been slower in computer vision. In this paper we attempt to
address this issue by investigating the transferability of various
state-of-the-art foundation models to medical image classification tasks.
Specifically, we evaluate the performance of five foundation models, namely
SAM, SEEM, DINOv2, BLIP, and OpenCLIP across four well-established medical
imaging datasets. We explore different training settings to fully harness the
potential of these models. Our study shows mixed results. DINOv2 consistently
outperforms the standard practice of ImageNet pretraining. However, other
foundation models failed to consistently beat this established baseline
indicating limitations in their transferability to medical image classification
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huix_J/0/1/0/all/0/1&quot;&gt;Joana Pal&amp;#xe9;s Huix&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganeshan_A/0/1/0/all/0/1&quot;&gt;Adithya Raju Ganeshan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haslum_J/0/1/0/all/0/1&quot;&gt;Johan Fredin Haslum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soderberg_M/0/1/0/all/0/1&quot;&gt;Magnus S&amp;#xf6;derberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsoukas_C/0/1/0/all/0/1&quot;&gt;Christos Matsoukas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1&quot;&gt;Kevin Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19721">
<title>Promise:Prompt-driven 3D Medical Image Segmentation Using Pretrained Image Foundation Models. (arXiv:2310.19721v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19721</link>
<description rdf:parseType="Literal">&lt;p&gt;To address prevalent issues in medical imaging, such as data acquisition
challenges and label availability, transfer learning from natural to medical
image domains serves as a viable strategy to produce reliable segmentation
results. However, several existing barriers between domains need to be broken
down, including addressing contrast discrepancies, managing anatomical
variability, and adapting 2D pretrained models for 3D segmentation tasks. In
this paper, we propose ProMISe,a prompt-driven 3D medical image segmentation
model using only a single point prompt to leverage knowledge from a pretrained
2D image foundation model. In particular, we use the pretrained vision
transformer from the Segment Anything Model (SAM) and integrate lightweight
adapters to extract depth-related (3D) spatial context without updating the
pretrained weights. For robust results, a hybrid network with complementary
encoders is designed, and a boundary-aware loss is proposed to achieve precise
boundaries. We evaluate our model on two public datasets for colon and pancreas
tumor segmentations, respectively. Compared to the state-of-the-art
segmentation methods with and without prompt engineering, our proposed method
achieves superior performance. The code is publicly available at
https://github.com/MedICL-VU/ProMISe.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dewei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiacheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oguz_I/0/1/0/all/0/1&quot;&gt;Ipek Oguz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02877">
<title>Inner-IoU: More Effective Intersection over Union Loss with Auxiliary Bounding Box. (arXiv:2311.02877v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02877</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of detectors, Bounding Box Regression (BBR) loss
function has constantly updated and optimized. However, the existing IoU-based
BBR still focus on accelerating convergence by adding new loss terms, ignoring
the limitations of IoU loss term itself. Although theoretically IoU loss can
effectively describe the state of bounding box regression,in practical
applications, it cannot adjust itself according to different detectors and
detection tasks, and does not have strong generalization. Based on the above,
we first analyzed the BBR model and concluded that distinguishing different
regression samples and using different scales of auxiliary bounding boxes to
calculate losses can effectively accelerate the bounding box regression
process. For high IoU samples, using smaller auxiliary bounding boxes to
calculate losses can accelerate convergence, while larger auxiliary bounding
boxes are suitable for low IoU samples. Then, we propose Inner-IoU loss, which
calculates IoU loss through auxiliary bounding boxes. For different datasets
and detectors, we introduce a scaling factor ratio to control the scale size of
the auxiliary bounding boxes for calculating losses. Finally, integrate
Inner-IoU into the existing IoU-based loss functions for simulation and
comparative experiments. The experiment result demonstrate a further
enhancement in detection performance with the utilization of the method
proposed in this paper, verifying the effectiveness and generalization ability
of Inner-IoU loss. Code is available at
https://github.com/malagoutou/Inner-IoU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Cong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuaijie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03402">
<title>CycleCL: Self-supervised Learning for Periodic Videos. (arXiv:2311.03402v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03402</link>
<description rdf:parseType="Literal">&lt;p&gt;Analyzing periodic video sequences is a key topic in applications such as
automatic production systems, remote sensing, medical applications, or physical
training. An example is counting repetitions of a physical exercise. Due to the
distinct characteristics of periodic data, self-supervised methods designed for
standard image datasets do not capture changes relevant to the progression of
the cycle and fail to ignore unrelated noise. They thus do not work well on
periodic data. In this paper, we propose CycleCL, a self-supervised learning
method specifically designed to work with periodic data. We start from the
insight that a good visual representation for periodic data should be sensitive
to the phase of a cycle, but be invariant to the exact repetition, i.e. it
should generate identical representations for a specific phase throughout all
repetitions. We exploit the repetitions in videos to design a novel contrastive
learning method based on a triplet loss that optimizes for these desired
properties. Our method uses pre-trained features to sample pairs of frames from
approximately the same phase and negative pairs of frames from different
phases. Then, we iterate between optimizing a feature encoder and resampling
triplets, until convergence. By optimizing a model this way, we are able to
learn features that have the mentioned desired properties. We evaluate CycleCL
on an industrial and multiple human actions datasets, where it significantly
outperforms previous video-based self-supervised learning methods on all tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Destro_M/0/1/0/all/0/1&quot;&gt;Matteo Destro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gygli_M/0/1/0/all/0/1&quot;&gt;Michael Gygli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05410">
<title>Linear Gaussian Bounding Box Representation and Ring-Shaped Rotated Convolution for Oriented Object Detection. (arXiv:2311.05410v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05410</link>
<description rdf:parseType="Literal">&lt;p&gt;In oriented object detection, current representations of oriented bounding
boxes (OBBs) often suffer from boundary discontinuity problem. Methods of
designing continuous regression losses do not essentially solve this problem.
Although Gaussian bounding box (GBB) representation avoids this problem,
directly regressing GBB is susceptible to numerical instability. We propose
linear GBB (LGBB), a novel OBB representation. By linearly transforming the
elements of GBB, LGBB avoids the boundary discontinuity problem and has high
numerical stability. In addition, existing convolution-based rotation-sensitive
feature extraction methods only have local receptive fields, resulting in slow
feature aggregation. We propose ring-shaped rotated convolution (RRC), which
adaptively rotates feature maps to arbitrary orientations to extract
rotation-sensitive features under a ring-shaped receptive field, rapidly
aggregating features and contextual information. Experimental results
demonstrate that LGBB and RRC achieve state-of-the-art performance.
Furthermore, integrating LGBB and RRC into various models effectively improves
detection accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunkai Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Junfeng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_F/0/1/0/all/0/1&quot;&gt;Fengshui Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1&quot;&gt;Min Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06176">
<title>Automatic Report Generation for Histopathology images using pre-trained Vision Transformers. (arXiv:2311.06176v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06176</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning for histopathology has been successfully used for disease
classification, image segmentation and more. However, combining image and text
modalities using current state-of-the-art methods has been a challenge due to
the high resolution of histopathology images. Automatic report generation for
histopathology images is one such challenge. In this work, we show that using
an existing pre-trained Vision Transformer in a two-step process of first using
it to encode 4096x4096 sized patches of the Whole Slide Image (WSI) and then
using it as the encoder and an LSTM decoder for report generation, we can build
a fairly performant and portable report generation mechanism that takes into
account the whole of the high resolution image, instead of just the patches. We
are also able to use representations from an existing powerful pre-trained
hierarchical vision transformer and show its usefulness in not just zero shot
classification but also for report generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1&quot;&gt;Saurav Sengupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1&quot;&gt;Donald E. Brown&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07247">
<title>Simultaneous Clutter Detection and Semantic Segmentation of Moving Objects for Automotive Radar Data. (arXiv:2311.07247v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07247</link>
<description rdf:parseType="Literal">&lt;p&gt;The unique properties of radar sensors, such as their robustness to adverse
weather conditions, make them an important part of the environment perception
system of autonomous vehicles. One of the first steps during the processing of
radar point clouds is often the detection of clutter, i.e. erroneous points
that do not correspond to real objects. Another common objective is the
semantic segmentation of moving road users. These two problems are handled
strictly separate from each other in literature. The employed neural networks
are always focused entirely on only one of the tasks. In contrast to this, we
examine ways to solve both tasks at the same time with a single jointly used
model. In addition to a new augmented multi-head architecture, we also devise a
method to represent a network&apos;s predictions for the two tasks with only one
output value. This novel approach allows us to solve the tasks simultaneously
with the same inference time as a conventional task-specific model. In an
extensive evaluation, we show that our setup is highly effective and
outperforms every existing network for semantic segmentation on the RadarScenes
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kopp_J/0/1/0/all/0/1&quot;&gt;Johannes Kopp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kellner_D/0/1/0/all/0/1&quot;&gt;Dominik Kellner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piroli_A/0/1/0/all/0/1&quot;&gt;Aldi Piroli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dallabetta_V/0/1/0/all/0/1&quot;&gt;Vinzenz Dallabetta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1&quot;&gt;Klaus Dietmayer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07362">
<title>Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision. (arXiv:2311.07362v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07362</link>
<description rdf:parseType="Literal">&lt;p&gt;Large multimodal models (LMMs) suffer from multimodal hallucination, where
they provide incorrect responses misaligned with the given visual information.
Recent works have conjectured that one of the reasons behind multimodal
hallucination might be due to the vision encoder failing to ground on the image
properly. To mitigate this issue, we propose a novel approach that leverages
self-feedback as visual cues. Building on this approach, we introduce Volcano,
a multimodal self-feedback guided revision model. Volcano generates natural
language feedback to its initial response based on the provided visual
information and utilizes this feedback to self-revise its initial response.
Volcano effectively reduces multimodal hallucination and achieves
state-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general
multimodal abilities and outperforms previous models on MM-Vet and MMBench.
Through a qualitative analysis, we show that Volcano&apos;s feedback is properly
grounded on the image than the initial response. This indicates that Volcano
can provide itself with richer visual information, helping alleviate multimodal
hallucination. We publicly release Volcano models of 7B and 13B sizes along
with the data and code at https://github.com/kaistAI/Volcano.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seongyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sue Hyun Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1&quot;&gt;Yongrae Jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1&quot;&gt;Minjoon Seo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01994">
<title>Waving Goodbye to Low-Res: A Diffusion-Wavelet Approach for Image Super-Resolution. (arXiv:2304.01994v2 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2304.01994</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel Diffusion-Wavelet (DiWa) approach for
Single-Image Super-Resolution (SISR). It leverages the strengths of Denoising
Diffusion Probabilistic Models (DDPMs) and Discrete Wavelet Transformation
(DWT). By enabling DDPMs to operate in the DWT domain, our DDPM models
effectively hallucinate high-frequency information for super-resolved images on
the wavelet spectrum, resulting in high-quality and detailed reconstructions in
image space. Quantitatively, we outperform state-of-the-art diffusion-based
SISR methods, namely SR3 and SRDiff, regarding PSNR, SSIM, and LPIPS on both
face (8x scaling) and general (4x scaling) SR benchmarks. Meanwhile, using DWT
enabled us to use fewer parameters than the compared models: 92M parameters
instead of 550M compared to SR3 and 9.3M instead of 12M compared to SRDiff.
Additionally, our method outperforms other state-of-the-art generative methods
on classical general SR datasets while saving inference time. Finally, our work
highlights its potential for various applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moser_B/0/1/0/all/0/1&quot;&gt;Brian Moser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frolov_S/0/1/0/all/0/1&quot;&gt;Stanislav Frolov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1&quot;&gt;Federico Raue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palacio_S/0/1/0/all/0/1&quot;&gt;Sebastian Palacio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1&quot;&gt;Andreas Dengel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06794">
<title>CL-Flow:Strengthening the Normalizing Flows by Contrastive Learning for Better Anomaly Detection. (arXiv:2311.06794v1 [cs.IR] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2311.06794</link>
<description rdf:parseType="Literal">&lt;p&gt;In the anomaly detection field, the scarcity of anomalous samples has
directed the current research emphasis towards unsupervised anomaly detection.
While these unsupervised anomaly detection methods offer convenience, they also
overlook the crucial prior information embedded within anomalous samples.
Moreover, among numerous deep learning methods, supervised methods generally
exhibit superior performance compared to unsupervised methods. Considering the
reasons mentioned above, we propose a self-supervised anomaly detection
approach that combines contrastive learning with 2D-Flow to achieve more
precise detection outcomes and expedited inference processes. On one hand, we
introduce a novel approach to anomaly synthesis, yielding anomalous samples in
accordance with authentic industrial scenarios, alongside their surrogate
annotations. On the other hand, having obtained a substantial number of
anomalous samples, we enhance the 2D-Flow framework by incorporating
contrastive learning, leveraging diverse proxy tasks to fine-tune the network.
Our approach enables the network to learn more precise mapping relationships
from self-generated labels while retaining the lightweight characteristics of
the 2D-Flow. Compared to mainstream unsupervised approaches, our
self-supervised method demonstrates superior detection accuracy, fewer
additional model parameters, and faster inference speed. Furthermore, the
entire training and inference process is end-to-end. Our approach showcases new
state-of-the-art results, achieving a performance of 99.6\% in image-level
AUROC on the MVTecAD dataset and 96.8\% in image-level AUROC on the BTAD
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shunfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yueyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1&quot;&gt;Haichi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_C/0/1/0/all/0/1&quot;&gt;Chenyang Bi&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>