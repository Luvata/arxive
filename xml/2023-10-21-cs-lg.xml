<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-10-19T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12156" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12169" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12184" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12238" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12248" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12294" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12298" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12303" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12309" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12324" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12346" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12353" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12359" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12395" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12407" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12428" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12432" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12447" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12457" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12487" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12498" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12500" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12505" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12547" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12560" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12563" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12565" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12570" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12585" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12612" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12632" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12660" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12663" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12667" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12671" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12690" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12752" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12765" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12768" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12771" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12774" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12785" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12803" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12804" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12808" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12809" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12817" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12822" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12900" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1811.11479" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.08117" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.13001" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.08717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.14276" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.07439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.07162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.07940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.06949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.06348" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.13623" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.00617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.07469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.12321" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01328" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.03098" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13047" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.14090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07063" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10398" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10557" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10744" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15538" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16291" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16546" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03659" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06599" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07960" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09983" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04228" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06368" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00608" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05270" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.10194" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12871" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02031" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02554" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03178" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05161" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07560" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08237" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10060" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10537" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10638" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10692" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11569" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12069" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.10170" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10541" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2310.12156">
<title>Operator-Based Detecting, Learning, and Stabilizing Unstable Periodic Orbits of Chaotic Attractors. (arXiv:2310.12156v1 [nlin.AO])</title>
<link>http://arxiv.org/abs/2310.12156</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper examines the use of operator-theoretic approaches to the analysis
of chaotic systems through the lens of their unstable periodic orbits (UPOs).
Our approach involves three data-driven steps for detecting, identifying, and
stabilizing UPOs. We demonstrate the use of kernel integral operators within
delay coordinates as an innovative method for UPO detection. For identifying
the dynamic behavior associated with each individual UPO, we utilize the
Koopman operator to present the dynamics as linear equations in the space of
Koopman eigenfunctions. This allows for characterizing the chaotic attractor by
investigating its principal dynamical modes across varying UPOs. We extend this
methodology into an interpretable machine learning framework aimed at
stabilizing strange attractors on their UPOs. To illustrate the efficacy of our
approach, we apply it to the Lorenz attractor as a case study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/nlin/1/au:+Tavasoli_A/0/1/0/all/0/1&quot;&gt;Ali Tavasoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/nlin/1/au:+Shakeri_H/0/1/0/all/0/1&quot;&gt;Heman Shakeri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12162">
<title>AI Potentiality and Awareness: A Position Paper from the Perspective of Human-AI Teaming in Cybersecurity. (arXiv:2310.12162v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2310.12162</link>
<description rdf:parseType="Literal">&lt;p&gt;This position paper explores the broad landscape of AI potentiality in the
context of cybersecurity, with a particular emphasis on its possible risk
factors with awareness, which can be managed by incorporating human experts in
the loop, i.e., &quot;Human-AI&quot; teaming. As artificial intelligence (AI)
technologies advance, they will provide unparalleled opportunities for attack
identification, incident response, and recovery. However, the successful
deployment of AI into cybersecurity measures necessitates an in-depth
understanding of its capabilities, challenges, and ethical and legal
implications to handle associated risk factors in real-world application areas.
Towards this, we emphasize the importance of a balanced approach that
incorporates AI&apos;s computational power with human expertise. AI systems may
proactively discover vulnerabilities and detect anomalies through pattern
recognition, and predictive modeling, significantly enhancing speed and
accuracy. Human experts can explain AI-generated decisions to stakeholders,
regulators, and end-users in critical situations, ensuring responsibility and
accountability, which helps establish trust in AI-driven security solutions.
Therefore, in this position paper, we argue that human-AI teaming is worthwhile
in cybersecurity, in which human expertise such as intuition, critical
thinking, or contextual understanding is combined with AI&apos;s computational power
to improve overall cyber defenses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarker_I/0/1/0/all/0/1&quot;&gt;Iqbal H. Sarker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janicke_H/0/1/0/all/0/1&quot;&gt;Helge Janicke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammad_N/0/1/0/all/0/1&quot;&gt;Nazeeruddin Mohammad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watters_P/0/1/0/all/0/1&quot;&gt;Paul Watters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nepal_S/0/1/0/all/0/1&quot;&gt;Surya Nepal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12168">
<title>RK-core: An Established Methodology for Exploring the Hierarchical Structure within Datasets. (arXiv:2310.12168v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12168</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the field of machine learning has undergone a transition from
model-centric to data-centric. The advancements in diverse learning tasks have
been propelled by the accumulation of more extensive datasets, subsequently
facilitating the training of larger models on these datasets. However, these
datasets remain relatively under-explored. To this end, we introduce a
pioneering approach known as RK-core, to empower gaining a deeper understanding
of the intricate hierarchical structure within datasets. Across several
benchmark datasets, we find that samples with low coreness values appear less
representative of their respective categories, and conversely, those with high
coreness values exhibit greater representativeness. Correspondingly, samples
with high coreness values make a more substantial contribution to the
performance in comparison to those with low coreness values. Building upon
this, we further employ RK-core to analyze the hierarchical structure of
samples with different coreset selection methods. Remarkably, we find that a
high-quality coreset should exhibit hierarchical diversity instead of solely
opting for representative samples. The code is available at
https://github.com/yaolu-zjut/Kcore.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yutian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1&quot;&gt;Jiaqi Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zuohui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1&quot;&gt;Qi Xuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12169">
<title>Enhanced Graph Neural Networks with Ego-Centric Spectral Subgraph Embeddings Augmentation. (arXiv:2310.12169v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2310.12169</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have shown remarkable merit in performing
various learning-based tasks in complex networks. The superior performance of
GNNs often correlates with the availability and quality of node-level features
in the input networks. However, for many network applications, such node-level
information may be missing or unreliable, thereby limiting the applicability
and efficacy of GNNs. To address this limitation, we present a novel approach
denoted as Ego-centric Spectral subGraph Embedding Augmentation (ESGEA), which
aims to enhance and design node features, particularly in scenarios where
information is lacking. Our method leverages the topological structure of the
local subgraph to create topology-aware node features. The subgraph features
are generated using an efficient spectral graph embedding technique, and they
serve as node features that capture the local topological organization of the
network. The explicit node features, if present, are then enhanced with the
subgraph embeddings in order to improve the overall performance. ESGEA is
compatible with any GNN-based architecture and is effective even in the absence
of node features. We evaluate the proposed method in a social network graph
classification task where node attributes are unavailable, as well as in a node
classification task where node features are corrupted or even absent. The
evaluation results on seven datasets and eight baseline models indicate up to a
10% improvement in AUC and a 7% improvement in accuracy for graph and node
classification tasks, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Said_A/0/1/0/all/0/1&quot;&gt;Anwar Said&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shabbir_M/0/1/0/all/0/1&quot;&gt;Mudassir Shabbir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derr_T/0/1/0/all/0/1&quot;&gt;Tyler Derr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbas_W/0/1/0/all/0/1&quot;&gt;Waseem Abbas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koutsoukos_X/0/1/0/all/0/1&quot;&gt;Xenofon Koutsoukos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12184">
<title>Architectural Implications of GNN Aggregation Programming Abstractions. (arXiv:2310.12184v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12184</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) have gained significant popularity due to the
powerful capability to extract useful representations from graph data. As the
need for efficient GNN computation intensifies, a variety of programming
abstractions designed for optimizing GNN Aggregation have emerged to facilitate
acceleration. However, there is no comprehensive evaluation and analysis upon
existing abstractions, thus no clear consensus on which approach is better. In
this letter, we classify existing programming abstractions for GNN Aggregation
by the dimension of data organization and propagation method. By constructing
these abstractions on a state-of-the-art GNN library, we perform a thorough and
detailed characterization study to compare their performance and efficiency,
and provide several insights on future GNN acceleration based on our analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yingjie Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianlei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Ao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_T/0/1/0/all/0/1&quot;&gt;Tong Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1&quot;&gt;Chunming Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12209">
<title>Fast Parameter Inference on Pulsar Timing Arrays with Normalizing Flows. (arXiv:2310.12209v1 [astro-ph.IM])</title>
<link>http://arxiv.org/abs/2310.12209</link>
<description rdf:parseType="Literal">&lt;p&gt;Pulsar timing arrays (PTAs) perform Bayesian posterior inference with
expensive MCMC methods. Given a dataset of ~10-100 pulsars and O(10^3) timing
residuals each, producing a posterior distribution for the stochastic
gravitational wave background (SGWB) can take days to a week. The computational
bottleneck arises because the likelihood evaluation required for MCMC is
extremely costly when considering the dimensionality of the search space.
Fortunately, generating simulated data is fast, so modern simulation-based
inference techniques can be brought to bear on the problem. In this paper, we
demonstrate how conditional normalizing flows trained on simulated data can be
used for extremely fast and accurate estimation of the SGWB posteriors,
reducing the sampling time from weeks to a matter of seconds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Shih_D/0/1/0/all/0/1&quot;&gt;David Shih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Freytsis_M/0/1/0/all/0/1&quot;&gt;Marat Freytsis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Taylor_S/0/1/0/all/0/1&quot;&gt;Stephen R. Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Dror_J/0/1/0/all/0/1&quot;&gt;Jeff A. Dror&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Smyth_N/0/1/0/all/0/1&quot;&gt;Nolan Smyth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12238">
<title>Few-Shot In-Context Imitation Learning via Implicit Graph Alignment. (arXiv:2310.12238v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2310.12238</link>
<description rdf:parseType="Literal">&lt;p&gt;Consider the following problem: given a few demonstrations of a task across a
few different objects, how can a robot learn to perform that same task on new,
previously unseen objects? This is challenging because the large variety of
objects within a class makes it difficult to infer the task-relevant
relationship between the new objects and the objects in the demonstrations. We
address this by formulating imitation learning as a conditional alignment
problem between graph representations of objects. Consequently, we show that
this conditioning allows for in-context learning, where a robot can perform a
task on a set of new objects immediately after the demonstrations, without any
prior knowledge about the object class or any further training. In our
experiments, we explore and validate our design choices, and we show that our
method is highly effective for few-shot learning of several real-world,
everyday tasks, whilst outperforming baselines. Videos are available on our
project webpage at https://www.robot-learning.uk/implicit-graph-alignment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vosylius_V/0/1/0/all/0/1&quot;&gt;Vitalis Vosylius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1&quot;&gt;Edward Johns&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12243">
<title>REVAMP: Automated Simulations of Adversarial Attacks on Arbitrary Objects in Realistic Scenes. (arXiv:2310.12243v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12243</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning models, such as those used in an autonomous vehicle are
vulnerable to adversarial attacks where an attacker could place an adversarial
object in the environment, leading to mis-classification. Generating these
adversarial objects in the digital space has been extensively studied, however
successfully transferring these attacks from the digital realm to the physical
realm has proven challenging when controlling for real-world environmental
factors. In response to these limitations, we introduce REVAMP, an easy-to-use
Python library that is the first-of-its-kind tool for creating attack scenarios
with arbitrary objects and simulating realistic environmental factors,
lighting, reflection, and refraction. REVAMP enables researchers and
practitioners to swiftly explore various scenarios within the digital realm by
offering a wide range of configurable options for designing experiments and
using differentiable rendering to reproduce physically plausible adversarial
objects. We will demonstrate and invite the audience to try REVAMP to produce
an adversarial texture on a chosen object while having control over various
scene parameters. The audience will choose a scene, an object to attack, the
desired attack class, and the number of camera positions to use. Then, in real
time, we show how this altered texture causes the chosen object to be
mis-classified, showcasing the potential of REVAMP in real-world scenarios.
REVAMP is open-source and available at https://github.com/poloclub/revamp.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hull_M/0/1/0/all/0/1&quot;&gt;Matthew Hull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zijie J. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1&quot;&gt;Duen Horng Chau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12244">
<title>A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm. (arXiv:2310.12244v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12244</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain incremental learning aims to adapt to a sequence of domains with
access to only a small subset of data (i.e., memory) from previous domains.
Various methods have been proposed for this problem, but it is still unclear
how they are related and when practitioners should choose one method over
another. In response, we propose a unified framework, dubbed Unified Domain
Incremental Learning (UDIL), for domain incremental learning with memory. Our
UDIL **unifies** various existing methods, and our theoretical analysis shows
that UDIL always achieves a tighter generalization error bound compared to
these methods. The key insight is that different existing methods correspond to
our bound with different **fixed** coefficients; based on insights from this
unification, our UDIL allows **adaptive** coefficients during training, thereby
always achieving the tightest bound. Empirical results show that our UDIL
outperforms the state-of-the-art domain incremental learning methods on both
synthetic and real-world datasets. Code will be available at
https://github.com/Wang-ML-Lab/unified-continual-learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Haizhou Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12248">
<title>A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs. (arXiv:2310.12248v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12248</link>
<description rdf:parseType="Literal">&lt;p&gt;Linear temporal logic (LTL) and omega-regular objectives -- a superset of LTL
-- have seen recent use as a way to express non-Markovian objectives in
reinforcement learning. We introduce a model-based probably approximately
correct (PAC) learning algorithm for omega-regular objectives in Markov
decision processes. Unlike prior approaches, our algorithm learns from sampled
trajectories of the system and does not require prior knowledge of the system&apos;s
topology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_M/0/1/0/all/0/1&quot;&gt;Mateo Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Somenzi_F/0/1/0/all/0/1&quot;&gt;Fabio Somenzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1&quot;&gt;Ashutosh Trivedi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12262">
<title>Improving SCGAN&apos;s Similarity Constraint and Learning a Better Disentangled Representation. (arXiv:2310.12262v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12262</link>
<description rdf:parseType="Literal">&lt;p&gt;SCGAN adds a similarity constraint between generated images and conditions as
a regularization term on generative adversarial networks. Similarity constraint
works as a tutor to instruct the generator network to comprehend the difference
of representations based on conditions. We understand how SCGAN works on a
deeper level. This understanding makes us realize that the similarity
constraint functions like the contrastive loss function. We believe that a
model with high understanding and intelligence measures the similarity between
images based on their structure and high level features, just like humans do.
Two major changes we applied to SCGAN in order to make a modified model are
using SSIM to measure similarity between images and applying contrastive loss
principles to the similarity constraint. The modified model performs better
using FID and FactorVAE metrics. The modified model also has better
generalisability compared to other models. Keywords Generative Adversarial
Nets, Unsupervised Learning, Disentangled Representation Learning, Contrastive
Disentanglement, SSIM
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yazdanpanah_I/0/1/0/all/0/1&quot;&gt;Iman Yazdanpanah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12274">
<title>An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning. (arXiv:2310.12274v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12274</link>
<description rdf:parseType="Literal">&lt;p&gt;Textural Inversion, a prompt learning method, learns a singular embedding for
a new &quot;word&quot; to represent image style and appearance, allowing it to be
integrated into natural language sentences to generate novel synthesised
images. However, identifying and integrating multiple object-level concepts
within one scene poses significant challenges even when embeddings for
individual concepts are attainable. This is further confirmed by our empirical
tests. To address this challenge, we introduce a framework for Multi-Concept
Prompt Learning (MCPL), where multiple new &quot;words&quot; are simultaneously learned
from a single sentence-image pair. To enhance the accuracy of word-concept
correlation, we propose three regularisation techniques: Attention Masking
(AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss
(PromptCL) to separate the embeddings of different concepts; and Bind adjective
(Bind adj.) to associate new &quot;words&quot; with known words. We evaluate via image
generation, editing, and attention visualisation with diverse images. Extensive
quantitative comparisons demonstrate that our method can learn more
semantically disentangled concepts with enhanced word-concept correlation.
Additionally, we introduce a novel dataset and evaluation protocol tailored for
this new task of learning object-level concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Chen Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanno_R/0/1/0/all/0/1&quot;&gt;Ryutaro Tanno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saseendran_A/0/1/0/all/0/1&quot;&gt;Amrutha Saseendran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diethe_T/0/1/0/all/0/1&quot;&gt;Tom Diethe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teare_P/0/1/0/all/0/1&quot;&gt;Philip Teare&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12281">
<title>Enhancing the Performance of Automated Grade Prediction in MOOC using Graph Representation Learning. (arXiv:2310.12281v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12281</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, Massive Open Online Courses (MOOCs) have gained significant
traction as a rapidly growing phenomenon in online learning. Unlike traditional
classrooms, MOOCs offer a unique opportunity to cater to a diverse audience
from different backgrounds and geographical locations. Renowned universities
and MOOC-specific providers, such as Coursera, offer MOOC courses on various
subjects. Automated assessment tasks like grade and early dropout predictions
are necessary due to the high enrollment and limited direct interaction between
teachers and learners. However, current automated assessment approaches
overlook the structural links between different entities involved in the
downstream tasks, such as the students and courses. Our hypothesis suggests
that these structural relationships, manifested through an interaction graph,
contain valuable information that can enhance the performance of the task at
hand. To validate this, we construct a unique knowledge graph for a large MOOC
dataset, which will be publicly available to the research community.
Furthermore, we utilize graph embedding techniques to extract latent structural
information encoded in the interactions between entities in the dataset. These
techniques do not require ground truth labels and can be utilized for various
tasks. Finally, by combining entity-specific features, behavioral features, and
extracted structural features, we enhance the performance of predictive machine
learning models in student assignment grade prediction. Our experiments
demonstrate that structural features can significantly improve the predictive
performance of downstream assessment tasks. The code and data are available in
\url{https://github.com/DSAatUSU/MOOPer_grade_prediction}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farokhi_S/0/1/0/all/0/1&quot;&gt;Soheila Farokhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaramala_A/0/1/0/all/0/1&quot;&gt;Aswani Yaramala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiangtao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Muhammad F. A. Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xiaojun Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karimi_H/0/1/0/all/0/1&quot;&gt;Hamid Karimi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12294">
<title>Open-Set Multivariate Time-Series Anomaly Detection. (arXiv:2310.12294v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12294</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerous methods for time series anomaly detection (TSAD) methods have
emerged in recent years. Most existing methods are unsupervised and assume the
availability of normal training samples only, while few supervised methods have
shown superior performance by incorporating labeled anomalous samples in the
training phase. However, certain anomaly types are inherently challenging for
unsupervised methods to differentiate from normal data, while supervised
methods are constrained to detecting anomalies resembling those present during
training, failing to generalize to unseen anomaly classes. This paper is the
first attempt in providing a novel approach for the open-set TSAD problem, in
which a small number of labeled anomalies from a limited class of anomalies are
visible in the training phase, with the objective of detecting both seen and
unseen anomaly classes in the test phase. The proposed method, called
Multivariate Open-Set timeseries Anomaly Detection (MOSAD) consists of three
primary modules: a Feature Extractor to extract meaningful time-series
features; a Multi-head Network consisting of Generative-, Deviation-, and
Contrastive heads for capturing both seen and unseen anomaly classes; and an
Anomaly Scoring module leveraging the insights of the three heads to detect
anomalies. Extensive experiments on three real-world datasets consistently show
that our approach surpasses existing methods under various experimental
settings, thus establishing a new state-of-the-art performance in the TSAD
field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1&quot;&gt;Thomas Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1&quot;&gt;Thi Kieu Khanh Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armanfard_N/0/1/0/all/0/1&quot;&gt;Narges Armanfard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12298">
<title>Jorge: Approximate Preconditioning for GPU-efficient Second-order Optimization. (arXiv:2310.12298v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12298</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite their better convergence properties compared to first-order
optimizers, second-order optimizers for deep learning have been less popular
due to their significant computational costs. The primary efficiency bottleneck
in such optimizers is matrix inverse calculations in the preconditioning step,
which are expensive to compute on GPUs. In this paper, we introduce Jorge, a
second-order optimizer that promises the best of both worlds -- rapid
convergence benefits of second-order methods, and high computational efficiency
typical of first-order methods. We address the primary computational bottleneck
of computing matrix inverses by completely eliminating them using an
approximation of the preconditioner computation. This makes Jorge extremely
efficient on GPUs in terms of wall-clock time. Further, we describe an approach
to determine Jorge&apos;s hyperparameters directly from a well-tuned SGD baseline,
thereby significantly minimizing tuning efforts. Our empirical evaluations
demonstrate the distinct advantages of using Jorge, outperforming
state-of-the-art optimizers such as SGD, AdamW, and Shampoo across multiple
deep learning models, both in terms of sample efficiency and wall-clock time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Siddharth Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sating_Z/0/1/0/all/0/1&quot;&gt;Zachary Sating&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatele_A/0/1/0/all/0/1&quot;&gt;Abhinav Bhatele&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12303">
<title>Document-Level Language Models for Machine Translation. (arXiv:2310.12303v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12303</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the known limitations, most machine translation systems today still
operate on the sentence-level. One reason for this is, that most parallel
training data is only sentence-level aligned, without document-level meta
information available. In this work, we set out to build context-aware
translation systems utilizing document-level monolingual data instead. This can
be achieved by combining any existing sentence-level translation model with a
document-level language model. We improve existing approaches by leveraging
recent advancements in model combination. Additionally, we propose novel
weighting techniques that make the system combination more flexible and
significantly reduce computational overhead. In a comprehensive evaluation on
four diverse translation tasks, we show that our extensions improve
document-targeted scores substantially and are also computationally more
efficient. However, we also find that in most scenarios, back-translation gives
even better results, at the cost of having to re-train the translation system.
Finally, we explore language model fusion in the light of recent advancements
in large language models. Our findings suggest that there might be strong
potential in utilizing large language models via model combination.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrick_F/0/1/0/all/0/1&quot;&gt;Frithjof Petrick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herold_C/0/1/0/all/0/1&quot;&gt;Christian Herold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrushkov_P/0/1/0/all/0/1&quot;&gt;Pavel Petrushkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khadivi_S/0/1/0/all/0/1&quot;&gt;Shahram Khadivi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1&quot;&gt;Hermann Ney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12304">
<title>Preference Optimization for Molecular Language Models. (arXiv:2310.12304v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2310.12304</link>
<description rdf:parseType="Literal">&lt;p&gt;Molecular language modeling is an effective approach to generating novel
chemical structures. However, these models do not \emph{a priori} encode
certain preferences a chemist may desire. We investigate the use of fine-tuning
using Direct Preference Optimization to better align generated molecules with
chemist preferences. Our findings suggest that this approach is simple,
efficient, and highly effective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Park_R/0/1/0/all/0/1&quot;&gt;Ryan Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Theisen_R/0/1/0/all/0/1&quot;&gt;Ryan Theisen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sahni_N/0/1/0/all/0/1&quot;&gt;Navriti Sahni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Patek_M/0/1/0/all/0/1&quot;&gt;Marcel Patek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cichonska_A/0/1/0/all/0/1&quot;&gt;Anna Cicho&amp;#x144;ska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rahman_R/0/1/0/all/0/1&quot;&gt;Rayees Rahman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12309">
<title>A Unifying Framework for Learning Argumentation Semantics. (arXiv:2310.12309v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.12309</link>
<description rdf:parseType="Literal">&lt;p&gt;Argumentation is a very active research field of Artificial Intelligence
concerned with the representation and evaluation of arguments used in dialogues
between humans and/or artificial agents. Acceptability semantics of formal
argumentation systems define the criteria for the acceptance or rejection of
arguments. Several software systems, known as argumentation solvers, have been
developed to compute the accepted/rejected arguments using such criteria. These
include systems that learn to identify the accepted arguments using
non-interpretable methods. In this paper we present a novel framework, which
uses an Inductive Logic Programming approach to learn the acceptability
semantics for several abstract and structured argumentation frameworks in an
interpretable way. Through an empirical evaluation we show that our framework
outperforms existing argumentation solvers, thus opening up new future research
directions in the area of formal argumentation and human-machine dialogues.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mileva_Z/0/1/0/all/0/1&quot;&gt;Zlatina Mileva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bikakis_A/0/1/0/all/0/1&quot;&gt;Antonis Bikakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DAsaro_F/0/1/0/all/0/1&quot;&gt;Fabio Aurelio D&amp;#x27;Asaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1&quot;&gt;Mark Law&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1&quot;&gt;Alessandra Russo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12324">
<title>Opportunities for Adaptive Experiments to Enable Continuous Improvement that Trades-off Instructor and Researcher Incentives. (arXiv:2310.12324v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2310.12324</link>
<description rdf:parseType="Literal">&lt;p&gt;Randomized experimental comparisons of alternative pedagogical strategies
could provide useful empirical evidence in instructors&apos; decision-making.
However, traditional experiments do not have a clear and simple pathway to
using data rapidly to try to increase the chances that students in an
experiment get the best conditions. Drawing inspiration from the use of machine
learning and experimentation in product development at leading technology
companies, we explore how adaptive experimentation might help in continuous
course improvement. In adaptive experiments, as different arms/conditions are
deployed to students, data is analyzed and used to change the experience for
future students. This can be done using machine learning algorithms to identify
which actions are more promising for improving student experience or outcomes.
This algorithm can then dynamically deploy the most effective conditions to
future students, resulting in better support for students&apos; needs. We illustrate
the approach with a case study providing a side-by-side comparison of
traditional and adaptive experimentation of self-explanation prompts in online
homework problems in a CS1 course. This provides a first step in exploring the
future of how this methodology can be useful in bridging research and practice
in doing continuous improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musabirov_I/0/1/0/all/0/1&quot;&gt;Ilya Musabirov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zavaleta_Bernuy_A/0/1/0/all/0/1&quot;&gt;Angela Zavaleta-Bernuy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liut_M/0/1/0/all/0/1&quot;&gt;Michael Liut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1&quot;&gt;Joseph Jay Williams&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12345">
<title>ClusT3: Information Invariant Test-Time Training. (arXiv:2310.12345v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12345</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning models have shown remarkable performance in a broad range of
vision tasks. However, they are often vulnerable against domain shifts at
test-time. Test-time training (TTT) methods have been developed in an attempt
to mitigate these vulnerabilities, where a secondary task is solved at training
time simultaneously with the main task, to be later used as an self-supervised
proxy task at test-time. In this work, we propose a novel unsupervised TTT
technique based on the maximization of Mutual Information between multi-scale
feature maps and a discrete latent representation, which can be integrated to
the standard training as an auxiliary clustering task. Experimental results
demonstrate competitive classification performance on different popular
test-time adaptation benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hakim_G/0/1/0/all/0/1&quot;&gt;Gustavo A. Vargas Hakim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osowiechi_D/0/1/0/all/0/1&quot;&gt;David Osowiechi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noori_M/0/1/0/all/0/1&quot;&gt;Mehrdad Noori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheraghalikhani_M/0/1/0/all/0/1&quot;&gt;Milad Cheraghalikhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1&quot;&gt;Ismail Ben Ayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1&quot;&gt;Christian Desrosiers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12346">
<title>Tracking electricity losses and their perceived causes using nighttime light and social media. (arXiv:2310.12346v1 [physics.soc-ph])</title>
<link>http://arxiv.org/abs/2310.12346</link>
<description rdf:parseType="Literal">&lt;p&gt;Urban environments are intricate systems where the breakdown of critical
infrastructure can impact both the economic and social well-being of
communities. Electricity systems hold particular significance, as they are
essential for other infrastructure, and disruptions can trigger widespread
consequences. Typically, assessing electricity availability requires
ground-level data, a challenge in conflict zones and regions with limited
access. This study shows how satellite imagery, social media, and information
extraction can monitor blackouts and their perceived causes. Night-time light
data (in March 2019 for Caracas, Venezuela) is used to indicate blackout
regions. Twitter data is used to determine sentiment and topic trends, while
statistical analysis and topic modeling delved into public perceptions
regarding blackout causes. The findings show an inverse relationship between
nighttime light intensity. Tweets mentioning the Venezuelan President displayed
heightened negativity and a greater prevalence of blame-related terms,
suggesting a perception of government accountability for the outages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kerber_S/0/1/0/all/0/1&quot;&gt;Samuel W Kerber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Duncan_N/0/1/0/all/0/1&quot;&gt;Nicholas A Duncan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+LHer_G/0/1/0/all/0/1&quot;&gt;Guillaume F LHer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bazilian_M/0/1/0/all/0/1&quot;&gt;Morgan Bazilian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Elvidge_C/0/1/0/all/0/1&quot;&gt;Chris Elvidge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Deinert_M/0/1/0/all/0/1&quot;&gt;Mark R Deinert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12350">
<title>Equipping Federated Graph Neural Networks with Structure-aware Group Fairness. (arXiv:2310.12350v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12350</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have been widely used for various types of graph
data processing and analytical tasks in different domains. Training GNNs over
centralized graph data can be infeasible due to privacy concerns and regulatory
restrictions. Thus, federated learning (FL) becomes a trending solution to
address this challenge in a distributed learning paradigm. However, as GNNs may
inherit historical bias from training data and lead to discriminatory
predictions, the bias of local models can be easily propagated to the global
model in distributed settings. This poses a new challenge in mitigating bias in
federated GNNs. To address this challenge, we propose $\text{F}^2$GNN, a Fair
Federated Graph Neural Network, that enhances group fairness of federated GNNs.
As bias can be sourced from both data and learning algorithms, $\text{F}^2$GNN
aims to mitigate both types of bias under federated settings. First, we provide
theoretical insights on the connection between data bias in a training graph
and statistical fairness metrics of the trained GNN models. Based on the
theoretical analysis, we design $\text{F}^2$GNN which contains two key
components: a fairness-aware local model update scheme that enhances group
fairness of the local models on the client side, and a fairness-weighted global
model update scheme that takes both data bias and fairness metrics of local
models into consideration in the aggregation process. We evaluate
$\text{F}^2$GNN empirically versus a number of baseline methods, and
demonstrate that $\text{F}^2$GNN outperforms these baselines in terms of both
fairness and model accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_N/0/1/0/all/0/1&quot;&gt;Nan Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiuling Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wendy Hui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1&quot;&gt;Violet Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1&quot;&gt;Yue Ning&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12353">
<title>Networkwide Traffic State Forecasting Using Exogenous Information: A Multi-Dimensional Graph Attention-Based Approach. (arXiv:2310.12353v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12353</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic state forecasting is crucial for traffic management and control
strategies, as well as user- and system-level decision making in the
transportation network. While traffic forecasting has been approached with a
variety of techniques over the last couple of decades, most approaches simply
rely on endogenous traffic variables for state prediction, despite the evidence
that exogenous factors can significantly impact traffic conditions. This paper
proposes a multi-dimensional spatio-temporal graph attention-based traffic
prediction approach (M-STGAT), which predicts traffic based on past
observations of speed, along with lane closure events, temperature, and
visibility across the transportation network. The approach is based on a graph
attention network architecture, which also learns based on the structure of the
transportation network on which these variables are observed. Numerical
experiments are performed using traffic speed and lane closure data from the
California Department of Transportation (Caltrans) Performance Measurement
System (PeMS). The corresponding weather data were downloaded from the National
Oceanic and Atmospheric Administration (NOOA) Automated Surface Observing
Systems (ASOS). For comparison, the numerical experiments implement three
alternative models which do not allow for the multi-dimensional input. The
M-STGAT is shown to outperform the three alternative models, when performing
tests using our primary data set for prediction with a 30-, 45-, and 60-minute
prediction horizon, in terms of three error measures: Mean Absolute Error
(MAE), Root Mean Square Error (RMSE) and Mean Absolute Percentage Error (MAPE).
However, the model&apos;s transferability can vary for different transfer data sets
and this aspect may require further investigation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1&quot;&gt;Syed Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filipovska_M/0/1/0/all/0/1&quot;&gt;Monika Filipovska&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12359">
<title>MARVEL: Multi-Agent Reinforcement-Learning for Large-Scale Variable Speed Limits. (arXiv:2310.12359v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2310.12359</link>
<description rdf:parseType="Literal">&lt;p&gt;Variable speed limit (VSL) control is a promising traffic management strategy
for enhancing safety and mobility. This work introduces MARVEL, a multi-agent
reinforcement learning (MARL) framework for implementing large-scale VSL
control on freeway corridors using only commonly available data. The agents
learn through a reward structure that incorporates adaptability to traffic
conditions, safety, and mobility; enabling coordination among the agents. The
proposed framework scales to cover corridors with many gantries thanks to a
parameter sharing among all VSL agents. The agents are trained in a
microsimulation environment based on a short freeway stretch with 8 gantries
spanning 7 miles and tested with 34 gantries spanning 17 miles of I-24 near
Nashville, TN. MARVEL improves traffic safety by 63.4% compared to the no
control scenario and enhances traffic mobility by 14.6% compared to a
state-of-the-practice algorithm that has been deployed on I-24. An
explainability analysis is undertaken to explore the learned policy under
different traffic conditions and the results provide insights into the
decision-making process of agents. Finally, we test the policy learned from the
simulation-based experiments on real input data from I-24 to illustrate the
potential deployment capability of the learned policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuhang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quinones_Grueiro_M/0/1/0/all/0/1&quot;&gt;Marcos Quinones-Grueiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiyao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanbing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barbour_W/0/1/0/all/0/1&quot;&gt;William Barbour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_G/0/1/0/all/0/1&quot;&gt;Gautam Biswas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Work_D/0/1/0/all/0/1&quot;&gt;Daniel Work&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12370">
<title>No-Regret Learning in Bilateral Trade via Global Budget Balance. (arXiv:2310.12370v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2310.12370</link>
<description rdf:parseType="Literal">&lt;p&gt;Bilateral trade revolves around the challenge of facilitating transactions
between two strategic agents -- a seller and a buyer -- both of whom have a
private valuations for the item. We study the online version of the problem, in
which at each time step a new seller and buyer arrive. The learner&apos;s task is to
set a price for each agent, without any knowledge about their valuations. The
sequence of sellers and buyers is chosen by an oblivious adversary. In this
setting, known negative results rule out the possibility of designing
algorithms with sublinear regret when the learner has to guarantee budget
balance for each iteration. In this paper, we introduce the notion of global
budget balance, which requires the agent to be budget balance only over the
entire time horizon. By requiring global budget balance, we provide the first
no-regret algorithms for bilateral trade with adversarial inputs under various
feedback models. First, we show that in the full-feedback model the learner can
guarantee $\tilde{O}(\sqrt{T})$ regret against the best fixed prices in
hindsight, which is order-wise optimal. Then, in the case of partial feedback
models, we provide an algorithm guaranteeing a $\tilde{O}(T^{3/4})$ regret
upper bound with one-bit feedback, which we complement with a nearly-matching
lower bound. Finally, we investigate how these results vary when measuring
regret using an alternative benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernasconi_M/0/1/0/all/0/1&quot;&gt;Martino Bernasconi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castiglioni_M/0/1/0/all/0/1&quot;&gt;Matteo Castiglioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celli_A/0/1/0/all/0/1&quot;&gt;Andrea Celli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fusco_F/0/1/0/all/0/1&quot;&gt;Federico Fusco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12387">
<title>Learning to Solve Climate Sensor Placement Problems with a Transformer. (arXiv:2310.12387v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12387</link>
<description rdf:parseType="Literal">&lt;p&gt;The optimal placement of sensors for environmental monitoring and disaster
management is a challenging problem due to its NP-hard nature. Traditional
methods for sensor placement involve exact, approximation, or heuristic
approaches, with the latter being the most widely used. However, heuristic
methods are limited by expert intuition and experience. Deep learning (DL) has
emerged as a promising approach for generating heuristic algorithms
automatically. In this paper, we introduce a novel sensor placement approach
focused on learning improvement heuristics using deep reinforcement learning
(RL) methods. Our approach leverages an RL formulation for learning improvement
heuristics, driven by an actor-critic algorithm for training the policy
network. We compare our method with several state-of-the-art approaches by
conducting comprehensive experiments, demonstrating the effectiveness and
superiority of our proposed approach in producing high-quality solutions. Our
work presents a promising direction for applying advanced DL and RL techniques
to challenging climate sensor placement problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_V/0/1/0/all/0/1&quot;&gt;Victoria Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Gang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Hui Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bryce Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_J/0/1/0/all/0/1&quot;&gt;Jochen Schmidt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12395">
<title>Closed-Form Diffusion Models. (arXiv:2310.12395v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12395</link>
<description rdf:parseType="Literal">&lt;p&gt;Score-based generative models (SGMs) sample from a target distribution by
iteratively transforming noise using the score function of the perturbed
target. For any finite training set, this score function can be evaluated in
closed form, but the resulting SGM memorizes its training data and does not
generate novel samples. In practice, one approximates the score by training a
neural network via score-matching. The error in this approximation promotes
generalization, but neural SGMs are costly to train and sample, and the
effective regularization this error provides is not well-understood
theoretically. In this work, we instead explicitly smooth the closed-form score
to obtain an SGM that generates novel samples without training. We analyze our
model and propose an efficient nearest-neighbor-based estimator of its score
function. Using this estimator, our method achieves sampling times competitive
with neural SGMs while running on consumer-grade CPUs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scarvelis_C/0/1/0/all/0/1&quot;&gt;Christopher Scarvelis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borde_H/0/1/0/all/0/1&quot;&gt;Haitz S&amp;#xe1;ez de Oc&amp;#xe1;riz Borde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solomon_J/0/1/0/all/0/1&quot;&gt;Justin Solomon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12403">
<title>Cooperative Minibatching in Graph Neural Networks. (arXiv:2310.12403v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12403</link>
<description rdf:parseType="Literal">&lt;p&gt;Significant computational resources are required to train Graph Neural
Networks (GNNs) at a large scale, and the process is highly data-intensive. One
of the most effective ways to reduce resource requirements is minibatch
training coupled with graph sampling. GNNs have the unique property that items
in a minibatch have overlapping data. However, the commonly implemented
Independent Minibatching approach assigns each Processing Element (PE) its own
minibatch to process, leading to duplicated computations and input data access
across PEs. This amplifies the Neighborhood Explosion Phenomenon (NEP), which
is the main bottleneck limiting scaling. To reduce the effects of NEP in the
multi-PE setting, we propose a new approach called Cooperative Minibatching.
Our approach capitalizes on the fact that the size of the sampled subgraph is a
concave function of the batch size, leading to significant reductions in the
amount of work per seed vertex as batch sizes increase. Hence, it is favorable
for processors equipped with a fast interconnect to work on a large minibatch
together as a single larger processor, instead of working on separate smaller
minibatches, even though global batch size is identical. We also show how to
take advantage of the same phenomenon in serial execution by generating
dependent consecutive minibatches. Our experimental evaluations show up to 4x
bandwidth savings for fetching vertex embeddings, by simply increasing this
dependency without harming model convergence. Combining our proposed
approaches, we achieve up to 64% speedup over Independent Minibatching on
single-node multi-GPU systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balin_M/0/1/0/all/0/1&quot;&gt;Muhammed Fatih Balin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+LaSalle_D/0/1/0/all/0/1&quot;&gt;Dominique LaSalle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Catalyurek_U/0/1/0/all/0/1&quot;&gt;&amp;#xdc;mit V. &amp;#xc7;ataly&amp;#xfc;rek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12404">
<title>Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative Editing. (arXiv:2310.12404v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2310.12404</link>
<description rdf:parseType="Literal">&lt;p&gt;Creating music is iterative, requiring varied methods at each stage. However,
existing AI music systems fall short in orchestrating multiple subsystems for
diverse needs. To address this gap, we introduce Loop Copilot, a novel system
that enables users to generate and iteratively refine music through an
interactive, multi-round dialogue interface. The system uses a large language
model to interpret user intentions and select appropriate AI models for task
execution. Each backend model is specialized for a specific task, and their
outputs are aggregated to meet the user&apos;s requirements. To ensure musical
coherence, essential attributes are maintained in a centralized table. We
evaluate the effectiveness of the proposed system through semi-structured
interviews and questionnaires, highlighting its utility not only in
facilitating music creation but also its potential for broader applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yixiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maezawa_A/0/1/0/all/0/1&quot;&gt;Akira Maezawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1&quot;&gt;Gus Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamamoto_K/0/1/0/all/0/1&quot;&gt;Kazuhiko Yamamoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dixon_S/0/1/0/all/0/1&quot;&gt;Simon Dixon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12407">
<title>Classification-Aided Robust Multiple Target Tracking Using Neural Enhanced Message Passing. (arXiv:2310.12407v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12407</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the challenge of tracking an unknown number of targets in strong
clutter environments using measurements from a radar sensor. Leveraging the
range-Doppler spectra information, we identify the measurement classes, which
serve as additional information to enhance clutter rejection and data
association, thus bolstering the robustness of target tracking. We first
introduce a novel neural enhanced message passing approach, where the beliefs
obtained by the unified message passing are fed into the neural network as
additional information. The output beliefs are then utilized to refine the
original beliefs. Then, we propose a classification-aided robust multiple
target tracking algorithm, employing the neural enhanced message passing
technique. This algorithm is comprised of three modules: a message-passing
module, a neural network module, and a Dempster-Shafer module. The
message-passing module is used to represent the statistical model by the factor
graph and infers target kinematic states, visibility states, and data
associations based on the spatial measurement information. The neural network
module is employed to extract features from range-Doppler spectra and derive
beliefs on whether a measurement is target-generated or clutter-generated. The
Dempster-Shafer module is used to fuse the beliefs obtained from both the
factor graph and the neural network. As a result, our proposed algorithm adopts
a model-and-data-driven framework, effectively enhancing clutter suppression
and data association, leading to significant improvements in multiple target
tracking performance. We validate the effectiveness of our approach using both
simulated and real data scenarios, demonstrating its capability to handle
challenging tracking scenarios in practical radar applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xianglong Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zengfu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Q/0/1/0/all/0/1&quot;&gt;Quan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_T/0/1/0/all/0/1&quot;&gt;Tao Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1&quot;&gt;Hua Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12408">
<title>Provable Guarantees for Neural Networks via Gradient Feature Learning. (arXiv:2310.12408v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12408</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks have achieved remarkable empirical performance, while the
current theoretical analysis is not adequate for understanding their success,
e.g., the Neural Tangent Kernel approach fails to capture their key feature
learning ability, while recent analyses on feature learning are typically
problem-specific. This work proposes a unified analysis framework for two-layer
networks trained by gradient descent. The framework is centered around the
principle of feature learning from gradients, and its effectiveness is
demonstrated by applications in several prototypical problems, such as mixtures
of Gaussians and parity functions. The framework also sheds light on
interesting network learning phenomena such as feature learning beyond kernels
and the lottery ticket hypothesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhenmei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Junyi Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yingyu Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12421">
<title>Detecting and Mitigating Algorithmic Bias in Binary Classification using Causal Modeling. (arXiv:2310.12421v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12421</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes the use of causal modeling to detect and mitigate
algorithmic bias. We provide a brief description of causal modeling and a
general overview of our approach. We then use the Adult dataset, which is
available for download from the UC Irvine Machine Learning Repository, to
develop (1) a prediction model, which is treated as a black box, and (2) a
causal model for bias mitigation. In this paper, we focus on gender bias and
the problem of binary classification. We show that gender bias in the
prediction model is statistically significant at the 0.05 level. We demonstrate
the effectiveness of the causal model in mitigating gender bias by
cross-validation. Furthermore, we show that the overall classification accuracy
is improved slightly. Our novel approach is intuitive, easy-to-use, and can be
implemented using existing statistical software tools such as &quot;lavaan&quot; in R.
Hence, it enhances explainability and promotes trust.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_W/0/1/0/all/0/1&quot;&gt;Wendy Hui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lau_W/0/1/0/all/0/1&quot;&gt;Wai Kwong Lau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12425">
<title>Automated Repair of Declarative Software Specifications in the Era of Large Language Models. (arXiv:2310.12425v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2310.12425</link>
<description rdf:parseType="Literal">&lt;p&gt;The growing adoption of declarative software specification languages, coupled
with their inherent difficulty in debugging, has underscored the need for
effective and automated repair techniques applicable to such languages.
Researchers have recently explored various methods to automatically repair
declarative software specifications, such as template-based repair,
feedback-driven iterative repair, and bounded exhaustive approaches. The latest
developments in large language models provide new opportunities for the
automatic repair of declarative specifications. In this study, we assess the
effectiveness of utilizing OpenAI&apos;s ChatGPT to repair software specifications
written in the Alloy declarative language. Unlike imperative languages,
specifications in Alloy are not executed but rather translated into logical
formulas and evaluated using backend constraint solvers to identify
specification instances and counterexamples to assertions. Our evaluation
focuses on ChatGPT&apos;s ability to improve the correctness and completeness of
Alloy declarative specifications through automatic repairs. We analyze the
results produced by ChatGPT and compare them with those of leading automatic
Alloy repair methods. Our study revealed that while ChatGPT falls short in
comparison to existing techniques, it was able to successfully repair bugs that
no other technique could address. Our analysis also identified errors in
ChatGPT&apos;s generated repairs, including improper operator usage, type errors,
higher-order logic misuse, and relational arity mismatches. Additionally, we
observed instances of hallucinations in ChatGPT-generated repairs and
inconsistency in its results. Our study provides valuable insights for software
practitioners, researchers, and tool builders considering ChatGPT for
declarative specification repairs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1&quot;&gt;Md Rashedul Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_I/0/1/0/all/0/1&quot;&gt;Iftekhar Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagheri_H/0/1/0/all/0/1&quot;&gt;Hamid Bagheri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12428">
<title>Towards Enhanced Local Explainability of Random Forests: a Proximity-Based Approach. (arXiv:2310.12428v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2310.12428</link>
<description rdf:parseType="Literal">&lt;p&gt;We initiate a novel approach to explain the out of sample performance of
random forest (RF) models by exploiting the fact that any RF can be formulated
as an adaptive weighted K nearest-neighbors model. Specifically, we use the
proximity between points in the feature space learned by the RF to re-write
random forest predictions exactly as a weighted average of the target labels of
training data points. This linearity facilitates a local notion of
explainability of RF predictions that generates attributions for any model
prediction across observations in the training set, and thereby complements
established methods like SHAP, which instead generates attributions for a model
prediction across dimensions of the feature space. We demonstrate this approach
in the context of a bond pricing model trained on US corporate bond trades, and
compare our approach to various existing approaches to model explainability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rosaler_J/0/1/0/all/0/1&quot;&gt;Joshua Rosaler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Desai_D/0/1/0/all/0/1&quot;&gt;Dhruv Desai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sarmah_B/0/1/0/all/0/1&quot;&gt;Bhaskarjit Sarmah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vamvourellis_D/0/1/0/all/0/1&quot;&gt;Dimitrios Vamvourellis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Onay_D/0/1/0/all/0/1&quot;&gt;Deran Onay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mehta_D/0/1/0/all/0/1&quot;&gt;Dhagash Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pasquali_S/0/1/0/all/0/1&quot;&gt;Stefano Pasquali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12432">
<title>CAT: Closed-loop Adversarial Training for Safe End-to-End Driving. (arXiv:2310.12432v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12432</link>
<description rdf:parseType="Literal">&lt;p&gt;Driving safety is a top priority for autonomous vehicles. Orthogonal to prior
work handling accident-prone traffic events by algorithm designs at the policy
level, we investigate a Closed-loop Adversarial Training (CAT) framework for
safe end-to-end driving in this paper through the lens of environment
augmentation. CAT aims to continuously improve the safety of driving agents by
training the agent on safety-critical scenarios that are dynamically generated
over time. A novel resampling technique is developed to turn log-replay
real-world driving scenarios into safety-critical ones via probabilistic
factorization, where the adversarial traffic generation is modeled as the
multiplication of standard motion prediction sub-problems. Consequently, CAT
can launch more efficient physical attacks compared to existing safety-critical
scenario generation methods and yields a significantly less computational cost
in the iterative learning pipeline. We incorporate CAT into the MetaDrive
simulator and validate our approach on hundreds of driving scenarios imported
from real-world driving datasets. Experimental results demonstrate that CAT can
effectively generate adversarial scenarios countering the agent being trained.
After training, the agent can achieve superior driving safety in both
log-replay and safety-critical traffic scenarios on the held-out test set. Code
and data are available at https://metadriverse.github.io/cat.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Linrui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1&quot;&gt;Zhenghao Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Quanyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bolei Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12442">
<title>Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer. (arXiv:2310.12442v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12442</link>
<description rdf:parseType="Literal">&lt;p&gt;Pretrained transformer models have demonstrated remarkable performance across
various natural language processing tasks. These models leverage the attention
mechanism to capture long- and short-range dependencies in the sequence.
However, the (full) attention mechanism incurs high computational cost -
quadratic in the sequence length, which is not affordable in tasks with long
sequences, e.g., inputs with 8k tokens. Although sparse attention can be used
to improve computational efficiency, as suggested in existing work, it has
limited modeling capacity and often fails to capture complicated dependencies
in long sequences. To tackle this challenge, we propose MASFormer, an
easy-to-implement transformer variant with Mixed Attention Spans. Specifically,
MASFormer is equipped with full attention to capture long-range dependencies,
but only at a small number of layers. For the remaining layers, MASformer only
employs sparse attention to capture short-range dependencies. Our experiments
on natural language modeling and generation tasks show that a decoder-only
MASFormer model of 1.3B parameters can achieve competitive performance to
vanilla transformers with full attention while significantly reducing
computational cost (up to 75%). Additionally, we investigate the effectiveness
of continual training with long sequence data and how sequence length impacts
downstream generation performance, which may be of independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qingru Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ram_D/0/1/0/all/0/1&quot;&gt;Dhananjay Ram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hawkins_C/0/1/0/all/0/1&quot;&gt;Cole Hawkins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_S/0/1/0/all/0/1&quot;&gt;Sheng Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tuo Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12447">
<title>Constrained Reweighting of Distributions: an Optimal Transport Approach. (arXiv:2310.12447v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2310.12447</link>
<description rdf:parseType="Literal">&lt;p&gt;We commonly encounter the problem of identifying an optimally weight adjusted
version of the empirical distribution of observed data, adhering to predefined
constraints on the weights. Such constraints often manifest as restrictions on
the moments, tail behaviour, shapes, number of modes, etc., of the resulting
weight adjusted empirical distribution. In this article, we substantially
enhance the flexibility of such methodology by introducing a nonparametrically
imbued distributional constraints on the weights, and developing a general
framework leveraging the maximum entropy principle and tools from optimal
transport. The key idea is to ensure that the maximum entropy weight adjusted
empirical distribution of the observed data is close to a pre-specified
probability distribution in terms of the optimal transport metric while
allowing for subtle departures. The versatility of the framework is
demonstrated in the context of three disparate applications where data
re-weighting is warranted to satisfy side constraints on the optimization
problem at the heart of the statistical task: namely, portfolio allocation,
semi-parametric inference for complex surveys, and ensuring algorithmic
fairness in machine learning algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chakraborty_A/0/1/0/all/0/1&quot;&gt;Abhisek Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bhattacharya_A/0/1/0/all/0/1&quot;&gt;Anirban Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pati_D/0/1/0/all/0/1&quot;&gt;Debdeep Pati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12451">
<title>MTS-LOF: Medical Time-Series Representation Learning via Occlusion-Invariant Features. (arXiv:2310.12451v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12451</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical time series data are indispensable in healthcare, providing critical
insights for disease diagnosis, treatment planning, and patient management. The
exponential growth in data complexity, driven by advanced sensor technologies,
has presented challenges related to data labeling. Self-supervised learning
(SSL) has emerged as a transformative approach to address these challenges,
eliminating the need for extensive human annotation. In this study, we
introduce a novel framework for Medical Time Series Representation Learning,
known as MTS-LOF. MTS-LOF leverages the strengths of contrastive learning and
Masked Autoencoder (MAE) methods, offering a unique approach to representation
learning for medical time series data. By combining these techniques, MTS-LOF
enhances the potential of healthcare applications by providing more
sophisticated, context-rich representations. Additionally, MTS-LOF employs a
multi-masking strategy to facilitate occlusion-invariant feature learning. This
approach allows the model to create multiple views of the data by masking
portions of it. By minimizing the discrepancy between the representations of
these masked patches and the fully visible patches, MTS-LOF learns to capture
rich contextual information within medical time series datasets. The results of
experiments conducted on diverse medical time series datasets demonstrate the
superiority of MTS-LOF over other methods. These findings hold promise for
significantly enhancing healthcare applications by improving representation
learning. Furthermore, our work delves into the integration of joint-embedding
SSL and MAE techniques, shedding light on the intricate interplay between
temporal and structural dependencies in healthcare data. This understanding is
crucial, as it allows us to grasp the complexities of healthcare data analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huayu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carreon_Rascon_A/0/1/0/all/0/1&quot;&gt;Ana S. Carreon-Rascon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiwen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1&quot;&gt;Geng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Ao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12457">
<title>MuseGNN: Interpretable and Convergent Graph Neural Network Layers at Scale. (arXiv:2310.12457v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12457</link>
<description rdf:parseType="Literal">&lt;p&gt;Among the many variants of graph neural network (GNN) architectures capable
of modeling data with cross-instance relations, an important subclass involves
layers designed such that the forward pass iteratively reduces a
graph-regularized energy function of interest. In this way, node embeddings
produced at the output layer dually serve as both predictive features for
solving downstream tasks (e.g., node classification) and energy function
minimizers that inherit desirable inductive biases and interpretability.
However, scaling GNN architectures constructed in this way remains challenging,
in part because the convergence of the forward pass may involve models with
considerable depth. To tackle this limitation, we propose a sampling-based
energy function and scalable GNN layers that iteratively reduce it, guided by
convergence guarantees in certain settings. We also instantiate a full GNN
architecture based on these designs, and the model achieves competitive
accuracy and scalability when applied to the largest publicly-available node
classification benchmark exceeding 1TB in size.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Haitian Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Renjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xiao Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zhenkun Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Minjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1&quot;&gt;David Wipf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12461">
<title>Balanced Group Convolution: An Improved Group Convolution Based on Approximability Estimates. (arXiv:2310.12461v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12461</link>
<description rdf:parseType="Literal">&lt;p&gt;The performance of neural networks has been significantly improved by
increasing the number of channels in convolutional layers. However, this
increase in performance comes with a higher computational cost, resulting in
numerous studies focused on reducing it. One promising approach to address this
issue is group convolution, which effectively reduces the computational cost by
grouping channels. However, to the best of our knowledge, there has been no
theoretical analysis on how well the group convolution approximates the
standard convolution. In this paper, we mathematically analyze the
approximation of the group convolution to the standard convolution with respect
to the number of groups. Furthermore, we propose a novel variant of the group
convolution called balanced group convolution, which shows a higher
approximation with a small additional computational cost. We provide
experimental results that validate our theoretical findings and demonstrate the
superior performance of the balanced group convolution over other variants of
group convolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Youngkyu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jongho Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chang-Ock Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12462">
<title>Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights. (arXiv:2310.12462v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12462</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of deep learning, transformers have emerged as a dominant
architecture, particularly in natural language processing tasks. However, with
their widespread adoption, concerns regarding the security and privacy of the
data processed by these models have arisen. In this paper, we address a pivotal
question: Can the data fed into transformers be recovered using their attention
weights and outputs? We introduce a theoretical framework to tackle this
problem. Specifically, we present an algorithm that aims to recover the input
data $X \in \mathbb{R}^{d \times n}$ from given attention weights $W = QK^\top
\in \mathbb{R}^{d \times d}$ and output $B \in \mathbb{R}^{n \times n}$ by
minimizing the loss function $L(X)$. This loss function captures the
discrepancy between the expected output and the actual output of the
transformer. Our findings have significant implications for the Localized
Layer-wise Mechanism (LLM), suggesting potential vulnerabilities in the model&apos;s
design from a security and privacy perspective. This work underscores the
importance of understanding and safeguarding the internal workings of
transformers to ensure the confidentiality of processed data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yichuan Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Shenghao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chiwun Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12487">
<title>Improved Operator Learning by Orthogonal Attention. (arXiv:2310.12487v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12487</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural operators, as an efficient surrogate model for learning the solutions
of PDEs, have received extensive attention in the field of scientific machine
learning. Among them, attention-based neural operators have become one of the
mainstreams in related research. However, existing approaches overfit the
limited training data due to the considerable number of parameters in the
attention mechanism. To address this, we develop an orthogonal attention based
on the eigendecomposition of the kernel integral operator and the neural
approximation of eigenfunctions. The orthogonalization naturally poses a proper
regularization effect on the resulting neural operator, which aids in resisting
overfitting and boosting generalization. Experiments on six standard neural
operator benchmark datasets comprising both regular and irregular geometries
show that our method can outperform competing baselines with decent margins.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zipeng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1&quot;&gt;Zhongkai Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bokai Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhijie Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12494">
<title>SDGym: Low-Code Reinforcement Learning Environments using System Dynamics Models. (arXiv:2310.12494v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12494</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the long-term impact of algorithmic interventions on society is
vital to achieving responsible AI. Traditional evaluation strategies often fall
short due to the complex, adaptive and dynamic nature of society. While
reinforcement learning (RL) can be a powerful approach for optimizing decisions
in dynamic settings, the difficulty of realistic environment design remains a
barrier to building robust agents that perform well in practical settings. To
address this issue we tap into the field of system dynamics (SD) as a
complementary method that incorporates collaborative simulation model
specification practices. We introduce SDGym, a low-code library built on the
OpenAI Gym framework which enables the generation of custom RL environments
based on SD simulation models. Through a feasibility study we validate that
well specified, rich RL environments can be generated from preexisting SD
models and a few lines of configuration code. We demonstrate the capabilities
of the SDGym environment using an SD model of the electric vehicle adoption
problem. We compare two SD simulators, PySD and BPTK-Py for parity, and train a
D4PG agent using the Acme framework to showcase learning and environment
interaction. Our preliminary findings underscore the dual potential of SD to
improve RL environment design and for RL to improve dynamic policy discovery
within SD models. By open-sourcing SDGym, the intent is to galvanize further
research and promote adoption across the SD and RL communities, thereby
catalyzing collaboration in this emerging interdisciplinary space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klu_E/0/1/0/all/0/1&quot;&gt;Emmanuel Klu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sethi_S/0/1/0/all/0/1&quot;&gt;Sameer Sethi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Passey_D/0/1/0/all/0/1&quot;&gt;DJ Passey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_D/0/1/0/all/0/1&quot;&gt;Donald Martin Jr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12498">
<title>Quasi Manhattan Wasserstein Distance. (arXiv:2310.12498v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12498</link>
<description rdf:parseType="Literal">&lt;p&gt;The Quasi Manhattan Wasserstein Distance (QMWD) is a metric designed to
quantify the dissimilarity between two matrices by combining elements of the
Wasserstein Distance with specific transformations. It offers improved time and
space complexity compared to the Manhattan Wasserstein Distance (MWD) while
maintaining accuracy. QMWD is particularly advantageous for large datasets or
situations with limited computational resources. This article provides a
detailed explanation of QMWD, its computation, complexity analysis, and
comparisons with WD and MWD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1&quot;&gt;Evan Unit Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12500">
<title>American Option Pricing using Self-Attention GRU and Shapley Value Interpretation. (arXiv:2310.12500v1 [q-fin.PR])</title>
<link>http://arxiv.org/abs/2310.12500</link>
<description rdf:parseType="Literal">&lt;p&gt;Options, serving as a crucial financial instrument, are used by investors to
manage and mitigate their investment risks within the securities market.
Precisely predicting the present price of an option enables investors to make
informed and efficient decisions. In this paper, we propose a machine learning
method for forecasting the prices of SPY (ETF) option based on gated recurrent
unit (GRU) and self-attention mechanism. We first partitioned the raw dataset
into 15 subsets according to moneyness and days to maturity criteria. For each
subset, we matched the corresponding U.S. government bond rates and Implied
Volatility Indices. This segmentation allows for a more insightful exploration
of the impacts of risk-free rates and underlying volatility on option pricing.
Next, we built four different machine learning models, including multilayer
perceptron (MLP), long short-term memory (LSTM), self-attention LSTM, and
self-attention GRU in comparison to the traditional binomial model. The
empirical result shows that self-attention GRU with historical data outperforms
other models due to its ability to capture complex temporal dependencies and
leverage the contextual information embedded in the historical data. Finally,
in order to unveil the &quot;black box&quot; of artificial intelligence, we employed the
SHapley Additive exPlanations (SHAP) method to interpret and analyze the
prediction results of the self-attention GRU model with historical data. This
provides insights into the significance and contributions of different input
features on the pricing of American-style options.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yanhui Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12505">
<title>Attack Prompt Generation for Red Teaming and Defending Large Language Models. (arXiv:2310.12505v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12505</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are susceptible to red teaming attacks, which
can induce LLMs to generate harmful content. Previous research constructs
attack prompts via manual or automatic methods, which have their own
limitations on construction cost and quality. To address these issues, we
propose an integrated approach that combines manual and automatic methods to
economically generate high-quality attack prompts. Specifically, considering
the impressive capabilities of newly emerged LLMs, we propose an attack
framework to instruct LLMs to mimic human-generated prompts through in-context
learning. Furthermore, we propose a defense framework that fine-tunes victim
LLMs through iterative interactions with the attack framework to enhance their
safety against red teaming attacks. Extensive experiments on different LLMs
validate the effectiveness of our proposed attack and defense frameworks.
Additionally, we release a series of attack prompts datasets named SAP with
varying sizes, facilitating the safety evaluation and enhancement of more LLMs.
Our code and dataset is available on https://github.com/Aatrox103/SAP .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1&quot;&gt;Boyi Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1&quot;&gt;Fuli Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiangnan He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12508">
<title>SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation. (arXiv:2310.12508v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12508</link>
<description rdf:parseType="Literal">&lt;p&gt;With evolving data regulations, machine unlearning (MU) has become an
important tool for fostering trust and safety in today&apos;s AI models. However,
existing MU methods focusing on data and/or weight perspectives often grapple
with limitations in unlearning accuracy, stability, and cross-domain
applicability. To address these challenges, we introduce the concept of &apos;weight
saliency&apos; in MU, drawing parallels with input saliency in model explanation.
This innovation directs MU&apos;s attention toward specific model weights rather
than the entire model, improving effectiveness and efficiency. The resultant
method that we call saliency unlearning (SalUn) narrows the performance gap
with &apos;exact&apos; unlearning (model retraining from scratch after removing the
forgetting dataset). To the best of our knowledge, SalUn is the first
principled MU approach adaptable enough to effectively erase the influence of
forgetting data, classes, or concepts in both image classification and
generation. For example, SalUn yields a stability advantage in high-variance
random data forgetting, e.g., with a 0.2% gap compared to exact unlearning on
the CIFAR-10 dataset. Moreover, in preventing conditional diffusion models from
generating harmful images, SalUn achieves nearly 100% unlearning accuracy,
outperforming current state-of-the-art baselines like Erased Stable Diffusion
and Forget-Me-Not.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Chongyu Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiancheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yihua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1&quot;&gt;Dennis Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1&quot;&gt;Eric Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sijia Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12515">
<title>WeaveNet for Approximating Two-sided Matching Problems. (arXiv:2310.12515v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12515</link>
<description rdf:parseType="Literal">&lt;p&gt;Matching, a task to optimally assign limited resources under constraints, is
a fundamental technology for society. The task potentially has various
objectives, conditions, and constraints; however, the efficient neural network
architecture for matching is underexplored. This paper proposes a novel graph
neural network (GNN), \textit{WeaveNet}, designed for bipartite graphs. Since a
bipartite graph is generally dense, general GNN architectures lose node-wise
information by over-smoothing when deeply stacked. Such a phenomenon is
undesirable for solving matching problems. WeaveNet avoids it by preserving
edge-wise information while passing messages densely to reach a better
solution. To evaluate the model, we approximated one of the \textit{strongly
NP-hard} problems, \textit{fair stable matching}. Despite its inherent
difficulties and the network&apos;s general purpose design, our model reached a
comparative performance with state-of-the-art algorithms specially designed for
stable matching for small numbers of agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sone_S/0/1/0/all/0/1&quot;&gt;Shusaku Sone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jiaxin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_A/0/1/0/all/0/1&quot;&gt;Atsushi Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiba_N/0/1/0/all/0/1&quot;&gt;Naoya Chiba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1&quot;&gt;Yoshitaka Ushiku&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12516">
<title>Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks. (arXiv:2310.12516v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12516</link>
<description rdf:parseType="Literal">&lt;p&gt;Although remarkable progress has been achieved in preventing large language
model (LLM) hallucinations using instruction tuning and retrieval augmentation,
it remains challenging to measure the reliability of LLMs using human-crafted
evaluation data which is not available for many tasks and domains and could
suffer from data leakage. Inspired by adversarial machine learning, this paper
aims to develop a method of automatically generating evaluation data by
appropriately modifying existing data on which LLMs behave faithfully.
Specifically, this paper presents AutoDebug, an LLM-based framework to use
prompting chaining to generate transferable adversarial attacks in the form of
question-answering examples. We seek to understand the extent to which these
examples trigger the hallucination behaviors of LLMs.
&lt;/p&gt;
&lt;p&gt;We implement AutoDebug using ChatGPT and evaluate the resulting two variants
of a popular open-domain question-answering dataset, Natural Questions (NQ), on
a collection of open-source and proprietary LLMs under various prompting
settings. Our generated evaluation data is human-readable and, as we show,
humans can answer these modified questions well. Nevertheless, we observe
pronounced accuracy drops across multiple LLMs including GPT-4. Our
experimental results show that LLMs are likely to hallucinate in two categories
of question-answering scenarios where (1) there are conflicts between knowledge
given in the prompt and their parametric knowledge, or (2) the knowledge
expressed in the prompt is complex. Finally, we find that the adversarial
examples generated by our method are transferable across all considered LLMs.
The examples generated by a small model can be used to debug a much larger
model, making our approach cost-effective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xiaodong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaodong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1&quot;&gt;Dan Roth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12522">
<title>Named Entity Recognition for Monitoring Plant Health Threats in Tweets: a ChouBERT Approach. (arXiv:2310.12522v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12522</link>
<description rdf:parseType="Literal">&lt;p&gt;An important application scenario of precision agriculture is detecting and
measuring crop health threats using sensors and data analysis techniques.
However, the textual data are still under-explored among the existing solutions
due to the lack of labelled data and fine-grained semantic resources. Recent
research suggests that the increasing connectivity of farmers and the emergence
of online farming communities make social media like Twitter a participatory
platform for detecting unfamiliar plant health events if we can extract
essential information from unstructured textual data. ChouBERT is a French
pre-trained language model that can identify Tweets concerning observations of
plant health issues with generalizability on unseen natural hazards. This paper
tackles the lack of labelled data by further studying ChouBERT&apos;s know-how on
token-level annotation tasks over small labeled sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Shufan Jiang&lt;/a&gt; (CRESTIC, ISEP), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Angarita_R/0/1/0/all/0/1&quot;&gt;Rafael Angarita&lt;/a&gt; (ISEP), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cormier_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Cormier&lt;/a&gt; (CRESTIC), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rousseaux_F/0/1/0/all/0/1&quot;&gt;Francis Rousseaux&lt;/a&gt; (CRESTIC)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12526">
<title>Parallel Bayesian Optimization Using Satisficing Thompson Sampling for Time-Sensitive Black-Box Optimization. (arXiv:2310.12526v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12526</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization (BO) is widely used for black-box optimization
problems, and have been shown to perform well in various real-world tasks.
However, most of the existing BO methods aim to learn the optimal solution,
which may become infeasible when the parameter space is extremely large or the
problem is time-sensitive. In these contexts, switching to a satisficing
solution that requires less information can result in better performance. In
this work, we focus on time-sensitive black-box optimization problems and
propose satisficing Thompson sampling-based parallel Bayesian optimization
(STS-PBO) approaches, including synchronous and asynchronous versions. We shift
the target from an optimal solution to a satisficing solution that is easier to
learn. The rate-distortion theory is introduced to construct a loss function
that balances the amount of information that needs to be learned with
sub-optimality, and the Blahut-Arimoto algorithm is adopted to compute the
target solution that reaches the minimum information rate under the distortion
limit at each step. Both discounted and undiscounted Bayesian cumulative regret
bounds are theoretically derived for the proposed STS-PBO approaches. The
effectiveness of the proposed methods is demonstrated on a fast-charging design
problem of Lithium-ion batteries. The results are accordant with theoretical
analyses, and show that our STS-PBO methods outperform both sequential
counterparts and parallel BO with traditional Thompson sampling in both
synchronous and asynchronous settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xiaobin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Benben Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12527">
<title>Testing the Consistency of Performance Scores Reported for Binary Classification Problems. (arXiv:2310.12527v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12527</link>
<description rdf:parseType="Literal">&lt;p&gt;Binary classification is a fundamental task in machine learning, with
applications spanning various scientific domains. Whether scientists are
conducting fundamental research or refining practical applications, they
typically assess and rank classification techniques based on performance
metrics such as accuracy, sensitivity, and specificity. However, reported
performance scores may not always serve as a reliable basis for research
ranking. This can be attributed to undisclosed or unconventional practices
related to cross-validation, typographical errors, and other factors. In a
given experimental setup, with a specific number of positive and negative test
items, most performance scores can assume specific, interrelated values. In
this paper, we introduce numerical techniques to assess the consistency of
reported performance scores and the assumed experimental setup. Importantly,
the proposed approach does not rely on statistical inference but uses numerical
methods to identify inconsistencies with certainty. Through three different
applications related to medicine, we demonstrate how the proposed techniques
can effectively detect inconsistencies, thereby safeguarding the integrity of
research fields. To benefit the scientific community, we have made the
consistency tests available in an open-source Python package.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fazekas_A/0/1/0/all/0/1&quot;&gt;Attila Fazekas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovacs_G/0/1/0/all/0/1&quot;&gt;Gy&amp;#xf6;rgy Kov&amp;#xe1;cs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12528">
<title>Constructing Impactful Machine Learning Research for Astronomy: Best Practices for Researchers and Reviewers. (arXiv:2310.12528v1 [astro-ph.IM])</title>
<link>http://arxiv.org/abs/2310.12528</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning has rapidly become a tool of choice for the astronomical
community. It is being applied across a wide range of wavelengths and problems,
from the classification of transients to neural network emulators of
cosmological simulations, and is shifting paradigms about how we generate and
report scientific results. At the same time, this class of method comes with
its own set of best practices, challenges, and drawbacks, which, at present,
are often reported on incompletely in the astrophysical literature. With this
paper, we aim to provide a primer to the astronomical community, including
authors, reviewers, and editors, on how to implement machine learning models
and report their results in a way that ensures the accuracy of the results,
reproducibility of the findings, and usefulness of the method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Huppenkothen_D/0/1/0/all/0/1&quot;&gt;D. Huppenkothen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Ntampaka_M/0/1/0/all/0/1&quot;&gt;M. Ntampaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Ho_M/0/1/0/all/0/1&quot;&gt;M. Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Fouesneau_M/0/1/0/all/0/1&quot;&gt;M. Fouesneau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Nord_B/0/1/0/all/0/1&quot;&gt;B. Nord&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Peek_J/0/1/0/all/0/1&quot;&gt;J. E. G. Peek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Walmsley_M/0/1/0/all/0/1&quot;&gt;M. Walmsley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;J. F. Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Avestruz_C/0/1/0/all/0/1&quot;&gt;C. Avestruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Buck_T/0/1/0/all/0/1&quot;&gt;T. Buck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Brescia_M/0/1/0/all/0/1&quot;&gt;M. Brescia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Finkbeiner_D/0/1/0/all/0/1&quot;&gt;D. P. Finkbeiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Goulding_A/0/1/0/all/0/1&quot;&gt;A. D. Goulding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Kacprzak_T/0/1/0/all/0/1&quot;&gt;T. Kacprzak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Melchior_P/0/1/0/all/0/1&quot;&gt;P. Melchior&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Pasquato_M/0/1/0/all/0/1&quot;&gt;M. Pasquato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Ramachandra_N/0/1/0/all/0/1&quot;&gt;N. Ramachandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Ting_Y/0/1/0/all/0/1&quot;&gt;Y.-S. Ting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Ven_G/0/1/0/all/0/1&quot;&gt;G. van de Ven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Villar_S/0/1/0/all/0/1&quot;&gt;S. Villar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Villar_V/0/1/0/all/0/1&quot;&gt;V.A. Villar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Zinger_E/0/1/0/all/0/1&quot;&gt;E. Zinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12544">
<title>Neural Likelihood Approximation for Integer Valued Time Series Data. (arXiv:2310.12544v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2310.12544</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic processes defined on integer valued state spaces are popular
within the physical and biological sciences. These models are necessary for
capturing the dynamics of small systems where the individual nature of the
populations cannot be ignored and stochastic effects are important. The
inference of the parameters of such models, from time series data, is difficult
due to intractability of the likelihood; current methods, based on simulations
of the underlying model, can be so computationally expensive as to be
prohibitive. In this paper we construct a neural likelihood approximation for
integer valued time series data using causal convolutions, which allows us to
evaluate the likelihood of the whole time series in parallel. We demonstrate
our method by performing inference on a number of ecological and
epidemiological models, showing that we can accurately approximate the true
posterior while achieving significant computational speed ups in situations
where current methods struggle.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+OLoughlin_L/0/1/0/all/0/1&quot;&gt;Luke O&amp;#x27;Loughlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maclean_J/0/1/0/all/0/1&quot;&gt;John Maclean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Black_A/0/1/0/all/0/1&quot;&gt;Andrew Black&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12547">
<title>PGA: Personalizing Grasping Agents with Single Human-Robot Interaction. (arXiv:2310.12547v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2310.12547</link>
<description rdf:parseType="Literal">&lt;p&gt;Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that
ground and grasp objects based on natural language instructions. While robots
capable of recognizing personal objects like &quot;my wallet&quot; can interact more
naturally with non-expert users, current LCRG systems primarily limit robots to
understanding only generic expressions. To this end, we introduce a task
scenario GraspMine with a novel dataset that aims to locate and grasp personal
objects given personal indicators via learning from a single human-robot
interaction. To address GraspMine, we propose Personalized Grasping Agent
(PGA), that learns personal objects by propagating user-given information
through a Reminiscence-a collection of raw images from the user&apos;s environment.
Specifically, PGA acquires personal object information by a user presenting a
personal object with its associated indicator, followed by PGA inspecting the
object by rotating it. Based on the acquired information, PGA pseudo-labels
objects in the Reminiscence by our proposed label propagation algorithm.
Harnessing the information acquired from the interactions and the
pseudo-labeled objects in the Reminiscence, PGA adapts the object grounding
model to grasp personal objects. Experiments on GraspMine show that PGA
significantly outperforms baseline methods both in offline and online settings,
signifying its effectiveness and personalization applicability on real-world
scenarios. Finally, qualitative analysis shows the effectiveness of PGA through
a detailed investigation of results in each phase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junghyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1&quot;&gt;Gi-Cheon Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jaein Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Seoyun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_M/0/1/0/all/0/1&quot;&gt;Minjoon Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Byoung-Tak Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12553">
<title>Explanation-Based Training with Differentiable Insertion/Deletion Metric-Aware Regularizers. (arXiv:2310.12553v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12553</link>
<description rdf:parseType="Literal">&lt;p&gt;The quality of explanations for the predictions of complex machine learning
predictors is often measured using insertion and deletion metrics, which assess
the faithfulness of the explanations, i.e., how correctly the explanations
reflect the predictor&apos;s behavior. To improve the faithfulness, we propose
insertion/deletion metric-aware explanation-based optimization (ID-ExpO), which
optimizes differentiable predictors to improve both insertion and deletion
scores of the explanations while keeping their predictive accuracy. Since the
original insertion and deletion metrics are indifferentiable with respect to
the explanations and directly unavailable for gradient-based optimization, we
extend the metrics to be differentiable and use them to formalize insertion and
deletion metric-based regularizers. The experimental results on image and
tabular datasets show that the deep neural networks-based predictors fine-tuned
using ID-ExpO enable popular post-hoc explainers to produce more faithful and
easy-to-interpret explanations while keeping high predictive accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoshikawa_u/0/1/0/all/0/1&quot;&gt;uya Yoshikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iwata_T/0/1/0/all/0/1&quot;&gt;Tomoharu Iwata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12560">
<title>Fast Model Debias with Machine Unlearning. (arXiv:2310.12560v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12560</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent discoveries have revealed that deep neural networks might behave in a
biased manner in many real-world scenarios. For instance, deep networks trained
on a large-scale face recognition dataset CelebA tend to predict blonde hair
for females and black hair for males. Such biases not only jeopardize the
robustness of models but also perpetuate and amplify social biases, which is
especially concerning for automated decision-making processes in healthcare,
recruitment, etc., as they could exacerbate unfair economic and social
inequalities among different groups. Existing debiasing methods suffer from
high costs in bias labeling or model re-training, while also exhibiting a
deficiency in terms of elucidating the origins of biases within the model. To
this respect, we propose a fast model debiasing framework (FMD) which offers an
efficient approach to identify, evaluate and remove biases inherent in trained
models. The FMD identifies biased attributes through an explicit counterfactual
concept and quantifies the influence of data samples with influence functions.
Moreover, we design a machine unlearning-based strategy to efficiently and
effectively remove the bias in a trained model with a small counterfactual
dataset. Experiments on the Colored MNIST, CelebA, and Adult Income datasets
along with experiments with large language models demonstrate that our method
achieves superior or competing accuracies compared with state-of-the-art
methods while attaining significantly fewer biases and requiring much less
debiasing cost. Notably, our method requires only a small external dataset and
updating a minimal amount of model parameters, without the requirement of
access to training data that may be too large or unavailable in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ruizhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianfei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Huimin Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1&quot;&gt;Jianhong Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1&quot;&gt;Tianxiang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jin Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Joey Tianyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zuozhu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12563">
<title>Approximate information maximization for bandit games. (arXiv:2310.12563v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2310.12563</link>
<description rdf:parseType="Literal">&lt;p&gt;Entropy maximization and free energy minimization are general physical
principles for modeling the dynamics of various physical systems. Notable
examples include modeling decision-making within the brain using the
free-energy principle, optimizing the accuracy-complexity trade-off when
accessing hidden variables with the information bottleneck principle (Tishby et
al., 2000), and navigation in random environments using information
maximization (Vergassola et al., 2007). Built on this principle, we propose a
new class of bandit algorithms that maximize an approximation to the
information of a key variable within the system. To this end, we develop an
approximated analytical physics-based representation of an entropy to forecast
the information gain of each action and greedily choose the one with the
largest information gain. This method yields strong performances in classical
bandit settings. Motivated by its empirical success, we prove its asymptotic
optimality for the two-armed bandit problem with Gaussian rewards. Owing to its
ability to encompass the system&apos;s properties in a global physical functional,
this approach can be efficiently adapted to more complex bandit settings,
calling for further investigation of information maximization approaches for
multi-armed bandit problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barbier__Chebbah_A/0/1/0/all/0/1&quot;&gt;Alex Barbier--Chebbah&lt;/a&gt; (IP, CNRS, UPCit&amp;#xe9;), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vestergaard_C/0/1/0/all/0/1&quot;&gt;Christian L. Vestergaard&lt;/a&gt; (IP, CNRS, UPCit&amp;#xe9;), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Masson_J/0/1/0/all/0/1&quot;&gt;Jean-Baptiste Masson&lt;/a&gt; (IP, CNRS, UPCit&amp;#xe9;), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boursier_E/0/1/0/all/0/1&quot;&gt;Etienne Boursier&lt;/a&gt; (CELESTE)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12565">
<title>Open-World Lifelong Graph Learning. (arXiv:2310.12565v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12565</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of lifelong graph learning in an open-world scenario,
where a model needs to deal with new tasks and potentially unknown classes. We
utilize Out-of-Distribution (OOD) detection methods to recognize new classes
and adapt existing non-graph OOD detection methods to graph data. Crucially, we
suggest performing new class detection by combining OOD detection methods with
information aggregated from the graph neighborhood. Most OOD detection methods
avoid determining a crisp threshold for deciding whether a vertex is OOD. To
tackle this problem, we propose a Weakly-supervised Relevance Feedback
(Open-WRF) method, which decreases the sensitivity to thresholds in OOD
detection. We evaluate our approach on six benchmark datasets. Our results show
that the proposed neighborhood aggregation method for OOD scores outperforms
existing methods independent of the underlying graph neural network.
Furthermore, we demonstrate that our Open-WRF method is more robust to
threshold selection and analyze the influence of graph neighborhood on OOD
detection. The aggregation and threshold methods are compatible with arbitrary
graph neural networks and OOD detection methods, making our approach versatile
and applicable to many real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffmann_M/0/1/0/all/0/1&quot;&gt;Marcel Hoffmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1&quot;&gt;Lukas Galke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1&quot;&gt;Ansgar Scherp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12567">
<title>Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark. (arXiv:2310.12567v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.12567</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) systems possess significant potential to drive
societal progress. However, their deployment often faces obstacles due to
substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a
solution to optimize policies while simultaneously adhering to multiple
constraints, thereby addressing the challenge of integrating reinforcement
learning in safety-critical scenarios. In this paper, we present an environment
suite called Safety-Gymnasium, which encompasses safety-critical tasks in both
single and multi-agent scenarios, accepting vector and vision-only input.
Additionally, we offer a library of algorithms named Safe Policy Optimization
(SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive
library can serve as a validation tool for the research community. By
introducing this benchmark, we aim to facilitate the evaluation and comparison
of safety performance, thus fostering the development of reinforcement learning
for safer, more reliable, and responsible real-world applications. The website
of this project can be accessed at
https://sites.google.com/view/safety-gymnasium.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jiaming Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Borong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiayi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xuehai Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weidong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1&quot;&gt;Ruiyang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1&quot;&gt;Yiran Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Juntao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yaodong Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12568">
<title>Julearn: an easy-to-use library for leakage-free evaluation and inspection of ML models. (arXiv:2310.12568v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12568</link>
<description rdf:parseType="Literal">&lt;p&gt;The fast-paced development of machine learning (ML) methods coupled with its
increasing adoption in research poses challenges for researchers without
extensive training in ML. In neuroscience, for example, ML can help understand
brain-behavior relationships, diagnose diseases, and develop biomarkers using
various data sources like magnetic resonance imaging and
electroencephalography. The primary objective of ML is to build models that can
make accurate predictions on unseen data. Researchers aim to prove the
existence of such generalizable models by evaluating performance using
techniques such as cross-validation (CV), which uses systematic subsampling to
estimate the generalization performance. Choosing a CV scheme and evaluating an
ML pipeline can be challenging and, if used improperly, can lead to
overestimated results and incorrect interpretations.
&lt;/p&gt;
&lt;p&gt;We created julearn, an open-source Python library, that allow researchers to
design and evaluate complex ML pipelines without encountering in common
pitfalls. In this manuscript, we present the rationale behind julearn&apos;s design,
its core features, and showcase three examples of previously-published research
projects that can be easily implemented using this novel library. Julearn aims
to simplify the entry into the ML world by providing an easy-to-use environment
with built in guards against some of the most common ML pitfalls. With its
design, unique features and simple interface, it poses as a useful Python-based
library for research projects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamdan_S/0/1/0/all/0/1&quot;&gt;Sami Hamdan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+More_S/0/1/0/all/0/1&quot;&gt;Shammi More&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sasse_L/0/1/0/all/0/1&quot;&gt;Leonard Sasse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komeyer_V/0/1/0/all/0/1&quot;&gt;Vera Komeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patil_K/0/1/0/all/0/1&quot;&gt;Kaustubh R. Patil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raimondo_F/0/1/0/all/0/1&quot;&gt;Federico Raimondo&lt;/a&gt; (for the Alzheimer&amp;#x27;s Disease Neuroimaging Initiative)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12570">
<title>DA-TransUNet: Integrating Spatial and Channel Dual Attention with Transformer U-Net for Medical Image Segmentation. (arXiv:2310.12570v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2310.12570</link>
<description rdf:parseType="Literal">&lt;p&gt;Great progress has been made in automatic medical image segmentation due to
powerful deep representation learning. The influence of transformer has led to
research into its variants, and large-scale replacement of traditional CNN
modules. However, such trend often overlooks the intrinsic feature extraction
capabilities of the transformer and potential refinements to both the model and
the transformer module through minor adjustments. This study proposes a novel
deep medical image segmentation framework, called DA-TransUNet, aiming to
introduce the Transformer and dual attention block into the encoder and decoder
of the traditional U-shaped architecture. Unlike prior transformer-based
solutions, our DA-TransUNet utilizes attention mechanism of transformer and
multifaceted feature extraction of DA-Block, which can efficiently combine
global, local, and multi-scale features to enhance medical image segmentation.
Meanwhile, experimental results show that a dual attention block is added
before the Transformer layer to facilitate feature extraction in the U-net
structure. Furthermore, incorporating dual attention blocks in skip connections
can enhance feature transfer to the decoder, thereby improving image
segmentation performance. Experimental results across various benchmark of
medical image segmentation reveal that DA-TransUNet significantly outperforms
the state-of-the-art methods. The codes and parameters of our model will be
publicly available at https://github.com/SUN-1024/DA-TransUnet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_G/0/1/0/all/0/1&quot;&gt;Guanqun Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yizhi Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kong_W/0/1/0/all/0/1&quot;&gt;Weikun Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zichang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jianhua Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Racharak_T/0/1/0/all/0/1&quot;&gt;Teeradaj Racharak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_L/0/1/0/all/0/1&quot;&gt;Le-Minh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xin_J/0/1/0/all/0/1&quot;&gt;Junyi Xin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12585">
<title>Time-Aware Representation Learning for Time-Sensitive Question Answering. (arXiv:2310.12585v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12585</link>
<description rdf:parseType="Literal">&lt;p&gt;Time is one of the crucial factors in real-world question answering (QA)
problems. However, language models have difficulty understanding the
relationships between time specifiers, such as &apos;after&apos; and &apos;before&apos;, and
numbers, since existing QA datasets do not include sufficient time expressions.
To address this issue, we propose a Time-Context aware Question Answering
(TCQA) framework. We suggest a Time-Context dependent Span Extraction (TCSE)
task, and build a time-context dependent data generation framework for model
training. Moreover, we present a metric to evaluate the time awareness of the
QA model using TCSE. The TCSE task consists of a question and four sentence
candidates classified as correct or incorrect based on time and context. The
model is trained to extract the answer span from the sentence that is both
correct in time and context. The model trained with TCQA outperforms baseline
models up to 8.5 of the F1-score in the TimeQA dataset. Our dataset and code
are available at https://github.com/sonjbin/TCQA
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Son_J/0/1/0/all/0/1&quot;&gt;Jungbin Son&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1&quot;&gt;Alice Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12595">
<title>Causal Similarity-Based Hierarchical Bayesian Models. (arXiv:2310.12595v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12595</link>
<description rdf:parseType="Literal">&lt;p&gt;The key challenge underlying machine learning is generalisation to new data.
This work studies generalisation for datasets consisting of related tasks that
may differ in causal mechanisms. For example, observational medical data for
complex diseases suffers from heterogeneity in causal mechanisms of disease
across patients, creating challenges for machine learning algorithms that need
to generalise to new patients outside of the training dataset. Common
approaches for learning supervised models with heterogeneous datasets include
learning a global model for the entire dataset, learning local models for each
tasks&apos; data, or utilising hierarchical, meta-learning and multi-task learning
approaches to learn how to generalise from data pooled across multiple tasks.
In this paper we propose causal similarity-based hierarchical Bayesian models
to improve generalisation to new tasks by learning how to pool data from
training tasks with similar causal mechanisms. We apply this general modelling
principle to Bayesian neural networks and compare a variety of methods for
estimating causal task similarity (for both known and unknown causal models).
We demonstrate the benefits of our approach and applicability to real world
problems through a range of experiments on simulated and real data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wharrie_S/0/1/0/all/0/1&quot;&gt;Sophie Wharrie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaski_S/0/1/0/all/0/1&quot;&gt;Samuel Kaski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12609">
<title>Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning. (arXiv:2310.12609v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2310.12609</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have risen as a powerful tool in robotics due to their
flexibility and multi-modality. While some of these methods effectively address
complex problems, they often depend heavily on inference-time obstacle
detection and require additional equipment. Addressing these challenges, we
present a method that, during inference time, simultaneously generates only
reachable goals and plans motions that avoid obstacles, all from a single
visual input. Central to our approach is the novel use of a collision-avoiding
diffusion kernel for training. Through evaluations against behavior-cloning and
classical diffusion models, our framework has proven its robustness. It is
particularly effective in multi-modal environments, navigating toward goals and
avoiding unreachable ones blocked by obstacles, while ensuring collision
avoidance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1&quot;&gt;Junwoo Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_H/0/1/0/all/0/1&quot;&gt;Hyunwoo Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jiwoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1&quot;&gt;Soochul Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Joohwan Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakash_N/0/1/0/all/0/1&quot;&gt;Nikhil Prakash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jongeun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horowitz_R/0/1/0/all/0/1&quot;&gt;Roberto Horowitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12612">
<title>How a student becomes a teacher: learning and forgetting through Spectral methods. (arXiv:2310.12612v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12612</link>
<description rdf:parseType="Literal">&lt;p&gt;In theoretical ML, the teacher-student paradigm is often employed as an
effective metaphor for real-life tuition. The above scheme proves particularly
relevant when the student network is overparameterized as compared to the
teacher network. Under these operating conditions, it is tempting to speculate
that the student ability to handle the given task could be eventually stored in
a sub-portion of the whole network. This latter should be to some extent
reminiscent of the frozen teacher structure, according to suitable metrics,
while being approximately invariant across different architectures of the
student candidate network. Unfortunately, state-of-the-art conventional
learning techniques could not help in identifying the existence of such an
invariant subnetwork, due to the inherent degree of non-convexity that
characterizes the examined problem. In this work, we take a leap forward by
proposing a radically different optimization scheme which builds on a spectral
representation of the linear transfer of information between layers. The
gradient is hence calculated with respect to both eigenvalues and eigenvectors
with negligible increase in terms of computational and complexity load, as
compared to standard training algorithms. Working in this framework, we could
isolate a stable student substructure, that mirrors the true complexity of the
teacher in terms of computing neurons, path distribution and topological
attributes. When pruning unimportant nodes of the trained student, as follows a
ranking that reflects the optimized eigenvalues, no degradation in the recorded
performance is seen above a threshold that corresponds to the effective teacher
size. The observed behavior can be pictured as a genuine second-order phase
transition that bears universality traits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giambagli_L/0/1/0/all/0/1&quot;&gt;Lorenzo Giambagli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buffoni_L/0/1/0/all/0/1&quot;&gt;Lorenzo Buffoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chicchi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Chicchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fanelli_D/0/1/0/all/0/1&quot;&gt;Duccio Fanelli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12629">
<title>An Improved Metarounding Algorithm via Frank-Wolfe. (arXiv:2310.12629v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2310.12629</link>
<description rdf:parseType="Literal">&lt;p&gt;Metarounding is an approach to convert an approximation algorithm for linear
optimization over some combinatorial classes to an online linear optimization
algorithm for the same class. We propose a new metarounding algorithm under a
natural assumption that a relax-based approximation algorithm exists for the
combinatorial class. Our algorithm is much more efficient in both theoretical
and practical aspects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitsuboshi_R/0/1/0/all/0/1&quot;&gt;Ryotaro Mitsuboshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatano_K/0/1/0/all/0/1&quot;&gt;Kohei Hatano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takimoto_E/0/1/0/all/0/1&quot;&gt;Eiji Takimoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12631">
<title>Inverse Renormalization Group of Disordered Systems. (arXiv:2310.12631v1 [cond-mat.stat-mech])</title>
<link>http://arxiv.org/abs/2310.12631</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose inverse renormalization group transformations to construct
approximate configurations for lattice volumes that have not yet been accessed
by supercomputers or large-scale simulations in the study of spin glasses.
Specifically, starting from lattices of volume $V=8^{3}$ in the case of the
three-dimensional Edwards-Anderson model we employ machine learning algorithms
to construct rescaled lattices up to $V&apos;=128^{3}$, which we utilize to extract
two critical exponents. We conclude by discussing how to incorporate numerical
exactness within inverse renormalization group approaches of disordered
systems, thus opening up the opportunity to explore a sustainable and
energy-efficient generation of exact configurations for increasing lattice
volumes without the use of dedicated supercomputers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Bachtis_D/0/1/0/all/0/1&quot;&gt;Dimitrios Bachtis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12632">
<title>Towards a Deep Learning-based Online Quality Prediction System for Welding Processes. (arXiv:2310.12632v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12632</link>
<description rdf:parseType="Literal">&lt;p&gt;The digitization of manufacturing processes enables promising applications
for machine learning-assisted quality assurance. A widely used manufacturing
process that can strongly benefit from data-driven solutions is \ac{GMAW}. The
welding process is characterized by complex cause-effect relationships between
material properties, process conditions and weld quality. In non-laboratory
environments with frequently changing process parameters, accurate
determination of weld quality by destructive testing is economically
unfeasible. Deep learning offers the potential to identify the relationships in
available process data and predict the weld quality from process observations.
In this paper, we present a concept for a deep learning based predictive
quality system in \ac{GMAW}. At its core, the concept involves a pipeline
consisting of four major phases: collection and management of multi-sensor data
(e.g. current and voltage), real-time processing and feature engineering of the
time series data by means of autoencoders, training and deployment of suitable
recurrent deep learning models for quality predictions, and model evolutions
under changing process conditions using continual learning. The concept
provides the foundation for future research activities in which we will realize
an online predictive quality system for running production.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hahn_Y/0/1/0/all/0/1&quot;&gt;Yannik Hahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maack_R/0/1/0/all/0/1&quot;&gt;Robert Maack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buchholz_G/0/1/0/all/0/1&quot;&gt;Guido Buchholz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purrio_M/0/1/0/all/0/1&quot;&gt;Marion Purrio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Angerhausen_M/0/1/0/all/0/1&quot;&gt;Matthias Angerhausen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tercan_H/0/1/0/all/0/1&quot;&gt;Hasan Tercan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meisen_T/0/1/0/all/0/1&quot;&gt;Tobias Meisen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12660">
<title>Gradient Descent Fails to Learn High-frequency Functions and Modular Arithmetic. (arXiv:2310.12660v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12660</link>
<description rdf:parseType="Literal">&lt;p&gt;Classes of target functions containing a large number of approximately
orthogonal elements are known to be hard to learn by the Statistical Query
algorithms. Recently this classical fact re-emerged in a theory of
gradient-based optimization of neural networks. In the novel framework, the
hardness of a class is usually quantified by the variance of the gradient with
respect to a random choice of a target function.
&lt;/p&gt;
&lt;p&gt;A set of functions of the form $x\to ax \bmod p$, where $a$ is taken from
${\mathbb Z}_p$, has attracted some attention from deep learning theorists and
cryptographers recently. This class can be understood as a subset of
$p$-periodic functions on ${\mathbb Z}$ and is tightly connected with a class
of high-frequency periodic functions on the real line.
&lt;/p&gt;
&lt;p&gt;We present a mathematical analysis of limitations and challenges associated
with using gradient-based learning techniques to train a high-frequency
periodic function or modular multiplication from examples. We highlight that
the variance of the gradient is negligibly small in both cases when either a
frequency or the prime base $p$ is large. This in turn prevents such a learning
algorithm from being successful.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takhanov_R/0/1/0/all/0/1&quot;&gt;Rustem Takhanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tezekbayev_M/0/1/0/all/0/1&quot;&gt;Maxat Tezekbayev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pak_A/0/1/0/all/0/1&quot;&gt;Artur Pak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bolatov_A/0/1/0/all/0/1&quot;&gt;Arman Bolatov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assylbekov_Z/0/1/0/all/0/1&quot;&gt;Zhenisbek Assylbekov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12663">
<title>Knowledge from Uncertainty in Evidential Deep Learning. (arXiv:2310.12663v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12663</link>
<description rdf:parseType="Literal">&lt;p&gt;This work reveals an evidential signal that emerges from the uncertainty
value in Evidential Deep Learning (EDL). EDL is one example of a class of
uncertainty-aware deep learning approaches designed to provide confidence (or
epistemic uncertainty) about the current test sample. In particular for
computer vision and bidirectional encoder large language models, the
`evidential signal&apos; arising from the Dirichlet strength in EDL can, in some
cases, discriminate between classes, which is particularly strong when using
large language models. We hypothesise that the KL regularisation term causes
EDL to couple aleatoric and epistemic uncertainty. In this paper, we
empirically investigate the correlations between misclassification and
evaluated uncertainty, and show that EDL&apos;s `evidential signal&apos; is due to
misclassification bias. We critically evaluate EDL with other Dirichlet-based
approaches, namely Generative Evidential Neural Networks (EDL-GEN) and Prior
Networks, and show theoretically and empirically the differences between these
loss functions. We conclude that EDL&apos;s coupling of uncertainty arises from
these differences due to the use (or lack) of out-of-distribution samples
during training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davies_C/0/1/0/all/0/1&quot;&gt;Cai Davies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vilamala_M/0/1/0/all/0/1&quot;&gt;Marc Roig Vilamala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preece_A/0/1/0/all/0/1&quot;&gt;Alun D. Preece&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cerutti_F/0/1/0/all/0/1&quot;&gt;Federico Cerutti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaplan_L/0/1/0/all/0/1&quot;&gt;Lance M. Kaplan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1&quot;&gt;Supriyo Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12667">
<title>STANLEY: Stochastic Gradient Anisotropic Langevin Dynamics for Learning Energy-Based Models. (arXiv:2310.12667v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2310.12667</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose in this paper, STANLEY, a STochastic gradient ANisotropic LangEvin
dYnamics, for sampling high dimensional data. With the growing efficacy and
potential of Energy-Based modeling, also known as non-normalized probabilistic
modeling, for modeling a generative process of different natures of high
dimensional data observations, we present an end-to-end learning algorithm for
Energy-Based models (EBM) with the purpose of improving the quality of the
resulting sampled data points. While the unknown normalizing constant of EBMs
makes the training procedure intractable, resorting to Markov Chain Monte Carlo
(MCMC) is in general a viable option. Realizing what MCMC entails for the EBM
training, we propose in this paper, a novel high dimensional sampling method,
based on an anisotropic stepsize and a gradient-informed covariance matrix,
embedded into a discretized Langevin diffusion. We motivate the necessity for
an anisotropic update of the negative samples in the Markov Chain by the
nonlinearity of the backbone of the EBM, here a Convolutional Neural Network.
Our resulting method, namely STANLEY, is an optimization algorithm for training
Energy-Based models via our newly introduced MCMC method. We provide a
theoretical understanding of our sampling scheme by proving that the sampler
leads to a geometrically uniformly ergodic Markov Chain. Several image
generation experiments are provided in our paper to show the effectiveness of
our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karimi_B/0/1/0/all/0/1&quot;&gt;Belhal Karimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jianwen Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Ping Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12671">
<title>Neural networks for insurance pricing with frequency and severity data: a benchmark study from data preprocessing to technical tariff. (arXiv:2310.12671v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12671</link>
<description rdf:parseType="Literal">&lt;p&gt;Insurers usually turn to generalized linear models for modelling claim
frequency and severity data. Due to their success in other fields, machine
learning techniques are gaining popularity within the actuarial toolbox. Our
paper contributes to the literature on frequency-severity insurance pricing
with machine learning via deep learning structures. We present a benchmark
study on four insurance data sets with frequency and severity targets in the
presence of multiple types of input features. We compare in detail the
performance of: a generalized linear model on binned input data, a
gradient-boosted tree model, a feed-forward neural network (FFNN), and the
combined actuarial neural network (CANN). Our CANNs combine a baseline
prediction established with a GLM and GBM, respectively, with a neural network
correction. We explain the data preprocessing steps with specific focus on the
multiple types of input features typically present in tabular insurance data
sets, such as postal codes, numeric and categorical covariates. Autoencoders
are used to embed the categorical variables into the neural network and we
explore their potential advantages in a frequency-severity setting. Finally, we
construct global surrogate models for the neural nets&apos; frequency and severity
models. These surrogates enable the translation of the essential insights
captured by the FFNNs or CANNs to GLMs. As such, a technical tariff table
results that can easily be deployed in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holvoet_F/0/1/0/all/0/1&quot;&gt;Freek Holvoet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antonio_K/0/1/0/all/0/1&quot;&gt;Katrien Antonio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henckaerts_R/0/1/0/all/0/1&quot;&gt;Roel Henckaerts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12680">
<title>On the Optimization and Generalization of Multi-head Attention. (arXiv:2310.12680v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12680</link>
<description rdf:parseType="Literal">&lt;p&gt;The training and generalization dynamics of the Transformer&apos;s core mechanism,
namely the Attention mechanism, remain under-explored. Besides, existing
analyses primarily focus on single-head attention. Inspired by the demonstrated
benefits of overparameterization when training fully-connected networks, we
investigate the potential optimization and generalization advantages of using
multiple attention heads. Towards this goal, we derive convergence and
generalization guarantees for gradient-descent training of a single-layer
multi-head self-attention model, under a suitable realizability condition on
the data. We then establish primitive conditions on the initialization that
ensure realizability holds. Finally, we demonstrate that these conditions are
satisfied for a simple tokenized-mixture model. We expect the analysis can be
extended to various data-model and architecture variations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deora_P/0/1/0/all/0/1&quot;&gt;Puneesh Deora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghaderi_R/0/1/0/all/0/1&quot;&gt;Rouzbeh Ghaderi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taheri_H/0/1/0/all/0/1&quot;&gt;Hossein Taheri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thrampoulidis_C/0/1/0/all/0/1&quot;&gt;Christos Thrampoulidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12688">
<title>Compression of Recurrent Neural Networks using Matrix Factorization. (arXiv:2310.12688v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12688</link>
<description rdf:parseType="Literal">&lt;p&gt;Compressing neural networks is a key step when deploying models for real-time
or embedded applications. Factorizing the model&apos;s matrices using low-rank
approximations is a promising method for achieving compression. While it is
possible to set the rank before training, this approach is neither flexible nor
optimal. In this work, we propose a post-training rank-selection method called
Rank-Tuning that selects a different rank for each matrix. Used in combination
with training adaptations, our method achieves high compression rates with no
or little performance degradation. Our numerical experiments on signal
processing tasks show that we can compress recurrent neural networks up to 14x
with at most 1.4% relative performance reduction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maison_L/0/1/0/all/0/1&quot;&gt;Lucas Maison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bourboux_H/0/1/0/all/0/1&quot;&gt;H&amp;#xe9;lion du Mas des Bourboux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courtat_T/0/1/0/all/0/1&quot;&gt;Thomas Courtat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12690">
<title>Neurosymbolic Grounding for Compositional World Models. (arXiv:2310.12690v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12690</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Cosmos, a framework for object-centric world modeling that is
designed for compositional generalization (CG), i.e., high performance on
unseen input scenes obtained through the composition of known visual &quot;atoms.&quot;
The central insight behind Cosmos is the use of a novel form of neurosymbolic
grounding. Specifically, the framework introduces two new tools: (i)
neurosymbolic scene encodings, which represent each entity in a scene using a
real vector computed using a neural encoder, as well as a vector of composable
symbols describing attributes of the entity, and (ii) a neurosymbolic attention
mechanism that binds these entities to learned rules of interaction. Cosmos is
end-to-end differentiable; also, unlike traditional neurosymbolic methods that
require representations to be manually mapped to symbols, it computes an
entity&apos;s symbolic attributes using vision-language foundation models. Through
an evaluation that considers two different forms of CG on an established
blocks-pushing domain, we show that the framework establishes a new
state-of-the-art for CG in world modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sehgal_A/0/1/0/all/0/1&quot;&gt;Atharva Sehgal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grayeli_A/0/1/0/all/0/1&quot;&gt;Arya Grayeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jennifer J. Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1&quot;&gt;Swarat Chaudhuri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12713">
<title>Learn from the Past: A Proxy based Adversarial Defense Framework to Boost Robustness. (arXiv:2310.12713v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12713</link>
<description rdf:parseType="Literal">&lt;p&gt;In light of the vulnerability of deep learning models to adversarial samples
and the ensuing security issues, a range of methods, including Adversarial
Training (AT) as a prominent representative, aimed at enhancing model
robustness against various adversarial attacks, have seen rapid development.
However, existing methods essentially assist the current state of target model
to defend against parameter-oriented adversarial attacks with explicit or
implicit computation burdens, which also suffers from unstable convergence
behavior due to inconsistency of optimization trajectories. Diverging from
previous work, this paper reconsiders the update rule of target model and
corresponding deficiency to defend based on its current state. By introducing
the historical state of the target model as a proxy, which is endowed with much
prior information for defense, we formulate a two-stage update rule, resulting
in a general adversarial defense framework, which we refer to as `LAST&apos; ({\bf
L}earn from the P{\bf ast}). Besides, we devise a Self Distillation (SD) based
defense objective to constrain the update process of the proxy model without
the introduction of larger teacher models. Experimentally, we demonstrate
consistent and significant performance enhancements by refining a series of
single-step and multi-step AT methods (e.g., up to $\bf 9.2\%$ and $\bf 20.5\%$
improvement of Robust Accuracy (RA) on CIFAR10 and CIFAR100 datasets,
respectively) across various datasets, backbones and attack modalities, and
validate its ability to enhance training stability and ameliorate catastrophic
overfitting issues meanwhile.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yaohua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jiaxin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_X/0/1/0/all/0/1&quot;&gt;Xianghao Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Risheng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12743">
<title>Canonical normalizing flows for manifold learning. (arXiv:2310.12743v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2310.12743</link>
<description rdf:parseType="Literal">&lt;p&gt;Manifold learning flows are a class of generative modelling techniques that
assume a low-dimensional manifold description of the data. The embedding of
such manifold into the high-dimensional space of the data is achieved via
learnable invertible transformations. Therefore, once the manifold is properly
aligned via a reconstruction loss, the probability density is tractable on the
manifold and maximum likelihood can be used optimize the network parameters.
Naturally, the lower-dimensional representation of the data requires an
injective-mapping. Recent approaches were able to enforce that density aligns
with the modelled manifold, while efficiently calculating the density
volume-change term when embedding to the higher-dimensional space. However,
unless the injective-mapping is analytically predefined, the learned manifold
is not necessarily an efficient representation of the data. Namely, the latent
dimensions of such models frequently learn an entangled intrinsic basis with
degenerate information being stored in each dimension. Alternatively, if a
locally orthogonal and/or sparse basis is to be learned, here coined canonical
intrinsic basis, it can serve in learning a more compact latent space
representation. Towards this end, we propose a canonical manifold learning flow
method, where a novel optimization objective enforces the transformation matrix
to have few prominent and orthogonal basis functions. Canonical manifold flow
yields a more efficient use of the latent space, automatically generating fewer
prominent and distinct dimensions to represent data, and consequently a better
approximation of target distributions than other manifold flow methods in most
experiments we conducted, resulting in lower FID scores.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Flouris_K/0/1/0/all/0/1&quot;&gt;Kyriakos Flouris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Konukoglu_E/0/1/0/all/0/1&quot;&gt;Ender Konukoglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12746">
<title>TabuLa: Harnessing Language Models for Tabular Data Synthesis. (arXiv:2310.12746v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12746</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the ubiquitous use of tabular data in industries and the growing
concerns in data privacy and security, tabular data synthesis emerges as a
critical research area. The recent state-of-the-art methods show that large
language models (LLMs) can be adopted to generate realistic tabular data. As
LLMs pre-process tabular data as full text, they have the advantage of avoiding
the curse of dimensionality associated with one-hot encoding high-dimensional
data. However, their long training time and limited re-usability on new tasks
prevent them from replacing exiting tabular generative models. In this paper,
we propose Tabula, a tabular data synthesizer based on the language model
structure. Through Tabula, we demonstrate the inherent limitation of employing
pre-trained language models designed for natural language processing (NLP) in
the context of tabular data synthesis. Our investigation delves into the
development of a dedicated foundational model tailored specifically for tabular
data synthesis. Additionally, we propose a token sequence compression strategy
to significantly reduce training time while preserving the quality of synthetic
data. Extensive experiments on six datasets demonstrate that using a language
model structure without loading the well-trained model weights yields a better
starting model for tabular data synthesis. Moreover, the Tabula model,
previously trained on other tabular data, serves as an excellent foundation
model for new tabular data synthesis tasks. Additionally, the token sequence
compression method substantially reduces the model&apos;s training time. Results
show that Tabula averagely reduces 46.2% training time per epoch comparing to
current LLMs-based state-of-the-art algorithm and consistently achieves even
higher synthetic data utility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zilong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Birke_R/0/1/0/all/0/1&quot;&gt;Robert Birke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lydia Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12752">
<title>Discretize Relaxed Solution of Spectral Clustering via a Non-Heuristic Algorithm. (arXiv:2310.12752v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12752</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral clustering and its extensions usually consist of two steps: (1)
constructing a graph and computing the relaxed solution; (2) discretizing
relaxed solutions. Although the former has been extensively investigated, the
discretization techniques are mainly heuristic methods, e.g., k-means, spectral
rotation. Unfortunately, the goal of the existing methods is not to find a
discrete solution that minimizes the original objective. In other words, the
primary drawback is the neglect of the original objective when computing the
discrete solution. Inspired by the first-order optimization algorithms, we
propose to develop a first-order term to bridge the original problem and
discretization algorithm, which is the first non-heuristic to the best of our
knowledge. Since the non-heuristic method is aware of the original graph cut
problem, the final discrete solution is more reliable and achieves the
preferable loss value. We also theoretically show that the continuous optimum
is beneficial to discretization algorithms though simply finding its closest
discrete solution is an existing heuristic algorithm which is also unreliable.
Sufficient experiments significantly show the superiority of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuelong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12765">
<title>Energy-Based Models For Speech Synthesis. (arXiv:2310.12765v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2310.12765</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently there has been a lot of interest in non-autoregressive (non-AR)
models for speech synthesis, such as FastSpeech 2 and diffusion models. Unlike
AR models, these models do not have autoregressive dependencies among outputs
which makes inference efficient. This paper expands the range of available
non-AR models with another member called energy-based models (EBMs). The paper
describes how noise contrastive estimation, which relies on the comparison
between positive and negative samples, can be used to train EBMs. It proposes a
number of strategies for generating effective negative samples, including using
high-performing AR models. It also describes how sampling from EBMs can be
performed using Langevin Markov Chain Monte-Carlo (MCMC). The use of Langevin
MCMC enables to draw connections between EBMs and currently popular diffusion
models. Experiments on LJSpeech dataset show that the proposed approach offers
improvements over Tacotron 2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wanli Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zehai Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ragni_A/0/1/0/all/0/1&quot;&gt;Anton Ragni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12766">
<title>Transformer-based Entity Legal Form Classification. (arXiv:2310.12766v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12766</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the application of Transformer-based language models for
classifying entity legal forms from raw legal entity names. Specifically, we
employ various BERT variants and compare their performance against multiple
traditional baselines. Our evaluation encompasses a substantial subset of
freely available Legal Entity Identifier (LEI) data, comprising over 1.1
million legal entities from 30 different legal jurisdictions. The ground truth
labels for classification per jurisdiction are taken from the Entity Legal Form
(ELF) code standard (ISO 20275). Our findings demonstrate that pre-trained BERT
variants outperform traditional text classification approaches in terms of F1
score, while also performing comparably well in the Macro F1 Score. Moreover,
the validity of our proposal is supported by the outcome of third-party expert
reviews conducted in ten selected jurisdictions. This study highlights the
significant potential of Transformer-based models in advancing data
standardization and data integration. The presented approaches can greatly
benefit financial institutions, corporations, governments and other
organizations in assessing business relationships, understanding risk exposure,
and promoting effective governance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arimond_A/0/1/0/all/0/1&quot;&gt;Alexander Arimond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molteni_M/0/1/0/all/0/1&quot;&gt;Mauro Molteni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jany_D/0/1/0/all/0/1&quot;&gt;Dominik Jany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manolova_Z/0/1/0/all/0/1&quot;&gt;Zornitsa Manolova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borth_D/0/1/0/all/0/1&quot;&gt;Damian Borth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoepner_A/0/1/0/all/0/1&quot;&gt;Andreas G.F. Hoepner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12768">
<title>SemantIC: Semantic Interference Cancellation Towards 6G Wireless Communications. (arXiv:2310.12768v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2310.12768</link>
<description rdf:parseType="Literal">&lt;p&gt;This letter proposes a novel anti-interference technique, semantic
interference cancellation (SemantIC), for enhancing information quality towards
the sixth-generation (6G) wireless networks. SemantIC only requires the
receiver to concatenate the channel decoder with a semantic auto-encoder. This
constructs a turbo loop which iteratively and alternately eliminates noise in
the signal domain and the semantic domain. From the viewpoint of network
information theory, the neural network of the semantic auto-encoder stores side
information by training, and provides side information in iterative decoding,
as an implementation of the Wyner-Ziv theorem. Simulation results verify the
performance improvement by SemantIC without extra channel resource cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wensheng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yuna Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lixin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Matsumoto_T/0/1/0/all/0/1&quot;&gt;Tad Matsumoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12771">
<title>Stochastic Average Gradient : A Simple Empirical Investigation. (arXiv:2310.12771v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12771</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the recent growth of theoretical studies and empirical successes of
neural networks, gradient backpropagation is still the most widely used
algorithm for training such networks. On the one hand, we have deterministic or
full gradient (FG) approaches that have a cost proportional to the amount of
training data used but have a linear convergence rate, and on the other hand,
stochastic gradient (SG) methods that have a cost independent of the size of
the dataset, but have a less optimal convergence rate than the determinist
approaches. To combine the cost of the stochastic approach with the convergence
rate of the deterministic approach, a stochastic average gradient (SAG) has
been proposed. SAG is a method for optimizing the sum of a finite number of
smooth convex functions. Like SG methods, the SAG method&apos;s iteration cost is
independent of the number of terms in the sum. In this work, we propose to
compare SAG to some standard optimizers used in machine learning. SAG converges
faster than other optimizers on simple toy problems and performs better than
many other optimizers on simple machine learning problems. We also propose a
combination of SAG with the momentum algorithm and Adam. These combinations
allow empirically higher speed and obtain better performance than the other
methods, especially when the landscape of the function to optimize presents
obstacles or is ill-conditioned.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Notsawo_P/0/1/0/all/0/1&quot;&gt;Pascal Junior Tikeng Notsawo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12773">
<title>Safe RLHF: Safe Reinforcement Learning from Human Feedback. (arXiv:2310.12773v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.12773</link>
<description rdf:parseType="Literal">&lt;p&gt;With the development of large language models (LLMs), striking a balance
between the performance and safety of AI systems has never been more critical.
However, the inherent tension between the objectives of helpfulness and
harmlessness presents a significant challenge during LLM training. To address
this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe
RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly
decouples human preferences regarding helpfulness and harmlessness, effectively
avoiding the crowdworkers&apos; confusion about the tension and allowing us to train
separate reward and cost models. We formalize the safety concern of LLMs as an
optimization task of maximizing the reward function while satisfying specified
cost constraints. Leveraging the Lagrangian method to solve this constrained
problem, Safe RLHF dynamically adjusts the balance between the two objectives
during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we
demonstrate a superior ability to mitigate harmful responses while enhancing
model performance compared to existing value-aligned algorithms.
Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with
collected human preferences, significantly improving its helpfulness and
harmlessness according to human evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Josef Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xuehai Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1&quot;&gt;Ruiyang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jiaming Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xinbo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mickel Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhou Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yaodong Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12774">
<title>Survival of the Most Influential Prompts: Efficient Black-Box Prompt Search via Clustering and Pruning. (arXiv:2310.12774v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12774</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt-based learning has been an effective paradigm for large pretrained
language models (LLM), enabling few-shot or even zero-shot learning. Black-box
prompt search has received growing interest recently for its distinctive
properties of gradient-free optimization, proven particularly useful and
powerful for model-as-a-service usage. However, the discrete nature and the
complexity of combinatorial optimization hinder the efficiency of modern
black-box approaches. Despite extensive research on search algorithms, the
crucial aspect of search space design and optimization has been largely
overlooked. In this paper, we first conduct a sensitivity analysis by prompting
LLM, revealing that only a small number of tokens exert a disproportionate
amount of influence on LLM predictions. Leveraging this insight, we propose the
Clustering and Pruning for Efficient Black-box Prompt Search (ClaPS), a simple
black-box search method that first clusters and prunes the search space to
focus exclusively on influential prompt tokens. By employing even simple search
methods within the pruned search space, ClaPS achieves state-of-the-art
performance across various tasks and LLMs, surpassing the performance of
complex approaches while significantly reducing search costs. Our findings
underscore the critical role of search space design and optimization in
enhancing both the usefulness and the efficiency of black-box prompt-based
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Han Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xingchen Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1&quot;&gt;Ivan Vuli&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1&quot;&gt;Anna Korhonen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12778">
<title>Label-Aware Automatic Verbalizer for Few-Shot Text Classification. (arXiv:2310.12778v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12778</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt-based learning has shown its effectiveness in few-shot text
classification. One important factor in its success is a verbalizer, which
translates output from a language model into a predicted class. Notably, the
simplest and widely acknowledged verbalizer employs manual labels to represent
the classes. However, manual selection does not guarantee the optimality of the
selected words when conditioned on the chosen language model. Therefore, we
propose Label-Aware Automatic Verbalizer (LAAV), effectively augmenting the
manual labels to achieve better few-shot classification results. Specifically,
we use the manual labels along with the conjunction &quot;and&quot; to induce the model
to generate more effective words for the verbalizer. The experimental results
on five datasets across five languages demonstrate that LAAV significantly
outperforms existing verbalizers. Furthermore, our analysis reveals that LAAV
suggests more relevant words compared to similar approaches, especially in
mid-to-low resource languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thaminkaew_T/0/1/0/all/0/1&quot;&gt;Thanakorn Thaminkaew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lertvittayakumjorn_P/0/1/0/all/0/1&quot;&gt;Piyawat Lertvittayakumjorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vateekul_P/0/1/0/all/0/1&quot;&gt;Peerapon Vateekul&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12781">
<title>Conditional Density Estimations from Privacy-Protected Data. (arXiv:2310.12781v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2310.12781</link>
<description rdf:parseType="Literal">&lt;p&gt;Many modern statistical analysis and machine learning applications require
training models on sensitive user data. Differential privacy provides a formal
guarantee that individual-level information about users does not leak. In this
framework, randomized algorithms inject calibrated noise into the confidential
data, resulting in privacy-protected datasets or queries. However, restricting
access to only the privatized data during statistical analysis makes it
computationally challenging to perform valid inferences on parameters
underlying the confidential data. In this work, we propose simulation-based
inference methods from privacy-protected datasets. Specifically, we use neural
conditional density estimators as a flexible family of distributions to
approximate the posterior distribution of model parameters given the observed
private query results. We illustrate our methods on discrete time-series data
under an infectious disease model and on ordinary linear regression models.
Illustrating the privacy-utility trade-off, our experiments and analysis
demonstrate the necessity and feasibility of designing valid statistical
inference procedures to correct for biases introduced by the privacy-protection
mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xiong_X/0/1/0/all/0/1&quot;&gt;Xiaofei Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ju_N/0/1/0/all/0/1&quot;&gt;Nianqiao P. Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sanguo Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12785">
<title>A Theoretical Approach to Characterize the Accuracy-Fairness Trade-off Pareto Frontier. (arXiv:2310.12785v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12785</link>
<description rdf:parseType="Literal">&lt;p&gt;While the accuracy-fairness trade-off has been frequently observed in the
literature of fair machine learning, rigorous theoretical analyses have been
scarce. To demystify this long-standing challenge, this work seeks to develop a
theoretical framework by characterizing the shape of the accuracy-fairness
trade-off Pareto frontier (FairFrontier), determined by a set of all optimal
Pareto classifiers that no other classifiers can dominate. Specifically, we
first demonstrate the existence of the trade-off in real-world scenarios and
then propose four potential categories to characterize the important properties
of the accuracy-fairness Pareto frontier. For each category, we identify the
necessary conditions that lead to corresponding trade-offs. Experimental
results on synthetic data suggest insightful findings of the proposed
framework: (1) When sensitive attributes can be fully interpreted by
non-sensitive attributes, FairFrontier is mostly continuous. (2) Accuracy can
suffer a \textit{sharp} decline when over-pursuing fairness. (3) Eliminate the
trade-off via a two-step streamlined approach. The proposed research enables an
in-depth understanding of the accuracy-fairness trade-off, pushing current fair
machine-learning research to a new frontier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hua Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ninghao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Mengnan Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12793">
<title>OODRobustBench: benchmarking and analyzing adversarial robustness under distribution shift. (arXiv:2310.12793v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12793</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing works have made great progress in improving adversarial robustness,
but typically test their method only on data from the same distribution as the
training data, i.e. in-distribution (ID) testing. As a result, it is unclear
how such robustness generalizes under input distribution shifts, i.e.
out-of-distribution (OOD) testing. This is a concerning omission as such
distribution shifts are unavoidable when methods are deployed in the wild. To
address this issue we propose a benchmark named OODRobustBench to
comprehensively assess OOD adversarial robustness using 23 dataset-wise shifts
(i.e. naturalistic shifts in input distribution) and 6 threat-wise shifts
(i.e., unforeseen adversarial threat models). OODRobustBench is used to assess
706 robust models using 60.7K adversarial evaluations. This large-scale
analysis shows that: 1) adversarial robustness suffers from a severe OOD
generalization issue; 2) ID robustness correlates strongly with OOD robustness,
in a positive linear way, under many distribution shifts. The latter enables
the prediction of OOD robustness from ID robustness. Based on this, we are able
to predict the upper limit of OOD robustness for existing robust training
schemes. The results suggest that achieving OOD robustness requires designing
novel methods beyond the conventional ones. Last, we discover that extra data,
data augmentation, advanced model architectures and particular regularization
approaches can improve OOD robustness. Noticeably, the discovered training
schemes, compared to the baseline, exhibit dramatically higher robustness under
threat shift while keeping high ID robustness, demonstrating new promising
solutions for robustness against both multi-attack and unforeseen attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yifei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sitawarin_C/0/1/0/all/0/1&quot;&gt;Chawin Sitawarin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spratling_M/0/1/0/all/0/1&quot;&gt;Michael Spratling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12800">
<title>Exploring Graph Neural Networks for Indian Legal Judgment Prediction. (arXiv:2310.12800v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12800</link>
<description rdf:parseType="Literal">&lt;p&gt;The burdensome impact of a skewed judges-to-cases ratio on the judicial
system manifests in an overwhelming backlog of pending cases alongside an
ongoing influx of new ones. To tackle this issue and expedite the judicial
process, the proposition of an automated system capable of suggesting case
outcomes based on factual evidence and precedent from past cases gains
significance. This research paper centres on developing a graph neural
network-based model to address the Legal Judgment Prediction (LJP) problem,
recognizing the intrinsic graph structure of judicial cases and making it a
binary node classification problem. We explored various embeddings as model
features, while nodes such as time nodes and judicial acts were added and
pruned to evaluate the model&apos;s performance. The study is done while considering
the ethical dimension of fairness in these predictions, considering gender and
name biases. A link prediction task is also conducted to assess the model&apos;s
proficiency in anticipating connections between two specified nodes. By
harnessing the capabilities of graph neural networks and incorporating fairness
analyses, this research aims to contribute insights towards streamlining the
adjudication process, enhancing judicial efficiency, and fostering a more
equitable legal landscape, ultimately alleviating the strain imposed by
mounting case backlogs. Our best-performing model with XLNet pre-trained
embeddings as its features gives the macro F1 score of 75% for the LJP task.
For link prediction, the same set of features is the best performing giving ROC
of more than 80%
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khatri_M/0/1/0/all/0/1&quot;&gt;Mann Khatri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yusuf_M/0/1/0/all/0/1&quot;&gt;Mirza Yusuf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_Y/0/1/0/all/0/1&quot;&gt;Yaman Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Rajiv Ratn Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1&quot;&gt;Ponnurangam Kumaraguru&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12802">
<title>An effective theory of collective deep learning. (arXiv:2310.12802v1 [physics.soc-ph])</title>
<link>http://arxiv.org/abs/2310.12802</link>
<description rdf:parseType="Literal">&lt;p&gt;Unraveling the emergence of collective learning in systems of coupled
artificial neural networks is an endeavor with broader implications for
physics, machine learning, neuroscience and society. Here we introduce a
minimal model that condenses several recent decentralized algorithms by
considering a competition between two terms: the local learning dynamics in the
parameters of each neural network unit, and a diffusive coupling among units
that tends to homogenize the parameters of the ensemble. We derive the
coarse-grained behavior of our model via an effective theory for linear
networks that we show is analogous to a deformed Ginzburg-Landau model with
quenched disorder. This framework predicts (depth-dependent)
disorder-order-disorder phase transitions in the parameters&apos; solutions that
reveal the onset of a collective learning phase, along with a depth-induced
delay of the critical point and a robust shape of the microscopic learning
path. We validate our theory in realistic ensembles of coupled nonlinear
networks trained in the MNIST dataset under privacy constraints. Interestingly,
experiments confirm that individual networks -- trained only with private data
-- can fully generalize to unseen data classes when the collective learning
phase emerges. Our work elucidates the physics of collective learning and
contributes to the mechanistic interpretability of deep learning in
decentralized settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Arola_Fernandez_L/0/1/0/all/0/1&quot;&gt;Llu&amp;#xed;s Arola-Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lacasa_L/0/1/0/all/0/1&quot;&gt;Lucas Lacasa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12803">
<title>Causal-structure Driven Augmentations for Text OOD Generalization. (arXiv:2310.12803v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12803</link>
<description rdf:parseType="Literal">&lt;p&gt;The reliance of text classifiers on spurious correlations can lead to poor
generalization at deployment, raising concerns about their use in
safety-critical domains such as healthcare. In this work, we propose to use
counterfactual data augmentation, guided by knowledge of the causal structure
of the data, to simulate interventions on spurious features and to learn more
robust text classifiers. We show that this strategy is appropriate in
prediction problems where the label is spuriously correlated with an attribute.
Under the assumptions of such problems, we discuss the favorable sample
complexity of counterfactual data augmentation, compared to importance
re-weighting. Pragmatically, we match examples using auxiliary data, based on
diff-in-diff methodology, and use a large language model (LLM) to represent a
conditional probability of text. Through extensive experimentation on learning
caregiver-invariant predictors of clinical diagnoses from medical narratives
and on semi-synthetic data, we demonstrate that our method for simulating
interventions improves out-of-distribution (OOD) accuracy compared to baseline
invariant learning algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1&quot;&gt;Amir Feder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wald_Y/0/1/0/all/0/1&quot;&gt;Yoav Wald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1&quot;&gt;Claudia Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saria_S/0/1/0/all/0/1&quot;&gt;Suchi Saria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blei_D/0/1/0/all/0/1&quot;&gt;David Blei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12804">
<title>Differentiable Vertex Fitting for Jet Flavour Tagging. (arXiv:2310.12804v1 [hep-ex])</title>
<link>http://arxiv.org/abs/2310.12804</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a differentiable vertex fitting algorithm that can be used for
secondary vertex fitting, and that can be seamlessly integrated into neural
networks for jet flavour tagging. Vertex fitting is formulated as an
optimization problem where gradients of the optimized solution vertex are
defined through implicit differentiation and can be passed to upstream or
downstream neural network components for network training. More broadly, this
is an application of differentiable programming to integrate physics knowledge
into neural network models in high energy physics. We demonstrate how
differentiable secondary vertex fitting can be integrated into larger
transformer-based models for flavour tagging and improve heavy flavour jet
classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Smith_R/0/1/0/all/0/1&quot;&gt;Rachel E. C. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Ochoa_I/0/1/0/all/0/1&quot;&gt;In&amp;#xea;s Ochoa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Inacio_R/0/1/0/all/0/1&quot;&gt;R&amp;#xfa;ben In&amp;#xe1;cio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Shoemaker_J/0/1/0/all/0/1&quot;&gt;Jonathan Shoemaker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Kagan_M/0/1/0/all/0/1&quot;&gt;Michael Kagan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12805">
<title>Detection and Evaluation of bias-inducing Features in Machine learning. (arXiv:2310.12805v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12805</link>
<description rdf:parseType="Literal">&lt;p&gt;The cause-to-effect analysis can help us decompose all the likely causes of a
problem, such as an undesirable business situation or unintended harm to the
individual(s). This implies that we can identify how the problems are
inherited, rank the causes to help prioritize fixes, simplify a complex problem
and visualize them. In the context of machine learning (ML), one can use
cause-to-effect analysis to understand the reason for the biased behavior of
the system. For example, we can examine the root causes of biases by checking
each feature for a potential cause of bias in the model. To approach this, one
can apply small changes to a given feature or a pair of features in the data,
following some guidelines and observing how it impacts the decision made by the
model (i.e., model prediction). Therefore, we can use cause-to-effect analysis
to identify the potential bias-inducing features, even when these features are
originally are unknown. This is important since most current methods require a
pre-identification of sensitive features for bias assessment and can actually
miss other relevant bias-inducing features, which is why systematic
identification of such features is necessary. Moreover, it often occurs that to
achieve an equitable outcome, one has to take into account sensitive features
in the model decision. Therefore, it should be up to the domain experts to
decide based on their knowledge of the context of a decision whether bias
induced by specific features is acceptable or not. In this study, we propose an
approach for systematically identifying all bias-inducing features of a model
to help support the decision-making of domain experts. We evaluated our
technique using four well-known datasets to showcase how our contribution can
help spearhead the standard procedure when developing, testing, maintaining,
and deploying fair/equitable machine learning systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Openja_M/0/1/0/all/0/1&quot;&gt;Moses Openja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laberge_G/0/1/0/all/0/1&quot;&gt;Gabriel Laberge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1&quot;&gt;Foutse Khomh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12806">
<title>DCSI -- An improved measure of cluster separability based on separation and connectedness. (arXiv:2310.12806v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2310.12806</link>
<description rdf:parseType="Literal">&lt;p&gt;Whether class labels in a given data set correspond to meaningful clusters is
crucial for the evaluation of clustering algorithms using real-world data sets.
This property can be quantified by separability measures. A review of the
existing literature shows that neither classification-based complexity measures
nor cluster validity indices (CVIs) adequately incorporate the central aspects
of separability for density-based clustering: between-class separation and
within-class connectedness. A newly developed measure (density cluster
separability index, DCSI) aims to quantify these two characteristics and can
also be used as a CVI. Extensive experiments on synthetic data indicate that
DCSI correlates strongly with the performance of DBSCAN measured via the
adjusted rand index (ARI) but lacks robustness when it comes to multi-class
data sets with overlapping classes that are ill-suited for density-based hard
clustering. Detailed evaluation on frequently used real-world data sets shows
that DCSI can correctly identify touching or overlapping classes that do not
form meaningful clusters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gauss_J/0/1/0/all/0/1&quot;&gt;Jana Gauss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scheipl_F/0/1/0/all/0/1&quot;&gt;Fabian Scheipl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Herrmann_M/0/1/0/all/0/1&quot;&gt;Moritz Herrmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12808">
<title>Model Merging by Uncertainty-Based Gradient Matching. (arXiv:2310.12808v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12808</link>
<description rdf:parseType="Literal">&lt;p&gt;Models trained on different datasets can be merged by a weighted-averaging of
their parameters, but why does it work and when can it fail? Here, we connect
the inaccuracy of weighted-averaging to mismatches in the gradients and propose
a new uncertainty-based scheme to improve the performance by reducing the
mismatch. The connection also reveals implicit assumptions in other schemes
such as averaging, task arithmetic, and Fisher-weighted averaging. Our new
method gives consistent improvements for large language models and vision
transformers, both in terms of performance and robustness to hyperparameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daheim_N/0/1/0/all/0/1&quot;&gt;Nico Daheim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mollenhoff_T/0/1/0/all/0/1&quot;&gt;Thomas M&amp;#xf6;llenhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1&quot;&gt;Edoardo Maria Ponti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1&quot;&gt;Iryna Gurevych&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Mohammad Emtiyaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12809">
<title>Hierarchical Forecasting at Scale. (arXiv:2310.12809v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12809</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing hierarchical forecasting techniques scale poorly when the number of
time series increases. We propose to learn a coherent forecast for millions of
time series with a single bottom-level forecast model by using a sparse loss
function that directly optimizes the hierarchical product and/or temporal
structure. The benefit of our sparse hierarchical loss function is that it
provides practitioners a method of producing bottom-level forecasts that are
coherent to any chosen cross-sectional or temporal hierarchy. In addition,
removing the need for a post-processing step as required in traditional
hierarchical forecasting techniques reduces the computational cost of the
prediction phase in the forecasting pipeline. On the public M5 dataset, our
sparse hierarchical loss function performs up to 10% (RMSE) better compared to
the baseline loss function. We implement our sparse hierarchical loss function
within an existing forecasting model at bol, a large European e-commerce
platform, resulting in an improved forecasting performance of 2% at the product
level. Finally, we found an increase in forecasting performance of about 5-10%
when evaluating the forecasting performance across the cross-sectional
hierarchies that we defined. These results demonstrate the usefulness of our
sparse hierarchical loss applied to a production forecasting system at a major
e-commerce platform.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sprangers_O/0/1/0/all/0/1&quot;&gt;Olivier Sprangers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wadman_W/0/1/0/all/0/1&quot;&gt;Wander Wadman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schelter_S/0/1/0/all/0/1&quot;&gt;Sebastian Schelter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1&quot;&gt;Maarten de Rijke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12815">
<title>Prompt Injection Attacks and Defenses in LLM-Integrated Applications. (arXiv:2310.12815v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2310.12815</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) are increasingly deployed as the backend for a
variety of real-world applications called LLM-Integrated Applications. Multiple
recent works showed that LLM-Integrated Applications are vulnerable to prompt
injection attacks, in which an attacker injects malicious instruction/data into
the input of those applications such that they produce results as the attacker
desires. However, existing works are limited to case studies. As a result, the
literature lacks a systematic understanding of prompt injection attacks and
their defenses. We aim to bridge the gap in this work. In particular, we
propose a general framework to formalize prompt injection attacks. Existing
attacks, which are discussed in research papers and blog posts, are special
cases in our framework. Our framework enables us to design a new attack by
combining existing attacks. Moreover, we also propose a framework to
systematize defenses against prompt injection attacks. Using our frameworks, we
conduct a systematic evaluation on prompt injection attacks and their defenses
with 10 LLMs and 7 tasks. We hope our frameworks can inspire future research in
this field. Our code is available at
https://github.com/liu00222/Open-Prompt-Injection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yupei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Yuqi Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_R/0/1/0/all/0/1&quot;&gt;Runpeng Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jinyuan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1&quot;&gt;Neil Zhenqiang Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12817">
<title>2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision. (arXiv:2310.12817v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12817</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a Multimodal Interlaced Transformer (MIT) that jointly considers
2D and 3D data for weakly supervised point cloud segmentation. Research studies
have shown that 2D and 3D features are complementary for point cloud
segmentation. However, existing methods require extra 2D annotations to achieve
2D-3D information fusion. Considering the high annotation cost of point clouds,
effective 2D and 3D feature fusion based on weakly supervised learning is in
great demand. To this end, we propose a transformer model with two encoders and
one decoder for weakly supervised point cloud segmentation using only
scene-level class tags. Specifically, the two encoders compute the
self-attended features for 3D point clouds and 2D multi-view images,
respectively. The decoder implements interlaced 2D-3D cross-attention and
carries out implicit 2D and 3D feature fusion. We alternately switch the roles
of queries and key-value pairs in the decoder layers. It turns out that the 2D
and 3D features are iteratively enriched by each other. Experiments show that
it performs favorably against existing weakly supervised point cloud
segmentation methods by a large margin on the S3DIS and ScanNet benchmarks. The
project page will be available at https://jimmy15923.github.io/mit_web/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng-Kun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Min-Hung Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1&quot;&gt;Yung-Yu Chuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yen-Yu Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12818">
<title>Boosting Inference Efficiency: Unleashing the Power of Parameter-Shared Pre-trained Language Models. (arXiv:2310.12818v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12818</link>
<description rdf:parseType="Literal">&lt;p&gt;Parameter-shared pre-trained language models (PLMs) have emerged as a
successful approach in resource-constrained environments, enabling substantial
reductions in model storage and memory costs without significant performance
compromise. However, it is important to note that parameter sharing does not
alleviate computational burdens associated with inference, thus impeding its
practicality in situations characterized by limited stringent latency
requirements or computational resources. Building upon neural ordinary
differential equations (ODEs), we introduce a straightforward technique to
enhance the inference efficiency of parameter-shared PLMs. Additionally, we
propose a simple pre-training technique that leads to fully or partially shared
models capable of achieving even greater inference acceleration. The
experimental results demonstrate the effectiveness of our methods on both
autoregressive and autoencoding PLMs, providing novel insights into more
efficient utilization of parameter-shared models in resource-constrained
settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weize Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaoyue Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yankai Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1&quot;&gt;Ruobing Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Maosong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12819">
<title>Hybrid Search for Efficient Planning with Completeness Guarantees. (arXiv:2310.12819v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.12819</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving complex planning problems has been a long-standing challenge in
computer science. Learning-based subgoal search methods have shown promise in
tackling these problems, but they often suffer from a lack of completeness
guarantees, meaning that they may fail to find a solution even if one exists.
In this paper, we propose an efficient approach to augment a subgoal search
method to achieve completeness in discrete action spaces. Specifically, we
augment the high-level search with low-level actions to execute a multi-level
(hybrid) search, which we call complete subgoal search. This solution achieves
the best of both worlds: the practical efficiency of high-level search and the
completeness of low-level search. We apply the proposed search method to a
recently proposed subgoal search algorithm and evaluate the algorithm trained
on offline data on complex planning problems. We demonstrate that our complete
subgoal search not only guarantees completeness but can even improve
performance in terms of search expansions for instances that the high-level
could solve without low-level augmentations. Our approach makes it possible to
apply subgoal-level planning for systems where completeness is a critical
requirement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kujanpaa_K/0/1/0/all/0/1&quot;&gt;Kalle Kujanp&amp;#xe4;&amp;#xe4;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pajarinen_J/0/1/0/all/0/1&quot;&gt;Joni Pajarinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilin_A/0/1/0/all/0/1&quot;&gt;Alexander Ilin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12822">
<title>Generating collective counterfactual explanations in score-based classification via mathematical optimization. (arXiv:2310.12822v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2310.12822</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the increasing use of Machine Learning models in high stakes decision
making settings, it has become increasingly important to have tools to
understand how models arrive at decisions. Assuming a trained Supervised
Classification model, explanations can be obtained via counterfactual analysis:
a counterfactual explanation of an instance indicates how this instance should
be minimally modified so that the perturbed instance is classified in the
desired class by the Machine Learning classification model. Most of the
Counterfactual Analysis literature focuses on the single-instance
single-counterfactual setting, in which the analysis is done for one single
instance to provide one single explanation. Taking a stakeholder&apos;s perspective,
in this paper we introduce the so-called collective counterfactual
explanations. By means of novel Mathematical Optimization models, we provide a
counterfactual explanation for each instance in a group of interest, so that
the total cost of the perturbations is minimized under some linking
constraints. Making the process of constructing counterfactuals collective
instead of individual enables us to detect the features that are critical to
the entire dataset to have the individuals classified in the desired class. Our
methodology allows for some instances to be treated individually, performing
the collective counterfactual analysis for a fraction of records of the group
of interest. This way, outliers are identified and handled appropriately. Under
some assumptions on the classifier and the space in which counterfactuals are
sought, finding collective counterfactuals is reduced to solving a convex
quadratic linearly constrained mixed integer optimization problem, which, for
datasets of moderate size, can be solved to optimality using existing solvers.
The performance of our approach is illustrated on real-world datasets,
demonstrating its usefulness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Carrizosa_E/0/1/0/all/0/1&quot;&gt;Emilio Carrizosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ramirez_Ayerbe_J/0/1/0/all/0/1&quot;&gt;Jasone Ram&amp;#xed;rez-Ayerbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Morales_D/0/1/0/all/0/1&quot;&gt;Dolores Romero Morales&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12823">
<title>AgentTuning: Enabling Generalized Agent Abilities for LLMs. (arXiv:2310.12823v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12823</link>
<description rdf:parseType="Literal">&lt;p&gt;Open large language models (LLMs) with great performance in various tasks
have significantly advanced the development of LLMs. However, they are far
inferior to commercial models such as ChatGPT and GPT-4 when acting as agents
to tackle complex tasks in the real world. These agent tasks employ LLMs as the
central controller responsible for planning, memorization, and tool
utilization, necessitating both fine-grained prompting methods and robust LLMs
to achieve satisfactory performance. Though many prompting methods have been
proposed to complete particular agent tasks, there is lack of research focusing
on improving the agent capabilities of LLMs themselves without compromising
their general abilities. In this work, we present AgentTuning, a simple and
general method to enhance the agent abilities of LLMs while maintaining their
general LLM capabilities. We construct AgentInstruct, a lightweight
instruction-tuning dataset containing high-quality interaction trajectories. We
employ a hybrid instruction-tuning strategy by combining AgentInstruct with
open-source instructions from general domains. AgentTuning is used to
instruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations show
that AgentTuning enables LLMs&apos; agent capabilities without compromising general
abilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agent
tasks, demonstrating generalized agent capabilities. We open source the
AgentInstruct and AgentLM-7B, 13B, and 70B models at
https://github.com/THUDM/AgentTuning , serving open and powerful alternatives
to commercial LLMs for agent tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Aohan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mingdao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1&quot;&gt;Rui Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bowen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jie Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12836">
<title>Knowledge-Augmented Language Model Verification. (arXiv:2310.12836v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12836</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent Language Models (LMs) have shown impressive capabilities in generating
texts with the knowledge internalized in parameters. Yet, LMs often generate
the factually incorrect responses to the given queries, since their knowledge
may be inaccurate, incomplete, and outdated. To address this problem, previous
works propose to augment LMs with the knowledge retrieved from an external
knowledge source. However, such approaches often show suboptimal text
generation performance due to two reasons: 1) the model may fail to retrieve
the knowledge relevant to the given query, or 2) the model may not faithfully
reflect the retrieved knowledge in the generated text. To overcome these, we
propose to verify the output and the knowledge of the knowledge-augmented LMs
with a separate verifier, which is a small LM that is trained to detect those
two types of errors through instruction-finetuning. Then, when the verifier
recognizes an error, we can rectify it by either retrieving new knowledge or
generating new text. Further, we use an ensemble of the outputs from different
instructions with a single verifier to enhance the reliability of the
verification processes. We validate the effectiveness of the proposed
verification steps on multiple question answering benchmarks, whose results
show that the proposed verifier effectively identifies retrieval and generation
errors, allowing LMs to provide more factually correct outputs. Our code is
available at https://github.com/JinheonBaek/KALMV.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1&quot;&gt;Jinheon Baek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1&quot;&gt;Soyeong Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1&quot;&gt;Minki Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jong C. Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sung Ju Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12842">
<title>Model-agnostic variable importance for predictive uncertainty: an entropy-based approach. (arXiv:2310.12842v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2310.12842</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to trust the predictions of a machine learning algorithm, it is
necessary to understand the factors that contribute to those predictions. In
the case of probabilistic and uncertainty-aware models, it is necessary to
understand not only the reasons for the predictions themselves, but also the
model&apos;s level of confidence in those predictions. In this paper, we show how
existing methods in explainability can be extended to uncertainty-aware models
and how such extensions can be used to understand the sources of uncertainty in
a model&apos;s predictive distribution. In particular, by adapting permutation
feature importance, partial dependence plots, and individual conditional
expectation plots, we demonstrate that novel insights into model behaviour may
be obtained and that these methods can be used to measure the impact of
features on both the entropy of the predictive distribution and the
log-likelihood of the ground truth labels under that distribution. With
experiments using both synthetic and real-world data, we demonstrate the
utility of these approaches in understanding both the sources of uncertainty
and their impact on model performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wood_D/0/1/0/all/0/1&quot;&gt;Danny Wood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Papamarkou_T/0/1/0/all/0/1&quot;&gt;Theodore Papamarkou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Benatan_M/0/1/0/all/0/1&quot;&gt;Matt Benatan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Allmendinger_R/0/1/0/all/0/1&quot;&gt;Richard Allmendinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12858">
<title>Audio Editing with Non-Rigid Text Prompts. (arXiv:2310.12858v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2310.12858</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we explore audio-editing with non-rigid text edits. We show
that the proposed editing pipeline is able to create audio edits that remain
faithful to the input audio. We explore text prompts that perform addition,
style transfer, and in-painting. We quantitatively and qualitatively show that
the edits are able to obtain results which outperform Audio-LDM, a recently
released text-prompted audio generation model. Qualitative inspection of the
results points out that the edits given by our approach remain more faithful to
the input audio in terms of keeping the original onsets and offsets of the
audio events.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paissan_F/0/1/0/all/0/1&quot;&gt;Francesco Paissan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhepei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravanelli_M/0/1/0/all/0/1&quot;&gt;Mirco Ravanelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1&quot;&gt;Paris Smaragdis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subakan_C/0/1/0/all/0/1&quot;&gt;Cem Subakan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12862">
<title>Fine-Tuning Generative Models as an Inference Method for Robotic Tasks. (arXiv:2310.12862v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12862</link>
<description rdf:parseType="Literal">&lt;p&gt;Adaptable models could greatly benefit robotic agents operating in the real
world, allowing them to deal with novel and varying conditions. While
approaches such as Bayesian inference are well-studied frameworks for adapting
models to evidence, we build on recent advances in deep generative models which
have greatly affected many areas of robotics. Harnessing modern GPU
acceleration, we investigate how to quickly adapt the sample generation of
neural network models to observations in robotic tasks. We propose a simple and
general method that is applicable to various deep generative models and robotic
environments. The key idea is to quickly fine-tune the model by fitting it to
generated samples matching the observed evidence, using the cross-entropy
method. We show that our method can be applied to both autoregressive models
and variational autoencoders, and demonstrate its usability in object shape
inference from grasping, inverse kinematics calculation, and point cloud
completion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krupnik_O/0/1/0/all/0/1&quot;&gt;Orr Krupnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafer_E/0/1/0/all/0/1&quot;&gt;Elisei Shafer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jurgenson_T/0/1/0/all/0/1&quot;&gt;Tom Jurgenson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamar_A/0/1/0/all/0/1&quot;&gt;Aviv Tamar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12893">
<title>Blind quantum machine learning with quantum bipartite correlator. (arXiv:2310.12893v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2310.12893</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed quantum computing is a promising computational paradigm for
performing computations that are beyond the reach of individual quantum
devices. Privacy in distributed quantum computing is critical for maintaining
confidentiality and protecting the data in the presence of untrusted computing
nodes. In this work, we introduce novel blind quantum machine learning
protocols based on the quantum bipartite correlator algorithm. Our protocols
have reduced communication overhead while preserving the privacy of data from
untrusted parties. We introduce robust algorithm-specific privacy-preserving
mechanisms with low computational overhead that do not require complex
cryptographic techniques. We then validate the effectiveness of the proposed
protocols through complexity and privacy analysis. Our findings pave the way
for advancements in distributed quantum computing, opening up new possibilities
for privacy-aware machine learning applications in the era of quantum
technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Changhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Boning Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Amer_O/0/1/0/all/0/1&quot;&gt;Omar Amer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Shaydulin_R/0/1/0/all/0/1&quot;&gt;Ruslan Shaydulin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Chakrabarti_S/0/1/0/all/0/1&quot;&gt;Shouvanik Chakrabarti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guoqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haowei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Schoch_I/0/1/0/all/0/1&quot;&gt;Isidor Schoch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kumar_N/0/1/0/all/0/1&quot;&gt;Niraj Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Lim_C/0/1/0/all/0/1&quot;&gt;Charles Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Ju Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Cappellaro_P/0/1/0/all/0/1&quot;&gt;Paola Cappellaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Pistoia_M/0/1/0/all/0/1&quot;&gt;Marco Pistoia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12900">
<title>Personalized human mobility prediction for HuMob challenge. (arXiv:2310.12900v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12900</link>
<description rdf:parseType="Literal">&lt;p&gt;We explain the methodology used to create the data submitted to HuMob
Challenge, a data analysis competition for human mobility prediction. We
adopted a personalized model to predict the individual&apos;s movement trajectory
from their data, instead of predicting from the overall movement, based on the
hypothesis that human movement is unique to each person. We devised the
features such as the date and time, activity time, days of the week, time of
day, and frequency of visits to POI (Point of Interest). As additional
features, we incorporated the movement of other individuals with similar
behavior patterns through the employment of clustering. The machine learning
model we adopted was the Support Vector Regression (SVR). We performed accuracy
through offline assessment and carried out feature selection and parameter
tuning. Although overall dataset provided consists of 100,000 users trajectory,
our method use only 20,000 target users data, and do not need to use other
80,000 data. Despite the personalized model&apos;s traditional feature engineering
approach, this model yields reasonably good accuracy with lower computational
cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suzuki_M/0/1/0/all/0/1&quot;&gt;Masahiro Suzuki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furuta_S/0/1/0/all/0/1&quot;&gt;Shomu Furuta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fukazawa_Y/0/1/0/all/0/1&quot;&gt;Yusuke Fukazawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1811.11479">
<title>Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data. (arXiv:1811.11479v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1811.11479</link>
<description rdf:parseType="Literal">&lt;p&gt;On-device machine learning (ML) enables the training process to exploit a
massive amount of user-generated private data samples. To enjoy this benefit,
inter-device communication overhead should be minimized. With this end, we
propose federated distillation (FD), a distributed model training algorithm
whose communication payload size is much smaller than a benchmark scheme,
federated learning (FL), particularly when the model size is large. Moreover,
user-generated data samples are likely to become non-IID across devices, which
commonly degrades the performance compared to the case with an IID dataset. To
cope with this, we propose federated augmentation (FAug), where each device
collectively trains a generative model, and thereby augments its local data
towards yielding an IID dataset. Empirical studies demonstrate that FD with
FAug yields around 26x less communication overhead while achieving 95-98% test
accuracy compared to FL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_E/0/1/0/all/0/1&quot;&gt;Eunjeong Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Seungeun Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyesung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jihong Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1&quot;&gt;Mehdi Bennis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seong-Lyun Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.08117">
<title>Neural networks with linear threshold activations: structure and algorithms. (arXiv:2111.08117v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2111.08117</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article we present new results on neural networks with linear
threshold activation functions. We precisely characterize the class of
functions that are representable by such neural networks and show that 2 hidden
layers are necessary and sufficient to represent any function representable in
the class. This is a surprising result in the light of recent exact
representability investigations for neural networks using other popular
activation functions like rectified linear units (ReLU). We also give precise
bounds on the sizes of the neural networks required to represent any function
in the class. Finally, we design an algorithm to solve the empirical risk
minimization (ERM) problem to global optimality for these neural networks with
a fixed architecture. The algorithm&apos;s running time is polynomial in the size of
the data sample, if the input dimension and the size of the network
architecture are considered fixed constants. The algorithm is unique in the
sense that it works for any architecture with any number of layers, whereas
previous polynomial time globally optimal algorithms work only for very
restricted classes of architectures. Using these insights, we propose a new
class of neural networks that we call shortcut linear threshold networks. To
the best of our knowledge, this way of designing neural networks has not been
explored before in the literature. We show that these neural networks have
several desirable theoretical properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalife_S/0/1/0/all/0/1&quot;&gt;Sammy Khalife&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hongyu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1&quot;&gt;Amitabh Basu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.13001">
<title>Deep Discriminative to Kernel Density Networks for Calibrated Inference. (arXiv:2201.13001v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2201.13001</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep discriminative approaches like random forests and deep neural networks
have recently found applications in many important real-world scenarios.
However, deploying these learning algorithms in safety-critical applications
raises concerns, particularly when it comes to ensuring confidence calibration
for both in-distribution and out-of-distribution data points. Many popular
methods for in-distribution (ID) calibration, such as isotonic regression and
Platt&apos;s sigmoidal regression, exhibit excellent ID calibration performance but
often at the cost of classification accuracy. Moreover, these methods are not
calibrated for the entire feature space, leading to overconfidence in the case
of out-of-distribution (OOD) samples. In this paper, we leveraged the fact that
deep models, including both random forests and deep-nets, learn internal
representations which are unions of polytopes with affine activation functions
to conceptualize them both as partitioning rules of the feature space. We
replace the affine function in each polytope populated by the training data
with a Gaussian kernel. We propose sufficient conditions for our proposed
methods to be consistent estimators of the corresponding class conditional
densities. Moreover, our experiments on both tabular and vision benchmarks show
that the proposed approaches obtain well-calibrated posteriors while mostly
preserving or improving the classification accuracy of the original algorithm
for in-distribution region, and extrapolates beyond the training data to handle
out-of-distribution inputs appropriately.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_J/0/1/0/all/0/1&quot;&gt;Jayanta Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1&quot;&gt;Will LeVine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haoyin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1&quot;&gt;Ashwin De Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomita_T/0/1/0/all/0/1&quot;&gt;Tyler M. Tomita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geisa_A/0/1/0/all/0/1&quot;&gt;Ali Geisa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1&quot;&gt;Tiffany Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desman_J/0/1/0/all/0/1&quot;&gt;Jacob Desman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1&quot;&gt;Joshua T. Vogelstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.08717">
<title>Relational Self-Supervised Learning. (arXiv:2203.08717v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.08717</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised Learning (SSL) including the mainstream contrastive learning
has achieved great success in learning visual representations without data
annotations. However, most methods mainly focus on the instance level
information (\ie, the different augmented images of the same instance should
have the same feature or cluster into the same class), but there is a lack of
attention on the relationships between different instances. In this paper, we
introduce a novel SSL paradigm, which we term as relational self-supervised
learning (ReSSL) framework that learns representations by modeling the
relationship between different instances. Specifically, our proposed method
employs sharpened distribution of pairwise similarities among different
instances as \textit{relation} metric, which is thus utilized to match the
feature embeddings of different augmentations. To boost the performance, we
argue that weak augmentations matter to represent a more reliable relation, and
leverage momentum strategy for practical efficiency. The designed asymmetric
predictor head and an InfoNCE warm-up strategy enhance the robustness to
hyper-parameters and benefit the resulting performance. Experimental results
show that our proposed ReSSL substantially outperforms the state-of-the-art
methods across different network architectures, including various lightweight
networks (\eg, EfficientNet and MobileNet).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1&quot;&gt;Mingkai Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1&quot;&gt;Shan You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Changshui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.14276">
<title>Example-based Hypernetworks for Out-of-Distribution Generalization. (arXiv:2203.14276v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2203.14276</link>
<description rdf:parseType="Literal">&lt;p&gt;As Natural Language Processing (NLP) algorithms continually achieve new
milestones, out-of-distribution generalization remains a significant challenge.
This paper addresses the issue of multi-source adaptation for unfamiliar
domains: We leverage labeled data from multiple source domains to generalize to
unknown target domains at training. Our innovative framework employs
example-based Hypernetwork adaptation: a T5 encoder-decoder initially generates
a unique signature from an input example, embedding it within the source
domains&apos; semantic space. This signature is subsequently utilized by a
Hypernetwork to generate the task classifier&apos;s weights. We evaluated our method
across two tasks - sentiment classification and natural language inference - in
29 adaptation scenarios, where it outpaced established algorithms. In an
advanced version, the signature also enriches the input example&apos;s
representation. We also compare our finetuned architecture to few-shot GPT-3,
demonstrating its effectiveness in essential use cases. To our knowledge, this
marks the first application of Hypernetworks to the adaptation for unknown
domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volk_T/0/1/0/all/0/1&quot;&gt;Tomer Volk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_David_E/0/1/0/all/0/1&quot;&gt;Eyal Ben-David&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amosy_O/0/1/0/all/0/1&quot;&gt;Ohad Amosy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1&quot;&gt;Gal Chechik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1&quot;&gt;Roi Reichart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.07439">
<title>INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold. (arXiv:2204.07439v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.07439</link>
<description rdf:parseType="Literal">&lt;p&gt;Binary Neural Networks (BNNs) have emerged as a promising solution for
reducing the memory footprint and compute costs of deep neural networks, but
they suffer from quality degradation due to the lack of freedom as activations
and weights are constrained to the binary values. To compensate for the
accuracy drop, we propose a novel BNN design called Binary Neural Network with
INSTAnce-aware threshold (INSTA-BNN), which controls the quantization threshold
dynamically in an input-dependent or instance-aware manner. According to our
observation, higher-order statistics can be a representative metric to estimate
the characteristics of the input distribution. INSTA-BNN is designed to adjust
the threshold dynamically considering various information, including
higher-order statistics, but it is also optimized judiciously to realize
minimal overhead on a real device. Our extensive study shows that INSTA-BNN
outperforms the baseline by 3.0% and 2.8% on the ImageNet classification task
with comparable computing cost, achieving 68.5% and 72.2% top-1 accuracy on
ResNet-18 and MobileNetV1 based models, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Changhun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyungjun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1&quot;&gt;Eunhyeok Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jae-Joon Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.07162">
<title>Category-Agnostic 6D Pose Estimation with Conditional Neural Processes. (arXiv:2206.07162v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.07162</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel meta-learning approach for 6D pose estimation on unknown
objects. In contrast to ``instance-level&quot; and ``category-level&quot; pose estimation
methods, our algorithm learns object representation in a category-agnostic way,
which endows it with strong generalization capabilities across object
categories. Specifically, we employ a neural process-based meta-learning
approach to train an encoder to capture texture and geometry of an object in a
latent representation, based on very few RGB-D images and ground-truth
keypoints. The latent representation is then used by a simultaneously
meta-trained decoder to predict the 6D pose of the object in new images.
Furthermore, we propose a novel geometry-aware decoder for the keypoint
prediction using a Graph Neural Network (GNN), which explicitly takes geometric
constraints specific to each object into consideration. To evaluate our
algorithm, extensive experiments are conducted on the \linemod dataset, and on
our new fully-annotated synthetic datasets generated from Multiple Categories
in Multiple Scenes (MCMS). Experimental results demonstrate that our model
performs well on unseen objects with very different shapes and appearances.
Remarkably, our model also shows robust performance on occluded scenes although
trained fully on data without occlusion. To our knowledge, this is the first
work exploring \textbf{cross-category level} 6D pose estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yumeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1&quot;&gt;Ning Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziesche_H/0/1/0/all/0/1&quot;&gt;Hanna Ziesche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1&quot;&gt;Gerhard Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.07940">
<title>When Rigidity Hurts: Soft Consistency Regularization for Probabilistic Hierarchical Time Series Forecasting. (arXiv:2206.07940v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.07940</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic hierarchical time-series forecasting is an important variant of
time-series forecasting, where the goal is to model and forecast multivariate
time-series that have underlying hierarchical relations. Most methods focus on
point predictions and do not provide well-calibrated probabilistic forecasts
distributions. Recent state-of-art probabilistic forecasting methods also
impose hierarchical relations on point predictions and samples of distribution
which does not account for coherency of forecast distributions. Previous works
also silently assume that datasets are always consistent with given
hierarchical relations and do not adapt to real-world datasets that show
deviation from this assumption. We close both these gap and propose PROFHiT,
which is a fully probabilistic hierarchical forecasting model that jointly
models forecast distribution of entire hierarchy. PROFHiT uses a flexible
probabilistic Bayesian approach and introduces a novel Distributional Coherency
regularization to learn from hierarchical relations for entire forecast
distribution that enables robust and calibrated forecasts as well as adapt to
datasets of varying hierarchical consistency. On evaluating PROFHiT over wide
range of datasets, we observed 41-88% better performance in accuracy and
significantly better calibration. Due to modeling the coherency over full
distribution, we observed that PROFHiT can robustly provide reliable forecasts
even if up to 10% of input time-series data is missing where other methods&apos;
performance severely degrade by over 70%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamarthi_H/0/1/0/all/0/1&quot;&gt;Harshavardhan Kamarthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1&quot;&gt;Lingkai Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1&quot;&gt;Alexander Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakash_B/0/1/0/all/0/1&quot;&gt;B. Aditya Prakash&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.06949">
<title>Seeking the Truth Beyond the Data. An Unsupervised Machine Learning Approach. (arXiv:2207.06949v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2207.06949</link>
<description rdf:parseType="Literal">&lt;p&gt;Clustering is an unsupervised machine learning methodology where unlabeled
elements/objects are grouped together aiming to the construction of
well-established clusters that their elements are classified according to their
similarity. The goal of this process is to provide a useful aid to the
researcher that will help her/him to identify patterns among the data. Dealing
with large databases, such patterns may not be easily detectable without the
contribution of a clustering algorithm. This article provides a deep
description of the most widely used clustering methodologies accompanied by
useful presentations concerning suitable parameter selection and
initializations. Simultaneously, this article not only represents a review
highlighting the major elements of examined clustering techniques but
emphasizes the comparison of these algorithms&apos; clustering efficiency based on 3
datasets, revealing their existing weaknesses and capabilities through accuracy
and complexity, during the confrontation of discrete and continuous
observations. The produced results help us extract valuable conclusions about
the appropriateness of the examined clustering techniques in accordance with
the dataset&apos;s size.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Saligkaras_D/0/1/0/all/0/1&quot;&gt;Dimitrios Saligkaras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Papageorgiou_V/0/1/0/all/0/1&quot;&gt;Vasileios E. Papageorgiou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.06348">
<title>Can Brain Signals Reveal Inner Alignment with Human Languages?. (arXiv:2208.06348v4 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2208.06348</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain Signals, such as Electroencephalography (EEG), and human languages have
been widely explored independently for many downstream tasks, however, the
connection between them has not been well explored. In this study, we explore
the relationship and dependency between EEG and language. To study at the
representation level, we introduced \textbf{MTAM}, a \textbf{M}ultimodal
\textbf{T}ransformer \textbf{A}lignment \textbf{M}odel, to observe coordinated
representations between the two modalities. We used various relationship
alignment-seeking techniques, such as Canonical Correlation Analysis and
Wasserstein Distance, as loss functions to transfigure features. On downstream
applications, sentiment analysis and relation detection, we achieved new
state-of-the-art results on two datasets, ZuCo and K-EmoCon. Our method
achieved an F1-score improvement of 1.7% on K-EmoCon and 9.3% on Zuco datasets
for sentiment analysis, and 7.4% on ZuCo for relation detection. In addition,
we provide interpretations of the performance improvement: (1) feature
distribution shows the effectiveness of the alignment module for discovering
and encoding the relationship between EEG and language; (2) alignment weights
show the influence of different language semantics as well as EEG frequency
features; (3) brain topographical maps provide an intuitive demonstration of
the connectivity in the brain regions. Our code is available at
\url{https://github.com/Jason-Qiu/EEG_Language_Alignment}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Han_W/0/1/0/all/0/1&quot;&gt;William Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jielin Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jiacheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mengdi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Weber_D/0/1/0/all/0/1&quot;&gt;Douglas Weber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Ding Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.13623">
<title>Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook. (arXiv:2210.13623v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2210.13623</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, reinforcement learning and bandits have transformed a wide
range of real-world applications including healthcare, finance, recommendation
systems, robotics, and last but not least, the speech and natural language
processing. While most speech and language applications of reinforcement
learning algorithms are centered around improving the training of deep neural
networks with its flexible optimization properties, there are still many
grounds to explore to utilize the benefits of reinforcement learning, such as
its reward-driven adaptability, state representations, temporal structures and
generalizability. In this survey, we present an overview of recent advancements
of reinforcement learning and bandits, and discuss how they can be effectively
employed to solve speech and natural language processing problems with models
that are adaptive, interactive and scalable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Baihan Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.00617">
<title>Convergence of policy gradient methods for finite-horizon stochastic linear-quadratic control problems. (arXiv:2211.00617v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2211.00617</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the global linear convergence of policy gradient (PG) methods for
finite-horizon continuous-time exploratory linear-quadratic control (LQC)
problems. The setting includes stochastic LQC problems with indefinite costs
and allows additional entropy regularisers in the objective. We consider a
continuous-time Gaussian policy whose mean is linear in the state variable and
whose covariance is state-independent. Contrary to discrete-time problems, the
cost is noncoercive in the policy and not all descent directions lead to
bounded iterates. We propose geometry-aware gradient descents for the mean and
covariance of the policy using the Fisher geometry and the Bures-Wasserstein
geometry, respectively. The policy iterates are shown to satisfy an a-priori
bound, and converge globally to the optimal policy with a linear rate. We
further propose a novel PG method with discrete-time policies. The algorithm
leverages the continuous-time analysis, and achieves a robust linear
convergence across different action frequencies. A numerical experiment
confirms the convergence and robustness of the proposed algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Giegrich_M/0/1/0/all/0/1&quot;&gt;Michael Giegrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Reisinger_C/0/1/0/all/0/1&quot;&gt;Christoph Reisinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yufei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01365">
<title>An Information-Theoretic Analysis of Compute-Optimal Neural Scaling Laws. (arXiv:2212.01365v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01365</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the compute-optimal trade-off between model and training data set
sizes for large neural networks. Our result suggests a linear relation similar
to that supported by the empirical analysis of chinchilla. While that work
studies transformer-based large language models trained on the MassiveText
corpus gopher, as a starting point for development of a mathematical theory, we
focus on a simpler learning model and data generating process, each based on a
neural network with a sigmoidal output unit and single hidden layer of ReLU
activation units. We introduce general error upper bounds for a class of
algorithms which incrementally update a statistic (for example gradient
descent). For a particular learning model inspired by barron 1993, we establish
an upper bound on the minimal information-theoretically achievable expected
error as a function of model and data set sizes. We then derive allocations of
computation that minimize this bound. We present empirical results which
suggest that this approximation correctly identifies an asymptotic linear
compute-optimal scaling. This approximation also generates new insights. Among
other things, it suggests that, as the input dimension or latent space
complexity grows, as might be the case for example if a longer history of
tokens is taken as input to a language model, a larger fraction of the compute
budget should be allocated to growing the learning model rather than training
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1&quot;&gt;Hong Jun Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1&quot;&gt;Benjamin Van Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.07469">
<title>Learning threshold neurons via the &quot;edge of stability&quot;. (arXiv:2212.07469v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.07469</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing analyses of neural network training often operate under the
unrealistic assumption of an extremely small learning rate. This lies in stark
contrast to practical wisdom and empirical studies, such as the work of J.
Cohen et al. (ICLR 2021), which exhibit startling new phenomena (the &quot;edge of
stability&quot; or &quot;unstable convergence&quot;) and potential benefits for generalization
in the large learning rate regime. Despite a flurry of recent works on this
topic, however, the latter effect is still poorly understood. In this paper, we
take a step towards understanding genuinely non-convex training dynamics with
large learning rates by performing a detailed analysis of gradient descent for
simplified models of two-layer neural networks. For these models, we provably
establish the edge of stability phenomenon and discover a sharp phase
transition for the step size below which the neural network fails to learn
&quot;threshold-like&quot; neurons (i.e., neurons with a non-zero first-layer bias). This
elucidates one possible mechanism by which the edge of stability can in fact
lead to better generalization, as threshold neurons are basic building blocks
with useful inductive bias for many tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_K/0/1/0/all/0/1&quot;&gt;Kwangjun Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Bubeck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chewi_S/0/1/0/all/0/1&quot;&gt;Sinho Chewi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yin Tat Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suarez_F/0/1/0/all/0/1&quot;&gt;Felipe Suarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09730">
<title>Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units. (arXiv:2212.09730v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09730</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce DISSC, a novel, lightweight method that converts the rhythm,
pitch contour and timbre of a recording to a target speaker in a textless
manner. Unlike DISSC, most voice conversion (VC) methods focus primarily on
timbre, and ignore people&apos;s unique speaking style (prosody). The proposed
approach uses a pretrained, self-supervised model for encoding speech to
discrete units, which makes it simple, effective, and fast to train. All
conversion modules are only trained on reconstruction like tasks, thus suitable
for any-to-many VC with no paired data. We introduce a suite of quantitative
and qualitative evaluation metrics for this setup, and empirically demonstrate
that DISSC significantly outperforms the evaluated baselines. Code and samples
are available at https://pages.cs.huji.ac.il/adiyoss-lab/dissc/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maimon_G/0/1/0/all/0/1&quot;&gt;Gallil Maimon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1&quot;&gt;Yossi Adi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.12321">
<title>Neural Relation Graph: A Unified Framework for Identifying Label Noise and Outlier Data. (arXiv:2301.12321v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.12321</link>
<description rdf:parseType="Literal">&lt;p&gt;Diagnosing and cleaning data is a crucial step for building robust machine
learning systems. However, identifying problems within large-scale datasets
with real-world distributions is challenging due to the presence of complex
issues such as label errors, under-representation, and outliers. In this paper,
we propose a unified approach for identifying the problematic data by utilizing
a largely ignored source of information: a relational structure of data in the
feature-embedded space. To this end, we present scalable and effective
algorithms for detecting label errors and outlier data based on the relational
graph structure of data. We further introduce a visualization tool that
provides contextual information of a data point in the feature-embedded space,
serving as an effective tool for interactively diagnosing data. We evaluate the
label error and outlier/out-of-distribution (OOD) detection performances of our
approach on the large-scale image, speech, and language domain tasks, including
ImageNet, ESC-50, and SST2. Our approach achieves state-of-the-art detection
performance on all tasks considered and demonstrates its effectiveness in
debugging large-scale real-world datasets across various domains. We release
codes at https://github.com/snu-mllab/Neural-Relation-Graph.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jang-Hyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1&quot;&gt;Sangdoo Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Hyun Oh Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01328">
<title>IC3: Image Captioning by Committee Consensus. (arXiv:2302.01328v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01328</link>
<description rdf:parseType="Literal">&lt;p&gt;If you ask a human to describe an image, they might do so in a thousand
different ways. Traditionally, image captioning models are trained to generate
a single &quot;best&quot; (most like a reference) image caption. Unfortunately, doing so
encourages captions that are &quot;informationally impoverished,&quot; and focus on only
a subset of the possible details, while ignoring other potentially useful
information in the scene. In this work, we introduce a simple, yet novel,
method: &quot;Image Captioning by Committee Consensus&quot; (IC3), designed to generate a
single caption that captures high-level details from several annotator
viewpoints. Humans rate captions produced by IC3 at least as helpful as
baseline SOTA models more than two thirds of the time, and IC3 can improve the
performance of SOTA automated recall systems by up to 84%, outperforming single
human-generated reference captions, and indicating significant improvements
over SOTA approaches for visual description. Code is available at
https://davidmchan.github.io/caption-by-committee/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1&quot;&gt;David M. Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myers_A/0/1/0/all/0/1&quot;&gt;Austin Myers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vijayanarasimhan_S/0/1/0/all/0/1&quot;&gt;Sudheendra Vijayanarasimhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1&quot;&gt;David A. Ross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canny_J/0/1/0/all/0/1&quot;&gt;John Canny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.03098">
<title>One-shot Empirical Privacy Estimation for Federated Learning. (arXiv:2302.03098v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.03098</link>
<description rdf:parseType="Literal">&lt;p&gt;Privacy estimation techniques for differentially private (DP) algorithms are
useful for comparing against analytical bounds, or to empirically measure
privacy loss in settings where known analytical bounds are not tight. However,
existing privacy auditing techniques usually make strong assumptions on the
adversary (e.g., knowledge of intermediate model iterates or the training data
distribution), are tailored to specific tasks, model architectures, or DP
algorithm, and/or require retraining the model many times (typically on the
order of thousands). These shortcomings make deploying such techniques at scale
difficult in practice, especially in federated settings where model training
can take days or weeks. In this work, we present a novel ``one-shot&apos;&apos; approach
that can systematically address these challenges, allowing efficient auditing
or estimation of the privacy loss of a model during the same, single training
run used to fit model parameters, and without requiring any a priori knowledge
about the model architecture, task, or DP training algorithm. We show that our
method provides provably correct estimates for the privacy loss under the
Gaussian mechanism, and we demonstrate its performance on well-established FL
benchmark datasets under several adversarial threat models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andrew_G/0/1/0/all/0/1&quot;&gt;Galen Andrew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1&quot;&gt;Peter Kairouz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Sewoong Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oprea_A/0/1/0/all/0/1&quot;&gt;Alina Oprea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McMahan_H/0/1/0/all/0/1&quot;&gt;H. Brendan McMahan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suriyakumar_V/0/1/0/all/0/1&quot;&gt;Vinith Suriyakumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08724">
<title>Piecewise Deterministic Markov Processes for Bayesian Neural Networks. (arXiv:2302.08724v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08724</link>
<description rdf:parseType="Literal">&lt;p&gt;Inference on modern Bayesian Neural Networks (BNNs) often relies on a
variational inference treatment, imposing violated assumptions of independence
and the form of the posterior. Traditional MCMC approaches avoid these
assumptions at the cost of increased computation due to its incompatibility to
subsampling of the likelihood. New Piecewise Deterministic Markov Process
(PDMP) samplers permit subsampling, though introduce a model specific
inhomogenous Poisson Process (IPPs) which is difficult to sample from. This
work introduces a new generic and adaptive thinning scheme for sampling from
these IPPs, and demonstrates how this approach can accelerate the application
of PDMPs for inference in BNNs. Experimentation illustrates how inference with
these methods is computationally feasible, can improve predictive accuracy,
MCMC mixing performance, and provide informative uncertainty measurements when
compared against other approximate inference schemes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goan_E/0/1/0/all/0/1&quot;&gt;Ethan Goan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perrin_D/0/1/0/all/0/1&quot;&gt;Dimitri Perrin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mengersen_K/0/1/0/all/0/1&quot;&gt;Kerrie Mengersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fookes_C/0/1/0/all/0/1&quot;&gt;Clinton Fookes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11084">
<title>Test-Time Distribution Normalization for Contrastively Learned Vision-language Models. (arXiv:2302.11084v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11084</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in the field of vision-language contrastive learning have made it
possible for many downstream applications to be carried out efficiently and
accurately by simply taking the dot product between image and text
representations. One of the most representative approaches proposed recently
known as CLIP has garnered widespread adoption due to its effectiveness. CLIP
is trained with an InfoNCE loss that takes into account both positive and
negative samples to help learn a much more robust representation space. This
paper reveals that the common downstream practice of taking a dot product is
only a zeroth-order approximation of the optimization goal, resulting in a loss
of information during test-time. Intuitively, since the model has been
optimized based on the InfoNCE loss, test-time procedures should also be in
alignment. The question lies in how one can retrieve any semblance of negative
samples information during inference in a computationally efficient way. To
this end, we propose Distribution Normalization (DN), where we approximate the
mean representation of a batch of test samples and use such a mean to represent
what would be analogous to negative samples in the InfoNCE loss. DN requires no
retraining or fine-tuning and can be effortlessly applied during inference.
Extensive experiments on a wide variety of downstream tasks exhibit a clear
advantage of DN over the dot product on top of other existing test-time
augmentation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yifei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Juntao Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fengyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zabih_R/0/1/0/all/0/1&quot;&gt;Ramin Zabih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Ser-Nam Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12410">
<title>EDGI: Equivariant Diffusion for Planning with Embodied Agents. (arXiv:2303.12410v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12410</link>
<description rdf:parseType="Literal">&lt;p&gt;Embodied agents operate in a structured world, often solving tasks with
spatial, temporal, and permutation symmetries. Most algorithms for planning and
model-based reinforcement learning (MBRL) do not take this rich geometric
structure into account, leading to sample inefficiency and poor generalization.
We introduce the Equivariant Diffuser for Generating Interactions (EDGI), an
algorithm for MBRL and planning that is equivariant with respect to the product
of the spatial symmetry group SE(3), the discrete-time translation group Z, and
the object permutation group Sn. EDGI follows the Diffuser framework (Janner et
al., 2022) in treating both learning a world model and planning in it as a
conditional generative modeling problem, training a diffusion model on an
offline trajectory dataset. We introduce a new SE(3)xZxSn-equivariant diffusion
model that supports multiple representations. We integrate this model in a
planning loop, where conditioning and classifier guidance let us softly break
the symmetry for specific tasks as needed. On object manipulation and
navigation tasks, EDGI is substantially more sample efficient and generalizes
better across the symmetry group than non-equivariant models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brehmer_J/0/1/0/all/0/1&quot;&gt;Johann Brehmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bose_J/0/1/0/all/0/1&quot;&gt;Joey Bose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haan_P/0/1/0/all/0/1&quot;&gt;Pim de Haan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1&quot;&gt;Taco Cohen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13047">
<title>Towards Better Dynamic Graph Learning: New Architecture and Unified Library. (arXiv:2303.13047v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13047</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose DyGFormer, a new Transformer-based architecture for dynamic graph
learning. DyGFormer is conceptually simple and only needs to learn from nodes&apos;
historical first-hop interactions by: (1) a neighbor co-occurrence encoding
scheme that explores the correlations of the source node and destination node
based on their historical sequences; (2) a patching technique that divides each
sequence into multiple patches and feeds them to Transformer, allowing the
model to effectively and efficiently benefit from longer histories. We also
introduce DyGLib, a unified library with standard training pipelines,
extensible coding interfaces, and comprehensive evaluating protocols to promote
reproducible, scalable, and credible dynamic graph learning research. By
performing exhaustive experiments on thirteen datasets for dynamic link
prediction and dynamic node classification tasks, we find that DyGFormer
achieves state-of-the-art performance on most of the datasets, demonstrating
its effectiveness in capturing nodes&apos; correlations and long-term temporal
dependencies. Moreover, some results of baselines are inconsistent with
previous reports, which may be caused by their diverse but less rigorous
implementations, showing the importance of DyGLib. All the used resources are
publicly available at https://github.com/yule-BUAA/DyGLib.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Le Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Leilei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bowen Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_W/0/1/0/all/0/1&quot;&gt;Weifeng Lv&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.14090">
<title>Physics-informed neural networks in the recreation of hydrodynamic simulations from dark matter. (arXiv:2303.14090v2 [astro-ph.CO] UPDATED)</title>
<link>http://arxiv.org/abs/2303.14090</link>
<description rdf:parseType="Literal">&lt;p&gt;Physics-informed neural networks have emerged as a coherent framework for
building predictive models that combine statistical patterns with domain
knowledge. The underlying notion is to enrich the optimization loss function
with known relationships to constrain the space of possible solutions.
Hydrodynamic simulations are a core constituent of modern cosmology, while the
required computations are both expensive and time-consuming. At the same time,
the comparatively fast simulation of dark matter requires fewer resources,
which has led to the emergence of machine learning algorithms for baryon
inpainting as an active area of research; here, recreating the scatter found in
hydrodynamic simulations is an ongoing challenge. This paper presents the first
application of physics-informed neural networks to baryon inpainting by
combining advances in neural network architectures with physical constraints,
injecting theory on baryon conversion efficiency into the model loss function.
We also introduce a punitive prediction comparison based on the
Kullback-Leibler divergence, which enforces scatter reproduction. By
simultaneously extracting the complete set of baryonic properties for the Simba
suite of cosmological simulations, our results demonstrate improved accuracy of
baryonic predictions based on dark matter halo properties, successful recovery
of the fundamental metallicity relation, and retrieve scatter that traces the
target simulation&apos;s distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Dai_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Moews_B/0/1/0/all/0/1&quot;&gt;Ben Moews&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Vilalta_R/0/1/0/all/0/1&quot;&gt;Ricardo Vilalta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Dave_R/0/1/0/all/0/1&quot;&gt;Romeel Dave&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06762">
<title>Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study. (arXiv:2304.06762v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06762</link>
<description rdf:parseType="Literal">&lt;p&gt;Large decoder-only language models (LMs) can be largely improved in terms of
perplexity by retrieval (e.g., RETRO), but its impact on text generation
quality and downstream task accuracy is unclear. Thus, it is still an open
question: shall we pretrain large autoregressive LMs with retrieval? To answer
it, we perform a comprehensive study on a scalable pre-trained
retrieval-augmented LM (i.e., RETRO) compared with standard GPT and
retrieval-augmented GPT incorporated at fine-tuning or inference stages. We
first provide the recipe to reproduce RETRO up to 9.5B parameters while
retrieving a text corpus with 330B tokens. Based on that, we have the following
novel findings: i) RETRO outperforms GPT on text generation with much less
degeneration (i.e., repetition), moderately higher factual accuracy, and
slightly lower toxicity with a nontoxic retrieval database. ii) On the LM
Evaluation Harness benchmark, RETRO largely outperforms GPT on
knowledge-intensive tasks, but is on par with GPT on other tasks. Furthermore,
we introduce a simple variant of the model, RETRO++, which largely improves
open-domain QA results of original RETRO (e.g., EM score +8.6 on Natural
Question) and significantly outperforms retrieval-augmented GPT in both
fine-tuning and zero-shot evaluation settings. Our findings highlight the
promising direction of pretraining autoregressive LMs with retrieval as future
foundation models. We release our implementation at:
https://github.com/NVIDIA/Megatron-LM#retro.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Boxin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1&quot;&gt;Wei Ping&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McAfee_L/0/1/0/all/0/1&quot;&gt;Lawrence McAfee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zihan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1&quot;&gt;Mohammad Shoeybi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yi Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuchaiev_O/0/1/0/all/0/1&quot;&gt;Oleksii Kuchaiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1&quot;&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1&quot;&gt;Bryan Catanzaro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07063">
<title>Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors. (arXiv:2304.07063v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07063</link>
<description rdf:parseType="Literal">&lt;p&gt;Reasoning on knowledge graphs is a challenging task because it utilizes
observed information to predict the missing one. Particularly, answering
complex queries based on first-order logic is one of the crucial tasks to
verify learning to reason abilities for generalization and composition.
Recently, the prevailing method is query embedding which learns the embedding
of a set of entities and treats logic operations as set operations and has
shown great empirical success. Though there has been much research following
the same formulation, many of its claims lack a formal and systematic
inspection. In this paper, we rethink this formulation and justify many of the
previous claims by characterizing the scope of queries investigated previously
and precisely identifying the gap between its formulation and its goal, as well
as providing complexity analysis for the currently investigated queries.
Moreover, we develop a new dataset containing ten new types of queries with
features that have never been considered and therefore can provide a thorough
investigation of complex queries. Finally, we propose a new neural-symbolic
method, Fuzzy Inference with Truth value (FIT), where we equip the neural link
predictors with fuzzy logic theory to support end-to-end learning using complex
queries with provable reasoning capability. Empirical results show that our
method outperforms previous methods significantly in the new dataset and also
surpasses previous methods in the existing dataset at the same time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Hang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yangqiu Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08503">
<title>A Scalable Test Problem Generator for Sequential Transfer Optimization. (arXiv:2304.08503v4 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08503</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequential transfer optimization (STO), which aims to improve the
optimization performance on a task of interest by exploiting the knowledge
captured from several previously-solved optimization tasks stored in a
database, has been gaining increasing research attention over the years.
However, despite the remarkable advances in algorithm design, the development
of a systematic benchmark suite for comprehensive comparisons of STO algorithms
received far less attention. Existing test problems are either simply generated
by assembling other benchmark functions or extended from specific practical
problems with limited scalability. The relationships between the optimal
solutions of the source and target tasks in these problems are also often
manually configured, limiting their ability to model different similarity
relationships presented in real-world problems. Consequently, the good
performance achieved by an algorithm on these problems might be biased and hard
to be generalized to other problems. In light of the above, in this study, we
first introduce four concepts for characterizing STO problems and present an
important problem feature, namely similarity distribution, which quantitatively
delineates the relationship between the optima of the source and target tasks.
Then, we present the general design guidelines of STO problems and a particular
STO problem generator with good scalability. Specifically, the similarity
distribution of a problem can be easily customized, enabling a continuous
spectrum of representation of the diverse similarity relationships of
real-world problems. Lastly, a benchmark suite with 12 STO problems featured by
a variety of customized similarity relationships is developed using the
proposed generator. The source code of the problem generator is available at
https://github.com/XmingHsueh/STOP-G.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1&quot;&gt;Xiaoming Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cuie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1&quot;&gt;Liang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Linqi Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1&quot;&gt;Kay Chen Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09310">
<title>The Adaptive $\tau$-Lasso: Robustness and Oracle Properties. (arXiv:2304.09310v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09310</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a new regularized version of the robust
$\tau$-regression estimator for analyzing high-dimensional datasets subject to
gross contamination in the response variables and covariates (explanatory
variables). The resulting estimator, termed adaptive $\tau$-Lasso, is robust to
outliers and high-leverage points. It also incorporates an adaptive
$\ell_1$-norm penalty term, which enables the selection of relevant variables
and reduces the bias associated with large true regression coefficients. More
specifically, this adaptive $\ell_1$-norm penalty term assigns a weight to each
regression coefficient. For a fixed number of predictors $p$, we show that the
adaptive $\tau$-Lasso has the oracle property, ensuring both variable-selection
consistency and asymptotic normality. Asymptotic normality applies only to the
entries of the regression vector corresponding to the true support, assuming
knowledge of the true regression vector support. We characterize its robustness
via the finite-sample breakdown point and the influence function. We carry out
extensive simulations and observe that the class of $\tau$-Lasso estimators
exhibits robustness and reliable performance in both contaminated and
uncontaminated data settings. We also validate our theoretical findings on
robustness properties through simulation experiments. In the face of outliers
and high-leverage points, the adaptive $\tau$-Lasso and $\tau$-Lasso estimators
achieve the best performance or close-to-best performance in terms of
prediction and variable selection accuracy compared to other competing
regularized estimators for all scenarios considered in this study. Therefore,
the adaptive $\tau$-Lasso and $\tau$-Lasso estimators can be effectively
employed for a variety of sparse linear regression problems, particularly in
high-dimensional settings and when the data is contaminated by outliers and
high-leverage points.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mozafari_Majd_E/0/1/0/all/0/1&quot;&gt;Emadaldin Mozafari-Majd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Koivunen_V/0/1/0/all/0/1&quot;&gt;Visa Koivunen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10398">
<title>Multi-label Node Classification On Graph-Structured Data. (arXiv:2304.10398v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10398</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have shown state-of-the-art improvements in node
classification tasks on graphs. While these improvements have been largely
demonstrated in a multi-class classification scenario, a more general and
realistic scenario in which each node could have multiple labels has so far
received little attention. The first challenge in conducting focused studies on
multi-label node classification is the limited number of publicly available
multi-label graph datasets. Therefore, as our first contribution, we collect
and release three real-world biological datasets and develop a multi-label
graph generator to generate datasets with tunable properties. While high label
similarity (high homophily) is usually attributed to the success of GNNs, we
argue that a multi-label scenario does not follow the usual semantics of
homophily and heterophily so far defined for a multi-class scenario. As our
second contribution, we define homophily and Cross-Class Neighborhood
Similarity for the multi-label scenario and provide a thorough analyses of the
collected $9$ multi-label datasets. Finally, we perform a large-scale
comparative study with $8$ methods and $9$ datasets and analyse the
performances of the methods to assess the progress made by current state of the
art in the multi-label node classification scenario. We release our benchmark
at https://github.com/Tianqi-py/MLGNC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tianqi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1&quot;&gt;Ngan Thi Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanjalic_A/0/1/0/all/0/1&quot;&gt;Alan Hanjalic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khosla_M/0/1/0/all/0/1&quot;&gt;Megha Khosla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10557">
<title>An Introduction to Transformers. (arXiv:2304.10557v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10557</link>
<description rdf:parseType="Literal">&lt;p&gt;The transformer is a neural network component that can be used to learn
useful representations of sequences or sets of data-points. The transformer has
driven recent advances in natural language processing, computer vision, and
spatio-temporal modelling. There are many introductions to transformers, but
most do not contain precise mathematical descriptions of the architecture and
the intuitions behind the design choices are often also missing. Moreover, as
research takes a winding path, the explanations for the components of the
transformer can be idiosyncratic. In this note we aim for a mathematically
precise, intuitive, and clean description of the transformer architecture. We
will not discuss training as this is rather standard. We assume that the reader
is familiar with fundamental topics in machine learning including multi-layer
perceptrons, linear transformations, softmax functions and basic probability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1&quot;&gt;Richard E. Turner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12410">
<title>PEFT-Ref: A Modular Reference Architecture and Typology for Parameter-Efficient Finetuning Techniques. (arXiv:2304.12410v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12410</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent parameter-efficient finetuning (PEFT) techniques aim to improve over
the considerable cost of fully finetuning large pretrained language models
(PLM). As different PEFT techniques proliferate, it is becoming difficult to
compare them, in particular in terms of (i) the structure and functionality
they add to the PLM, (ii) the different types and degrees of efficiency
improvements achieved, (iii) performance at different downstream tasks, and
(iv) how differences in structure and functionality relate to efficiency and
task performance. To facilitate such comparisons, this paper presents a
reference architecture which standardises aspects shared by different PEFT
techniques, while isolating differences to specific locations and interactions
with the standard components. Through this process of standardising and
isolating differences, a modular view of PEFT techniques emerges, supporting
not only direct comparison of different techniques and their efficiency and
task performance, but also systematic exploration of reusability and
composability of the different types of finetuned modules. We demonstrate how
the reference architecture can be applied to understand properties and relative
advantages of PEFT techniques, hence to inform selection of techniques for
specific tasks, and design choices for new PEFT techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabry_M/0/1/0/all/0/1&quot;&gt;Mohammed Sabry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belz_A/0/1/0/all/0/1&quot;&gt;Anya Belz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12526">
<title>Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models. (arXiv:2304.12526v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12526</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models are powerful, but they require a lot of time and data to
train. We propose Patch Diffusion, a generic patch-wise training framework, to
significantly reduce the training time costs while improving data efficiency,
which thus helps democratize diffusion model training to broader users. At the
core of our innovations is a new conditional score function at the patch level,
where the patch location in the original image is included as additional
coordinate channels, while the patch size is randomized and diversified
throughout training to encode the cross-region dependency at multiple scales.
Sampling with our method is as easy as in the original diffusion model. Through
Patch Diffusion, we could achieve $\mathbf{\ge 2\times}$ faster training, while
maintaining comparable or better generation quality. Patch Diffusion meanwhile
improves the performance of diffusion models trained on relatively small
datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve
outstanding FID scores in line with state-of-the-art benchmarks: 1.77 on
CelebA-64$\times$64, 1.93 on AFHQv2-Wild-64$\times$64, and 2.72 on
ImageNet-256$\times$256. We share our code and pre-trained models at
https://github.com/Zhendong-Wang/Patch-Diffusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhendong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yifan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Huangjie Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peihao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1&quot;&gt;Pengcheng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weizhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingyuan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03495">
<title>Automatic Prompt Optimization with &quot;Gradient Descent&quot; and Beam Search. (arXiv:2305.03495v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03495</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown impressive performance as general
purpose agents, but their abilities remain highly dependent on prompts which
are hand written with onerous trial-and-error effort. We propose a simple and
nonparametric solution to this problem, Automatic Prompt Optimization (APO),
which is inspired by numerical gradient descent to automatically improve
prompts, assuming access to training data and an LLM API. The algorithm uses
minibatches of data to form natural language &quot;gradients&quot; that criticize the
current prompt. The gradients are then &quot;propagated&quot; into the prompt by editing
the prompt in the opposite semantic direction of the gradient. These gradient
descent steps are guided by a beam search and bandit selection procedure which
significantly improves algorithmic efficiency. Preliminary results across three
benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest
that Automatic Prompt Optimization can outperform prior prompt editing
techniques and improve an initial prompt&apos;s performance by up to 31%, by using
data to rewrite vague task descriptions into more precise annotation
instructions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1&quot;&gt;Reid Pryzant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iter_D/0/1/0/all/0/1&quot;&gt;Dan Iter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jerry Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yin Tat Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chenguang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1&quot;&gt;Michael Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05799">
<title>Seeing double with a multifunctional reservoir computer. (arXiv:2305.05799v2 [math.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05799</link>
<description rdf:parseType="Literal">&lt;p&gt;Multifunctional biological neural networks exploit multistability in order to
perform multiple tasks without changing any network properties. Enabling
artificial neural networks (ANNs) to obtain certain multistabilities in order
to perform several tasks, where each task is related to a particular attractor
in the network&apos;s state space, naturally has many benefits from a machine
learning perspective. Given the association to multistability, in this paper we
explore how the relationship between different attractors influences the
ability of a reservoir computer (RC), which is a dynamical system in the form
of an ANN, to achieve multifunctionality. We construct the `seeing double&apos;
problem to systematically study how a RC reconstructs a coexistence of
attractors when there is an overlap between them. As the amount of overlap
increases, we discover that for multifunctionality to occur, there is a
critical dependence on a suitable choice of the spectral radius for the RC&apos;s
internal network connections. A bifurcation analysis reveals how
multifunctionality emerges and is destroyed as the RC enters a chaotic regime
that can lead to chaotic itinerancy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Flynn_A/0/1/0/all/0/1&quot;&gt;Andrew Flynn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tsachouridis_V/0/1/0/all/0/1&quot;&gt;Vassilios A. Tsachouridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Amann_A/0/1/0/all/0/1&quot;&gt;Andreas Amann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10744">
<title>Online Resource Allocation in Episodic Markov Decision Processes. (arXiv:2305.10744v3 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10744</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies a long-term resource allocation problem over multiple
periods where each period requires a multi-stage decision-making process. We
formulate the problem as an online allocation problem in an episodic
finite-horizon constrained Markov decision process with an unknown
non-stationary transition function and stochastic non-stationary reward and
resource consumption functions. We propose the observe-then-decide regime and
improve the existing decide-then-observe regime, while the two settings differ
in how the observations and feedback about the reward and resource consumption
functions are given to the decision-maker. We develop an online dual mirror
descent algorithm that achieves near-optimal regret bounds for both settings.
For the observe-then-decide regime, we prove that the expected regret against
the dynamic clairvoyant optimal policy is bounded by $\tilde
O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ where $\rho\in(0,1)$ is the budget parameter,
$H$ is the length of the horizon, $S$ and $A$ are the numbers of states and
actions, and $T$ is the number of episodes. For the decide-then-observe regime,
we show that the regret against the static optimal policy that has access to
the mean reward and mean resource consumption functions is bounded by $\tilde
O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ with high probability. We test the numerical
efficiency of our method for a variant of the resource-constrained inventory
management problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Duksang Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Overman_W/0/1/0/all/0/1&quot;&gt;William Overman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dabeen Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12467">
<title>Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks. (arXiv:2305.12467v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12467</link>
<description rdf:parseType="Literal">&lt;p&gt;The training process of ReLU neural networks often exhibits complicated
nonlinear phenomena. The nonlinearity of models and non-convexity of loss pose
significant challenges for theoretical analysis. Therefore, most previous
theoretical works on the optimization dynamics of neural networks focus either
on local analysis (like the end of training) or approximate linear models (like
Neural Tangent Kernel). In this work, we conduct a complete theoretical
characterization of the training process of a two-layer ReLU network trained by
Gradient Flow on a linearly separable data. In this specific setting, our
analysis captures the whole optimization process starting from random
initialization to final convergence. Despite the relatively simple model and
data that we studied, we reveal four different phases from the whole training
process showing a general simplifying-to-complicating learning trend. Specific
nonlinear behaviors can also be precisely identified and captured
theoretically, such as initial condensation, saddle-to-plateau dynamics,
plateau escape, changes of activation patterns, learning with increasing
complexity, etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mingze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chao Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14381">
<title>Connecting Multi-modal Contrastive Representations. (arXiv:2305.14381v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14381</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal Contrastive Representation learning aims to encode different
modalities into a semantically aligned shared space. This paradigm shows
remarkable generalization ability on numerous downstream tasks across various
modalities. However, the reliance on massive high-quality data pairs limits its
further development on more modalities. This paper proposes a novel
training-efficient method for learning MCR without paired data called
Connecting Multi-modal Contrastive Representations (C-MCR). Specifically, given
two existing MCRs pre-trained on (A, B) and (B, C) modality pairs, we project
them to a new space and use the data from the overlapping modality B to
aligning the two MCRs in the new space. Meanwhile, since the modality pairs (A,
B) and (B, C) are already aligned within each MCR, the connection learned by
overlapping modality can also be transferred to non-overlapping modality pair
(A, C). To unleash the potential of C-MCR, we further introduce a
semantic-enhanced inter- and intra-MCR connection method. We first enhance the
semantic consistency and completion of embeddings across different modalities
for more robust alignment. Then we utilize the inter-MCR alignment to establish
the connection, and employ the intra-MCR alignment to better maintain the
connection for inputs from non-overlapping modalities. To demonstrate the
effectiveness of C-MCR, we connect CLIP and CLAP via texts to derive
audio-visual representations, and integrate CLIP and ULIP via images for
3D-language representations. Remarkably, without using any paired data, C-MCR
for audio-visual achieves state-of-the-art performance on audio-image
retrieval, audio-visual source localization, and counterfactual audio-image
recognition tasks. Furthermore, C-MCR for 3D-language also attains advanced
zero-shot 3D point cloud classification accuracy on ModelNet40.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zehan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xize Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haifeng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiageng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Li Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linjun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yongqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_A/0/1/0/all/0/1&quot;&gt;Aoxiong Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Ziang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhou Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15118">
<title>Fairness in Streaming Submodular Maximization over a Matroid Constraint. (arXiv:2305.15118v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15118</link>
<description rdf:parseType="Literal">&lt;p&gt;Streaming submodular maximization is a natural model for the task of
selecting a representative subset from a large-scale dataset. If datapoints
have sensitive attributes such as gender or race, it becomes important to
enforce fairness to avoid bias and discrimination. This has spurred significant
interest in developing fair machine learning algorithms. Recently, such
algorithms have been developed for monotone submodular maximization under a
cardinality constraint.
&lt;/p&gt;
&lt;p&gt;In this paper, we study the natural generalization of this problem to a
matroid constraint. We give streaming algorithms as well as impossibility
results that provide trade-offs between efficiency, quality and fairness. We
validate our findings empirically on a range of well-known real-world
applications: exemplar-based clustering, movie recommendation, and maximum
coverage in social networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halabi_M/0/1/0/all/0/1&quot;&gt;Marwa El Halabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fusco_F/0/1/0/all/0/1&quot;&gt;Federico Fusco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norouzi_Fard_A/0/1/0/all/0/1&quot;&gt;Ashkan Norouzi-Fard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tardos_J/0/1/0/all/0/1&quot;&gt;Jakab Tardos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarnawski_J/0/1/0/all/0/1&quot;&gt;Jakub Tarnawski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15538">
<title>Post-processing Private Synthetic Data for Improving Utility on Selected Measures. (arXiv:2305.15538v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15538</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing private synthetic data generation algorithms are agnostic to
downstream tasks. However, end users may have specific requirements that the
synthetic data must satisfy. Failure to meet these requirements could
significantly reduce the utility of the data for downstream use. We introduce a
post-processing technique that improves the utility of the synthetic data with
respect to measures selected by the end user, while preserving strong privacy
guarantees and dataset quality. Our technique involves resampling from the
synthetic data to filter out samples that do not meet the selected utility
measures, using an efficient stochastic first-order algorithm to find optimal
resampling weights. Through comprehensive numerical experiments, we demonstrate
that our approach consistently improves the utility of synthetic data across
multiple benchmark datasets and state-of-the-art synthetic data generation
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sudalairaj_S/0/1/0/all/0/1&quot;&gt;Shivchander Sudalairaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henning_J/0/1/0/all/0/1&quot;&gt;John Henning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greenewald_K/0/1/0/all/0/1&quot;&gt;Kristjan Greenewald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1&quot;&gt;Akash Srivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16291">
<title>Voyager: An Open-Ended Embodied Agent with Large Language Models. (arXiv:2305.16291v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16291</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Voyager, the first LLM-powered embodied lifelong learning agent
in Minecraft that continuously explores the world, acquires diverse skills, and
makes novel discoveries without human intervention. Voyager consists of three
key components: 1) an automatic curriculum that maximizes exploration, 2) an
ever-growing skill library of executable code for storing and retrieving
complex behaviors, and 3) a new iterative prompting mechanism that incorporates
environment feedback, execution errors, and self-verification for program
improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses
the need for model parameter fine-tuning. The skills developed by Voyager are
temporally extended, interpretable, and compositional, which compounds the
agent&apos;s abilities rapidly and alleviates catastrophic forgetting. Empirically,
Voyager shows strong in-context lifelong learning capability and exhibits
exceptional proficiency in playing Minecraft. It obtains 3.3x more unique
items, travels 2.3x longer distances, and unlocks key tech tree milestones up
to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill
library in a new Minecraft world to solve novel tasks from scratch, while other
techniques struggle to generalize. We open-source our full codebase and prompts
at https://voyager.minedojo.org/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guanzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yuqi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yunfan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandlekar_A/0/1/0/all/0/1&quot;&gt;Ajay Mandlekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuke Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Linxi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1&quot;&gt;Anima Anandkumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16546">
<title>Preliminary studies: Comparing LSTM and BLSTM Deep Neural Networks for Power Consumption Prediction. (arXiv:2305.16546v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16546</link>
<description rdf:parseType="Literal">&lt;p&gt;Electric consumption prediction methods are investigated for many reasons
such as decision-making related to energy efficiency as well as for
anticipating demand in the energy market dynamics. The objective of the present
work is the comparison between two Deep Learning models, namely the Long
Short-Term Memory (LSTM) and Bi-directional LSTM (BLSTM) for univariate
electric consumption Time Series (TS) short-term forecast. The Data Sets (DSs)
were selected for their different contexts and scales, aiming the assessment of
the models&apos; robustness. Four DSs were used, related to the power consumption
of: (a) a household in France; (b) a university building in Santar\&apos;em, Brazil;
(c) the T\&apos;etouan city zones, in Morocco; and (c) the Singapore aggregated
electric demand. The metrics RMSE, MAE, MAPE and R2 were calculated in a TS
cross-validation scheme. The Friedman&apos;s test was applied to normalized RMSE
(NRMSE) results, showing that BLSTM outperforms LSTM with statistically
significant difference (p = 0.0455), corroborating the fact that bidirectional
weight updating improves significantly the LSTM performance concerning
different scales of electric power consumption.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_D/0/1/0/all/0/1&quot;&gt;Davi Guimar&amp;#xe3;es da Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meneses_A/0/1/0/all/0/1&quot;&gt;Anderson Alvarenga de Moura Meneses&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19435">
<title>AdANNS: A Framework for Adaptive Semantic Search. (arXiv:2305.19435v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19435</link>
<description rdf:parseType="Literal">&lt;p&gt;Web-scale search systems learn an encoder to embed a given query which is
then hooked into an approximate nearest neighbor search (ANNS) pipeline to
retrieve similar data points. To accurately capture tail queries and data
points, learned representations typically are rigid, high-dimensional vectors
that are generally used as-is in the entire ANNS pipeline and can lead to
computationally expensive retrieval. In this paper, we argue that instead of
rigid representations, different stages of ANNS can leverage adaptive
representations of varying capacities to achieve significantly better
accuracy-compute trade-offs, i.e., stages of ANNS that can get away with more
approximate computation should use a lower-capacity representation of the same
data point. To this end, we introduce AdANNS, a novel ANNS design framework
that explicitly leverages the flexibility of Matryoshka Representations. We
demonstrate state-of-the-art accuracy-compute trade-offs using novel
AdANNS-based key ANNS building blocks like search data structures (AdANNS-IVF)
and quantization (AdANNS-OPQ). For example on ImageNet retrieval, AdANNS-IVF is
up to 1.5% more accurate than the rigid representations-based IVF at the same
compute budget; and matches accuracy while being up to 90x faster in wall-clock
time. For Natural Questions, 32-byte AdANNS-OPQ matches the accuracy of the
64-byte OPQ baseline constructed using rigid representations -- same accuracy
at half the cost! We further show that the gains from AdANNS translate to
modern-day composite ANNS indices that combine search structures and
quantization. Finally, we demonstrate that AdANNS can enable inference-time
adaptivity for compute-aware search on ANNS indices built non-adaptively on
matryoshka representations. Code is open-sourced at
https://github.com/RAIVNLab/AdANNS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rege_A/0/1/0/all/0/1&quot;&gt;Aniket Rege&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1&quot;&gt;Aditya Kusupati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+S_S/0/1/0/all/0/1&quot;&gt;Sharan Ranjit S&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1&quot;&gt;Alan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1&quot;&gt;Qingqing Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1&quot;&gt;Sham Kakade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1&quot;&gt;Prateek Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1&quot;&gt;Ali Farhadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19713">
<title>Red Teaming Language Model Detectors with Language Models. (arXiv:2305.19713v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19713</link>
<description rdf:parseType="Literal">&lt;p&gt;The prevalence and strong capability of large language models (LLMs) present
significant safety and ethical risks if exploited by malicious users. To
prevent the potentially deceptive usage of LLMs, recent works have proposed
algorithms to detect LLM-generated text and protect LLMs. In this paper, we
investigate the robustness and reliability of these LLM detectors under
adversarial attacks. We study two types of attack strategies: 1) replacing
certain words in an LLM&apos;s output with their synonyms given the context; 2)
automatically searching for an instructional prompt to alter the writing style
of the generation. In both strategies, we leverage an auxiliary LLM to generate
the word replacements or the instructional prompt. Different from previous
works, we consider a challenging setting where the auxiliary LLM can also be
protected by a detector. Experiments reveal that our attacks effectively
compromise the performance of all detectors in the study with plausible
generations, underscoring the urgent need to improve the robustness of
LLM-generated text detection systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhouxing Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yihan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1&quot;&gt;Fan Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiangning Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00477">
<title>Make Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00477</link>
<description rdf:parseType="Literal">&lt;p&gt;Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs)
has emerged as a highly successful approach, with training only a small number
of parameters without sacrificing performance and becoming the de-facto
learning paradigm with the increasing size of PLMs. However, existing PEFT
methods are not memory-efficient, because they still require caching most of
the intermediate activations for the gradient calculation, akin to fine-tuning.
One effective way to reduce the activation memory is to apply a reversible
model, so the intermediate activations are not necessary to be cached and can
be recomputed. Nevertheless, modifying a PLM to its reversible variant is not
straightforward, since the reversible model has a distinct architecture from
the currently released PLMs. In this paper, we first investigate what is a key
factor for the success of existing PEFT methods, and realize that it&apos;s
essential to preserve the PLM&apos;s starting point when initializing a PEFT method.
With this finding, we propose memory-efficient fine-tuning (MEFT) that inserts
adapters into a PLM, preserving the PLM&apos;s starting point and making it
reversible without additional pre-training. We evaluate MEFT on the GLUE
benchmark and five question-answering tasks with various backbones, BERT,
RoBERTa, BART and OPT. MEFT significantly reduces the activation memory up to
84% of full fine-tuning with a negligible amount of trainable parameters.
Moreover, MEFT achieves the same score on GLUE and a comparable score on the
question-answering tasks as full fine-tuning. A similar finding is also
observed for the image classification task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1&quot;&gt;Baohao Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1&quot;&gt;Shaomu Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monz_C/0/1/0/all/0/1&quot;&gt;Christof Monz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03659">
<title>Schema First! Learn Versatile Knowledge Graph Embeddings by Capturing Semantics with MASCHInE. (arXiv:2306.03659v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03659</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graph embedding models (KGEMs) have gained considerable traction in
recent years. These models learn a vector representation of knowledge graph
entities and relations, a.k.a. knowledge graph embeddings (KGEs). Learning
versatile KGEs is desirable as it makes them useful for a broad range of tasks.
However, KGEMs are usually trained for a specific task, which makes their
embeddings task-dependent. In parallel, the widespread assumption that KGEMs
actually create a semantic representation of the underlying entities and
relations (e.g., project similar entities closer than dissimilar ones) has been
challenged. In this work, we design heuristics for generating protographs --
small, modified versions of a KG that leverage RDF/S information. The learnt
protograph-based embeddings are meant to encapsulate the semantics of a KG, and
can be leveraged in learning KGEs that, in turn, also better capture semantics.
Extensive experiments on various evaluation benchmarks demonstrate the
soundness of this approach, which we call Modular and Agnostic SCHema-based
Integration of protograph Embeddings (MASCHInE). In particular, MASCHInE helps
produce more versatile KGEs that yield substantially better performance for
entity clustering and node classification tasks. For link prediction, using
MASCHinE substantially increases the number of semantically valid predictions
with equivalent rank-based performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hubert_N/0/1/0/all/0/1&quot;&gt;Nicolas Hubert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1&quot;&gt;Heiko Paulheim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monnin_P/0/1/0/all/0/1&quot;&gt;Pierre Monnin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brun_A/0/1/0/all/0/1&quot;&gt;Armelle Brun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monticolo_D/0/1/0/all/0/1&quot;&gt;Davy Monticolo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04542">
<title>On the Design Fundamentals of Diffusion Models: A Survey. (arXiv:2306.04542v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04542</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models are generative models, which gradually add and remove noise
to learn the underlying distribution of training data for data generation. The
components of diffusion models have gained significant attention with many
design choices proposed. Existing reviews have primarily focused on
higher-level solutions, thereby covering less on the design fundamentals of
components. This study seeks to address this gap by providing a comprehensive
and coherent review on component-wise design choices in diffusion models.
Specifically, we organize this review according to their three key components,
namely the forward process, the reverse process, and the sampling procedure.
This allows us to provide a fine-grained perspective of diffusion models,
benefiting future studies in the analysis of individual components, the
applicability of design choices, and the implementation of diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Z/0/1/0/all/0/1&quot;&gt;Ziyi Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koulieris_G/0/1/0/all/0/1&quot;&gt;George Alex Koulieris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1&quot;&gt;Hubert P. H. Shum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06344">
<title>Language-Guided Traffic Simulation via Scene-Level Diffusion. (arXiv:2306.06344v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06344</link>
<description rdf:parseType="Literal">&lt;p&gt;Realistic and controllable traffic simulation is a core capability that is
necessary to accelerate autonomous vehicle (AV) development. However, current
approaches for controlling learning-based traffic models require significant
domain expertise and are difficult for practitioners to use. To remedy this, we
present CTG++, a scene-level conditional diffusion model that can be guided by
language instructions. Developing this requires tackling two challenges: the
need for a realistic and controllable traffic model backbone, and an effective
method to interface with a traffic model using language. To address these
challenges, we first propose a scene-level diffusion model equipped with a
spatio-temporal transformer backbone, which generates realistic and
controllable traffic. We then harness a large language model (LLM) to convert a
user&apos;s query into a loss function, guiding the diffusion model towards
query-compliant generation. Through comprehensive evaluation, we demonstrate
the effectiveness of our proposed method in generating realistic,
query-compliant traffic simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1&quot;&gt;Ziyuan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rempe_D/0/1/0/all/0/1&quot;&gt;Davis Rempe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ivanovic_B/0/1/0/all/0/1&quot;&gt;Boris Ivanovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yulong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Danfei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1&quot;&gt;Marco Pavone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1&quot;&gt;Baishakhi Ray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06599">
<title>Variational Imbalanced Regression: Fair Uncertainty Quantification via Probabilistic Smoothing. (arXiv:2306.06599v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06599</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing regression models tend to fall short in both accuracy and
uncertainty estimation when the label distribution is imbalanced. In this
paper, we propose a probabilistic deep learning model, dubbed variational
imbalanced regression (VIR), which not only performs well in imbalanced
regression but naturally produces reasonable uncertainty estimation as a
byproduct. Different from typical variational autoencoders assuming I.I.D.
representations (a data point&apos;s representation is not directly affected by
other data points), our VIR borrows data with similar regression labels to
compute the latent representation&apos;s variational distribution; furthermore,
different from deterministic regression models producing point estimates, VIR
predicts the entire normal-inverse-gamma distributions and modulates the
associated conjugate distributions to impose probabilistic reweighting on the
imbalanced data, thereby providing better uncertainty estimation. Experiments
in several real-world datasets show that our VIR can outperform
state-of-the-art imbalanced regression models in terms of both accuracy and
uncertainty estimation. Code will soon be available at
https://github.com/Wang-ML-Lab/variational-imbalanced-regression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06798">
<title>Kepler: Robust Learning for Faster Parametric Query Optimization. (arXiv:2306.06798v2 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06798</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing parametric query optimization (PQO) techniques rely on
traditional query optimizer cost models, which are often inaccurate and result
in suboptimal query performance. We propose Kepler, an end-to-end
learning-based approach to PQO that demonstrates significant speedups in query
latency over a traditional query optimizer. Central to our method is Row Count
Evolution (RCE), a novel plan generation algorithm based on perturbations in
the sub-plan cardinality space. While previous approaches require accurate cost
models, we bypass this requirement by evaluating candidate plans via actual
execution data and training an ML model to predict the fastest plan given
parameter binding values. Our models leverage recent advances in neural network
uncertainty in order to robustly predict faster plans while avoiding
regressions in query performance. Experimentally, we show that Kepler achieves
significant improvements in query runtime on multiple datasets on PostgreSQL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_L/0/1/0/all/0/1&quot;&gt;Lyric Doshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_V/0/1/0/all/0/1&quot;&gt;Vincent Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_G/0/1/0/all/0/1&quot;&gt;Gaurav Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marcus_R/0/1/0/all/0/1&quot;&gt;Ryan Marcus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haoyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altinbuken_D/0/1/0/all/0/1&quot;&gt;Deniz Altinb&amp;#xfc;ken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brevdo_E/0/1/0/all/0/1&quot;&gt;Eugene Brevdo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fraser_C/0/1/0/all/0/1&quot;&gt;Campbell Fraser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07960">
<title>Symmetric Neural-Collapse Representations with Supervised Contrastive Loss: The Impact of ReLU and Batching. (arXiv:2306.07960v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07960</link>
<description rdf:parseType="Literal">&lt;p&gt;Supervised contrastive loss (SCL) is a competitive and often superior
alternative to the cross-entropy loss for classification. While prior studies
have demonstrated that both losses yield symmetric training representations
under balanced data, this symmetry breaks under class imbalances. This paper
presents an intriguing discovery: the introduction of a ReLU activation at the
final layer effectively restores the symmetry in SCL-learned representations.
We arrive at this finding analytically, by establishing that the global
minimizers of an unconstrained features model with SCL loss and entry-wise
non-negativity constraints form an orthogonal frame. Extensive experiments
conducted across various datasets, architectures, and imbalance scenarios
corroborate our finding. Importantly, our experiments reveal that the inclusion
of the ReLU activation restores symmetry without compromising test accuracy.
This constitutes the first geometry characterization of SCL under imbalances.
Additionally, our analysis and experiments underscore the pivotal role of batch
selection strategies in representation geometry. By proving necessary and
sufficient conditions for mini-batch choices that ensure invariant symmetric
representations, we introduce batch-binding as an efficient strategy that
guarantees these conditions hold.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kini_G/0/1/0/all/0/1&quot;&gt;Ganesh Ramachandra Kini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vakilian_V/0/1/0/all/0/1&quot;&gt;Vala Vakilian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behnia_T/0/1/0/all/0/1&quot;&gt;Tina Behnia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gill_J/0/1/0/all/0/1&quot;&gt;Jaidev Gill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thrampoulidis_C/0/1/0/all/0/1&quot;&gt;Christos Thrampoulidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08141">
<title>ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations. (arXiv:2306.08141v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08141</link>
<description rdf:parseType="Literal">&lt;p&gt;As generative AI becomes more prevalent, it is important to study how human
users interact with such models. In this work, we investigate how people use
text-to-image models to generate desired target images. To study this
interaction, we created ArtWhisperer, an online game where users are given a
target image and are tasked with iteratively finding a prompt that creates a
similar-looking image as the target. Through this game, we recorded over 50,000
human-AI interactions; each interaction corresponds to one text prompt created
by a user and the corresponding generated image. The majority of these are
repeated interactions where a user iterates to find the best prompt for their
target image, making this a unique sequential dataset for studying human-AI
collaborations. In an initial analysis of this dataset, we identify several
characteristics of prompt interactions and user strategies. People submit
diverse prompts and are able to discover a variety of text descriptions that
generate similar images. Interestingly, prompt diversity does not decrease as
users find better prompts. We further propose a new metric to quantify the
steerability of AI using our dataset. We define steerability as the expected
number of interactions required to adequately complete a task. We estimate this
value by fitting a Markov chain for each target task and calculating the
expected time to reach an adequate score in the Markov chain. We quantify and
compare AI steerability across different types of target images and two
different models, finding that images of cities and natural world images are
more steerable than artistic and fantasy images. These findings provide
insights into human-AI interaction behavior, present a concrete method of
assessing AI steerability, and demonstrate the general utility of the
ArtWhisperer dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vodrahalli_K/0/1/0/all/0/1&quot;&gt;Kailas Vodrahalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08670">
<title>The Power of Populations in Decentralized Learning Dynamics. (arXiv:2306.08670v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08670</link>
<description rdf:parseType="Literal">&lt;p&gt;We study a distributed multi-armed bandit setting among a population of $n$
memory-constrained nodes in the gossip model: at each round, every node locally
adopts one of $m$ arms, observes a reward drawn from the arm&apos;s (adversarially
chosen) distribution, and then communicates with a randomly sampled neighbor,
exchanging information to determine its policy in the next round. We introduce
and analyze several families of dynamics for this task that are decentralized:
each node&apos;s decision is entirely local and depends only on its most recently
obtained reward and that of the neighbor it sampled. We show a connection
between the global evolution of these decentralized dynamics with a certain
class of &quot;zero-sum&quot; multiplicative weights update algorithms, and we develop a
general framework for analyzing the population-level regret of these natural
protocols. Using this framework, we derive sublinear regret bounds under a wide
range of parameter regimes (i.e., the size of the population and number of
arms) for both the stationary reward setting (where the mean of each arm&apos;s
distribution is fixed over time) and the adversarial reward setting (where
means can vary over time). Further, we show that these protocols can
approximately optimize convex functions over the simplex when the reward
distributions are generated from a stochastic gradient oracle.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazarsfeld_J/0/1/0/all/0/1&quot;&gt;John Lazarsfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1&quot;&gt;Dan Alistarh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09983">
<title>Evaluating Superhuman Models with Consistency Checks. (arXiv:2306.09983v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09983</link>
<description rdf:parseType="Literal">&lt;p&gt;If machine learning models were to achieve superhuman abilities at various
reasoning or decision-making tasks, how would we go about evaluating such
models, given that humans would necessarily be poor proxies for ground truth?
In this paper, we propose a framework for evaluating superhuman models via
consistency checks. Our premise is that while the correctness of superhuman
decisions may be impossible to evaluate, we can still surface mistakes if the
model&apos;s decisions fail to satisfy certain logical, human-interpretable rules.
We instantiate our framework on three tasks where correctness of decisions is
hard to evaluate due to either superhuman model abilities, or to otherwise
missing ground truth: evaluating chess positions, forecasting future events,
and making legal judgments. We show that regardless of a model&apos;s (possibly
superhuman) performance on these tasks, we can discover logical inconsistencies
in decision making. For example: a chess engine assigning opposing valuations
to semantically identical boards; GPT-4 forecasting that sports records will
evolve non-monotonically over time; or an AI judge assigning bail to a
defendant only after we add a felony to their criminal record.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fluri_L/0/1/0/all/0/1&quot;&gt;Lukas Fluri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paleka_D/0/1/0/all/0/1&quot;&gt;Daniel Paleka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1&quot;&gt;Florian Tram&amp;#xe8;r&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11586">
<title>Provably Powerful Graph Neural Networks for Directed Multigraphs. (arXiv:2306.11586v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11586</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper analyses a set of simple adaptations that transform standard
message-passing Graph Neural Networks (GNN) into provably powerful directed
multigraph neural networks. The adaptations include multigraph port numbering,
ego IDs, and reverse message passing. We prove that the combination of these
theoretically enables the detection of any directed subgraph pattern. To
validate the effectiveness of our proposed adaptations in practice, we conduct
experiments on synthetic subgraph detection tasks, which demonstrate
outstanding performance with almost perfect results. Moreover, we apply our
proposed adaptations to two financial crime analysis tasks. We observe dramatic
improvements in detecting money laundering transactions, improving the
minority-class F1 score of a standard message-passing GNN by up to 30%, and
closely matching or outperforming tree-based and GNN baselines. Similarly
impressive results are observed on a real-world phishing detection dataset,
boosting three standard GNNs&apos; F1 scores by around 15% and outperforming all
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Egressy_B/0/1/0/all/0/1&quot;&gt;B&amp;#xe9;ni Egressy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niederhausern_L/0/1/0/all/0/1&quot;&gt;Luc von Niederh&amp;#xe4;usern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blanusa_J/0/1/0/all/0/1&quot;&gt;Jovan Blanusa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altman_E/0/1/0/all/0/1&quot;&gt;Erik Altman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1&quot;&gt;Roger Wattenhofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atasu_K/0/1/0/all/0/1&quot;&gt;Kubilay Atasu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12129">
<title>Machine Learning Based Compensation for Inconsistencies in Knitted Force Sensors. (arXiv:2306.12129v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12129</link>
<description rdf:parseType="Literal">&lt;p&gt;Knitted sensors frequently suffer from inconsistencies due to innate effects
such as offset, relaxation, and drift. These properties, in combination, make
it challenging to reliably map from sensor data to physical actuation. In this
paper, we demonstrate a method for counteracting this by applying processing
using a minimal artificial neural network (ANN) in combination with
straightforward pre-processing. We apply a number of exponential smoothing
filters on a re-sampled sensor signal, to produce features that preserve
different levels of historical sensor data and, in combination, represent an
adequate state of previous sensor actuation. By training a three-layer ANN with
a total of 8 neurons, we manage to significantly improve the mapping between
sensor reading and actuation force. Our findings also show that our technique
translates to sensors of reasonably different composition in terms of material
and structure, and it can furthermore be applied to related physical features
such as strain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aigner_R/0/1/0/all/0/1&quot;&gt;Roland Aigner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Stockl_A/0/1/0/all/0/1&quot;&gt;Andreas St&amp;#xf6;ckl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15687">
<title>Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale. (arXiv:2306.15687v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15687</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale generative models such as GPT and DALL-E have revolutionized the
research community. These models not only generate high fidelity outputs, but
are also generalists which can solve tasks not explicitly taught. In contrast,
speech generative models are still primitive in terms of scale and task
generalization. In this paper, we present Voicebox, the most versatile
text-guided generative model for speech at scale. Voicebox is a
non-autoregressive flow-matching model trained to infill speech, given audio
context and text, trained on over 50K hours of speech that are not filtered or
enhanced. Similar to GPT, Voicebox can perform many different tasks through
in-context learning, but is more flexible as it can also condition on future
context. Voicebox can be used for mono or cross-lingual zero-shot
text-to-speech synthesis, noise removal, content editing, style conversion, and
diverse sample generation. In particular, Voicebox outperforms the
state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs
1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to
20 times faster. Audio samples can be found in
\url{https://voicebox.metademolab.com}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Le_M/0/1/0/all/0/1&quot;&gt;Matthew Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vyas_A/0/1/0/all/0/1&quot;&gt;Apoorv Vyas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bowen Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Karrer_B/0/1/0/all/0/1&quot;&gt;Brian Karrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sari_L/0/1/0/all/0/1&quot;&gt;Leda Sari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moritz_R/0/1/0/all/0/1&quot;&gt;Rashel Moritz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Williamson_M/0/1/0/all/0/1&quot;&gt;Mary Williamson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Manohar_V/0/1/0/all/0/1&quot;&gt;Vimal Manohar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Adi_Y/0/1/0/all/0/1&quot;&gt;Yossi Adi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mahadeokar_J/0/1/0/all/0/1&quot;&gt;Jay Mahadeokar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1&quot;&gt;Wei-Ning Hsu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03810">
<title>URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates. (arXiv:2307.03810v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03810</link>
<description rdf:parseType="Literal">&lt;p&gt;Representation learning has significantly driven the field to develop
pretrained models that can act as a valuable starting point when transferring
to new datasets. With the rising demand for reliable machine learning and
uncertainty quantification, there is a need for pretrained models that not only
provide embeddings but also transferable uncertainty estimates. To guide the
development of such models, we propose the Uncertainty-aware Representation
Learning (URL) benchmark. Besides the transferability of the representations,
it also measures the zero-shot transferability of the uncertainty estimate
using a novel metric. We apply URL to evaluate eleven uncertainty quantifiers
that are pretrained on ImageNet and transferred to eight downstream datasets.
We find that approaches that focus on the uncertainty of the representation
itself or estimate the prediction risk directly outperform those that are based
on the probabilities of upstream classes. Yet, achieving transferable
uncertainty quantification remains an open challenge. Our findings indicate
that it is not necessarily in conflict with traditional representation learning
goals. Code is provided under https://github.com/mkirchhof/url .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirchhof_M/0/1/0/all/0/1&quot;&gt;Michael Kirchhof&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mucsanyi_B/0/1/0/all/0/1&quot;&gt;B&amp;#xe1;lint Mucs&amp;#xe1;nyi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Seong Joon Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1&quot;&gt;Enkelejda Kasneci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04228">
<title>Bayesian tomography using polynomial chaos expansion and deep generative networks. (arXiv:2307.04228v4 [physics.geo-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04228</link>
<description rdf:parseType="Literal">&lt;p&gt;Implementations of Markov chain Monte Carlo (MCMC) methods need to confront
two fundamental challenges: accurate representation of prior information and
efficient evaluation of likelihoods. Principal component analysis (PCA) and
related techniques can in some cases facilitate the definition and sampling of
the prior distribution, as well as the training of accurate surrogate models,
using for instance, polynomial chaos expansion (PCE). However, complex
geological priors with sharp contrasts necessitate more complex
dimensionality-reduction techniques, such as, deep generative models (DGMs). By
sampling a low-dimensional prior probability distribution defined in the
low-dimensional latent space of such a model, it becomes possible to
efficiently sample the physical domain at the price of a generator that is
typically highly non-linear. Training a surrogate that is capable of capturing
intricate non-linear relationships between latent parameters and outputs of
forward modeling presents a notable challenge. Indeed, while PCE models provide
high accuracy when the input-output relationship can be effectively
approximated by relatively low-degree multivariate polynomials, this condition
is typically not met when employing latent variables derived from DGMs. In this
contribution, we present a strategy combining the excellent reconstruction
performances of a variational autoencoder (VAE) with the accuracy of PCA-PCE
surrogate modeling in the context of Bayesian ground penetrating radar (GPR)
traveltime tomography. Within the MCMC process, the parametrization of the VAE
is leveraged for prior exploration and sample proposals. Concurrently,
surrogate modeling is conducted using PCE, which operates on either globally or
locally defined principal components of the VAE samples under examination.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Meles_G/0/1/0/all/0/1&quot;&gt;Giovanni Angelo Meles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Amaya_M/0/1/0/all/0/1&quot;&gt;Macarena Amaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Levy_S/0/1/0/all/0/1&quot;&gt;Shiran Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Marelli_S/0/1/0/all/0/1&quot;&gt;Stefano Marelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Linde_N/0/1/0/all/0/1&quot;&gt;Niklas Linde&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04661">
<title>On the power of graph neural networks and the role of the activation function. (arXiv:2307.04661v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04661</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article we present new results about the expressivity of Graph Neural
Networks (GNNs). We prove that for any GNN with piecewise polynomial
activations, whose architecture size does not grow with the graph input sizes,
there exists a pair of non-isomorphic rooted trees of depth two such that the
GNN cannot distinguish their root vertex up to an arbitrary number of
iterations. The proof relies on tools from the algebra of symmetric
polynomials. In contrast, it was already known that unbounded GNNs (those whose
size is allowed to change with the graph sizes) with piecewise polynomial
activations can distinguish these vertices in only two iterations. Our results
imply a strict separation between bounded and unbounded size GNNs, answering an
open question formulated by [Grohe, 2021]. We next prove that if one allows
activations that are not piecewise polynomial, then in two iterations a single
neuron perceptron can distinguish the root vertices of any pair of
nonisomorphic trees of depth two (our results hold for activations like the
sigmoid, hyperbolic tan and others). This shows how the power of graph neural
networks can change drastically if one changes the activation function of the
neural networks. The proof of this result utilizes the Lindemann-Weierstrauss
theorem from transcendental number theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalife_S/0/1/0/all/0/1&quot;&gt;Sammy Khalife&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1&quot;&gt;Amitabh Basu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05141">
<title>Deep Probabilistic Movement Primitives with a Bayesian Aggregator. (arXiv:2307.05141v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05141</link>
<description rdf:parseType="Literal">&lt;p&gt;Movement primitives are trainable parametric models that reproduce robotic
movements starting from a limited set of demonstrations. Previous works
proposed simple linear models that exhibited high sample efficiency and
generalization power by allowing temporal modulation of movements (reproducing
movements faster or slower), blending (merging two movements into one),
via-point conditioning (constraining a movement to meet some particular
via-points) and context conditioning (generation of movements based on an
observed variable, e.g., position of an object). Previous works have proposed
neural network-based motor primitive models, having demonstrated their capacity
to perform tasks with some forms of input conditioning or time-modulation
representations. However, there has not been a single unified deep motor
primitive&apos;s model proposed that is capable of all previous operations, limiting
neural motor primitive&apos;s potential applications. This paper proposes a deep
movement primitive architecture that encodes all the operations above and uses
a Bayesian context aggregator that allows a more sound context conditioning and
blending. Our results demonstrate our approach can scale to reproduce complex
motions on a larger variety of input choices compared to baselines while
maintaining operations of linear movement primitives provide.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Przystupa_M/0/1/0/all/0/1&quot;&gt;Michael Przystupa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haghverd_F/0/1/0/all/0/1&quot;&gt;Faezeh Haghverd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jagersand_M/0/1/0/all/0/1&quot;&gt;Martin Jagersand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tosatto_S/0/1/0/all/0/1&quot;&gt;Samuele Tosatto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06125">
<title>Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation. (arXiv:2307.06125v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06125</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing object-search approaches enable robots to search through free
pathways, however, robots operating in unstructured human-centered environments
frequently also have to manipulate the environment to their needs. In this
work, we introduce a novel interactive multi-object search task in which a
robot has to open doors to navigate rooms and search inside cabinets and
drawers to find target objects. These new challenges require combining
manipulation and navigation skills in unexplored environments. We present
HIMOS, a hierarchical reinforcement learning approach that learns to compose
exploration, navigation, and manipulation skills. To achieve this, we design an
abstract high-level action space around a semantic map memory and leverage the
explored environment as instance navigation points. We perform extensive
experiments in simulation and the real world that demonstrate that, with
accurate perception, the decision making of HIMOS effectively transfers to new
environments in a zero-shot manner. It shows robustness to unseen subpolicies,
failures in their execution, and different robot kinematics. These capabilities
open the door to a wide range of downstream tasks across embodied AI and
real-world use cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmalstieg_F/0/1/0/all/0/1&quot;&gt;Fabian Schmalstieg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honerkamp_D/0/1/0/all/0/1&quot;&gt;Daniel Honerkamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welschehold_T/0/1/0/all/0/1&quot;&gt;Tim Welschehold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1&quot;&gt;Abhinav Valada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09688">
<title>Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation. (arXiv:2307.09688v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09688</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling customer shopping intentions is a crucial task for e-commerce, as it
directly impacts user experience and engagement. Thus, accurately understanding
customer preferences is essential for providing personalized recommendations.
Session-based recommendation, which utilizes customer session data to predict
their next interaction, has become increasingly popular. However, existing
session datasets have limitations in terms of item attributes, user diversity,
and dataset scale. As a result, they cannot comprehensively capture the
spectrum of user behaviors and preferences. To bridge this gap, we present the
Amazon Multilingual Multi-locale Shopping Session Dataset, namely Amazon-M2. It
is the first multilingual dataset consisting of millions of user sessions from
six different locales, where the major languages of products are English,
German, Japanese, French, Italian, and Spanish. Remarkably, the dataset can
help us enhance personalization and understanding of user preferences, which
can benefit various existing tasks as well as enable new tasks. To test the
potential of the dataset, we introduce three tasks in this work: (1)
next-product recommendation, (2) next-product recommendation with domain
shifts, and (3) next-product title generation. With the above tasks, we
benchmark a range of algorithms on our proposed dataset, drawing new insights
for further research and practice. In addition, based on the proposed dataset
and tasks, we hosted a competition in the KDD CUP 2023 and have attracted
thousands of users and submissions. The winning solutions and the associated
workshop can be accessed at our website https://kddcup23.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1&quot;&gt;Wei Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1&quot;&gt;Haitao Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Haoming Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hongzhi Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1&quot;&gt;Haoyu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hanqing Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhengyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruirui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Monica Xiao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goutam_R/0/1/0/all/0/1&quot;&gt;Rahul Goutam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haiyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subbian_K/0/1/0/all/0/1&quot;&gt;Karthik Subbian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Suhang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yizhou Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiliang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1&quot;&gt;Bing Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xianfeng Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06368">
<title>Topic-Level Bayesian Surprise and Serendipity for Recommender Systems. (arXiv:2308.06368v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06368</link>
<description rdf:parseType="Literal">&lt;p&gt;A recommender system that optimizes its recommendations solely to fit a
user&apos;s history of ratings for consumed items can create a filter bubble,
wherein the user does not get to experience items from novel, unseen
categories. One approach to mitigate this undesired behavior is to recommend
items with high potential for serendipity, namely surprising items that are
likely to be highly rated. In this paper, we propose a content-based
formulation of serendipity that is rooted in Bayesian surprise and use it to
measure the serendipity of items after they are consumed and rated by the user.
When coupled with a collaborative-filtering component that identifies similar
users, this enables recommending items with high potential for serendipity. To
facilitate the evaluation of topic-level models for surprise and serendipity,
we introduce a dataset of book reading histories extracted from Goodreads,
containing over 26 thousand users and close to 1.3 million books, where we
manually annotate 449 books read by 4 users in terms of their time-dependent,
topic-level surprise. Experimental evaluations show that models that use
Bayesian surprise correlate much better with the manual annotations of
topic-level surprise than distance-based heuristics, and also obtain better
serendipitous item recommendation performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_T/0/1/0/all/0/1&quot;&gt;Tonmoy Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bunescu_R/0/1/0/all/0/1&quot;&gt;Razvan Bunescu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00608">
<title>Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair. (arXiv:2309.00608v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00608</link>
<description rdf:parseType="Literal">&lt;p&gt;During Automated Program Repair (APR), it can be challenging to synthesize
correct patches for real-world systems in general-purpose programming
languages. Recent Large Language Models (LLMs) have been shown to be helpful
&quot;copilots&quot; in assisting developers with various coding tasks, and have also
been directly applied for patch synthesis. However, most LLMs treat programs as
sequences of tokens, meaning that they are ignorant of the underlying semantics
constraints of the target programming language. This results in plenty of
statically invalid generated patches, impeding the practicality of the
technique. Therefore, we propose Repilot, a framework to further copilot the AI
&quot;copilots&quot; (i.e., LLMs) by synthesizing more valid patches during the repair
process. Our key insight is that many LLMs produce outputs autoregressively
(i.e., token by token), resembling human writing programs, which can be
significantly boosted and guided through a Completion Engine. Repilot
synergistically synthesizes a candidate patch through the interaction between
an LLM and a Completion Engine, which 1) prunes away infeasible tokens
suggested by the LLM and 2) proactively completes the token based on the
suggestions provided by the Completion Engine. Our evaluation on a subset of
the widely-used Defects4j 1.2 and 2.0 datasets shows that Repilot fixes 66 and
50 bugs, respectively, surpassing the best-performing baseline by 14 and 16
bugs fixed. More importantly, Repilot is capable of producing more valid and
correct patches than the base LLM when given the same generation budget.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1&quot;&gt;Chunqiu Steven Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lingming Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01319">
<title>An ML-assisted OTFS vs. OFDM adaptable modem. (arXiv:2309.01319v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01319</link>
<description rdf:parseType="Literal">&lt;p&gt;The Orthogonal-Time-Frequency-Space (OTFS) signaling is known to be resilient
to doubly-dispersive channels, which impacts high mobility scenarios. On the
other hand, the Orthogonal-Frequency-Division-Multiplexing (OFDM) waveforms
enjoy the benefits of the reuse of legacy architectures, simplicity of receiver
design, and low-complexity detection. Several studies that compare the
performance of OFDM and OTFS have indicated mixed outcomes due to the plethora
of system parameters at play beyond high-mobility conditions. In this work, we
exemplify this observation using simulations and propose a deep neural network
(DNN)-based adaptation scheme to switch between using either an OTFS or OFDM
signal processing chain at the transmitter and receiver for optimal
mean-squared-error (MSE) performance. The DNN classifier is trained to switch
between the two schemes by observing the channel condition, received SNR, and
modulation format. We compare the performance of the OTFS, OFDM, and the
proposed switched-waveform scheme. The simulations indicate superior
performance with the proposed scheme with a well-trained DNN, thus improving
the MSE performance of the communication significantly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ahmed_I/0/1/0/all/0/1&quot;&gt;I. Zakir Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sadjadpour_H/0/1/0/all/0/1&quot;&gt;Hamid R. Sadjadpour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05270">
<title>CONFLATOR: Incorporating Switching Point based Rotatory Positional Encodings for Code-Mixed Language Modeling. (arXiv:2309.05270v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05270</link>
<description rdf:parseType="Literal">&lt;p&gt;The mixing of two or more languages is called Code-Mixing (CM). CM is a
social norm in multilingual societies. Neural Language Models (NLMs) like
transformers have been effective on many NLP tasks. However, NLM for CM is an
under-explored area. Though transformers are capable and powerful, they cannot
always encode positional information since they are non-recurrent. Therefore,
to enrich word information and incorporate positional information, positional
encoding is defined. We hypothesize that Switching Points (SPs), i.e.,
junctions in the text where the language switches (L1 -&amp;gt; L2 or L2 -&amp;gt; L1), pose
a challenge for CM Language Models (LMs), and hence give special emphasis to
SPs in the modeling process. We experiment with several positional encoding
mechanisms and show that rotatory positional encodings along with switching
point information yield the best results.
&lt;/p&gt;
&lt;p&gt;We introduce CONFLATOR: a neural language modeling approach for code-mixed
languages. CONFLATOR tries to learn to emphasize switching points using smarter
positional encoding, both at unigram and bigram levels. CONFLATOR outperforms
the state-of-the-art on two tasks based on code-mixed Hindi and English
(Hinglish): (i) sentiment analysis and (ii) machine translation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1&quot;&gt;Mohsin Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teja_K/0/1/0/all/0/1&quot;&gt;Kandukuri Sai Teja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1&quot;&gt;Neeharika Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1&quot;&gt;Parth Patwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_A/0/1/0/all/0/1&quot;&gt;Anubhab Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vinija Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1&quot;&gt;Aman Chadha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Amitava Das&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.10194">
<title>The Kernel Density Integral Transformation. (arXiv:2309.10194v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2309.10194</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature preprocessing continues to play a critical role when applying machine
learning and statistical methods to tabular data. In this paper, we propose the
use of the kernel density integral transformation as a feature preprocessing
step. Our approach subsumes the two leading feature preprocessing methods as
limiting cases: linear min-max scaling and quantile transformation. We
demonstrate that, without hyperparameter tuning, the kernel density integral
transformation can be used as a simple drop-in replacement for either method,
offering protection from the weaknesses of each. Alternatively, with tuning of
a single continuous hyperparameter, we frequently outperform both of these
methods. Finally, we show that the kernel density transformation can be
profitably applied to statistical data analysis, particularly in correlation
analysis and univariate clustering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McCarter_C/0/1/0/all/0/1&quot;&gt;Calvin McCarter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12871">
<title>AnglE-optimized Text Embeddings. (arXiv:2309.12871v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12871</link>
<description rdf:parseType="Literal">&lt;p&gt;High-quality text embedding is pivotal in improving semantic textual
similarity (STS) tasks, which are crucial components in Large Language Model
(LLM) applications. However, a common challenge existing text embedding models
face is the problem of vanishing gradients, primarily due to their reliance on
the cosine function in the optimization objective, which has saturation zones.
To address this issue, this paper proposes a novel angle-optimized text
embedding model called AnglE. The core idea of AnglE is to introduce angle
optimization in a complex space. This novel approach effectively mitigates the
adverse effects of the saturation zone in the cosine function, which can impede
gradient and hinder optimization processes. To set up a comprehensive STS
evaluation, we experimented on existing short-text STS datasets and a newly
collected long-text STS dataset from GitHub Issues. Furthermore, we examine
domain-specific STS scenarios with limited labeled data and explore how AnglE
works with LLM-annotated data. Extensive experiments were conducted on various
tasks including short-text STS, long-text STS, and domain-specific STS tasks.
The results show that AnglE outperforms the state-of-the-art (SOTA) STS models
that ignore the cosine saturation zone. These findings demonstrate the ability
of AnglE to generate high-quality text embeddings and the usefulness of angle
optimization in STS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xianming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jing Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01225">
<title>A path-norm toolkit for modern networks: consequences, promises and challenges. (arXiv:2310.01225v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01225</link>
<description rdf:parseType="Literal">&lt;p&gt;This work introduces the first toolkit around path-norms that is fully able
to encompass general DAG ReLU networks with biases, skip connections and any
operation based on the extraction of order statistics: max pooling, GroupSort
etc. This toolkit notably allows us to establish generalization bounds for
modern neural networks that are not only the most widely applicable path-norm
based ones, but also recover or beat the sharpest known bounds of this type.
These extended path-norms further enjoy the usual benefits of path-norms: ease
of computation, invariance under the symmetries of the network, and improved
sharpness on feedforward networks compared to the product of operators&apos; norms,
another complexity measure most commonly used.
&lt;/p&gt;
&lt;p&gt;The versatility of the toolkit and its ease of implementation allow us to
challenge the concrete promises of path-norm-based generalization bounds, by
numerically evaluating the sharpest known bounds for ResNets on ImageNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gonon_A/0/1/0/all/0/1&quot;&gt;Antoine Gonon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brisebarre_N/0/1/0/all/0/1&quot;&gt;Nicolas Brisebarre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Riccietti_E/0/1/0/all/0/1&quot;&gt;Elisa Riccietti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gribonval_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Gribonval&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02031">
<title>OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02031</link>
<description rdf:parseType="Literal">&lt;p&gt;Ocean science, which delves into the oceans that are reservoirs of life and
biodiversity, is of great significance given that oceans cover over 70% of our
planet&apos;s surface. Recently, advances in Large Language Models (LLMs) have
transformed the paradigm in science. Despite the success in other domains,
current LLMs often fall short in catering to the needs of domain experts like
oceanographers, and the potential of LLMs for ocean science is under-explored.
The intrinsic reason may be the immense and intricate nature of ocean data as
well as the necessity for higher granularity and richness in knowledge. To
alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean
domain, which is expert in various ocean science tasks. We propose DoInstruct,
a novel framework to automatically obtain a large volume of ocean domain
instruction data, which generates instructions based on multi-agent
collaboration. Additionally, we construct the first oceanography benchmark,
OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though
comprehensive experiments, OceanGPT not only shows a higher level of knowledge
expertise for oceans science tasks but also gains preliminary embodied
intelligence capabilities in ocean technology. Codes, data and checkpoints will
soon be available at https://github.com/zjunlp/KnowLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1&quot;&gt;Zhen Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yida Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1&quot;&gt;Yixin Ou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1&quot;&gt;Daxiong Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1&quot;&gt;Guozhou Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02227">
<title>SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training. (arXiv:2310.02227v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02227</link>
<description rdf:parseType="Literal">&lt;p&gt;In an era where symbolic mathematical equations are indispensable for
modeling complex natural phenomena, scientific inquiry often involves
collecting observations and translating them into mathematical expressions.
Recently, deep learning has emerged as a powerful tool for extracting insights
from data. However, existing models typically specialize in either numeric or
symbolic domains, and are usually trained in a supervised manner tailored to
specific tasks. This approach neglects the substantial benefits that could
arise from a task-agnostic unified understanding between symbolic equations and
their numeric counterparts. To bridge the gap, we introduce SNIP, a
Symbolic-Numeric Integrated Pre-training, which employs joint contrastive
learning between symbolic and numeric domains, enhancing their mutual
similarities in the pre-trained embeddings. By performing latent space
analysis, we observe that SNIP provides cross-domain insights into the
representations, revealing that symbolic supervision enhances the embeddings of
numeric data and vice versa. We evaluate SNIP across diverse tasks, including
symbolic-to-numeric mathematical property prediction and numeric-to-symbolic
equation discovery, commonly known as symbolic regression. Results show that
SNIP effectively transfers to various tasks, consistently outperforming fully
supervised baselines and competing strongly with established task-specific
methods, especially in few-shot learning scenarios where available data is
limited.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meidani_K/0/1/0/all/0/1&quot;&gt;Kazem Meidani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shojaee_P/0/1/0/all/0/1&quot;&gt;Parshin Shojaee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1&quot;&gt;Chandan K. Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farimani_A/0/1/0/all/0/1&quot;&gt;Amir Barati Farimani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02554">
<title>zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning. (arXiv:2310.02554v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02554</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) is a machine learning paradigm, which enables
multiple and decentralized clients to collaboratively train a model under the
orchestration of a central aggregator. Traditional FL solutions rely on the
trust assumption of the centralized aggregator, which forms cohorts of clients
in a fair and honest manner. However, a malicious aggregator, in reality, could
abandon and replace the client&apos;s training models, or launch Sybil attacks to
insert fake clients. Such malicious behaviors give the aggregator more power to
control clients in the FL setting and determine the final training results. In
this work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) to
tackle the issue of a malicious aggregator during the training model
aggregation process. To guarantee the correct aggregation results, the
aggregator needs to provide a proof per round. The proof can demonstrate to the
clients that the aggregator executes the intended behavior faithfully. To
further reduce the verification cost of clients, we employ a blockchain to
handle the proof in a zero-knowledge way, where miners (i.e., the nodes
validating and maintaining the blockchain data) can verify the proof without
knowing the clients&apos; local and aggregated models. The theoretical analysis and
empirical results show that zkFL can achieve better security and privacy than
traditional FL, without modifying the underlying FL network structure or
heavily compromising the training speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1&quot;&gt;Nanqing Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiahao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knottenbelt_W/0/1/0/all/0/1&quot;&gt;William Knottenbelt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03178">
<title>Digital Ethics in Federated Learning. (arXiv:2310.03178v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03178</link>
<description rdf:parseType="Literal">&lt;p&gt;The Internet of Things (IoT) consistently generates vast amounts of data,
sparking increasing concern over the protection of data privacy and the
limitation of data misuse. Federated learning (FL) facilitates collaborative
capabilities among multiple parties by sharing machine learning (ML) model
parameters instead of raw user data, and it has recently gained significant
attention for its potential in privacy preservation and learning efficiency
enhancement. In this paper, we highlight the digital ethics concerns that arise
when human-centric devices serve as clients in FL. More specifically,
challenges of game dynamics, fairness, incentive, and continuity arise in FL
due to differences in perspectives and objectives between clients and the
server. We analyze these challenges and their solutions from the perspectives
of both the client and the server, and through the viewpoints of centralized
and decentralized FL. Finally, we explore the opportunities in FL for
human-centric IoT as directions for future development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Liangqi Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1&quot;&gt;Christopher G. Brinton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05161">
<title>Recurrent Neural Language Models as Probabilistic Finite-state Automata. (arXiv:2310.05161v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05161</link>
<description rdf:parseType="Literal">&lt;p&gt;Studying language models (LMs) in terms of well-understood formalisms allows
us to precisely characterize their abilities and limitations. Previous work has
investigated the representational capacity of recurrent neural network (RNN)
LMs in terms of their capacity to recognize unweighted formal languages.
However, LMs do not describe unweighted formal languages -- rather, they define
probability distributions over strings. In this work, we study what classes of
such probability distributions RNN LMs can represent, which allows us to make
more direct statements about their capabilities. We show that simple RNNs are
equivalent to a subclass of probabilistic finite-state automata, and can thus
model a strict subset of probability distributions expressible by finite-state
models. Furthermore, we study the space complexity of representing finite-state
LMs with RNNs. We show that, to represent an arbitrary deterministic
finite-state LM with $N$ states over an alphabet $\Sigma$, an RNN requires
$\Omega\left(N |\Sigma|\right)$ neurons. These results present a first step
towards characterizing the classes of distributions RNN LMs can represent and
thus help us understand their capabilities and limitations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Svete_A/0/1/0/all/0/1&quot;&gt;Anej Svete&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1&quot;&gt;Ryan Cotterell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07488">
<title>KwaiYiiMath: Technical Report. (arXiv:2310.07488v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07488</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in large language models (LLMs) have demonstrated
remarkable abilities in handling a variety of natural language processing (NLP)
downstream tasks, even on mathematical tasks requiring multi-step reasoning. In
this report, we introduce the KwaiYiiMath which enhances the mathematical
reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT)
and Reinforced Learning from Human Feedback (RLHF), including on both English
and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale
Chinese primary school mathematics test set (named KMath), consisting of 188
examples to evaluate the correctness of the problem-solving process generated
by the models. Empirical studies demonstrate that KwaiYiiMath can achieve
state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with
the similar size models, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jiayi Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Lei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Pengli Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhengzong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhirui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shengnan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xue Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xucheng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1&quot;&gt;Yiqiao Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1&quot;&gt;Chao Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Chengru Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1&quot;&gt;Junchen Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zijia Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fuzheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhongyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Di Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1&quot;&gt;Kun Gai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07560">
<title>ROMO: Retrieval-enhanced Offline Model-based Optimization. (arXiv:2310.07560v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07560</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-driven black-box model-based optimization (MBO) problems arise in a
great number of practical application scenarios, where the goal is to find a
design over the whole space maximizing a black-box target function based on a
static offline dataset. In this work, we consider a more general but
challenging MBO setting, named constrained MBO (CoMBO), where only part of the
design space can be optimized while the rest is constrained by the environment.
A new challenge arising from CoMBO is that most observed designs that satisfy
the constraints are mediocre in evaluation. Therefore, we focus on optimizing
these mediocre designs in the offline dataset while maintaining the given
constraints rather than further boosting the best observed design in the
traditional MBO setting. We propose retrieval-enhanced offline model-based
optimization (ROMO), a new derivable forward approach that retrieves the
offline dataset and aggregates relevant samples to provide a trusted
prediction, and use it for gradient-based optimization. ROMO is simple to
implement and outperforms state-of-the-art approaches in the CoMBO setting.
Empirically, we conduct experiments on a synthetic Hartmann (3D) function
dataset, an industrial CIO dataset, and a suite of modified tasks in the
Design-Bench benchmark. Results show that ROMO performs well in a wide range of
constrained optimization tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingcheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haoran Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Hulei Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1&quot;&gt;Hongqiao Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1&quot;&gt;Zheng Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07587">
<title>Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient Balancer. (arXiv:2310.07587v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07587</link>
<description rdf:parseType="Literal">&lt;p&gt;Data privacy and long-tailed distribution are the norms rather than the
exception in many real-world tasks. This paper investigates a federated
long-tailed learning (Fed-LT) task in which each client holds a locally
heterogeneous dataset; if the datasets can be globally aggregated, they jointly
exhibit a long-tailed distribution. Under such a setting, existing federated
optimization and/or centralized long-tailed learning methods hardly apply due
to challenges in (a) characterizing the global long-tailed distribution under
privacy constraints and (b) adjusting the local learning strategy to cope with
the head-tail imbalance. In response, we propose a method termed
$\texttt{Fed-GraB}$, comprised of a Self-adjusting Gradient Balancer (SGB)
module that re-weights clients&apos; gradients in a closed-loop manner, based on the
feedback of global long-tailed distribution evaluated by a Direct Prior
Analyzer (DPA) module. Using $\texttt{Fed-GraB}$, clients can effectively
alleviate the distribution drift caused by data heterogeneity during the model
training process and obtain a global model with better performance on the
minority classes while maintaining the performance of the majority classes.
Extensive experiments demonstrate that $\texttt{Fed-GraB}$ achieves
state-of-the-art performance on representative datasets such as CIFAR-10-LT,
CIFAR-100-LT, ImageNet-LT, and iNaturalist.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zikai Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zihan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Songshang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hualiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jin Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Joey Tianyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Howard Hao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zuozhu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07940">
<title>Cost-Driven Hardware-Software Co-Optimization of Machine Learning Pipelines. (arXiv:2310.07940v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07940</link>
<description rdf:parseType="Literal">&lt;p&gt;Researchers have long touted a vision of the future enabled by a
proliferation of internet-of-things devices, including smart sensors, homes,
and cities. Increasingly, embedding intelligence in such devices involves the
use of deep neural networks. However, their storage and processing requirements
make them prohibitive for cheap, off-the-shelf platforms. Overcoming those
requirements is necessary for enabling widely-applicable smart devices. While
many ways of making models smaller and more efficient have been developed,
there is a lack of understanding of which ones are best suited for particular
scenarios. More importantly for edge platforms, those choices cannot be
analyzed in isolation from cost and user experience. In this work, we
holistically explore how quantization, model scaling, and multi-modality
interact with system components such as memory, sensors, and processors. We
perform this hardware/software co-design from the cost, latency, and
user-experience perspective, and develop a set of guidelines for optimal system
design and model deployment for the most cost-constrained platforms. We
demonstrate our approach using an end-to-end, on-device, biometric user
authentication system using a $20 ESP-EYE board.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1&quot;&gt;Ravit Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romaszkan_W/0/1/0/all/0/1&quot;&gt;Wojciech Romaszkan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Feiqian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1&quot;&gt;Puneet Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_A/0/1/0/all/0/1&quot;&gt;Ankur Mehta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08237">
<title>Towards a Unified Analysis of Kernel-based Methods Under Covariate Shift. (arXiv:2310.08237v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08237</link>
<description rdf:parseType="Literal">&lt;p&gt;Covariate shift occurs prevalently in practice, where the input distributions
of the source and target data are substantially different. Despite its
practical importance in various learning problems, most of the existing methods
only focus on some specific learning tasks and are not well validated
theoretically and numerically. To tackle this problem, we propose a unified
analysis of general nonparametric methods in a reproducing kernel Hilbert space
(RKHS) under covariate shift. Our theoretical results are established for a
general loss belonging to a rich loss function family, which includes many
commonly used methods as special cases, such as mean regression, quantile
regression, likelihood-based classification, and margin-based classification.
Two types of covariate shift problems are the focus of this paper and the sharp
convergence rates are established for a general loss function to provide a
unified theoretical analysis, which concurs with the optimal results in
literature where the squared loss is used. Extensive numerical studies on
synthetic and real examples confirm our theoretical findings and further
illustrate the effectiveness of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feng_X/0/1/0/all/0/1&quot;&gt;Xingdong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Caixing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingnan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08854">
<title>Rank-DETR for High Quality Object Detection. (arXiv:2310.08854v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08854</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern detection transformers (DETRs) use a set of object queries to predict
a list of bounding boxes, sort them by their classification confidence scores,
and select the top-ranked predictions as the final detection results for the
given input image. A highly performant object detector requires accurate
ranking for the bounding box predictions. For DETR-based detectors, the
top-ranked bounding boxes suffer from less accurate localization quality due to
the misalignment between classification scores and localization accuracy, thus
impeding the construction of high-quality detectors. In this work, we introduce
a simple and highly performant DETR-based object detector by proposing a series
of rank-oriented designs, combinedly called Rank-DETR. Our key contributions
include: (i) a rank-oriented architecture design that can prompt positive
predictions and suppress the negative ones to ensure lower false positive
rates, as well as (ii) a rank-oriented loss function and matching cost design
that prioritizes predictions of more accurate localization accuracy during
ranking to boost the AP under high IoU thresholds. We apply our method to
improve the recent SOTA methods (e.g., H-DETR and DINO-DETR) and report strong
COCO object detection results when using different backbones such as
ResNet-$50$, Swin-T, and Swin-L, demonstrating the effectiveness of our
approach. Code is available at \url{https://github.com/LeapLabTHU/Rank-DETR}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1&quot;&gt;Yifan Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Weicong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1&quot;&gt;Yiduo Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuhui Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yukang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Han Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Gao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10060">
<title>Data Augmentation for Time-Series Classification: An Extensive Empirical Study and Comprehensive Survey. (arXiv:2310.10060v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10060</link>
<description rdf:parseType="Literal">&lt;p&gt;Data Augmentation (DA) has emerged as an indispensable strategy in Time
Series Classification (TSC), primarily due to its capacity to amplify training
samples, thereby bolstering model robustness, diversifying datasets, and
curtailing overfitting. However, the current landscape of DA in TSC is plagued
with fragmented literature reviews, nebulous methodological taxonomies,
inadequate evaluative measures, and a dearth of accessible, user-oriented
tools. In light of these challenges, this study embarks on an exhaustive
dissection of DA methodologies within the TSC realm. Our initial approach
involved an extensive literature review spanning a decade, revealing that
contemporary surveys scarcely capture the breadth of advancements in DA for
TSC, prompting us to meticulously analyze over 100 scholarly articles to
distill more than 60 unique DA techniques. This rigorous analysis precipitated
the formulation of a novel taxonomy, purpose-built for the intricacies of DA in
TSC, categorizing techniques into five principal echelons:
Transformation-Based, Pattern-Based, Generative, Decomposition-Based, and
Automated Data Augmentation. Our taxonomy promises to serve as a robust
navigational aid for scholars, offering clarity and direction in method
selection. Addressing the conspicuous absence of holistic evaluations for
prevalent DA techniques, we executed an all-encompassing empirical assessment,
wherein upwards of 15 DA strategies were subjected to scrutiny across 8 UCR
time-series datasets, employing ResNet and a multi-faceted evaluation paradigm
encompassing Accuracy, Method Ranking, and Residual Analysis, yielding a
benchmark accuracy of 88.94 +- 11.83%. Our investigation underscored the
inconsistent efficacies of DA techniques, with...
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zijun Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lingbo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tianhua Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10537">
<title>Microscaling Data Formats for Deep Learning. (arXiv:2310.10537v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10537</link>
<description rdf:parseType="Literal">&lt;p&gt;Narrow bit-width data formats are key to reducing the computational and
storage costs of modern deep learning applications. This paper evaluates
Microscaling (MX) data formats that combine a per-block scaling factor with
narrow floating-point and integer types for individual elements. MX formats
balance the competing needs of hardware efficiency, model accuracy, and user
friction. Empirical results on over two dozen benchmarks demonstrate
practicality of MX data formats as a drop-in replacement for baseline FP32 for
AI inference and training with low user friction. We also show the first
instance of training generative language models at sub-8-bit weights,
activations, and gradients with minimal accuracy loss and no modifications to
the training recipe.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rouhani_B/0/1/0/all/0/1&quot;&gt;Bita Darvish Rouhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Ritchie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+More_A/0/1/0/all/0/1&quot;&gt;Ankit More&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hall_M/0/1/0/all/0/1&quot;&gt;Mathew Hall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khodamoradi_A/0/1/0/all/0/1&quot;&gt;Alireza Khodamoradi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Summer Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choudhary_D/0/1/0/all/0/1&quot;&gt;Dhruv Choudhary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cornea_M/0/1/0/all/0/1&quot;&gt;Marius Cornea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dellinger_E/0/1/0/all/0/1&quot;&gt;Eric Dellinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denolf_K/0/1/0/all/0/1&quot;&gt;Kristof Denolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dusan_S/0/1/0/all/0/1&quot;&gt;Stosic Dusan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elango_V/0/1/0/all/0/1&quot;&gt;Venmugil Elango&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golub_M/0/1/0/all/0/1&quot;&gt;Maximilian Golub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinecke_A/0/1/0/all/0/1&quot;&gt;Alexander Heinecke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+James_Roxby_P/0/1/0/all/0/1&quot;&gt;Phil James-Roxby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jani_D/0/1/0/all/0/1&quot;&gt;Dharmesh Jani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolhe_G/0/1/0/all/0/1&quot;&gt;Gaurav Kolhe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langhammer_M/0/1/0/all/0/1&quot;&gt;Martin Langhammer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Ada Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melnick_L/0/1/0/all/0/1&quot;&gt;Levi Melnick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mesmakhosroshahi_M/0/1/0/all/0/1&quot;&gt;Maral Mesmakhosroshahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1&quot;&gt;Andres Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulte_M/0/1/0/all/0/1&quot;&gt;Michael Schulte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafipour_R/0/1/0/all/0/1&quot;&gt;Rasoul Shafipour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1&quot;&gt;Lei Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siu_M/0/1/0/all/0/1&quot;&gt;Michael Siu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubey_P/0/1/0/all/0/1&quot;&gt;Pradeep Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Micikevicius_P/0/1/0/all/0/1&quot;&gt;Paulius Micikevicius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naumov_M/0/1/0/all/0/1&quot;&gt;Maxim Naumov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verrilli_C/0/1/0/all/0/1&quot;&gt;Colin Verrilli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wittig_R/0/1/0/all/0/1&quot;&gt;Ralph Wittig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burger_D/0/1/0/all/0/1&quot;&gt;Doug Burger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_E/0/1/0/all/0/1&quot;&gt;Eric Chung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10638">
<title>In-Context Pretraining: Language Modeling Beyond Document Boundaries. (arXiv:2310.10638v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10638</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LMs) are currently trained to predict tokens given
document prefixes, enabling them to directly perform long-form generation and
prompting-style tasks which can be reduced to document completion. Existing
pretraining pipelines train LMs by concatenating random sets of short documents
to create input contexts but the prior documents provide no signal for
predicting the next document. We instead present In-Context Pretraining, a new
approach where language models are pretrained on a sequence of related
documents, thereby explicitly encouraging them to read and reason across
document boundaries. We can do In-Context Pretraining by simply changing the
document ordering so that each context contains related documents, and directly
applying existing pretraining pipelines. However, this document sorting problem
is challenging. There are billions of documents and we would like the sort to
maximize contextual similarity for every document without repeating any data.
To do this, we introduce approximate algorithms for finding related documents
with efficient nearest neighbor search and constructing coherent input contexts
with a graph traversal algorithm. Our experiments show In-Context Pretraining
offers a simple and scalable approach to significantly enhance LMs&apos;performance:
we see notable improvements in tasks that require more complex contextual
reasoning, including in-context learning (+8%), reading comprehension (+15%),
faithfulness to previous contexts (+16%), long-context reasoning (+5%), and
retrieval augmentation (+9%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Weijia Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1&quot;&gt;Sewon Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lomeli_M/0/1/0/all/0/1&quot;&gt;Maria Lomeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chunting Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Margaret Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_V/0/1/0/all/0/1&quot;&gt;Victoria Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1&quot;&gt;Noah A. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1&quot;&gt;Luke Zettlemoyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yih_S/0/1/0/all/0/1&quot;&gt;Scott Yih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1&quot;&gt;Mike Lewis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10692">
<title>ACES: Generating Diverse Programming Puzzles with Autotelic Language Models and Semantic Descriptors. (arXiv:2310.10692v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10692</link>
<description rdf:parseType="Literal">&lt;p&gt;Finding and selecting new and interesting problems to solve is at the heart
of curiosity, science and innovation. We here study automated problem
generation in the context of the open-ended space of python programming
puzzles. Existing generative models often aim at modeling a reference
distribution without any explicit diversity optimization. Other methods
explicitly optimizing for diversity do so either in limited hand-coded
representation spaces or in uninterpretable learned embedding spaces that may
not align with human perceptions of interesting variations. With ACES
(Autotelic Code Exploration via Semantic descriptors), we introduce a new
autotelic generation method that leverages semantic descriptors produced by a
large language model (LLM) to directly optimize for interesting diversity, as
well as few-shot-based generation. Each puzzle is labeled along 10 dimensions,
each capturing a programming skill required to solve it. ACES generates and
pursues novel and feasible goals to explore that abstract semantic space,
slowly discovering a diversity of solvable programming puzzles in any given
run. Across a set of experiments, we show that ACES discovers a richer
diversity of puzzles than existing diversity-maximizing algorithms as measured
across a range of diversity metrics. We further study whether and in which
conditions this diversity can translate into the successful training of puzzle
solving models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pourcel_J/0/1/0/all/0/1&quot;&gt;Julien Pourcel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colas_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Colas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1&quot;&gt;Pierre-Yves Oudeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teodorescu_L/0/1/0/all/0/1&quot;&gt;Laetitia Teodorescu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11009">
<title>Adaptive Pairwise Encodings for Link Prediction. (arXiv:2310.11009v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11009</link>
<description rdf:parseType="Literal">&lt;p&gt;Link prediction is a common task on graph-structured data that has seen
applications in a variety of domains. Classically, hand-crafted heuristics were
used for this task. Heuristic measures are chosen such that they correlate well
with the underlying factors related to link formation. In recent years, a new
class of methods has emerged that combines the advantages of message-passing
neural networks (MPNN) and heuristics methods. These methods perform
predictions by using the output of an MPNN in conjunction with a &quot;pairwise
encoding&quot; that captures the relationship between nodes in the candidate link.
They have been shown to achieve strong performance on numerous datasets.
However, current pairwise encodings often contain a strong inductive bias,
using the same underlying factors to classify all links. This limits the
ability of existing methods to learn how to properly classify a variety of
different links that may form from different factors. To address this
limitation, we propose a new method, LPFormer, which attempts to adaptively
learn the pairwise encodings for each link. LPFormer models the link factors
via an attention module that learns the pairwise encoding that exists between
nodes by modeling multiple factors integral to link prediction. Extensive
experiments demonstrate that LPFormer can achieve SOTA performance on numerous
datasets while maintaining efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shomer_H/0/1/0/all/0/1&quot;&gt;Harry Shomer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1&quot;&gt;Haitao Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Juanhui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bo Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiliang Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11102">
<title>HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning. (arXiv:2310.11102v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11102</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative self-supervised learning (SSL) has exhibited significant potential
and garnered increasing interest in graph learning. In this study, we aim to
explore the problem of generative SSL in the context of heterogeneous graph
learning (HGL). The previous SSL approaches for heterogeneous graphs have
primarily relied on contrastive learning, necessitating the design of complex
views to capture heterogeneity. However, existing generative SSL methods have
not fully leveraged the capabilities of generative models to address the
challenges of HGL. In this paper, we present HGCVAE, a novel contrastive
variational graph auto-encoder that liberates HGL from the burden of intricate
heterogeneity capturing. Instead of focusing on complicated heterogeneity,
HGCVAE harnesses the full potential of generative SSL. HGCVAE innovatively
consolidates contrastive learning with generative SSL, introducing several key
innovations. Firstly, we employ a progressive mechanism to generate
high-quality hard negative samples for contrastive learning, utilizing the
power of variational inference. Additionally, we present a dynamic mask
strategy to ensure effective and stable learning. Moreover, we propose an
enhanced scaled cosine error as the criterion for better attribute
reconstruction. As an initial step in combining generative and contrastive SSL,
HGCVAE achieves remarkable results compared to various state-of-the-art
baselines, confirming its superiority.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yulan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhirui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1&quot;&gt;Sheng Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1&quot;&gt;Junchen Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fuzheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhongyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11466">
<title>Protein 3D Graph Structure Learning for Robust Structure-based Protein Property Prediction. (arXiv:2310.11466v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11466</link>
<description rdf:parseType="Literal">&lt;p&gt;Protein structure-based property prediction has emerged as a promising
approach for various biological tasks, such as protein function prediction and
sub-cellular location estimation. The existing methods highly rely on
experimental protein structure data and fail in scenarios where these data are
unavailable. Predicted protein structures from AI tools (e.g., AlphaFold2) were
utilized as alternatives. However, we observed that current practices, which
simply employ accurately predicted structures during inference, suffer from
notable degradation in prediction accuracy. While similar phenomena have been
extensively studied in general fields (e.g., Computer Vision) as model
robustness, their impact on protein property prediction remains unexplored. In
this paper, we first investigate the reason behind the performance decrease
when utilizing predicted structures, attributing it to the structure embedding
bias from the perspective of structure representation learning. To study this
problem, we identify a Protein 3D Graph Structure Learning Problem for Robust
Protein Property Prediction (PGSL-RP3), collect benchmark datasets, and present
a protein Structure embedding Alignment Optimization framework (SAO) to
mitigate the problem of structure embedding bias between the predicted and
experimental protein structures. Extensive experiments have shown that our
framework is model-agnostic and effective in improving the property prediction
of both predicted structures and experimental structures. The benchmark
datasets and codes will be released to benefit the community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yufei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Siyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jin Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lirong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_O/0/1/0/all/0/1&quot;&gt;Odin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Haitao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1&quot;&gt;Jingqi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zihan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jiangbin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Stan.ZQ.Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11569">
<title>When Rigidity Hurts: Soft Consistency Regularization for Probabilistic Hierarchical Time Series Forecasting. (arXiv:2310.11569v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11569</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic hierarchical time-series forecasting is an important variant of
time-series forecasting, where the goal is to model and forecast multivariate
time-series that have underlying hierarchical relations. Most methods focus on
point predictions and do not provide well-calibrated probabilistic forecasts
distributions. Recent state-of-art probabilistic forecasting methods also
impose hierarchical relations on point predictions and samples of distribution
which does not account for coherency of forecast distributions. Previous works
also silently assume that datasets are always consistent with given
hierarchical relations and do not adapt to real-world datasets that show
deviation from this assumption. We close both these gap and propose PROFHiT,
which is a fully probabilistic hierarchical forecasting model that jointly
models forecast distribution of entire hierarchy. PROFHiT uses a flexible
probabilistic Bayesian approach and introduces a novel Distributional Coherency
regularization to learn from hierarchical relations for entire forecast
distribution that enables robust and calibrated forecasts as well as adapt to
datasets of varying hierarchical consistency. On evaluating PROFHiT over wide
range of datasets, we observed 41-88% better performance in accuracy and
significantly better calibration. Due to modeling the coherency over full
distribution, we observed that PROFHiT can robustly provide reliable forecasts
even if up to 10% of input time-series data is missing where other methods&apos;
performance severely degrade by over 70%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamarthi_H/0/1/0/all/0/1&quot;&gt;Harshavardhan Kamarthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1&quot;&gt;Lingkai Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1&quot;&gt;Alexander Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakash_B/0/1/0/all/0/1&quot;&gt;B. Aditya Prakash&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11762">
<title>A Quasi-Wasserstein Loss for Learning Graph Neural Networks. (arXiv:2310.11762v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11762</link>
<description rdf:parseType="Literal">&lt;p&gt;When learning graph neural networks (GNNs) in node-level prediction tasks,
most existing loss functions are applied for each node independently, even if
node embeddings and their labels are non-i.i.d. because of their graph
structures. To eliminate such inconsistency, in this study we propose a novel
Quasi-Wasserstein (QW) loss with the help of the optimal transport defined on
graphs, leading to new learning and prediction paradigms of GNNs. In
particular, we design a &quot;Quasi-Wasserstein&quot; distance between the observed
multi-dimensional node labels and their estimations, optimizing the label
transport defined on graph edges. The estimations are parameterized by a GNN in
which the optimal label transport may determine the graph edge weights
optionally. By reformulating the strict constraint of the label transport to a
Bregman divergence-based regularizer, we obtain the proposed Quasi-Wasserstein
loss associated with two efficient solvers learning the GNN together with
optimal label transport. When predicting node labels, our model combines the
output of the GNN with the residual component provided by the optimal label
transport, leading to a new transductive prediction paradigm. Experiments show
that the proposed QW loss applies to various GNNs and helps to improve their
performance in node-level classification and regression tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Minjie Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hongteng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11971">
<title>Improving Generalization of Alignment with Human Preferences through Group Invariant Learning. (arXiv:2310.11971v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11971</link>
<description rdf:parseType="Literal">&lt;p&gt;The success of AI assistants based on language models (LLMs) hinges crucially
on Reinforcement Learning from Human Feedback (RLHF), which enables the
generation of responses more aligned with human preferences. As universal AI
assistants, there&apos;s a growing expectation for them to perform consistently
across various domains. However, previous work shows that Reinforcement
Learning (RL) often exploits shortcuts to attain high rewards and overlooks
challenging samples. This focus on quick reward gains undermines both the
stability in training and the model&apos;s ability to generalize to new, unseen
data. In this work, we propose a novel approach that can learn a consistent
policy via RL across various data groups or domains. Given the challenges
associated with acquiring group annotations, our method automatically
classifies data into different groups, deliberately maximizing performance
variance. Then, we optimize the policy to perform well on challenging groups.
Lastly, leveraging the established groups, our approach adaptively adjusts the
exploration space, allocating more learning capacity to more challenging data
and preventing the model from over-optimizing on simpler data. Experimental
results indicate that our approach significantly enhances training stability
and model generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1&quot;&gt;Rui Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;Wei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1&quot;&gt;Yuan Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1&quot;&gt;Wenbin Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1&quot;&gt;Shihan Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1&quot;&gt;Zhiheng Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haoran Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1&quot;&gt;Tao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12069">
<title>Transformers for scientific data: a pedagogical review for astronomers. (arXiv:2310.12069v2 [astro-ph.IM] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12069</link>
<description rdf:parseType="Literal">&lt;p&gt;The deep learning architecture associated with ChatGPT and related generative
AI products is known as transformers. Initially applied to Natural Language
Processing, transformers and the self-attention mechanism they exploit have
gained widespread interest across the natural sciences. The goal of this
pedagogical and informal review is to introduce transformers to scientists. The
review includes the mathematics underlying the attention mechanism, a
description of the original transformer architecture, and a section on
applications to time series and imaging data in astronomy. We include a
Frequently Asked Questions section for readers who are curious about generative
AI or interested in getting started with transformers for their research
problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Tanoglidis_D/0/1/0/all/0/1&quot;&gt;Dimitrios Tanoglidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Jain_B/0/1/0/all/0/1&quot;&gt;Bhuvnesh Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Qu_H/0/1/0/all/0/1&quot;&gt;Helen Qu&lt;/a&gt; (University of Pennsylvania)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04934">
<title>Generative Pretrained Autoregressive Transformer Graph Neural Network applied to the Analysis and Discovery of Novel Proteins. (arXiv:2305.04934v2 [q-bio.BM] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2305.04934</link>
<description rdf:parseType="Literal">&lt;p&gt;We report a flexible language-model based deep learning strategy, applied
here to solve complex forward and inverse problems in protein modeling, based
on an attention neural network that integrates transformer and graph
convolutional architectures in a causal multi-headed graph mechanism, to
realize a generative pretrained model. The model is applied to predict
secondary structure content (per-residue level and overall content), protein
solubility, and sequencing tasks. Further trained on inverse tasks, the model
is rendered capable of designing proteins with these properties as target
features. The model is formulated as a general framework, completely
prompt-based, and can be adapted for a variety of downstream tasks. We find
that adding additional tasks yields emergent synergies that the model exploits
in improving overall performance, beyond what would be possible by training a
model on each dataset alone. Case studies are presented to validate the method,
yielding protein designs specifically focused on structural proteins, but also
exploring the applicability in the design of soluble, antimicrobial
biomaterials. While our model is trained to ultimately perform 8 distinct
tasks, with available datasets it can be extended to solve additional problems.
In a broader sense, this work illustrates a form of multiscale modeling that
relates a set of ultimate building blocks (here, byte-level utf8 characters
that define the nature of the physical system at hand) to complex output. This
materiomic scheme captures complex emergent relationships between universal
building block and resulting properties via a synergizing learning capacity to
express a set of potentialities embedded in the knowledge used in training, via
the interplay of universality and diversity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Buehler_M/0/1/0/all/0/1&quot;&gt;Markus J. Buehler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.10170">
<title>Generative modeling, design and analysis of spider silk protein sequences for enhanced mechanical properties. (arXiv:2309.10170v1 [cond-mat.mtrl-sci] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2309.10170</link>
<description rdf:parseType="Literal">&lt;p&gt;Spider silks are remarkable materials characterized by superb mechanical
properties such as strength, extensibility and lightweightedness. Yet, to date,
limited models are available to fully explore sequence-property relationships
for analysis and design. Here we propose a custom generative large-language
model to enable design of novel spider silk protein sequences to meet complex
combinations of target mechanical properties. The model, pretrained on a large
set of protein sequences, is fine-tuned on ~1,000 major ampullate spidroin
(MaSp) sequences for which associated fiber-level mechanical properties exist,
to yield an end-to-end forward and inverse generative strategy. Performance is
assessed through: (1), a novelty analysis and protein type classification for
generated spidroin sequences through BLAST searches, (2) property evaluation
and comparison with similar sequences, (3) comparison of molecular structures,
as well as, and (4) a detailed sequence motif analyses. We generate silk
sequences with property combinations that do not exist in nature, and develop a
deep understanding the mechanistic roles of sequence patterns in achieving
overarching key mechanical properties (elastic modulus, strength, toughness,
failure strain). The model provides an efficient approach to expand the silkome
dataset, facilitating further sequence-structure analyses of silks, and
establishes a foundation for synthetic silk design and optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Wei Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Kaplan_D/0/1/0/all/0/1&quot;&gt;David L. Kaplan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Buehler_M/0/1/0/all/0/1&quot;&gt;Markus J. Buehler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10541">
<title>Efficient Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories. (arXiv:2310.10541v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2310.10541</link>
<description rdf:parseType="Literal">&lt;p&gt;Training a large and state-of-the-art machine learning model typically
necessitates the use of large-scale datasets, which, in turn, makes the
training and parameter-tuning process expensive and time-consuming. Some
researchers opt to distil information from real-world datasets into tiny and
compact synthetic datasets while maintaining their ability to train a
well-performing model, hence proposing a data-efficient method known as Dataset
Distillation (DD). Despite recent progress in this field, existing methods
still underperform and cannot effectively replace large datasets. In this
paper, unlike previous methods that focus solely on improving the efficacy of
student distillation, we are the first to recognize the important interplay
between expert and student. We argue the significant impact of expert
smoothness when employing more potent expert trajectories in subsequent dataset
distillation. Based on this, we introduce the integration of clipping loss and
gradient penalty to regulate the rate of parameter changes in expert
trajectories. Furthermore, in response to the sensitivity exhibited towards
randomly initialized variables during distillation, we propose representative
initialization for synthetic dataset and balanced inner-loop loss. Finally, we
present two enhancement strategies, namely intermediate matching loss and
weight perturbation, to mitigate the potential occurrence of cumulative errors.
We conduct extensive experiments on datasets of different scales, sizes, and
resolutions. The results demonstrate that the proposed method significantly
outperforms prior methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jiyuan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenzhuo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1&quot;&gt;Kwok-Yan Lam&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>