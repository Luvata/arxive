<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-03T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01349" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01369" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01377" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01384" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01426" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01458" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01459" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01482" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01493" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01519" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01523" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01537" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01600" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01623" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01626" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01728" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01732" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01801" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01814" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01830" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01835" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01841" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01843" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.03958" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.00086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.03932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.09806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07618" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10675" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16914" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08163" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12113" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13307" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16127" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16476" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01078" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01262" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.01343">
<title>IoTGeM: Generalizable Models for Behaviour-Based IoT Attack Detection. (arXiv:2401.01343v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.01343</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous research on behaviour-based attack detection on networks of IoT
devices has resulted in machine learning models whose ability to adapt to
unseen data is limited, and often not demonstrated. In this paper we present an
approach for modelling IoT network attacks that focuses on generalizability,
yet also leads to better detection and performance. First, we present an
improved rolling window approach for feature extraction, and introduce a
multi-step feature selection process that reduces overfitting. Second, we build
and test models using isolated train and test datasets, thereby avoiding common
data leaks that have limited the generalizability of previous models. Third, we
rigorously evaluate our methodology using a diverse portfolio of machine
learning models, evaluation metrics and datasets. Finally, we build confidence
in the models by using explainable AI techniques, allowing us to identify the
features that underlie accurate detection of attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kostas_K/0/1/0/all/0/1&quot;&gt;Kahraman Kostas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Just_M/0/1/0/all/0/1&quot;&gt;Mike Just&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lones_M/0/1/0/all/0/1&quot;&gt;Michael A. Lones&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01349">
<title>The Anatomy Spread of Online Opinion Polarization: The Pivotal Role of Super-Spreaders in Social Networks. (arXiv:2401.01349v1 [physics.soc-ph])</title>
<link>http://arxiv.org/abs/2401.01349</link>
<description rdf:parseType="Literal">&lt;p&gt;The study investigates the role of &apos;superspreaders&apos; in shaping opinions
within networks, distinguishing three types: A, B, and C. Type A has a
significant influence in shaping opinions, Type B acts as a counterbalance to
A, and Type C functions like media, providing an objective viewpoint and
potentially regulating A and B&apos;s influence. The research uses a confidence
coefficient and z-score to survey superspreaders&apos; behaviors, with a focus on
the conditions affecting group dynamics and opinion formation, including
environmental factors and forgetfulness over time. The findings offer insights
for improving online communication security and understanding social influence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kawahata_Y/0/1/0/all/0/1&quot;&gt;Yasuko Kawahata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01364">
<title>Multi-Modal Cognitive Maps based on Neural Networks trained on Successor Representations. (arXiv:2401.01364v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2401.01364</link>
<description rdf:parseType="Literal">&lt;p&gt;Cognitive maps are a proposed concept on how the brain efficiently organizes
memories and retrieves context out of them. The entorhinal-hippocampal complex
is heavily involved in episodic and relational memory processing, as well as
spatial navigation and is thought to built cognitive maps via place and grid
cells. To make use of the promising properties of cognitive maps, we set up a
multi-modal neural network using successor representations which is able to
model place cell dynamics and cognitive map representations. Here, we use
multi-modal inputs consisting of images and word embeddings. The network learns
the similarities between novel inputs and the training database and therefore
the representation of the cognitive map successfully. Subsequently, the
prediction of the network can be used to infer from one modality to another
with over $90\%$ accuracy. The proposed method could therefore be a building
block to improve current AI systems for better understanding of the environment
and the different modalities in which objects appear. The association of
specific modalities with certain encounters can therefore lead to context
awareness in novel situations when similar encounters with less information
occur and additional information can be inferred from the learned cognitive
map. Cognitive maps, as represented by the entorhinal-hippocampal complex in
the brain, organize and retrieve context from memories, suggesting that large
language models (LLMs) like ChatGPT could harness similar architectures to
function as a high-level processing center, akin to how the hippocampus
operates within the cortex hierarchy. Finally, by utilizing multi-modal inputs,
LLMs can potentially bridge the gap between different forms of data (like
images and words), paving the way for context-awareness and grounding of
abstract concepts through learned associations, addressing the grounding
problem in AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Stoewer_P/0/1/0/all/0/1&quot;&gt;Paul Stoewer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Schilling_A/0/1/0/all/0/1&quot;&gt;Achim Schilling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Maier_A/0/1/0/all/0/1&quot;&gt;Andreas Maier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Krauss_P/0/1/0/all/0/1&quot;&gt;Patrick Krauss&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01369">
<title>RL-MPCA: A Reinforcement Learning Based Multi-Phase Computation Allocation Approach for Recommender Systems. (arXiv:2401.01369v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2401.01369</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender systems aim to recommend the most suitable items to users from a
large number of candidates. Their computation cost grows as the number of user
requests and the complexity of services (or models) increases. Under the
limitation of computation resources (CRs), how to make a trade-off between
computation cost and business revenue becomes an essential question. The
existing studies focus on dynamically allocating CRs in queue truncation
scenarios (i.e., allocating the size of candidates), and formulate the CR
allocation problem as an optimization problem with constraints. Some of them
focus on single-phase CR allocation, and others focus on multi-phase CR
allocation but introduce some assumptions about queue truncation scenarios.
However, these assumptions do not hold in other scenarios, such as retrieval
channel selection and prediction model selection. Moreover, existing studies
ignore the state transition process of requests between different phases,
limiting the effectiveness of their approaches.
&lt;/p&gt;
&lt;p&gt;This paper proposes a Reinforcement Learning (RL) based Multi-Phase
Computation Allocation approach (RL-MPCA), which aims to maximize the total
business revenue under the limitation of CRs. RL-MPCA formulates the CR
allocation problem as a Weakly Coupled MDP problem and solves it with an
RL-based approach. Specifically, RL-MPCA designs a novel deep Q-network to
adapt to various CR allocation scenarios, and calibrates the Q-value by
introducing multiple adaptive Lagrange multipliers (adaptive-$\lambda$) to
avoid violating the global CR constraints. Finally, experiments on the offline
simulation environment and online real-world recommender system validate the
effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiahong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1&quot;&gt;Shunhui Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guoliang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1&quot;&gt;Bo Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Qianlong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Lebin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xingxing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01373">
<title>Boosting Defect Detection in Manufacturing using Tensor Convolutional Neural Networks. (arXiv:2401.01373v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01373</link>
<description rdf:parseType="Literal">&lt;p&gt;Defect detection is one of the most important yet challenging tasks in the
quality control stage in the manufacturing sector. In this work, we introduce a
Tensor Convolutional Neural Network (T-CNN) and examine its performance on a
real defect detection application in one of the components of the ultrasonic
sensors produced at Robert Bosch&apos;s manufacturing plants. Our quantum-inspired
T-CNN operates on a reduced model parameter space to substantially improve the
training speed and performance of an equivalent CNN model without sacrificing
accuracy. More specifically, we demonstrate how T-CNNs are able to reach the
same performance as classical CNNs as measured by quality metrics, with up to
fifteen times fewer parameters and 4% to 19% faster training times. Our results
demonstrate that the T-CNN greatly outperforms the results of traditional human
visual inspection, providing value in a current real application in
manufacturing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_Ramiro_P/0/1/0/all/0/1&quot;&gt;Pablo Martin-Ramiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maza_U/0/1/0/all/0/1&quot;&gt;Unai Sainz de la Maza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orus_R/0/1/0/all/0/1&quot;&gt;Roman Orus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mugel_S/0/1/0/all/0/1&quot;&gt;Samuel Mugel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01377">
<title>Does Few-shot Learning Suffer from Backdoor Attacks?. (arXiv:2401.01377v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.01377</link>
<description rdf:parseType="Literal">&lt;p&gt;The field of few-shot learning (FSL) has shown promising results in scenarios
where training data is limited, but its vulnerability to backdoor attacks
remains largely unexplored. We first explore this topic by first evaluating the
performance of the existing backdoor attack methods on few-shot learning
scenarios. Unlike in standard supervised learning, existing backdoor attack
methods failed to perform an effective attack in FSL due to two main issues.
Firstly, the model tends to overfit to either benign features or trigger
features, causing a tough trade-off between attack success rate and benign
accuracy. Secondly, due to the small number of training samples, the dirty
label or visible trigger in the support set can be easily detected by victims,
which reduces the stealthiness of attacks. It seemed that FSL could survive
from backdoor attacks. However, in this paper, we propose the Few-shot Learning
Backdoor Attack (FLBA) to show that FSL can still be vulnerable to backdoor
attacks. Specifically, we first generate a trigger to maximize the gap between
poisoned and benign features. It enables the model to learn both benign and
trigger features, which solves the problem of overfitting. To make it more
stealthy, we hide the trigger by optimizing two types of imperceptible
perturbation, namely attractive and repulsive perturbation, instead of
attaching the trigger directly. Once we obtain the perturbations, we can poison
all samples in the benign support set into a hidden poisoned support set and
fine-tune the model on it. Our method demonstrates a high Attack Success Rate
(ASR) in FSL tasks with different few-shot learning paradigms while preserving
clean accuracy and maintaining stealthiness. This study reveals that few-shot
learning still suffers from backdoor attacks, and its security should be given
attention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xiaojun Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jindong Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xun_Y/0/1/0/all/0/1&quot;&gt;Yuan Xun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1&quot;&gt;Siyuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xiaochun Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01383">
<title>Predicting Infant Brain Connectivity with Federated Multi-Trajectory GNNs using Scarce Data. (arXiv:2401.01383v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2401.01383</link>
<description rdf:parseType="Literal">&lt;p&gt;The understanding of the convoluted evolution of infant brain networks during
the first postnatal year is pivotal for identifying the dynamics of early brain
connectivity development. Existing deep learning solutions suffer from three
major limitations. First, they cannot generalize to multi-trajectory prediction
tasks, where each graph trajectory corresponds to a particular imaging modality
or connectivity type (e.g., T1-w MRI). Second, existing models require
extensive training datasets to achieve satisfactory performance which are often
challenging to obtain. Third, they do not efficiently utilize incomplete time
series data. To address these limitations, we introduce FedGmTE-Net++, a
federated graph-based multi-trajectory evolution network. Using the power of
federation, we aggregate local learnings among diverse hospitals with limited
datasets. As a result, we enhance the performance of each hospital&apos;s local
generative model, while preserving data privacy. The three key innovations of
FedGmTE-Net++ are: (i) presenting the first federated learning framework
specifically designed for brain multi-trajectory evolution prediction in a
data-scarce environment, (ii) incorporating an auxiliary regularizer in the
local objective function to exploit all the longitudinal brain connectivity
within the evolution trajectory and maximize data utilization, (iii)
introducing a two-step imputation process, comprising a preliminary KNN-based
precompletion followed by an imputation refinement step that employs regressors
to improve similarity scores and refine imputations. Our comprehensive
experimental results showed the outperformance of FedGmTE-Net++ in brain
multi-trajectory prediction from a single baseline graph in comparison with
benchmark methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Pistos_M/0/1/0/all/0/1&quot;&gt;Michalis Pistos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rekik_I/0/1/0/all/0/1&quot;&gt;Islem Rekik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01384">
<title>Strong Transitivity Relations and Graph Neural Networks. (arXiv:2401.01384v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2401.01384</link>
<description rdf:parseType="Literal">&lt;p&gt;Local neighborhoods play a crucial role in embedding generation in
graph-based learning. It is commonly believed that nodes ought to have
embeddings that resemble those of their neighbors. In this research, we try to
carefully expand the concept of similarity from nearby neighborhoods to the
entire graph. We provide an extension of similarity that is based on
transitivity relations, which enables Graph Neural Networks (GNNs) to capture
both global similarities and local similarities over the whole graph. We
introduce Transitivity Graph Neural Network (TransGNN), which more than local
node similarities, takes into account global similarities by distinguishing
strong transitivity relations from weak ones and exploiting them. We evaluate
our model over several real-world datasets and showed that it considerably
improves the performance of several well-known GNN models, for tasks such as
node classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamadi_Y/0/1/0/all/0/1&quot;&gt;Yassin Mohamadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chehreghani_M/0/1/0/all/0/1&quot;&gt;Mostafa Haghir Chehreghani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01388">
<title>Directional Antenna Systems for Long-Range Through-Wall Human Activity Recognition. (arXiv:2401.01388v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01388</link>
<description rdf:parseType="Literal">&lt;p&gt;WiFi Channel State Information (CSI)-based human activity recognition (HAR)
enables contactless, long-range sensing in spatially constrained environments
while preserving visual privacy. However, despite the presence of numerous
WiFi-enabled devices around us, few expose CSI to users, resulting in a lack of
sensing hardware options. Variants of the Espressif ESP32 have emerged as
potential low-cost and easy-to-deploy solutions for WiFi CSI-based HAR. In this
work, four ESP32-S3-based 2.4GHz directional antenna systems are evaluated for
their ability to facilitate long-range through-wall HAR. Two promising systems
are proposed, one of which combines the ESP32-S3 with a directional biquad
antenna. This combination represents, to the best of our knowledge, the first
demonstration of such a system in WiFi-based HAR. The second system relies on
the built-in printed inverted-F antenna (PIFA) of the ESP32-S3 and achieves
directionality through a plane reflector. In a comprehensive evaluation of
line-of-sight (LOS) and non-line-of-sight (NLOS) HAR performance, both systems
are deployed in an office environment spanning a distance of 18 meters across
five rooms. In this experimental setup, the Wallhack1.8k dataset, comprising
1806 CSI amplitude spectrograms of human activities, is collected and made
publicly available. Based on Wallhack1.8k, we train activity recognition models
using the EfficientNetV2 architecture to assess system performance in LOS and
NLOS scenarios. For the core NLOS activity recognition problem, the biquad
antenna and PIFA-based systems achieve accuracies of 92.0$\pm$3.5 and
86.8$\pm$4.7, respectively, demonstrating the feasibility of long-range
through-wall HAR with the proposed systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strohmayer_J/0/1/0/all/0/1&quot;&gt;Julian Strohmayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kampel_M/0/1/0/all/0/1&quot;&gt;Martin Kampel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01405">
<title>Quantifying the Uniqueness of Donald Trump in Presidential Discourse. (arXiv:2401.01405v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.01405</link>
<description rdf:parseType="Literal">&lt;p&gt;Does Donald Trump speak differently from other presidents? If so, in what
ways? Are these differences confined to any single medium of communication? To
investigate these questions, this paper introduces a novel metric of uniqueness
based on large language models, develops a new lexicon for divisive speech, and
presents a framework for comparing the lexical features of political opponents.
Applying these tools to a variety of corpora of presidential speeches, we find
considerable evidence that Trump&apos;s speech patterns diverge from those of all
major party nominees for the presidency in recent history. Some notable
findings include Trump&apos;s employment of particularly divisive and antagonistic
language targeting of his political opponents and his patterns of repetition
for emphasis. Furthermore, Trump is significantly more distinctive than his
fellow Republicans, whose uniqueness values are comparably closer to those of
the Democrats. These differences hold across a variety of measurement
strategies, arise on both the campaign trail and in official presidential
addresses, and do not appear to be an artifact of secular time trends.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Karen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meitus_A/0/1/0/all/0/1&quot;&gt;Alexander A. Meitus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chase_M/0/1/0/all/0/1&quot;&gt;Milo Chase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Grace Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mykland_A/0/1/0/all/0/1&quot;&gt;Anne Mykland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Howell_W/0/1/0/all/0/1&quot;&gt;William Howell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chenhao Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01425">
<title>SwapTransformer: highway overtaking tactical planner model via imitation learning on OSHA dataset. (arXiv:2401.01425v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.01425</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates the high-level decision-making problem in highway
scenarios regarding lane changing and over-taking other slower vehicles. In
particular, this paper aims to improve the Travel Assist feature for automatic
overtaking and lane changes on highways. About 9 million samples including lane
images and other dynamic objects are collected in simulation. This data;
Overtaking on Simulated HighwAys (OSHA) dataset is released to tackle this
challenge. To solve this problem, an architecture called SwapTransformer is
designed and implemented as an imitation learning approach on the OSHA dataset.
Moreover, auxiliary tasks such as future points and car distance network
predictions are proposed to aid the model in better understanding the
surrounding environment. The performance of the proposed solution is compared
with a multi-layer perceptron (MLP) and multi-head self-attention networks as
baselines in a simulation environment. We also demonstrate the performance of
the model with and without auxiliary tasks. All models are evaluated based on
different metrics such as time to finish each lap, number of overtakes, and
speed difference with speed limit. The evaluation shows that the
SwapTransformer model outperforms other models in different traffic densities
in the inference phase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shamsoshoara_A/0/1/0/all/0/1&quot;&gt;Alireza Shamsoshoara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salih_S/0/1/0/all/0/1&quot;&gt;Safin B Salih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aghazadeh_P/0/1/0/all/0/1&quot;&gt;Pedram Aghazadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01426">
<title>Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference. (arXiv:2401.01426v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.01426</link>
<description rdf:parseType="Literal">&lt;p&gt;Pearl&apos;s causal hierarchy establishes a clear separation between
observational, interventional, and counterfactual questions. Researchers
proposed sound and complete algorithms to compute identifiable causal queries
at a given level of the hierarchy using the causal structure and data from the
lower levels of the hierarchy. However, most of these algorithms assume that we
can accurately estimate the probability distribution of the data, which is an
impractical assumption for high-dimensional variables such as images. On the
other hand, modern generative deep learning architectures can be trained to
learn how to accurately sample from such high-dimensional distributions.
Especially with the recent rise of foundation models for images, it is
desirable to leverage pre-trained models to answer causal queries with such
high-dimensional data. To address this, we propose a sequential training
algorithm that, given the causal structure and a pre-trained conditional
generative model, can train a deep causal generative model, which utilizes the
pre-trained model and can provably sample from identifiable interventional and
counterfactual distributions. Our algorithm, called Modular-DCM, uses
adversarial training to learn the network weights, and to the best of our
knowledge, is the first algorithm that can make use of pre-trained models and
provably sample from any identifiable causal query in the presence of latent
confounders with high-dimensional data. We demonstrate the utility of our
algorithm using semi-synthetic and real-world datasets containing images as
variables in the causal structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Md Musfiqur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kocaoglu_M/0/1/0/all/0/1&quot;&gt;Murat Kocaoglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01442">
<title>Hierarchical Over-the-Air Federated Learning with Awareness of Interference and Data Heterogeneity. (arXiv:2401.01442v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2401.01442</link>
<description rdf:parseType="Literal">&lt;p&gt;When implementing hierarchical federated learning over wireless networks,
scalability assurance and the ability to handle both interference and device
data heterogeneity are crucial. This work introduces a learning method designed
to address these challenges, along with a scalable transmission scheme that
efficiently uses a single wireless resource through over-the-air computation.
To provide resistance against data heterogeneity, we employ gradient
aggregations. Meanwhile, the impact of interference is minimized through
optimized receiver normalizing factors. For this, we model a multi-cluster
wireless network using stochastic geometry, and characterize the mean squared
error of the aggregation estimations as a function of the network parameters.
We show that despite the interference and the data heterogeneity, the proposed
scheme achieves high learning accuracy and can significantly outperform the
conventional hierarchical algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azimi_Abarghouyi_S/0/1/0/all/0/1&quot;&gt;Seyed Mohammad Azimi-Abarghouyi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fodor_V/0/1/0/all/0/1&quot;&gt;Viktoria Fodor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01458">
<title>Concurrent Self-testing of Neural Networks Using Uncertainty Fingerprint. (arXiv:2401.01458v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.01458</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks (NNs) are increasingly used in always-on safety-critical
applications deployed on hardware accelerators (NN-HAs) employing various
memory technologies. Reliable continuous operation of NN is essential for
safety-critical applications. During online operation, NNs are susceptible to
single and multiple permanent and soft errors due to factors such as radiation,
aging, and thermal effects. Explicit NN-HA testing methods cannot detect
transient faults during inference, are unsuitable for always-on applications,
and require extensive test vector generation and storage. Therefore, in this
paper, we propose the \emph{uncertainty fingerprint} approach representing the
online fault status of NN. Furthermore, we propose a dual head NN topology
specifically designed to produce uncertainty fingerprints and the primary
prediction of the NN in \emph{a single shot}. During the online operation, by
matching the uncertainty fingerprint, we can concurrently self-test NNs with up
to $100\%$ coverage with a low false positive rate while maintaining a similar
performance of the primary task. Compared to existing works, memory overhead is
reduced by up to $243.7$ MB, multiply and accumulate (MAC) operation is reduced
by up to $10000\times$, and false-positive rates are reduced by up to $89\%$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1&quot;&gt;Soyed Tuhin Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+tahoori_M/0/1/0/all/0/1&quot;&gt;Mehdi B. tahoori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01459">
<title>Outlier Ranking in Large-Scale Public Health Streams. (arXiv:2401.01459v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.01459</link>
<description rdf:parseType="Literal">&lt;p&gt;Disease control experts inspect public health data streams daily for outliers
worth investigating, like those corresponding to data quality issues or disease
outbreaks. However, they can only examine a few of the thousands of
maximally-tied outliers returned by univariate outlier detection methods
applied to large-scale public health data streams. To help experts distinguish
the most important outliers from these thousands of tied outliers, we propose a
new task for algorithms to rank the outputs of any univariate method applied to
each of many streams. Our novel algorithm for this task, which leverages
hierarchical networks and extreme value analysis, performed the best across
traditional outlier detection metrics in a human-expert evaluation using public
health data streams. Most importantly, experts have used our open-source Python
implementation since April 2023 and report identifying outliers worth
investigating 9.1x faster than their prior baseline. Other organizations can
readily adapt this implementation to create rankings from the outputs of their
tailored univariate methods across large-scale streams.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1&quot;&gt;Ananya Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Townes_T/0/1/0/all/0/1&quot;&gt;Tina Townes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gormley_N/0/1/0/all/0/1&quot;&gt;Nolan Gormley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neureiter_L/0/1/0/all/0/1&quot;&gt;Luke Neureiter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenfeld_R/0/1/0/all/0/1&quot;&gt;Roni Rosenfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilder_B/0/1/0/all/0/1&quot;&gt;Bryan Wilder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01469">
<title>Question-Answering Based Summarization of Electronic Health Records using Retrieval Augmented Generation. (arXiv:2401.01469v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.01469</link>
<description rdf:parseType="Literal">&lt;p&gt;Summarization of electronic health records (EHRs) can substantially minimize
&apos;screen time&apos; for both patients as well as medical personnel. In recent years
summarization of EHRs have employed machine learning pipelines using state of
the art neural models. However, these models have produced less than adequate
results that are attributed to the difficulty of obtaining sufficient annotated
data for training. Moreover, the requirement to consider the entire content of
an EHR in summarization has resulted in poor performance due to the fact that
attention mechanisms in modern large language models (LLMs) adds a quadratic
complexity in terms of the size of the input. We propose here a method that
mitigates these shortcomings by combining semantic search, retrieval augmented
generation (RAG) and question-answering using the latest LLMs. In our approach
summarization is the extraction of answers to specific questions that are
deemed important by subject-matter experts (SMEs). Our approach is quite
efficient; requires minimal to no training; does not suffer from the
&apos;hallucination&apos; problem of LLMs; and it ensures diversity, since the summary
will not have repeated content but diverse answers to specific questions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saba_W/0/1/0/all/0/1&quot;&gt;Walid Saba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wendelken_S/0/1/0/all/0/1&quot;&gt;Suzanne Wendelken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanahan_J/0/1/0/all/0/1&quot;&gt;James. Shanahan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01470">
<title>Token Propagation Controller for Efficient Vision Transformer. (arXiv:2401.01470v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01470</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers (ViTs) have achieved promising results on a variety of
Computer Vision tasks, however their quadratic complexity in the number of
input tokens has limited their application specially in resource-constrained
settings. Previous approaches that employ gradual token reduction to address
this challenge assume that token redundancy in one layer implies redundancy in
all the following layers. We empirically demonstrate that this assumption is
often not correct, i.e., tokens that are redundant in one layer can be useful
in later layers. We employ this key insight to propose a novel token
propagation controller (TPC) that incorporates two different
token-distributions, i.e., pause probability and restart probability to control
the reduction and reuse of tokens respectively, which results in more efficient
token utilization. To improve the estimates of token distributions, we propose
a smoothing mechanism that acts as a regularizer and helps remove noisy
outliers. Furthermore, to improve the training-stability of our proposed TPC,
we introduce a model stabilizer that is able to implicitly encode local image
structures and minimize accuracy fluctuations during model training. We present
extensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT
and Swin models to demonstrate the effectiveness of our proposed method. For
example, compared to baseline models, our proposed method improves the
inference speed of the DeiT-S by 250% while increasing the classification
accuracy by 1.0%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wentao Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01482">
<title>Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition. (arXiv:2401.01482v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01482</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing object recognition models have been shown to lack robustness in
diverse geographical scenarios due to significant domain shifts in design and
context. Class representations need to be adapted to more accurately reflect an
object concept under these shifts. In the absence of training data from target
geographies, we hypothesize that geography-specific descriptive knowledge of
object categories can be leveraged to enhance robustness. For this purpose, we
explore the feasibility of probing a large-language model for
geography-specific object knowledge, and we investigate integrating knowledge
in zero-shot and learnable soft prompting with the CLIP vision-language model.
In particular, we propose a geography knowledge regularization method to ensure
that soft prompts trained on a source set of geographies generalize to an
unseen target set of geographies. Our gains on DollarStreet when generalizing
from a model trained only on data from Europe are as large as +2.8 on countries
from Africa, and +4.6 on the hardest classes. We further show competitive
performance vs. few-shot target training, and provide insights into how
descriptive knowledge captures geographical differences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buettner_K/0/1/0/all/0/1&quot;&gt;Kyle Buettner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malakouti_S/0/1/0/all/0/1&quot;&gt;Sina Malakouti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Lorraine Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1&quot;&gt;Adriana Kovashka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01484">
<title>Uncertainty Regularized Evidential Regression. (arXiv:2401.01484v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.01484</link>
<description rdf:parseType="Literal">&lt;p&gt;The Evidential Regression Network (ERN) represents a novel approach that
integrates deep learning with Dempster-Shafer&apos;s theory to predict a target and
quantify the associated uncertainty. Guided by the underlying theory, specific
activation functions must be employed to enforce non-negative values, which is
a constraint that compromises model performance by limiting its ability to
learn from all samples. This paper provides a theoretical analysis of this
limitation and introduces an improvement to overcome it. Initially, we define
the region where the models can&apos;t effectively learn from the samples. Following
this, we thoroughly analyze the ERN and investigate this constraint. Leveraging
the insights from our analysis, we address the limitation by introducing a
novel regularization term that empowers the ERN to learn from the whole
training set. Our extensive experiments substantiate our theoretical findings
and demonstrate the effectiveness of the proposed solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1&quot;&gt;Kai Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tiejin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hua Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1&quot;&gt;Liang Zhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01489">
<title>The Neuron as a Direct Data-Driven Controller. (arXiv:2401.01489v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2401.01489</link>
<description rdf:parseType="Literal">&lt;p&gt;In the quest to model neuronal function amidst gaps in physiological data, a
promising strategy is to develop a normative theory that interprets neuronal
physiology as optimizing a computational objective. This study extends the
current normative models, which primarily optimize prediction, by
conceptualizing neurons as optimal feedback controllers. We posit that neurons,
especially those beyond early sensory areas, act as controllers, steering their
environment towards a specific desired state through their output. This
environment comprises both synaptically interlinked neurons and external motor
sensory feedback loops, enabling neurons to evaluate the effectiveness of their
control via synaptic feedback. Utilizing the novel Direct Data-Driven Control
(DD-DC) framework, we model neurons as biologically feasible controllers which
implicitly identify loop dynamics, infer latent states and optimize control.
Our DD-DC neuron model explains various neurophysiological phenomena: the shift
from potentiation to depression in Spike-Timing-Dependent Plasticity (STDP)
with its asymmetry, the duration and adaptive nature of feedforward and
feedback neuronal filters, the imprecision in spike generation under constant
stimulation, and the characteristic operational variability and noise in the
brain. Our model presents a significant departure from the traditional,
feedforward, instant-response McCulloch-Pitts-Rosenblatt neuron, offering a
novel and biologically-informed fundamental unit for constructing neural
networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Moore_J/0/1/0/all/0/1&quot;&gt;Jason Moore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Genkin_A/0/1/0/all/0/1&quot;&gt;Alexander Genkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tournoy_M/0/1/0/all/0/1&quot;&gt;Magnus Tournoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Pughe_Sanford_J/0/1/0/all/0/1&quot;&gt;Joshua Pughe-Sanford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Steveninck_R/0/1/0/all/0/1&quot;&gt;Rob R. de Ruyter van Steveninck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chklovskii_D/0/1/0/all/0/1&quot;&gt;Dmitri B. Chklovskii&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01493">
<title>Free Lunch for Federated Remote Sensing Target Fine-Grained Classification: A Parameter-Efficient Framework. (arXiv:2401.01493v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.01493</link>
<description rdf:parseType="Literal">&lt;p&gt;Remote Sensing Target Fine-grained Classification (TFGC) is of great
significance in both military and civilian fields. Due to location differences,
growth in data size, and centralized server storage constraints, these data are
usually stored under different databases across regions/countries. However,
privacy laws and national security concerns constrain researchers from
accessing these sensitive remote sensing images for further analysis.
Additionally, low-resource remote sensing devices encounter challenges in terms
of communication overhead and efficiency when dealing with the ever-increasing
data and model scales. To solve the above challenges, this paper proposes a
novel Privacy-Reserving TFGC Framework based on Federated Learning, dubbed
PRFL. The proposed framework allows each client to learn global and local
knowledge to enhance the local representation of private data in environments
with extreme statistical heterogeneity (non. Independent and Identically
Distributed, IID). Thus, it provides highly customized models to clients with
differentiated data distributions. Moreover, the framework minimizes
communication overhead and improves efficiency while ensuring satisfactory
performance, thereby enhancing robustness and practical applicability under
resource-scarce conditions. We demonstrate the effectiveness of the proposed
PRFL on the classical TFGC task by leveraging four public datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shengchao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1&quot;&gt;Ting Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Huan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiahao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Sufen Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lina Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01496">
<title>From Pixel to Slide image: Polarization Modality-based Pathological Diagnosis Using Representation Learning. (arXiv:2401.01496v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.01496</link>
<description rdf:parseType="Literal">&lt;p&gt;Thyroid cancer is the most common endocrine malignancy, and accurately
distinguishing between benign and malignant thyroid tumors is crucial for
developing effective treatment plans in clinical practice. Pathologically,
thyroid tumors pose diagnostic challenges due to improper specimen sampling. In
this study, we have designed a three-stage model using representation learning
to integrate pixel-level and slice-level annotations for distinguishing thyroid
tumors. This structure includes a pathology structure recognition method to
predict structures related to thyroid tumors, an encoder-decoder network to
extract pixel-level annotation information by learning the feature
representations of image blocks, and an attention-based learning mechanism for
the final classification task. This mechanism learns the importance of
different image blocks in a pathological region, globally considering the
information from each block. In the third stage, all information from the image
blocks in a region is aggregated using attention mechanisms, followed by
classification to determine the category of the region. Experimental results
demonstrate that our proposed method can predict microscopic structures more
accurately. After color-coding, the method achieves results on unstained
pathology slides that approximate the quality of Hematoxylin and eosin
staining, reducing the need for stained pathology slides. Furthermore, by
leveraging the concept of indirect measurement and extracting polarized
features from structures correlated with lesions, the proposed method can also
classify samples where membrane structures cannot be obtained through sampling,
providing a potential objective and highly accurate indirect diagnostic
technique for thyroid tumors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jia Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yao Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Hui Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01519">
<title>Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review. (arXiv:2401.01519v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.01519</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the frontiers of large language models (LLMs) in
psychology applications. Psychology has undergone several theoretical changes,
and the current use of Artificial Intelligence (AI) and Machine Learning,
particularly LLMs, promises to open up new research directions. We provide a
detailed exploration of how LLMs like ChatGPT are transforming psychological
research. It discusses the impact of LLMs across various branches of
psychology, including cognitive and behavioral, clinical and counseling,
educational and developmental, and social and cultural psychology, highlighting
their potential to simulate aspects of human cognition and behavior. The paper
delves into the capabilities of these models to emulate human-like text
generation, offering innovative tools for literature review, hypothesis
generation, experimental design, experimental subjects, data analysis, academic
writing, and peer review in psychology. While LLMs are essential in advancing
research methodologies in psychology, the paper also cautions about their
technical and ethical challenges. There are issues like data privacy, the
ethical implications of using LLMs in psychological research, and the need for
a deeper understanding of these models&apos; limitations. Researchers should
responsibly use LLMs in psychological studies, adhering to ethical standards
and considering the potential consequences of deploying these technologies in
sensitive areas. Overall, the article provides a comprehensive overview of the
current state of LLMs in psychology, exploring potential benefits and
challenges. It serves as a call to action for researchers to leverage LLLs&apos;
advantages responsibly while addressing associated risks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1&quot;&gt;Luoma Ke&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1&quot;&gt;Song Tong&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Peng Chen&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1&quot;&gt;Kaiping Peng&lt;/a&gt; (1) ((1) Department of Psychology, Tsinghua University, (2) School of Social Science, Tsinghua University)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01523">
<title>GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse. (arXiv:2401.01523v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.01523</link>
<description rdf:parseType="Literal">&lt;p&gt;The exponential growth of social media has profoundly transformed how
information is created, disseminated, and absorbed, exceeding any precedent in
the digital age. Regrettably, this explosion has also spawned a significant
increase in the online abuse of memes. Evaluating the negative impact of memes
is notably challenging, owing to their often subtle and implicit meanings,
which are not directly conveyed through the overt text and imagery. In light of
this, large multimodal models (LMMs) have emerged as a focal point of interest
due to their remarkable capabilities in handling diverse multimodal tasks. In
response to this development, our paper aims to thoroughly examine the capacity
of various LMMs (e.g. GPT-4V) to discern and respond to the nuanced aspects of
social abuse manifested in memes. We introduce the comprehensive meme
benchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes
such as implicit hate speech, sexism, and cyberbullying, etc. Utilizing
GOAT-Bench, we delve into the ability of LMMs to accurately assess hatefulness,
misogyny, offensiveness, sarcasm, and harmful content. Our extensive
experiments across a range of LMMs reveal that current models still exhibit a
deficiency in safety awareness, showing insensitivity to various forms of
implicit abuse. We posit that this shortfall represents a critical impediment
to the realization of safe artificial intelligence. The GOAT-Bench and
accompanying resources are publicly accessible at https://goatlmm.github.io/,
contributing to ongoing research in this vital field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hongzhan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Ziyang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Ruichao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jing Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01537">
<title>The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers. (arXiv:2401.01537v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.01537</link>
<description rdf:parseType="Literal">&lt;p&gt;The area of Machine Learning as a Service (MLaaS) is experiencing increased
implementation due to recent advancements in the AI (Artificial Intelligence)
industry. However, this spike has prompted concerns regarding AI defense
mechanisms, specifically regarding potential covert attacks from third-party
providers that cannot be entirely trusted. Recent research has uncovered that
auditory backdoors may use certain modifications as their initiating mechanism.
DynamicTrigger is introduced as a methodology for carrying out dynamic backdoor
attacks that use cleverly designed tweaks to ensure that corrupted samples are
indistinguishable from clean. By utilizing fluctuating signal sampling rates
and masking speaker identities through dynamic sound triggers (such as the
clapping of hands), it is possible to deceive speech recognition systems (ASR).
Our empirical testing demonstrates that DynamicTrigger is both potent and
stealthy, achieving impressive success rates during covert attacks while
maintaining exceptional accuracy with non-poisoned datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mengara_O/0/1/0/all/0/1&quot;&gt;Orson Mengara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01542">
<title>Adversarial Machine Learning-Enabled Anonymization of OpenWiFi Data. (arXiv:2401.01542v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2401.01542</link>
<description rdf:parseType="Literal">&lt;p&gt;Data privacy and protection through anonymization is a critical issue for
network operators or data owners before it is forwarded for other possible use
of data. With the adoption of Artificial Intelligence (AI), data anonymization
augments the likelihood of covering up necessary sensitive information;
preventing data leakage and information loss. OpenWiFi networks are vulnerable
to any adversary who is trying to gain access or knowledge on traffic
regardless of the knowledge possessed by data owners. The odds for discovery of
actual traffic information is addressed by applied conditional tabular
generative adversarial network (CTGAN). CTGAN yields synthetic data; which
disguises as actual data but fostering hidden acute information of actual data.
In this paper, the similarity assessment of synthetic with actual data is
showcased in terms of clustering algorithms followed by a comparison of
performance for unsupervised cluster validation metrics. A well-known
algorithm, K-means outperforms other algorithms in terms of similarity
assessment of synthetic data over real data while achieving nearest scores
0.634, 23714.57, and 0.598 as Silhouette, Calinski and Harabasz and Davies
Bouldin metric respectively. On exploiting a comparative analysis in validation
scores among several algorithms, K-means forms the epitome of unsupervised
clustering algorithms ensuring explicit usage of synthetic data at the same
time a replacement for real data. Hence, the experimental results aim to show
the viability of using CTGAN-generated synthetic data in lieu of publishing
anonymized data to be utilized in various applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuili_S/0/1/0/all/0/1&quot;&gt;Samhita Kuili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dabbour_K/0/1/0/all/0/1&quot;&gt;Kareem Dabbour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_I/0/1/0/all/0/1&quot;&gt;Irtiza Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herscovich_A/0/1/0/all/0/1&quot;&gt;Andrea Herscovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kantarci_B/0/1/0/all/0/1&quot;&gt;Burak Kantarci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chenier_M/0/1/0/all/0/1&quot;&gt;Marcel Chenier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erol_Kantarci_M/0/1/0/all/0/1&quot;&gt;Melike Erol-Kantarci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01596">
<title>MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries. (arXiv:2401.01596v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.01596</link>
<description rdf:parseType="Literal">&lt;p&gt;In the healthcare domain, summarizing medical questions posed by patients is
critical for improving doctor-patient interactions and medical decision-making.
Although medical data has grown in complexity and quantity, the current body of
research in this domain has primarily concentrated on text-based methods,
overlooking the integration of visual cues. Also prior works in the area of
medical question summarisation have been limited to the English language. This
work introduces the task of multimodal medical question summarization for
codemixed input in a low-resource setting. To address this gap, we introduce
the Multimodal Medical Codemixed Question Summarization MMCQS dataset, which
combines Hindi-English codemixed medical queries with visual aids. This
integration enriches the representation of a patient&apos;s medical condition,
providing a more comprehensive perspective. We also propose a framework named
MedSumm that leverages the power of LLMs and VLMs for this task. By utilizing
our MMCQS dataset, we demonstrate the value of integrating visual information
from images to improve the creation of medically detailed summaries. This
multimodal strategy not only improves healthcare decision-making but also
promotes a deeper comprehension of patient queries, paving the way for future
exploration in personalized and responsive medical care. Our dataset, code, and
pre-trained models will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1&quot;&gt;Akash Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1&quot;&gt;Arkadeep Acharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_P/0/1/0/all/0/1&quot;&gt;Prince Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaudgaul_A/0/1/0/all/0/1&quot;&gt;Aniket Gaudgaul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumdar_R/0/1/0/all/0/1&quot;&gt;Rajdeep Majumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1&quot;&gt;Sriparna Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1&quot;&gt;Aman Chadha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1&quot;&gt;Raghav Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1&quot;&gt;Setu Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1&quot;&gt;Shivani Agarwal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01600">
<title>PLLaMa: An Open-source Large Language Model for Plant Science. (arXiv:2401.01600v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.01600</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have exhibited remarkable capabilities in
understanding and interacting with natural language across various sectors.
However, their effectiveness is limited in specialized areas requiring high
accuracy, such as plant science, due to a lack of specific expertise in these
fields. This paper introduces PLLaMa, an open-source language model that
evolved from LLaMa-2. It&apos;s enhanced with a comprehensive database, comprising
more than 1.5 million scholarly articles in plant science. This development
significantly enriches PLLaMa with extensive knowledge and proficiency in plant
and agricultural sciences. Our initial tests, involving specific datasets
related to plants and agriculture, show that PLLaMa substantially improves its
understanding of plant science-related topics. Moreover, we have formed an
international panel of professionals, including plant scientists, agricultural
engineers, and plant breeders. This team plays a crucial role in verifying the
accuracy of PLLaMa&apos;s responses to various academic inquiries, ensuring its
effective and reliable application in the field. To support further research
and development, we have made the model&apos;s checkpoints and source codes
accessible to the scientific community. These resources are available for
download at \url{https://github.com/Xianjun-Yang/PLLaMa}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xianjun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Junfeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1&quot;&gt;Wenxin Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexandersson_E/0/1/0/all/0/1&quot;&gt;Erik Alexandersson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01614">
<title>GPT-4V(ision) is a Generalist Web Agent, if Grounded. (arXiv:2401.01614v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2401.01614</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent development on large multimodal models (LMMs), especially
GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries
of multimodal models beyond traditional tasks like image captioning and visual
question answering. In this work, we explore the potential of LMMs like GPT-4V
as a generalist web agent that can follow natural language instructions to
complete tasks on any given website. We propose SEEACT, a generalist web agent
that harnesses the power of LMMs for integrated visual understanding and acting
on the web. We evaluate on the recent MIND2WEB benchmark. In addition to
standard offline evaluation on cached websites, we enable a new online
evaluation setting by developing a tool that allows running web agents on live
websites. We show that GPT-4V presents a great potential for web agents - it
can successfully complete 50% of the tasks on live websites if we manually
ground its textual plans into actions on the websites. This substantially
outperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)
specifically fine-tuned for web agents. However, grounding still remains a
major challenge. Existing LMM grounding strategies like set-of-mark prompting
turns out not effective for web agents, and the best grounding strategy we
develop in this paper leverages both the HTML text and visuals. Yet, there is
still a substantial gap with oracle grounding, leaving ample room for further
improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1&quot;&gt;Boyuan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gou_B/0/1/0/all/0/1&quot;&gt;Boyu Gou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kil_J/0/1/0/all/0/1&quot;&gt;Jihyung Kil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Huan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yu Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01620">
<title>Large Language Model Capabilities in Perioperative Risk Prediction and Prognostication. (arXiv:2401.01620v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.01620</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate whether general-domain large language models such as GPT-4
Turbo can perform risk stratification and predict post-operative outcome
measures using a description of the procedure and a patient&apos;s clinical notes
derived from the electronic health record. We examine predictive performance on
8 different tasks: prediction of ASA Physical Status Classification, hospital
admission, ICU admission, unplanned admission, hospital mortality, PACU Phase 1
duration, hospital duration, and ICU duration. Few-shot and chain-of-thought
prompting improves predictive performance for several of the tasks. We achieve
F1 scores of 0.50 for ASA Physical Status Classification, 0.81 for ICU
admission, and 0.86 for hospital mortality. Performance on duration prediction
tasks were universally poor across all prompt strategies. Current generation
large language models can assist clinicians in perioperative risk
stratification on classification tasks and produce high-quality natural
language summaries and explanations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_P/0/1/0/all/0/1&quot;&gt;Philip Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fong_C/0/1/0/all/0/1&quot;&gt;Christine T Fong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walters_A/0/1/0/all/0/1&quot;&gt;Andrew M Walters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aghaeepour_N/0/1/0/all/0/1&quot;&gt;Nima Aghaeepour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yetisgen_M/0/1/0/all/0/1&quot;&gt;Meliha Yetisgen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OReilly_Shah_V/0/1/0/all/0/1&quot;&gt;Vikas N O&amp;#x27;Reilly-Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01623">
<title>Can AI Be as Creative as Humans?. (arXiv:2401.01623v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.01623</link>
<description rdf:parseType="Literal">&lt;p&gt;Creativity serves as a cornerstone for societal progress and innovation, but
its assessment remains a complex and often subjective endeavor. With the rise
of advanced generative AI models capable of tasks once reserved for human
creativity, the study of AI&apos;s creative potential becomes imperative for its
responsible development and application. This paper addresses the complexities
in defining and evaluating creativity by introducing a new concept called
Relative Creativity. Instead of trying to define creativity universally, we
shift the focus to whether AI can match the creative abilities of a
hypothetical human. This perspective draws inspiration from the Turing Test,
expanding upon it to address the challenges and subjectivities inherent in
evaluating creativity. This methodological shift facilitates a statistically
quantifiable evaluation of AI&apos;s creativity, which we term Statistical
Creativity. This approach allows for direct comparisons of AI&apos;s creative
abilities with those of specific human groups. Building on this foundation, we
discuss the application of statistical creativity in contemporary
prompt-conditioned autoregressive models. In addition to defining and analyzing
a measure of creativity, we introduce an actionable training guideline,
effectively bridging the gap between theoretical quantification of creativity
and practical model training. Through these multifaceted contributions, the
paper establishes a cohesive, continuously evolving, and transformative
framework for assessing and fostering statistical creativity in AI models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haonan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1&quot;&gt;Michael Mozer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Linjun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1&quot;&gt;Anirudh Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamb_A/0/1/0/all/0/1&quot;&gt;Alex Lamb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhun Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1&quot;&gt;Michael Qizhe Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_H/0/1/0/all/0/1&quot;&gt;Hannah Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1&quot;&gt;Kenji Kawaguchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01626">
<title>On the Expressive Power of Graph Neural Networks. (arXiv:2401.01626v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.01626</link>
<description rdf:parseType="Literal">&lt;p&gt;The study of Graph Neural Networks has received considerable interest in the
past few years. By extending deep learning to graph-structured data, GNNs can
solve a diverse set of tasks in fields including social science, chemistry, and
medicine. The development of GNN architectures has largely been focused on
improving empirical performance on tasks like node or graph classification.
However, a line of recent work has instead sought to find GNN architectures
that have desirable theoretical properties - by studying their expressive power
and designing architectures that maximize this expressiveness.
&lt;/p&gt;
&lt;p&gt;While there is no consensus on the best way to define the expressiveness of a
GNN, it can be viewed from several well-motivated perspectives. Perhaps the
most natural approach is to study the universal approximation properties of
GNNs, much in the way that this has been studied extensively for MLPs. Another
direction focuses on the extent to which GNNs can distinguish between different
graph structures, relating this to the graph isomorphism test. Besides, a GNN&apos;s
ability to compute graph properties such as graph moments has been suggested as
another form of expressiveness. All of these different definitions are
complementary and have yielded different recommendations for GNN architecture
choices. In this paper, we would like to give an overview of the notion of
&quot;expressive power&quot; of GNNs and provide some valuable insights regarding the
design choices of GNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nalwade_A/0/1/0/all/0/1&quot;&gt;Ashwin Nalwade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marshall_K/0/1/0/all/0/1&quot;&gt;Kelly Marshall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eladi_A/0/1/0/all/0/1&quot;&gt;Axel Eladi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_U/0/1/0/all/0/1&quot;&gt;Umang Sharma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01629">
<title>Synthetic Data in AI: Challenges, Applications, and Ethical Implications. (arXiv:2401.01629v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.01629</link>
<description rdf:parseType="Literal">&lt;p&gt;In the rapidly evolving field of artificial intelligence, the creation and
utilization of synthetic datasets have become increasingly significant. This
report delves into the multifaceted aspects of synthetic data, particularly
emphasizing the challenges and potential biases these datasets may harbor. It
explores the methodologies behind synthetic data generation, spanning
traditional statistical models to advanced deep learning techniques, and
examines their applications across diverse domains. The report also critically
addresses the ethical considerations and legal implications associated with
synthetic datasets, highlighting the urgent need for mechanisms to ensure
fairness, mitigate biases, and uphold ethical standards in AI development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1&quot;&gt;Shuang Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1&quot;&gt;Wenfeng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1&quot;&gt;Tao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiping Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haonan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1&quot;&gt;Chunlin Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhangjun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;He Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01630">
<title>A Cybersecurity Risk Analysis Framework for Systems with Artificial Intelligence Components. (arXiv:2401.01630v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.01630</link>
<description rdf:parseType="Literal">&lt;p&gt;The introduction of the European Union Artificial Intelligence Act, the NIST
Artificial Intelligence Risk Management Framework, and related norms demands a
better understanding and implementation of novel risk analysis approaches to
evaluate systems with Artificial Intelligence components. This paper provides a
cybersecurity risk analysis framework that can help assessing such systems. We
use an illustrative example concerning automated driving systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camacho_J/0/1/0/all/0/1&quot;&gt;Jose Manuel Camacho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couce_Vieira_A/0/1/0/all/0/1&quot;&gt;Aitor Couce-Vieira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arroyo_D/0/1/0/all/0/1&quot;&gt;David Arroyo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Insua_D/0/1/0/all/0/1&quot;&gt;David Rios Insua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01651">
<title>AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated by AI. (arXiv:2401.01651v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01651</link>
<description rdf:parseType="Literal">&lt;p&gt;The burgeoning field of Artificial Intelligence Generated Content (AIGC) is
witnessing rapid advancements, particularly in video generation. This paper
introduces AIGCBench, a pioneering comprehensive and scalable benchmark
designed to evaluate a variety of video generation tasks, with a primary focus
on Image-to-Video (I2V) generation. AIGCBench tackles the limitations of
existing benchmarks, which suffer from a lack of diverse datasets, by including
a varied and open-domain image-text dataset that evaluates different
state-of-the-art algorithms under equivalent conditions. We employ a novel text
combiner and GPT-4 to create rich text prompts, which are then used to generate
images via advanced Text-to-Image models. To establish a unified evaluation
framework for video generation tasks, our benchmark includes 11 metrics
spanning four dimensions to assess algorithm performance. These dimensions are
control-video alignment, motion effects, temporal consistency, and video
quality. These metrics are both reference video-dependent and video-free,
ensuring a comprehensive evaluation strategy. The evaluation standard proposed
correlates well with human judgment, providing insights into the strengths and
weaknesses of current I2V algorithms. The findings from our extensive
experiments aim to stimulate further research and development in the I2V field.
AIGCBench represents a significant step toward creating standardized benchmarks
for the broader AIGC landscape, proposing an adaptable and equitable framework
for future assessments of video generation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1&quot;&gt;Fanda Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chunjie Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1&quot;&gt;Jianfeng Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wanling Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01656">
<title>Deep Automated Mechanism Design for Integrating Ad Auction and Allocation in Feed. (arXiv:2401.01656v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2401.01656</link>
<description rdf:parseType="Literal">&lt;p&gt;E-commerce platforms usually present an ordered list, mixed with several
organic items and an advertisement, in response to each user&apos;s page view
request. This list, the outcome of ad auction and allocation processes,
directly impacts the platform&apos;s ad revenue and gross merchandise volume (GMV).
Specifically, the ad auction determines which ad is displayed and the
corresponding payment, while the ad allocation decides the display positions of
the advertisement and organic items. The prevalent methods of segregating the
ad auction and allocation into two distinct stages face two problems: 1) Ad
auction does not consider externalities, such as the influence of actual
display position and context on ad Click-Through Rate (CTR); 2) The ad
allocation, which utilizes the auction-winning ad&apos;s payment to determine the
display position dynamically, fails to maintain incentive compatibility (IC)
for the advertisement. For instance, in the auction stage employing the
traditional Generalized Second Price (GSP) , even if the winning ad increases
its bid, its payment remains unchanged. This implies that the advertisement
cannot secure a better position and thus loses the opportunity to achieve
higher utility in the subsequent ad allocation stage. Previous research often
focused on one of the two stages, neglecting the two-stage problem, which may
result in suboptimal outcomes...
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuejian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Bingqi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1&quot;&gt;Fei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yongkang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xingxing Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01728">
<title>Ravnest: Decentralized Asynchronous Training on Heterogeneous Devices. (arXiv:2401.01728v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.01728</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern deep learning models, growing larger and more complex, have
demonstrated exceptional generalization and accuracy due to training on huge
datasets. This trend is expected to continue. However, the increasing size of
these models poses challenges in training, as traditional centralized methods
are limited by memory constraints at such scales. This paper proposes an
asynchronous decentralized training paradigm for large modern deep learning
models that harnesses the compute power of regular heterogeneous PCs with
limited resources connected across the internet to achieve favourable
performance metrics. Ravnest facilitates decentralized training by efficiently
organizing compute nodes into clusters with similar data transfer rates and
compute capabilities, without necessitating that each node hosts the entire
model. These clusters engage in $\textit{Zero-Bubble Asynchronous Model
Parallel}$ training, and a $\textit{Parallel Multi-Ring All-Reduce}$ method is
employed to effectively execute global parameter averaging across all clusters.
We have framed our asynchronous SGD loss function as a block structured
optimization problem with delayed updates and derived an optimal convergence
rate of $O\left(\frac{1}{\sqrt{K}}\right)$. We further discuss linear speedup
with respect to the number of participating clusters and the bound on the
staleness parameter.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1&quot;&gt;Anirudh Rajiv Menon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menon_U/0/1/0/all/0/1&quot;&gt;Unnikrishnan Menon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahirwar_K/0/1/0/all/0/1&quot;&gt;Kailash Ahirwar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01732">
<title>Task and Explanation Network. (arXiv:2401.01732v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.01732</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainability in deep networks has gained increased importance in recent
years. We argue herein that an AI must be tasked not just with a task but also
with an explanation of why said task was accomplished as such. We present a
basic framework -- Task and Explanation Network (TENet) -- which fully
integrates task completion and its explanation. We believe that the field of AI
as a whole should insist -- quite emphatically -- on explainability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sipper_M/0/1/0/all/0/1&quot;&gt;Moshe Sipper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01753">
<title>A Generative AI Assistant to Accelerate Cloud Migration. (arXiv:2401.01753v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.01753</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a tool that leverages generative AI to accelerate the migration of
on-premises applications to the cloud. The Cloud Migration LLM accepts input
from the user specifying the parameters of their migration, and outputs a
migration strategy with an architecture diagram. A user study suggests that the
migration LLM can assist inexperienced users in finding the right cloud
migration profile, while avoiding complexities of a manual approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaidya_A/0/1/0/all/0/1&quot;&gt;Amal Vaidya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vankayalapati_M/0/1/0/all/0/1&quot;&gt;Mohan Krishna Vankayalapati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1&quot;&gt;Jacky Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibraimoski_S/0/1/0/all/0/1&quot;&gt;Senad Ibraimoski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1&quot;&gt;Sean Moran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01754">
<title>Using AI/ML to Find and Remediate Enterprise Secrets in Code &amp; Document Sharing Platforms. (arXiv:2401.01754v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.01754</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new challenge to the software development community: 1)
leveraging AI to accurately detect and flag up secrets in code and on popular
document sharing platforms that frequently used by developers, such as
Confluence and 2) automatically remediating the detections (e.g. by suggesting
password vault functionality). This is a challenging, and mostly unaddressed
task. Existing methods leverage heuristics and regular expressions, that can be
very noisy, and therefore increase toil on developers. The next step -
modifying code itself - to automatically remediate a detection, is a complex
task. We introduce two baseline AI models that have good detection performance
and propose an automatic mechanism for remediating secrets found in code,
opening up the study of this task to the wider community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerr_G/0/1/0/all/0/1&quot;&gt;Gregor Kerr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Algorry_D/0/1/0/all/0/1&quot;&gt;David Algorry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibraimoski_S/0/1/0/all/0/1&quot;&gt;Senad Ibraimoski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maciver_P/0/1/0/all/0/1&quot;&gt;Peter Maciver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1&quot;&gt;Sean Moran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01755">
<title>Incremental FastPitch: Chunk-based High Quality Text to Speech. (arXiv:2401.01755v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.01755</link>
<description rdf:parseType="Literal">&lt;p&gt;Parallel text-to-speech models have been widely applied for real-time speech
synthesis, and they offer more controllability and a much faster synthesis
process compared with conventional auto-regressive models. Although parallel
models have benefits in many aspects, they become naturally unfit for
incremental synthesis due to their fully parallel architecture such as
transformer. In this work, we propose Incremental FastPitch, a novel FastPitch
variant capable of incrementally producing high-quality Mel chunks by improving
the architecture with chunk-based FFT blocks, training with receptive-field
constrained chunk attention masks, and inference with fixed size past model
states. Experimental results show that our proposal can produce speech quality
comparable to the parallel FastPitch, with a significant lower latency that
allows even lower response time for real-time speech applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Muyang Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1&quot;&gt;Junjie Lai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01772">
<title>A Novel Paradigm for Neural Computation: X-Net with Learnable Neurons and Adaptable Structure. (arXiv:2401.01772v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.01772</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial neural networks (ANNs) have permeated various disciplinary
domains, ranging from bioinformatics to financial analytics, where their
application has become an indispensable facet of contemporary scientific
research endeavors. However, the inherent limitations of traditional neural
networks arise due to their relatively fixed network structures and activation
functions. 1, The type of activation function is single and relatively fixed,
which leads to poor &quot;unit representation ability&quot; of the network, and it is
often used to solve simple problems with very complex networks; 2, the network
structure is not adaptive, it is easy to cause network structure redundant or
insufficient. To address the aforementioned issues, this study proposes a novel
neural network called X-Net. By utilizing our designed Alternating
Backpropagation mechanism, X-Net dynamically selects appropriate activation
functions based on derivative information during training to enhance the
network&apos;s representation capability for specific tasks. Simultaneously, it
accurately adjusts the network structure at the neuron level to accommodate
tasks of varying complexities and reduce computational costs. Through a series
of experiments, we demonstrate the dual advantages of X-Net in terms of
reducing model size and improving representation power. Specifically, in terms
of the number of parameters, X-Net is only 3$\%$ of baselines on average, and
only 1.4$\%$ under some tasks. In terms of representation ability, X-Net can
achieve an average $R^2$=0.985 on the fitting task by only optimizing the
activation function without introducing any parameters. Finally, we also tested
the ability of X-Net to help scientific discovery on data from multiple
disciplines such as society, energy, environment, and aerospace, and achieved
concise and good results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weijun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lina Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Min Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_M/0/1/0/all/0/1&quot;&gt;Meilan Hao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01788">
<title>Applications of machine learning and IoT for Outdoor Air Pollution Monitoring and Prediction: A Systematic Literature Review. (arXiv:2401.01788v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.01788</link>
<description rdf:parseType="Literal">&lt;p&gt;According to the World Health Organization (WHO), air pollution kills seven
million people every year. Outdoor air pollution is a major environmental
health problem affecting low, middle, and high-income countries. In the past
few years, the research community has explored IoT-enabled machine learning
applications for outdoor air pollution prediction. The general objective of
this paper is to systematically review applications of machine learning and
Internet of Things (IoT) for outdoor air pollution prediction and the
combination of monitoring sensors and input features used. Two research
questions were formulated for this review. 1086 publications were collected in
the initial PRISMA stage. After the screening and eligibility phases, 37 papers
were selected for inclusion. A cost-based analysis was conducted on the
findings to highlight high-cost monitoring, low-cost IoT and hybrid enabled
prediction. Three methods of prediction were identified: time series,
feature-based and spatio-temporal. This review&apos;s findings identify major
limitations in applications found in the literature, namely lack of coverage,
lack of diversity of data and lack of inclusion of context-specific features.
This review proposes directions for future research and underlines practical
implications in healthcare, urban planning, global synergy and smart cities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gryech_I/0/1/0/all/0/1&quot;&gt;Ihsane Gryech&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assad_C/0/1/0/all/0/1&quot;&gt;Chaimae Assad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghogho_M/0/1/0/all/0/1&quot;&gt;Mounir Ghogho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobbane_A/0/1/0/all/0/1&quot;&gt;Abdellatif Kobbane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01789">
<title>Deep learning the Hurst parameter of linear fractional processes and assessing its reliability. (arXiv:2401.01789v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2401.01789</link>
<description rdf:parseType="Literal">&lt;p&gt;This research explores the reliability of deep learning, specifically Long
Short-Term Memory (LSTM) networks, for estimating the Hurst parameter in
fractional stochastic processes. The study focuses on three types of processes:
fractional Brownian motion (fBm), fractional Ornstein-Uhlenbeck (fOU) process,
and linear fractional stable motions (lfsm). The work involves a fast
generation of extensive datasets for fBm and fOU to train the LSTM network on a
large volume of data in a feasible time. The study analyses the accuracy of the
LSTM network&apos;s Hurst parameter estimation regarding various performance
measures like RMSE, MAE, MRE, and quantiles of the absolute and relative
errors. It finds that LSTM outperforms the traditional statistical methods in
the case of fBm and fOU processes; however, it has limited accuracy on lfsm
processes. The research also delves into the implications of training length
and valuation sequence length on the LSTM&apos;s performance. The methodology is
applied by estimating the Hurst parameter in Li-ion battery degradation data
and obtaining confidence bounds for the estimation. The study concludes that
while deep learning methods show promise in parameter estimation of fractional
processes, their effectiveness is contingent on the process type and the
quality of training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boros_D/0/1/0/all/0/1&quot;&gt;D&amp;#xe1;niel Boros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Csanady_B/0/1/0/all/0/1&quot;&gt;B&amp;#xe1;lint Csan&amp;#xe1;dy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ivkovic_I/0/1/0/all/0/1&quot;&gt;Iv&amp;#xe1;n Ivkovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nagy_L/0/1/0/all/0/1&quot;&gt;L&amp;#xf3;r&amp;#xe1;nt Nagy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lukacs_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe1;s Luk&amp;#xe1;cs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Markus_L/0/1/0/all/0/1&quot;&gt;L&amp;#xe1;szl&amp;#xf3; M&amp;#xe1;rkus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01792">
<title>CoMoSVC: Consistency Model-based Singing Voice Conversion. (arXiv:2401.01792v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2401.01792</link>
<description rdf:parseType="Literal">&lt;p&gt;The diffusion-based Singing Voice Conversion (SVC) methods have achieved
remarkable performances, producing natural audios with high similarity to the
target timbre. However, the iterative sampling process results in slow
inference speed, and acceleration thus becomes crucial. In this paper, we
propose CoMoSVC, a consistency model-based SVC method, which aims to achieve
both high-quality generation and high-speed sampling. A diffusion-based teacher
model is first specially designed for SVC, and a student model is further
distilled under self-consistency properties to achieve one-step sampling.
Experiments on a single NVIDIA GTX4090 GPU reveal that although CoMoSVC has a
significantly faster inference speed than the state-of-the-art (SOTA)
diffusion-based SVC system, it still achieves comparable or superior conversion
performance based on both subjective and objective metrics. Audio samples and
codes are available at https://comosvc.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yiwen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ye_Z/0/1/0/all/0/1&quot;&gt;Zhen Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xue_W/0/1/0/all/0/1&quot;&gt;Wei Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1&quot;&gt;Xu Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qifeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yike Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01801">
<title>A quatum inspired neural network for geometric modeling. (arXiv:2401.01801v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.01801</link>
<description rdf:parseType="Literal">&lt;p&gt;By conceiving physical systems as 3D many-body point clouds, geometric graph
neural networks (GNNs), such as SE(3)/E(3) equivalent GNNs, have showcased
promising performance. In particular, their effective message-passing mechanics
make them adept at modeling molecules and crystalline materials. However,
current geometric GNNs only offer a mean-field approximation of the many-body
system, encapsulated within two-body message passing, thus falling short in
capturing intricate relationships within these geometric graphs. To address
this limitation, tensor networks, widely employed by computational physics to
handle manybody systems using high-order tensors, have been introduced.
Nevertheless, integrating these tensorized networks into the message-passing
framework of GNNs faces scalability and symmetry conservation (e.g.,
permutation and rotation) challenges. In response, we introduce an innovative
equivariant Matrix Product State (MPS)-based message-passing strategy, through
achieving an efficient implementation of the tensor contraction operation. Our
method effectively models complex many-body relationships, suppressing
mean-field approximations, and captures symmetries within geometric graphs.
Importantly, it seamlessly replaces the standard message-passing and
layer-aggregation modules intrinsic to geometric GNNs. We empirically validate
the superior accuracy of our approach on benchmark tasks, including predicting
classical Newton systems and quantum tensor Hamiltonian matrices. To our
knowledge, our approach represents the inaugural utilization of parameterized
geometric tensor networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1&quot;&gt;Weitao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shengchao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Hongyu Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01814">
<title>Large Language Models Relearn Removed Concepts. (arXiv:2401.01814v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.01814</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in model editing through neuron pruning hold promise for removing
undesirable concepts from large language models. However, it remains unclear
whether models have the capacity to reacquire pruned concepts after editing. To
investigate this, we evaluate concept relearning in models by tracking concept
saliency and similarity in pruned neurons during retraining. Our findings
reveal that models can quickly regain performance post-pruning by relocating
advanced concepts to earlier layers and reallocating pruned concepts to primed
neurons with similar semantics. This demonstrates that models exhibit
polysemantic capacities and can blend old and new concepts in individual
neurons. While neuron pruning provides interpretability into model concepts,
our results highlight the challenges of permanent concept removal for improved
model \textit{safety}. Monitoring concept reemergence and developing techniques
to mitigate relearning of unsafe concepts will be important directions for more
robust model editing. Overall, our work strongly demonstrates the resilience
and fluidity of concept representations in LLMs post concept removal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_M/0/1/0/all/0/1&quot;&gt;Michelle Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1&quot;&gt;Shay B. Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1&quot;&gt;Fazl Barez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01830">
<title>Iterative Mask Filling: An Effective Text Augmentation Method Using Masked Language Modeling. (arXiv:2401.01830v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.01830</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation is an effective technique for improving the performance of
machine learning models. However, it has not been explored as extensively in
natural language processing (NLP) as it has in computer vision. In this paper,
we propose a novel text augmentation method that leverages the Fill-Mask
feature of the transformer-based BERT model. Our method involves iteratively
masking words in a sentence and replacing them with language model predictions.
We have tested our proposed method on various NLP tasks and found it to be
effective in many cases. Our results are presented along with a comparison to
existing augmentation methods. Experimental results show that our proposed
method significantly improves performance, especially on topic classification
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kesgin_H/0/1/0/all/0/1&quot;&gt;Himmet Toprak Kesgin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amasyali_M/0/1/0/all/0/1&quot;&gt;Mehmet Fatih Amasyali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01835">
<title>Concurrent Brainstorming &amp; Hypothesis Satisfying: An Iterative Framework for Enhanced Retrieval-Augmented Generation (R2CBR3H-SR). (arXiv:2401.01835v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2401.01835</link>
<description rdf:parseType="Literal">&lt;p&gt;Addressing the complexity of comprehensive information retrieval, this study
introduces an innovative, iterative retrieval-augmented generation system. Our
approach uniquely integrates a vector-space driven re-ranking mechanism with
concurrent brainstorming to expedite the retrieval of highly relevant
documents, thereby streamlining the generation of potential queries. This sets
the stage for our novel hybrid process, which synergistically combines
hypothesis formulation with satisfying decision-making strategy to determine
content adequacy, leveraging a chain of thought-based prompting technique. This
unified hypothesize-satisfied phase intelligently distills information to
ascertain whether user queries have been satisfactorily addressed. Upon
reaching this criterion, the system refines its output into a concise
representation, maximizing conceptual density with minimal verbosity. The
iterative nature of the workflow enhances process efficiency and accuracy.
Crucially, the concurrency within the brainstorming phase significantly
accelerates recursive operations, facilitating rapid convergence to solution
satisfaction. Compared to conventional methods, our system demonstrates a
marked improvement in computational time and cost-effectiveness. This research
advances the state-of-the-art in intelligent retrieval systems, setting a new
benchmark for resource-efficient information extraction and abstraction in
knowledge-intensive applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahmansoori_A/0/1/0/all/0/1&quot;&gt;Arash Shahmansoori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01836">
<title>NODEC: Neural ODE For Optimal Control of Unknown Dynamical Systems. (arXiv:2401.01836v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.01836</link>
<description rdf:parseType="Literal">&lt;p&gt;Controlling complex dynamical systems is generally associated with minimizing
certain control objectives with known dynamics under the variational calculus
framework. For systems with unknown dynamics, an additional step of dynamics
modeling is required. However, any inaccuracy in dynamics modeling will lead to
sub-optimality in the resulting control function. Another set of approaches for
controlling unknown dynamical systems - reinforcement learning, folds the
dynamics modeling into controller training via value function approximation or
policy gradient through extensively interacting with the environment, but it
suffers from low data efficiency. To address these, we introduce NODEC, a novel
framework for controlling unknown dynamical systems, which combines dynamics
modelling and controller training using a coupled neural ODE model. Through an
intriguing interplay between the two coupled neural networks, NODEC learns
system dynamics as well as optimal controls that guides the unknown dynamical
system towards target states. Our experiments demonstrate the effectiveness and
data efficiency of NODEC for learning optimal control of unknown dynamical
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chi_C/0/1/0/all/0/1&quot;&gt;Cheng Chi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01841">
<title>Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov Decision Processes. (arXiv:2401.01841v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.01841</link>
<description rdf:parseType="Literal">&lt;p&gt;A fundamental (and largely open) challenge in sequential decision-making is
dealing with non-stationary environments, where exogenous environmental
conditions change over time. Such problems are traditionally modeled as
non-stationary Markov decision processes (NSMDP). However, existing approaches
for decision-making in NSMDPs have two major shortcomings: first, they assume
that the updated environmental dynamics at the current time are known (although
future dynamics can change); and second, planning is largely pessimistic, i.e.,
the agent acts ``safely&apos;&apos; to account for the non-stationary evolution of the
environment. We argue that both these assumptions are invalid in practice --
updated environmental conditions are rarely known, and as the agent interacts
with the environment, it can learn about the updated dynamics and avoid being
pessimistic, at least in states whose dynamics it is confident about. We
present a heuristic search algorithm called \textit{Adaptive Monte Carlo Tree
Search (ADA-MCTS)} that addresses these challenges. We show that the agent can
learn the updated dynamics of the environment over time and then act as it
learns, i.e., if the agent is in a region of the state space about which it has
updated knowledge, it can avoid being pessimistic. To quantify ``updated
knowledge,&apos;&apos; we disintegrate the aleatoric and epistemic uncertainty in the
agent&apos;s updated belief and show how the agent can use these estimates for
decision-making. We compare the proposed approach with the multiple
state-of-the-art approaches in decision-making across multiple well-established
open-source problems and empirically show that our approach is faster and
highly adaptive without sacrificing safety.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Baiting Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1&quot;&gt;Abhishek Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukhopadhyay_A/0/1/0/all/0/1&quot;&gt;Ayan Mukhopadhyay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01843">
<title>Investigating Semi-Supervised Learning Algorithms in Text Datasets. (arXiv:2401.01843v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.01843</link>
<description rdf:parseType="Literal">&lt;p&gt;Using large training datasets enhances the generalization capabilities of
neural networks. Semi-supervised learning (SSL) is useful when there are few
labeled data and a lot of unlabeled data. SSL methods that use data
augmentation are most successful for image datasets. In contrast, texts do not
have consistent augmentation methods as images. Consequently, methods that use
augmentation are not as effective in text data as they are in image data. In
this study, we compared SSL algorithms that do not require augmentation; these
are self-training, co-training, tri-training, and tri-training with
disagreement. In the experiments, we used 4 different text datasets for
different tasks. We examined the algorithms from a variety of perspectives by
asking experiment questions and suggested several improvements. Among the
algorithms, tri-training with disagreement showed the closest performance to
the Oracle; however, performance gap shows that new semi-supervised algorithms
or improvements in existing methods are needed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kesgin_H/0/1/0/all/0/1&quot;&gt;Himmet Toprak Kesgin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amasyali_M/0/1/0/all/0/1&quot;&gt;Mehmet Fatih Amasyali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01851">
<title>The Power of Training: How Different Neural Network Setups Influence the Energy Demand. (arXiv:2401.01851v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.01851</link>
<description rdf:parseType="Literal">&lt;p&gt;This work examines the effects of variations in machine learning training
regimes and learning paradigms on the corresponding energy consumption. While
increasing data availability and innovation in high-performance hardware fuels
the training of sophisticated models, it also supports the fading perception of
energy consumption and carbon emission. Therefore, the goal of this work is to
create awareness about the energy impact of general training parameters and
processes, from learning rate over batch size to knowledge transfer. Multiple
setups with different hyperparameter initializations are evaluated on two
different hardware configurations to obtain meaningful results. Experiments on
pretraining and multitask training are conducted on top of the baseline results
to determine their potential towards sustainable machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geissler_D/0/1/0/all/0/1&quot;&gt;Daniel Gei&amp;#xdf;ler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mengxi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suh_S/0/1/0/all/0/1&quot;&gt;Sungho Suh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lukowicz_P/0/1/0/all/0/1&quot;&gt;Paul Lukowicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01854">
<title>Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.01854</link>
<description rdf:parseType="Literal">&lt;p&gt;As instruction-tuned large language models (LLMs) gain global adoption, their
ability to follow instructions in multiple languages becomes increasingly
crucial. One promising approach is cross-lingual transfer, where a model
acquires specific functionality on some language by finetuning on another
language. In this work, we investigate how multilinguality during instruction
tuning of a multilingual LLM affects instruction-following across languages. We
first show that many languages transfer some instruction-following capabilities
to other languages from even monolingual tuning. Furthermore, we find that only
40 multilingual examples in an English tuning set substantially improve
multilingual instruction-following, both in seen and unseen languages during
tuning. In general, we observe that models tuned on multilingual mixtures
exhibit comparable or superior performance in several languages compared to
monolingually tuned models, despite training on 10x fewer examples in those
languages. Finally, we find that increasing the number of languages in the
instruction tuning set from 1 to only 2, 3, or 4 increases cross-lingual
generalization. Our results suggest that building massively multilingual
instruction-tuned models can be done with only a very small set of multilingual
instruction-responses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaham_U/0/1/0/all/0/1&quot;&gt;Uri Shaham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1&quot;&gt;Jonathan Herzig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1&quot;&gt;Roee Aharoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1&quot;&gt;Idan Szpektor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1&quot;&gt;Reut Tsarfaty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eyal_M/0/1/0/all/0/1&quot;&gt;Matan Eyal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01868">
<title>Step length measurement in the wild using FMCW radar. (arXiv:2401.01868v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01868</link>
<description rdf:parseType="Literal">&lt;p&gt;With an aging population, numerous assistive and monitoring technologies are
under development to enable older adults to age in place. To facilitate aging
in place predicting risk factors such as falls, and hospitalization and
providing early interventions are important. Much of the work on ambient
monitoring for risk prediction has centered on gait speed analysis, utilizing
privacy-preserving sensors like radar. Despite compelling evidence that
monitoring step length, in addition to gait speed, is crucial for predicting
risk, radar-based methods have not explored step length measurement in the
home. Furthermore, laboratory experiments on step length measurement using
radars are limited to proof of concept studies with few healthy subjects. To
address this gap, a radar-based step length measurement system for the home is
proposed based on detection and tracking using radar point cloud, followed by
Doppler speed profiling of the torso to obtain step lengths in the home. The
proposed method was evaluated in a clinical environment, involving 35 frail
older adults, to establish its validity. Additionally, the method was assessed
in people&apos;s homes, with 21 frail older adults who had participated in the
clinical assessment. The proposed radar-based step length measurement method
was compared to the gold standard Zeno Walkway Gait Analysis System, revealing
a 4.5cm/8.3% error in a clinical setting. Furthermore, it exhibited excellent
reliability (ICC(2,k)=0.91, 95% CI 0.82 to 0.96) in uncontrolled home settings.
The method also proved accurate in uncontrolled home settings, as indicated by
a strong agreement (ICC(3,k)=0.81 (95% CI 0.53 to 0.92)) between home
measurements and in-clinic assessments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siva_P/0/1/0/all/0/1&quot;&gt;Parthipan Siva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alexander Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hewston_P/0/1/0/all/0/1&quot;&gt;Patricia Hewston&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ioannidis_G/0/1/0/all/0/1&quot;&gt;George Ioannidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adachi_D/0/1/0/all/0/1&quot;&gt;Dr. Jonathan Adachi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabinovich_D/0/1/0/all/0/1&quot;&gt;Dr. Alexander Rabinovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1&quot;&gt;Andrea Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papaioannou_A/0/1/0/all/0/1&quot;&gt;Alexandra Papaioannou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.03958">
<title>TrAISformer -- A Transformer Network with Sparse Augmented Data Representation and Cross Entropy Loss for AIS-based Vessel Trajectory Prediction. (arXiv:2109.03958v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2109.03958</link>
<description rdf:parseType="Literal">&lt;p&gt;Vessel trajectory prediction plays a pivotal role in numerous maritime
applications and services. While the Automatic Identification System (AIS)
offers a rich source of information to address this task, forecasting vessel
trajectory using AIS data remains challenging, even for modern machine learning
techniques, because of the inherent heterogeneous and multimodal nature of
motion data. In this paper, we propose a novel approach to tackle these
challenges. We introduce a discrete, high-dimensional representation of AIS
data and a new loss function designed to explicitly address heterogeneity and
multimodality. The proposed model-referred to as TrAISformer-is a modified
transformer network that extracts long-term temporal patterns in AIS vessel
trajectories in the proposed enriched space to forecast the positions of
vessels several hours ahead. We report experimental results on real, publicly
available AIS data. TrAISformer significantly outperforms state-of-the-art
methods, with an average prediction performance below 10 nautical miles up to
~10 hours.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duong Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fablet_R/0/1/0/all/0/1&quot;&gt;Ronan Fablet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.00086">
<title>Disentangled (Un)Controllable Features. (arXiv:2211.00086v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.00086</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of MDPs with high-dimensional states, downstream tasks are
predominantly applied on a compressed, low-dimensional representation of the
original input space. A variety of learning objectives have therefore been used
to attain useful representations. However, these representations usually lack
interpretability of the different features. We present a novel approach that is
able to disentangle latent features into a controllable and an uncontrollable
partition. We illustrate that the resulting partitioned representations are
easily interpretable on three types of environments and show that, in a
distribution of procedurally generated maze environments, it is feasible to
interpretably employ a planning algorithm in the isolated controllable latent
partition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kooi_J/0/1/0/all/0/1&quot;&gt;Jacob E. Kooi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoogendoorn_M/0/1/0/all/0/1&quot;&gt;Mark Hoogendoorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francois_Lavet_V/0/1/0/all/0/1&quot;&gt;Vincent Fran&amp;#xe7;ois-Lavet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.03932">
<title>Low Variance Off-policy Evaluation with State-based Importance Sampling. (arXiv:2212.03932v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.03932</link>
<description rdf:parseType="Literal">&lt;p&gt;In off-policy reinforcement learning, a behaviour policy performs exploratory
interactions with the environment to obtain state-action-reward samples which
are then used to learn a target policy that optimises the expected return. This
leads to a problem of off-policy evaluation, where one needs to evaluate the
target policy from samples collected by the often unrelated behaviour policy.
Importance sampling is a traditional statistical technique that is often
applied to off-policy evaluation. While importance sampling estimators are
unbiased, their variance increases exponentially with the horizon of the
decision process due to computing the importance weight as a product of action
probability ratios, yielding estimates with low accuracy for domains involving
long-term planning. This paper proposes state-based importance sampling, which
drops the action probability ratios of sub-trajectories with ``negligible
states&apos;&apos; -- roughly speaking, those for which the chosen actions have no impact
on the return estimate -- from the computation of the importance weight.
Theoretical results show this reduces the ordinary importance sampling variance
from $O(\exp(H))$ to $O(\exp(X))$ where $X &amp;lt; H$ is the largest subtrajectory
with non-negligible states. To identify negligible states, two search
algorithms are proposed, one based on covariance testing and one based on
state-action values. We formulate state-based variants of ordinary importance
sampling, weighted importance sampling, per-decision importance sampling,
incremental importance sampling, doubly robust off-policy evaluation, and
stationary density ratio estimation. Experiments in four distinct domains show
that state-based methods consistently yield reduced variance and improved
accuracy compared to their traditional counterparts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bossens_D/0/1/0/all/0/1&quot;&gt;David M. Bossens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomas_P/0/1/0/all/0/1&quot;&gt;Philip S. Thomas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.09806">
<title>Logit-Q Dynamics for Efficient Learning in Stochastic Teams. (arXiv:2302.09806v2 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/2302.09806</link>
<description rdf:parseType="Literal">&lt;p&gt;We present two logit-Q learning dynamics combining the classical and
independent log-linear learning updates with an on-policy value iteration
update for efficient learning in stochastic games. We show that the logit-Q
dynamics presented reach (near) efficient equilibrium in stochastic teams. We
quantify a bound on the approximation error. We also show the rationality of
the logit-Q dynamics against agents following pure stationary strategies and
the convergence of the dynamics in stochastic games where the reward functions
induce potential games, yet only a single agent controls the state transitions
beyond stochastic teams. The key idea is to approximate the dynamics with a
fictional scenario where the Q-function estimates are stationary over
finite-length epochs only for analysis. We then couple the dynamics in the main
and fictional scenarios to show that these two scenarios become more and more
similar across epochs due to the vanishing step size.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sayin_M/0/1/0/all/0/1&quot;&gt;Muhammed O. Sayin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unlu_O/0/1/0/all/0/1&quot;&gt;Onur Unlu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18891">
<title>EmotionGesture: Audio-Driven Diverse Emotional Co-Speech 3D Gesture Generation. (arXiv:2305.18891v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18891</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating vivid and diverse 3D co-speech gestures is crucial for various
applications in animating virtual avatars. While most existing methods can
generate gestures from audio directly, they usually overlook that emotion is
one of the key factors of authentic co-speech gesture generation. In this work,
we propose EmotionGesture, a novel framework for synthesizing vivid and diverse
emotional co-speech 3D gestures from audio. Considering emotion is often
entangled with the rhythmic beat in speech audio, we first develop an
Emotion-Beat Mining module (EBM) to extract the emotion and audio beat features
as well as model their correlation via a transcript-based visual-rhythm
alignment. Then, we propose an initial pose based Spatial-Temporal Prompter
(STP) to generate future gestures from the given initial poses. STP effectively
models the spatial-temporal correlations between the initial poses and the
future gestures, thus producing the spatial-temporal coherent pose prompt. Once
we obtain pose prompts, emotion, and audio beat features, we will generate 3D
co-speech gestures through a transformer architecture. However, considering the
poses of existing datasets often contain jittering effects, this would lead to
generating unstable gestures. To address this issue, we propose an effective
objective function, dubbed Motion-Smooth Loss. Specifically, we model motion
offset to compensate for jittering ground-truth by forcing gestures to be
smooth. Last, we present an emotion-conditioned VAE to sample emotion features,
enabling us to generate diverse emotional results. Extensive experiments
demonstrate that our framework outperforms the state-of-the-art, achieving
vivid and diverse emotional co-speech 3D gestures. Our code and dataset will be
released at the project page:
https://xingqunqi-lab.github.io/Emotion-Gesture-Web/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xingqun Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lincheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Jie Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_H/0/1/0/all/0/1&quot;&gt;Haoran Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07618">
<title>Hyperbolic Graph Diffusion Model. (arXiv:2306.07618v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07618</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion generative models (DMs) have achieved promising results in image
and graph generation. However, real-world graphs, such as social networks,
molecular graphs, and traffic graphs, generally share non-Euclidean topologies
and hidden hierarchies. For example, the degree distributions of graphs are
mostly power-law distributions. The current latent diffusion model embeds the
hierarchical data in a Euclidean space, which leads to distortions and
interferes with modeling the distribution. Instead, hyperbolic space has been
found to be more suitable for capturing complex hierarchical structures due to
its exponential growth property. In order to simultaneously utilize the data
generation capabilities of diffusion models and the ability of hyperbolic
embeddings to extract latent hierarchical distributions, we propose a novel
graph generation method called, Hyperbolic Graph Diffusion Model (HGDM), which
consists of an auto-encoder to encode nodes into successive hyperbolic
embeddings, and a DM that operates in the hyperbolic latent space. HGDM
captures the crucial graph structure distributions by constructing a hyperbolic
potential node space that incorporates edge information. Extensive experiments
show that HGDM achieves better performance in generic graph and molecule
generation benchmarks, with a $48\%$ improvement in the quality of graph
generation with highly hierarchical structures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1&quot;&gt;Lingfeng Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xuan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_M/0/1/0/all/0/1&quot;&gt;Mingjie Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xiangxiang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Daxin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xian Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10675">
<title>LaDe: The First Comprehensive Last-mile Delivery Dataset from Industry. (arXiv:2306.10675v2 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10675</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world last-mile delivery datasets are crucial for research in logistics,
supply chain management, and spatio-temporal data mining. Despite a plethora of
algorithms developed to date, no widely accepted, publicly available last-mile
delivery dataset exists to support research in this field. In this paper, we
introduce \texttt{LaDe}, the first publicly available last-mile delivery
dataset with millions of packages from the industry. LaDe has three unique
characteristics: (1) Large-scale. It involves 10,677k packages of 21k couriers
over 6 months of real-world operation. (2) Comprehensive information. It offers
original package information, such as its location and time requirements, as
well as task-event information, which records when and where the courier is
while events such as task-accept and task-finish events happen. (3) Diversity.
The dataset includes data from various scenarios, including package pick-up and
delivery, and from multiple cities, each with its unique spatio-temporal
patterns due to their distinct characteristics such as populations. We verify
LaDe on three tasks by running several classical baseline models per task. We
believe that the large-scale, comprehensive, diverse feature of LaDe can offer
unparalleled opportunities to researchers in the supply chain community, data
mining community, and beyond. The dataset homepage is publicly available at
https://huggingface.co/datasets/Cainiao-AI/LaDe.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lixia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Haomin Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Haoyuan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1&quot;&gt;Xiaowei Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yutong Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_E/0/1/0/all/0/1&quot;&gt;Ergang Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhen_J/0/1/0/all/0/1&quot;&gt;Jianbin Zhen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1&quot;&gt;Junhong Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Liuqing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1&quot;&gt;Roger Zimmermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Youfang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1&quot;&gt;Huaiyu Wan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16914">
<title>Computationally Assisted Quality Control for Public Health Data Streams. (arXiv:2306.16914v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16914</link>
<description rdf:parseType="Literal">&lt;p&gt;Irregularities in public health data streams (like COVID-19 Cases) hamper
data-driven decision-making for public health stakeholders. A real-time,
computer-generated list of the most important, outlying data points from
thousands of daily-updated public health data streams could assist an expert
reviewer in identifying these irregularities. However, existing outlier
detection frameworks perform poorly on this task because they do not account
for the data volume or for the statistical properties of public health streams.
Accordingly, we developed FlaSH (Flagging Streams in public Health), a
practical outlier detection framework for public health data users that uses
simple, scalable models to capture these statistical properties explicitly. In
an experiment where human experts evaluate FlaSH and existing methods
(including deep learning approaches), FlaSH scales to the data volume of this
task, matches or exceeds these other methods in mean accuracy, and identifies
the outlier points that users empirically rate as more helpful. Based on these
results, FlaSH has been deployed on data streams used by public health
stakeholders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1&quot;&gt;Ananya Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazaitis_K/0/1/0/all/0/1&quot;&gt;Kathryn Mazaitis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenfeld_R/0/1/0/all/0/1&quot;&gt;Roni Rosenfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilder_B/0/1/0/all/0/1&quot;&gt;Bryan Wilder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17408">
<title>LMBot: Distilling Graph Knowledge into Language Model for Graph-less Deployment in Twitter Bot Detection. (arXiv:2306.17408v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17408</link>
<description rdf:parseType="Literal">&lt;p&gt;As malicious actors employ increasingly advanced and widespread bots to
disseminate misinformation and manipulate public opinion, the detection of
Twitter bots has become a crucial task. Though graph-based Twitter bot
detection methods achieve state-of-the-art performance, we find that their
inference depends on the neighbor users multi-hop away from the targets, and
fetching neighbors is time-consuming and may introduce bias. At the same time,
we find that after finetuning on Twitter bot detection, pretrained language
models achieve competitive performance and do not require a graph structure
during deployment. Inspired by this finding, we propose a novel bot detection
framework LMBot that distills the knowledge of graph neural networks (GNNs)
into language models (LMs) for graph-less deployment in Twitter bot detection
to combat the challenge of data dependency. Moreover, LMBot is compatible with
graph-based and graph-less datasets. Specifically, we first represent each user
as a textual sequence and feed them into the LM for domain adaptation. For
graph-based datasets, the output of LMs provides input features for the GNN,
enabling it to optimize for bot detection and distill knowledge back to the LM
in an iterative, mutually enhancing process. Armed with the LM, we can perform
graph-less inference, which resolves the graph data dependency and sampling
bias issues. For datasets without graph structure, we simply replace the GNN
with an MLP, which has also shown strong performance. Our experiments
demonstrate that LMBot achieves state-of-the-art performance on four Twitter
bot detection benchmarks. Extensive studies also show that LMBot is more
robust, versatile, and efficient compared to graph-based Twitter bot detection
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zijian Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zhaoxuan Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zifeng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongrui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qinghua Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1&quot;&gt;Minnan Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05134">
<title>TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation. (arXiv:2307.05134v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05134</link>
<description rdf:parseType="Literal">&lt;p&gt;The progress in the generation of synthetic images has made it crucial to
assess their quality. While several metrics have been proposed to assess the
rendering of images, it is crucial for Text-to-Image (T2I) models, which
generate images based on a prompt, to consider additional aspects such as to
which extent the generated image matches the important content of the prompt.
Moreover, although the generated images usually result from a random starting
point, the influence of this one is generally not considered. In this article,
we propose a new metric based on prompt templates to study the alignment
between the content specified in the prompt and the corresponding generated
images. It allows us to better characterize the alignment in terms of the type
of the specified objects, their number, and their color. We conducted a study
on several recent T2I models about various aspects. An additional interesting
result we obtained with our approach is that image quality can vary drastically
depending on the noise used as a seed for the images. We also quantify the
influence of the number of concepts in the prompt, their order as well as their
(color) attributes. Finally, our method allows us to identify some seeds that
produce better images than others, opening novel directions of research on this
understudied topic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grimal_P/0/1/0/all/0/1&quot;&gt;Paul Grimal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borgne_H/0/1/0/all/0/1&quot;&gt;Herv&amp;#xe9; Le Borgne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferret_O/0/1/0/all/0/1&quot;&gt;Olivier Ferret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tourille_J/0/1/0/all/0/1&quot;&gt;Julien Tourille&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09595">
<title>Minimum Coverage Sets for Training Robust Ad Hoc Teamwork Agents. (arXiv:2308.09595v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09595</link>
<description rdf:parseType="Literal">&lt;p&gt;Robustly cooperating with unseen agents and human partners presents
significant challenges due to the diverse cooperative conventions these
partners may adopt. Existing Ad Hoc Teamwork (AHT) methods address this
challenge by training an agent with a population of diverse teammate policies
obtained through maximizing specific diversity metrics. However, prior
heuristic-based diversity metrics do not always maximize the agent&apos;s robustness
in all cooperative problems. In this work, we first propose that maximizing an
AHT agent&apos;s robustness requires it to emulate policies in the minimum coverage
set (MCS), the set of best-response policies to any partner policies in the
environment. We then introduce the L-BRDiv algorithm that generates a set of
teammate policies that, when used for AHT training, encourage agents to emulate
policies from the MCS. L-BRDiv works by solving a constrained optimization
problem to jointly train teammate policies for AHT training and approximating
AHT agent policies that are members of the MCS. We empirically demonstrate that
L-BRDiv produces more robust AHT agents than state-of-the-art methods in a
broader range of two-player cooperative problems without the need for extensive
hyperparameter tuning for its objectives. Our study shows that L-BRDiv
outperforms the baseline methods by prioritizing discovering distinct members
of the MCS instead of repeatedly finding redundant policies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1&quot;&gt;Arrasy Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jiaxun Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1&quot;&gt;Peter Stone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08163">
<title>Self-Assessment Tests are Unreliable Measures of LLM Personality. (arXiv:2309.08163v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08163</link>
<description rdf:parseType="Literal">&lt;p&gt;As large language models (LLM) evolve in their capabilities, various recent
studies have tried to quantify their behavior using psychological tools created
to study human behavior. One such example is the measurement of &quot;personality&quot;
of LLMs using self-assessment personality tests developed to measure human
personality. Yet almost none of these works verify the applicability of these
tests on LLMs. In this paper, we analyze the reliability of LLM personality
scores obtained from self-assessment personality tests using two simple
experiments. We first introduce the property of prompt sensitivity, where three
semantically equivalent prompts representing three intuitive ways of
administering self-assessment tests on LLMs are used to measure the personality
of the same LLM. We find that all three prompts lead to very different
personality scores, a difference that is statistically significant for all
traits in a large majority of scenarios. We then introduce the property of
option-order symmetry for personality measurement of LLMs. Since most of the
self-assessment tests exist in the form of multiple choice question (MCQ)
questions, we argue that the scores should also be robust to not just the
prompt template but also the order in which the options are presented. This
test unsurprisingly reveals that the self-assessment test scores are not robust
to the order of the options. These simple tests, done on ChatGPT and three
Llama2 models of different sizes, show that self-assessment personality tests
created for humans are unreliable measures of personality in LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Akshat Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anumanchipalli_G/0/1/0/all/0/1&quot;&gt;Gopala Anumanchipalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12113">
<title>Incentivizing Massive Unknown Workers for Budget-Limited Crowdsensing: From Off-Line and On-Line Perspectives. (arXiv:2309.12113v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12113</link>
<description rdf:parseType="Literal">&lt;p&gt;How to incentivize strategic workers using limited budget is a very
fundamental problem for crowdsensing systems; nevertheless, since the sensing
abilities of the workers may not always be known as prior knowledge due to the
diversities of their sensor devices and behaviors, it is difficult to properly
select and pay the unknown workers. Although the uncertainties of the workers
can be addressed by the standard Combinatorial Multi-Armed Bandit (CMAB)
framework in existing proposals through a trade-off between exploration and
exploitation, we may not have sufficient budget to enable the trade-off among
the individual workers, especially when the number of the workers is huge while
the budget is limited. Moreover, the standard CMAB usually assumes the workers
always stay in the system, whereas the workers may join in or depart from the
system over time, such that what we have learnt for an individual worker cannot
be applied after the worker leaves. To address the above challenging issues, in
this paper, we first propose an off-line Context-Aware CMAB-based Incentive
(CACI) mechanism. We innovate in leveraging the exploration-exploitation
trade-off in an elaborately partitioned context space instead of the individual
workers, to effectively incentivize the massive unknown workers with a very
limited budget. We also extend the above basic idea to the on-line setting
where unknown workers may join in or depart from the systems dynamically, and
propose an on-line version of the CACI mechanism. We perform rigorous
theoretical analysis to reveal the upper bounds on the regrets of our CACI
mechanisms and to prove their truthfulness and individual rationality,
respectively. Extensive experiments on both synthetic and real datasets are
also conducted to verify the efficacy of our mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1&quot;&gt;Yuqi Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Huan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1&quot;&gt;Pengfei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1&quot;&gt;Lingjie Duan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12867">
<title>Accurate and Fast Compressed Video Captioning. (arXiv:2309.12867v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12867</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing video captioning approaches typically require to first sample video
frames from a decoded video and then conduct a subsequent process (e.g.,
feature extraction and/or captioning model learning). In this pipeline, manual
frame sampling may ignore key information in videos and thus degrade
performance. Additionally, redundant information in the sampled frames may
result in low efficiency in the inference of video captioning. Addressing this,
we study video captioning from a different perspective in compressed domain,
which brings multi-fold advantages over the existing pipeline: 1) Compared to
raw images from the decoded video, the compressed video, consisting of
I-frames, motion vectors and residuals, is highly distinguishable, which allows
us to leverage the entire video for learning without manual sampling through a
specialized model design; 2) The captioning model is more efficient in
inference as smaller and less redundant information is processed. We propose a
simple yet effective end-to-end transformer in the compressed domain for video
captioning that enables learning from the compressed video for captioning. We
show that even with a simple design, our method can achieve state-of-the-art
performance on different benchmarks while running almost 2x faster than
existing approaches. Code is available at https://github.com/acherstyx/CoCap.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yaojie Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Heng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1&quot;&gt;Longyin Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Libo Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00488">
<title>On Memorization and Privacy Risks of Sharpness Aware Minimization. (arXiv:2310.00488v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00488</link>
<description rdf:parseType="Literal">&lt;p&gt;In many recent works, there is an increased focus on designing algorithms
that seek flatter optima for neural network loss optimization as there is
empirical evidence that it leads to better generalization performance in many
datasets. In this work, we dissect these performance gains through the lens of
data memorization in overparameterized models. We define a new metric that
helps us identify which data points specifically do algorithms seeking flatter
optima do better when compared to vanilla SGD. We find that the generalization
gains achieved by Sharpness Aware Minimization (SAM) are particularly
pronounced for atypical data points, which necessitate memorization. This
insight helps us unearth higher privacy risks associated with SAM, which we
verify through exhaustive empirical evaluations. Finally, we propose mitigation
strategies to achieve a more desirable accuracy vs privacy tradeoff.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Young In Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1&quot;&gt;Pratiksha Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Royset_J/0/1/0/all/0/1&quot;&gt;Johannes O. Royset&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khanna_R/0/1/0/all/0/1&quot;&gt;Rajiv Khanna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04171">
<title>Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection. (arXiv:2310.04171v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04171</link>
<description rdf:parseType="Literal">&lt;p&gt;Fraud detection aims to discover fraudsters deceiving other users by, for
example, leaving fake reviews or making abnormal transactions. Graph-based
fraud detection methods consider this task as a classification problem with two
classes: frauds or normal. We address this problem using Graph Neural Networks
(GNNs) by proposing a dynamic relation-attentive aggregation mechanism. Based
on the observation that many real-world graphs include different types of
relations, we propose to learn a node representation per relation and aggregate
the node representations using a learnable attention function that assigns a
different attention coefficient to each relation. Furthermore, we combine the
node representations from different layers to consider both the local and
global structures of a target node, which is beneficial to improving the
performance of fraud detection on graphs with heterophily. By employing dynamic
graph attention in all the aggregation processes, our method adaptively
computes the attention coefficients for each node. Experimental results show
that our method, DRAG, outperforms state-of-the-art fraud detection methods on
real-world benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Heehyeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jinhyeok Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whang_J/0/1/0/all/0/1&quot;&gt;Joyce Jiyoung Whang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04444">
<title>What&apos;s the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04444</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt engineering is crucial for deploying LLMs but is poorly understood
mathematically. We formalize LLM systems as a class of discrete stochastic
dynamical systems to explore prompt engineering through the lens of control
theory. We investigate the reachable set of output token sequences $R_y(\mathbf
x_0)$ for which there exists a control input sequence $\mathbf u$ for each
$\mathbf y \in R_y(\mathbf x_0)$ that steers the LLM to output $\mathbf y$ from
initial state sequence $\mathbf x_0$. We offer analytic analysis on the
limitations on the controllability of self-attention in terms of reachable set,
where we prove an upper bound on the reachable set of outputs $R_y(\mathbf
x_0)$ as a function of the singular values of the parameter matrices. We
present complementary empirical analysis on the controllability of a panel of
LLMs, including Falcon-7b, Llama-7b, and Falcon-40b. Our results demonstrate a
lower bound on the reachable set of outputs $R_y(\mathbf x_0)$ w.r.t. initial
state sequences $\mathbf x_0$ sampled from the Wikitext dataset. We find that
the correct next Wikitext token following sequence $\mathbf x_0$ is reachable
over 97% of the time with prompts of $k\leq 10$ tokens. We also establish that
the top 75 most likely next tokens, as estimated by the LLM itself, are
reachable at least 85% of the time with prompts of $k\leq 10$ tokens.
Intriguingly, short prompt sequences can dramatically alter the likelihood of
specific outputs, even making the least likely tokens become the most likely
ones. This control-centric analysis of LLMs demonstrates the significant and
poorly understood role of input sequences in steering output probabilities,
offering a foundational perspective for enhancing language model system
capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhargava_A/0/1/0/all/0/1&quot;&gt;Aman Bhargava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witkowski_C/0/1/0/all/0/1&quot;&gt;Cameron Witkowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Manav Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomson_M/0/1/0/all/0/1&quot;&gt;Matt Thomson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05195">
<title>GMMFormer: Gaussian-Mixture-Model Based Transformer for Efficient Partially Relevant Video Retrieval. (arXiv:2310.05195v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05195</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a text query, partially relevant video retrieval (PRVR) seeks to find
untrimmed videos containing pertinent moments in a database. For PRVR, clip
modeling is essential to capture the partial relationship between texts and
videos. Current PRVR methods adopt scanning-based clip construction to achieve
explicit clip modeling, which is information-redundant and requires a large
storage overhead. To solve the efficiency problem of PRVR methods, this paper
proposes GMMFormer, a Gaussian-Mixture-Model based Transformer which models
clip representations implicitly. During frame interactions, we incorporate
Gaussian-Mixture-Model constraints to focus each frame on its adjacent frames
instead of the whole video. Then generated representations will contain
multi-scale clip information, achieving implicit clip modeling. In addition,
PRVR methods ignore semantic differences between text queries relevant to the
same video, leading to a sparse embedding space. We propose a query diverse
loss to distinguish these text queries, making the embedding space more
intensive and contain more semantic information. Extensive experiments on three
large-scale video datasets (i.e., TVR, ActivityNet Captions, and Charades-STA)
demonstrate the superiority and efficiency of GMMFormer. Code is available at
\url{https://github.com/huangmozhi9527/GMMFormer}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuting Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Ziyun Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shu-Tao Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06452">
<title>Understanding the Effects of RLHF on LLM Generalisation and Diversity. (arXiv:2310.06452v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06452</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) fine-tuned with reinforcement learning from
human feedback (RLHF) have been used in some of the most widely deployed AI
models to date, such as OpenAI&apos;s ChatGPT or Anthropic&apos;s Claude. % , or Meta&apos;s
LLaMA-2. While there has been significant work developing these methods, our
understanding of the benefits and downsides of each stage in RLHF is still
limited. To fill this gap, we present an extensive analysis of how each stage
of the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF)
affects two key properties: out-of-distribution (OOD) generalisation and output
diversity. OOD generalisation is crucial given the wide range of real-world
scenarios in which these models are being used, while output diversity refers
to the model&apos;s ability to generate varied outputs and is important for a
variety of use cases. We perform our analysis across two base models on both
summarisation and instruction following tasks, the latter being highly relevant
for current LLM use cases. We find that RLHF generalises better than SFT to new
inputs, particularly as the distribution shift between train and test becomes
larger. However, RLHF significantly reduces output diversity compared to SFT
across a variety of measures, implying a tradeoff in current LLM fine-tuning
methods between generalisation and diversity. Our results provide guidance on
which fine-tuning method should be used depending on the application, and show
that more research is needed to improve the tradeoff between generalisation and
diversity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirk_R/0/1/0/all/0/1&quot;&gt;Robert Kirk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mediratta_I/0/1/0/all/0/1&quot;&gt;Ishita Mediratta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nalmpantis_C/0/1/0/all/0/1&quot;&gt;Christoforos Nalmpantis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luketina_J/0/1/0/all/0/1&quot;&gt;Jelena Luketina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hambro_E/0/1/0/all/0/1&quot;&gt;Eric Hambro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grefenstette_E/0/1/0/all/0/1&quot;&gt;Edward Grefenstette&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1&quot;&gt;Roberta Raileanu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19387">
<title>Othello is Solved. (arXiv:2310.19387v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19387</link>
<description rdf:parseType="Literal">&lt;p&gt;The game of Othello is one of the world&apos;s most complex and popular games that
has yet to be computationally solved. Othello has roughly ten octodecillion (10
to the 58th power) possible game records and ten octillion (10 to the 28th
power) possible game positions. The challenge of solving Othello, determining
the outcome of a game with no mistake made by either player, has long been a
grand challenge in computer science. This paper announces a significant
milestone: Othello is now solved. It is computationally proved that perfect
play by both players lead to a draw. Strong Othello software has long been
built using heuristically designed search techniques. Solving a game provides a
solution that enables the software to play the game perfectly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takizawa_H/0/1/0/all/0/1&quot;&gt;Hiroki Takizawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19923">
<title>Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19923</link>
<description rdf:parseType="Literal">&lt;p&gt;Text embedding models have emerged as powerful tools for transforming
sentences into fixed-sized feature vectors that encapsulate semantic
information. While these models are essential for tasks like information
retrieval, semantic clustering, and text re-ranking, most existing open-source
models, especially those built on architectures like BERT, struggle to
represent lengthy documents and often resort to truncation. One common approach
to mitigate this challenge involves splitting documents into smaller paragraphs
for embedding. However, this strategy results in a much larger set of vectors,
consequently leading to increased memory consumption and computationally
intensive vector searches with elevated latency.
&lt;/p&gt;
&lt;p&gt;To address these challenges, we introduce Jina Embeddings 2, an open-source
text embedding model capable of accommodating up to 8192 tokens. This model is
designed to transcend the conventional 512-token limit and adeptly process long
documents. Jina Embeddings 2 not only achieves state-of-the-art performance on
a range of embedding-related tasks in the MTEB benchmark but also matches the
performance of OpenAI&apos;s proprietary ada-002 model. Additionally, our
experiments indicate that an extended context can enhance performance in tasks
such as NarrativeQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunther_M/0/1/0/all/0/1&quot;&gt;Michael G&amp;#xfc;nther&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_J/0/1/0/all/0/1&quot;&gt;Jackmin Ong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohr_I/0/1/0/all/0/1&quot;&gt;Isabelle Mohr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdessalem_A/0/1/0/all/0/1&quot;&gt;Alaeddine Abdessalem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abel_T/0/1/0/all/0/1&quot;&gt;Tanguy Abel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akram_M/0/1/0/all/0/1&quot;&gt;Mohammad Kalim Akram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guzman_S/0/1/0/all/0/1&quot;&gt;Susana Guzman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mastrapas_G/0/1/0/all/0/1&quot;&gt;Georgios Mastrapas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sturua_S/0/1/0/all/0/1&quot;&gt;Saba Sturua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werk_M/0/1/0/all/0/1&quot;&gt;Maximilian Werk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1&quot;&gt;Han Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11467">
<title>SkateboardAI: The Coolest Video Action Recognition for Skateboarding. (arXiv:2311.11467v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11467</link>
<description rdf:parseType="Literal">&lt;p&gt;Impressed by the coolest skateboarding sports program from 2021 Tokyo Olympic
Games, we are the first to curate the original real-world video datasets
&quot;SkateboardAI&quot; in the wild, even self-design and implement diverse uni-modal
and multi-modal video action recognition approaches to recognize different
tricks accurately. For uni-modal methods, we separately apply (1) CNN and LSTM;
(2) CNN and BiLSTM; (3) CNN and BiLSTM with effective attention mechanisms; (4)
Transformer-based action recognition pipeline. Transferred to the multi-modal
conditions, we investigated the two-stream Inflated-3D architecture on
&quot;SkateboardAI&quot; datasets to compare its performance with uni-modal cases. In
sum, our objective is developing an excellent AI sport referee for the coolest
skateboarding competitions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanxiao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06281">
<title>EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models. (arXiv:2312.06281v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06281</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce EQ-Bench, a novel benchmark designed to evaluate aspects of
emotional intelligence in Large Language Models (LLMs). We assess the ability
of LLMs to understand complex emotions and social interactions by asking them
to predict the intensity of emotional states of characters in a dialogue. The
benchmark is able to discriminate effectively between a wide range of models.
We find that EQ-Bench correlates strongly with comprehensive multi-domain
benchmarks like MMLU (Hendrycks et al., 2020) (r=0.97), indicating that we may
be capturing similar aspects of broad intelligence. Our benchmark produces
highly repeatable results using a set of 60 English-language questions. We also
provide open-source code for an automated benchmarking pipeline at
https://github.com/EQ-bench/EQ-Bench and a leaderboard at https://eqbench.com
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paech_S/0/1/0/all/0/1&quot;&gt;Samuel J. Paech&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10997">
<title>Retrieval-Augmented Generation for Large Language Models: A Survey. (arXiv:2312.10997v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10997</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) demonstrate significant capabilities but face
challenges such as hallucination, outdated knowledge, and non-transparent,
untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has
emerged as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the models,
particularly for knowledge-intensive tasks, and allows for continuous knowledge
updates and integration of domain-specific information. RAG synergistically
merges LLMs&apos; intrinsic knowledge with the vast, dynamic repositories of
external databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing the Naive RAG,
the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the
tripartite foundation of RAG frameworks, which includes the retrieval , the
generation and the augmentation techniques. The paper highlights the
state-of-the-art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG systems.
Furthermore, this paper introduces the metrics and benchmarks for assessing RAG
models, along with the most up-to-date evaluation framework. In conclusion, the
paper delineates prospective avenues for research, including the identification
of challenges, the expansion of multi-modalities, and the progression of the
RAG infrastructure and its ecosystem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yunfan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yun Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1&quot;&gt;Kangxiang Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jinliu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_Y/0/1/0/all/0/1&quot;&gt;Yuxi Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yi Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiawei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qianyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haofen Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13307">
<title>Not All Steps are Equal: Efficient Generation with Progressive Diffusion Models. (arXiv:2312.13307v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13307</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have demonstrated remarkable efficacy in various generative
tasks with the predictive prowess of denoising model. Currently, these models
employ a uniform denoising approach across all timesteps. However, the inherent
variations in noisy latents at each timestep lead to conflicts during training,
constraining the potential of diffusion models. To address this challenge, we
propose a novel two-stage training strategy termed Step-Adaptive Training. In
the initial stage, a base denoising model is trained to encompass all
timesteps. Subsequently, we partition the timesteps into distinct groups,
fine-tuning the model within each group to achieve specialized denoising
capabilities. Recognizing that the difficulties of predicting noise at
different timesteps vary, we introduce a diverse model size requirement. We
dynamically adjust the model size for each timestep by estimating task
difficulty based on its signal-to-noise ratio before fine-tuning. This
adjustment is facilitated by a proxy-based structural importance assessment
mechanism, enabling precise and efficient pruning of the base denoising model.
Our experiments validate the effectiveness of the proposed training strategy,
demonstrating an improvement in the FID score on CIFAR10 by over 0.3 while
utilizing only 80\% of the computational resources. This innovative approach
not only enhances model performance but also significantly reduces
computational costs, opening new avenues for the development and application of
diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1&quot;&gt;Xiu Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1&quot;&gt;Shan You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15097">
<title>Recourse under Model Multiplicity via Argumentative Ensembling (Technical Report). (arXiv:2312.15097v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15097</link>
<description rdf:parseType="Literal">&lt;p&gt;Model Multiplicity (MM) arises when multiple, equally performing machine
learning models can be trained to solve the same prediction task. Recent
studies show that models obtained under MM may produce inconsistent predictions
for the same input. When this occurs, it becomes challenging to provide
counterfactual explanations (CEs), a common means for offering recourse
recommendations to individuals negatively affected by models&apos; predictions. In
this paper, we formalise this problem, which we name recourse-aware ensembling,
and identify several desirable properties which methods for solving it should
satisfy. We show that existing ensembling methods, naturally extended in
different ways to provide CEs, fail to satisfy these properties. We then
introduce argumentative ensembling, deploying computational argumentation to
guarantee robustness of CEs to MM, while also accommodating customisable user
preferences. We show theoretically and experimentally that argumentative
ensembling satisfies properties which the existing methods lack, and that the
trade-offs are minimal wrt accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Junqi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rago_A/0/1/0/all/0/1&quot;&gt;Antonio Rago&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leofante_F/0/1/0/all/0/1&quot;&gt;Francesco Leofante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1&quot;&gt;Francesca Toni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15661">
<title>Unlocking the Potential of Large Language Models for Explainable Recommendations. (arXiv:2312.15661v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15661</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating user-friendly explanations regarding why an item is recommended
has become increasingly common, largely due to advances in language generation
technology, which can enhance user trust and facilitate more informed
decision-making when using online services. However, existing explainable
recommendation systems focus on using small-size language models. It remains
uncertain what impact replacing the explanation generator with the recently
emerging large language models (LLMs) would have. Can we expect unprecedented
results?
&lt;/p&gt;
&lt;p&gt;In this study, we propose LLMXRec, a simple yet effective two-stage
explainable recommendation framework aimed at further boosting the explanation
quality by employing LLMs. Unlike most existing LLM-based recommendation works,
a key characteristic of LLMXRec is its emphasis on the close collaboration
between previous recommender models and LLM-based explanation generators.
Specifically, by adopting several key fine-tuning techniques, including
parameter-efficient instructing tuning and personalized prompt techniques,
controllable and fluent explanations can be well generated to achieve the goal
of explanation recommendation. Most notably, we provide three different
perspectives to evaluate the effectiveness of the explanations. Finally, we
conduct extensive experiments over several benchmark recommender models and
publicly available datasets. The experimental results not only yield positive
results in terms of effectiveness and efficiency but also uncover some
previously unknown outcomes. To facilitate further explorations in this area,
the full code and detailed original results are open-sourced at
https://github.com/GodFire66666/LLM_rec_explanation/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yucong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Mingyue Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Junyu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1&quot;&gt;Enhong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16127">
<title>LLM-SAP: Large Language Model Situational Awareness Based Planning. (arXiv:2312.16127v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16127</link>
<description rdf:parseType="Literal">&lt;p&gt;This work pioneers evaluating emergent planning capabilities based on
situational awareness in large language models. We contribute (i) novel
benchmarks and metrics for standardized assessment; (ii) a unique dataset to
spur progress; and (iii) demonstrations that prompting and multi-agent schemes
significantly enhance planning performance in context-sensitive planning tasks.
Positioning this within a situated agent and automated planning research, we
highlight inherent reliability challenges--efficiently mapping world states to
actions without environmental guidance remains open despite simulated domain
advances. Although out-of-scope, limitations around validation methodology and
data availability indicate exciting directions, including fine-tuning on
expanded planning corpora and optimizations for triggering fast latent
planning. By conclusively demonstrating current methods&apos; promise and
limitations via rigorous comparison, we catalyze investigating reliable
goal-directed reasoning for situated agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liman Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1&quot;&gt;Hanyang Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16476">
<title>SVGDreamer: Text Guided SVG Generation with Diffusion Model. (arXiv:2312.16476v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16476</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, text-guided scalable vector graphics (SVGs) synthesis has shown
promise in domains such as iconography and sketch. However, existing
text-to-SVG generation methods lack editability and struggle with visual
quality and result diversity. To address these limitations, we propose a novel
text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer
incorporates a semantic-driven image vectorization (SIVE) process that enables
the decomposition of synthesis into foreground objects and background, thereby
enhancing editability. Specifically, the SIVE process introduce attention-based
primitive control and an attention-mask loss function for effective control and
manipulation of individual elements. Additionally, we propose a Vectorized
Particle-based Score Distillation (VPSD) approach to tackle the challenges of
color over-saturation, vector primitives over-smoothing, and limited result
diversity in existing text-to-SVG generation methods. Furthermore, on the basis
of VPSD, we introduce Reward Feedback Learning (ReFL) to accelerate VPSD
convergence and improve aesthetic appeal. Extensive experiments have been
conducted to validate the effectiveness of SVGDreamer, demonstrating its
superiority over baseline methods in terms of editability, visual quality, and
diversity. The code and demo of SVGDreamer can be found at
\href{https://ximinng.github.io/SVGDreamer-project/}{https://ximinng.github.io/SVGDreamer-project/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1&quot;&gt;Ximing Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Haitao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chuang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qian Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00110">
<title>Diffusion Model with Perceptual Loss. (arXiv:2401.00110v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00110</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models trained with mean squared error loss tend to generate
unrealistic samples. Current state-of-the-art models rely on classifier-free
guidance to improve sample quality, yet its surprising effectiveness is not
fully understood. In this paper, We show that the effectiveness of
classifier-free guidance partly originates from it being a form of implicit
perceptual guidance. As a result, we can directly incorporate perceptual loss
in diffusion training to improve sample quality. Since the score matching
objective used in diffusion training strongly resembles the denoising
autoencoder objective used in unsupervised training of perceptual networks, the
diffusion model itself is a perceptual network and can be used to generate
meaningful perceptual loss. We propose a novel self-perceptual objective that
results in diffusion models capable of generating more realistic samples. For
conditional generation, our method only improves sample quality without
entanglement with the conditional input and therefore does not sacrifice sample
diversity. Our method can also improve sample quality for unconditional
generation, which was not possible with classifier-free guidance before.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Shanchuan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00430">
<title>Brain-Conditional Multimodal Synthesis: A Survey and Taxonomy. (arXiv:2401.00430v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00430</link>
<description rdf:parseType="Literal">&lt;p&gt;In the era of Artificial Intelligence Generated Content (AIGC), conditional
multimodal synthesis technologies (e.g., text-to-image, text-to-video,
text-to-audio, etc) are gradually reshaping the natural content in the real
world. The key to multimodal synthesis technology is to establish the mapping
relationship between different modalities. Brain signals, serving as potential
reflections of how the brain interprets external information, exhibit a
distinctive One-to-Many correspondence with various external modalities. This
correspondence makes brain signals emerge as a promising guiding condition for
multimodal content synthesis. Brian-conditional multimodal synthesis refers to
decoding brain signals back to perceptual experience, which is crucial for
developing practical brain-computer interface systems and unraveling complex
mechanisms underlying how the brain perceives and comprehends external stimuli.
This survey comprehensively examines the emerging field of AIGC-based
Brain-conditional Multimodal Synthesis, termed AIGC-Brain, to delineate the
current landscape and future directions. To begin, related brain neuroimaging
datasets, functional brain regions, and mainstream generative models are
introduced as the foundation of AIGC-Brain decoding and analysis. Next, we
provide a comprehensive taxonomy for AIGC-Brain decoding models and present
task-specific representative work and detailed implementation strategies to
facilitate comparison and in-depth analysis. Quality assessments are then
introduced for both qualitative and quantitative evaluation. Finally, this
survey explores insights gained, providing current challenges and outlining
prospects of AIGC-Brain. Being the inaugural survey in this domain, this paper
paves the way for the progress of AIGC-Brain research, offering a foundational
overview to guide future work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_W/0/1/0/all/0/1&quot;&gt;Weijian Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1&quot;&gt;Pengfei Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhijun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01078">
<title>Vietnamese Poem Generation &amp; The Prospect Of Cross-Language Poem-To-Poem Translation. (arXiv:2401.01078v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01078</link>
<description rdf:parseType="Literal">&lt;p&gt;Poetry generation has been a challenging task in the field of Natural
Language Processing, as it requires the model to understand the nuances of
language, sentiment, and style. In this paper, we propose using Large Language
Models to generate Vietnamese poems from natural language prompts, thereby
facilitating an intuitive process with enhanced content control. Our most
efficacious model, the GPT-3 Babbage variant, achieves a custom evaluation
score of 0.8, specifically tailored to the &quot;luc bat&quot; genre of Vietnamese
poetry. Furthermore, we also explore the idea of paraphrasing poems into normal
text prompts and yield a relatively high score of 0.718 in the &quot;luc bat&quot; genre.
This experiment presents the potential for cross-Language poem-to-poem
translation with translated poems as the inputs while concurrently maintaining
complete control over the generated content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1&quot;&gt;Triet Minh Huynh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Q/0/1/0/all/0/1&quot;&gt;Quan Le Bao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01262">
<title>Fairness Certification for Natural Language Processing and Large Language Models. (arXiv:2401.01262v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01262</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural Language Processing (NLP) plays an important role in our daily lives,
particularly due to the enormous progress of Large Language Models (LLM).
However, NLP has many fairness-critical use cases, e.g., as an expert system in
recruitment or as an LLM-based tutor in education. Since NLP is based on human
language, potentially harmful biases can diffuse into NLP systems and produce
unfair results, discriminate against minorities or generate legal issues.
Hence, it is important to develop a fairness certification for NLP approaches.
We follow a qualitative research approach towards a fairness certification for
NLP. In particular, we have reviewed a large body of literature on algorithmic
fairness, and we have conducted semi-structured expert interviews with a wide
range of experts from that area. We have systematically devised six fairness
criteria for NLP, which can be further refined into 18 sub-categories. Our
criteria offer a foundation for operationalizing and testing processes to
certify fairness, both from the perspective of the auditor and the audited
organization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freiberger_V/0/1/0/all/0/1&quot;&gt;Vincent Freiberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buchmann_E/0/1/0/all/0/1&quot;&gt;Erik Buchmann&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>