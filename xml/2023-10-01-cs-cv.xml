<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-09-28T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15883" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15889" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15941" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15954" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15977" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15991" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16023" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16053" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16058" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16064" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16127" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16139" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16140" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16148" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16164" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16179" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16205" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16207" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16208" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16210" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16237" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16257" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16264" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16283" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16301" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16309" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16351" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16354" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16372" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16375" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16390" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16414" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16460" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16483" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16499" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16511" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16524" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16536" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16569" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16585" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16592" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16608" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16627" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16643" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16649" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16650" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16653" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16654" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16662" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16668" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16669" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16671" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.09882" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.05856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.06984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.08217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.10754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.09907" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.09976" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03037" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.04116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16296" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15883" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00914" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05671" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02347" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09283" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12499" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08543" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01340" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06612" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09301" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11500" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13302" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13556" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14181" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14564" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15411" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15523" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15564" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2309.15850">
<title>Reflection Invariance Learning for Few-shot Semantic Segmentation. (arXiv:2309.15850v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.15850</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot semantic segmentation (FSS) aims to segment objects of unseen
classes in query images with only a few annotated support images. Existing FSS
algorithms typically focus on mining category representations from the
single-view support to match semantic objects of the single-view query.
However, the limited annotated samples render the single-view matching struggle
to perceive the reflection invariance of novel objects, which results in a
restricted learning space for novel categories and further induces a biased
segmentation with demoted parsing performance. To address this challenge, this
paper proposes a fresh few-shot segmentation framework to mine the reflection
invariance in a multi-view matching manner. Specifically, original and
reflection support features from different perspectives with the same semantics
are learnable fused to obtain the reflection invariance prototype with a
stronger category representation ability. Simultaneously, aiming at providing
better prior guidance, the Reflection Invariance Prior Mask Generation (RIPMG)
module is proposed to integrate prior knowledge from different perspectives.
Finally, segmentation predictions from varying views are complementarily merged
in the Reflection Invariance Semantic Prediction (RISP) module to yield precise
segmentation predictions. Extensive experiments on both PASCAL-$5^\textit{i}$
and COCO-$20^\textit{i}$ datasets demonstrate the effectiveness of our approach
and show that our method could achieve state-of-the-art performance. Code is
available at \url{https://anonymous.4open.science/r/RILFS-A4D1}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1&quot;&gt;Qinglong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuntian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaokang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15883">
<title>Highly Efficient SNNs for High-speed Object Detection. (arXiv:2309.15883v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.15883</link>
<description rdf:parseType="Literal">&lt;p&gt;The high biological properties and low energy consumption of Spiking Neural
Networks (SNNs) have brought much attention in recent years. However, the
converted SNNs generally need large time steps to achieve satisfactory
performance, which will result in high inference latency and computational
resources increase. In this work, we propose a highly efficient and fast SNN
for object detection. First, we build an initial compact ANN by using
quantization training method of convolution layer fold batch normalization
layer and neural network modification. Second, we theoretically analyze how to
obtain the low complexity SNN correctly. Then, we propose a scale-aware
pseudoquantization scheme to guarantee the correctness of the compact ANN to
SNN. Third, we propose a continuous inference scheme by using a Feed-Forward
Integrate-and-Fire (FewdIF) neuron to realize high-speed object detection.
Experimental results show that our efficient SNN can achieve 118X speedup on
GPU with only 1.5MB parameters for object detection tasks. We further verify
our SNN on FPGA platform and the proposed model can achieve 800+FPS object
detection with extremely low latency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_N/0/1/0/all/0/1&quot;&gt;Nemin Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chuang Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15889">
<title>High Perceptual Quality Wireless Image Delivery with Denoising Diffusion Models. (arXiv:2309.15889v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.15889</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the image transmission problem over a noisy wireless channel via
deep learning-based joint source-channel coding (DeepJSCC) along with a
denoising diffusion probabilistic model (DDPM) at the receiver. Specifically,
we are interested in the perception-distortion trade-off in the practical
finite block length regime, in which separate source and channel coding can be
highly suboptimal. We introduce a novel scheme that utilizes the range-null
space decomposition of the target image. We transmit the range-space of the
image after encoding and employ DDPM to progressively refine its null space
contents. Through extensive experiments, we demonstrate significant
improvements in distortion and perceptual quality of reconstructed images
compared to standard DeepJSCC and the state-of-the-art generative
learning-based method. We will publicly share our source code to facilitate
further research and reproducibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yilmaz_S/0/1/0/all/0/1&quot;&gt;Selim F. Yilmaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Niu_X/0/1/0/all/0/1&quot;&gt;Xueyan Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bai_B/0/1/0/all/0/1&quot;&gt;Bo Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_W/0/1/0/all/0/1&quot;&gt;Wei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Deng_L/0/1/0/all/0/1&quot;&gt;Lei Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gunduz_D/0/1/0/all/0/1&quot;&gt;Deniz Gunduz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15915">
<title>Zero-Shot and Few-Shot Video Question Answering with Multi-Modal Prompts. (arXiv:2309.15915v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.15915</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent vision-language models are driven by large-scale pretrained models.
However, adapting pretrained models on limited data presents challenges such as
overfitting, catastrophic forgetting, and the cross-modal gap between vision
and language. We introduce a parameter-efficient method to address these
challenges, combining multimodal prompt learning and a transformer-based
mapping network, while keeping the pretrained models frozen. Our experiments on
several video question answering benchmarks demonstrate the superiority of our
approach in terms of performance and parameter efficiency on both zero-shot and
few-shot settings. Our code is available at https://engindeniz.github.io/vitis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engin_D/0/1/0/all/0/1&quot;&gt;Deniz Engin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1&quot;&gt;Yannis Avrithis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15940">
<title>Context-Aware Entity Grounding with Open-Vocabulary 3D Scene Graphs. (arXiv:2309.15940v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2309.15940</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an Open-Vocabulary 3D Scene Graph (OVSG), a formal framework for
grounding a variety of entities, such as object instances, agents, and regions,
with free-form text-based queries. Unlike conventional semantic-based object
localization approaches, our system facilitates context-aware entity
localization, allowing for queries such as ``pick up a cup on a kitchen table&quot;
or ``navigate to a sofa on which someone is sitting&quot;. In contrast to existing
research on 3D scene graphs, OVSG supports free-form text input and
open-vocabulary querying. Through a series of comparative experiments using the
ScanNet dataset and a self-collected dataset, we demonstrate that our proposed
approach significantly surpasses the performance of previous semantic-based
localization techniques. Moreover, we highlight the practical application of
OVSG in real-world robot navigation and manipulation experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Haonan Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boyalakuntla_K/0/1/0/all/0/1&quot;&gt;Kowndinya Boyalakuntla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Shiyang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1&quot;&gt;Siwei Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_E/0/1/0/all/0/1&quot;&gt;Eric Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keskar_S/0/1/0/all/0/1&quot;&gt;Shreesh Keskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1&quot;&gt;Shijie Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbas_A/0/1/0/all/0/1&quot;&gt;Adeeb Abbas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Lifeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bekris_K/0/1/0/all/0/1&quot;&gt;Kostas Bekris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boularias_A/0/1/0/all/0/1&quot;&gt;Abdeslam Boularias&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15941">
<title>AutoEncoding Tree for City Generation and Applications. (arXiv:2309.15941v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.15941</link>
<description rdf:parseType="Literal">&lt;p&gt;City modeling and generation have attracted an increased interest in various
applications, including gaming, urban planning, and autonomous driving. Unlike
previous works focused on the generation of single objects or indoor scenes,
the huge volumes of spatial data in cities pose a challenge to the generative
models. Furthermore, few publicly available 3D real-world city datasets also
hinder the development of methods for city generation. In this paper, we first
collect over 3,000,000 geo-referenced objects for the city of New York, Zurich,
Tokyo, Berlin, Boston and several other large cities. Based on this dataset, we
propose AETree, a tree-structured auto-encoder neural network, for city
generation. Specifically, we first propose a novel Spatial-Geometric Distance
(SGD) metric to measure the similarity between building layouts and then
construct a binary tree over the raw geometric data of building based on the
SGD metric. Next, we present a tree-structured network whose encoder learns to
extract and merge spatial information from bottom-up iteratively. The resulting
global representation is reversely decoded for reconstruction or generation. To
address the issue of long-dependency as the level of the tree increases, a Long
Short-Term Memory (LSTM) Cell is employed as a basic network element of the
proposed AETree. Moreover, we introduce a novel metric, Overlapping Area Ratio
(OAR), to quantitatively evaluate the generation results. Experiments on the
collected dataset demonstrate the effectiveness of the proposed model on 2D and
3D city generation. Furthermore, the latent features learned by AETree can
serve downstream urban planning applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1&quot;&gt;Wenyu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1&quot;&gt;Congcong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chok_L/0/1/0/all/0/1&quot;&gt;Lazarus Chok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Yan Liang Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1&quot;&gt;Sheung Lung Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1&quot;&gt;Chen Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15954">
<title>The Devil is in the Details: A Deep Dive into the Rabbit Hole of Data Filtering. (arXiv:2309.15954v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.15954</link>
<description rdf:parseType="Literal">&lt;p&gt;The quality of pre-training data plays a critical role in the performance of
foundation models. Popular foundation models often design their own recipe for
data filtering, which makes it hard to analyze and compare different data
filtering approaches. DataComp is a new benchmark dedicated to evaluating
different methods for data filtering. This paper describes our learning and
solution when participating in the DataComp challenge. Our filtering strategy
includes three stages: single-modality filtering, cross-modality filtering, and
data distribution alignment. We integrate existing methods and propose new
solutions, such as computing CLIP score on horizontally flipped images to
mitigate the interference of scene text, using vision and language models to
retrieve training samples for target downstream tasks, rebalancing the data
distribution to improve the efficiency of allocating the computational budget,
etc. We slice and dice our design choices, provide in-depth analysis, and
discuss open questions. Our approach outperforms the best method from the
DataComp paper by over 4% on the average performance of 38 tasks and by over 2%
on ImageNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haichao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sateesh Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Linjie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Heng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15977">
<title>Neural Acoustic Context Field: Rendering Realistic Room Impulse Response With Neural Fields. (arXiv:2309.15977v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2309.15977</link>
<description rdf:parseType="Literal">&lt;p&gt;Room impulse response (RIR), which measures the sound propagation within an
environment, is critical for synthesizing high-fidelity audio for a given
environment. Some prior work has proposed representing RIR as a neural field
function of the sound emitter and receiver positions. However, these methods do
not sufficiently consider the acoustic properties of an audio scene, leading to
unsatisfactory performance. This letter proposes a novel Neural Acoustic
Context Field approach, called NACF, to parameterize an audio scene by
leveraging multiple acoustic contexts, such as geometry, material property, and
spatial information. Driven by the unique properties of RIR, i.e., temporal
un-smoothness and monotonic energy attenuation, we design a temporal
correlation module and multi-scale energy decay criterion. Experimental results
show that NACF outperforms existing field-based methods by a notable margin.
Please visit our project page for more qualitative results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1&quot;&gt;Susan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yapeng Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Anurag Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chenliang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15978">
<title>Assessment of Local Climate Zone Products via Simplified Classification Rule with 3D Building Maps. (arXiv:2309.15978v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.15978</link>
<description rdf:parseType="Literal">&lt;p&gt;This study assesses the performance of a global Local Climate Zone (LCZ)
product. We examined the built-type classes of LCZs in three major metropolitan
areas within the U.S. A reference LCZ was constructed using a simple rule-based
method based on high-resolution 3D building maps. Our evaluation demonstrated
that the global LCZ product struggles to differentiate classes that demand
precise building footprint information (Classes 6 and 9), and classes that
necessitate the identification of subtle differences in building elevation
(Classes 4-6). Additionally, we identified inconsistent tendencies, where the
distribution of classes skews differently across different cities, suggesting
the presence of a data distribution shift problem in the machine learning-based
LCZ classifier. Our findings shed light on the uncertainties in global LCZ
maps, help identify the LCZ classes that are the most challenging to
distinguish, and offer insight into future plans for LCZ development and
validation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Hunsoo Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cervini_G/0/1/0/all/0/1&quot;&gt;Gaia Cervini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1&quot;&gt;Jinha Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15991">
<title>Targeted Image Data Augmentation Increases Basic Skills Captioning Robustness. (arXiv:2309.15991v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.15991</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial neural networks typically struggle in generalizing to
out-of-context examples. One reason for this limitation is caused by having
datasets that incorporate only partial information regarding the potential
correlational structure of the world. In this work, we propose TIDA (Targeted
Image-editing Data Augmentation), a targeted data augmentation method focused
on improving models&apos; human-like abilities (e.g., gender recognition) by filling
the correlational structure gap using a text-to-image generative model. More
specifically, TIDA identifies specific skills in captions describing images
(e.g., the presence of a specific gender in the image), changes the caption
(e.g., &quot;woman&quot; to &quot;man&quot;), and then uses a text-to-image model to edit the image
in order to match the novel caption (e.g., uniquely changing a woman to a man
while maintaining the context identical). Based on the Flickr30K benchmark, we
show that, compared with the original data set, a TIDA-enhanced dataset related
to gender, color, and counting abilities induces better performance in several
image captioning metrics. Furthermore, on top of relying on the classical BLEU
metric, we conduct a fine-grained analysis of the improvements of our models
against the baseline in different ways. We compared text-to-image generative
models and found different behaviors of the image captioning models in terms of
encoding visual encoding and textual decoding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barriere_V/0/1/0/all/0/1&quot;&gt;Valentin Barriere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rio_F/0/1/0/all/0/1&quot;&gt;Felipe del Rio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferari_A/0/1/0/all/0/1&quot;&gt;Andres Carvallo De Ferari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aspillaga_C/0/1/0/all/0/1&quot;&gt;Carlos Aspillaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herrera_Berg_E/0/1/0/all/0/1&quot;&gt;Eugenio Herrera-Berg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calderon_C/0/1/0/all/0/1&quot;&gt;Cristian Buc Calderon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16019">
<title>GasMono: Geometry-Aided Self-Supervised Monocular Depth Estimation for Indoor Scenes. (arXiv:2309.16019v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16019</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper tackles the challenges of self-supervised monocular depth
estimation in indoor scenes caused by large rotation between frames and low
texture. We ease the learning process by obtaining coarse camera poses from
monocular sequences through multi-view geometry to deal with the former.
However, we found that limited by the scale ambiguity across different scenes
in the training dataset, a na\&quot;ive introduction of geometric coarse poses
cannot play a positive role in performance improvement, which is
counter-intuitive. To address this problem, we propose to refine those poses
during training through rotation and translation/scale optimization. To soften
the effect of the low texture, we combine the global reasoning of vision
transformers with an overfitting-aware, iterative self-distillation mechanism,
providing more accurate depth guidance coming from the network itself.
Experiments on NYUv2, ScanNet, 7scenes, and KITTI datasets support the
effectiveness of each component in our framework, which sets a new
state-of-the-art for indoor self-supervised monocular depth estimation, as well
as outstanding generalization ability. Code and models are available at
https://github.com/zxcqlf/GasMono
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chaoqiang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poggi_M/0/1/0/all/0/1&quot;&gt;Matteo Poggi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tosi_F/0/1/0/all/0/1&quot;&gt;Fabio Tosi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Lei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Qiyu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mattoccia_S/0/1/0/all/0/1&quot;&gt;Stefano Mattoccia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16020">
<title>GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization. (arXiv:2309.16020v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16020</link>
<description rdf:parseType="Literal">&lt;p&gt;Worldwide Geo-localization aims to pinpoint the precise location of images
taken anywhere on Earth. This task has considerable challenges due to immense
variation in geographic landscapes. The image-to-image retrieval-based
approaches fail to solve this problem on a global scale as it is not feasible
to construct a large gallery of images covering the entire world. Instead,
existing approaches divide the globe into discrete geographic cells,
transforming the problem into a classification task. However, their performance
is limited by the predefined classes and often results in inaccurate
localizations when an image&apos;s location significantly deviates from its class
center. To overcome these limitations, we propose GeoCLIP, a novel
CLIP-inspired Image-to-GPS retrieval approach that enforces alignment between
the image and its corresponding GPS locations. GeoCLIP&apos;s location encoder
models the Earth as a continuous function by employing positional encoding
through random Fourier features and constructing a hierarchical representation
that captures information at varying resolutions to yield a semantically rich
high-dimensional feature suitable to use even beyond geo-localization. To the
best of our knowledge, this is the first work employing GPS encoding for
geo-localization. We demonstrate the efficacy of our method via extensive
experiments and ablations on benchmark datasets. We achieve competitive
performance with just 20% of training data, highlighting its effectiveness even
in limited-data settings. Furthermore, we qualitatively demonstrate
geo-localization using a text query by leveraging CLIP backbone of our image
encoder.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cepeda_V/0/1/0/all/0/1&quot;&gt;Vicente Vivanco Cepeda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1&quot;&gt;Gaurav Kumar Nayak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mubarak Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16023">
<title>Q-REG: End-to-End Trainable Point Cloud Registration with Surface Curvature. (arXiv:2309.16023v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16023</link>
<description rdf:parseType="Literal">&lt;p&gt;Point cloud registration has seen recent success with several learning-based
methods that focus on correspondence matching and, as such, optimize only for
this objective. Following the learning step of correspondence matching, they
evaluate the estimated rigid transformation with a RANSAC-like framework. While
it is an indispensable component of these methods, it prevents a fully
end-to-end training, leaving the objective to minimize the pose error
nonserved. We present a novel solution, Q-REG, which utilizes rich geometric
information to estimate the rigid pose from a single correspondence. Q-REG
allows to formalize the robust estimation as an exhaustive search, hence
enabling end-to-end training that optimizes over both objectives of
correspondence matching and rigid pose estimation. We demonstrate in the
experiments that Q-REG is agnostic to the correspondence matching method and
provides consistent improvement both when used only in inference and in
end-to-end training. It sets a new state-of-the-art on the 3DMatch, KITTI, and
ModelNet benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1&quot;&gt;Shengze Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barath_D/0/1/0/all/0/1&quot;&gt;Daniel Barath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1&quot;&gt;Marc Pollefeys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armeni_I/0/1/0/all/0/1&quot;&gt;Iro Armeni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16040">
<title>Handbook on Leveraging Lines for Two-View Relative Pose Estimation. (arXiv:2309.16040v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16040</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an approach for estimating the relative pose between calibrated
image pairs by jointly exploiting points, lines, and their coincidences in a
hybrid manner. We investigate all possible configurations where these data
modalities can be used together and review the minimal solvers available in the
literature. Our hybrid framework combines the advantages of all configurations,
enabling robust and accurate estimation in challenging environments. In
addition, we design a method for jointly estimating multiple vanishing point
correspondences in two images, and a bundle adjustment that considers all
relevant data modalities. Experiments on various indoor and outdoor datasets
show that our approach outperforms point-based methods, improving
AUC@10$^\circ$ by 1-7 points while running at comparable speeds. The source
code of the solvers and hybrid framework will be made public.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hruby_P/0/1/0/all/0/1&quot;&gt;Petr Hruby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shaohui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pautrat_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Pautrat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1&quot;&gt;Marc Pollefeys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barath_D/0/1/0/all/0/1&quot;&gt;Daniel Barath&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16053">
<title>Diagnosis of Helicobacter pylori using AutoEncoders for the Detection of Anomalous Staining Patterns in Immunohistochemistry Images. (arXiv:2309.16053v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.16053</link>
<description rdf:parseType="Literal">&lt;p&gt;This work addresses the detection of Helicobacter pylori a bacterium
classified since 1994 as class 1 carcinogen to humans. By its highest
specificity and sensitivity, the preferred diagnosis technique is the analysis
of histological images with immunohistochemical staining, a process in which
certain stained antibodies bind to antigens of the biological element of
interest. This analysis is a time demanding task, which is currently done by an
expert pathologist that visually inspects the digitized samples.
&lt;/p&gt;
&lt;p&gt;We propose to use autoencoders to learn latent patterns of healthy tissue and
detect H. pylori as an anomaly in image staining. Unlike existing
classification approaches, an autoencoder is able to learn patterns in an
unsupervised manner (without the need of image annotations) with high
performance. In particular, our model has an overall 91% of accuracy with 86\%
sensitivity, 96% specificity and 0.97 AUC in the detection of H. pylori.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cano_P/0/1/0/all/0/1&quot;&gt;Pau Cano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Caravaca_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;lvaro Caravaca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gil_D/0/1/0/all/0/1&quot;&gt;Debora Gil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Musulen_E/0/1/0/all/0/1&quot;&gt;Eva Musulen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16058">
<title>AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model. (arXiv:2309.16058v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16058</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Any-Modality Augmented Language Model (AnyMAL), a unified model
that reasons over diverse input modality signals (i.e. text, image, video,
audio, IMU motion sensor), and generates textual responses. AnyMAL inherits the
powerful text-based reasoning abilities of the state-of-the-art LLMs including
LLaMA-2 (70B), and converts modality-specific signals to the joint textual
space through a pre-trained aligner module. To further strengthen the
multimodal LLM&apos;s capabilities, we fine-tune the model with a multimodal
instruction set manually collected to cover diverse topics and tasks beyond
simple QAs. We conduct comprehensive empirical analysis comprising both human
and automatic evaluations, and demonstrate state-of-the-art performance on
various multimodal tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1&quot;&gt;Seungwhan Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1&quot;&gt;Andrea Madotto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhaojiang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagarajan_T/0/1/0/all/0/1&quot;&gt;Tushar Nagarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1&quot;&gt;Matt Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Shashank Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1&quot;&gt;Chun-Fu Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murugesan_P/0/1/0/all/0/1&quot;&gt;Prakash Murugesan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heidari_P/0/1/0/all/0/1&quot;&gt;Peyman Heidari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinet_K/0/1/0/all/0/1&quot;&gt;Kavya Srinet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damavandi_B/0/1/0/all/0/1&quot;&gt;Babak Damavandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Anuj Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16064">
<title>Masked autoencoders are scalable learners of cellular morphology. (arXiv:2309.16064v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16064</link>
<description rdf:parseType="Literal">&lt;p&gt;Inferring biological relationships from cellular phenotypes in high-content
microscopy screens provides significant opportunity and challenge in biological
research. Prior results have shown that deep vision models can capture
biological signal better than hand-crafted features. This work explores how
weakly supervised and self-supervised deep learning approaches scale when
training larger models on larger datasets. Our results show that both CNN- and
ViT-based masked autoencoders significantly outperform weakly supervised
models. At the high-end of our scale, a ViT-L/8 trained on over 3.5-billion
unique crops sampled from 95-million microscopy images achieves relative
improvements as high as 28% over our best weakly supervised models at inferring
known biological relationships curated from public databases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kraus_O/0/1/0/all/0/1&quot;&gt;Oren Kraus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kenyon_Dean_K/0/1/0/all/0/1&quot;&gt;Kian Kenyon-Dean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saberian_S/0/1/0/all/0/1&quot;&gt;Saber Saberian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fallah_M/0/1/0/all/0/1&quot;&gt;Maryam Fallah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McLean_P/0/1/0/all/0/1&quot;&gt;Peter McLean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leung_J/0/1/0/all/0/1&quot;&gt;Jess Leung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1&quot;&gt;Vasudev Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Ayla Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balakrishnan_J/0/1/0/all/0/1&quot;&gt;Jia Balakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celik_S/0/1/0/all/0/1&quot;&gt;Safiye Celik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sypetkowski_M/0/1/0/all/0/1&quot;&gt;Maciej Sypetkowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Chi Vicky Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morse_K/0/1/0/all/0/1&quot;&gt;Kristen Morse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makes_M/0/1/0/all/0/1&quot;&gt;Maureen Makes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mabey_B/0/1/0/all/0/1&quot;&gt;Ben Mabey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Earnshaw_B/0/1/0/all/0/1&quot;&gt;Berton Earnshaw&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16108">
<title>Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words. (arXiv:2309.16108v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16108</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformer (ViT) has emerged as a powerful architecture in the realm
of modern computer vision. However, its application in certain imaging fields,
such as microscopy and satellite imaging, presents unique challenges. In these
domains, images often contain multiple channels, each carrying semantically
distinct and independent information. Furthermore, the model must demonstrate
robustness to sparsity in input channels, as they may not be densely available
during training or testing. In this paper, we propose a modification to the ViT
architecture that enhances reasoning across the input channels and introduce
Hierarchical Channel Sampling (HCS) as an additional regularization technique
to ensure robustness when only partial channels are presented during test time.
Our proposed model, ChannelViT, constructs patch tokens independently from each
input channel and utilizes a learnable channel embedding that is added to the
patch tokens, similar to positional embeddings. We evaluate the performance of
ChannelViT on ImageNet, JUMP-CP (microscopy cell imaging), and So2Sat
(satellite imaging). Our results show that ChannelViT outperforms ViT on
classification tasks and generalizes well, even when a subset of input channels
is used during testing. Across our experiments, HCS proves to be a powerful
regularizer, independent of the architecture employed, suggesting itself as a
straightforward technique for robust ViT training. Lastly, we find that
ChannelViT generalizes effectively even when there is limited access to all
channels during training, highlighting its potential for multi-channel imaging
under real-world conditions with sparse sensors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1&quot;&gt;Yujia Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sivanandan_S/0/1/0/all/0/1&quot;&gt;Srinivasan Sivanandan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karaletsos_T/0/1/0/all/0/1&quot;&gt;Theofanis Karaletsos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16110">
<title>Learning Effective NeRFs and SDFs Representations with 3D Generative Adversarial Networks for 3D Object Generation: Technical Report for ICCV 2023 OmniObject3D Challenge. (arXiv:2309.16110v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16110</link>
<description rdf:parseType="Literal">&lt;p&gt;In this technical report, we present a solution for 3D object generation of
ICCV 2023 OmniObject3D Challenge. In recent years, 3D object generation has
made great process and achieved promising results, but it remains a challenging
task due to the difficulty of generating complex, textured and high-fidelity
results. To resolve this problem, we study learning effective NeRFs and SDFs
representations with 3D Generative Adversarial Networks (GANs) for 3D object
generation. Specifically, inspired by recent works, we use the efficient
geometry-aware 3D GANs as the backbone incorporating with label embedding and
color mapping, which enables to train the model on different taxonomies
simultaneously. Then, through a decoder, we aggregate the resulting features to
generate Neural Radiance Fields (NeRFs) based representations for rendering
high-fidelity synthetic images. Meanwhile, we optimize Signed Distance
Functions (SDFs) to effectively represent objects with 3D meshes. Besides, we
observe that this model can be effectively trained with only a few images of
each object from a variety of classes, instead of using a great number of
images per object or training one model per class. With this pipeline, we can
optimize an effective model for 3D object generation. This solution is one of
the final top-3-place solutions in the ICCV 2023 OmniObject3D Challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zheyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yibo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1&quot;&gt;Guile Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1&quot;&gt;Tongtong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yuan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bingbing Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16118">
<title>D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation. (arXiv:2309.16118v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2309.16118</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene representation has been a crucial design choice in robotic manipulation
systems. An ideal representation should be 3D, dynamic, and semantic to meet
the demands of diverse manipulation tasks. However, previous works often lack
all three properties simultaneously. In this work, we introduce D$^3$Fields -
dynamic 3D descriptor fields. These fields capture the dynamics of the
underlying 3D environment and encode both semantic features and instance masks.
Specifically, we project arbitrary 3D points in the workspace onto multi-view
2D visual observations and interpolate features derived from foundational
models. The resulting fused descriptor fields allow for flexible goal
specifications using 2D images with varied contexts, styles, and instances. To
evaluate the effectiveness of these descriptor fields, we apply our
representation to a wide range of robotic manipulation tasks in a zero-shot
manner. Through extensive evaluation in both real-world scenarios and
simulations, we demonstrate that D$^3$Fields are both generalizable and
effective for zero-shot robotic manipulation tasks. In quantitative comparisons
with state-of-the-art dense descriptors, such as Dense Object Nets and DINO,
D$^3$Fields exhibit significantly better generalization abilities and
manipulation accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yixuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingtong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1&quot;&gt;Katherine Driggs-Campbell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1&quot;&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunzhu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16126">
<title>UVL: A Unified Framework for Video Tampering Localization. (arXiv:2309.16126v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16126</link>
<description rdf:parseType="Literal">&lt;p&gt;With the development of deep learning technology, various forgery methods
emerge endlessly. Meanwhile, methods to detect these fake videos have also
achieved excellent performance on some datasets. However, these methods suffer
from poor generalization to unknown videos and are inefficient for new forgery
methods. To address this challenging problem, we propose UVL, a novel unified
video tampering localization framework for synthesizing forgeries.
Specifically, UVL extracts common features of synthetic forgeries: boundary
artifacts of synthetic edges, unnatural distribution of generated pixels, and
noncorrelation between the forgery region and the original. These features are
widely present in different types of synthetic forgeries and help improve
generalization for detecting unknown videos. Extensive experiments on three
types of synthetic forgery: video inpainting, video splicing and DeepFake show
that the proposed UVL achieves state-of-the-art performance on various
benchmarks and outperforms existing methods by a large margin on cross-dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_P/0/1/0/all/0/1&quot;&gt;Pengfei Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xianfeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinchuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yun Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16127">
<title>Open Compound Domain Adaptation with Object Style Compensation for Semantic Segmentation. (arXiv:2309.16127v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16127</link>
<description rdf:parseType="Literal">&lt;p&gt;Many methods of semantic image segmentation have borrowed the success of open
compound domain adaptation. They minimize the style gap between the images of
source and target domains, more easily predicting the accurate pseudo
annotations for target domain&apos;s images that train segmentation network. The
existing methods globally adapt the scene style of the images, whereas the
object styles of different categories or instances are adapted improperly. This
paper proposes the Object Style Compensation, where we construct the
Object-Level Discrepancy Memory with multiple sets of discrepancy features. The
discrepancy features in a set capture the style changes of the same category&apos;s
object instances adapted from target to source domains. We learn the
discrepancy features from the images of source and target domains, storing the
discrepancy features in memory. With this memory, we select appropriate
discrepancy features for compensating the style information of the object
instances of various categories, adapting the object styles to a unified style
of source domain. Our method enables a more accurate computation of the pseudo
annotations for target domain&apos;s images, thus yielding state-of-the-art results
on different datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1&quot;&gt;Tingliang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Hao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xueyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1&quot;&gt;Wei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_L/0/1/0/all/0/1&quot;&gt;Liang Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yanlin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Di Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16128">
<title>Joint Correcting and Refinement for Balanced Low-Light Image Enhancement. (arXiv:2309.16128v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16128</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-light image enhancement tasks demand an appropriate balance among
brightness, color, and illumination. While existing methods often focus on one
aspect of the image without considering how to pay attention to this balance,
which will cause problems of color distortion and overexposure etc. This
seriously affects both human visual perception and the performance of
high-level visual models. In this work, a novel synergistic structure is
proposed which can balance brightness, color, and illumination more
effectively. Specifically, the proposed method, so-called Joint Correcting and
Refinement Network (JCRNet), which mainly consists of three stages to balance
brightness, color, and illumination of enhancement. Stage 1: we utilize a basic
encoder-decoder and local supervision mechanism to extract local information
and more comprehensive details for enhancement. Stage 2: cross-stage feature
transmission and spatial feature transformation further facilitate color
correction and feature refinement. Stage 3: we employ a dynamic illumination
adjustment approach to embed residuals between predicted and ground truth
images into the model, adaptively adjusting illumination balance. Extensive
experiments demonstrate that the proposed method exhibits comprehensive
performance advantages over 21 state-of-the-art methods on 9 benchmark
datasets. Furthermore, a more persuasive experiment has been conducted to
validate our approach the effectiveness in downstream visual tasks (e.g.,
saliency detection). Compared to several enhancement models, the proposed
method effectively improves the segmentation results and quantitative metrics
of saliency detection. The source code will be available at
https://github.com/woshiyll/JCRNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1&quot;&gt;Nana Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Hong Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yahong Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16133">
<title>MASK4D: Mask Transformer for 4D Panoptic Segmentation. (arXiv:2309.16133v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16133</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately perceiving and tracking instances over time is essential for the
decision-making processes of autonomous agents interacting safely in dynamic
environments. With this intention, we propose Mask4D for the challenging task
of 4D panoptic segmentation of LiDAR point clouds. Mask4D is the first
transformer-based approach unifying semantic instance segmentation and tracking
of sparse and irregular sequences of 3D point clouds into a single joint model.
Our model directly predicts semantic instances and their temporal associations
without relying on any hand-crafted non-learned association strategies such as
probabilistic clustering or voting-based center prediction. Instead, Mask4D
introduces spatio-temporal instance queries which encode the semantic and
geometric properties of each semantic tracklet in the sequence. In an in-depth
study, we find that it is critical to promote spatially compact instance
predictions as spatio-temporal instance queries tend to merge multiple
semantically similar instances, even if they are spatially distant. To this
end, we regress 6-DOF bounding box parameters from spatio-temporal instance
queries, which is used as an auxiliary task to foster spatially compact
predictions. Mask4D achieves a new state-of-the-art on the SemanticKITTI test
set with a score of 68.4 LSTQ, improving upon published top-performing methods
by at least +4.5%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yilmaz_K/0/1/0/all/0/1&quot;&gt;Kadir Yilmaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schult_J/0/1/0/all/0/1&quot;&gt;Jonas Schult&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nekrasov_A/0/1/0/all/0/1&quot;&gt;Alexey Nekrasov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leibe_B/0/1/0/all/0/1&quot;&gt;Bastian Leibe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16135">
<title>A dual-branch model with inter- and intra-branch contrastive loss for long-tailed recognition. (arXiv:2309.16135v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16135</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world data often exhibits a long-tailed distribution, in which head
classes occupy most of the data, while tail classes only have very few samples.
Models trained on long-tailed datasets have poor adaptability to tail classes
and the decision boundaries are ambiguous. Therefore, in this paper, we propose
a simple yet effective model, named Dual-Branch Long-Tailed Recognition
(DB-LTR), which includes an imbalanced learning branch and a Contrastive
Learning Branch (CoLB). The imbalanced learning branch, which consists of a
shared backbone and a linear classifier, leverages common imbalanced learning
approaches to tackle the data imbalance issue. In CoLB, we learn a prototype
for each tail class, and calculate an inter-branch contrastive loss, an
intra-branch contrastive loss and a metric loss. CoLB can improve the
capability of the model in adapting to tail classes and assist the imbalanced
learning branch to learn a well-represented feature space and discriminative
decision boundary. Extensive experiments on three long-tailed benchmark
datasets, i.e., CIFAR100-LT, ImageNet-LT and Places-LT, show that our DB-LTR is
competitive and superior to the comparative methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qiong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tianlin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1&quot;&gt;Geren Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1&quot;&gt;Enlu Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16137">
<title>Context-I2W: Mapping Images to Context-dependent Words for Accurate Zero-Shot Composed Image Retrieval. (arXiv:2309.16137v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16137</link>
<description rdf:parseType="Literal">&lt;p&gt;Different from Composed Image Retrieval task that requires expensive labels
for training task-specific models, Zero-Shot Composed Image Retrieval (ZS-CIR)
involves diverse tasks with a broad range of visual content manipulation intent
that could be related to domain, scene, object, and attribute. The key
challenge for ZS-CIR tasks is to learn a more accurate image representation
that has adaptive attention to the reference image for various manipulation
descriptions. In this paper, we propose a novel context-dependent mapping
network, named Context-I2W, for adaptively converting description-relevant
Image information into a pseudo-word token composed of the description for
accurate ZS-CIR. Specifically, an Intent View Selector first dynamically learns
a rotation rule to map the identical image to a task-specific manipulation
view. Then a Visual Target Extractor further captures local information
covering the main targets in ZS-CIR tasks under the guidance of multiple
learnable queries. The two complementary modules work together to map an image
to a context-dependent pseudo-word token without extra supervision. Our model
shows strong generalization ability on four ZS-CIR tasks, including domain
conversion, object composition, object manipulation, and attribute
manipulation. It obtains consistent and significant performance boosts ranging
from 1.88% to 3.60% over the best methods and achieves new state-of-the-art
results on ZS-CIR. Our code is available at
https://github.com/Pter61/context_i2w.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yuanmin Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1&quot;&gt;Keke Gai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiamin_Z/0/1/0/all/0/1&quot;&gt;Zhuang Jiamin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_G/0/1/0/all/0/1&quot;&gt;Gang Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yue Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qi Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16139">
<title>Two-Step Active Learning for Instance Segmentation with Uncertainty and Diversity Sampling. (arXiv:2309.16139v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16139</link>
<description rdf:parseType="Literal">&lt;p&gt;Training high-quality instance segmentation models requires an abundance of
labeled images with instance masks and classifications, which is often
expensive to procure. Active learning addresses this challenge by striving for
optimum performance with minimal labeling cost by selecting the most
informative and representative images for labeling. Despite its potential,
active learning has been less explored in instance segmentation compared to
other tasks like image classification, which require less labeling. In this
study, we propose a post-hoc active learning algorithm that integrates
uncertainty-based sampling with diversity-based sampling. Our proposed
algorithm is not only simple and easy to implement, but it also delivers
superior performance on various datasets. Its practical application is
demonstrated on a real-world overhead imagery dataset, where it increases the
labeling efficiency fivefold.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Ke Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albro_S/0/1/0/all/0/1&quot;&gt;Stephen Albro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeSalvo_G/0/1/0/all/0/1&quot;&gt;Giulia DeSalvo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kothawade_S/0/1/0/all/0/1&quot;&gt;Suraj Kothawade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashwan_A/0/1/0/all/0/1&quot;&gt;Abdullah Rashwan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tavakkol_S/0/1/0/all/0/1&quot;&gt;Sasan Tavakkol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1&quot;&gt;Kayhan Batmanghelich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xiaoqi Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16140">
<title>CLIP-Hand3D: Exploiting 3D Hand Pose Estimation via Context-Aware Prompting. (arXiv:2309.16140v1 [cs.MM])</title>
<link>http://arxiv.org/abs/2309.16140</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive Language-Image Pre-training (CLIP) starts to emerge in many
computer vision tasks and has achieved promising performance. However, it
remains underexplored whether CLIP can be generalized to 3D hand pose
estimation, as bridging text prompts with pose-aware features presents
significant challenges due to the discrete nature of joint positions in 3D
space. In this paper, we make one of the first attempts to propose a novel 3D
hand pose estimator from monocular images, dubbed as CLIP-Hand3D, which
successfully bridges the gap between text prompts and irregular detailed pose
distribution. In particular, the distribution order of hand joints in various
3D space directions is derived from pose labels, forming corresponding text
prompts that are subsequently encoded into text representations.
Simultaneously, 21 hand joints in the 3D space are retrieved, and their spatial
distribution (in x, y, and z axes) is encoded to form pose-aware features.
Subsequently, we maximize semantic consistency for a pair of pose-text features
following a CLIP-based contrastive learning paradigm. Furthermore, a
coarse-to-fine mesh regressor is designed, which is capable of effectively
querying joint-aware cues from the feature pyramid. Extensive experiments on
several public hand benchmarks show that the proposed model attains a
significantly faster inference speed while achieving state-of-the-art
performance compared to methods utilizing the similar scale backbone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Shaoxiang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1&quot;&gt;Qing Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1&quot;&gt;Lin Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Junyu Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16141">
<title>Align before Search: Aligning Ads Image to Text for Accurate Cross-Modal Sponsored Search. (arXiv:2309.16141v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16141</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-Modal sponsored search displays multi-modal advertisements (ads) when
consumers look for desired products by natural language queries in search
engines. Since multi-modal ads bring complementary details for query-ads
matching, the ability to align ads-specific information in both images and
texts is crucial for accurate and flexible sponsored search. Conventional
research mainly studies from the view of modeling the implicit correlations
between images and texts for query-ads matching, ignoring the alignment of
detailed product information and resulting in suboptimal search performance.In
this work, we propose a simple alignment network for explicitly mapping
fine-grained visual parts in ads images to the corresponding text, which
leverages the co-occurrence structure consistency between vision and language
spaces without requiring expensive labeled training data. Moreover, we propose
a novel model for cross-modal sponsored search that effectively conducts the
cross-modal alignment and query-ads matching in two separate processes. In this
way, the model matches the multi-modal input in the same language space,
resulting in a superior performance with merely half of the training data. Our
model outperforms the state-of-the-art models by 2.57% on a large commercial
dataset. Besides sponsored search, our alignment method is applicable for
general cross-modal search. We study a typical cross-modal retrieval task on
the MSCOCO dataset, which achieves consistent performance improvement and
proves the generalization ability of our method. Our code is available at
https://github.com/Pter61/AlignCMSS/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yuanmin Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1&quot;&gt;Keke Gai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yujing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yue Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_G/0/1/0/all/0/1&quot;&gt;Gang Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qi Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16143">
<title>Generative Semi-supervised Learning with Meta-Optimized Synthetic Samples. (arXiv:2309.16143v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16143</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised learning (SSL) is a promising approach for training deep
classification models using labeled and unlabeled datasets. However, existing
SSL methods rely on a large unlabeled dataset, which may not always be
available in many real-world applications due to legal constraints (e.g.,
GDPR). In this paper, we investigate the research question: Can we train SSL
models without real unlabeled datasets? Instead of using real unlabeled
datasets, we propose an SSL method using synthetic datasets generated from
generative foundation models trained on datasets containing millions of samples
in diverse domains (e.g., ImageNet). Our main concepts are identifying
synthetic samples that emulate unlabeled samples from generative foundation
models and training classifiers using these synthetic samples. To achieve this,
our method is formulated as an alternating optimization problem: (i)
meta-learning of generative foundation models and (ii) SSL of classifiers using
real labeled and synthetic unlabeled samples. For (i), we propose a
meta-learning objective that optimizes latent variables to generate samples
that resemble real labeled samples and minimize the validation loss. For (ii),
we propose a simple unsupervised loss function that regularizes the feature
extractors of classifiers to maximize the performance improvement obtained from
synthetic samples. We confirm that our method outperforms baselines using
generative foundation models on SSL. We also demonstrate that our methods
outperform SSL using real unlabeled datasets in scenarios with extremely small
amounts of labeled datasets. This suggests that synthetic samples have the
potential to provide improvement gains more efficiently than real unlabeled
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1&quot;&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16148">
<title>OSM-Net: One-to-Many One-shot Talking Head Generation with Spontaneous Head Motions. (arXiv:2309.16148v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16148</link>
<description rdf:parseType="Literal">&lt;p&gt;One-shot talking head generation has no explicit head movement reference,
thus it is difficult to generate talking heads with head motions. Some existing
works only edit the mouth area and generate still talking heads, leading to
unreal talking head performance. Other works construct one-to-one mapping
between audio signal and head motion sequences, introducing ambiguity
correspondences into the mapping since people can behave differently in head
motions when speaking the same content. This unreasonable mapping form fails to
model the diversity and produces either nearly static or even exaggerated head
motions, which are unnatural and strange. Therefore, the one-shot talking head
generation task is actually a one-to-many ill-posed problem and people present
diverse head motions when speaking. Based on the above observation, we propose
OSM-Net, a \textit{one-to-many} one-shot talking head generation network with
natural head motions. OSM-Net constructs a motion space that contains rich and
various clip-level head motion features. Each basis of the space represents a
feature of meaningful head motion in a clip rather than just a frame, thus
providing more coherent and natural motion changes in talking heads. The
driving audio is mapped into the motion space, around which various motion
features can be sampled within a reasonable range to achieve the one-to-many
mapping. Besides, the landmark constraint and time window feature input improve
the accurate expression feature extraction and video generation. Extensive
experiments show that OSM-Net generates more natural realistic head motions
under reasonable one-to-many mapping paradigm compared with other methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1&quot;&gt;Yesheng Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Cai Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jiao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jizhong Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16164">
<title>Learning to Terminate in Object Navigation. (arXiv:2309.16164v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2309.16164</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper tackles the critical challenge of object navigation in autonomous
navigation systems, particularly focusing on the problem of target approach and
episode termination in environments with long optimal episode length in Deep
Reinforcement Learning (DRL) based methods. While effective in environment
exploration and object localization, conventional DRL methods often struggle
with optimal path planning and termination recognition due to a lack of depth
information. To overcome these limitations, we propose a novel approach, namely
the Depth-Inference Termination Agent (DITA), which incorporates a supervised
model called the Judge Model to implicitly infer object-wise depth and decide
termination jointly with reinforcement learning. We train our judge model along
with reinforcement learning in parallel and supervise the former efficiently by
reward signal. Our evaluation shows the method is demonstrating superior
performance, we achieve a 9.3% gain on success rate than our baseline method
across all room types and gain 51.2% improvements on long episodes environment
while maintaining slightly better Success Weighted by Path Length (SPL). Code
and resources, visualization are available at:
https://github.com/HuskyKingdom/DITA_acml2023
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yuhang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chun-Yi Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16179">
<title>BEVHeight++: Toward Robust Visual Centric 3D Object Detection. (arXiv:2309.16179v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16179</link>
<description rdf:parseType="Literal">&lt;p&gt;While most recent autonomous driving system focuses on developing perception
methods on ego-vehicle sensors, people tend to overlook an alternative approach
to leverage intelligent roadside cameras to extend the perception ability
beyond the visual range. We discover that the state-of-the-art vision-centric
bird&apos;s eye view detection methods have inferior performances on roadside
cameras. This is because these methods mainly focus on recovering the depth
regarding the camera center, where the depth difference between the car and the
ground quickly shrinks while the distance increases. In this paper, we propose
a simple yet effective approach, dubbed BEVHeight++, to address this issue. In
essence, we regress the height to the ground to achieve a distance-agnostic
formulation to ease the optimization process of camera-only perception methods.
By incorporating both height and depth encoding techniques, we achieve a more
accurate and robust projection from 2D to BEV spaces. On popular 3D detection
benchmarks of roadside cameras, our method surpasses all previous
vision-centric methods by a significant margin. In terms of the ego-vehicle
scenario, our BEVHeight++ possesses superior over depth-only methods.
Specifically, it yields a notable improvement of +1.9% NDS and +1.1% mAP over
BEVDepth when evaluated on the nuScenes validation set. Moreover, on the
nuScenes test set, our method achieves substantial advancements, with an
increase of +2.8% NDS and +1.7% mAP, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1&quot;&gt;Tao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Peng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1&quot;&gt;Kun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Li Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Kaicheng Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16189">
<title>Cloth2Body: Generating 3D Human Body Mesh from 2D Clothing. (arXiv:2309.16189v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16189</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we define and study a new Cloth2Body problem which has a goal
of generating 3D human body meshes from a 2D clothing image. Unlike the
existing human mesh recovery problem, Cloth2Body needs to address new and
emerging challenges raised by the partial observation of the input and the high
diversity of the output. Indeed, there are three specific challenges. First,
how to locate and pose human bodies into the clothes. Second, how to
effectively estimate body shapes out of various clothing types. Finally, how to
generate diverse and plausible results from a 2D clothing image. To this end,
we propose an end-to-end framework that can accurately estimate 3D body mesh
parameterized by pose and shape from a 2D clothing image. Along this line, we
first utilize Kinematics-aware Pose Estimation to estimate body pose
parameters. 3D skeleton is employed as a proxy followed by an inverse
kinematics module to boost the estimation accuracy. We additionally design an
adaptive depth trick to align the re-projected 3D mesh better with 2D clothing
image by disentangling the effects of object size and camera extrinsic. Next,
we propose Physics-informed Shape Estimation to estimate body shape parameters.
3D shape parameters are predicted based on partial body measurements estimated
from RGB image, which not only improves pixel-wise human-cloth alignment, but
also enables flexible user editing. Finally, we design Evolution-based pose
generation method, a skeleton transplanting method inspired by genetic
algorithms to generate diverse reasonable poses during inference. As shown by
experimental results on both synthetic and real-world data, the proposed
framework achieves state-of-the-art performance and can effectively recover
natural and diverse 3D body meshes from 2D images that align well with
clothing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1&quot;&gt;Lu Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Liqian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1&quot;&gt;Shenhan Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16205">
<title>DiffGAN-F2S: Symmetric and Efficient Denoising Diffusion GANs for Structural Connectivity Prediction from Brain fMRI. (arXiv:2309.16205v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16205</link>
<description rdf:parseType="Literal">&lt;p&gt;Mapping from functional connectivity (FC) to structural connectivity (SC) can
facilitate multimodal brain network fusion and discover potential biomarkers
for clinical implications. However, it is challenging to directly bridge the
reliable non-linear mapping relations between SC and functional magnetic
resonance imaging (fMRI). In this paper, a novel diffusision generative
adversarial network-based fMRI-to-SC (DiffGAN-F2S) model is proposed to predict
SC from brain fMRI in an end-to-end manner. To be specific, the proposed
DiffGAN-F2S leverages denoising diffusion probabilistic models (DDPMs) and
adversarial learning to efficiently generate high-fidelity SC through a few
steps from fMRI. By designing the dual-channel multi-head spatial attention
(DMSA) and graph convolutional modules, the symmetric graph generator first
captures global relations among direct and indirect connected brain regions,
then models the local brain region interactions. It can uncover the complex
mapping relations between fMRI and structural connectivity. Furthermore, the
spatially connected consistency loss is devised to constrain the generator to
preserve global-local topological information for accurate intrinsic SC
prediction. Testing on the public Alzheimer&apos;s Disease Neuroimaging Initiative
(ADNI) dataset, the proposed model can effectively generate empirical
SC-preserved connectivity from four-dimensional imaging data and shows superior
performance in SC prediction compared with other related models. Furthermore,
the proposed model can identify the vast majority of important brain regions
and connections derived from the empirical method, providing an alternative way
to fuse multimodal brain networks and analyze clinical disease.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_Q/0/1/0/all/0/1&quot;&gt;Qiankun Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruiheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Di_Y/0/1/0/all/0/1&quot;&gt;Yi Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1&quot;&gt;Hao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_C/0/1/0/all/0/1&quot;&gt;Changhong Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xuhang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuqiang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16206">
<title>Cross-Modal Transformer GAN: Brain Structural-Functional Deep Fusing Network for Alzheimer&apos;s Disease Analysis. (arXiv:2309.16206v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.16206</link>
<description rdf:parseType="Literal">&lt;p&gt;Fusing structural-functional images of the brain has shown great potential to
analyze the deterioration of Alzheimer&apos;s disease (AD). However, it is a big
challenge to effectively fuse the correlated and complementary information from
multimodal neuroimages. In this paper, a novel model termed cross-modal
transformer generative adversarial network (CT-GAN) is proposed to effectively
fuse the functional and structural information contained in functional magnetic
resonance imaging (fMRI) and diffusion tensor imaging (DTI). The CT-GAN can
learn topological features and generate multimodal connectivity from multimodal
imaging data in an efficient end-to-end manner. Moreover, the swapping
bi-attention mechanism is designed to gradually align common features and
effectively enhance the complementary features between modalities. By analyzing
the generated connectivity features, the proposed model can identify AD-related
brain connections. Evaluations on the public ADNI dataset show that the
proposed CT-GAN can dramatically improve prediction performance and detect
AD-related brain regions effectively. The proposed model also provides new
insights for detecting AD-related abnormal neural circuits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zuo_Q/0/1/0/all/0/1&quot;&gt;Qiankun Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Junren Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuqiang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16207">
<title>Parameter-Saving Adversarial Training: Reinforcing Multi-Perturbation Robustness via Hypernetworks. (arXiv:2309.16207v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16207</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial training serves as one of the most popular and effective methods
to defend against adversarial perturbations. However, most defense mechanisms
only consider a single type of perturbation while various attack methods might
be adopted to perform stronger adversarial attacks against the deployed model
in real-world scenarios, e.g., $\ell_2$ or $\ell_\infty$. Defending against
various attacks can be a challenging problem since multi-perturbation
adversarial training and its variants only achieve suboptimal robustness
trade-offs, due to the theoretical limit to multi-perturbation robustness for a
single model. Besides, it is impractical to deploy large models in some
storage-efficient scenarios. To settle down these drawbacks, in this paper we
propose a novel multi-perturbation adversarial training framework,
parameter-saving adversarial training (PSAT), to reinforce multi-perturbation
robustness with an advantageous side effect of saving parameters, which
leverages hypernetworks to train specialized models against a single
perturbation and aggregate these specialized models to defend against multiple
perturbations. Eventually, we extensively evaluate and compare our proposed
method with state-of-the-art single/multi-perturbation robust methods against
various latest attack methods on different datasets, showing the robustness
superiority and parameter efficiency of our proposed method, e.g., for the
CIFAR-10 dataset with ResNet-50 as the backbone, PSAT saves approximately 80\%
of parameters with achieving the state-of-the-art robustness trade-off
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1&quot;&gt;Huihui Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1&quot;&gt;Minjing Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Siqi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camtepe_S/0/1/0/all/0/1&quot;&gt;Seyit Camtepe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nepal_S/0/1/0/all/0/1&quot;&gt;Surya Nepal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16208">
<title>Nonconvex third-order Tensor Recovery Based on Logarithmic Minimax Function. (arXiv:2309.16208v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16208</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent researches have shown that low-rank tensor recovery based non-convex
relaxation has gained extensive attention. In this context, we propose a new
Logarithmic Minimax (LM) function. The comparative analysis between the LM
function and the Logarithmic, Minimax concave penalty (MCP), and Minimax
Logarithmic concave penalty (MLCP) functions reveals that the proposed function
can protect large singular values while imposing stronger penalization on small
singular values. Based on this, we define a weighted tensor LM norm as a
non-convex relaxation for tensor tubal rank. Subsequently, we propose the
TLM-based low-rank tensor completion (LRTC) model and the TLM-based tensor
robust principal component analysis (TRPCA) model respectively. Furthermore, we
provide theoretical convergence guarantees for the proposed methods.
Comprehensive experiments were conducted on various real datasets, and a
comparison analysis was made with the similar EMLCP method. The results
demonstrate that the proposed method outperforms the state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongbing Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16210">
<title>Abdominal multi-organ segmentation in CT using Swinunter. (arXiv:2309.16210v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.16210</link>
<description rdf:parseType="Literal">&lt;p&gt;Abdominal multi-organ segmentation in computed tomography (CT) is crucial for
many clinical applications including disease detection and treatment planning.
Deep learning methods have shown unprecedented performance in this perspective.
However, it is still quite challenging to accurately segment different organs
utilizing a single network due to the vague boundaries of organs, the complex
background, and the substantially different organ size scales. In this work we
used make transformer-based model for training. It was found through previous
years&apos; competitions that basically all of the top 5 methods used CNN-based
methods, which is likely due to the lack of data volume that prevents
transformer-based methods from taking full advantage. The thousands of samples
in this competition may enable the transformer-based model to have more
excellent results. The results on the public validation set also show that the
transformer-based model can achieve an acceptable result and inference time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingjin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yongkang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yongyi Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16211">
<title>VDC: Versatile Data Cleanser for Detecting Dirty Samples via Visual-Linguistic Inconsistency. (arXiv:2309.16211v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16211</link>
<description rdf:parseType="Literal">&lt;p&gt;The role of data in building AI systems has recently been emphasized by the
emerging concept of data-centric AI. Unfortunately, in the real-world, datasets
may contain dirty samples, such as poisoned samples from backdoor attack, noisy
labels in crowdsourcing, and even hybrids of them. The presence of such dirty
samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect
dirty samples to improve the quality and realiability of dataset. Existing
detectors only focus on detecting poisoned samples or noisy labels, that are
often prone to weak generalization when dealing with dirty samples from other
domains.In this paper, we find a commonality of various dirty samples is
visual-linguistic inconsistency between images and associated labels. To
capture the semantic inconsistency between modalities, we propose versatile
data cleanser (VDC) leveraging the surpassing capabilities of multimodal large
language models (MLLM) in cross-modal alignment and reasoning.It consists of
three consecutive modules: the visual question generation module to generate
insightful questions about the image; the visual question answering module to
acquire the semantics of the visual content by answering the questions with
MLLM; followed by the visual answer evaluation module to evaluate the
inconsistency.Extensive experiments demonstrate its superior performance and
generalization to various categories and types of dirty samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zihao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingda Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1&quot;&gt;Shaokui Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bingzhe Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16217">
<title>GAFlow: Incorporating Gaussian Attention into Optical Flow. (arXiv:2309.16217v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16217</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical flow, or the estimation of motion fields from image sequences, is one
of the fundamental problems in computer vision. Unlike most pixel-wise tasks
that aim at achieving consistent representations of the same category, optical
flow raises extra demands for obtaining local discrimination and smoothness,
which yet is not fully explored by existing approaches. In this paper, we push
Gaussian Attention (GA) into the optical flow models to accentuate local
properties during representation learning and enforce the motion affinity
during matching. Specifically, we introduce a novel Gaussian-Constrained Layer
(GCL) which can be easily plugged into existing Transformer blocks to highlight
the local neighborhood that contains fine-grained structural information.
Moreover, for reliable motion analysis, we provide a new Gaussian-Guided
Attention Module (GGAM) which not only inherits properties from Gaussian
distribution to instinctively revolve around the neighbor fields of each point
but also is empowered to put the emphasis on contextually related regions
during matching. Our fully-equipped model, namely Gaussian Attention Flow
network (GAFlow), naturally incorporates a series of novel Gaussian-based
modules into the conventional optical flow framework for reliable motion
analysis. Extensive experiments on standard optical flow datasets consistently
demonstrate the exceptional performance of the proposed approach in terms of
both generalization ability evaluation and online benchmark testing. Code is
available at https://github.com/LA30/GAFlow.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_A/0/1/0/all/0/1&quot;&gt;Ao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1&quot;&gt;Lang Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chunyu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Haoqiang Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shuaicheng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16221">
<title>Off-the-shelf bin picking workcell with visual pose estimation: A case study on the world robot summit 2018 kitting task. (arXiv:2309.16221v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2309.16221</link>
<description rdf:parseType="Literal">&lt;p&gt;The World Robot Summit 2018 Assembly Challenge included four different tasks.
The kitting task, which required bin-picking, was the task in which the fewest
points were obtained. However, bin-picking is a vital skill that can
significantly increase the flexibility of robotic set-ups, and is, therefore,
an important research field. In recent years advancements have been made in
sensor technology and pose estimation algorithms. These advancements allow for
better performance when performing visual pose estimation.
&lt;/p&gt;
&lt;p&gt;This paper shows that by utilizing new vision sensors and pose estimation
algorithms pose estimation in bins can be performed successfully. We also
implement a workcell for bin picking along with a force based grasping approach
to perform the complete bin picking. Our set-up is tested on the World Robot
Summit 2018 Assembly Challenge and successfully obtains a higher score compared
with all teams at the competition. This demonstrate that current technology can
perform bin-picking at a much higher level compared with previous results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hagelskjaer_F/0/1/0/all/0/1&quot;&gt;Frederik Hagelskj&amp;#xe6;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorenzen_K/0/1/0/all/0/1&quot;&gt;Kasper H&amp;#xf8;j Lorenzen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kraft_D/0/1/0/all/0/1&quot;&gt;Dirk Kraft&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16237">
<title>Object Motion Guided Human Motion Synthesis. (arXiv:2309.16237v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16237</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling human behaviors in contextual environments has a wide range of
applications in character animation, embodied AI, VR/AR, and robotics. In
real-world scenarios, humans frequently interact with the environment and
manipulate various objects to complete daily tasks. In this work, we study the
problem of full-body human motion synthesis for the manipulation of large-sized
objects. We propose Object MOtion guided human MOtion synthesis (OMOMO), a
conditional diffusion framework that can generate full-body manipulation
behaviors from only the object motion. Since naively applying diffusion models
fails to precisely enforce contact constraints between the hands and the
object, OMOMO learns two separate denoising processes to first predict hand
positions from object motion and subsequently synthesize full-body poses based
on the predicted hand positions. By employing the hand positions as an
intermediate representation between the two denoising processes, we can
explicitly enforce contact constraints, resulting in more physically plausible
manipulation motions. With the learned model, we develop a novel system that
captures full-body human manipulation motions by simply attaching a smartphone
to the object being manipulated. Through extensive experiments, we demonstrate
the effectiveness of our proposed pipeline and its ability to generalize to
unseen objects. Additionally, as high-quality human-object interaction datasets
are scarce, we collect a large-scale dataset consisting of 3D object geometry,
object motion, and human motion. Our dataset contains human-object interaction
motion for 15 objects, with a total duration of approximately 10 hours.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiaman Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;C. Karen Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16249">
<title>FORB: A Flat Object Retrieval Benchmark for Universal Image Embedding. (arXiv:2309.16249v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16249</link>
<description rdf:parseType="Literal">&lt;p&gt;Image retrieval is a fundamental task in computer vision. Despite recent
advances in this field, many techniques have been evaluated on a limited number
of domains, with a small number of instance categories. Notably, most existing
works only consider domains like 3D landmarks, making it difficult to
generalize the conclusions made by these works to other domains, e.g., logo and
other 2D flat objects. To bridge this gap, we introduce a new dataset for
benchmarking visual search methods on flat images with diverse patterns. Our
flat object retrieval benchmark (FORB) supplements the commonly adopted 3D
object domain, and more importantly, it serves as a testbed for assessing the
image embedding quality on out-of-distribution domains. In this benchmark we
investigate the retrieval accuracy of representative methods in terms of
candidate ranks, as well as matching score margin, a viewpoint which is largely
ignored by many works. Our experiments not only highlight the challenges and
rich heterogeneity of FORB, but also reveal the hidden properties of different
retrieval strategies. The proposed benchmark is a growing project and we expect
to expand in both quantity and variety of objects. The dataset and supporting
codes are available at https://github.com/pxiangwu/FORB/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1&quot;&gt;Pengxiang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siman Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosa_K/0/1/0/all/0/1&quot;&gt;Kevin Dela Rosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Derek Hao Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16257">
<title>Nondestructive chicken egg fertility detection using CNN-transfer learning algorithms. (arXiv:2309.16257v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16257</link>
<description rdf:parseType="Literal">&lt;p&gt;This study explored the application of CNN-Transfer Learning for
nondestructive chicken egg fertility detection for precision poultry hatchery
practices. Four models, VGG16, ResNet50, InceptionNet, and MobileNet, were
trained and evaluated on a dataset (200 single egg images) using augmented
images (rotation, flip, scale, translation, and reflection). Although the
training results demonstrated that all models achieved high accuracy,
indicating their ability to accurately learn and classify chicken eggs&apos;
fertility state, when evaluated on the testing set, variations in accuracy and
performance were observed. InceptionNet exhibited the best overall performance,
accurately classifying fertile and non-fertile eggs. It demonstrated excellent
performance in both training and testing sets in all parameters of the
evaluation metrics. In testing set, it achieved an accuracy of 0.98, a
sensitivity of 1 for detecting fertile eggs, and a specificity of 0.96 for
identifying non-fertile eggs. The higher performance is attributed to its
unique architecture efficiently capturing features at different scales leading
to improved accuracy and robustness. Further optimization and fine-tuning of
the models might necessary to address the limitations in accurately detecting
fertile and non-fertile eggs in case of other models. This study highlighted
the potential of CNN-Transfer Learning for nondestructive fertility detection
and emphasizes the need for further research to enhance the models&apos;
capabilities and ensure accurate classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saifullah_S/0/1/0/all/0/1&quot;&gt;Shoffan Saifullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drezewski_R/0/1/0/all/0/1&quot;&gt;Rafal Drezewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yudhana_A/0/1/0/all/0/1&quot;&gt;Anton Yudhana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pranolo_A/0/1/0/all/0/1&quot;&gt;Andri Pranolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaswijanti_W/0/1/0/all/0/1&quot;&gt;Wilis Kaswijanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suryotomo_A/0/1/0/all/0/1&quot;&gt;Andiko Putro Suryotomo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Putra_S/0/1/0/all/0/1&quot;&gt;Seno Aji Putra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khaliduzzaman_A/0/1/0/all/0/1&quot;&gt;Alin Khaliduzzaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabuwono_A/0/1/0/all/0/1&quot;&gt;Anton Satria Prabuwono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Japkowicz_N/0/1/0/all/0/1&quot;&gt;Nathalie Japkowicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16264">
<title>GAMMA: Generalizable Articulation Modeling and Manipulation for Articulated Objects. (arXiv:2309.16264v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2309.16264</link>
<description rdf:parseType="Literal">&lt;p&gt;Articulated objects like cabinets and doors are widespread in daily life.
However, directly manipulating 3D articulated objects is challenging because
they have diverse geometrical shapes, semantic categories, and kinetic
constraints. Prior works mostly focused on recognizing and manipulating
articulated objects with specific joint types. They can either estimate the
joint parameters or distinguish suitable grasp poses to facilitate trajectory
planning. Although these approaches have succeeded in certain types of
articulated objects, they lack generalizability to unseen objects, which
significantly impedes their application in broader scenarios. In this paper, we
propose a novel framework of Generalizable Articulation Modeling and
Manipulating for Articulated Objects (GAMMA), which learns both articulation
modeling and grasp pose affordance from diverse articulated objects with
different categories. In addition, GAMMA adopts adaptive manipulation to
iteratively reduce the modeling errors and enhance manipulation performance. We
train GAMMA with the PartNet-Mobility dataset and evaluate with comprehensive
experiments in SAPIEN simulation and real-world Franka robot arms. Results show
that GAMMA significantly outperforms SOTA articulation modeling and
manipulation algorithms in unseen and cross-category articulated objects. We
will open-source all codes and datasets in both simulation and real robots for
reproduction in the final version. Images and videos are published on the
project website at: &lt;a href=&quot;http://sites.google.com/view/gamma-articulation&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qiaojun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenhai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1&quot;&gt;Ce Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1&quot;&gt;Lin Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weiming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Cewu Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16283">
<title>Self-supervised Cross-view Representation Reconstruction for Change Captioning. (arXiv:2309.16283v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16283</link>
<description rdf:parseType="Literal">&lt;p&gt;Change captioning aims to describe the difference between a pair of similar
images. Its key challenge is how to learn a stable difference representation
under pseudo changes caused by viewpoint change. In this paper, we address this
by proposing a self-supervised cross-view representation reconstruction
(SCORER) network. Concretely, we first design a multi-head token-wise matching
to model relationships between cross-view features from similar/dissimilar
images. Then, by maximizing cross-view contrastive alignment of two similar
images, SCORER learns two view-invariant image representations in a
self-supervised way. Based on these, we reconstruct the representations of
unchanged objects by cross-attention, thus learning a stable difference
representation for caption generation. Further, we devise a cross-modal
backward reasoning to improve the quality of caption. This module reversely
models a ``hallucination&apos;&apos; representation with the caption and ``before&apos;&apos;
representation. By pushing it closer to the ``after&apos;&apos; representation, we
enforce the caption to be informative about the difference in a self-supervised
manner. Extensive experiments show our method achieves the state-of-the-art
results on four datasets. The code is available at
https://github.com/tuyunbin/SCORER.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1&quot;&gt;Yunbin Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1&quot;&gt;Li Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1&quot;&gt;Zheng-Jun Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1&quot;&gt;Chenggang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qingming Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16301">
<title>Multi-scale Recurrent LSTM and Transformer Network for Depth Completion. (arXiv:2309.16301v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16301</link>
<description rdf:parseType="Literal">&lt;p&gt;Lidar depth completion is a new and hot topic of depth estimation. In this
task, it is the key and difficult point to fuse the features of color space and
depth space. In this paper, we migrate the classic LSTM and Transformer modules
from NLP to depth completion and redesign them appropriately. Specifically, we
use Forget gate, Update gate, Output gate, and Skip gate to achieve the
efficient fusion of color and depth features and perform loop optimization at
multiple scales. Finally, we further fuse the deep features through the
Transformer multi-head attention mechanism. Experimental results show that
without repetitive network structure and post-processing steps, our method can
achieve state-of-the-art performance by adding our modules to a simple
encoder-decoder network structure. Our method ranks first on the current
mainstream autonomous driving KITTI benchmark dataset. It can also be regarded
as a backbone network for other methods, which likewise achieves
state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xiaogang Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Yusong Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jian_S/0/1/0/all/0/1&quot;&gt;Songlei Jian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_Y/0/1/0/all/0/1&quot;&gt;Yonggang Che&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16306">
<title>Can the Query-based Object Detector Be Designed with Fewer Stages?. (arXiv:2309.16306v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16306</link>
<description rdf:parseType="Literal">&lt;p&gt;Query-based object detectors have made significant advancements since the
publication of DETR. However, most existing methods still rely on multi-stage
encoders and decoders, or a combination of both. Despite achieving high
accuracy, the multi-stage paradigm (typically consisting of 6 stages) suffers
from issues such as heavy computational burden, prompting us to reconsider its
necessity. In this paper, we explore multiple techniques to enhance query-based
detectors and, based on these findings, propose a novel model called GOLO
(Global Once and Local Once), which follows a two-stage decoding paradigm.
Compared to other mainstream query-based models with multi-stage decoders, our
model employs fewer decoder stages while still achieving considerable
performance. Experimental results on the COCO dataset demonstrate the
effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jialin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1&quot;&gt;Weifu Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yuhuan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_Q/0/1/0/all/0/1&quot;&gt;Qiang Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16309">
<title>Weakly-Supervised Video Anomaly Detection with Snippet Anomalous Attention. (arXiv:2309.16309v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16309</link>
<description rdf:parseType="Literal">&lt;p&gt;With a focus on abnormal events contained within untrimmed videos, there is
increasing interest among researchers in video anomaly detection. Among
different video anomaly detection scenarios, weakly-supervised video anomaly
detection poses a significant challenge as it lacks frame-wise labels during
the training stage, only relying on video-level labels as coarse supervision.
Previous methods have made attempts to either learn discriminative features in
an end-to-end manner or employ a twostage self-training strategy to generate
snippet-level pseudo labels. However, both approaches have certain limitations.
The former tends to overlook informative features at the snippet level, while
the latter can be susceptible to noises. In this paper, we propose an Anomalous
Attention mechanism for weakly-supervised anomaly detection to tackle the
aforementioned problems. Our approach takes into account snippet-level encoded
features without the supervision of pseudo labels. Specifically, our approach
first generates snippet-level anomalous attention and then feeds it together
with original anomaly scores into a Multi-branch Supervision Module. The module
learns different areas of the video, including areas that are challenging to
detect, and also assists the attention optimization. Experiments on benchmark
datasets XDViolence and UCF-Crime verify the effectiveness of our method.
Besides, thanks to the proposed snippet-level attention, we obtain a more
precise anomaly localization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yidan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yongxin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Wenhuan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yahong Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16337">
<title>Logarithm-transform aided Gaussian Sampling for Few-Shot Learning. (arXiv:2309.16337v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16337</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot image classification has recently witnessed the rise of
representation learning being utilised for models to adapt to new classes using
only a few training examples. Therefore, the properties of the representations,
such as their underlying probability distributions, assume vital importance.
Representations sampled from Gaussian distributions have been used in recent
works, [19] to train classifiers for few-shot classification. These methods
rely on transforming the distributions of experimental data to approximate
Gaussian distributions for their functioning. In this paper, I propose a novel
Gaussian transform, that outperforms existing methods on transforming
experimental data into Gaussian-like distributions. I then utilise this novel
transformation for few-shot image classification and show significant gains in
performance, while sampling lesser data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganatra_V/0/1/0/all/0/1&quot;&gt;Vaibhav Ganatra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16351">
<title>Dark Side Augmentation: Generating Diverse Night Examples for Metric Learning. (arXiv:2309.16351v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16351</link>
<description rdf:parseType="Literal">&lt;p&gt;Image retrieval methods based on CNN descriptors rely on metric learning from
a large number of diverse examples of positive and negative image pairs.
Domains, such as night-time images, with limited availability and variability
of training data suffer from poor retrieval performance even with methods
performing well on standard benchmarks. We propose to train a GAN-based
synthetic-image generator, translating available day-time image examples into
night images. Such a generator is used in metric learning as a form of
augmentation, supplying training data to the scarce domain. Various types of
generators are evaluated and analyzed. We contribute with a novel light-weight
GAN architecture that enforces the consistency between the original and
translated image through edge consistency. The proposed architecture also
allows a simultaneous training of an edge detector that operates on both night
and day images. To further increase the variability in the training examples
and to maximize the generalization of the trained model, we propose a novel
method of diverse anchor mining.
&lt;/p&gt;
&lt;p&gt;The proposed method improves over the state-of-the-art results on a standard
Tokyo 24/7 day-night retrieval benchmark while preserving the performance on
Oxford and Paris datasets. This is achieved without the need of training image
pairs of matching day and night images. The source code is available at
https://github.com/mohwald/gandtr .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohwald_A/0/1/0/all/0/1&quot;&gt;Albert Mohwald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jenicek_T/0/1/0/all/0/1&quot;&gt;Tomas Jenicek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chum_O/0/1/0/all/0/1&quot;&gt;Ond&amp;#x159;ej Chum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16354">
<title>Transformer-VQ: Linear-Time Transformers via Vector Quantization. (arXiv:2309.16354v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16354</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Transformer-VQ, a decoder-only transformer computing
softmax-based dense self-attention in linear time. Transformer-VQ&apos;s efficient
attention is enabled by vector-quantized keys and a novel caching mechanism. In
large-scale experiments, Transformer-VQ is shown highly competitive in quality,
with strong results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64
(3.16 bpb). Code: https://github.com/transformer-vq/transformer_vq
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lingle_L/0/1/0/all/0/1&quot;&gt;Lucas D. Lingle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16364">
<title>FG-NeRF: Flow-GAN based Probabilistic Neural Radiance Field for Independence-Assumption-Free Uncertainty Estimation. (arXiv:2309.16364v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16364</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural radiance fields with stochasticity have garnered significant interest
by enabling the sampling of plausible radiance fields and quantifying
uncertainty for downstream tasks. Existing works rely on the independence
assumption of points in the radiance field or the pixels in input views to
obtain tractable forms of the probability density function. However, this
assumption inadvertently impacts performance when dealing with intricate
geometry and texture. In this work, we propose an independence-assumption-free
probabilistic neural radiance field based on Flow-GAN. By combining the
generative capability of adversarial learning and the powerful expressivity of
normalizing flow, our method explicitly models the density-radiance
distribution of the whole scene. We represent our probabilistic NeRF as a
mean-shifted probabilistic residual neural model. Our model is trained without
an explicit likelihood function, thereby avoiding the independence assumption.
Specifically, We downsample the training images with different strides and
centers to form fixed-size patches which are used to train the generator with
patch-based adversarial learning. Through extensive experiments, our method
demonstrates state-of-the-art performance by predicting lower rendering errors
and more reliable uncertainty on both synthetic and real-world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1&quot;&gt;Songlin Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiazhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1&quot;&gt;Fanbo Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;He Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16372">
<title>Aperture Diffraction for Compact Snapshot Spectral Imaging. (arXiv:2309.16372v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16372</link>
<description rdf:parseType="Literal">&lt;p&gt;We demonstrate a compact, cost-effective snapshot spectral imaging system
named Aperture Diffraction Imaging Spectrometer (ADIS), which consists only of
an imaging lens with an ultra-thin orthogonal aperture mask and a mosaic filter
sensor, requiring no additional physical footprint compared to common RGB
cameras. Then we introduce a new optical design that each point in the object
space is multiplexed to discrete encoding locations on the mosaic filter sensor
by diffraction-based spatial-spectral projection engineering generated from the
orthogonal mask. The orthogonal projection is uniformly accepted to obtain a
weakly calibration-dependent data form to enhance modulation robustness.
Meanwhile, the Cascade Shift-Shuffle Spectral Transformer (CSST) with strong
perception of the diffraction degeneration is designed to solve a
sparsity-constrained inverse problem, realizing the volume reconstruction from
2D measurements with Large amount of aliasing. Our system is evaluated by
elaborating the imaging optical theory and reconstruction algorithm with
demonstrating the experimental imaging under a single exposure. Ultimately, we
achieve the sub-super-pixel spatial resolution and high spectral resolution
imaging. The code will be available at: https://github.com/Krito-ex/CSST.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1&quot;&gt;Tao Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1&quot;&gt;Hao Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Q/0/1/0/all/0/1&quot;&gt;Quan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yibo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xun Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16375">
<title>A Comprehensive Review on Tree Detection Methods Using Point Cloud and Aerial Imagery from Unmanned Aerial Vehicles. (arXiv:2309.16375v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16375</link>
<description rdf:parseType="Literal">&lt;p&gt;Unmanned Aerial Vehicles (UAVs) are considered cutting-edge technology with
highly cost-effective and flexible usage scenarios. Although many papers have
reviewed the application of UAVs in agriculture, the review of the application
for tree detection is still insufficient. This paper focuses on tree detection
methods applied to UAV data collected by UAVs. There are two kinds of data, the
point cloud and the images, which are acquired by the Light Detection and
Ranging (LiDAR) sensor and camera, respectively. Among the detection methods
using point-cloud data, this paper mainly classifies these methods according to
LiDAR and Digital Aerial Photography (DAP). For the detection methods using
images directly, this paper reviews these methods by whether or not to use the
Deep Learning (DL) method. Our review concludes and analyses the comparison and
combination between the application of LiDAR-based and DAP-based point cloud
data. The performance, relative merits, and application fields of the methods
are also introduced. Meanwhile, this review counts the number of tree detection
studies using different methods in recent years. From our statics, the
detection task using DL methods on the image has become a mainstream trend as
the number of DL-based detection researches increases to 45% of the total
number of tree detection studies up to 2022. As a result, this review could
help and guide researchers who want to carry out tree detection on specific
forests and for farmers to use UAVs in managing agriculture production.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_W/0/1/0/all/0/1&quot;&gt;Weijie Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_H/0/1/0/all/0/1&quot;&gt;Hann Woei Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Ye Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suandi_S/0/1/0/all/0/1&quot;&gt;Shahrel Azmin Suandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ismail_F/0/1/0/all/0/1&quot;&gt;Farzad Ismail&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16388">
<title>Biomedical Image Splicing Detection using Uncertainty-Guided Refinement. (arXiv:2309.16388v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16388</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, a surge in biomedical academic publications suspected of image
manipulation has led to numerous retractions, turning biomedical image
forensics into a research hotspot. While manipulation detectors are concerning,
the specific detection of splicing traces in biomedical images remains
underexplored. The disruptive factors within biomedical images, such as
artifacts, abnormal patterns, and noises, show misleading features like the
splicing traces, greatly increasing the challenge for this task. Moreover, the
scarcity of high-quality spliced biomedical images also limits potential
advancements in this field. In this work, we propose an Uncertainty-guided
Refinement Network (URN) to mitigate the effects of these disruptive factors.
Our URN can explicitly suppress the propagation of unreliable information flow
caused by disruptive factors among regions, thereby obtaining robust features.
Moreover, URN enables a concentration on the refinement of uncertainly
predicted regions during the decoding phase. Besides, we construct a dataset
for Biomedical image Splicing (BioSp) detection, which consists of 1,290
spliced images. Compared with existing datasets, BioSp comprises the largest
number of spliced images and the most diverse sources. Comprehensive
experiments on three benchmark datasets demonstrate the superiority of the
proposed method. Meanwhile, we verify the generalizability of URN when against
cross-dataset domain shifts and its robustness to resist post-processing
approaches. Our BioSp dataset will be released upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1&quot;&gt;Wenzhong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zitong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yizhong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Ying Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1&quot;&gt;Alex Kot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16390">
<title>An Enhanced Low-Resolution Image Recognition Method for Traffic Environments. (arXiv:2309.16390v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16390</link>
<description rdf:parseType="Literal">&lt;p&gt;Currently, low-resolution image recognition is confronted with a significant
challenge in the field of intelligent traffic perception. Compared to
high-resolution images, low-resolution images suffer from small size, low
quality, and lack of detail, leading to a notable decrease in the accuracy of
traditional neural network recognition algorithms. The key to low-resolution
image recognition lies in effective feature extraction. Therefore, this paper
delves into the fundamental dimensions of residual modules and their impact on
feature extraction and computational efficiency. Based on experiments, we
introduce a dual-branch residual network structure that leverages the basic
architecture of residual networks and a common feature subspace algorithm.
Additionally, it incorporates the utilization of intermediate-layer features to
enhance the accuracy of low-resolution image recognition. Furthermore, we
employ knowledge distillation to reduce network parameters and computational
overhead. Experimental results validate the effectiveness of this algorithm for
low-resolution image recognition in traffic environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zongcai Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zhenhai Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16393">
<title>HIC-YOLOv5: Improved YOLOv5 For Small Object Detection. (arXiv:2309.16393v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16393</link>
<description rdf:parseType="Literal">&lt;p&gt;Small object detection has been a challenging problem in the field of object
detection. There has been some works that proposes improvements for this task,
such as adding several attention blocks or changing the whole structure of
feature fusion networks. However, the computation cost of these models is
large, which makes deploying a real-time object detection system unfeasible,
while leaving room for improvement. To this end, an improved YOLOv5 model:
HIC-YOLOv5 is proposed to address the aforementioned problems. Firstly, an
additional prediction head specific to small objects is added to provide a
higher-resolution feature map for better prediction. Secondly, an involution
block is adopted between the backbone and neck to increase channel information
of the feature map. Moreover, an attention mechanism named CBAM is applied at
the end of the backbone, thus not only decreasing the computation cost compared
with previous works but also emphasizing the important information in both
channel and spatial domain. Our result shows that HIC-YOLOv5 has improved
mAP@[.5:.95] by 6.42% and mAP@0.5 by 9.38% on VisDrone-2019-DET dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shiyi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yini Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16414">
<title>AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models. (arXiv:2309.16414v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16414</link>
<description rdf:parseType="Literal">&lt;p&gt;Classifiers built upon vision-language models such as CLIP have shown
remarkable zero-shot performance across a broad range of image classification
tasks. Prior work has studied different ways of automatically creating
descriptor sets for every class based on prompt templates, ranging from
manually engineered templates over templates obtained from a large language
model to templates built from random words and characters. In contrast,
deriving zero-shot classifiers from the respective encoded class descriptors
has remained nearly unchanged, that is: classify to the class that maximizes
the cosine similarity between its averaged encoded class descriptors and the
encoded image. However, weighting all class descriptors equally can be
suboptimal when certain descriptors match visual clues on a given image better
than others. In this work, we propose AutoCLIP, a method for auto-tuning
zero-shot classifiers. AutoCLIP assigns to each prompt template per-image
weights, which are derived from statistics of class descriptor-image
similarities at inference time. AutoCLIP is fully unsupervised, has very low
overhead, and can be easily implemented in few lines of code. We show that for
a broad range of vision-language models, datasets, and prompt templates,
AutoCLIP outperforms baselines consistently and by up to 3 percent point
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metzen_J/0/1/0/all/0/1&quot;&gt;Jan Hendrik Metzen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saranrittichai_P/0/1/0/all/0/1&quot;&gt;Piyapat Saranrittichai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mummadi_C/0/1/0/all/0/1&quot;&gt;Chaithanya Kumar Mummadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16421">
<title>Distilling ODE Solvers of Diffusion Models into Smaller Steps. (arXiv:2309.16421v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16421</link>
<description rdf:parseType="Literal">&lt;p&gt;Distillation techniques have substantially improved the sampling speed of
diffusion models, allowing of the generation within only one step or a few
steps. However, these distillation methods require extensive training for each
dataset, sampler, and network, which limits their practical applicability. To
address this limitation, we propose a straightforward distillation approach,
Distilled-ODE solvers (D-ODE solvers), that optimizes the ODE solver rather
than training the denoising network. D-ODE solvers are formulated by simply
applying a single parameter adjustment to existing ODE solvers. Subsequently,
D-ODE solvers with smaller steps are optimized by ODE solvers with larger steps
through distillation over a batch of samples. Our comprehensive experiments
indicate that D-ODE solvers outperform existing ODE solvers, including DDIM,
PNDM, DPM-Solver, DEIS, and EDM, especially when generating samples with fewer
steps. Our method incur negligible computational overhead compared to previous
distillation techniques, enabling simple and rapid integration with previous
samplers. Qualitative analysis further shows that D-ODE solvers enhance image
quality while preserving the sampling trajectory of ODE solvers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sanghwan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fisher Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16435">
<title>Radar Instance Transformer: Reliable Moving Instance Segmentation in Sparse Radar Point Clouds. (arXiv:2309.16435v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16435</link>
<description rdf:parseType="Literal">&lt;p&gt;The perception of moving objects is crucial for autonomous robots performing
collision avoidance in dynamic environments. LiDARs and cameras tremendously
enhance scene interpretation but do not provide direct motion information and
face limitations under adverse weather. Radar sensors overcome these
limitations and provide Doppler velocities, delivering direct information on
dynamic objects. In this paper, we address the problem of moving instance
segmentation in radar point clouds to enhance scene interpretation for
safety-critical tasks. Our Radar Instance Transformer enriches the current
radar scan with temporal information without passing aggregated scans through a
neural network. We propose a full-resolution backbone to prevent information
loss in sparse point cloud processing. Our instance transformer head
incorporates essential information to enhance segmentation but also enables
reliable, class-agnostic instance assignments. In sum, our approach shows
superior performance on the new moving instance segmentation benchmarks,
including diverse environments, and provides model-agnostic modules to enhance
scene interpretation. The benchmark is based on the RadarScenes dataset and
will be made available upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeller_M/0/1/0/all/0/1&quot;&gt;Matthias Zeller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sandhu_V/0/1/0/all/0/1&quot;&gt;Vardeep S. Sandhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mersch_B/0/1/0/all/0/1&quot;&gt;Benedikt Mersch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behley_J/0/1/0/all/0/1&quot;&gt;Jens Behley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heidingsfeld_M/0/1/0/all/0/1&quot;&gt;Michael Heidingsfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stachniss_C/0/1/0/all/0/1&quot;&gt;Cyrill Stachniss&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16451">
<title>Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering. (arXiv:2309.16451v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16451</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing deep learning models have achieved promising performance in
recognizing skin diseases from dermoscopic images. However, these models can
only recognize samples from predefined categories, when they are deployed in
the clinic, data from new unknown categories are constantly emerging.
Therefore, it is crucial to automatically discover and identify new semantic
categories from new data. In this paper, we propose a new novel class discovery
framework for automatically discovering new semantic classes from dermoscopy
image datasets based on the knowledge of known classes. Specifically, we first
use contrastive learning to learn a robust and unbiased feature representation
based on all data from known and unknown categories. We then propose an
uncertainty-aware multi-view cross pseudo-supervision strategy, which is
trained jointly on all categories of data using pseudo labels generated by a
self-labeling strategy. Finally, we further refine the pseudo label by
aggregating neighborhood information through local sample similarity to improve
the clustering performance of the model for unknown categories. We conducted
extensive experiments on the dermatology dataset ISIC 2019, and the
experimental results show that our approach can effectively leverage knowledge
from known categories to discover new semantic categories. We also further
validated the effectiveness of the different modules through extensive ablation
experiments. Our code will be released soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1&quot;&gt;Wei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_L/0/1/0/all/0/1&quot;&gt;Lie Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1&quot;&gt;Kaimin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1&quot;&gt;Zongyuan Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16460">
<title>Diverse Target and Contribution Scheduling for Domain Generalization. (arXiv:2309.16460v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16460</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalization under the distribution shift has been a great challenge in
computer vision. The prevailing practice of directly employing the one-hot
labels as the training targets in domain generalization~(DG) can lead to
gradient conflicts, making it insufficient for capturing the intrinsic class
characteristics and hard to increase the intra-class variation. Besides,
existing methods in DG mostly overlook the distinct contributions of source
(seen) domains, resulting in uneven learning from these domains. To address
these issues, we firstly present a theoretical and empirical analysis of the
existence of gradient conflicts in DG, unveiling the previously unexplored
relationship between distribution shifts and gradient conflicts during the
optimization process. In this paper, we present a novel perspective of DG from
the empirical source domain&apos;s risk and propose a new paradigm for DG called
Diverse Target and Contribution Scheduling (DTCS). DTCS comprises two
innovative modules: Diverse Target Supervision (DTS) and Diverse Contribution
Balance (DCB), with the aim of addressing the limitations associated with the
common utilization of one-hot labels and equal contributions for source domains
in DG. In specific, DTS employs distinct soft labels as training targets to
account for various feature distributions across domains and thereby mitigates
the gradient conflicts, and DCB dynamically balances the contributions of
source domains by ensuring a fair decline in losses of different source
domains. Extensive experiments with analysis on four benchmark datasets show
that the proposed method achieves a competitive performance in comparison with
the state-of-the-art approaches, demonstrating the effectiveness and advantages
of the proposed DTCS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1&quot;&gt;Shaocong Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qianyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_C/0/1/0/all/0/1&quot;&gt;Chenhao Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lizhuang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yuan Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16483">
<title>Rethinking Domain Generalization: Discriminability and Generalizability. (arXiv:2309.16483v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16483</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain generalization (DG) endeavors to develop robust models that possess
strong generalizability while preserving excellent discriminability.
Nonetheless, pivotal DG techniques tend to improve the feature generalizability
by learning domain-invariant representations, inadvertently overlooking the
feature discriminability. On the one hand, the simultaneous attainment of
generalizability and discriminability of features presents a complex challenge,
often entailing inherent contradictions. This challenge becomes particularly
pronounced when domain-invariant features manifest reduced discriminability
owing to the inclusion of unstable factors, \emph{i.e.,} spurious correlations.
On the other hand, prevailing domain-invariant methods can be categorized as
category-level alignment, susceptible to discarding indispensable features
possessing substantial generalizability and narrowing intra-class variations.
To surmount these obstacles, we rethink DG from a new perspective that
concurrently imbues features with formidable discriminability and robust
generalizability, and present a novel framework, namely, Discriminative
Microscopic Distribution Alignment (DMDA). DMDA incorporates two core
components: Selective Channel Pruning~(SCP) and Micro-level Distribution
Alignment (MDA). Concretely, SCP attempts to curtail redundancy within neural
networks, prioritizing stable attributes conducive to accurate classification.
This approach alleviates the adverse effect of spurious domain invariance and
amplifies the feature discriminability. Besides, MDA accentuates micro-level
alignment within each class, going beyond mere category-level alignment. This
strategy accommodates sufficient generalizable features and facilitates
within-class variations. Extensive experiments on four benchmark datasets
corroborate the efficacy of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1&quot;&gt;Shaocong Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qianyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_C/0/1/0/all/0/1&quot;&gt;Chenhao Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lizhuang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yuan Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16486">
<title>HTC-DC Net: Monocular Height Estimation from Single Remote Sensing Images. (arXiv:2309.16486v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16486</link>
<description rdf:parseType="Literal">&lt;p&gt;3D geo-information is of great significance for understanding the living
environment; however, 3D perception from remote sensing data, especially on a
large scale, is restricted. To tackle this problem, we propose a method for
monocular height estimation from optical imagery, which is currently one of the
richest sources of remote sensing data. As an ill-posed problem, monocular
height estimation requires well-designed networks for enhanced representations
to improve performance. Moreover, the distribution of height values is
long-tailed with the low-height pixels, e.g., the background, as the head, and
thus trained networks are usually biased and tend to underestimate building
heights. To solve the problems, instead of formalizing the problem as a
regression task, we propose HTC-DC Net following the classification-regression
paradigm, with the head-tail cut (HTC) and the distribution-based constraints
(DCs) as the main contributions. HTC-DC Net is composed of the backbone network
as the feature extractor, the HTC-AdaBins module, and the hybrid regression
process. The HTC-AdaBins module serves as the classification phase to determine
bins adaptive to each input image. It is equipped with a vision transformer
encoder to incorporate local context with holistic information and involves an
HTC to address the long-tailed problem in monocular height estimation for
balancing the performances of foreground and background pixels. The hybrid
regression process does the regression via the smoothing of bins from the
classification phase, which is trained via DCs. The proposed network is tested
on three datasets of different resolutions, namely ISPRS Vaihingen (0.09 m),
DFC19 (1.3 m) and GBH (3 m). Experimental results show the superiority of the
proposed network over existing methods by large margins. Extensive ablation
studies demonstrate the effectiveness of each design component.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sining Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yilei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zhitong Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiao Xiang Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16494">
<title>Accurate and lightweight dehazing via multi-receptive-field non-local network and novel contrastive regularization. (arXiv:2309.16494v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16494</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, deep learning-based methods have dominated image dehazing domain.
Although very competitive dehazing performance has been achieved with
sophisticated models, effective solutions for extracting useful features are
still under-explored. In addition, non-local network, which has made a
breakthrough in many vision tasks, has not been appropriately applied to image
dehazing. Thus, a multi-receptive-field non-local network (MRFNLN) consisting
of the multi-stream feature attention block (MSFAB) and cross non-local block
(CNLB) is presented in this paper. We start with extracting richer features for
dehazing. Specifically, we design a multi-stream feature extraction (MSFE)
sub-block, which contains three parallel convolutions with different receptive
fields (i.e., $1\times 1$, $3\times 3$, $5\times 5$) for extracting multi-scale
features. Following MSFE, we employ an attention sub-block to make the model
adaptively focus on important channels/regions. The MSFE and attention
sub-blocks constitute our MSFAB. Then, we design a cross non-local block
(CNLB), which can capture long-range dependencies beyond the query. Instead of
the same input source of query branch, the key and value branches are enhanced
by fusing more preceding features. CNLB is computation-friendly by leveraging a
spatial pyramid down-sampling (SPDS) strategy to reduce the computation and
memory consumption without sacrificing the performance. Last but not least, a
novel detail-focused contrastive regularization (DFCR) is presented by
emphasizing the low-level details and ignoring the high-level semantic
information in the representation space. Comprehensive experimental results
demonstrate that the proposed MRFNLN model outperforms recent state-of-the-art
dehazing methods with less than 1.5 Million parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zewei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zixuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Ziqian Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xuecheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhe-Ming Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16495">
<title>Deep Single Models vs. Ensembles: Insights for a Fast Deployment of Parking Monitoring Systems. (arXiv:2309.16495v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16495</link>
<description rdf:parseType="Literal">&lt;p&gt;Searching for available parking spots in high-density urban centers is a
stressful task for drivers that can be mitigated by systems that know in
advance the nearest parking space available.
&lt;/p&gt;
&lt;p&gt;To this end, image-based systems offer cost advantages over other
sensor-based alternatives (e.g., ultrasonic sensors), requiring less physical
infrastructure for installation and maintenance.
&lt;/p&gt;
&lt;p&gt;Despite recent deep learning advances, deploying intelligent parking
monitoring is still a challenge since most approaches involve collecting and
labeling large amounts of data, which is laborious and time-consuming. Our
study aims to uncover the challenges in creating a global framework, trained
using publicly available labeled parking lot images, that performs accurately
across diverse scenarios, enabling the parking space monitoring as a
ready-to-use system to deploy in a new environment. Through exhaustive
experiments involving different datasets and deep learning architectures,
including fusion strategies and ensemble methods, we found that models trained
on diverse datasets can achieve 95\% accuracy without the burden of data
annotation and model training on the target parking lot
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hochuli_A/0/1/0/all/0/1&quot;&gt;Andre Gustavo Hochuli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barddal_J/0/1/0/all/0/1&quot;&gt;Jean Paul Barddal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palhano_G/0/1/0/all/0/1&quot;&gt;Gillian Cezar Palhano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendes_L/0/1/0/all/0/1&quot;&gt;Leonardo Matheus Mendes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almeida_P/0/1/0/all/0/1&quot;&gt;Paulo Ricardo Lisboa de Almeida&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16496">
<title>CCEdit: Creative and Controllable Video Editing via Diffusion Models. (arXiv:2309.16496v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16496</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present CCEdit, a versatile framework designed to address
the challenges of creative and controllable video editing. CCEdit accommodates
a wide spectrum of user editing requirements and enables enhanced creative
control through an innovative approach that decouples video structure and
appearance. We leverage the foundational ControlNet architecture to preserve
structural integrity, while seamlessly integrating adaptable temporal modules
compatible with state-of-the-art personalization techniques for text-to-image
generation, such as DreamBooth and LoRA.Furthermore, we introduce
reference-conditioned video editing, empowering users to exercise precise
creative control over video editing through the more manageable process of
editing key frames. Our extensive experimental evaluations confirm the
exceptional functionality and editing capabilities of the proposed CCEdit
framework. Demo video is available at
https://www.youtube.com/watch?v=UQw4jq-igN4.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1&quot;&gt;Ruoyu Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_W/0/1/0/all/0/1&quot;&gt;Wenming Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanhui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuhui Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1&quot;&gt;Jianmin Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhibo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1&quot;&gt;Baining Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16499">
<title>Cross-City Matters: A Multimodal Remote Sensing Benchmark Dataset for Cross-City Semantic Segmentation using High-Resolution Domain Adaptation Networks. (arXiv:2309.16499v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16499</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) approaches nowadays have gained remarkable
success in single-modality-dominated remote sensing (RS) applications,
especially with an emphasis on individual urban environments (e.g., single
cities or regions). Yet these AI models tend to meet the performance bottleneck
in the case studies across cities or regions, due to the lack of diverse RS
information and cutting-edge solutions with high generalization ability. To
this end, we build a new set of multimodal remote sensing benchmark datasets
(including hyperspectral, multispectral, SAR) for the study purpose of the
cross-city semantic segmentation task (called C2Seg dataset), which consists of
two cross-city scenes, i.e., Berlin-Augsburg (in Germany) and Beijing-Wuhan (in
China). Beyond the single city, we propose a high-resolution domain adaptation
network, HighDAN for short, to promote the AI model&apos;s generalization ability
from the multi-city environments. HighDAN is capable of retaining the spatially
topological structure of the studied urban scene well in a parallel high-to-low
resolution fusion fashion but also closing the gap derived from enormous
differences of RS image representations between different cities by means of
adversarial learning. In addition, the Dice loss is considered in HighDAN to
alleviate the class imbalance issue caused by factors across cities. Extensive
experiments conducted on the C2Seg dataset show the superiority of our HighDAN
in terms of segmentation performance and generalization ability, compared to
state-of-the-art competitors. The C2Seg dataset and the semantic segmentation
toolbox (involving the proposed HighDAN) will be available publicly at
https://github.com/danfenghong.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1&quot;&gt;Danfeng Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jing Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werner_M/0/1/0/all/0/1&quot;&gt;Martin Werner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chanussote_J/0/1/0/all/0/1&quot;&gt;Jocelyn Chanussote&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zipf_A/0/1/0/all/0/1&quot;&gt;Alexander Zipf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiao Xiang Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16511">
<title>Toloka Visual Question Answering Benchmark. (arXiv:2309.16511v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16511</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present Toloka Visual Question Answering, a new
crowdsourced dataset allowing comparing performance of machine learning systems
against human level of expertise in the grounding visual question answering
task. In this task, given an image and a textual question, one has to draw the
bounding box around the object correctly responding to that question. Every
image-question pair contains the response, with only one correct response per
image. Our dataset contains 45,199 pairs of images and questions in English,
provided with ground truth bounding boxes, split into train and two test
subsets. Besides describing the dataset and releasing it under a CC BY license,
we conducted a series of experiments on open source zero-shot baseline models
and organized a multi-phase competition at WSDM Cup that attracted 48
participants worldwide. However, by the time of paper submission, no machine
learning model outperformed the non-expert crowdsourcing baseline according to
the intersection over union evaluation score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ustalov_D/0/1/0/all/0/1&quot;&gt;Dmitry Ustalov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavlichenko_N/0/1/0/all/0/1&quot;&gt;Nikita Pavlichenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koshelev_S/0/1/0/all/0/1&quot;&gt;Sergey Koshelev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Likhobaba_D/0/1/0/all/0/1&quot;&gt;Daniil Likhobaba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smirnova_A/0/1/0/all/0/1&quot;&gt;Alisa Smirnova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16515">
<title>Latent Noise Segmentation: How Neural Noise Leads to the Emergence of Segmentation and Grouping. (arXiv:2309.16515v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16515</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks (DNNs) that achieve human-level performance in general
tasks like object segmentation typically require supervised labels. In
contrast, humans are able to perform these tasks effortlessly without
supervision. To accomplish this, the human visual system makes use of
perceptual grouping. Understanding how perceptual grouping arises in an
unsupervised manner is critical for improving both models of the visual system,
and computer vision models. In this work, we propose a counterintuitive
approach to unsupervised perceptual grouping and segmentation: that they arise
because of neural noise, rather than in spite of it. We (1) mathematically
demonstrate that under realistic assumptions, neural noise can be used to
separate objects from each other, and (2) show that adding noise in a DNN
enables the network to segment images even though it was never trained on any
segmentation labels. Interestingly, we find that (3) segmenting objects using
noise results in segmentation performance that aligns with the perceptual
grouping phenomena observed in humans. We introduce the Good Gestalt (GG)
datasets -- six datasets designed to specifically test perceptual grouping, and
show that our DNN models reproduce many important phenomena in human
perception, such as illusory contours, closure, continuity, proximity, and
occlusion. Finally, we (4) demonstrate the ecological plausibility of the
method by analyzing the sensitivity of the DNN to different magnitudes of
noise. We find that some model variants consistently succeed with remarkably
low levels of neural noise ($\sigma&amp;lt;0.001$), and surprisingly, that segmenting
this way requires as few as a handful of samples. Together, our results suggest
a novel unsupervised segmentation method requiring few assumptions, a new
explanation for the formation of perceptual grouping, and a potential benefit
of neural noise in the visual system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lonnqvist_B/0/1/0/all/0/1&quot;&gt;Ben Lonnqvist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhengqing Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herzog_M/0/1/0/all/0/1&quot;&gt;Michael H. Herzog&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16524">
<title>HOI4ABOT: Human-Object Interaction Anticipation for Human Intention Reading Collaborative roBOTs. (arXiv:2309.16524v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16524</link>
<description rdf:parseType="Literal">&lt;p&gt;Robots are becoming increasingly integrated into our lives, assisting us in
various tasks. To ensure effective collaboration between humans and robots, it
is essential that they understand our intentions and anticipate our actions. In
this paper, we propose a Human-Object Interaction (HOI) anticipation framework
for collaborative robots. We propose an efficient and robust transformer-based
model to detect and anticipate HOIs from videos. This enhanced anticipation
empowers robots to proactively assist humans, resulting in more efficient and
intuitive collaborations. Our model outperforms state-of-the-art results in HOI
detection and anticipation in VidHOI dataset with an increase of 1.76% and
1.04% in mAP respectively while being 15.4 times faster. We showcase the
effectiveness of our approach through experimental results in a real robot,
demonstrating that the robot&apos;s ability to anticipate HOIs is key for better
Human-Robot Interaction. More information can be found on our project webpage:
https://evm7.github.io/HOI4ABOT_page/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mascaro_E/0/1/0/all/0/1&quot;&gt;Esteve Valls Mascaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sliwowski_D/0/1/0/all/0/1&quot;&gt;Daniel Sliwowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dongheui Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16534">
<title>MotionLM: Multi-Agent Motion Forecasting as Language Modeling. (arXiv:2309.16534v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16534</link>
<description rdf:parseType="Literal">&lt;p&gt;Reliable forecasting of the future behavior of road agents is a critical
component to safe planning in autonomous vehicles. Here, we represent
continuous trajectories as sequences of discrete motion tokens and cast
multi-agent motion prediction as a language modeling task over this domain. Our
model, MotionLM, provides several advantages: First, it does not require
anchors or explicit latent variable optimization to learn multimodal
distributions. Instead, we leverage a single standard language modeling
objective, maximizing the average log probability over sequence tokens. Second,
our approach bypasses post-hoc interaction heuristics where individual agent
trajectory generation is conducted prior to interactive scoring. Instead,
MotionLM produces joint distributions over interactive agent futures in a
single autoregressive decoding process. In addition, the model&apos;s sequential
factorization enables temporally causal conditional rollouts. The proposed
approach establishes new state-of-the-art performance for multi-agent motion
prediction on the Waymo Open Motion Dataset, ranking 1st on the interactive
challenge leaderboard.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seff_A/0/1/0/all/0/1&quot;&gt;Ari Seff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cera_B/0/1/0/all/0/1&quot;&gt;Brian Cera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1&quot;&gt;Mason Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Aurick Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nayakanti_N/0/1/0/all/0/1&quot;&gt;Nigamaa Nayakanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Refaat_K/0/1/0/all/0/1&quot;&gt;Khaled S. Refaat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Rfou_R/0/1/0/all/0/1&quot;&gt;Rami Al-Rfou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sapp_B/0/1/0/all/0/1&quot;&gt;Benjamin Sapp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16536">
<title>Uncertainty Quantification for Eosinophil Segmentation. (arXiv:2309.16536v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.16536</link>
<description rdf:parseType="Literal">&lt;p&gt;Eosinophilic Esophagitis (EoE) is an allergic condition increasing in
prevalence. To diagnose EoE, pathologists must find 15 or more eosinophils
within a single high-power field (400X magnification). Determining whether or
not a patient has EoE can be an arduous process and any medical imaging
approaches used to assist diagnosis must consider both efficiency and
precision. We propose an improvement of Adorno et al&apos;s approach for quantifying
eosinphils using deep image segmentation. Our new approach leverages Monte
Carlo Dropout, a common approach in deep learning to reduce overfitting, to
provide uncertainty quantification on current deep learning models. The
uncertainty can be visualized in an output image to evaluate model performance,
provide insight to how deep learning algorithms function, and assist
pathologists in identifying eosinophils.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kevin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brown_D/0/1/0/all/0/1&quot;&gt;Donald Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Syed_S/0/1/0/all/0/1&quot;&gt;Sana Syed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Greene_A/0/1/0/all/0/1&quot;&gt;Adam Greene&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16553">
<title>MatrixCity: A Large-scale City Dataset for City-scale Neural Rendering and Beyond. (arXiv:2309.16553v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16553</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural radiance fields (NeRF) and its subsequent variants have led to
remarkable progress in neural rendering. While most of recent neural rendering
works focus on objects and small-scale scenes, developing neural rendering
methods for city-scale scenes is of great potential in many real-world
applications. However, this line of research is impeded by the absence of a
comprehensive and high-quality dataset, yet collecting such a dataset over real
city-scale scenes is costly, sensitive, and technically difficult. To this end,
we build a large-scale, comprehensive, and high-quality synthetic dataset for
city-scale neural rendering researches. Leveraging the Unreal Engine 5 City
Sample project, we develop a pipeline to easily collect aerial and street city
views, accompanied by ground-truth camera poses and a range of additional data
modalities. Flexible controls over environmental factors like light, weather,
human and car crowd are also available in our pipeline, supporting the need of
various tasks covering city-scale neural rendering and beyond. The resulting
pilot dataset, MatrixCity, contains 67k aerial images and 452k street images
from two city maps of total size $28km^2$. On top of MatrixCity, a thorough
benchmark is also conducted, which not only reveals unique challenges of the
task of city-scale neural rendering, but also highlights potential improvements
for future works. The dataset and code will be publicly available at our
project page: https://city-super.github.io/matrixcity/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yixuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Lihan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Linning Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiangli_Y/0/1/0/all/0/1&quot;&gt;Yuanbo Xiangli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16561">
<title>Voting Network for Contour Levee Farmland Segmentation and Classification. (arXiv:2309.16561v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16561</link>
<description rdf:parseType="Literal">&lt;p&gt;High-resolution aerial imagery allows fine details in the segmentation of
farmlands. However, small objects and features introduce distortions to the
delineation of object boundaries, and larger contextual views are needed to
mitigate class confusion. In this work, we present an end-to-end trainable
network for segmenting farmlands with contour levees from high-resolution
aerial imagery. A fusion block is devised that includes multiple voting blocks
to achieve image segmentation and classification. We integrate the fusion block
with a backbone and produce both semantic predictions and segmentation slices.
The segmentation slices are used to perform majority voting on the predictions.
The network is trained to assign the most likely class label of a segment to
its pixels, learning the concept of farmlands rather than analyzing
constitutive pixels separately. We evaluate our method using images from the
National Agriculture Imagery Program. Our method achieved an average accuracy
of 94.34\%. Compared to the state-of-the-art methods, the proposed method
obtains an improvement of 6.96% and 2.63% in the F1 score on average.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyarian_A/0/1/0/all/0/1&quot;&gt;Abolfazl Meyarian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xiaohui Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16569">
<title>Audio-Visual Speaker Verification via Joint Cross-Attention. (arXiv:2309.16569v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2309.16569</link>
<description rdf:parseType="Literal">&lt;p&gt;Speaker verification has been widely explored using speech signals, which has
shown significant improvement using deep models. Recently, there has been a
surge in exploring faces and voices as they can offer more complementary and
comprehensive information than relying only on a single modality of speech
signals. Though current methods in the literature on the fusion of faces and
voices have shown improvement over that of individual face or voice modalities,
the potential of audio-visual fusion is not fully explored for speaker
verification. Most of the existing methods based on audio-visual fusion either
rely on score-level fusion or simple feature concatenation. In this work, we
have explored cross-modal joint attention to fully leverage the inter-modal
complementary information and the intra-modal information for speaker
verification. Specifically, we estimate the cross-attention weights based on
the correlation between the joint feature presentation and that of the
individual feature representations in order to effectively capture both
intra-modal as well inter-modal relationships among the faces and voices. We
have shown that efficiently leveraging the intra- and inter-modal relationships
significantly improves the performance of audio-visual fusion for speaker
verification. The performance of the proposed approach has been evaluated on
the Voxceleb1 dataset. Results show that the proposed approach can
significantly outperform the state-of-the-art methods of audio-visual fusion
for speaker verification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Praveen_R/0/1/0/all/0/1&quot;&gt;R. Gnana Praveen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_J/0/1/0/all/0/1&quot;&gt;Jahangir Alam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16585">
<title>Text-to-3D using Gaussian Splatting. (arXiv:2309.16585v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16585</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present Gaussian Splatting based text-to-3D generation
(GSGEN), a novel approach for generating high-quality 3D objects. Previous
methods suffer from inaccurate geometry and limited fidelity due to the absence
of 3D prior and proper representation. We leverage 3D Gaussian Splatting, a
recent state-of-the-art representation, to address existing shortcomings by
exploiting the explicit nature that enables the incorporation of 3D prior.
Specifically, our method adopts a progressive optimization strategy, which
includes a geometry optimization stage and an appearance refinement stage. In
geometry optimization, a coarse representation is established under a 3D
geometry prior along with the ordinary 2D SDS loss, ensuring a sensible and
3D-consistent rough shape. Subsequently, the obtained Gaussians undergo an
iterative refinement to enrich details. In this stage, we increase the number
of Gaussians by compactness-based densification to enhance continuity and
improve fidelity. With these designs, our approach can generate 3D content with
delicate details and more accurate geometry. Extensive evaluations demonstrate
the effectiveness of our method, especially for capturing high-frequency
components. Video results are provided at https://gsgen3d.github.io. Our code
is available at https://github.com/gsgen3d/gsgen
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zilong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Feng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huaping Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16588">
<title>Vision Transformers Need Registers. (arXiv:2309.16588v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16588</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have recently emerged as a powerful tool for learning visual
representations. In this paper, we identify and characterize artifacts in
feature maps of both supervised and self-supervised ViT networks. The artifacts
correspond to high-norm tokens appearing during inference primarily in
low-informative background areas of images, that are repurposed for internal
computations. We propose a simple yet effective solution based on providing
additional tokens to the input sequence of the Vision Transformer to fill that
role. We show that this solution fixes that problem entirely for both
supervised and self-supervised models, sets a new state of the art for
self-supervised visual models on dense visual prediction tasks, enables object
discovery methods with larger models, and most importantly leads to smoother
feature maps and attention maps for downstream visual processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darcet_T/0/1/0/all/0/1&quot;&gt;Timoth&amp;#xe9;e Darcet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oquab_M/0/1/0/all/0/1&quot;&gt;Maxime Oquab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mairal_J/0/1/0/all/0/1&quot;&gt;Julien Mairal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1&quot;&gt;Piotr Bojanowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16592">
<title>Tensor Factorization for Leveraging Cross-Modal Knowledge in Data-Constrained Infrared Object Detection. (arXiv:2309.16592v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16592</link>
<description rdf:parseType="Literal">&lt;p&gt;The primary bottleneck towards obtaining good recognition performance in IR
images is the lack of sufficient labeled training data, owing to the cost of
acquiring such data. Realizing that object detection methods for the RGB
modality are quite robust (at least for some commonplace classes, like person,
car, etc.), thanks to the giant training sets that exist, in this work we seek
to leverage cues from the RGB modality to scale object detectors to the IR
modality, while preserving model performance in the RGB modality. At the core
of our method, is a novel tensor decomposition method called TensorFact which
splits the convolution kernels of a layer of a Convolutional Neural Network
(CNN) into low-rank factor matrices, with fewer parameters than the original
CNN. We first pretrain these factor matrices on the RGB modality, for which
plenty of training data are assumed to exist and then augment only a few
trainable parameters for training on the IR modality to avoid over-fitting,
while encouraging them to capture complementary cues from those trained only on
the RGB modality. We validate our approach empirically by first assessing how
well our TensorFact decomposed network performs at the task of detecting
objects in RGB images vis-a-vis the original network and then look at how well
it adapts to IR images of the FLIR ADAS v1 dataset. For the latter, we train
models under scenarios that pose challenges stemming from data paucity. From
the experiments, we observe that: (i) TensorFact shows performance gains on RGB
images; (ii) further, this pre-trained model, when fine-tuned, outperforms a
standard state-of-the-art object detector on the FLIR ADAS v1 dataset by about
4% in terms of mAP 50 score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1&quot;&gt;Manish Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_M/0/1/0/all/0/1&quot;&gt;Moitreya Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1&quot;&gt;Kuan-Chuan Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lohit_S/0/1/0/all/0/1&quot;&gt;Suhas Lohit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1&quot;&gt;Michael Jones&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16608">
<title>KV Inversion: KV Embeddings Learning for Text-Conditioned Real Image Action Editing. (arXiv:2309.16608v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16608</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-conditioned image editing is a recently emerged and highly practical
task, and its potential is immeasurable. However, most of the concurrent
methods are unable to perform action editing, i.e. they can not produce results
that conform to the action semantics of the editing prompt and preserve the
content of the original image. To solve the problem of action editing, we
propose KV Inversion, a method that can achieve satisfactory reconstruction
performance and action editing, which can solve two major problems: 1) the
edited result can match the corresponding action, and 2) the edited object can
retain the texture and identity of the original real image. In addition, our
method does not require training the Stable Diffusion model itself, nor does it
require scanning a large-scale dataset to perform time-consuming training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiancheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yifan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1&quot;&gt;Jin Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shifeng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16627">
<title>Class Activation Map-based Weakly supervised Hemorrhage Segmentation using Resnet-LSTM in Non-Contrast Computed Tomography images. (arXiv:2309.16627v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.16627</link>
<description rdf:parseType="Literal">&lt;p&gt;In clinical settings, intracranial hemorrhages (ICH) are routinely diagnosed
using non-contrast CT (NCCT) for severity assessment. Accurate automated
segmentation of ICH lesions is the initial and essential step, immensely useful
for such assessment. However, compared to other structural imaging modalities
such as MRI, in NCCT images ICH appears with very low contrast and poor SNR.
Over recent years, deep learning (DL)-based methods have shown great potential,
however, training them requires a huge amount of manually annotated
lesion-level labels, with sufficient diversity to capture the characteristics
of ICH. In this work, we propose a novel weakly supervised DL method for ICH
segmentation on NCCT scans, using image-level binary classification labels,
which are less time-consuming and labor-efficient when compared to the manual
labeling of individual ICH lesions. Our method initially determines the
approximate location of ICH using class activation maps from a classification
network, which is trained to learn dependencies across contiguous slices. We
further refine the ICH segmentation using pseudo-ICH masks obtained in an
unsupervised manner. The method is flexible and uses a computationally light
architecture during testing. On evaluating our method on the validation data of
the MICCAI 2022 INSTANCE challenge, our method achieves a Dice value of 0.55,
comparable with those of existing weakly supervised method (Dice value of
0.47), despite training on a much smaller training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ramananda_S/0/1/0/all/0/1&quot;&gt;Shreyas H Ramananda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sundaresan_V/0/1/0/all/0/1&quot;&gt;Vaanathi Sundaresan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16633">
<title>Mixup Your Own Pairs. (arXiv:2309.16633v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16633</link>
<description rdf:parseType="Literal">&lt;p&gt;In representation learning, regression has traditionally received less
attention than classification. Directly applying representation learning
techniques designed for classification to regression often results in
fragmented representations in the latent space, yielding sub-optimal
performance. In this paper, we argue that the potential of contrastive learning
for regression has been overshadowed due to the neglect of two crucial aspects:
ordinality-awareness and hardness. To address these challenges, we advocate
&quot;mixup your own contrastive pairs for supervised contrastive regression&quot;,
instead of relying solely on real/augmented samples. Specifically, we propose
Supervised Contrastive Learning for Regression with Mixup (SupReMix). It takes
anchor-inclusive mixtures (mixup of the anchor and a distinct negative sample)
as hard negative pairs and anchor-exclusive mixtures (mixup of two distinct
negative samples) as hard positive pairs at the embedding level. This strategy
formulates harder contrastive pairs by integrating richer ordinal information.
Through extensive experiments on six regression datasets including 2D images,
volumetric images, text, tabular data, and time-series signals, coupled with
theoretical analysis, we demonstrate that SupReMix pre-training fosters
continuous ordered representations of regression data, resulting in significant
improvement in regression performance. Furthermore, SupReMix is superior to
other approaches in a range of regression challenges including transfer
learning, imbalanced training data, and scenarios with fewer training samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yilei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zijian Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chongyao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wangchunshu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Juan Helen Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16634">
<title>End-to-End (Instance)-Image Goal Navigation through Correspondence as an Emergent Phenomenon. (arXiv:2309.16634v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16634</link>
<description rdf:parseType="Literal">&lt;p&gt;Most recent work in goal oriented visual navigation resorts to large-scale
machine learning in simulated environments. The main challenge lies in learning
compact representations generalizable to unseen environments and in learning
high-capacity perception modules capable of reasoning on high-dimensional
input. The latter is particularly difficult when the goal is not given as a
category (&quot;ObjectNav&quot;) but as an exemplar image (&quot;ImageNav&quot;), as the perception
module needs to learn a comparison strategy requiring to solve an underlying
visual correspondence problem. This has been shown to be difficult from reward
alone or with standard auxiliary tasks. We address this problem through a
sequence of two pretext tasks, which serve as a prior for what we argue is one
of the main bottleneck in perception, extremely wide-baseline relative pose
estimation and visibility prediction in complex scenes. The first pretext task,
cross-view completion is a proxy for the underlying visual correspondence
problem, while the second task addresses goal detection and finding directly.
We propose a new dual encoder with a large-capacity binocular ViT model and
show that correspondence solutions naturally emerge from the training signals.
Experiments show significant improvements and SOTA performance on the two
benchmarks, ImageNav and the Instance-ImageNav variant, where camera intrinsics
and height differ between observation and goal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bono_G/0/1/0/all/0/1&quot;&gt;Guillaume Bono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antsfeld_L/0/1/0/all/0/1&quot;&gt;Leonid Antsfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chidlovskii_B/0/1/0/all/0/1&quot;&gt;Boris Chidlovskii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinzaepfel_P/0/1/0/all/0/1&quot;&gt;Philippe Weinzaepfel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1&quot;&gt;Christian Wolf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16643">
<title>Deep Geometrized Cartoon Line Inbetweening. (arXiv:2309.16643v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16643</link>
<description rdf:parseType="Literal">&lt;p&gt;We aim to address a significant but understudied problem in the anime
industry, namely the inbetweening of cartoon line drawings. Inbetweening
involves generating intermediate frames between two black-and-white line
drawings and is a time-consuming and expensive process that can benefit from
automation. However, existing frame interpolation methods that rely on matching
and warping whole raster images are unsuitable for line inbetweening and often
produce blurring artifacts that damage the intricate line structures. To
preserve the precision and detail of the line drawings, we propose a new
approach, AnimeInbet, which geometrizes raster line drawings into graphs of
endpoints and reframes the inbetweening task as a graph fusion problem with
vertex repositioning. Our method can effectively capture the sparsity and
unique structure of line drawings while preserving the details during
inbetweening. This is made possible via our novel modules, i.e., vertex
geometric embedding, a vertex correspondence Transformer, an effective
mechanism for vertex repositioning and a visibility predictor. To train our
method, we introduce MixamoLine240, a new dataset of line drawings with ground
truth vectorization and matching labels. Our experiments demonstrate that
AnimeInbet synthesizes high-quality, clean, and complete intermediate line
drawings, outperforming existing methods quantitatively and qualitatively,
especially in cases with large motions. Data and code are available at
https://github.com/lisiyao21/AnimeInbet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siyao_L/0/1/0/all/0/1&quot;&gt;Li Siyao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_T/0/1/0/all/0/1&quot;&gt;Tianpei Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1&quot;&gt;Weiye Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Henghui Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1&quot;&gt;Chen Change Loy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16646">
<title>Improving Equivariance in State-of-the-Art Supervised Depth and Normal Predictors. (arXiv:2309.16646v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16646</link>
<description rdf:parseType="Literal">&lt;p&gt;Dense depth and surface normal predictors should possess the equivariant
property to cropping-and-resizing -- cropping the input image should result in
cropping the same output image. However, we find that state-of-the-art depth
and normal predictors, despite having strong performances, surprisingly do not
respect equivariance. The problem exists even when crop-and-resize data
augmentation is employed during training. To remedy this, we propose an
equivariant regularization technique, consisting of an averaging procedure and
a self-consistency loss, to explicitly promote cropping-and-resizing
equivariance in depth and normal networks. Our approach can be applied to both
CNN and Transformer architectures, does not incur extra cost during testing,
and notably improves the supervised and semi-supervised learning performance of
dense predictors on Taskonomy tasks. Finally, finetuning with our loss on
unlabeled images improves not only equivariance but also accuracy of
state-of-the-art depth and normal predictors when evaluated on NYU-v2. GitHub
link: https://github.com/mikuhatsune/equivariance
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yuanyi Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattad_A/0/1/0/all/0/1&quot;&gt;Anand Bhattad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1&quot;&gt;David Forsyth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16649">
<title>FLIP: Cross-domain Face Anti-spoofing with Language Guidance. (arXiv:2309.16649v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16649</link>
<description rdf:parseType="Literal">&lt;p&gt;Face anti-spoofing (FAS) or presentation attack detection is an essential
component of face recognition systems deployed in security-critical
applications. Existing FAS methods have poor generalizability to unseen spoof
types, camera sensors, and environmental conditions. Recently, vision
transformer (ViT) models have been shown to be effective for the FAS task due
to their ability to capture long-range dependencies among image patches.
However, adaptive modules or auxiliary loss functions are often required to
adapt pre-trained ViT weights learned on large-scale datasets such as ImageNet.
In this work, we first show that initializing ViTs with multimodal (e.g., CLIP)
pre-trained weights improves generalizability for the FAS task, which is in
line with the zero-shot transfer capabilities of vision-language pre-trained
(VLP) models. We then propose a novel approach for robust cross-domain FAS by
grounding visual representations with the help of natural language.
Specifically, we show that aligning the image representation with an ensemble
of class descriptions (based on natural language semantics) improves FAS
generalizability in low-data regimes. Finally, we propose a multimodal
contrastive learning strategy to boost feature generalization further and
bridge the gap between source and target domains. Extensive experiments on
three standard protocols demonstrate that our method significantly outperforms
the state-of-the-art methods, achieving better zero-shot transfer performance
than five-shot transfer of adaptive ViTs. Code:
https://github.com/koushiksrivats/FLIP
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivatsan_K/0/1/0/all/0/1&quot;&gt;Koushik Srivatsan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1&quot;&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nandakumar_K/0/1/0/all/0/1&quot;&gt;Karthik Nandakumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16650">
<title>ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning. (arXiv:2309.16650v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2309.16650</link>
<description rdf:parseType="Literal">&lt;p&gt;For robots to perform a wide variety of tasks, they require a 3D
representation of the world that is semantically rich, yet compact and
efficient for task-driven perception and planning. Recent approaches have
attempted to leverage features from large vision-language models to encode
semantics in 3D representations. However, these approaches tend to produce maps
with per-point feature vectors, which do not scale well in larger environments,
nor do they contain semantic spatial relationships between entities in the
environment, which are useful for downstream planning. In this work, we propose
ConceptGraphs, an open-vocabulary graph-structured representation for 3D
scenes. ConceptGraphs is built by leveraging 2D foundation models and fusing
their output to 3D by multi-view association. The resulting representations
generalize to novel semantic classes, without the need to collect large 3D
datasets or finetune models. We demonstrate the utility of this representation
through a number of downstream planning tasks that are specified through
abstract (language) prompts and require complex reasoning over spatial and
semantic concepts. (Project page: https://concept-graphs.github.io/ Explainer
video: https://youtu.be/mRhNkQwRYnc )
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1&quot;&gt;Qiao Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuwajerwala_A/0/1/0/all/0/1&quot;&gt;Alihusein Kuwajerwala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morin_S/0/1/0/all/0/1&quot;&gt;Sacha Morin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jatavallabhula_K/0/1/0/all/0/1&quot;&gt;Krishna Murthy Jatavallabhula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_B/0/1/0/all/0/1&quot;&gt;Bipasha Sen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Aditya Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivera_C/0/1/0/all/0/1&quot;&gt;Corban Rivera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1&quot;&gt;William Paul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellis_K/0/1/0/all/0/1&quot;&gt;Kirsty Ellis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1&quot;&gt;Rama Chellappa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1&quot;&gt;Chuang Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melo_C/0/1/0/all/0/1&quot;&gt;Celso Miguel de Melo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1&quot;&gt;Antonio Torralba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shkurti_F/0/1/0/all/0/1&quot;&gt;Florian Shkurti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paull_L/0/1/0/all/0/1&quot;&gt;Liam Paull&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16653">
<title>DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation. (arXiv:2309.16653v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16653</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in 3D content creation mostly leverage optimization-based 3D
generation via score distillation sampling (SDS). Though promising results have
been exhibited, these methods often suffer from slow per-sample optimization,
limiting their practical usage. In this paper, we propose DreamGaussian, a
novel 3D content generation framework that achieves both efficiency and quality
simultaneously. Our key insight is to design a generative 3D Gaussian Splatting
model with companioned mesh extraction and texture refinement in UV space. In
contrast to the occupancy pruning used in Neural Radiance Fields, we
demonstrate that the progressive densification of 3D Gaussians converges
significantly faster for 3D generative tasks. To further enhance the texture
quality and facilitate downstream applications, we introduce an efficient
algorithm to convert 3D Gaussians into textured meshes and apply a fine-tuning
stage to refine the details. Extensive experiments demonstrate the superior
efficiency and competitive generation quality of our proposed approach.
Notably, DreamGaussian produces high-quality textured meshes in just 2 minutes
from a single-view image, achieving approximately 10 times acceleration
compared to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiaxiang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jiawei Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1&quot;&gt;Gang Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16654">
<title>Novel Deep Learning Pipeline for Automatic Weapon Detection. (arXiv:2309.16654v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16654</link>
<description rdf:parseType="Literal">&lt;p&gt;Weapon and gun violence have recently become a pressing issue today. The
degree of these crimes and activities has risen to the point of being termed as
an epidemic. This prevalent misuse of weapons calls for an automatic system
that detects weapons in real-time. Real-time surveillance video is captured and
recorded in almost all public forums and places. These videos contain abundant
raw data which can be extracted and processed into meaningful information. This
paper proposes a novel pipeline consisting of an ensemble of convolutional
neural networks with distinct architectures. Each neural network is trained
with a unique mini-batch with little to no overlap in the training samples.
This paper will present several promising results using multiple datasets
associated with comparing the proposed architecture and state-of-the-art (SoA)
models. The proposed pipeline produced an average increase of 5% in accuracy,
specificity, and recall compared to the SoA systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sivakumar_H/0/1/0/all/0/1&quot;&gt;Haribharathi Sivakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+R_V/0/1/0/all/0/1&quot;&gt;Vijay Arvind.R&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+V_P/0/1/0/all/0/1&quot;&gt;Pawan Ragavendhar V&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balamurugan_G/0/1/0/all/0/1&quot;&gt;G.Balamurugan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16656">
<title>Visual In-Context Learning for Few-Shot Eczema Segmentation. (arXiv:2309.16656v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16656</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated diagnosis of eczema from digital camera images is crucial for
developing applications that allow patients to self-monitor their recovery. An
important component of this is the segmentation of eczema region from such
images. Current methods for eczema segmentation rely on deep neural networks
such as convolutional (CNN)-based U-Net or transformer-based Swin U-Net. While
effective, these methods require high volume of annotated data, which can be
difficult to obtain. Here, we investigate the capabilities of visual in-context
learning that can perform few-shot eczema segmentation with just a handful of
examples and without any need for retraining models. Specifically, we propose a
strategy for applying in-context learning for eczema segmentation with a
generalist vision model called SegGPT. When benchmarked on a dataset of
annotated eczema images, we show that SegGPT with just 2 representative example
images from the training dataset performs better (mIoU: 36.69) than a CNN U-Net
trained on 428 images (mIoU: 32.60). We also discover that using more number of
examples for SegGPT may in fact be harmful to its performance. Our result
highlights the importance of visual in-context learning in developing faster
and better solutions to skin imaging tasks. Our result also paves the way for
developing inclusive solutions that can cater to minorities in the demographics
who are typically heavily under-represented in the training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1&quot;&gt;Neelesh Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aran_O/0/1/0/all/0/1&quot;&gt;Oya Aran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1&quot;&gt;Venugopal Vasudevan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16661">
<title>SA2-Net: Scale-aware Attention Network for Microscopic Image Segmentation. (arXiv:2309.16661v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16661</link>
<description rdf:parseType="Literal">&lt;p&gt;Microscopic image segmentation is a challenging task, wherein the objective
is to assign semantic labels to each pixel in a given microscopic image. While
convolutional neural networks (CNNs) form the foundation of many existing
frameworks, they often struggle to explicitly capture long-range dependencies.
Although transformers were initially devised to address this issue using
self-attention, it has been proven that both local and global features are
crucial for addressing diverse challenges in microscopic images, including
variations in shape, size, appearance, and target region density. In this
paper, we introduce SA2-Net, an attention-guided method that leverages
multi-scale feature learning to effectively handle diverse structures within
microscopic images. Specifically, we propose scale-aware attention (SA2) module
designed to capture inherent variations in scales and shapes of microscopic
regions, such as cells, for accurate segmentation. This module incorporates
local attention at each level of multi-stage features, as well as global
attention across multiple resolutions. Furthermore, we address the issue of
blurred region boundaries (e.g., cell boundaries) by introducing a novel
upsampling strategy called the Adaptive Up-Attention (AuA) module. This module
enhances the discriminative ability for improved localization of microscopic
regions using an explicit attention mechanism. Extensive experiments on five
challenging datasets demonstrate the benefits of our SA2-Net model. Our source
code is publicly available at \url{https://github.com/mustansarfiaz/SA2-Net}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiaz_M/0/1/0/all/0/1&quot;&gt;Mustansar Fiaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heidari_M/0/1/0/all/0/1&quot;&gt;Moein Heidari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1&quot;&gt;Rao Muhammad Anwer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cholakkal_H/0/1/0/all/0/1&quot;&gt;Hisham Cholakkal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16662">
<title>Geodesic Regression Characterizes 3D Shape Changes in the Female Brain During Menstruation. (arXiv:2309.16662v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16662</link>
<description rdf:parseType="Literal">&lt;p&gt;Women are at higher risk of Alzheimer&apos;s and other neurological diseases after
menopause, and yet research connecting female brain health to sex hormone
fluctuations is limited. We seek to investigate this connection by developing
tools that quantify 3D shape changes that occur in the brain during sex hormone
fluctuations. Geodesic regression on the space of 3D discrete surfaces offers a
principled way to characterize the evolution of a brain&apos;s shape. However, in
its current form, this approach is too computationally expensive for practical
use. In this paper, we propose approximation schemes that accelerate geodesic
regression on shape spaces of 3D discrete surfaces. We also provide rules of
thumb for when each approximation can be used. We test our approach on
synthetic data to quantify the speed-accuracy trade-off of these approximations
and show that practitioners can expect very significant speed-up while only
sacrificing little accuracy. Finally, we apply the method to real brain shape
data and produce the first characterization of how the female hippocampus
changes shape during the menstrual cycle as a function of progesterone: a
characterization made (practically) possible by our approximation schemes. Our
work paves the way for comprehensive, practical shape analyses in the fields of
bio-medicine and computer vision. Our implementation is publicly available on
GitHub: https://github.com/bioshape-lab/my28brains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myers_A/0/1/0/all/0/1&quot;&gt;Adele Myers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_C/0/1/0/all/0/1&quot;&gt;Caitlin Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobs_E/0/1/0/all/0/1&quot;&gt;Emily Jacobs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miolane_N/0/1/0/all/0/1&quot;&gt;Nina Miolane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16668">
<title>RealFill: Reference-Driven Generation for Authentic Image Completion. (arXiv:2309.16668v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16668</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in generative imagery have brought forth outpainting and
inpainting models that can produce high-quality, plausible image content in
unknown regions, but the content these models hallucinate is necessarily
inauthentic, since the models lack sufficient context about the true scene. In
this work, we propose RealFill, a novel generative approach for image
completion that fills in missing regions of an image with the content that
should have been there. RealFill is a generative inpainting model that is
personalized using only a few reference images of a scene. These reference
images do not have to be aligned with the target image, and can be taken with
drastically varying viewpoints, lighting conditions, camera apertures, or image
styles. Once personalized, RealFill is able to complete a target image with
visually compelling contents that are faithful to the original scene. We
evaluate RealFill on a new image completion benchmark that covers a set of
diverse and challenging scenarios, and find that it outperforms existing
approaches by a large margin. See more results on our project page:
https://realfill.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Luming Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruiz_N/0/1/0/all/0/1&quot;&gt;Nataniel Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1&quot;&gt;Qinghao Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holynski_A/0/1/0/all/0/1&quot;&gt;Aleksander Holynski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1&quot;&gt;David E. Jacobs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1&quot;&gt;Bharath Hariharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pritch_Y/0/1/0/all/0/1&quot;&gt;Yael Pritch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wadhwa_N/0/1/0/all/0/1&quot;&gt;Neal Wadhwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1&quot;&gt;Kfir Aberman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubinstein_M/0/1/0/all/0/1&quot;&gt;Michael Rubinstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16669">
<title>Training a Large Video Model on a Single Machine in a Day. (arXiv:2309.16669v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16669</link>
<description rdf:parseType="Literal">&lt;p&gt;Videos are big, complex to pre-process, and slow to train on.
State-of-the-art large-scale video models are trained on clusters of 32 or more
GPUs for several days. As a consequence, academia largely ceded the training of
large video models to industry. In this paper, we show how to still train a
state-of-the-art video model on a single machine with eight consumer-grade GPUs
in a day. We identify three bottlenecks, IO, CPU, and GPU computation, and
optimize each. The result is a highly efficient video training pipeline. For
comparable architectures, our pipeline achieves higher accuracies with
$\frac{1}{8}$ of the computation compared to prior work. Code is available at
https://github.com/zhaoyue-zephyrus/AVION.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yue Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krahenbuhl_P/0/1/0/all/0/1&quot;&gt;Philipp Kr&amp;#xe4;henb&amp;#xfc;hl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16670">
<title>Decaf: Monocular Deformation Capture for Face and Hand Interactions. (arXiv:2309.16670v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16670</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing methods for 3D tracking from monocular RGB videos predominantly
consider articulated and rigid objects. Modelling dense non-rigid object
deformations in this setting remained largely unaddressed so far, although such
effects can improve the realism of the downstream applications such as AR/VR
and avatar communications. This is due to the severe ill-posedness of the
monocular view setting and the associated challenges. While it is possible to
naively track multiple non-rigid objects independently using 3D templates or
parametric 3D models, such an approach would suffer from multiple artefacts in
the resulting 3D estimates such as depth ambiguity, unnatural intra-object
collisions and missing or implausible deformations. Hence, this paper
introduces the first method that addresses the fundamental challenges depicted
above and that allows tracking human hands interacting with human faces in 3D
from single monocular RGB videos. We model hands as articulated objects
inducing non-rigid face deformations during an active interaction. Our method
relies on a new hand-face motion and interaction capture dataset with realistic
face deformations acquired with a markerless multi-view camera system. As a
pivotal step in its creation, we process the reconstructed raw 3D shapes with
position-based dynamics and an approach for non-uniform stiffness estimation of
the head tissues, which results in plausible annotations of the surface
deformations, hand-face contact regions and head-hand positions. At the core of
our neural approach are a variational auto-encoder supplying the hand-face
depth prior and modules that guide the 3D tracking by estimating the contacts
and the deformations. Our final 3D hand and face reconstructions are realistic
and more plausible compared to several baselines applicable in our setting,
both quantitatively and qualitatively.
https://vcai.mpi-inf.mpg.de/projects/Decaf
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shimada_S/0/1/0/all/0/1&quot;&gt;Soshi Shimada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1&quot;&gt;Vladislav Golyanik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1&quot;&gt;Patrick P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1&quot;&gt;Christian Theobalt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16671">
<title>Demystifying CLIP Data. (arXiv:2309.16671v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16671</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive Language-Image Pre-training (CLIP) is an approach that has
advanced research and applications in computer vision, fueling modern
recognition systems and generative models. We believe that the main ingredient
to the success of CLIP is its data and not the model architecture or
pre-training objective. However, CLIP only provides very limited information
about its data and how it has been collected, leading to works that aim to
reproduce CLIP&apos;s data by filtering with its model parameters. In this work, we
intend to reveal CLIP&apos;s data curation approach and in our pursuit of making it
open to the community introduce Metadata-Curated Language-Image Pre-training
(MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP&apos;s
concepts) and yields a balanced subset over the metadata distribution. Our
experimental study rigorously isolates the model and training settings,
concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M
image-text data pairs outperforms CLIP&apos;s data on multiple standard benchmarks.
In zero-shot ImageNet classification, MetaCLIP achieves 70.8% accuracy,
surpassing CLIP&apos;s 68.3% on ViT-B models. Scaling to 1B data, while maintaining
the same training budget, attains 72.4%. Our observations hold across various
model sizes, exemplified by ViT-H achieving 80.5%, without any
bells-and-whistles. Curation code and training data distribution on metadata is
made available at https://github.com/facebookresearch/MetaCLIP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Saining Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1&quot;&gt;Xiaoqing Ellen Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Po-Yao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Howes_R/0/1/0/all/0/1&quot;&gt;Russell Howes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1&quot;&gt;Vasu Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shang-Wen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1&quot;&gt;Gargi Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1&quot;&gt;Luke Zettlemoyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1&quot;&gt;Christoph Feichtenhofer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16672">
<title>Learning to Transform for Generalizable Instance-wise Invariance. (arXiv:2309.16672v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16672</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer vision research has long aimed to build systems that are robust to
spatial transformations found in natural data. Traditionally, this is done
using data augmentation or hard-coding invariances into the architecture.
However, too much or too little invariance can hurt, and the correct amount is
unknown a priori and dependent on the instance. Ideally, the appropriate
invariance would be learned from data and inferred at test-time.
&lt;/p&gt;
&lt;p&gt;We treat invariance as a prediction problem. Given any image, we use a
normalizing flow to predict a distribution over transformations and average the
predictions over them. Since this distribution only depends on the instance, we
can align instances before classifying them and generalize invariance across
classes. The same distribution can also be used to adapt to out-of-distribution
poses. This normalizing flow is trained end-to-end and can learn a much larger
range of transformations than Augerino and InstaAug. When used as data
augmentation, our method shows accuracy and robustness gains on CIFAR 10,
CIFAR10-LT, and TinyImageNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singhal_U/0/1/0/all/0/1&quot;&gt;Utkarsh Singhal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esteves_C/0/1/0/all/0/1&quot;&gt;Carlos Esteves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makadia_A/0/1/0/all/0/1&quot;&gt;Ameesh Makadia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Stella X. Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.09882">
<title>Hierarchical Attention-based Age Estimation and Bias Estimation. (arXiv:2103.09882v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2103.09882</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we propose a novel deep-learning approach for age estimation
based on face images. We first introduce a dual image augmentation-aggregation
approach based on attention. This allows the network to jointly utilize
multiple face image augmentations whose embeddings are aggregated by a
Transformer-Encoder. The resulting aggregated embedding is shown to better
encode the face image attributes. We then propose a probabilistic hierarchical
regression framework that combines a discrete probabilistic estimate of age
labels, with a corresponding ensemble of regressors. Each regressor is
particularly adapted and trained to refine the probabilistic estimate over a
range of ages. Our scheme is shown to outperform contemporary schemes and
provide a new state-of-the-art age estimation accuracy, when applied to the
MORPH II dataset for age estimation. Last, we introduce a bias analysis of
state-of-the-art age estimation results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hiba_S/0/1/0/all/0/1&quot;&gt;Shakediel Hiba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keller_Y/0/1/0/all/0/1&quot;&gt;Yosi Keller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.05856">
<title>Just Noticeable Difference Modeling for Face Recognition System. (arXiv:2209.05856v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.05856</link>
<description rdf:parseType="Literal">&lt;p&gt;High-quality face images are required to guarantee the stability and
reliability of automatic face recognition (FR) systems in surveillance and
security scenarios. However, a massive amount of face data is usually
compressed before being analyzed due to limitations on transmission or storage.
The compressed images may lose the powerful identity information, resulting in
the performance degradation of the FR system. Herein, we make the first attempt
to study just noticeable difference (JND) for the FR system, which can be
defined as the maximum distortion that the FR system cannot notice. More
specifically, we establish a JND dataset including 3530 original images and
137,670 compressed images generated by advanced reference encoding/decoding
software based on the Versatile Video Coding (VVC) standard (VTM-15.0).
Subsequently, we develop a novel JND prediction model to directly infer JND
images for the FR system. In particular, in order to maximum redundancy removal
without impairment of robust identity information, we apply the encoder with
multiple feature extraction and attention-based feature decomposition modules
to progressively decompose face features into two uncorrelated components,
i.e., identity and residual features, via self-supervised learning. Then, the
residual feature is fed into the decoder to generate the residual map. Finally,
the predicted JND map is obtained by subtracting the residual map from the
original image. Experimental results have demonstrated that the proposed model
achieves higher accuracy of JND map prediction compared with the
state-of-the-art JND models, and is capable of saving more bits while
maintaining the performance of the FR system compared with VTM-15.0.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1&quot;&gt;Zhangkai Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Baoliang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shurun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanli Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1&quot;&gt;Sam Kwong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.06984">
<title>QDTrack: Quasi-Dense Similarity Learning for Appearance-Only Multiple Object Tracking. (arXiv:2210.06984v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.06984</link>
<description rdf:parseType="Literal">&lt;p&gt;Similarity learning has been recognized as a crucial step for object
tracking. However, existing multiple object tracking methods only use sparse
ground truth matching as the training objective, while ignoring the majority of
the informative regions in images. In this paper, we present Quasi-Dense
Similarity Learning, which densely samples hundreds of object regions on a pair
of images for contrastive learning. We combine this similarity learning with
multiple existing object detectors to build Quasi-Dense Tracking (QDTrack),
which does not require displacement regression or motion priors. We find that
the resulting distinctive feature space admits a simple nearest neighbor search
at inference time for object association. In addition, we show that our
similarity learning scheme is not limited to video data, but can learn
effective instance similarity even from static input, enabling a competitive
tracking performance without training on videos or using tracking supervision.
We conduct extensive experiments on a wide variety of popular MOT benchmarks.
We find that, despite its simplicity, QDTrack rivals the performance of
state-of-the-art tracking methods on all benchmarks and sets a new
state-of-the-art on the large-scale BDD100K MOT benchmark, while introducing
negligible computational overhead to the detector.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_T/0/1/0/all/0/1&quot;&gt;Tobias Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Thomas E. Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1&quot;&gt;Jiangmiao Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Linlu Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haofeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fisher Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.08217">
<title>A Low-Shot Object Counting Network With Iterative Prototype Adaptation. (arXiv:2211.08217v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.08217</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider low-shot counting of arbitrary semantic categories in the image
using only few annotated exemplars (few-shot) or no exemplars (no-shot). The
standard few-shot pipeline follows extraction of appearance queries from
exemplars and matching them with image features to infer the object counts.
Existing methods extract queries by feature pooling which neglects the shape
information (e.g., size and aspect) and leads to a reduced object localization
accuracy and count estimates. We propose a Low-shot Object Counting network
with iterative prototype Adaptation (LOCA). Our main contribution is the new
object prototype extraction module, which iteratively fuses the exemplar shape
and appearance information with image features. The module is easily adapted to
zero-shot scenarios, enabling LOCA to cover the entire spectrum of low-shot
counting problems. LOCA outperforms all recent state-of-the-art methods on
FSC147 benchmark by 20-30% in RMSE on one-shot and few-shot and achieves
state-of-the-art on zero-shot scenarios, while demonstrating better
generalization capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Djukic_N/0/1/0/all/0/1&quot;&gt;Nikola Djukic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lukezic_A/0/1/0/all/0/1&quot;&gt;Alan Lukezic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zavrtanik_V/0/1/0/all/0/1&quot;&gt;Vitjan Zavrtanik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kristan_M/0/1/0/all/0/1&quot;&gt;Matej Kristan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.10754">
<title>HALSIE: Hybrid Approach to Learning Segmentation by Simultaneously Exploiting Image and Event Modalities. (arXiv:2211.10754v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.10754</link>
<description rdf:parseType="Literal">&lt;p&gt;Event cameras detect changes in per-pixel intensity to generate asynchronous
`event streams&apos;. They offer great potential for accurate semantic map retrieval
in real-time autonomous systems owing to their much higher temporal resolution
and high dynamic range (HDR) compared to conventional cameras. However,
existing implementations for event-based segmentation suffer from sub-optimal
performance since these temporally dense events only measure the varying
component of a visual signal, limiting their ability to encode dense spatial
context compared to frames. To address this issue, we propose a hybrid
end-to-end learning framework HALSIE, utilizing three key concepts to reduce
inference cost by up to $20\times$ versus prior art while retaining similar
performance: First, a simple and efficient cross-domain learning scheme to
extract complementary spatio-temporal embeddings from both frames and events.
Second, a specially designed dual-encoder scheme with Spiking Neural Network
(SNN) and Artificial Neural Network (ANN) branches to minimize latency while
retaining cross-domain feature aggregation. Third, a multi-scale cue mixer to
model rich representations of the fused embeddings. These qualities of HALSIE
allow for a very lightweight architecture achieving state-of-the-art
segmentation performance on DDD-17, MVSEC, and DSEC-Semantic datasets with up
to $33\times$ higher parameter efficiency and favorable inference cost (17.9mJ
per cycle). Our ablation study also brings new insights into effective design
choices that can prove beneficial for research across other vision tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1&quot;&gt;Shristi Das Biswas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kosta_A/0/1/0/all/0/1&quot;&gt;Adarsh Kosta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liyanagedera_C/0/1/0/all/0/1&quot;&gt;Chamika Liyanagedera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apolinario_M/0/1/0/all/0/1&quot;&gt;Marco Apolinario&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1&quot;&gt;Kaushik Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.09907">
<title>General Rotation Invariance Learning for Point Clouds via Weight-Feature Alignment. (arXiv:2302.09907v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.09907</link>
<description rdf:parseType="Literal">&lt;p&gt;Compared to 2D images, 3D point clouds are much more sensitive to rotations.
We expect the point features describing certain patterns to keep invariant to
the rotation transformation. There are many recent SOTA works dedicated to
rotation-invariant learning for 3D point clouds. However, current
rotation-invariant methods lack generalizability on the point clouds in the
open scenes due to the reliance on the global distribution, \ie the global
scene and backgrounds. Considering that the output activation is a function of
the pattern and its orientation, we need to eliminate the effect of the
orientation.In this paper, inspired by the idea that the network weights can be
considered a set of points distributed in the same 3D space as the input
points, we propose Weight-Feature Alignment (WFA) to construct a local
Invariant Reference Frame (IRF) via aligning the features with the principal
axes of the network weights. Our WFA algorithm provides a general solution for
the point clouds of all scenes. WFA ensures the model achieves the target that
the response activity is a necessary and sufficient condition of the pattern
matching degree. Practically, we perform experiments on the point clouds of
both single objects and open large-range scenes. The results suggest that our
method almost bridges the gap between rotation invariance learning and normal
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Liang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yibo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenxiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Binbin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1&quot;&gt;Deng Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaofei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1&quot;&gt;Ronghua Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.09976">
<title>Discouraging posterior collapse in hierarchical Variational Autoencoders using context. (arXiv:2302.09976v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.09976</link>
<description rdf:parseType="Literal">&lt;p&gt;Hierarchical Variational Autoencoders (VAEs) are among the most popular
likelihood-based generative models. There is a consensus that the top-down
hierarchical VAEs allow effective learning of deep latent structures and avoid
problems like posterior collapse. Here, we show that this is not necessarily
the case, and the problem of collapsing posteriors remains. To discourage this
issue, we propose a deep hierarchical VAE with a context on top. Specifically,
we use a Discrete Cosine Transform to obtain the last latent variable. In a
series of experiments, we observe that the proposed modification allows us to
achieve better utilization of the latent space and does not harm the model&apos;s
generative abilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuzina_A/0/1/0/all/0/1&quot;&gt;Anna Kuzina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomczak_J/0/1/0/all/0/1&quot;&gt;Jakub M. Tomczak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03037">
<title>EvCenterNet: Uncertainty Estimation for Object Detection using Evidential Learning. (arXiv:2303.03037v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03037</link>
<description rdf:parseType="Literal">&lt;p&gt;Uncertainty estimation is crucial in safety-critical settings such as
automated driving as it provides valuable information for several downstream
tasks including high-level decision making and path planning. In this work, we
propose EvCenterNet, a novel uncertainty-aware 2D object detection framework
using evidential learning to directly estimate both classification and
regression uncertainties. To employ evidential learning for object detection,
we devise a combination of evidential and focal loss functions for the sparse
heatmap inputs. We introduce class-balanced weighting for regression and
heatmap prediction to tackle the class imbalance encountered by evidential
learning. Moreover, we propose a learning scheme to actively utilize the
predicted heatmap uncertainties to improve the detection performance by
focusing on the most uncertain points. We train our model on the KITTI dataset
and evaluate it on challenging out-of-distribution datasets including BDD100K
and nuImages. Our experiments demonstrate that our approach improves the
precision and minimizes the execution time loss in relation to the base model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nallapareddy_M/0/1/0/all/0/1&quot;&gt;Monish R. Nallapareddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sirohi_K/0/1/0/all/0/1&quot;&gt;Kshitij Sirohi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+J%2E_P/0/1/0/all/0/1&quot;&gt;Paulo L. J. Drews-Jr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1&quot;&gt;Wolfram Burgard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Chih-Hong Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1&quot;&gt;Abhinav Valada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.04116">
<title>TrafficBots: Towards World Models for Autonomous Driving Simulation and Motion Prediction. (arXiv:2303.04116v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2303.04116</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-driven simulation has become a favorable way to train and test
autonomous driving algorithms. The idea of replacing the actual environment
with a learned simulator has also been explored in model-based reinforcement
learning in the context of world models. In this work, we show data-driven
traffic simulation can be formulated as a world model. We present TrafficBots,
a multi-agent policy built upon motion prediction and end-to-end driving, and
based on TrafficBots we obtain a world model tailored for the planning module
of autonomous vehicles. Existing data-driven traffic simulators are lacking
configurability and scalability. To generate configurable behaviors, for each
agent we introduce a destination as navigational information, and a
time-invariant latent personality that specifies the behavioral style. To
improve the scalability, we present a new scheme of positional encoding for
angles, allowing all agents to share the same vectorized context and the use of
an architecture based on dot-product attention. As a result, we can simulate
all traffic participants seen in dense urban scenarios. Experiments on the
Waymo open motion dataset show TrafficBots can simulate realistic multi-agent
behaviors and achieve good performance on the motion prediction task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhejun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1&quot;&gt;Alexander Liniger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1&quot;&gt;Dengxin Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fisher Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11003">
<title>Tubelet-Contrastive Self-Supervision for Video-Efficient Generalization. (arXiv:2303.11003v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11003</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a self-supervised method for learning motion-focused video
representations. Existing approaches minimize distances between temporally
augmented videos, which maintain high spatial similarity. We instead propose to
learn similarities between videos with identical local motion dynamics but an
otherwise different appearance. We do so by adding synthetic motion
trajectories to videos which we refer to as tubelets. By simulating different
tubelet motions and applying transformations, such as scaling and rotation, we
introduce motion patterns beyond what is present in the pretraining data. This
allows us to learn a video representation that is remarkably data efficient:
our approach maintains performance when using only 25\% of the pretraining
videos. Experiments on 10 diverse downstream settings demonstrate our
competitive performance and generalizability to new domains and fine-grained
actions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thoker_F/0/1/0/all/0/1&quot;&gt;Fida Mohammad Thoker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doughty_H/0/1/0/all/0/1&quot;&gt;Hazel Doughty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1&quot;&gt;Cees Snoek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16296">
<title>Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels. (arXiv:2303.16296v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16296</link>
<description rdf:parseType="Literal">&lt;p&gt;The soft Dice loss (SDL) has taken a pivotal role in numerous automated
segmentation pipelines in the medical imaging community. Over the last years,
some reasons behind its superior functioning have been uncovered and further
optimizations have been explored. However, there is currently no implementation
that supports its direct utilization in scenarios involving soft labels. Hence,
a synergy between the use of SDL and research leveraging the use of soft
labels, also in the context of model calibration, is still missing. In this
work, we introduce Dice semimetric losses (DMLs), which (i) are by design
identical to SDL in a standard setting with hard labels, but (ii) can be
employed in settings with soft labels. Our experiments on the public QUBIQ,
LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels
(e.g.\ averaging, label smoothing, and knowledge distillation) over hard labels
(e.g.\ majority voting and random selection). As a result, we obtain superior
Dice scores and model calibration, which supports the wider adoption of DMLs in
practice. The code is available at
\href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zifu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popordanoska_T/0/1/0/all/0/1&quot;&gt;Teodora Popordanoska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertels_J/0/1/0/all/0/1&quot;&gt;Jeroen Bertels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemmens_R/0/1/0/all/0/1&quot;&gt;Robin Lemmens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1&quot;&gt;Matthew B. Blaschko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12405">
<title>Synthesizing Stable Reduced-Order Visuomotor Policies for Nonlinear Systems via Sums-of-Squares Optimization. (arXiv:2304.12405v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12405</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a method for synthesizing dynamic, reduced-order output-feedback
polynomial control policies for control-affine nonlinear systems which
guarantees runtime stability to a goal state, when using visual observations
and a learned perception module in the feedback control loop. We leverage
Lyapunov analysis to formulate the problem of synthesizing such policies. This
problem is nonconvex in the policy parameters and the Lyapunov function that is
used to prove the stability of the policy. To solve this problem approximately,
we propose two approaches: the first solves a sequence of sum-of-squares
optimization problems to iteratively improve a policy which is provably-stable
by construction, while the second directly performs gradient-based optimization
on the parameters of the polynomial policy, and its closed-loop stability is
verified a posteriori. We extend our approach to provide stability guarantees
in the presence of observation noise, which realistically arises due to errors
in the learned perception module. We evaluate our approach on several
underactuated nonlinear systems, including pendula and quadrotors, showing that
our guarantees translate to empirical stability when controlling these systems
from images, while baseline approaches can fail to reliably stabilize the
system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_G/0/1/0/all/0/1&quot;&gt;Glen Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tedrake_R/0/1/0/all/0/1&quot;&gt;Russ Tedrake&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03701">
<title>LMEye: An Interactive Perception Network for Large Language Models. (arXiv:2305.03701v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03701</link>
<description rdf:parseType="Literal">&lt;p&gt;Training a Multimodal Large Language Model (MLLM) from scratch, like GPT-4,
is resource-intensive. Regarding Large Language Models (LLMs) as the core
processor for multimodal information, our paper introduces LMEye, a human-like
eye with a play-and-plug interactive perception network, designed to enable
dynamic interaction between LLMs and external vision information. Previous
methods incorporate visual information into LLMs with a simple visual mapping
network or Q-former from BLIP-2. Such networks project the image feature once
yet do not consider the interaction between the image and the human input
query. Hence, the obtained visual information without being connected to human
intention may be inadequate for LLMs to generate intention-following responses,
which we refer to as static visual information. LMEye addresses this issue by
allowing the LLM to request the desired visual information aligned with various
human instructions, which we term as the dynamic visual information
interaction. Specifically, LMEye consists of a simple visual mapping network to
provide the basic perception of an image for LLMs. It also contains additional
modules responsible for acquiring requests from LLMs, performing request-based
visual information interaction, and transmitting the resulting interacted
visual information to LLMs, respectively. In this way, LLMs act to understand
the human query, deliver the corresponding request to the request-based visual
information interaction module, and generate the response based on the
interleaved multimodal information. We evaluate LMEye through extensive
experiments on some multimodal benchmarks, demonstrating that it significantly
improves the zero-shot performance on various multimodal tasks compared to
previous methods, with less parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Baotian Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06141">
<title>Active Semantic Localization with Graph Neural Embedding. (arXiv:2305.06141v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06141</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic localization, i.e., robot self-localization with semantic image
modality, is critical in recently emerging embodied AI applications (e.g.,
point-goal navigation, object-goal navigation, vision language navigation) and
topological mapping applications (e.g., graph neural SLAM, ego-centric
topological map). However, most existing works on semantic localization focus
on passive vision tasks without viewpoint planning, or rely on additional rich
modalities (e.g., depth measurements). Thus, the problem is largely unsolved.
In this work, we explore a lightweight, entirely CPU-based, domain-adaptive
semantic localization framework, called graph neural localizer. Our approach is
inspired by two recently emerging technologies: (1) Scene graph, which combines
the viewpoint- and appearance- invariance of local and global features; (2)
Graph neural network, which enables direct learning/recognition of graph data
(i.e., non-vector data). Specifically, a graph convolutional neural network is
first trained as a scene graph classifier for passive vision, and then its
knowledge is transferred to a reinforcement-learning planner for active vision.
Experiments on two scenarios, self-supervised learning and unsupervised domain
adaptation, using a photo-realistic Habitat simulator validate the
effectiveness of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoshida_M/0/1/0/all/0/1&quot;&gt;Mitsuki Yoshida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanaka_K/0/1/0/all/0/1&quot;&gt;Kanji Tanaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamamoto_R/0/1/0/all/0/1&quot;&gt;Ryogo Yamamoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iwata_D/0/1/0/all/0/1&quot;&gt;Daiki Iwata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14093">
<title>Weakly Supervised 3D Open-vocabulary Segmentation. (arXiv:2305.14093v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14093</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-vocabulary segmentation of 3D scenes is a fundamental function of human
perception and thus a crucial objective in computer vision research. However,
this task is heavily impeded by the lack of large-scale and diverse 3D
open-vocabulary segmentation datasets for training robust and generalizable
models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation
models helps but it compromises the open-vocabulary feature as the 2D models
are mostly finetuned with close-vocabulary datasets. We tackle the challenges
in 3D open-vocabulary segmentation by exploiting pre-trained foundation models
CLIP and DINO in a weakly supervised manner. Specifically, given only the
open-vocabulary text descriptions of the objects in a scene, we distill the
open-vocabulary multimodal knowledge and object reasoning capability of CLIP
and DINO into a neural radiance field (NeRF), which effectively lifts 2D
features into view-consistent 3D segmentation. A notable aspect of our approach
is that it does not require any manual segmentation annotations for either the
foundation models or the distillation process. Extensive experiments show that
our method even outperforms fully supervised models trained with segmentation
annotations in certain scenes, suggesting that 3D open-vocabulary segmentation
can be effectively learned from 2D images and text-image pairs. Code is
available at \url{https://github.com/Kunhao-Liu/3D-OVS}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kunhao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1&quot;&gt;Fangneng Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiahui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Muyu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yingchen Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saddik_A/0/1/0/all/0/1&quot;&gt;Abdulmotaleb El Saddik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1&quot;&gt;Christian Theobalt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Shijian Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15883">
<title>RC-BEVFusion: A Plug-In Module for Radar-Camera Bird&apos;s Eye View Feature Fusion. (arXiv:2305.15883v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15883</link>
<description rdf:parseType="Literal">&lt;p&gt;Radars and cameras belong to the most frequently used sensors for advanced
driver assistance systems and automated driving research. However, there has
been surprisingly little research on radar-camera fusion with neural networks.
One of the reasons is a lack of large-scale automotive datasets with radar and
unmasked camera data, with the exception of the nuScenes dataset. Another
reason is the difficulty of effectively fusing the sparse radar point cloud on
the bird&apos;s eye view (BEV) plane with the dense images on the perspective plane.
The recent trend of camera-based 3D object detection using BEV features has
enabled a new type of fusion, which is better suited for radars. In this work,
we present RC-BEVFusion, a modular radar-camera fusion network on the BEV
plane. We propose BEVFeatureNet, a novel radar encoder branch, and show that it
can be incorporated into several state-of-the-art camera-based architectures.
We show significant performance gains of up to 28% increase in the nuScenes
detection score, which is an important step in radar-camera fusion research.
Without tuning our model for the nuScenes benchmark, we achieve the best result
among all published methods in the radar-camera fusion category.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stacker_L/0/1/0/all/0/1&quot;&gt;Lukas St&amp;#xe4;cker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Shashank Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heidenreich_P/0/1/0/all/0/1&quot;&gt;Philipp Heidenreich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rambach_J/0/1/0/all/0/1&quot;&gt;Jason Rambach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1&quot;&gt;Didier Stricker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18135">
<title>Alignment-free HDR Deghosting with Semantics Consistent Transformer. (arXiv:2305.18135v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18135</link>
<description rdf:parseType="Literal">&lt;p&gt;High dynamic range (HDR) imaging aims to retrieve information from multiple
low-dynamic range inputs to generate realistic output. The essence is to
leverage the contextual information, including both dynamic and static
semantics, for better image generation. Existing methods often focus on the
spatial misalignment across input frames caused by the foreground and/or camera
motion. However, there is no research on jointly leveraging the dynamic and
static context in a simultaneous manner. To delve into this problem, we propose
a novel alignment-free network with a Semantics Consistent Transformer (SCTNet)
with both spatial and channel attention modules in the network. The spatial
attention aims to deal with the intra-image correlation to model the dynamic
motion, while the channel attention enables the inter-image intertwining to
enhance the semantic consistency across frames. Aside from this, we introduce a
novel realistic HDR dataset with more variations in foreground objects,
environmental factors, and larger motions. Extensive comparisons on both
conventional datasets and ours validate the effectiveness of our method,
achieving the best trade-off on the performance and the computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tel_S/0/1/0/all/0/1&quot;&gt;Steven Tel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zongwei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yulun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heyrman_B/0/1/0/all/0/1&quot;&gt;Barth&amp;#xe9;l&amp;#xe9;my Heyrman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demonceaux_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Demonceaux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1&quot;&gt;Radu Timofte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ginhac_D/0/1/0/all/0/1&quot;&gt;Dominique Ginhac&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00914">
<title>Conditioning Diffusion Models via Attributes and Semantic Masks for Face Generation. (arXiv:2306.00914v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00914</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models have shown impressive results in generating realistic
images of faces. GANs managed to generate high-quality, high-fidelity images
when conditioned on semantic masks, but they still lack the ability to
diversify their output. Diffusion models partially solve this problem and are
able to generate diverse samples given the same condition. In this paper, we
propose a multi-conditioning approach for diffusion models via cross-attention
exploiting both attributes and semantic masks to generate high-quality and
controllable face images. We also studied the impact of applying
perceptual-focused loss weighting into the latent space instead of the pixel
space. Our method extends the previous approaches by introducing conditioning
on more than one set of features, guaranteeing a more fine-grained control over
the generated face images. We evaluate our approach on the CelebA-HQ dataset,
and we show that it can generate realistic and diverse samples while allowing
for fine-grained control over multiple attributes and semantic regions.
Additionally, we perform an ablation study to evaluate the impact of different
conditioning strategies on the quality and diversity of the generated images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giambi_N/0/1/0/all/0/1&quot;&gt;Nico Giambi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lisanti_G/0/1/0/all/0/1&quot;&gt;Giuseppe Lisanti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05671">
<title>Topology-Aware Uncertainty for Image Segmentation. (arXiv:2306.05671v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05671</link>
<description rdf:parseType="Literal">&lt;p&gt;Segmentation of curvilinear structures such as vasculature and road networks
is challenging due to relatively weak signals and complex geometry/topology. To
facilitate and accelerate large scale annotation, one has to adopt
semi-automatic approaches such as proofreading by experts. In this work, we
focus on uncertainty estimation for such tasks, so that highly uncertain, and
thus error-prone structures can be identified for human annotators to verify.
Unlike most existing works, which provide pixel-wise uncertainty maps, we
stipulate it is crucial to estimate uncertainty in the units of topological
structures, e.g., small pieces of connections and branches. To achieve this, we
leverage tools from topological data analysis, specifically discrete Morse
theory (DMT), to first capture the structures, and then reason about their
uncertainties. To model the uncertainty, we (1) propose a joint prediction
model that estimates the uncertainty of a structure while taking the
neighboring structures into consideration (inter-structural uncertainty); (2)
propose a novel Probabilistic DMT to model the inherent uncertainty within each
structure (intra-structural uncertainty) by sampling its representations via a
perturb-and-walk scheme. On various 2D and 3D datasets, our method produces
better structure-wise uncertainty maps compared to existing works. Code
available at https://github.com/Saumya-Gupta-26/struct-uncertainty
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Saumya Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yikai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaoling Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasanna_P/0/1/0/all/0/1&quot;&gt;Prateek Prasanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08836">
<title>Probabilistic-based Feature Embedding of 4-D Light Fields for Compressive Imaging and Denoising. (arXiv:2306.08836v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08836</link>
<description rdf:parseType="Literal">&lt;p&gt;The high-dimensional nature of the 4-D light field (LF) poses great
challenges in achieving efficient and effective feature embedding, that
severely impacts the performance of downstream tasks. To tackle this crucial
issue, in contrast to existing methods with empirically-designed architectures,
we propose a probabilistic-based feature embedding (PFE), which learns a
feature embedding architecture by assembling various low-dimensional
convolution patterns in a probability space for fully capturing spatial-angular
information. Building upon the proposed PFE, we then leverage the intrinsic
linear imaging model of the coded aperture camera to construct a
cycle-consistent 4-D LF reconstruction network from coded measurements.
Moreover, we incorporate PFE into an iterative optimization framework for 4-D
LF denoising. Our extensive experiments demonstrate the significant superiority
of our methods on both real-world and synthetic 4-D LF images, both
quantitatively and qualitatively, when compared with state-of-the-art methods.
The source code will be publicly available at
https://github.com/lyuxianqiang/LFCA-CR-NET.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lyu_X/0/1/0/all/0/1&quot;&gt;Xianqiang Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Junhui Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09304">
<title>Radars for Autonomous Driving: A Review of Deep Learning Methods and Challenges. (arXiv:2306.09304v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09304</link>
<description rdf:parseType="Literal">&lt;p&gt;Radar is a key component of the suite of perception sensors used for safe and
reliable navigation of autonomous vehicles. Its unique capabilities include
high-resolution velocity imaging, detection of agents in occlusion and over
long ranges, and robust performance in adverse weather conditions. However, the
usage of radar data presents some challenges: it is characterized by low
resolution, sparsity, clutter, high uncertainty, and lack of good datasets.
These challenges have limited radar deep learning research. As a result,
current radar models are often influenced by lidar and vision models, which are
focused on optical features that are relatively weak in radar data, thus
resulting in under-utilization of radar&apos;s capabilities and diminishing its
contribution to autonomous perception. This review seeks to encourage further
deep learning research on autonomous radar data by 1) identifying key research
themes, and 2) offering a comprehensive overview of current opportunities and
challenges in the field. Topics covered include early and late fusion,
occupancy flow estimation, uncertainty modeling, and multipath detection. The
paper also discusses radar fundamentals and data representation, presents a
curated list of recent radar datasets, and reviews state-of-the-art lidar and
vision models relevant for radar research. For a summary of the paper and more
results, visit the website: autonomous-radars.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastav_A/0/1/0/all/0/1&quot;&gt;Arvind Srivastav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandal_S/0/1/0/all/0/1&quot;&gt;Soumyajit Mandal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02245">
<title>Set Learning for Accurate and Calibrated Models. (arXiv:2307.02245v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02245</link>
<description rdf:parseType="Literal">&lt;p&gt;Model overconfidence and poor calibration are common in machine learning and
difficult to account for when applying standard empirical risk minimization. In
this work, we propose a novel method to alleviate these problems that we call
odd-$k$-out learning (OKO), which minimizes the cross-entropy error for sets
rather than for single examples. This naturally allows the model to capture
correlations across data examples and achieves both better accuracy and
calibration, especially in limited training data and class-imbalanced regimes.
Perhaps surprisingly, OKO often yields better calibration even when training
with hard labels and dropping any additional calibration parameter tuning, such
as temperature scaling. We provide theoretical justification, establishing that
OKO naturally yields better calibration, and provide extensive experimental
analyses that corroborate our theoretical findings. We emphasize that OKO is a
general framework that can be easily adapted to many settings and the trained
model can be applied to single examples at inference time, without introducing
significant run-time overhead or architecture changes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muttenthaler_L/0/1/0/all/0/1&quot;&gt;Lukas Muttenthaler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vandermeulen_R/0/1/0/all/0/1&quot;&gt;Robert A. Vandermeulen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiuyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1&quot;&gt;Thomas Unterthiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1&quot;&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02347">
<title>Detecting Images Generated by Deep Diffusion Models using their Local Intrinsic Dimensionality. (arXiv:2307.02347v7 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02347</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models recently have been successfully applied for the visual
synthesis of strikingly realistic appearing images. This raises strong concerns
about their potential for malicious purposes. In this paper, we propose using
the lightweight multi Local Intrinsic Dimensionality (multiLID), which has been
originally developed in context of the detection of adversarial examples, for
the automatic detection of synthetic images and the identification of the
according generator networks. In contrast to many existing detection
approaches, which often only work for GAN-generated images, the proposed method
provides close to perfect detection results in many realistic use cases.
Extensive experiments on known and newly created datasets demonstrate that the
proposed multiLID approach exhibits superiority in diffusion detection and
model identification. Since the empirical evaluations of recent publications on
the detection of generated images are often mainly focused on the
&quot;LSUN-Bedroom&quot; dataset, we further establish a comprehensive benchmark for the
detection of diffusion-generated images, including samples from several
diffusion models with different image sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorenz_P/0/1/0/all/0/1&quot;&gt;Peter Lorenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durall_R/0/1/0/all/0/1&quot;&gt;Ricard Durall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1&quot;&gt;Janis Keuper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09283">
<title>RepViT: Revisiting Mobile CNN From ViT Perspective. (arXiv:2307.09283v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09283</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, lightweight Vision Transformers (ViTs) demonstrate superior
performance and lower latency compared with lightweight Convolutional Neural
Networks (CNNs) on resource-constrained mobile devices. This improvement is
usually attributed to the multi-head self-attention module, which enables the
model to learn global representations. However, the architectural disparities
between lightweight ViTs and lightweight CNNs have not been adequately
examined. In this study, we revisit the efficient design of lightweight CNNs
and emphasize their potential for mobile devices. We incrementally enhance the
mobile-friendliness of a standard lightweight CNN, specifically MobileNetV3, by
integrating the efficient architectural choices of lightweight ViTs. This ends
up with a new family of pure lightweight CNNs, namely RepViT. Extensive
experiments show that RepViT outperforms existing state-of-the-art lightweight
ViTs and exhibits favorable latency in various vision tasks. On ImageNet,
RepViT achieves over 80\% top-1 accuracy with 1ms latency on an iPhone 12,
which is the first time for a lightweight model, to the best of our knowledge.
Our largest model, RepViT-M2.3, obtains 83.7\% accuracy with only 2.3ms
latency. The code and trained models are available at
\url{https://github.com/jameslahm/RepViT}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Ao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zijia Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jungong Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1&quot;&gt;Guiguang Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12499">
<title>AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models. (arXiv:2307.12499v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12499</link>
<description rdf:parseType="Literal">&lt;p&gt;Unrestricted adversarial attacks present a serious threat to deep learning
models and adversarial defense techniques. They pose severe security problems
for deep learning applications because they can effectively bypass defense
mechanisms. However, previous attack methods often utilize Generative
Adversarial Networks (GANs), which are not theoretically provable and thus
generate unrealistic examples by incorporating adversarial objectives,
especially for large-scale datasets like ImageNet. In this paper, we propose a
new method, called AdvDiff, to generate unrestricted adversarial examples with
diffusion models. We design two novel adversarial guidance techniques to
conduct adversarial sampling in the reverse generation process of diffusion
models. These two techniques are effective and stable to generate high-quality,
realistic adversarial examples by integrating gradients of the target
classifier interpretably. Experimental results on MNIST and ImageNet datasets
demonstrate that AdvDiff is effective to generate unrestricted adversarial
examples, which outperforms GAN-based methods in terms of attack performance
and generation quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xuelong Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Kaisheng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1&quot;&gt;Bin Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15506">
<title>Improving Image Quality of Sparse-view Lung Cancer CT Images with a Convolutional Neural Network. (arXiv:2307.15506v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15506</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: To improve the image quality of sparse-view computed tomography (CT)
images with a U-Net for lung cancer detection and to determine the best
trade-off between number of views, image quality, and diagnostic confidence.
&lt;/p&gt;
&lt;p&gt;Methods: CT images from 41 subjects (34 with lung cancer, seven healthy) were
retrospectively selected (01.2016-12.2018) and forward projected onto 2048-view
sinograms. Six corresponding sparse-view CT data subsets at varying levels of
undersampling were reconstructed from sinograms using filtered backprojection
with 16, 32, 64, 128, 256, and 512 views, respectively. A dual-frame U-Net was
trained and evaluated for each subsampling level on 8,658 images from 22
diseased subjects. A representative image per scan was selected from 19
subjects (12 diseased, seven healthy) for a single-blinded reader study. The
selected slices, for all levels of subsampling, with and without
post-processing by the U-Net model, were presented to three readers. Image
quality and diagnostic confidence were ranked using pre-defined scales.
Subjective nodule segmentation was evaluated utilizing sensitivity (Se) and
Dice Similarity Coefficient (DSC) with 95% confidence intervals (CI).
&lt;/p&gt;
&lt;p&gt;Results: The 64-projection sparse-view images resulted in Se = 0.89 and DSC =
0.81 [0.75,0.86] while their counterparts, post-processed with the U-Net, had
improved metrics (Se = 0.94, DSC = 0.85 [0.82,0.87]). Fewer views lead to
insufficient quality for diagnostic purposes. For increased views, no
substantial discrepancies were noted between the sparse-view and post-processed
images.
&lt;/p&gt;
&lt;p&gt;Conclusion: Projection views can be reduced from 2048 to 64 while maintaining
image quality and the confidence of the radiologists on a satisfactory level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ries_A/0/1/0/all/0/1&quot;&gt;Annika Ries&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dorosti_T/0/1/0/all/0/1&quot;&gt;Tina Dorosti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thalhammer_J/0/1/0/all/0/1&quot;&gt;Johannes Thalhammer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sasse_D/0/1/0/all/0/1&quot;&gt;Daniel Sasse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sauter_A/0/1/0/all/0/1&quot;&gt;Andreas Sauter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meurer_F/0/1/0/all/0/1&quot;&gt;Felix Meurer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benne_A/0/1/0/all/0/1&quot;&gt;Ashley Benne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lasser_T/0/1/0/all/0/1&quot;&gt;Tobias Lasser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfeiffer_F/0/1/0/all/0/1&quot;&gt;Franz Pfeiffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaff_F/0/1/0/all/0/1&quot;&gt;Florian Schaff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfeiffer_D/0/1/0/all/0/1&quot;&gt;Daniela Pfeiffer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15567">
<title>Panoptic Scene Graph Generation with Semantics-prototype Learning. (arXiv:2307.15567v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15567</link>
<description rdf:parseType="Literal">&lt;p&gt;Panoptic Scene Graph Generation (PSG) parses objects and predicts their
relationships (predicate) to connect human language and visual scenes. However,
different language preferences of annotators and semantic overlaps between
predicates lead to biased predicate annotations in the dataset, i.e. different
predicates for same object pairs. Biased predicate annotations make PSG models
struggle in constructing a clear decision plane among predicates, which greatly
hinders the real application of PSG models. To address the intrinsic bias
above, we propose a novel framework named ADTrans to adaptively transfer biased
predicate annotations to informative and unified ones. To promise consistency
and accuracy during the transfer process, we propose to measure the invariance
of representations in each predicate class, and learn unbiased prototypes of
predicates with different intensities. Meanwhile, we continuously measure the
distribution changes between each presentation and its prototype, and
constantly screen potential biased data. Finally, with the unbiased
predicate-prototype representation embedding space, biased annotations are
easily identified. Experiments show that ADTrans significantly improves the
performance of benchmark models, achieving a new state-of-the-art performance,
and shows great generalization and effectiveness on multiple datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1&quot;&gt;Wei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yiming Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mengze Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;You Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1&quot;&gt;Lina Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1&quot;&gt;Roger Zimmermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16262">
<title>An objective validation of polyp and instrument segmentation methods in colonoscopy through Medico 2020 polyp segmentation and MedAI 2021 transparency challenges. (arXiv:2307.16262v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16262</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic analysis of colonoscopy images has been an active field of research
motivated by the importance of early detection of precancerous polyps. However,
detecting polyps during the live examination can be challenging due to various
factors such as variation of skills and experience among the endoscopists, lack
of attentiveness, and fatigue leading to a high polyp miss-rate. Deep learning
has emerged as a promising solution to this challenge as it can assist
endoscopists in detecting and classifying overlooked polyps and abnormalities
in real time. In addition to the algorithm&apos;s accuracy, transparency and
interpretability are crucial to explaining the whys and hows of the algorithm&apos;s
prediction. Further, most algorithms are developed in private data, closed
source, or proprietary software, and methods lack reproducibility. Therefore,
to promote the development of efficient and transparent methods, we have
organized the &quot;Medico automatic polyp segmentation (Medico 2020)&quot; and &quot;MedAI:
Transparency in Medical Image Segmentation (MedAI 2021)&quot; competitions. We
present a comprehensive summary and analyze each contribution, highlight the
strength of the best-performing methods, and discuss the possibility of
clinical translations of such methods into the clinic. For the transparency
task, a multi-disciplinary team, including expert gastroenterologists, accessed
each submission and evaluated the team based on open-source practices, failure
case analysis, ablation studies, usability and understandability of evaluations
to gain a deeper understanding of the models&apos; credibility for clinical
deployment. Through the comprehensive analysis of the challenge, we not only
highlight the advancements in polyp and surgical instrument segmentation but
also encourage qualitative evaluation for building more transparent and
understandable AI-based colonoscopy systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jha_D/0/1/0/all/0/1&quot;&gt;Debesh Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sharma_V/0/1/0/all/0/1&quot;&gt;Vanshali Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Banik_D/0/1/0/all/0/1&quot;&gt;Debapriya Banik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhattacharya_D/0/1/0/all/0/1&quot;&gt;Debayan Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Roy_K/0/1/0/all/0/1&quot;&gt;Kaushiki Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hicks_S/0/1/0/all/0/1&quot;&gt;Steven A. Hicks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tomar_N/0/1/0/all/0/1&quot;&gt;Nikhil Kumar Tomar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thambawita_V/0/1/0/all/0/1&quot;&gt;Vajira Thambawita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Krenzer_A/0/1/0/all/0/1&quot;&gt;Adrian Krenzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ji_G/0/1/0/all/0/1&quot;&gt;Ge-Peng Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Poudel_S/0/1/0/all/0/1&quot;&gt;Sahadev Poudel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Batchkala_G/0/1/0/all/0/1&quot;&gt;George Batchkala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alam_S/0/1/0/all/0/1&quot;&gt;Saruar Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ahmed_A/0/1/0/all/0/1&quot;&gt;Awadelrahman M. A. Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Trinh_Q/0/1/0/all/0/1&quot;&gt;Quoc-Huy Trinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_Z/0/1/0/all/0/1&quot;&gt;Zeshan Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tien-Phat Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shrestha_S/0/1/0/all/0/1&quot;&gt;Shruti Shrestha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nathan_S/0/1/0/all/0/1&quot;&gt;Sabari Nathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gwak_J/0/1/0/all/0/1&quot;&gt;Jeonghwan Gwak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jha_R/0/1/0/all/0/1&quot;&gt;Ritika K. Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schlaefer_A/0/1/0/all/0/1&quot;&gt;Alexander Schlaefer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhattacharjee_D/0/1/0/all/0/1&quot;&gt;Debotosh Bhattacharjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhuyan_M/0/1/0/all/0/1&quot;&gt;M.K. Bhuyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Das_P/0/1/0/all/0/1&quot;&gt;Pradip K. Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fan_D/0/1/0/all/0/1&quot;&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Parsa_S/0/1/0/all/0/1&quot;&gt;Sravanthi Parsa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1&quot;&gt;Sharib Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1&quot;&gt;Michael A. Riegler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1&quot;&gt;P&amp;#xe5;l Halvorsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lange_T/0/1/0/all/0/1&quot;&gt;Thomas De Lange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bagci_U/0/1/0/all/0/1&quot;&gt;Ulas Bagci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01045">
<title>Dynamic Token Pruning in Plain Vision Transformers for Semantic Segmentation. (arXiv:2308.01045v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01045</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers have achieved leading performance on various visual tasks
yet still suffer from high computational complexity. The situation deteriorates
in dense prediction tasks like semantic segmentation, as high-resolution inputs
and outputs usually imply more tokens involved in computations. Directly
removing the less attentive tokens has been discussed for the image
classification task but can not be extended to semantic segmentation since a
dense prediction is required for every patch. To this end, this work introduces
a Dynamic Token Pruning (DToP) method based on the early exit of tokens for
semantic segmentation. Motivated by the coarse-to-fine segmentation process by
humans, we naturally split the widely adopted auxiliary-loss-based network
architecture into several stages, where each auxiliary block grades every
token&apos;s difficulty level. We can finalize the prediction of easy tokens in
advance without completing the entire forward pass. Moreover, we keep $k$
highest confidence tokens for each semantic category to uphold the
representative context information. Thus, computational complexity will change
with the difficulty of the input, akin to the way humans do segmentation.
Experiments suggest that the proposed DToP architecture reduces on average
$20\% - 35\%$ of computational cost for current semantic segmentation methods
based on plain vision transformers without accuracy degradation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1&quot;&gt;Quan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bowen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiajun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fagui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yifan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06595">
<title>VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use. (arXiv:2308.06595v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06595</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for
evaluation of instruction-following vision-language models for real-world use.
Our starting point is curating 70 &apos;instruction families&apos; that we envision
instruction tuned vision-language models should be able to address. Extending
beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to
game playing and creative generation. Following curation, our dataset comprises
592 test queries, each with a human-authored instruction-conditioned caption.
These descriptions surface instruction-specific factors, e.g., for an
instruction asking about the accessibility of a storefront for wheelchair
users, the instruction-conditioned caption describes ramps/potential obstacles.
These descriptions enable 1) collecting human-verified reference outputs for
each instance; and 2) automatic evaluation of candidate multimodal generations
using a text-only LLM, aligning with human judgment. We quantify quality gaps
between models and references using both human and automatic evaluations; e.g.,
the top-performing instruction-following model wins against the GPT-4 reference
in just 27% of the comparison. VisIT-Bench is dynamic to participate,
practitioners simply submit their model&apos;s response on the project website;
Data, code and leaderboard is available at visit-bench.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1&quot;&gt;Hritik Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1&quot;&gt;Jack Hessel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1&quot;&gt;Rulin Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wanrong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awadalla_A/0/1/0/all/0/1&quot;&gt;Anas Awadalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1&quot;&gt;Josh Gardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1&quot;&gt;Rohan Taori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schimdt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schimdt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08543">
<title>InsightMapper: A Closer Look at Inner-instance Information for Vectorized High-Definition Mapping. (arXiv:2308.08543v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08543</link>
<description rdf:parseType="Literal">&lt;p&gt;Vectorized high-definition (HD) maps contain detailed information about
surrounding road elements, which are crucial for various downstream tasks in
modern autonomous driving vehicles, such as vehicle planning and control.
Recent works have attempted to directly detect the vectorized HD map as a point
set prediction task, resulting in significant improvements in detection
performance. However, these approaches fail to analyze and exploit the
inner-instance correlations between predicted points, impeding further
advancements. To address these challenges, we investigate the utilization of
inner-$\textbf{INS}$tance information for vectorized h$\textbf{IGH}$-definition
mapping through $\textbf{T}$ransformers and introduce InsightMapper. This paper
presents three novel designs within InsightMapper that leverage inner-instance
information in distinct ways, including hybrid query generation, inner-instance
query fusion, and inner-instance feature aggregation. Comparative experiments
are conducted on the NuScenes dataset, showcasing the superiority of our
proposed method. InsightMapper surpasses previous state-of-the-art (SOTA)
methods by 5.78 mAP and 5.12 TOPO, which assess topology correctness.
Simultaneously, InsightMapper maintains high efficiency during both training
and inference phases, resulting in remarkable comprehensive performance. The
project page for this work is available at
https://tonyxuqaq.github.io/InsightMapper/ .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhenhua Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1&quot;&gt;Kenneth K.Y. Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hengshuang Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01340">
<title>MDSC: Towards Evaluating the Style Consistency Between Music and Dance. (arXiv:2309.01340v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01340</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose MDSC(Music-Dance-Style Consistency), the first evaluation metric
which assesses to what degree the dance moves and music match. Existing metrics
can only evaluate the fidelity and diversity of motion and the degree of
rhythmic matching between music and motion. MDSC measures how stylistically
correlated the generated dance motion sequences and the conditioning music
sequences are. We found that directly measuring the embedding distance between
motion and music is not an optimal solution. We instead tackle this through
modelling it as a clustering problem. Specifically, 1) we pre-train a music
encoder and a motion encoder, then 2) we learn to map and align the motion and
music embedding in joint space by jointly minimizing the intra-cluster distance
and maximizing the inter-cluster distance, and 3) for evaluation purpose, we
encode the dance moves into embedding and measure the intra-cluster and
inter-cluster distances, as well as the ratio between them. We evaluate our
metric on the results of several music-conditioned motion generation methods,
combined with user study, we found that our proposed metric is a robust
evaluation metric in measuring the music-dance style correlation. The code is
available at: https://github.com/zixiangzhou916/MDSC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zixiang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05832">
<title>Instance-Agnostic Geometry and Contact Dynamics Learning. (arXiv:2309.05832v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05832</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents an instance-agnostic learning framework that fuses vision
with dynamics to simultaneously learn shape, pose trajectories, and physical
properties via the use of geometry as a shared representation. Unlike many
contact learning approaches that assume motion capture input and a known shape
prior for the collision model, our proposed framework learns an object&apos;s
geometric and dynamic properties from RGBD video, without requiring either
category-level or instance-level shape priors. We integrate a vision system,
BundleSDF, with a dynamics system, ContactNets, and propose a cyclic training
pipeline to use the output from the dynamics module to refine the poses and the
geometry from the vision module, using perspective reprojection. Experiments
demonstrate our framework&apos;s ability to learn the geometry and dynamics of rigid
and convex objects and improve upon the current tracking framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Mengti Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bowen Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bianchini_B/0/1/0/all/0/1&quot;&gt;Bibit Bianchini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_C/0/1/0/all/0/1&quot;&gt;Camillo Jose Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Posa_M/0/1/0/all/0/1&quot;&gt;Michael Posa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06129">
<title>LEyes: A Lightweight Framework for Deep Learning-Based Eye Tracking using Synthetic Eye Images. (arXiv:2309.06129v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06129</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has bolstered gaze estimation techniques, but real-world
deployment has been impeded by inadequate training datasets. This problem is
exacerbated by both hardware-induced variations in eye images and inherent
biological differences across the recorded participants, leading to both
feature and pixel-level variance that hinders the generalizability of models
trained on specific datasets. While synthetic datasets can be a solution, their
creation is both time and resource-intensive. To address this problem, we
present a framework called Light Eyes or &quot;LEyes&quot; which, unlike conventional
photorealistic methods, only models key image features required for video-based
eye tracking using simple light distributions. LEyes facilitates easy
configuration for training neural networks across diverse gaze-estimation
tasks. We demonstrate that models trained using LEyes are consistently on-par
or outperform other state-of-the-art algorithms in terms of pupil and CR
localization across well-known datasets. In addition, a LEyes trained model
outperforms the industry standard eye tracker using significantly more
cost-effective hardware. Going forward, we are confident that LEyes will
revolutionize synthetic data generation for gaze estimation models, and lead to
significant improvements of the next generation video-based eye trackers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byrne_S/0/1/0/all/0/1&quot;&gt;Sean Anthony Byrne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maquiling_V/0/1/0/all/0/1&quot;&gt;Virmarie Maquiling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nystrom_M/0/1/0/all/0/1&quot;&gt;Marcus Nystr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1&quot;&gt;Enkelejda Kasneci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niehorster_D/0/1/0/all/0/1&quot;&gt;Diederick C. Niehorster&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06612">
<title>Harmonic-NAS: Hardware-Aware Multimodal Neural Architecture Search on Resource-constrained Devices. (arXiv:2309.06612v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06612</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent surge of interest surrounding Multimodal Neural Networks (MM-NN)
is attributed to their ability to effectively process and integrate multiscale
information from diverse data sources. MM-NNs extract and fuse features from
multiple modalities using adequate unimodal backbones and specific fusion
networks. Although this helps strengthen the multimodal information
representation, designing such networks is labor-intensive. It requires tuning
the architectural parameters of the unimodal backbones, choosing the fusing
point, and selecting the operations for fusion. Furthermore, multimodality AI
is emerging as a cutting-edge option in Internet of Things (IoT) systems where
inference latency and energy consumption are critical metrics in addition to
accuracy. In this paper, we propose Harmonic-NAS, a framework for the joint
optimization of unimodal backbones and multimodal fusion networks with hardware
awareness on resource-constrained devices. Harmonic-NAS involves a two-tier
optimization approach for the unimodal backbone architectures and fusion
strategy and operators. By incorporating the hardware dimension into the
optimization, evaluation results on various devices and multimodal datasets
have demonstrated the superiority of Harmonic-NAS over state-of-the-art
approaches achieving up to 10.9% accuracy improvement, 1.91x latency reduction,
and 2.14x energy efficiency gain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghebriout_M/0/1/0/all/0/1&quot;&gt;Mohamed Imed Eddine Ghebriout&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouzidi_H/0/1/0/all/0/1&quot;&gt;Halima Bouzidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niar_S/0/1/0/all/0/1&quot;&gt;Smail Niar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouarnoughi_H/0/1/0/all/0/1&quot;&gt;Hamza Ouarnoughi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07461">
<title>Detecting Unknown Attacks in IoT Environments: An Open Set Classifier for Enhanced Network Intrusion Detection. (arXiv:2309.07461v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07461</link>
<description rdf:parseType="Literal">&lt;p&gt;The widespread integration of Internet of Things (IoT) devices across all
facets of life has ushered in an era of interconnectedness, creating new
avenues for cybersecurity challenges and underscoring the need for robust
intrusion detection systems. However, traditional security systems are designed
with a closed-world perspective and often face challenges in dealing with the
ever-evolving threat landscape, where new and unfamiliar attacks are constantly
emerging. In this paper, we introduce a framework aimed at mitigating the open
set recognition (OSR) problem in the realm of Network Intrusion Detection
Systems (NIDS) tailored for IoT environments. Our framework capitalizes on
image-based representations of packet-level data, extracting spatial and
temporal patterns from network traffic. Additionally, we integrate stacking and
sub-clustering techniques, enabling the identification of unknown attacks by
effectively modeling the complex and diverse nature of benign behavior. The
empirical results prominently underscore the framework&apos;s efficacy, boasting an
impressive 88\% detection rate for previously unseen attacks when compared
against existing approaches and recent advancements. Future work will perform
extensive experimentation across various openness levels and attack scenarios,
further strengthening the adaptability and performance of our proposed solution
in safeguarding IoT environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farrukh_Y/0/1/0/all/0/1&quot;&gt;Yasir Ali Farrukh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wali_S/0/1/0/all/0/1&quot;&gt;Syed Wali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_I/0/1/0/all/0/1&quot;&gt;Irfan Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bastian_N/0/1/0/all/0/1&quot;&gt;Nathaniel D. Bastian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09301">
<title>RenderIH: A Large-scale Synthetic Dataset for 3D Interacting Hand Pose Estimation. (arXiv:2309.09301v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09301</link>
<description rdf:parseType="Literal">&lt;p&gt;The current interacting hand (IH) datasets are relatively simplistic in terms
of background and texture, with hand joints being annotated by a machine
annotator, which may result in inaccuracies, and the diversity of pose
distribution is limited. However, the variability of background, pose
distribution, and texture can greatly influence the generalization ability.
Therefore, we present a large-scale synthetic dataset RenderIH for interacting
hands with accurate and diverse pose annotations. The dataset contains 1M
photo-realistic images with varied backgrounds, perspectives, and hand
textures. To generate natural and diverse interacting poses, we propose a new
pose optimization algorithm. Additionally, for better pose estimation accuracy,
we introduce a transformer-based pose estimation network, TransHand, to
leverage the correlation between interacting hands and verify the effectiveness
of RenderIH in improving results. Our dataset is model-agnostic and can improve
more accuracy of any hand pose estimation method in comparison to other real or
synthetic datasets. Experiments have shown that pretraining on our synthetic
data can significantly decrease the error from 6.76mm to 5.79mm, and our
Transhand surpasses contemporary methods. Our dataset and code are available at
https://github.com/adwardlee/RenderIH.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lijun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1&quot;&gt;Linrui Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xindi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mengyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09979">
<title>General In-Hand Object Rotation with Vision and Touch. (arXiv:2309.09979v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09979</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce RotateIt, a system that enables fingertip-based object rotation
along multiple axes by leveraging multimodal sensory inputs. Our system is
trained in simulation, where it has access to ground-truth object shapes and
physical properties. Then we distill it to operate on realistic yet noisy
simulated visuotactile and proprioceptive sensory inputs. These multimodal
inputs are fused via a visuotactile transformer, enabling online inference of
object shapes and physical properties during deployment. We show significant
performance improvements over prior methods and the importance of visual and
tactile sensing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1&quot;&gt;Haozhi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_B/0/1/0/all/0/1&quot;&gt;Brent Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1&quot;&gt;Sudharshan Suresh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lambeta_M/0/1/0/all/0/1&quot;&gt;Mike Lambeta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calandra_R/0/1/0/all/0/1&quot;&gt;Roberto Calandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1&quot;&gt;Jitendra Malik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11500">
<title>A Large-scale Dataset for Audio-Language Representation Learning. (arXiv:2309.11500v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2309.11500</link>
<description rdf:parseType="Literal">&lt;p&gt;The AI community has made significant strides in developing powerful
foundation models, driven by large-scale multimodal datasets. However, in the
audio representation learning community, the present audio-language datasets
suffer from limitations such as insufficient volume, simplistic content, and
arduous collection procedures. To tackle these challenges, we present an
innovative and automatic audio caption generation pipeline based on a series of
public tools or APIs, and construct a large-scale, high-quality, audio-language
dataset, named as Auto-ACD, comprising over 1.9M audio-text pairs. To
demonstrate the effectiveness of the proposed dataset, we train popular models
on our dataset and show performance improvement on various downstream tasks,
namely, audio-language retrieval, audio captioning, environment classification.
In addition, we establish a novel test set and provide a benchmark for
audio-text tasks. The proposed dataset will be released at
https://auto-acd.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Luoyi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xuenan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Mengyue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weidi Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13302">
<title>Gaining the Sparse Rewards by Exploring Binary Lottery Tickets in Spiking Neural Network. (arXiv:2309.13302v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13302</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking Neural Network (SNN) as a brain-inspired strategy receives lots of
attention because of the high-sparsity and low-power properties derived from
its inherent spiking information state. To further improve the efficiency of
SNN, some works declare that the Lottery Tickets (LTs) Hypothesis, which
indicates that the Artificial Neural Network (ANN) contains a subnetwork
without sacrificing the performance of the original network, also exists in
SNN. However, the spiking information handled by SNN has a natural similarity
and affinity with binarization in sparsification. Therefore, to further explore
SNN efficiency, this paper focuses on (1) the presence or absence of LTs in the
binary SNN, and (2) whether the spiking mechanism is a superior strategy in
terms of handling binary information compared to simple model binarization. To
certify these consumptions, a sparse training method is proposed to find Binary
Weights Spiking Lottery Tickets (BinW-SLT) under different network structures.
Through comprehensive evaluations, we show that BinW-SLT could attain up to
+5.86% and +3.17% improvement on CIFAR-10 and CIFAR-100 compared with binary
LTs, as well as achieve 1.86x and 8.92x energy saving compared with
full-precision SNN and ANN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jiahang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_E/0/1/0/all/0/1&quot;&gt;Erjia Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Pu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Mengshu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaxu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jize Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xue Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1&quot;&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kaidi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Renjing Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13393">
<title>AgriSORT: A Simple Online Real-time Tracking-by-Detection framework for robotics in precision agriculture. (arXiv:2309.13393v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13393</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of multi-object tracking (MOT) consists in detecting and tracking
all the objects in a video sequence while keeping a unique identifier for each
object. It is a challenging and fundamental problem for robotics. In precision
agriculture the challenge of achieving a satisfactory solution is amplified by
extreme camera motion, sudden illumination changes, and strong occlusions. Most
modern trackers rely on the appearance of objects rather than motion for
association, which can be ineffective when most targets are static objects with
the same appearance, as in the agricultural case. To this end, on the trail of
SORT [5], we propose AgriSORT, a simple, online, real-time
tracking-by-detection pipeline for precision agriculture based only on motion
information that allows for accurate and fast propagation of tracks between
frames. The main focuses of AgriSORT are efficiency, flexibility, minimal
dependencies, and ease of deployment on robotic platforms. We test the proposed
pipeline on a novel MOT benchmark specifically tailored for the agricultural
context, based on video sequences taken in a table grape vineyard, particularly
challenging due to strong self-similarity and density of the instances. Both
the code and the dataset are available for future comparisons.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saraceni_L/0/1/0/all/0/1&quot;&gt;Leonardo Saraceni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Motoi_I/0/1/0/all/0/1&quot;&gt;Ionut M. Motoi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nardi_D/0/1/0/all/0/1&quot;&gt;Daniele Nardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ciarfuglia_T/0/1/0/all/0/1&quot;&gt;Thomas A. Ciarfuglia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13556">
<title>LOGICSEG: Parsing Visual Semantics with Neural Logic Learning and Reasoning. (arXiv:2309.13556v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13556</link>
<description rdf:parseType="Literal">&lt;p&gt;Current high-performance semantic segmentation models are purely data-driven
sub-symbolic approaches and blind to the structured nature of the visual world.
This is in stark contrast to human cognition which abstracts visual perceptions
at multiple levels and conducts symbolic reasoning with such structured
abstraction. To fill these fundamental gaps, we devise LOGICSEG, a holistic
visual semantic parser that integrates neural inductive learning and logic
reasoning with both rich data and symbolic knowledge. In particular, the
semantic concepts of interest are structured as a hierarchy, from which a set
of constraints are derived for describing the symbolic relations and formalized
as first-order logic rules. After fuzzy logic-based continuous relaxation,
logical formulae are grounded onto data and neural computational graphs, hence
enabling logic-induced network training. During inference, logical constraints
are packaged into an iterative process and injected into the network in a form
of several matrix multiplications, so as to achieve hierarchy-coherent
prediction with logic reasoning. These designs together make LOGICSEG a general
and compact neural-logic machine that is readily integrated into existing
segmentation models. Extensive experiments over four datasets with various
segmentation models and backbones verify the effectiveness and generality of
LOGICSEG. We believe this study opens a new avenue for visual semantic parsing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liulei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenguan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13655">
<title>Adaptation of the super resolution SOTA for Art Restoration in camera capture images. (arXiv:2309.13655v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13655</link>
<description rdf:parseType="Literal">&lt;p&gt;Preserving cultural heritage is of paramount importance. In the domain of art
restoration, developing a computer vision model capable of effectively
restoring deteriorated images of art pieces was difficult, but now we have a
good computer vision state-of-art. Traditional restoration methods are often
time-consuming and require extensive expertise. The aim of this work is to
design an automated solution based on computer vision models that can enhance
and reconstruct degraded artworks, improving their visual quality while
preserving their original characteristics and artifacts. The model should
handle a diverse range of deterioration types, including but not limited to
noise, blur, scratches, fading, and other common forms of degradation. We adapt
the current state-of-art for the image super-resolution based on the Diffusion
Model (DM) and fine-tune it for Image art restoration. Our results show that
instead of fine-tunning multiple different models for different kinds of
degradation, fine-tuning one super-resolution. We train it on multiple datasets
to make it robust. code link: https://github.com/Naagar/art_restoration_DM
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagar_S/0/1/0/all/0/1&quot;&gt;Sandeep Nagar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bala_A/0/1/0/all/0/1&quot;&gt;Abhinaba Bala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patnaik_S/0/1/0/all/0/1&quot;&gt;Sai Amrit Patnaik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14181">
<title>Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision. (arXiv:2309.14181v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14181</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid evolution of Multi-modality Large Language Models (MLLMs) has
catalyzed a shift in computer vision from specialized models to general-purpose
foundation models. Nevertheless, there is still an inadequacy in assessing the
abilities of MLLMs on low-level visual perception and understanding. To address
this gap, we present Q-Bench, a holistic benchmark crafted to systematically
evaluate potential abilities of MLLMs on three realms: low-level visual
perception, low-level visual description, and overall visual quality
assessment. a) To evaluate the low-level perception ability, we construct the
LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped
with a human-asked question focusing on its low-level attributes. We then
measure the correctness of MLLMs on answering these questions. b) To examine
the description ability of MLLMs on low-level information, we propose the
LLDescribe dataset consisting of long expert-labelled golden low-level text
descriptions on 499 images, and a GPT-involved comparison pipeline between
outputs of MLLMs and the golden descriptions. c) Besides these two tasks, we
further measure their visual quality assessment ability to align with human
opinion scores. Specifically, we design a softmax-based strategy that enables
MLLMs to predict quantifiable quality scores, and evaluate them on various
existing image quality assessment (IQA) datasets. Our evaluation across the
three abilities confirms that MLLMs possess preliminary low-level visual
skills. However, these skills are still unstable and relatively imprecise,
indicating the need for specific enhancements on MLLMs towards these abilities.
We hope that our benchmark can encourage the research community to delve deeper
to discover and enhance these untapped potentials of MLLMs. Project Page:
https://vqassessment.github.io/Q-Bench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haoning Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zicheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Erli Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaofeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1&quot;&gt;Liang Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Annan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wenxiu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1&quot;&gt;Qiong Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1&quot;&gt;Guangtao Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weisi Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14329">
<title>Innovative Digital Storytelling with AIGC: Exploration and Discussion of Recent Advances. (arXiv:2309.14329v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14329</link>
<description rdf:parseType="Literal">&lt;p&gt;Digital storytelling, as an art form, has struggled with cost-quality
balance. The emergence of AI-generated Content (AIGC) is considered as a
potential solution for efficient digital storytelling production. However, the
specific form, effects, and impacts of this fusion remain unclear, leaving the
boundaries of AIGC combined with storytelling undefined. This work explores the
current integration state of AIGC and digital storytelling, investigates the
artistic value of their fusion in a sample project, and addresses common issues
through interviews. Through our study, we conclude that AIGC, while proficient
in image creation, voiceover production, and music composition, falls short of
replacing humans due to the irreplaceable elements of human creativity and
aesthetic sensibilities at present, especially in complex character animations,
facial expressions, and sound effects. The research objective is to increase
public awareness of the current state, limitations, and challenges arising from
combining AIGC and digital storytelling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1&quot;&gt;Rongzhang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1&quot;&gt;Changyue Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wayne Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14564">
<title>Generative Escher Meshes. (arXiv:2309.14564v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14564</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a fully-automatic, text-guided generative method for
producing periodic, repeating, tile-able 2D art, such as the one seen on
floors, mosaics, ceramics, and the work of M.C. Escher. In contrast to the
standard concept of a seamless texture, i.e., square images that are seamless
when tiled, our method generates non-square tilings which comprise solely of
repeating copies of the same object. It achieves this by optimizing both
geometry and color of a 2D mesh, in order to generate a non-square tile in the
shape and appearance of the desired object, with close to no additional
background details. We enable geometric optimization of tilings by our key
technical contribution: an unconstrained, differentiable parameterization of
the space of all possible tileable shapes for a given symmetry group. Namely,
we prove that modifying the laplacian used in a 2D mesh-mapping technique -
Orbifold Tutte Embedding - can achieve all possible tiling configurations for a
chosen planar symmetry group. We thus consider both the mesh&apos;s tile-shape and
its texture as optimizable parameters, rendering the textured mesh via a
differentiable renderer. We leverage a trained image diffusion model to define
a loss on the resulting image, thereby updating the mesh&apos;s parameters based on
its appearance matching the text prompt. We show our method is able to produce
plausible, appealing results, with non-trivial tiles, for a variety of
different periodic tiling patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1&quot;&gt;Noam Aigerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groueix_T/0/1/0/all/0/1&quot;&gt;Thibault Groueix&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14704">
<title>Tile Classification Based Viewport Prediction with Multi-modal Fusion Transformer. (arXiv:2309.14704v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14704</link>
<description rdf:parseType="Literal">&lt;p&gt;Viewport prediction is a crucial aspect of tile-based 360 video streaming
system. However, existing trajectory based methods lack of robustness, also
oversimplify the process of information construction and fusion between
different modality inputs, leading to the error accumulation problem. In this
paper, we propose a tile classification based viewport prediction method with
Multi-modal Fusion Transformer, namely MFTR. Specifically, MFTR utilizes
transformer-based networks to extract the long-range dependencies within each
modality, then mine intra- and inter-modality relations to capture the combined
impact of user historical inputs and video contents on future viewport
selection. In addition, MFTR categorizes future tiles into two categories: user
interested or not, and selects future viewport as the region that contains most
user interested tiles. Comparing with predicting head trajectories, choosing
future viewport based on tile&apos;s binary classification results exhibits better
robustness and interpretability. To evaluate our proposed MFTR, we conduct
extensive experiments on two widely used PVS-HM and Xu-Gaze dataset. MFTR shows
superior performance over state-of-the-art methods in terms of average
prediction accuracy and overlap ratio, also presents competitive computation
efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhihao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiwei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weizhan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1&quot;&gt;Caixia Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qinghua Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wangdu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14793">
<title>Semantic Map Learning of Traffic Light to Lane Assignment based on Motion Data. (arXiv:2309.14793v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14793</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding which traffic light controls which lane is crucial to navigate
intersections safely. Autonomous vehicles commonly rely on High Definition (HD)
maps that contain information about the assignment of traffic lights to lanes.
The manual provisioning of this information is tedious, expensive, and not
scalable. To remedy these issues, our novel approach derives the assignments
from traffic light states and the corresponding motion patterns of vehicle
traffic. This works in an automated way and independently of the geometric
arrangement. We show the effectiveness of basic statistical approaches for this
task by implementing and evaluating a pattern-based contribution method. In
addition, our novel rejection method includes accompanying safety
considerations by leveraging statistical hypothesis testing. Finally, we
propose a dataset transformation to re-purpose available motion prediction
datasets for semantic map learning. Our publicly available API for the Lyft
Level 5 dataset enables researchers to develop and evaluate their own
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monninger_T/0/1/0/all/0/1&quot;&gt;Thomas Monninger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weber_A/0/1/0/all/0/1&quot;&gt;Andreas Weber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Staab_S/0/1/0/all/0/1&quot;&gt;Steffen Staab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15411">
<title>3D Multiple Object Tracking on Autonomous Driving: A Literature Review. (arXiv:2309.15411v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15411</link>
<description rdf:parseType="Literal">&lt;p&gt;3D multi-object tracking (3D MOT) stands as a pivotal domain within
autonomous driving, experiencing a surge in scholarly interest and commercial
promise over recent years. Despite its paramount significance, 3D MOT confronts
a myriad of formidable challenges, encompassing abrupt alterations in object
appearances, pervasive occlusion, the presence of diminutive targets, data
sparsity, missed detections, and the unpredictable initiation and termination
of object motion trajectories. Countless methodologies have emerged to grapple
with these issues, yet 3D MOT endures as a formidable problem that warrants
further exploration. This paper undertakes a comprehensive examination,
assessment, and synthesis of the research landscape in this domain, remaining
attuned to the latest developments in 3D MOT while suggesting prospective
avenues for future investigation. Our exploration commences with a systematic
exposition of key facets of 3D MOT and its associated domains, including
problem delineation, classification, methodological approaches, fundamental
principles, and empirical investigations. Subsequently, we categorize these
methodologies into distinct groups, dissecting each group meticulously with
regard to its challenges, underlying rationale, progress, merits, and demerits.
Furthermore, we present a concise recapitulation of experimental metrics and
offer an overview of prevalent datasets, facilitating a quantitative comparison
for a more intuitive assessment. Lastly, our deliberations culminate in a
discussion of the prevailing research landscape, highlighting extant challenges
and charting possible directions for 3D MOT research. We present a structured
and lucid road-map to guide forthcoming endeavors in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Liang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xin Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15523">
<title>Improving Facade Parsing with Vision Transformers and Line Integration. (arXiv:2309.15523v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15523</link>
<description rdf:parseType="Literal">&lt;p&gt;Facade parsing stands as a pivotal computer vision task with far-reaching
applications in areas like architecture, urban planning, and energy efficiency.
Despite the recent success of deep learning-based methods in yielding
impressive results on certain open-source datasets, their viability for
real-world applications remains uncertain. Real-world scenarios are
considerably more intricate, demanding greater computational efficiency.
Existing datasets often fall short in representing these settings, and previous
methods frequently rely on extra models to enhance accuracy, which requires
much computation cost. In this paper, we introduce Comprehensive Facade Parsing
(CFP), a dataset meticulously designed to encompass the intricacies of
real-world facade parsing tasks. Comprising a total of 602 high-resolution
street-view images, this dataset captures a diverse array of challenging
scenarios, including sloping angles and densely clustered buildings, with
painstakingly curated annotations for each image. We introduce a new pipeline
known as Revision-based Transformer Facade Parsing (RTFP). This marks the
pioneering utilization of Vision Transformers (ViT) in facade parsing, and our
experimental results definitively substantiate its merit. We also design Line
Acquisition, Filtering, and Revision (LAFR), an efficient yet accurate revision
algorithm that can improve the segment result solely from simple line detection
using prior knowledge of the facade. In ECP 2011, RueMonge 2014, and our CFP,
we evaluate the superiority of our method. The dataset and code are available
at https://github.com/wbw520/RTFP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bowen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaxing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ran Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunqin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liangzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1&quot;&gt;Yuta Nakashima&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15564">
<title>Jointly Training Large Autoregressive Multimodal Models. (arXiv:2309.15564v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15564</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, advances in the large-scale pretraining of language and
text-to-image models have revolutionized the field of machine learning. Yet,
integrating these two modalities into a single, robust model capable of
generating seamless multimodal outputs remains a significant challenge. To
address this gap, we present the Joint Autoregressive Mixture (JAM) framework,
a modular approach that systematically fuses existing text and image generation
models. We also introduce a specialized, data-efficient instruction-tuning
strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned
model demonstrates unparalleled performance in generating high-quality
multimodal outputs and represents the first model explicitly designed for this
purpose.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aiello_E/0/1/0/all/0/1&quot;&gt;Emanuele Aiello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lili Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1&quot;&gt;Yixin Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1&quot;&gt;Armen Aghajanyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1&quot;&gt;Barlas Oguz&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>