<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-14T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08376" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08384" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08459" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08479" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08514" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08524" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08555" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08558" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08563" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08592" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08593" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08594" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08600" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08606" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08624" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08628" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08636" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08644" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08650" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08653" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08659" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08664" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08673" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08675" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08692" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08697" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08733" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08735" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08740" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08744" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08760" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08768" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08774" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08785" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08822" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08825" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08839" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08843" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08846" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08853" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08859" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08864" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08871" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08872" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08876" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08878" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08879" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08881" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08882" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08883" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08885" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08888" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08889" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08895" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08912" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08914" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08939" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08951" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08952" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08975" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08977" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08983" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08985" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09005" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09022" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09028" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09031" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09037" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09038" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.04720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.17255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.10347" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.05575" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.07207" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.09753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.02529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.05788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.05724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.05742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.02997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.04246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.06244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01903" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05164" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10547" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13921" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16321" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10007" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10311" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00293" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01146" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12618" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02668" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12608" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05804" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06372" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08820" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02558" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06497" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12342" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14294" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15939" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17804" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00878" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02252" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04265" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05284" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05476" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06733" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07062" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07180" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07353" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07533" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07853" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07871" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07976" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08078" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.08376">
<title>A global optimization SAR image segmentation model can be easily transformed to a general ROF denoising model. (arXiv:2312.08376v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08376</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel locally statistical active contour model
(LACM) based on Aubert-Aujol (AA) denoising model and variational level set
method, which can be used for SAR images segmentation with intensity
inhomogeneity. Then we transform the proposed model into a global optimization
model by using convex relaxation technique. Firstly, we apply the Split Bregman
technique to transform the global optimization model into two alternating
optimization processes of Shrink operator and Laplace operator, which is called
SB_LACM model. Moreover, we propose two fast models to solve the global
optimization model , which are more efficient than the SB_LACM model. The first
model is: we add the proximal function to transform the global optimization
model to a general ROF model[29], which can be solved by a fast denoising
algorithm proposed by R.-Q.Jia, and H.Zhao; Thus we obtain a fast segmentation
algorithm with global optimization solver that does not involve partial
differential equations or difference equation, and only need simple difference
computation. The second model is: we use a different splitting approach than
one model to transform the global optimization model into a differentiable term
and a general ROF model term, which can be solved by the same technique as the
first model. Experiments using some challenging synthetic images and Envisat
SAR images demonstrate the superiority of our proposed models with respect to
the state-of-the-art models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guangming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jing Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08378">
<title>Singular Value Penalization and Semantic Data Augmentation for Fully Test-Time Adaptation. (arXiv:2312.08378v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08378</link>
<description rdf:parseType="Literal">&lt;p&gt;Fully test-time adaptation (FTTA) adapts a model that is trained on a source
domain to a target domain during the testing phase, where the two domains
follow different distributions and source data is unavailable during the
training phase. Existing methods usually adopt entropy minimization to reduce
the uncertainty of target prediction results, and improve the FTTA performance
accordingly. However, they fail to ensure the diversity in target prediction
results. Recent domain adaptation study has shown that maximizing the sum of
singular values of prediction results can simultaneously enhance their
confidence (discriminability) and diversity. However, during the training
phase, larger singular values usually take up a dominant position in loss
maximization. This results in the model being more inclined to enhance
discriminability for easily distinguishable classes, and the improvement in
diversity is insufficiently effective. Furthermore, the adaptation and
prediction in FTTA only use data from the current batch, which may lead to the
risk of overfitting. To address the aforementioned issues, we propose
maximizing the sum of singular values while minimizing their variance. This
enables the model&apos;s focus toward the smaller singular values, enhancing
discriminability between more challenging classes and effectively increasing
the diversity of prediction results. Moreover, we incorporate data from the
previous batch to realize semantic data augmentation for the current batch,
reducing the risk of overfitting. Extensive experiments on benchmark datasets
show our proposed approach outperforms some compared state-of-the-art FTTA
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Houcheng Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Daixian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengzhu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08384">
<title>Taking it further: leveraging pseudo labels for field delineation across label-scarce smallholder regions. (arXiv:2312.08384v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08384</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning allows for resource-efficient geographic transfer of
pre-trained field delineation models. However, the scarcity of labeled data for
complex and dynamic smallholder landscapes, particularly in Sub-Saharan Africa,
remains a major bottleneck for large-area field delineation. This study
explores opportunities of using sparse field delineation pseudo labels for
fine-tuning models across geographies and sensor characteristics. We build on a
FracTAL ResUNet trained for crop field delineation in India (median field size
of 0.24 ha) and use this pre-trained model to generate pseudo labels in
Mozambique (median field size of 0.06 ha). We designed multiple pseudo label
selection strategies and compared the quantities, area properties, seasonal
distribution, and spatial agreement of the pseudo labels against
human-annotated training labels (n = 1,512). We then used the human-annotated
labels and the pseudo labels for model fine-tuning and compared predictions
against human field annotations (n = 2,199). Our results indicate i) a good
baseline performance of the pre-trained model in both field delineation and
field size estimation, and ii) the added value of regional fine-tuning with
performance improvements in nearly all experiments. Moreover, we found iii)
substantial performance increases when using only pseudo labels (up to 77% of
the IoU increases and 68% of the RMSE decreases obtained by human labels), and
iv) additional performance increases when complementing human annotations with
pseudo labels. Pseudo labels can be efficiently generated at scale and thus
facilitate domain adaptation in label-scarce settings. The workflow presented
here is a stepping stone for overcoming the persisting data gaps in
heterogeneous smallholder agriculture of Sub-Saharan Africa, where labels are
commonly scarce.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rufin_P/0/1/0/all/0/1&quot;&gt;Philippe Rufin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sherrie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lisboa_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe1; Nogueira Lisboa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemmerling_J/0/1/0/all/0/1&quot;&gt;Jan Hemmerling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tulbure_M/0/1/0/all/0/1&quot;&gt;Mirela G. Tulbure&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyfroidt_P/0/1/0/all/0/1&quot;&gt;Patrick Meyfroidt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08408">
<title>Explainable AI in Grassland Monitoring: Enhancing Model Performance and Domain Adaptability. (arXiv:2312.08408v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08408</link>
<description rdf:parseType="Literal">&lt;p&gt;Grasslands are known for their high biodiversity and ability to provide
multiple ecosystem services. Challenges in automating the identification of
indicator plants are key obstacles to large-scale grassland monitoring. These
challenges stem from the scarcity of extensive datasets, the distributional
shifts between generic and grassland-specific datasets, and the inherent
opacity of deep learning models. This paper delves into the latter two
challenges, with a specific focus on transfer learning and eXplainable
Artificial Intelligence (XAI) approaches to grassland monitoring, highlighting
the novelty of XAI in this domain. We analyze various transfer learning methods
to bridge the distributional gaps between generic and grassland-specific
datasets. Additionally, we showcase how explainable AI techniques can unveil
the model&apos;s domain adaptation capabilities, employing quantitative assessments
to evaluate the model&apos;s proficiency in accurately centering relevant input
features around the object of interest. This research contributes valuable
insights for enhancing model performance through transfer learning and
measuring domain adaptability with explainable AI, showing significant promise
for broader applications within the agricultural community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shanghua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hedstrom_A/0/1/0/all/0/1&quot;&gt;Anna Hedstr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basavegowda_D/0/1/0/all/0/1&quot;&gt;Deepak Hanike Basavegowda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weltzien_C/0/1/0/all/0/1&quot;&gt;Cornelia Weltzien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1&quot;&gt;Marina M.-C. H&amp;#xf6;hne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08459">
<title>FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models. (arXiv:2312.08459v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08459</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce FaceTalk, a novel generative approach designed for synthesizing
high-fidelity 3D motion sequences of talking human heads from input audio
signal. To capture the expressive, detailed nature of human heads, including
hair, ears, and finer-scale eye movements, we propose to couple speech signal
with the latent space of neural parametric head models to create high-fidelity,
temporally coherent motion sequences. We propose a new latent diffusion model
for this task, operating in the expression space of neural parametric head
models, to synthesize audio-driven realistic head sequences. In the absence of
a dataset with corresponding NPHM expressions to audio, we optimize for these
correspondences to produce a dataset of temporally-optimized NPHM expressions
fit to audio-video recordings of people talking. To the best of our knowledge,
this is the first work to propose a generative approach for realistic and
high-quality motion synthesis of volumetric human heads, representing a
significant advancement in the field of audio-driven 3D animation. Notably, our
approach stands out in its ability to generate plausible motion sequences that
can produce high-fidelity head animation coupled with the NPHM shape space. Our
experimental results substantiate the effectiveness of FaceTalk, consistently
achieving superior and visually natural motion, encompassing diverse facial
expressions and styles, outperforming existing methods by 75% in perceptual
user study evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aneja_S/0/1/0/all/0/1&quot;&gt;Shivangi Aneja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1&quot;&gt;Justus Thies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Angela Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1&quot;&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08479">
<title>Vision Transformer-Based Deep Learning for Histologic Classification of Endometrial Cancer. (arXiv:2312.08479v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08479</link>
<description rdf:parseType="Literal">&lt;p&gt;Endometrial cancer, the sixth most common cancer in females worldwide,
presents as a heterogeneous group with certain types prone to recurrence.
Precise histologic evaluation of endometrial cancer is essential for effective
patient management and determining the best treatment modalities. This study
introduces EndoNet, a transformer-based deep learning approach for histologic
classification of endometrial cancer. EndoNet uses convolutional neural
networks for extracting histologic features and a vision transformer for
aggregating these features and classifying slides based on their visual
characteristics. The model was trained on 929 digitized hematoxylin and
eosin-stained whole slide images of endometrial cancer from hysterectomy cases
at Dartmouth Health. It classifies these slides into low grade (Endometroid
Grades 1 and 2) and high-grade (endometroid carcinoma FIGO grade 3, uterine
serous carcinoma, carcinosarcoma) categories. EndoNet was evaluated on an
internal test set of 218 slides and an external test set of 100 random slides
from the public TCGA database. The model achieved a weighted average F1-score
of 0.92 (95% CI: 0.87-0.95) and an AUC of 0.93 (95% CI: 0.88-0.96) on the
internal test, and 0.86 (95% CI: 0.80-0.94) for F1-score and 0.86 (95% CI:
0.75-0.93) for AUC on the external test. Pending further validation, EndoNet
has the potential to assist pathologists in classifying challenging gynecologic
pathology tumors and enhancing patient care.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_M/0/1/0/all/0/1&quot;&gt;Manu Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tafe_L/0/1/0/all/0/1&quot;&gt;Laura J. Tafe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;James X. Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1&quot;&gt;Kristen E. Muller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hondelink_L/0/1/0/all/0/1&quot;&gt;Liesbeth Hondelink&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bentz_J/0/1/0/all/0/1&quot;&gt;Jessica L. Bentz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassanpour_S/0/1/0/all/0/1&quot;&gt;Saeed Hassanpour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08488">
<title>PnP for Two-Dimensional Pose Estimation. (arXiv:2312.08488v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.08488</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a PnP algorithm for a camera constrained to two-dimensional
movement (applicable, for instance, to many wheeled robotics platforms).
Leveraging this assumption allows performance improvements over 3D PnP
algorithms due to the reduction in search space dimensionality. It also reduces
the incidence of ambiguous pose estimates (as, in most cases, the spurious
solutions fall outside the plane of movement). Our algorithm finds an
approximate solution using geometric criteria and refines its prediction
iteratively. We compare this algorithm to existing 3D PnP algorithms in the
cases of general and coplanar point configurations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Joshua Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08514">
<title>M3T: Multi-Scale Memory Matching for Video Object Segmentation and Tracking. (arXiv:2312.08514v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08514</link>
<description rdf:parseType="Literal">&lt;p&gt;Video Object Segmentation (VOS) has became increasingly important with
availability of larger datasets and more complex and realistic settings, which
involve long videos with global motion (e.g, in egocentric settings), depicting
small objects undergoing both rigid and non-rigid (including state)
deformations. While a number of recent approaches have been explored for this
task, these data characteristics still present challenges. In this work we
propose a novel, DETR-style encoder-decoder architecture, which focuses on
systematically analyzing and addressing aforementioned challenges.
Specifically, our model enables on-line inference with long videos in a
windowed fashion, by breaking the video into clips and propagating context
among them using time-coded memory. We illustrate that short clip length and
longer memory with learned time-coding are important design choices for
achieving state-of-the-art (SoTA) performance. Further, we propose multi-scale
matching and decoding to ensure sensitivity and accuracy for small objects.
Finally, we propose a novel training strategy that focuses learning on portions
of the video where an object undergoes significant deformations -- a form of
&quot;soft&quot; hard-negative mining, implemented as loss-reweighting. Collectively,
these technical contributions allow our model to achieve SoTA performance on
two complex datasets -- VISOR and VOST. A series of detailed ablations validate
our design choices as well as provide insights into the importance of parameter
choices and their impact on performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_R/0/1/0/all/0/1&quot;&gt;Raghav Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1&quot;&gt;Wan-Cyuan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siam_M/0/1/0/all/0/1&quot;&gt;Mennatullah Siam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1&quot;&gt;Leonid Sigal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08524">
<title>A FUNQUE Approach to the Quality Assessment of Compressed HDR Videos. (arXiv:2312.08524v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.08524</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have seen steady growth in the popularity and availability of
High Dynamic Range (HDR) content, particularly videos, streamed over the
internet. As a result, assessing the subjective quality of HDR videos, which
are generally subjected to compression, is of increasing importance. In
particular, we target the task of full-reference quality assessment of
compressed HDR videos. The state-of-the-art (SOTA) approach HDRMAX involves
augmenting off-the-shelf video quality models, such as VMAF, with features
computed on non-linearly transformed video frames. However, HDRMAX increases
the computational complexity of models like VMAF. Here, we show that an
efficient class of video quality prediction models named FUNQUE+ achieves SOTA
accuracy. This shows that the FUNQUE+ models are flexible alternatives to VMAF
that achieve higher HDR video quality prediction accuracy at lower
computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Venkataramanan_A/0/1/0/all/0/1&quot;&gt;Abhinau K. Venkataramanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Stejerean_C/0/1/0/all/0/1&quot;&gt;Cosmin Stejerean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Katsavounidis_I/0/1/0/all/0/1&quot;&gt;Ioannis Katsavounidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1&quot;&gt;Alan C. Bovik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08548">
<title>EVP: Enhanced Visual Perception using Inverse Multi-Attentive Feature Refinement and Regularized Image-Text Alignment. (arXiv:2312.08548v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08548</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents the network architecture EVP (Enhanced Visual Perception).
EVP builds on the previous work VPD which paved the way to use the Stable
Diffusion network for computer vision tasks. We propose two major enhancements.
First, we develop the Inverse Multi-Attentive Feature Refinement (IMAFR) module
which enhances feature learning capabilities by aggregating spatial information
from higher pyramid levels. Second, we propose a novel image-text alignment
module for improved feature extraction of the Stable Diffusion backbone. The
resulting architecture is suitable for a wide variety of tasks and we
demonstrate its performance in the context of single-image depth estimation
with a specialized decoder using classification-based bins and referring
segmentation with an off-the-shelf decoder. Comprehensive experiments conducted
on established datasets show that EVP achieves state-of-the-art results in
single-image depth estimation for indoor (NYU Depth v2, 11.8% RMSE improvement
over VPD) and outdoor (KITTI) environments, as well as referring segmentation
(RefCOCO, 2.53 IoU improvement over ReLA). The code and pre-trained models are
publicly available at https://github.com/Lavreniuk/EVP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavreniuk_M/0/1/0/all/0/1&quot;&gt;Mykola Lavreniuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1&quot;&gt;Shariq Farooq Bhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1&quot;&gt;Matthias M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1&quot;&gt;Peter Wonka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08555">
<title>KDAS3: Knowledge distillation via Attention Supervision, and Symmetrical structure guiding for Polyp Segmentation. (arXiv:2312.08555v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.08555</link>
<description rdf:parseType="Literal">&lt;p&gt;Polyp segmentation, a contentious issue in medical imaging, has seen numerous
proposed methods aimed at improving the quality of segmented masks. Currently,
state-of-the-art techniques yield impressive results. However, the sheer size
of these models poses challenges for practical industry applications. To
address this, we present a Knowledge Distillation framework, incorporating
attention supervision and the symmetrical guiding method. This framework is
designed to facilitate knowledge transfer from a teacher model to a more
compact student model with fewer parameters. Our experimental evaluation of the
framework assesses its effectiveness in enabling the student model to acquire
knowledge from the teacher efficiently. Additionally, our method serves to
prevent the student model from incorporating redundant features that could lead
to inaccurate predictions. Consequently, our method, boasting approximately 5
million parameters, achieves competitive results comparable to the
state-of-the-art approaches. The implementation can be found at:
https://github.com/huyquoctrinh/KDAS3
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Trinh_Q/0/1/0/all/0/1&quot;&gt;Quoc-Huy Trinh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08558">
<title>G-MEMP: Gaze-Enhanced Multimodal Ego-Motion Prediction in Driving. (arXiv:2312.08558v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08558</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the decision-making process of drivers is one of the keys to
ensuring road safety. While the driver intent and the resulting ego-motion
trajectory are valuable in developing driver-assistance systems, existing
methods mostly focus on the motions of other vehicles. In contrast, we focus on
inferring the ego trajectory of a driver&apos;s vehicle using their gaze data. For
this purpose, we first collect a new dataset, GEM, which contains high-fidelity
ego-motion videos paired with drivers&apos; eye-tracking data and GPS coordinates.
Next, we develop G-MEMP, a novel multimodal ego-trajectory prediction network
that combines GPS and video input with gaze data. We also propose a new metric
called Path Complexity Index (PCI) to measure the trajectory complexity. We
perform extensive evaluations of the proposed method on both GEM and DR(eye)VE,
an existing benchmark dataset. The results show that G-MEMP significantly
outperforms state-of-the-art methods in both benchmarks. Furthermore, ablation
studies demonstrate over 20% improvement in average displacement using gaze
data, particularly in challenging driving scenarios with a high PCI. The data,
code, and models can be found at https://eth-ait.github.io/g-memp/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akbiyik_M/0/1/0/all/0/1&quot;&gt;M. Eren Akbiyik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savov_N/0/1/0/all/0/1&quot;&gt;Nedko Savov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1&quot;&gt;Danda Pani Paudel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popovic_N/0/1/0/all/0/1&quot;&gt;Nikola Popovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vater_C/0/1/0/all/0/1&quot;&gt;Christian Vater&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1&quot;&gt;Otmar Hilliges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08563">
<title>Efficient-NeRF2NeRF: Streamlining Text-Driven 3D Editing with Multiview Correspondence-Enhanced Diffusion Models. (arXiv:2312.08563v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08563</link>
<description rdf:parseType="Literal">&lt;p&gt;The advancement of text-driven 3D content editing has been blessed by the
progress from 2D generative diffusion models. However, a major obstacle
hindering the widespread adoption of 3D content editing is its time-intensive
processing. This challenge arises from the iterative and refining steps
required to achieve consistent 3D outputs from 2D image-based generative
models. Recent state-of-the-art methods typically require optimization time
ranging from tens of minutes to several hours to edit a 3D scene using a single
GPU. In this work, we propose that by incorporating correspondence
regularization into diffusion models, the process of 3D editing can be
significantly accelerated. This approach is inspired by the notion that the
estimated samples during diffusion should be multiview-consistent during the
diffusion generation process. By leveraging this multiview consistency, we can
edit 3D content at a much faster speed. In most scenarios, our proposed
technique brings a 10$\times$ speed-up compared to the baseline method and
completes the editing of a 3D scene in 2 minutes with comparable quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Liangchen Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1&quot;&gt;Liangliang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiatao Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yifan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Junsong Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08567">
<title>ConFormer: A Novel Collection of Deep Learning Models to Assist Cardiologists in the Assessment of Cardiac Function. (arXiv:2312.08567v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.08567</link>
<description rdf:parseType="Literal">&lt;p&gt;Cardiovascular diseases, particularly heart failure, are a leading cause of
death globally. The early detection of heart failure through routine
echocardiogram screenings is often impeded by the high cost and labor-intensive
nature of these procedures, a barrier that can mean the difference between life
and death. This paper presents ConFormer, a novel deep learning model designed
to automate the estimation of Ejection Fraction (EF) and Left Ventricular Wall
Thickness from echocardiograms. The implementation of ConFormer has the
potential to enhance preventative cardiology by enabling cost-effective,
accessible, and comprehensive heart health monitoring, thereby saving countless
lives. The source code is available at https://github.com/Aether111/ConFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thomas_E/0/1/0/all/0/1&quot;&gt;Ethan Thomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aslam_S/0/1/0/all/0/1&quot;&gt;Salman Aslam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08568">
<title>NViST: In the Wild New View Synthesis from a Single Image with Transformers. (arXiv:2312.08568v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08568</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose NViST, a transformer-based model for novel-view synthesis from a
single image, trained on a large-scale dataset of in-the-wild images with
complex backgrounds. NViST transforms image inputs directly into a radiance
field, adopting a scalable transformer-based architecture. In practice, NViST
exploits the self-supervised features learnt by a masked autoencoder (MAE), and
learns a novel decoder that translates features to 3D tokens via
cross-attention and adaptive layer normalization. Our model is efficient at
inference since only a single forward-pass is needed to predict a 3D
representation, unlike methods that require test-time optimization or sampling
such as 3D-aware diffusion models. We tackle further limitations of current
new-view synthesis models. First, unlike most generative models that are
trained in a category-specific manner, often on synthetic datasets or on masked
inputs, our model is trained on MVImgNet, a large-scale dataset of real-world,
casually-captured videos containing hundreds of object categories with diverse
backgrounds. Secondly, our model does not require canonicalization of the
training data - i.e. aligning all objects with a frontal view - only needing
relative pose at training time which removes a substantial barrier to it being
used on casually captured datasets. We show results on unseen objects and
categories on MVImgNet and even casual phone captures. We conduct qualitative
and quantitative evaluations on MVImgNet and ShapeNet to show that our model
represents a step forward towards enabling true in-the-wild novel-view
synthesis from a single image.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_W/0/1/0/all/0/1&quot;&gt;Wonbong Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agapito_L/0/1/0/all/0/1&quot;&gt;Lourdes Agapito&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08578">
<title>A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions. (arXiv:2312.08578v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08578</link>
<description rdf:parseType="Literal">&lt;p&gt;Curation methods for massive vision-language datasets trade off between
dataset size and quality. However, even the highest quality of available
curated captions are far too short to capture the rich visual detail in an
image. To show the value of dense and highly-aligned image-text pairs, we
collect the Densely Captioned Images (DCI) dataset, containing 8012 natural
images human-annotated with mask-aligned descriptions averaging above 1000
words each. With precise and reliable captions associated with specific parts
of an image, we can evaluate vision-language models&apos; (VLMs) understanding of
image content with a novel task that matches each caption with its
corresponding subcrop. As current models are often limited to 77 text tokens,
we also introduce a summarized version (sDCI) in which each caption length is
limited. We show that modern techniques that make progress on standard
benchmarks do not correspond with significant improvement on our sDCI based
benchmark. Lastly, we finetune CLIP using sDCI and show significant
improvements over the baseline despite a small training set. By releasing the
first human annotated dense image captioning dataset, we hope to enable the
development of new benchmarks or fine-tuning recipes for the next generation of
VLMs to come.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urbanek_J/0/1/0/all/0/1&quot;&gt;Jack Urbanek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bordes_F/0/1/0/all/0/1&quot;&gt;Florian Bordes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Astolfi_P/0/1/0/all/0/1&quot;&gt;Pietro Astolfi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williamson_M/0/1/0/all/0/1&quot;&gt;Mary Williamson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1&quot;&gt;Vasu Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_Soriano_A/0/1/0/all/0/1&quot;&gt;Adriana Romero-Soriano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08586">
<title>Estimating calibration error under label shift without labels. (arXiv:2312.08586v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08586</link>
<description rdf:parseType="Literal">&lt;p&gt;In the face of dataset shift, model calibration plays a pivotal role in
ensuring the reliability of machine learning systems. Calibration error (CE) is
an indicator of the alignment between the predicted probabilities and the
classifier accuracy. While prior works have delved into the implications of
dataset shift on calibration, existing CE estimators assume access to labels
from the target domain, which are often unavailable in practice, i.e., when the
model is deployed and used. This work addresses such challenging scenario, and
proposes a novel CE estimator under label shift, which is characterized by
changes in the marginal label distribution $p(Y)$, while keeping the
conditional $p(X|Y)$ constant between the source and target distributions. Our
contribution is an approach, which, by leveraging importance re-weighting of
the labeled source distribution, provides consistent and asymptotically
unbiased CE estimation with respect to the shifted target distribution.
Empirical results across diverse real-world datasets, under various conditions
and label-shift intensities, demonstrate the effectiveness and reliability of
the proposed estimator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popordanoska_T/0/1/0/all/0/1&quot;&gt;Teodora Popordanoska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radevski_G/0/1/0/all/0/1&quot;&gt;Gorjan Radevski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1&quot;&gt;Tinne Tuytelaars&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1&quot;&gt;Matthew B. Blaschko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08591">
<title>Joint2Human: High-quality 3D Human Generation via Compact Spherical Embedding of 3D Joints. (arXiv:2312.08591v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08591</link>
<description rdf:parseType="Literal">&lt;p&gt;3D human generation is increasingly significant in various applications.
However, the direct use of 2D generative methods in 3D generation often results
in significant loss of local details, while methods that reconstruct geometry
from generated images struggle with global view consistency. In this work, we
introduce Joint2Human, a novel method that leverages 2D diffusion models to
generate detailed 3D human geometry directly, ensuring both global structure
and local details. To achieve this, we employ the Fourier occupancy field (FOF)
representation, enabling the direct production of 3D shapes as preliminary
results using 2D generative models. With the proposed high-frequency enhancer
and the multi-view recarving strategy, our method can seamlessly integrate the
details from different views into a uniform global shape.To better utilize the
3D human prior and enhance control over the generated geometry, we introduce a
compact spherical embedding of 3D joints. This allows for effective application
of pose guidance during the generation process. Additionally, our method is
capable of generating 3D humans guided by textual inputs. Our experimental
results demonstrate the capability of our method to ensure global structure,
local details, high resolution, and low computational cost, simultaneously.
More results and code can be found on our project page at
&lt;a href=&quot;http://cic.tju.edu.cn/faculty/likun/projects/Joint2Human.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Muxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1&quot;&gt;Qiao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1&quot;&gt;Zhuo Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1&quot;&gt;Chao Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1&quot;&gt;Zhou Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08592">
<title>Dietary Assessment with Multimodal ChatGPT: A Systematic Analysis. (arXiv:2312.08592v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08592</link>
<description rdf:parseType="Literal">&lt;p&gt;Conventional approaches to dietary assessment are primarily grounded in
self-reporting methods or structured interviews conducted under the supervision
of dietitians. These methods, however, are often subjective, potentially
inaccurate, and time-intensive. Although artificial intelligence (AI)-based
solutions have been devised to automate the dietary assessment process, these
prior AI methodologies encounter challenges in their ability to generalize
across a diverse range of food types, dietary behaviors, and cultural contexts.
This results in AI applications in the dietary field that possess a narrow
specialization and limited accuracy. Recently, the emergence of multimodal
foundation models such as GPT-4V powering the latest ChatGPT has exhibited
transformative potential across a wide range of tasks (e.g., Scene
understanding and image captioning) in numerous research domains. These models
have demonstrated remarkable generalist intelligence and accuracy, capable of
processing various data modalities. In this study, we explore the application
of multimodal ChatGPT within the realm of dietary assessment. Our findings
reveal that GPT-4V excels in food detection under challenging conditions with
accuracy up to 87.5% without any fine-tuning or adaptation using food-specific
datasets. By guiding the model with specific language prompts (e.g., African
cuisine), it shifts from recognizing common staples like rice and bread to
accurately identifying regional dishes like banku and ugali. Another GPT-4V&apos;s
standout feature is its contextual awareness. GPT-4V can leverage surrounding
objects as scale references to deduce the portion sizes of food items, further
enhancing its accuracy in translating food weight into nutritional content.
This alignment with the USDA National Nutrient Database underscores GPT-4V&apos;s
potential to advance nutritional science and dietary assessment techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_F/0/1/0/all/0/1&quot;&gt;Frank P.-W. Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jianing Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junhong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1&quot;&gt;Bo Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Wu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannarou_S/0/1/0/all/0/1&quot;&gt;Stamatia Giannarou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frost_G/0/1/0/all/0/1&quot;&gt;Gary Frost&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_B/0/1/0/all/0/1&quot;&gt;Benny Lo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08593">
<title>MOSaiC: a Web-based Platform for Collaborative Medical Video Assessment and Annotation. (arXiv:2312.08593v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08593</link>
<description rdf:parseType="Literal">&lt;p&gt;This technical report presents MOSaiC 3.6.2, a web-based collaborative
platform designed for the annotation and evaluation of medical videos. MOSaiC
is engineered to facilitate video-based assessment and accelerate surgical data
science projects. We provide an overview of MOSaiC&apos;s key functionalities,
encompassing group and video management, annotation tools, ontologies,
assessment capabilities, and user administration. Finally, we briefly describe
several medical data science studies where MOSaiC has been instrumental in the
dataset development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazellier_J/0/1/0/all/0/1&quot;&gt;Jean-Paul Mazellier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boujon_A/0/1/0/all/0/1&quot;&gt;Antoine Boujon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bour_Lang_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe9;line Bour-Lang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erharhd_M/0/1/0/all/0/1&quot;&gt;Ma&amp;#xeb;l Erharhd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waechter_J/0/1/0/all/0/1&quot;&gt;Julien Waechter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wernert_E/0/1/0/all/0/1&quot;&gt;Emilie Wernert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mascagni_P/0/1/0/all/0/1&quot;&gt;Pietro Mascagni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1&quot;&gt;Nicolas Padoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08594">
<title>CT-MVSNet: Efficient Multi-View Stereo with Cross-scale Transformer. (arXiv:2312.08594v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08594</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent deep multi-view stereo (MVS) methods have widely incorporated
transformers into cascade network for high-resolution depth estimation,
achieving impressive results. However, existing transformer-based methods are
constrained by their computational costs, preventing their extension to finer
stages. In this paper, we propose a novel cross-scale transformer (CT) that
processes feature representations at different stages without additional
computation. Specifically, we introduce an adaptive matching-aware transformer
(AMT) that employs different interactive attention combinations at multiple
scales. This combined strategy enables our network to capture intra-image
context information and enhance inter-image feature relationships. Besides, we
present a dual-feature guided aggregation (DFGA) that embeds the coarse global
semantic information into the finer cost volume construction to further
strengthen global and local feature awareness. Meanwhile, we design a feature
metric loss (FM Loss) that evaluates the feature bias before and after
transformation to reduce the impact of feature mismatch on depth estimation.
Extensive experiments on DTU dataset and Tanks and Temples (T\&amp;amp;T) benchmark
demonstrate that our method achieves state-of-the-art results. Code is
available at https://github.com/wscstrive/CT-MVSNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sicheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1&quot;&gt;Lei Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08600">
<title>CartoMark: a benchmark dataset for map pattern recognition and 1 map content retrieval with machine intelligence. (arXiv:2312.08600v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08600</link>
<description rdf:parseType="Literal">&lt;p&gt;Maps are fundamental medium to visualize and represent the real word in a
simple and 16 philosophical way. The emergence of the 3rd wave information has
made a proportion of maps are available to be generated ubiquitously, which
would significantly enrich the dimensions and perspectives to understand the
characteristics of the real world. However, a majority of map dataset have
never been discovered, acquired and effectively used, and the map data used in
many applications might not be completely fitted for the authentic demands of
these applications. This challenge is emerged due to the lack of numerous
well-labelled benchmark datasets for implementing the deep learning approaches
into identifying complicated map content. Thus, we develop a large-scale
benchmark dataset that includes well-labelled dataset for map text annotation
recognition, map scene classification, map super-resolution reconstruction, and
map style transferring. Furthermore, these well-labelled datasets would
facilitate the state-of-the-art machine intelligence technologies to conduct
map feature detection, map pattern recognition and map content retrieval. We
hope our efforts would be useful for AI-enhanced cartographical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiran Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yi Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Honghao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kaiyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1&quot;&gt;Zhenfeng Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhigang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiao Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08606">
<title>VQCNIR: Clearer Night Image Restoration with Vector-Quantized Codebook. (arXiv:2312.08606v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08606</link>
<description rdf:parseType="Literal">&lt;p&gt;Night photography often struggles with challenges like low light and
blurring, stemming from dark environments and prolonged exposures. Current
methods either disregard priors and directly fitting end-to-end networks,
leading to inconsistent illumination, or rely on unreliable handcrafted priors
to constrain the network, thereby bringing the greater error to the final
result. We believe in the strength of data-driven high-quality priors and
strive to offer a reliable and consistent prior, circumventing the restrictions
of manual priors. In this paper, we propose Clearer Night Image Restoration
with Vector-Quantized Codebook (VQCNIR) to achieve remarkable and consistent
restoration outcomes on real-world and synthetic benchmarks. To ensure the
faithful restoration of details and illumination, we propose the incorporation
of two essential modules: the Adaptive Illumination Enhancement Module (AIEM)
and the Deformable Bi-directional Cross-Attention (DBCA) module. The AIEM
leverages the inter-channel correlation of features to dynamically maintain
illumination consistency between degraded features and high-quality codebook
features. Meanwhile, the DBCA module effectively integrates texture and
structural information through bi-directional cross-attention and deformable
convolution, resulting in enhanced fine-grained detail and structural fidelity
across parallel decoders. Extensive experiments validate the remarkable
benefits of VQCNIR in enhancing image quality under low-light conditions,
showcasing its state-of-the-art performance on both synthetic and real-world
datasets. The code is available at https://github.com/AlexZou14/VQCNIR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1&quot;&gt;Wenbin Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1&quot;&gt;Hongxia Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1&quot;&gt;Tian Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Weipeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shasha Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hongsheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sixiang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08614">
<title>Factorization Vision Transformer: Modeling Long Range Dependency with Local Window Cost. (arXiv:2312.08614v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08614</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have astounding representational power but typically consume
considerable computation which is quadratic with image resolution. The
prevailing Swin transformer reduces computational costs through a local window
strategy. However, this strategy inevitably causes two drawbacks: (1) the local
window-based self-attention hinders global dependency modeling capability; (2)
recent studies point out that local windows impair robustness. To overcome
these challenges, we pursue a preferable trade-off between computational cost
and performance. Accordingly, we propose a novel factorization self-attention
mechanism (FaSA) that enjoys both the advantages of local window cost and
long-range dependency modeling capability. By factorizing the conventional
attention matrix into sparse sub-attention matrices, FaSA captures long-range
dependencies while aggregating mixed-grained information at a computational
cost equivalent to the local window-based self-attention. Leveraging FaSA, we
present the factorization vision transformer (FaViT) with a hierarchical
structure. FaViT achieves high performance and robustness, with linear
computational complexity concerning input image spatial resolution. Extensive
experiments have shown FaViT&apos;s advanced performance in classification and
downstream tasks. Furthermore, it also exhibits strong model robustness to
corrupted and biased data and hence demonstrates benefits in favor of practical
applications. In comparison to the baseline model Swin-T, our FaViT-B2
significantly improves classification accuracy by 1% and robustness by 7%,
while reducing model parameters by 14%. Our code will soon be publicly
available at https://github.com/q2479036243/FaViT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1&quot;&gt;Haolin Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Daquan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tingfa Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_Z/0/1/0/all/0/1&quot;&gt;Ziyang Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08624">
<title>Mixed Reality Communication for Medical Procedures: Teaching the Placement of a Central Venous Catheter. (arXiv:2312.08624v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08624</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical procedures are an essential part of healthcare delivery, and the
acquisition of procedural skills is a critical component of medical education.
Unfortunately, procedural skill is not evenly distributed among medical
providers. Skills may vary within departments or institutions, and across
geographic regions, depending on the provider&apos;s training and ongoing
experience. We present a mixed reality real-time communication system to
increase access to procedural skill training and to improve remote emergency
assistance. Our system allows a remote expert to guide a local operator through
a medical procedure. RGBD cameras capture a volumetric view of the local scene
including the patient, the operator, and the medical equipment. The volumetric
capture is augmented onto the remote expert&apos;s view to allow the expert to
spatially guide the local operator using visual and verbal instructions. We
evaluated our mixed reality communication system in a study in which experts
teach the ultrasound-guided placement of a central venous catheter (CVC) to
students in a simulation setting. The study compares state-of-the-art video
communication against our system. The results indicate that our system enhances
and offers new possibilities for visual communication compared to video
teleconference-based training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rebol_M/0/1/0/all/0/1&quot;&gt;Manuel Rebol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pietroszek_K/0/1/0/all/0/1&quot;&gt;Krzysztof Pietroszek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranniger_C/0/1/0/all/0/1&quot;&gt;Claudia Ranniger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hood_C/0/1/0/all/0/1&quot;&gt;Colton Hood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rutenberg_A/0/1/0/all/0/1&quot;&gt;Adam Rutenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sikka_N/0/1/0/all/0/1&quot;&gt;Neal Sikka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;David Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gutl_C/0/1/0/all/0/1&quot;&gt;Christian G&amp;#xfc;tl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08628">
<title>YOLO-OB: An improved anchor-free real-time multiscale colon polyp detector in colonoscopy. (arXiv:2312.08628v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08628</link>
<description rdf:parseType="Literal">&lt;p&gt;Colon cancer is expected to become the second leading cause of cancer death
in the United States in 2023. Although colonoscopy is one of the most effective
methods for early prevention of colon cancer, up to 30% of polyps may be missed
by endoscopists, thereby increasing patients&apos; risk of developing colon cancer.
Though deep neural networks have been proven to be an effective means of
enhancing the detection rate of polyps. However, the variation of polyp size
brings the following problems: (1) it is difficult to design an efficient and
sufficient multi-scale feature fusion structure; (2) matching polyps of
different sizes with fixed-size anchor boxes is a hard challenge. These
problems reduce the performance of polyp detection and also lower the model&apos;s
training and detection efficiency. To address these challenges, this paper
proposes a new model called YOLO-OB. Specifically, we developed a bidirectional
multiscale feature fusion structure, BiSPFPN, which could enhance the feature
fusion capability across different depths of a CNN. We employed the ObjectBox
detection head, which used a center-based anchor-free box regression strategy
that could detect polyps of different sizes on feature maps of any scale.
Experiments on the public dataset SUN and the self-collected colon polyp
dataset Union demonstrated that the proposed model significantly improved
various performance metrics of polyp detection, especially the recall rate.
Compared to the state-of-the-art results on the public dataset SUN, the
proposed method achieved a 6.73% increase on recall rate from 91.5% to 98.23%.
Furthermore, our YOLO-OB was able to achieve real-time polyp detection at a
speed of 39 frames per second using a RTX3090 graphics card. The implementation
of this paper can be found here: https://github.com/seanyan62/YOLO-OB.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_E/0/1/0/all/0/1&quot;&gt;Enmin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1&quot;&gt;Guangzhi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yunfeng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dongming Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1&quot;&gt;Bowen Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xianyuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08631">
<title>Semi-supervised Semantic Segmentation Meets Masked Modeling:Fine-grained Locality Learning Matters in Consistency Regularization. (arXiv:2312.08631v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08631</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised semantic segmentation aims to utilize limited labeled images
and abundant unlabeled images to achieve label-efficient learning, wherein the
weak-to-strong consistency regularization framework, popularized by FixMatch,
is widely used as a benchmark scheme. Despite its effectiveness, we observe
that such scheme struggles with satisfactory segmentation for the local
regions. This can be because it originally stems from the image classification
task and lacks specialized mechanisms to capture fine-grained local semantics
that prioritizes in dense prediction. To address this issue, we propose a novel
framework called \texttt{MaskMatch}, which enables fine-grained locality
learning to achieve better dense segmentation. On top of the original
teacher-student framework, we design a masked modeling proxy task that
encourages the student model to predict the segmentation given the unmasked
image patches (even with 30\% only) and enforces the predictions to be
consistent with pseudo-labels generated by the teacher model using the complete
image. Such design is motivated by the intuition that if the predictions are
more consistent given insufficient neighboring information, stronger
fine-grained locality perception is achieved. Besides, recognizing the
importance of reliable pseudo-labels in the above locality learning and the
original consistency learning scheme, we design a multi-scale ensembling
strategy that considers context at different levels of abstraction for
pseudo-label generation. Extensive experiments on benchmark datasets
demonstrate the superiority of our method against previous approaches and its
plug-and-play flexibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1&quot;&gt;Wentao Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhe Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Jiangpeng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zihan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_R/0/1/0/all/0/1&quot;&gt;Raymond Kai-yu Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jianhua Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08636">
<title>MmAP : Multi-modal Alignment Prompt for Cross-domain Multi-task Learning. (arXiv:2312.08636v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08636</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Task Learning (MTL) is designed to train multiple correlated tasks
simultaneously, thereby enhancing the performance of individual tasks.
Typically, a multi-task network structure consists of a shared backbone and
task-specific decoders. However, the complexity of the decoders increases with
the number of tasks. To tackle this challenge, we integrate the decoder-free
vision-language model CLIP, which exhibits robust zero-shot generalization
capability. Recently, parameter-efficient transfer learning methods have been
extensively explored with CLIP for adapting to downstream tasks, where prompt
tuning showcases strong potential. Nevertheless, these methods solely fine-tune
a single modality (text or visual), disrupting the modality structure of CLIP.
In this paper, we first propose Multi-modal Alignment Prompt (MmAP) for CLIP,
which aligns text and visual modalities during fine-tuning process. Building
upon MmAP, we develop an innovative multi-task prompt learning framework. On
the one hand, to maximize the complementarity of tasks with high similarity, we
utilize a gradient-driven task grouping method that partitions tasks into
several disjoint groups and assign a group-shared MmAP to each group. On the
other hand, to preserve the unique characteristics of each task, we assign an
task-specific MmAP to each task. Comprehensive experiments on two large
multi-task learning datasets demonstrate that our method achieves significant
performance improvements compared to full fine-tuning while only utilizing
approximately 0.09% of trainable parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_Y/0/1/0/all/0/1&quot;&gt;Yi Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;Junlong Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Ke Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1&quot;&gt;Shouhong Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08644">
<title>Generative Model-based Feature Knowledge Distillation for Action Recognition. (arXiv:2312.08644v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08644</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge distillation (KD), a technique widely employed in computer vision,
has emerged as a de facto standard for improving the performance of small
neural networks. However, prevailing KD-based approaches in video tasks
primarily focus on designing loss functions and fusing cross-modal information.
This overlooks the spatial-temporal feature semantics, resulting in limited
advancements in model compression. Addressing this gap, our paper introduces an
innovative knowledge distillation framework, with the generative model for
training a lightweight student model. In particular, the framework is organized
into two steps: the initial phase is Feature Representation, wherein a
generative model-based attention module is trained to represent feature
semantics; Subsequently, the Generative-based Feature Distillation phase
encompasses both Generative Distillation and Attention Distillation, with the
objective of transferring attention-based feature semantics with the generative
model. The efficacy of our approach is demonstrated through comprehensive
experiments on diverse popular datasets, proving considerable enhancements in
video action recognition task. Moreover, the effectiveness of our proposed
framework is validated in the context of more intricate video action detection
task. Our code is available at https://github.com/aaai-24/Generative-based-KD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guiqin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Peng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yanjiang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Cong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shusen Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08648">
<title>CLIP-guided Federated Learning on Heterogeneous and Long-Tailed Data. (arXiv:2312.08648v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08648</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) provides a decentralized machine learning paradigm
where a server collaborates with a group of clients to learn a global model
without accessing the clients&apos; data. User heterogeneity is a significant
challenge for FL, which together with the class-distribution imbalance further
enhances the difficulty of FL. Great progress has been made in large
vision-language models, such as Contrastive Language-Image Pre-training (CLIP),
which paves a new way for image classification and object recognition. Inspired
by the success of CLIP on few-shot and zero-shot learning, we use CLIP to
optimize the federated learning between server and client models under its
vision-language supervision. It is promising to mitigate the user heterogeneity
and class-distribution balance due to the powerful cross-modality
representation and rich open-vocabulary prior knowledge. In this paper, we
propose the CLIP-guided FL (CLIP2FL) method on heterogeneous and long-tailed
data. In CLIP2FL, the knowledge of the off-the-shelf CLIP model is transferred
to the client-server models, and a bridge is built between the client and
server. Specifically, for client-side learning, knowledge distillation is
conducted between client models and CLIP to improve the ability of client-side
feature representation. For server-side learning, in order to mitigate the
heterogeneity and class-distribution imbalance, we generate federated features
to retrain the server model. A prototype contrastive learning with the
supervision of the text encoder of CLIP is introduced to generate federated
features depending on the client-side gradients, and they are used to retrain a
balanced server classifier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jiangming Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shanshan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xiangbo Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yuan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1&quot;&gt;Yanyun Qu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08650">
<title>PhyOT: Physics-informed object tracking in surveillance cameras. (arXiv:2312.08650v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08650</link>
<description rdf:parseType="Literal">&lt;p&gt;While deep learning has been very successful in computer vision, real world
operating conditions such as lighting variation, background clutter, or
occlusion hinder its accuracy across several tasks. Prior work has shown that
hybrid models -- combining neural networks and heuristics/algorithms -- can
outperform vanilla deep learning for several computer vision tasks, such as
classification or tracking. We consider the case of object tracking, and
evaluate a hybrid model (PhyOT) that conceptualizes deep neural networks as
``sensors&apos;&apos; in a Kalman filter setup, where prior knowledge, in the form of
Newtonian laws of motion, is used to fuse sensor observations and to perform
improved estimations. Our experiments combine three neural networks, performing
position, indirect velocity and acceleration estimation, respectively, and
evaluate such a formulation on two benchmark datasets: a warehouse security
camera dataset that we collected and annotated and a traffic camera open
dataset. Results suggest that our PhyOT can track objects in extreme conditions
that the state-of-the-art deep neural networks fail while its performance in
general cases does not degrade significantly from that of existing deep
learning approaches. Results also suggest that our PhyOT components are
generalizable and transferable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamtue_K/0/1/0/all/0/1&quot;&gt;Kawisorn Kamtue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moura_J/0/1/0/all/0/1&quot;&gt;Jose M.F. Moura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sangpetch_O/0/1/0/all/0/1&quot;&gt;Orathai Sangpetch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_P/0/1/0/all/0/1&quot;&gt;Paulo Garcia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08653">
<title>A Simple Knowledge Distillation Framework for Open-world Object Detection. (arXiv:2312.08653v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08653</link>
<description rdf:parseType="Literal">&lt;p&gt;Open World Object Detection (OWOD) is a novel computer vision task with a
considerable challenge, bridging the gap between classic object detection (OD)
benchmarks and real-world object detection. In addition to detecting and
classifying seen/known objects, OWOD algorithms are expected to localize all
potential unseen/unknown objects and incrementally learn them. The large
pre-trained vision-language grounding models (VLM,eg, GLIP) have rich knowledge
about the open world, but are limited by text prompts and cannot localize
indescribable objects. However, there are many detection scenarios which
pre-defined language descriptions are unavailable during inference. In this
paper, we attempt to specialize the VLM model for OWOD task by distilling its
open-world knowledge into a language-agnostic detector. Surprisingly, we
observe that the combination of a simple knowledge distillation approach and
the automatic pseudo-labeling mechanism in OWOD can achieve better performance
for unknown object detection, even with a small amount of data. Unfortunately,
knowledge distillation for unknown objects severely affects the learning of
detectors with conventional structures for known objects, leading to
catastrophic forgetting. To alleviate these problems, we propose the
down-weight loss function for knowledge distillation from vision-language to
single vision modality. Meanwhile, we decouple the learning of localization and
recognition to reduce the impact of category interactions of known and unknown
objects on the localization learning process. Comprehensive experiments
performed on MS-COCO and PASCAL VOC demonstrate the effectiveness of our
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shuailei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuefeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Ying Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jiaqi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xinyu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Peihao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Enming Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08659">
<title>On the Image-Based Detection of Tomato and Corn leaves Diseases : An in-depth comparative experiments. (arXiv:2312.08659v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08659</link>
<description rdf:parseType="Literal">&lt;p&gt;The research introduces a novel plant disease detection model based on
Convolutional Neural Networks (CNN) for plant image classification, marking a
significant contribution to image categorization. The innovative training
approach enables a streamlined and efficient system implementation. The model
classifies two distinct plant diseases into four categories, presenting a novel
technique for plant disease identification. In Experiment 1, Inception-V3,
Dense-Net-121, ResNet-101-V2, and Xception models were employed for CNN
training. The newly created plant disease image dataset includes 1963 tomato
plant images and 7316 corn plant images from the PlantVillage dataset. Of
these, 1374 tomato images and 5121 corn images were used for training, while
589 tomato images and 2195 corn images were used for testing/validation.
Results indicate that the Xception model outperforms the other three models,
yielding val_accuracy values of 95.08% and 92.21% for the tomato and corn
datasets, with corresponding val_loss values of 0.3108 and 0.4204,
respectively. In Experiment 2, CNN with Batch Normalization achieved disease
detection rates of approximately 99.89% in the training set and val_accuracy
values exceeding 97.52%, accompanied by a val_loss of 0.103. Experiment 3
employed a CNN architecture as the base model, introducing additional layers in
Model 2, skip connections in Model 3, and regularizations in Model 4. Detailed
experiment results and model efficiency are outlined in the paper&apos;s sub-section
1.5. Experiment 4 involved combining all corn and tomato images, utilizing
various models, including MobileNet (val_accuracy=86.73%), EfficientNetB0
(val_accuracy=93.973%), Xception (val_accuracy=74.91%), InceptionResNetV2
(val_accuracy=31.03%), and CNN (59.79%). Additionally, our proposed model
achieved a val_accuracy of 84.42%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yasin_A/0/1/0/all/0/1&quot;&gt;Affan Yasin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fatima_R/0/1/0/all/0/1&quot;&gt;Rubia Fatima&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08664">
<title>SPEAL: Skeletal Prior Embedded Attention Learning for Cross-Source Point Cloud Registration. (arXiv:2312.08664v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08664</link>
<description rdf:parseType="Literal">&lt;p&gt;Point cloud registration, a fundamental task in 3D computer vision, has
remained largely unexplored in cross-source point clouds and unstructured
scenes. The primary challenges arise from noise, outliers, and variations in
scale and density. However, neglected geometric natures of point clouds
restricts the performance of current methods. In this paper, we propose a novel
method termed SPEAL to leverage skeletal representations for effective learning
of intrinsic topologies of point clouds, facilitating robust capture of
geometric intricacy. Specifically, we design the Skeleton Extraction Module to
extract skeleton points and skeletal features in an unsupervised manner, which
is inherently robust to noise and density variances. Then, we propose the
Skeleton-Aware GeoTransformer to encode high-level skeleton-aware features. It
explicitly captures the topological natures and inter-point-cloud skeletal
correlations with the noise-robust and density-invariant skeletal
representations. Next, we introduce the Correspondence Dual-Sampler to
facilitate correspondences by augmenting the correspondence set with skeletal
correspondences. Furthermore, we construct a challenging novel large-scale
cross-source point cloud dataset named KITTI CrossSource for benchmarking
cross-source point cloud registration methods. Extensive quantitative and
qualitative experiments are conducted to demonstrate our approach&apos;s superiority
and robustness on both cross-source and same-source datasets. To the best of
our knowledge, our approach is the first to facilitate point cloud registration
with skeletal geometric priors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_K/0/1/0/all/0/1&quot;&gt;Kezheng Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1&quot;&gt;Maoji Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qingshan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1&quot;&gt;Chenglu Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1&quot;&gt;Siqi Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cheng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08673">
<title>Segment Beyond View: Handling Partially Missing Modality for Audio-Visual Semantic Segmentation. (arXiv:2312.08673v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08673</link>
<description rdf:parseType="Literal">&lt;p&gt;Augmented Reality (AR) devices, emerging as prominent mobile interaction
platforms, face challenges in user safety, particularly concerning oncoming
vehicles. While some solutions leverage onboard camera arrays, these cameras
often have limited field-of-view (FoV) with front or downward perspectives.
Addressing this, we propose a new out-of-view semantic segmentation task and
Segment Beyond View (SBV), a novel audio-visual semantic segmentation method.
SBV supplements the visual modality, which miss the information beyond FoV,
with the auditory information using a teacher-student distillation model
(Omni2Ego). The model consists of a vision teacher utilising panoramic
information, an auditory teacher with 8-channel audio, and an audio-visual
student that takes views with limited FoV and binaural audio as input and
produce semantic segmentation for objects outside FoV. SBV outperforms existing
models in comparative evaluations and shows a consistent performance across
varying FoV ranges and in monaural audio settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Renjie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dayoub_F/0/1/0/all/0/1&quot;&gt;Feras Dayoub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hsiang-Ting Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08675">
<title>AVA: Inconspicuous Attribute Variation-based Adversarial Attack bypassing DeepFake Detection. (arXiv:2312.08675v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08675</link>
<description rdf:parseType="Literal">&lt;p&gt;While DeepFake applications are becoming popular in recent years, their
abuses pose a serious privacy threat. Unfortunately, most related detection
algorithms to mitigate the abuse issues are inherently vulnerable to
adversarial attacks because they are built atop DNN-based classification
models, and the literature has demonstrated that they could be bypassed by
introducing pixel-level perturbations. Though corresponding mitigation has been
proposed, we have identified a new attribute-variation-based adversarial attack
(AVA) that perturbs the latent space via a combination of Gaussian prior and
semantic discriminator to bypass such mitigation. It perturbs the semantics in
the attribute space of DeepFake images, which are inconspicuous to human beings
(e.g., mouth open) but can result in substantial differences in DeepFake
detection. We evaluate our proposed AVA attack on nine state-of-the-art
DeepFake detection algorithms and applications. The empirical results
demonstrate that AVA attack defeats the state-of-the-art black box attacks
against DeepFake detectors and achieves more than a 95% success rate on two
commercial DeepFake detectors. Moreover, our human study indicates that
AVA-generated DeepFake images are often imperceptible to humans, which presents
huge security and privacy concerns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xiangtao Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Li Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Shanqing Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_L/0/1/0/all/0/1&quot;&gt;Lei Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qingchuan Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08679">
<title>A Local Appearance Model for Volumetric Capture of Diverse Hairstyle. (arXiv:2312.08679v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08679</link>
<description rdf:parseType="Literal">&lt;p&gt;Hair plays a significant role in personal identity and appearance, making it
an essential component of high-quality, photorealistic avatars. Existing
approaches either focus on modeling the facial region only or rely on
personalized models, limiting their generalizability and scalability. In this
paper, we present a novel method for creating high-fidelity avatars with
diverse hairstyles. Our method leverages the local similarity across different
hairstyles and learns a universal hair appearance prior from multi-view
captures of hundreds of people. This prior model takes 3D-aligned features as
input and generates dense radiance fields conditioned on a sparse point cloud
with color. As our model splits different hairstyles into local primitives and
builds prior at that level, it is capable of handling various hair topologies.
Through experiments, we demonstrate that our model captures a diverse range of
hairstyles and generalizes well to challenging new hairstyles. Empirical
results show that our method improves the state-of-the-art approaches in
capturing and generating photorealistic, personalized avatars with complete
hair.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_G/0/1/0/all/0/1&quot;&gt;Giljoo Nam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bozic_A/0/1/0/all/0/1&quot;&gt;Aljaz Bozic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1&quot;&gt;Chen Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saragih_J/0/1/0/all/0/1&quot;&gt;Jason Saragih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1&quot;&gt;Michael Zollhoefer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hodgins_J/0/1/0/all/0/1&quot;&gt;Jessica Hodgins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08692">
<title>SpectralNeRF: Physically Based Spectral Rendering with Neural Radiance Field. (arXiv:2312.08692v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08692</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose SpectralNeRF, an end-to-end Neural Radiance Field
(NeRF)-based architecture for high-quality physically based rendering from a
novel spectral perspective. We modify the classical spectral rendering into two
main steps, 1) the generation of a series of spectrum maps spanning different
wavelengths, 2) the combination of these spectrum maps for the RGB output. Our
SpectralNeRF follows these two steps through the proposed multi-layer
perceptron (MLP)-based architecture (SpectralMLP) and Spectrum Attention UNet
(SAUNet). Given the ray origin and the ray direction, the SpectralMLP
constructs the spectral radiance field to obtain spectrum maps of novel views,
which are then sent to the SAUNet to produce RGB images of white-light
illumination. Applying NeRF to build up the spectral rendering is a more
physically-based way from the perspective of ray-tracing. Further, the spectral
radiance fields decompose difficult scenes and improve the performance of
NeRF-based methods. Comprehensive experimental results demonstrate the proposed
SpectralNeRF is superior to recent NeRF-based methods when synthesizing new
views on synthetic and real datasets. The codes and datasets are available at
https://github.com/liru0126/SpectralNeRF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ru Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guanghui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shengping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1&quot;&gt;Bing Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shuaicheng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08695">
<title>CPST: Comprehension-Preserving Style Transfer for Multi-Modal Narratives. (arXiv:2312.08695v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08695</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the challenges of style transfer in multi-modal visual
narratives. Among static visual narratives such as comics and manga, there are
distinct visual styles in terms of presentation. They include style features
across multiple dimensions, such as panel layout, size, shape, and color. They
include both visual and text media elements. The layout of both text and media
elements is also significant in terms of narrative communication. The
sequential transitions between panels are where readers make inferences about
the narrative world. These feature differences provide an interesting challenge
for style transfer in which there are distinctions between the processing of
features for each modality. We introduce the notion of comprehension-preserving
style transfer (CPST) in such multi-modal domains. CPST requires not only
traditional metrics of style transfer but also metrics of narrative
comprehension. To spur further research in this area, we present an annotated
dataset of comics and manga and an initial set of algorithms that utilize
separate style transfer modules for the visual, textual, and layout parameters.
To test whether the style transfer preserves narrative semantics, we evaluate
this algorithm through visual story cloze tests inspired by work in
computational cognition of narrative systems. Understanding the connection
between style and narrative semantics provides insight for applications ranging
from informational brochure designs to data storytelling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yi-Chun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jhala_A/0/1/0/all/0/1&quot;&gt;Arnav Jhala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08697">
<title>Incomplete Contrastive Multi-View Clustering with High-Confidence Guiding. (arXiv:2312.08697v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08697</link>
<description rdf:parseType="Literal">&lt;p&gt;Incomplete multi-view clustering becomes an important research problem, since
multi-view data with missing values are ubiquitous in real-world applications.
Although great efforts have been made for incomplete multi-view clustering,
there are still some challenges: 1) most existing methods didn&apos;t make full use
of multi-view information to deal with missing values; 2) most methods just
employ the consistent information within multi-view data but ignore the
complementary information; 3) For the existing incomplete multi-view clustering
methods, incomplete multi-view representation learning and clustering are
treated as independent processes, which leads to performance gap. In this work,
we proposed a novel Incomplete Contrastive Multi-View Clustering method with
high-confidence guiding (ICMVC). Firstly, we proposed a multi-view consistency
relation transfer plus graph convolutional network to tackle missing values
problem. Secondly, instance-level attention fusion and high-confidence guiding
are proposed to exploit the complementary information while instance-level
contrastive learning for latent representation is designed to employ the
consistent information. Thirdly, an end-to-end framework is proposed to
integrate multi-view missing values handling, multi-view representation
learning and clustering assignment for joint optimization. Experiments compared
with state-of-the-art approaches demonstrated the effectiveness and superiority
of our method. Our code is publicly available at
https://github.com/liunian-Jay/ICMVC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_G/0/1/0/all/0/1&quot;&gt;Guoqing Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_D/0/1/0/all/0/1&quot;&gt;Dianhui Chu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08700">
<title>RdimKD: Generic Distillation Paradigm by Dimensionality Reduction. (arXiv:2312.08700v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08700</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge Distillation (KD) emerges as one of the most promising compression
technologies to run advanced deep neural networks on resource-limited devices.
In order to train a small network (student) under the guidance of a large
network (teacher), the intuitive method is regularizing the feature maps or
logits of the student using the teacher&apos;s information. However, existing
methods either over-restrict the student to learn all information from the
teacher, which lead to some bad local minimum, or use various fancy and
elaborate modules to process and align features, which are complex and lack
generality. In this work, we proposed an abstract and general paradigm for the
KD task, referred to as DIMensionality Reduction KD (RdimKD), which solely
relies on dimensionality reduction, with a very minor modification to naive L2
loss. RdimKD straightforwardly utilizes a projection matrix to project both the
teacher&apos;s and student&apos;s feature maps onto a low-dimensional subspace, which are
then optimized during training. RdimKD achieves the goal in the simplest way
that not only does the student get valuable information from the teacher, but
it also ensures sufficient flexibility to adapt to the student&apos;s low-capacity
reality. Our extensive empirical findings indicate the effectiveness of RdimKD
across various learning tasks and diverse network architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yiqian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1&quot;&gt;Haotong Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_V/0/1/0/all/0/1&quot;&gt;Van Tung Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shouda Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08704">
<title>PairingNet: A Learning-based Pair-searching and -matching Network for Image Fragments. (arXiv:2312.08704v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08704</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a learning-based image fragment pair-searching and
-matching approach to solve the challenging restoration problem. Existing works
use rule-based methods to match similar contour shapes or textures, which are
always difficult to tune hyperparameters for extensive data and computationally
time-consuming. Therefore, we propose a neural network that can effectively
utilize neighbor textures with contour shape information to fundamentally
improve performance. First, we employ a graph-based network to extract the
local contour and texture features of fragments. Then, for the pair-searching
task, we adopt a linear transformer-based module to integrate these local
features and use contrastive loss to encode the global features of each
fragment. For the pair-matching task, we design a weighted fusion module to
dynamically fuse extracted local contour and texture features, and formulate a
similarity matrix for each pair of fragments to calculate the matching score
and infer the adjacent segment of contours. To faithfully evaluate our proposed
network, we created a new image fragment dataset through an algorithm we
designed that tears complete images into irregular fragments. The experimental
results show that our proposed network achieves excellent pair-searching
accuracy, reduces matching errors, and significantly reduces computational
time. Details, sourcecode, and data are available in our supplementary
material.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1&quot;&gt;Rixin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_D/0/1/0/all/0/1&quot;&gt;Ding Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_H/0/1/0/all/0/1&quot;&gt;Honglin Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chuntao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08720">
<title>Panel Transitions for Genre Analysis in Visual Narratives. (arXiv:2312.08720v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08720</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding how humans communicate and perceive narratives is important for
media technology research and development. This is particularly important in
current times when there are tools and algorithms that are easily available for
amateur users to create high-quality content. Narrative media develops over
time a set of recognizable patterns of features across similar artifacts. Genre
is one such grouping of artifacts for narrative media with similar patterns,
tropes, and story structures. While much work has been done on genre-based
classifications in text and video, we present a novel approach to do a
multi-modal analysis of genre based on comics and manga-style visual
narratives. We present a systematic feature analysis of an annotated dataset
that includes a variety of western and eastern visual books with annotations
for high-level narrative patterns. We then present a detailed analysis of the
contributions of high-level features to genre classification for this medium.
We highlight some of the limitations and challenges of our existing
computational approaches in modeling subjective labels. Our contributions to
the community are: a dataset of annotated manga books, a multi-modal analysis
of visual panels and text in a constrained and popular medium through
high-level features, and a systematic process for incorporating subjective
narrative patterns in computational models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yi-Chun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jhala_A/0/1/0/all/0/1&quot;&gt;Arnav Jhala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08730">
<title>Towards Robust and Expressive Whole-body Human Pose and Shape Estimation. (arXiv:2312.08730v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08730</link>
<description rdf:parseType="Literal">&lt;p&gt;Whole-body pose and shape estimation aims to jointly predict different
behaviors (e.g., pose, hand gesture, facial expression) of the entire human
body from a monocular image. Existing methods often exhibit degraded
performance under the complexity of in-the-wild scenarios. We argue that the
accuracy and reliability of these models are significantly affected by the
quality of the predicted \textit{bounding box}, e.g., the scale and alignment
of body parts. The natural discrepancy between the ideal bounding box
annotations and model detection results is particularly detrimental to the
performance of whole-body pose and shape estimation. In this paper, we propose
a novel framework to enhance the robustness of whole-body pose and shape
estimation. Our framework incorporates three new modules to address the above
challenges from three perspectives: \textbf{1) Localization Module} enhances
the model&apos;s awareness of the subject&apos;s location and semantics within the image
space. \textbf{2) Contrastive Feature Extraction Module} encourages the model
to be invariant to robust augmentations by incorporating contrastive loss with
dedicated positive samples. \textbf{3) Pixel Alignment Module} ensures the
reprojected mesh from the predicted camera and body model parameters are
accurate and pixel-aligned. We perform comprehensive experiments to demonstrate
the effectiveness of our proposed framework on body, hands, face and whole-body
benchmarks. Codebase is available at
\url{https://github.com/robosmplx/robosmplx}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+EnPang_H/0/1/0/all/0/1&quot;&gt;Hui EnPang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zhongang Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_Q/0/1/0/all/0/1&quot;&gt;Qingyi Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhonghua Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08733">
<title>VMT-Adapter: Parameter-Efficient Transfer Learning for Multi-Task Dense. (arXiv:2312.08733v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08733</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale pre-trained models have achieved remarkable success in various
computer vision tasks. A standard approach to leverage these models is to
fine-tune all model parameters for downstream tasks, which poses challenges in
terms of computational and storage costs. Recently, inspired by Natural
Language Processing (NLP), parameter-efficient transfer learning has been
successfully applied to vision tasks. However, most existing techniques
primarily focus on single-task adaptation, and despite limited research on
multi-task adaptation, these methods often exhibit suboptimal training and
inference efficiency. In this paper, we first propose an once-for-all Vision
Multi-Task Adapter (VMT-Adapter), which strikes approximately O(1) training and
inference efficiency w.r.t task number. Concretely, VMT-Adapter shares the
knowledge from multiple tasks to enhance cross-task interaction while preserves
task-specific knowledge via independent knowledge extraction modules. Notably,
since task-specific modules require few parameters, VMT-Adapter can handle an
arbitrary number of tasks with a negligible increase of trainable parameters.
We also propose VMT-Adapter-Lite, which further reduces the trainable
parameters by learning shared parameters between down- and up-projections.
Extensive experiments on four dense scene understanding tasks demonstrate the
superiority of VMT-Adapter(-Lite), achieving a 3.96%(1.34%) relative
improvement compared to single-task full fine-tuning, while utilizing merely
~1% (0.36%) trainable parameters of the pre-trained model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_Y/0/1/0/all/0/1&quot;&gt;Yi Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;Junlong Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhiwen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Ke Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08735">
<title>Polyper: Boundary Sensitive Polyp Segmentation. (arXiv:2312.08735v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08735</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new boundary sensitive framework for polyp segmentation, called
Polyper. Our method is motivated by a clinical approach that seasoned medical
practitioners often leverage the inherent features of interior polyp regions to
tackle blurred boundaries.Inspired by this, we propose explicitly leveraging
polyp regions to bolster the model&apos;s boundary discrimination capability while
minimizing computation. Our approach first extracts boundary and polyp regions
from the initial segmentation map through morphological operators. Then, we
design the boundary sensitive attention that concentrates on augmenting the
features near the boundary regions using the interior polyp regions&apos;s
characteristics to generate good segmentation results. Our proposed method can
be seamlessly integrated with classical encoder networks, like ResNet-50,
MiT-B1, and Swin Transformer. To evaluate the effectiveness of Polyper, we
conduct experiments on five publicly available challenging datasets, and
receive state-of-the-art performance on all of them. Code is available at
https://github.com/haoshao-nku/medical_seg.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1&quot;&gt;Hao Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1&quot;&gt;Qibin Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08740">
<title>Learning a Low-Rank Feature Representation: Achieving Better Trade-Off between Stability and Plasticity in Continual Learning. (arXiv:2312.08740v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08740</link>
<description rdf:parseType="Literal">&lt;p&gt;In continual learning, networks confront a trade-off between stability and
plasticity when trained on a sequence of tasks. To bolster plasticity without
sacrificing stability, we propose a novel training algorithm called LRFR. This
approach optimizes network parameters in the null space of the past tasks&apos;
feature representation matrix to guarantee the stability. Concurrently, we
judiciously select only a subset of neurons in each layer of the network while
training individual tasks to learn the past tasks&apos; feature representation
matrix in low-rank. This increases the null space dimension when designing
network parameters for subsequent tasks, thereby enhancing the plasticity.
Using CIFAR-100 and TinyImageNet as benchmark datasets for continual learning,
the proposed approach consistently outperforms state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhenrong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yi Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yik-Chung Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08744">
<title>GOEnFusion: Gradient Origin Encodings for 3D Forward Diffusion Models. (arXiv:2312.08744v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08744</link>
<description rdf:parseType="Literal">&lt;p&gt;The recently introduced Forward-Diffusion method allows to train a 3D
diffusion model using only 2D images for supervision. However, it does not
easily generalise to different 3D representations and requires a
computationally expensive auto-regressive sampling process to generate the
underlying 3D scenes. In this paper, we propose GOEn: Gradient Origin Encoding
(pronounced &quot;gone&quot;). GOEn can encode input images into any type of 3D
representation without the need to use a pre-trained image feature extractor.
It can also handle single, multiple or no source view(s) alike, by design, and
tries to maximise the information transfer from the views to the encodings. Our
proposed GOEnFusion model pairs GOEn encodings with a realisation of the
Forward-Diffusion model which addresses the limitations of the vanilla
Forward-Diffusion realisation. We evaluate how much information the GOEn
mechanism transfers to the encoded representations, and how well it captures
the prior distribution over the underlying 3D scenes, through the lens of a
partial AutoEncoder. Lastly, the efficacy of the GOEnFusion model is evaluated
on the recently proposed OmniObject3D dataset while comparing to the
state-of-the-art Forward and non-Forward-Diffusion models and other 3D
generative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karnewar_A/0/1/0/all/0/1&quot;&gt;Animesh Karnewar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1&quot;&gt;Andrea Vedaldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1&quot;&gt;Niloy J. Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Novotny_D/0/1/0/all/0/1&quot;&gt;David Novotny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08746">
<title>DreamDrone. (arXiv:2312.08746v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08746</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce DreamDrone, an innovative method for generating unbounded
flythrough scenes from textual prompts. Central to our method is a novel
feature-correspondence-guidance diffusion process, which utilizes the strong
correspondence of intermediate features in the diffusion model. Leveraging this
guidance strategy, we further propose an advanced technique for editing the
intermediate latent code, enabling the generation of subsequent novel views
with geometric consistency. Extensive experiments reveal that DreamDrone
significantly surpasses existing methods, delivering highly authentic scene
generation with exceptional visual quality. This approach marks a significant
step in zero-shot perpetual view generation from textual prompts, enabling the
creation of diverse scenes, including natural landscapes like oases and caves,
as well as complex urban settings such as Lego-style street views. Our code is
publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_H/0/1/0/all/0/1&quot;&gt;Hanyang Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1&quot;&gt;Dongze Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_M/0/1/0/all/0/1&quot;&gt;Michael Bi Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinchao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08754">
<title>UniDream: Unifying Diffusion Priors for Relightable Text-to-3D Generation. (arXiv:2312.08754v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08754</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in text-to-3D generation technology have significantly
advanced the conversion of textual descriptions into imaginative
well-geometrical and finely textured 3D objects. Despite these developments, a
prevalent limitation arises from the use of RGB data in diffusion or
reconstruction models, which often results in models with inherent lighting and
shadows effects that detract from their realism, thereby limiting their
usability in applications that demand accurate relighting capabilities. To
bridge this gap, we present UniDream, a text-to-3D generation framework by
incorporating unified diffusion priors. Our approach consists of three main
components: (1) a dual-phase training process to get albedo-normal aligned
multi-view diffusion and reconstruction models, (2) a progressive generation
procedure for geometry and albedo-textures based on Score Distillation Sample
(SDS) using the trained reconstruction and diffusion models, and (3) an
innovative application of SDS for finalizing PBR generation while keeping a
fixed albedo based on Stable Diffusion model. Extensive evaluations demonstrate
that UniDream surpasses existing methods in generating 3D objects with clearer
albedo textures, smoother surfaces, enhanced realism, and superior relighting
capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zexiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yangguang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Youtian Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1&quot;&gt;Sida Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yan-Pei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xiaojuan Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaoshui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1&quot;&gt;Ding Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08760">
<title>CF-NeRF: Camera Parameter Free Neural Radiance Fields with Incremental Learning. (arXiv:2312.08760v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08760</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRF) have demonstrated impressive performance in
novel view synthesis. However, NeRF and most of its variants still rely on
traditional complex pipelines to provide extrinsic and intrinsic camera
parameters, such as COLMAP. Recent works, like NeRFmm, BARF, and L2G-NeRF,
directly treat camera parameters as learnable and estimate them through
differential volume rendering. However, these methods work for forward-looking
scenes with slight motions and fail to tackle the rotation scenario in
practice. To overcome this limitation, we propose a novel \underline{c}amera
parameter \underline{f}ree neural radiance field (CF-NeRF), which incrementally
reconstructs 3D representations and recovers the camera parameters inspired by
incremental structure from motion (SfM). Given a sequence of images, CF-NeRF
estimates the camera parameters of images one by one and reconstructs the scene
through initialization, implicit localization, and implicit optimization. To
evaluate our method, we use a challenging real-world dataset NeRFBuster which
provides 12 scenes under complex trajectories. Results demonstrate that CF-NeRF
is robust to camera rotation and achieves state-of-the-art results without
providing prior information and constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1&quot;&gt;Qingsong Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Kaiyong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1&quot;&gt;Xiaowen Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_F/0/1/0/all/0/1&quot;&gt;Fei Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08764">
<title>CattleEyeView: A Multi-task Top-down View Cattle Dataset for Smarter Precision Livestock Farming. (arXiv:2312.08764v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08764</link>
<description rdf:parseType="Literal">&lt;p&gt;Cattle farming is one of the important and profitable agricultural
industries. Employing intelligent automated precision livestock farming systems
that can count animals, track the animals and their poses will raise
productivity and significantly reduce the heavy burden on its already limited
labor pool. To achieve such intelligent systems, a large cattle video dataset
is essential in developing and training such models. However, many current
animal datasets are tailored to few tasks or other types of animals, which
result in poorer model performance when applied to cattle. Moreover, they do
not provide top-down views of cattle. To address such limitations, we introduce
CattleEyeView dataset, the first top-down view multi-task cattle video dataset
for a variety of inter-related tasks (i.e., counting, detection, pose
estimation, tracking, instance segmentation) that are useful to count the
number of cows and assess their growth and well-being. The dataset contains 753
distinct top-down cow instances in 30,703 frames (14 video sequences). We
perform benchmark experiments to evaluate the model&apos;s performance for each
task. The dataset and codes can be found at
https://github.com/AnimalEyeQ/CattleEyeView.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_K/0/1/0/all/0/1&quot;&gt;Kian Eng Ong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Retta_S/0/1/0/all/0/1&quot;&gt;Sivaji Retta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_R/0/1/0/all/0/1&quot;&gt;Ramarajulu Srinivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1&quot;&gt;Shawn Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08766">
<title>A Dual Convolutional Neural Network Pipeline for Melanoma Diagnostics and Prognostics. (arXiv:2312.08766v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.08766</link>
<description rdf:parseType="Literal">&lt;p&gt;Melanoma is a type of cancer that begins in the cells controlling the pigment
of the skin, and it is often referred to as the most dangerous skin cancer.
Diagnosing melanoma can be time-consuming, and a recent increase in melanoma
incidents indicates a growing demand for a more efficient diagnostic process.
This paper presents a pipeline for melanoma diagnostics, leveraging two
convolutional neural networks, a diagnosis, and a prognosis model. The
diagnostic model is responsible for localizing malignant patches across whole
slide images and delivering a patient-level diagnosis as malignant or benign.
Further, the prognosis model utilizes the diagnostic model&apos;s output to provide
a patient-level prognosis as good or bad. The full pipeline has an F1 score of
0.79 when tested on data from the same distribution as it was trained on.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bo_Sande_M/0/1/0/all/0/1&quot;&gt;Marie B&amp;#xf8;-Sande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Benjaminsen_E/0/1/0/all/0/1&quot;&gt;Edvin Benjaminsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kanwal_N/0/1/0/all/0/1&quot;&gt;Neel Kanwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fuster_S/0/1/0/all/0/1&quot;&gt;Saul Fuster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hardardottir_H/0/1/0/all/0/1&quot;&gt;Helga Hardardottir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lundal_I/0/1/0/all/0/1&quot;&gt;Ingrid Lundal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Janssen_E/0/1/0/all/0/1&quot;&gt;Emiel A.M. Janssen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Engan_K/0/1/0/all/0/1&quot;&gt;Kjersti Engan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08768">
<title>Local Conditional Controlling for Text-to-Image Diffusion Models. (arXiv:2312.08768v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08768</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have exhibited impressive prowess in the text-to-image task.
Recent methods add image-level controls, e.g., edge and depth maps, to
manipulate the generation process together with text prompts to obtain desired
images. This controlling process is globally operated on the entire image,
which limits the flexibility of control regions. In this paper, we introduce a
new simple yet practical task setting: local control. It focuses on controlling
specific local areas according to user-defined image conditions, where the rest
areas are only conditioned by the original text prompt. This manner allows the
users to flexibly control the image generation in a fine-grained way. However,
it is non-trivial to achieve this goal. The naive manner of directly adding
local conditions may lead to the local control dominance problem. To mitigate
this problem, we propose a training-free method that leverages the updates of
noised latents and parameters in the cross-attention map during the denosing
process to promote concept generation in non-control areas. Moreover, we use
feature mask constraints to mitigate the degradation of synthesized image
quality caused by information differences inside and outside the local control
area. Extensive experiments demonstrate that our method can synthesize
high-quality images to the prompt under local control conditions. Code is
available at https://github.com/YibooZhao/Local-Control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yibo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1&quot;&gt;Liang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zekai Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hengjia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+lu_q/0/1/0/all/0/1&quot;&gt;qinglin lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Boxi Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08773">
<title>Offshore Wind Plant Instance Segmentation Using Sentinel-1 Time Series, GIS, and Semantic Segmentation Models. (arXiv:2312.08773v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08773</link>
<description rdf:parseType="Literal">&lt;p&gt;Offshore wind farms represent a renewable energy source with a significant
global growth trend, and their monitoring is strategic for territorial and
environmental planning. This study&apos;s primary objective is to detect offshore
wind plants at an instance level using semantic segmentation models and
Sentinel-1 time series. The secondary objectives are: (a) to develop a database
consisting of labeled data and S-1 time series; (b) to compare the performance
of five deep semantic segmentation architectures (U-Net, U-Net++, Feature
Pyramid Network - FPN, DeepLabv3+, and LinkNet); (c) develop a novel
augmentation strategy that shuffles the positions of the images within the time
series; (d) investigate different dimensions of time series intervals (1, 5,
10, and 15 images); and (e) evaluate the semantic-to-instance conversion
procedure. LinkNet was the top-performing model, followed by U-Net++ and U-Net,
while FPN and DeepLabv3+ presented the worst results. The evaluation of
semantic segmentation models reveals enhanced Intersection over Union (IoU)
(25%) and F-score metrics (18%) with the augmentation of time series images.
The study showcases the augmentation strategy&apos;s capability to mitigate biases
and precisely detect invariant targets. Furthermore, the conversion from
semantic to instance segmentation demonstrates its efficacy in accurately
isolating individual instances within classified regions - simplifying training
data and reducing annotation effort and complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carvalho_O/0/1/0/all/0/1&quot;&gt;Osmar Luiz Ferreira de Carvalho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Junior_O/0/1/0/all/0/1&quot;&gt;Osmar Abilio de Carvalho Junior&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albuquerque_A/0/1/0/all/0/1&quot;&gt;Anesmar Olino de Albuquerque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_D/0/1/0/all/0/1&quot;&gt;Daniel Guerreiro e Silva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08774">
<title>VSFormer: Visual-Spatial Fusion Transformer for Correspondence Pruning. (arXiv:2312.08774v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08774</link>
<description rdf:parseType="Literal">&lt;p&gt;Correspondence pruning aims to find correct matches (inliers) from an initial
set of putative correspondences, which is a fundamental task for many
applications. The process of finding is challenging, given the varying inlier
ratios between scenes/image pairs due to significant visual differences.
However, the performance of the existing methods is usually limited by the
problem of lacking visual cues (\eg texture, illumination, structure) of
scenes. In this paper, we propose a Visual-Spatial Fusion Transformer
(VSFormer) to identify inliers and recover camera poses accurately. Firstly, we
obtain highly abstract visual cues of a scene with the cross attention between
local features of two-view images. Then, we model these visual cues and
correspondences by a joint visual-spatial fusion module, simultaneously
embedding visual cues into correspondences for pruning. Additionally, to mine
the consistency of correspondences, we also design a novel module that combines
the KNN-based graph and the transformer, effectively capturing both local and
global contexts. Extensive experiments have demonstrated that the proposed
VSFormer outperforms state-of-the-art methods on outdoor and indoor benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_T/0/1/0/all/0/1&quot;&gt;Tangfei Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoqin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Li Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1&quot;&gt;Guobao Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08782">
<title>Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis. (arXiv:2312.08782v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.08782</link>
<description rdf:parseType="Literal">&lt;p&gt;Building general-purpose robots that can operate seamlessly, in any
environment, with any object, and utilizing various skills to complete diverse
tasks has been a long-standing goal in Artificial Intelligence. Unfortunately,
however, most existing robotic systems have been constrained - having been
designed for specific tasks, trained on specific datasets, and deployed within
specific environments. These systems usually require extensively-labeled data,
rely on task-specific models, have numerous generalization issues when deployed
in real-world scenarios, and struggle to remain robust to distribution shifts.
Motivated by the impressive open-set performance and content generation
capabilities of web-scale, large-capacity pre-trained models (i.e., foundation
models) in research fields such as Natural Language Processing (NLP) and
Computer Vision (CV), we devote this survey to exploring (i) how these existing
foundation models from NLP and CV can be applied to the field of robotics, and
also exploring (ii) what a robotics-specific foundation model would look like.
We begin by providing an overview of what constitutes a conventional robotic
system and the fundamental barriers to making it universally applicable. Next,
we establish a taxonomy to discuss current work exploring ways to leverage
existing foundation models for robotics and develop ones catered to robotics.
Finally, we discuss key challenges and promising future directions in using
foundation models for enabling general-purpose robotic systems. We encourage
readers to view our ``living`` GitHub repository of resources, including papers
reviewed in this survey as well as related projects and repositories for
developing foundation models for robotics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yafei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Quanting Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vidhi Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1&quot;&gt;Jonathan Francis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patrikar_J/0/1/0/all/0/1&quot;&gt;Jay Patrikar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keetha_N/0/1/0/all/0/1&quot;&gt;Nikhil Keetha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seungchan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yaqi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhibo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chong_Y/0/1/0/all/0/1&quot;&gt;Yu-Quan Chong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1&quot;&gt;Katia Sycara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_Roberson_M/0/1/0/all/0/1&quot;&gt;Matthew Johnson-Roberson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1&quot;&gt;Dhruv Batra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1&quot;&gt;Sebastian Scherer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1&quot;&gt;Zsolt Kira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bisk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08785">
<title>Managing the unknown: a survey on Open Set Recognition and tangential areas. (arXiv:2312.08785v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08785</link>
<description rdf:parseType="Literal">&lt;p&gt;In real-world scenarios classification models are often required to perform
robustly when predicting samples belonging to classes that have not appeared
during its training stage. Open Set Recognition addresses this issue by
devising models capable of detecting unknown classes from samples arriving
during the testing phase, while maintaining a good level of performance in the
classification of samples belonging to known classes. This review
comprehensively overviews the recent literature related to Open Set
Recognition, identifying common practices, limitations, and connections of this
field with other machine learning research areas, such as continual learning,
out-of-distribution detection, novelty detection, and uncertainty estimation.
Our work also uncovers open problems and suggests several research directions
that may motivate and articulate future efforts towards more safe Artificial
Intelligence methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barcina_Blanco_M/0/1/0/all/0/1&quot;&gt;Marcos Barcina-Blanco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lobo_J/0/1/0/all/0/1&quot;&gt;Jesus L. Lobo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Bringas_P/0/1/0/all/0/1&quot;&gt;Pablo Garcia-Bringas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ser_J/0/1/0/all/0/1&quot;&gt;Javier Del Ser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08805">
<title>Zoom in on the Plant: Fine-grained Analysis of Leaf, Stem and Vein Instances. (arXiv:2312.08805v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.08805</link>
<description rdf:parseType="Literal">&lt;p&gt;Robot perception is far from what humans are capable of. Humans do not only
have a complex semantic scene understanding but also extract fine-grained
intra-object properties for the salient ones. When humans look at plants, they
naturally perceive the plant architecture with its individual leaves and
branching system. In this work, we want to advance the granularity in plant
understanding for agricultural precision robots. We develop a model to extract
fine-grained phenotypic information, such as leaf-, stem-, and vein instances.
The underlying dataset RumexLeaves is made publicly available and is the first
of its kind with keypoint-guided polyline annotations leading along the line
from the lowest stem point along the leaf basal to the leaf apex. Furthermore,
we introduce an adapted metric POKS complying with the concept of
keypoint-guided polylines. In our experimental evaluation, we provide baseline
results for our newly introduced dataset while showcasing the benefits of POKS
over OKS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guldenring_R/0/1/0/all/0/1&quot;&gt;Ronja G&amp;#xfc;ldenring&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andersen_R/0/1/0/all/0/1&quot;&gt;Rasmus Eckholdt Andersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nalpantidis_L/0/1/0/all/0/1&quot;&gt;Lazaros Nalpantidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08822">
<title>Planning and Rendering: Towards End-to-End Product Poster Generation. (arXiv:2312.08822v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08822</link>
<description rdf:parseType="Literal">&lt;p&gt;End-to-end product poster generation significantly optimizes design
efficiency and reduces production costs. Prevailing methods predominantly rely
on image-inpainting methods to generate clean background images for given
products. Subsequently, poster layout generation methods are employed to
produce corresponding layout results. However, the background images may not be
suitable for accommodating textual content due to their complexity, and the
fixed location of products limits the diversity of layout results. To alleviate
these issues, we propose a novel product poster generation framework named
P\&amp;amp;R. The P\&amp;amp;R draws inspiration from the workflow of designers in creating
posters, which consists of two stages: Planning and Rendering. At the planning
stage, we propose a PlanNet to generate the layout of the product and other
visual components considering both the appearance features of the product and
semantic features of the text, which improves the diversity and rationality of
the layouts. At the rendering stage, we propose a RenderNet to generate the
background for the product while considering the generated layout, where a
spatial fusion module is introduced to fuse the layout of different visual
components. To foster the advancement of this field, we propose the first
end-to-end product poster generation dataset PPG30k, comprising 30k exquisite
product poster images along with comprehensive image and text annotations. Our
method outperforms the state-of-the-art product poster generation methods on
PPG30k. The PPG30k will be released soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhaochen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fengheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1&quot;&gt;Wei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Honghe Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;An Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yaoyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1&quot;&gt;Jingjing Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Junjie Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhangang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jingping Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhenglu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08825">
<title>Guided Diffusion from Self-Supervised Diffusion Features. (arXiv:2312.08825v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08825</link>
<description rdf:parseType="Literal">&lt;p&gt;Guidance serves as a key concept in diffusion models, yet its effectiveness
is often limited by the need for extra data annotation or classifier
pretraining. That is why guidance was harnessed from self-supervised learning
backbones, like DINO. However, recent studies have revealed that the feature
representation derived from diffusion model itself is discriminative for
numerous downstream tasks as well, which prompts us to propose a framework to
extract guidance from, and specifically for, diffusion models. Our research has
yielded several significant contributions. Firstly, the guidance signals from
diffusion models are on par with those from class-conditioned diffusion models.
Secondly, feature regularization, when based on the Sinkhorn-Knopp algorithm,
can further enhance feature discriminability in comparison to unconditional
diffusion models. Thirdly, we have constructed an online training approach that
can concurrently derive guidance from diffusion models for diffusion models.
Lastly, we have extended the application of diffusion models along the constant
velocity path of ODE to achieve a more favorable balance between sampling steps
and fidelity. The performance of our methods has been outstanding,
outperforming related baseline comparisons in large-resolution datasets, such
as ImageNet256, ImageNet256-100 and LSUN-Churches. Our code will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_V/0/1/0/all/0/1&quot;&gt;Vincent Tao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yunlu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1&quot;&gt;Mathilde Caron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1&quot;&gt;Yuki M. Asano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1&quot;&gt;Cees G. M. Snoek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1&quot;&gt;Bjorn Ommer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08839">
<title>Exploration of visual prompt in Grounded pre-trained open-set detection. (arXiv:2312.08839v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08839</link>
<description rdf:parseType="Literal">&lt;p&gt;Text prompts are crucial for generalizing pre-trained open-set object
detection models to new categories. However, current methods for text prompts
are limited as they require manual feedback when generalizing to new
categories, which restricts their ability to model complex scenes, often
leading to incorrect detection results. To address this limitation, we propose
a novel visual prompt method that learns new category knowledge from a few
labeled images, which generalizes the pre-trained detection model to the new
category. To allow visual prompts to represent new categories adequately, we
propose a statistical-based prompt construction module that is not limited by
predefined vocabulary lengths, thus allowing more vectors to be used when
representing categories. We further utilize the category dictionaries in the
pre-training dataset to design task-specific similarity dictionaries, which
make visual prompts more discriminative. We evaluate the method on the ODinW
dataset and show that it outperforms existing prompt learning methods and
performs more consistently in combinatorial inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qibo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1&quot;&gt;Weizhong Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuchang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mengdi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Li Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jian Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaozheng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08843">
<title>Diffusion-C: Unveiling the Generative Challenges of Diffusion Models through Corrupted Data. (arXiv:2312.08843v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08843</link>
<description rdf:parseType="Literal">&lt;p&gt;In our contemporary academic inquiry, we present &quot;Diffusion-C,&quot; a
foundational methodology to analyze the generative restrictions of Diffusion
Models, particularly those akin to GANs, DDPM, and DDIM. By employing input
visual data that has been subjected to a myriad of corruption modalities and
intensities, we elucidate the performance characteristics of those Diffusion
Models. The noise component takes center stage in our analysis, hypothesized to
be a pivotal element influencing the mechanics of deep learning systems. In our
rigorous expedition utilizing Diffusion-C, we have discerned the following
critical observations: (I) Within the milieu of generative models under the
Diffusion taxonomy, DDPM emerges as a paragon, consistently exhibiting superior
performance metrics. (II) Within the vast spectrum of corruption frameworks,
the fog and fractal corruptions notably undermine the functional robustness of
both DDPM and DDIM. (III) The vulnerability of Diffusion Models to these
particular corruptions is significantly influenced by topological and
statistical similarities, particularly concerning the alignment between mean
and variance. This scholarly work highlights Diffusion-C&apos;s core understandings
regarding the impacts of various corruptions, setting the stage for future
research endeavors in the realm of generative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_K/0/1/0/all/0/1&quot;&gt;Keywoong Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Suan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1&quot;&gt;Wookey Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08846">
<title>TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training. (arXiv:2312.08846v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08846</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised Multi-modal Contrastive Learning (SMCL) remarkably advances
modern Vision-Language Pre-training (VLP) models by aligning visual and
linguistic modalities. Due to noises in web-harvested text-image pairs,
however, scaling up training data volume in SMCL presents considerable
obstacles in terms of computational cost and data inefficiency. To improve data
efficiency in VLP, we propose Text-aware Image Mixing (TiMix), which integrates
mix-based data augmentation techniques into SMCL, yielding significant
performance improvements without significantly increasing computational
overhead. We provide a theoretical analysis of TiMixfrom a mutual information
(MI) perspective, showing that mixed data samples for cross-modal contrastive
learning implicitly serve as a regularizer for the contrastive loss. The
experimental results demonstrate that TiMix exhibits a comparable performance
on downstream tasks, even with a reduced amount of training data and shorter
training time, when benchmarked against existing methods. This work empirically
and theoretically demonstrates the potential of data mixing for data-efficient
and computationally viable VLP, benefiting broader VLP model adoption in
practical scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Chaoya Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ye_W/0/1/0/all/0/1&quot;&gt;Wei ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haiyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qinghao Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1&quot;&gt;Ming Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Ji Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shikun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08851">
<title>Achelous++: Power-Oriented Water-Surface Panoptic Perception Framework on Edge Devices based on Vision-Radar Fusion and Pruning of Heterogeneous Modalities. (arXiv:2312.08851v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08851</link>
<description rdf:parseType="Literal">&lt;p&gt;Urban water-surface robust perception serves as the foundation for
intelligent monitoring of aquatic environments and the autonomous navigation
and operation of unmanned vessels, especially in the context of waterway
safety. It is worth noting that current multi-sensor fusion and multi-task
learning models consume substantial power and heavily rely on high-power GPUs
for inference. This contributes to increased carbon emissions, a concern that
runs counter to the prevailing emphasis on environmental preservation and the
pursuit of sustainable, low-carbon urban environments. In light of these
concerns, this paper concentrates on low-power, lightweight, multi-task
panoptic perception through the fusion of visual and 4D radar data, which is
seen as a promising low-cost perception method. We propose a framework named
Achelous++ that facilitates the development and comprehensive evaluation of
multi-task water-surface panoptic perception models. Achelous++ can
simultaneously execute five perception tasks with high speed and low power
consumption, including object detection, object semantic segmentation,
drivable-area segmentation, waterline segmentation, and radar point cloud
semantic segmentation. Furthermore, to meet the demand for developers to
customize models for real-time inference on low-performance devices, a novel
multi-modal pruning strategy known as Heterogeneous-Aware SynFlow (HA-SynFlow)
is proposed. Besides, Achelous++ also supports random pruning at initialization
with different layer-wise sparsity, such as Uniform and Erdos-Renyi-Kernel
(ERK). Overall, our Achelous++ framework achieves state-of-the-art performance
on the WaterScenes benchmark, excelling in both accuracy and power efficiency
compared to other single-task and multi-task models. We release and maintain
the code at https://github.com/GuanRunwei/Achelous.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_R/0/1/0/all/0/1&quot;&gt;Runwei Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haocheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1&quot;&gt;Shanliang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Man_K/0/1/0/all/0/1&quot;&gt;Ka Lok Man&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaohui Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Limin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yong Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1&quot;&gt;Jeremy Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1&quot;&gt;Eng Gee Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1&quot;&gt;Weiping Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yutao Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08853">
<title>Guided Image Restoration via Simultaneous Feature and Image Guided Fusion. (arXiv:2312.08853v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08853</link>
<description rdf:parseType="Literal">&lt;p&gt;Guided image restoration (GIR), such as guided depth map super-resolution and
pan-sharpening, aims to enhance a target image using guidance information from
another image of the same scene. Currently, joint image filtering-inspired deep
learning-based methods represent the state-of-the-art for GIR tasks. Those
methods either deal with GIR in an end-to-end way by elaborately designing
filtering-oriented deep neural network (DNN) modules, focusing on the
feature-level fusion of inputs; or explicitly making use of the traditional
joint filtering mechanism by parameterizing filtering coefficients with DNNs,
working on image-level fusion. The former ones are good at recovering
contextual information but tend to lose fine-grained details, while the latter
ones can better retain textual information but might lead to content
distortions. In this work, to inherit the advantages of both methodologies
while mitigating their limitations, we proposed a Simultaneous Feature and
Image Guided Fusion (SFIGF) network, that simultaneously considers feature and
image-level guided fusion following the guided filter (GF) mechanism. In the
feature domain, we connect the cross-attention (CA) with GF, and propose a
GF-inspired CA module for better feature-level fusion; in the image domain, we
fully explore the GF mechanism and design GF-like structure for better
image-level fusion. Since guided fusion is implemented in both feature and
image domains, the proposed SFIGF is expected to faithfully reconstruct both
contextual and textual information from sources and thus lead to better GIR
results. We apply SFIGF to 4 typical GIR tasks, and experimental results on
these tasks demonstrate its effectiveness and general availability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qian Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jie Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1&quot;&gt;Hui Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1&quot;&gt;Deyu Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08859">
<title>BVI-Artefact: An Artefact Detection Benchmark Dataset for Streamed Videos. (arXiv:2312.08859v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08859</link>
<description rdf:parseType="Literal">&lt;p&gt;Professionally generated content (PGC) streamed online can contain visual
artefacts that degrade the quality of user experience. These artefacts arise
from different stages of the streaming pipeline, including acquisition,
post-production, compression, and transmission. To better guide streaming
experience enhancement, it is important to detect specific artefacts at the
user end in the absence of a pristine reference. In this work, we address the
lack of a comprehensive benchmark for artefact detection within streamed PGC,
via the creation and validation of a large database, BVI-Artefact. Considering
the ten most relevant artefact types encountered in video streaming, we
collected and generated 480 video sequences, each containing various artefacts
with associated binary artefact labels. Based on this new database, existing
artefact detection methods are benchmarked, with results showing the
challenging nature of this tasks and indicating the requirement of more
reliable artefact detection methods. To facilitate further research in this
area, we have made BVI-Artifact publicly available at
https://chenfeng-bristol.github.io/BVI-Artefact/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1&quot;&gt;Chen Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danier_D/0/1/0/all/0/1&quot;&gt;Duolikun Danier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bull_D/0/1/0/all/0/1&quot;&gt;David Bull&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08863">
<title>HeadRecon: High-Fidelity 3D Head Reconstruction from Monocular Video. (arXiv:2312.08863v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08863</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the reconstruction of high-fidelity 3D head models from static
portrait image has made great progress. However, most methods require
multi-view or multi-illumination information, which therefore put forward high
requirements for data acquisition. In this paper, we study the reconstruction
of high-fidelity 3D head models from arbitrary monocular videos. Non-rigid
structure from motion (NRSFM) methods have been widely used to solve such
problems according to the two-dimensional correspondence between different
frames. However, the inaccurate correspondence caused by high-complex hair
structures and various facial expression changes would heavily influence the
reconstruction accuracy. To tackle these problems, we propose a prior-guided
dynamic implicit neural network. Specifically, we design a two-part dynamic
deformation field to transform the current frame space to the canonical one. We
further model the head geometry in the canonical space with a learnable signed
distance field (SDF) and optimize it using the volumetric rendering with the
guidance of two-main head priors to improve the reconstruction accuracy and
robustness. Extensive ablation studies and comparisons with state-of-the-art
methods demonstrate the effectiveness and robustness of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xueying Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Juyong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08864">
<title>RankDVQA-mini: Knowledge Distillation-Driven Deep Video Quality Assessment. (arXiv:2312.08864v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.08864</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning-based video quality assessment (deep VQA) has demonstrated
significant potential in surpassing conventional metrics, with promising
improvements in terms of correlation with human perception. However, the
practical deployment of such deep VQA models is often limited due to their high
computational complexity and large memory requirements. To address this issue,
we aim to significantly reduce the model size and runtime of one of the
state-of-the-art deep VQA methods, RankDVQA, by employing a two-phase workflow
that integrates pruning-driven model compression with multi-level knowledge
distillation. The resulting lightweight quality metric, RankDVQA-mini, requires
less than 10% of the model parameters compared to its full version (14% in
terms of FLOPs), while still retaining a quality prediction performance that is
superior to most existing deep VQA methods. The source code of the
RankDVQA-mini has been released at
https://chenfeng-bristol.github.io/RankDVQA-mini/ for public evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_C/0/1/0/all/0/1&quot;&gt;Chen Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Danier_D/0/1/0/all/0/1&quot;&gt;Duolikun Danier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bull_D/0/1/0/all/0/1&quot;&gt;David Bull&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08865">
<title>Improving Cross-modal Alignment with Synthetic Pairs for Text-only Image Captioning. (arXiv:2312.08865v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08865</link>
<description rdf:parseType="Literal">&lt;p&gt;Although image captioning models have made significant advancements in recent
years, the majority of them heavily depend on high-quality datasets containing
paired images and texts which are costly to acquire. Previous works leverage
the CLIP&apos;s cross-modal association ability for image captioning, relying solely
on textual information under unsupervised settings. However, not only does a
modality gap exist between CLIP text and image features, but a discrepancy also
arises between training and inference due to the unavailability of real-world
images, which hinders the cross-modal alignment in text-only captioning. This
paper proposes a novel method to address these issues by incorporating
synthetic image-text pairs. A pre-trained text-to-image model is deployed to
obtain images that correspond to textual data, and the pseudo features of
generated images are optimized toward the real ones in the CLIP embedding
space. Furthermore, textual information is gathered to represent image
features, resulting in the image features with various semantics and the
bridged modality gap. To unify training and inference, synthetic image features
would serve as the training prefix for the language decoder, while real images
are used for inference. Additionally, salient objects in images are detected as
assistance to enhance the learning of modality alignment. Experimental results
demonstrate that our method obtains the state-of-the-art performance on
benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1&quot;&gt;Fanrong Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08866">
<title>MCANet: Medical Image Segmentation with Multi-Scale Cross-Axis Attention. (arXiv:2312.08866v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.08866</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiently capturing multi-scale information and building long-range
dependencies among pixels are essential for medical image segmentation because
of the various sizes and shapes of the lesion regions or organs. In this paper,
we present Multi-scale Cross-axis Attention (MCA) to solve the above
challenging issues based on the efficient axial attention. Instead of simply
connecting axial attention along the horizontal and vertical directions
sequentially, we propose to calculate dual cross attentions between two
parallel axial attentions to capture global information better. To process the
significant variations of lesion regions or organs in individual sizes and
shapes, we also use multiple convolutions of strip-shape kernels with different
kernel sizes in each axial attention path to improve the efficiency of the
proposed MCA in encoding spatial information. We build the proposed MCA upon
the MSCAN backbone, yielding our network, termed MCANet. Our MCANet with only
4M+ parameters performs even better than most previous works with heavy
backbones (e.g., Swin Transformer) on four challenging tasks, including skin
lesion segmentation, nuclei segmentation, abdominal multi-organ segmentation,
and polyp segmentation. Code is available at https:// github.com/ haoshao-nku/
medical seg.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shao_H/0/1/0/all/0/1&quot;&gt;Hao Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zeng_Q/0/1/0/all/0/1&quot;&gt;Quansheng Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hou_Q/0/1/0/all/0/1&quot;&gt;Qibin Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jufeng Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08869">
<title>I&apos;M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions. (arXiv:2312.08869v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08869</link>
<description rdf:parseType="Literal">&lt;p&gt;We are living in a world surrounded by diverse and &quot;smart&quot; devices with rich
modalities of sensing ability. Conveniently capturing the interactions between
us humans and these objects remains far-reaching. In this paper, we present
I&apos;m-HOI, a monocular scheme to faithfully capture the 3D motions of both the
human and object in a novel setting: using a minimal amount of RGB camera and
object-mounted Inertial Measurement Unit (IMU). It combines general motion
inference and category-aware refinement. For the former, we introduce a
holistic human-object tracking method to fuse the IMU signals and the RGB
stream and progressively recover the human motions and subsequently the
companion object motions. For the latter, we tailor a category-aware motion
diffusion model, which is conditioned on both the raw IMU observations and the
results from the previous stage under over-parameterization representation. It
significantly refines the initial results and generates vivid body, hand, and
object motions. Moreover, we contribute a large dataset with ground truth human
and object motions, dense RGB inputs, and rich object-mounted IMU measurements.
Extensive experiments demonstrate the effectiveness of I&apos;m-HOI under a hybrid
capture setting. Our dataset and code will be released to the community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chengfeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Juze Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;Jiashen Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Z/0/1/0/all/0/1&quot;&gt;Ziwei Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junye Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jingyi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingya Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08870">
<title>Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens. (arXiv:2312.08870v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08870</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in large video-language models have displayed promising
outcomes in video comprehension. Current approaches straightforwardly convert
video into language tokens and employ large language models for multi-modal
tasks. However, this method often leads to the generation of irrelevant
content, commonly known as &quot;hallucination&quot;, as the length of the text increases
and the impact of the video diminishes. To address this problem, we propose
Vista-LLaMA, a novel framework that maintains the consistent distance between
all visual tokens and any language tokens, irrespective of the generated text
length. Vista-LLaMA omits relative position encoding when determining attention
weights between visual and text tokens, retaining the position encoding for
text and text tokens. This amplifies the effect of visual tokens on text
generation, especially when the relative distance is longer between visual and
text tokens. The proposed attention mechanism significantly reduces the chance
of producing irrelevant text related to the video content. Furthermore, we
present a sequential visual projector that projects the current video frame
into tokens of language space with the assistance of the previous frame. This
approach not only captures the temporal relationship within the video, but also
allows less visual tokens to encompass the entire video. Our approach
significantly outperforms various previous methods (e.g., Video-ChatGPT,
MovieChat) on four challenging open-ended video question answering benchmarks.
We reach an accuracy of 60.7 on the zero-shot NExT-QA and 60.5 on the zero-shot
MSRVTT-QA, setting a new state-of-the-art performance. This project is
available at https://jinxxian.github.io/Vista-LLaMA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1&quot;&gt;Fan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xiaojie Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Heng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1&quot;&gt;Yuchen Xian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08871">
<title>VoxelKP: A Voxel-based Network Architecture for Human Keypoint Estimation in LiDAR Data. (arXiv:2312.08871v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08871</link>
<description rdf:parseType="Literal">&lt;p&gt;We present \textit{VoxelKP}, a novel fully sparse network architecture
tailored for human keypoint estimation in LiDAR data. The key challenge is that
objects are distributed sparsely in 3D space, while human keypoint detection
requires detailed local information wherever humans are present. We propose
four novel ideas in this paper. First, we propose sparse selective kernels to
capture multi-scale context. Second, we introduce sparse box-attention to focus
on learning spatial correlations between keypoints within each human instance.
Third, we incorporate a spatial encoding to leverage absolute 3D coordinates
when projecting 3D voxels to a 2D grid encoding a bird&apos;s eye view. Finally, we
propose hybrid feature learning to combine the processing of per-voxel features
with sparse convolution. We evaluate our method on the Waymo dataset and
achieve an improvement of $27\%$ on the MPJPE metric compared to the
state-of-the-art, \textit{HUM3DIL}, trained on the same data, and $12\%$
against the state-of-the-art, \textit{GC-KPL}, pretrained on a $25\times$
larger dataset. To the best of our knowledge, \textit{VoxelKP} is the first
single-staged, fully sparse network that is specifically designed for
addressing the challenging task of 3D keypoint estimation from LiDAR data,
achieving state-of-the-art performances. Our code is available at
\url{https://github.com/shijianjian/VoxelKP}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1&quot;&gt;Peter Wonka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08872">
<title>Semantic-Driven Initial Image Construction for Guided Image Synthesis in Diffusion Model. (arXiv:2312.08872v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08872</link>
<description rdf:parseType="Literal">&lt;p&gt;The initial noise image has demonstrated a significant influence on image
generation, and manipulating the initial noise image can effectively increase
control over the generation. All of the current generation is based only on a
single initial noise drawn from a normal distribution, which may not be suited
to the desired content specified by the prompt. In this research, we propose a
novel approach using pre-collected, semantically-informed pixel blocks from
multiple initial noises for the initial image construction to enhance control
over the image generation. The inherent tendencies of these pixel blocks can
easily generate specific content, thus effectively guiding the generation
process towards the desired content. The pursuit of tailored initial image
construction inevitably leads to deviations from the normal distribution, and
our experimental results show that the diffusion model exhibits a certain
degree of tolerance towards the distribution of initial images. Our approach
achieves state-of-the-art performance in the training-free layout-to-image
synthesis task, demonstrating the adaptability of the initial image
construction in guiding the content of the generated image. Our code will be
made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1&quot;&gt;Jiafeng Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xueting Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aizawa_K/0/1/0/all/0/1&quot;&gt;Kiyoharu Aizawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08873">
<title>Diffusion Cocktail: Fused Generation from Diffusion Models. (arXiv:2312.08873v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08873</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models excel at generating high-quality images and are easy to
extend, making them extremely popular among active users who have created an
extensive collection of diffusion models with various styles by fine-tuning
base models such as Stable Diffusion. Recent work has focused on uncovering
semantic and visual information encoded in various components of a diffusion
model, enabling better generation quality and more fine-grained control.
However, those methods target improving a single model and overlook the vastly
available collection of fine-tuned diffusion models. In this work, we study the
combinations of diffusion models. We propose Diffusion Cocktail (Ditail), a
training-free method that can accurately transfer content information between
two diffusion models. This allows us to perform diverse generations using a set
of diffusion models, resulting in novel images that are unlikely to be obtained
by a single model alone. We also explore utilizing Ditail for style transfer,
with the target style set by a diffusion model instead of an image. Ditail
offers a more detailed manipulation of the diffusion generation, thereby
enabling the vast community to integrate various styles and contents seamlessly
and generate any content of any style.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haoming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuanhe Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shengjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hongyi Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08874">
<title>Agent Attention: On the Integration of Softmax and Linear Attention. (arXiv:2312.08874v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08874</link>
<description rdf:parseType="Literal">&lt;p&gt;The attention module is the key component in Transformers. While the global
attention mechanism offers high expressiveness, its excessive computational
cost restricts its applicability in various scenarios. In this paper, we
propose a novel attention paradigm, Agent Attention, to strike a favorable
balance between computational efficiency and representation power.
Specifically, the Agent Attention, denoted as a quadruple $(Q, A, K, V)$,
introduces an additional set of agent tokens $A$ into the conventional
attention module. The agent tokens first act as the agent for the query tokens
$Q$ to aggregate information from $K$ and $V$, and then broadcast the
information back to $Q$. Given the number of agent tokens can be designed to be
much smaller than the number of query tokens, the agent attention is
significantly more efficient than the widely adopted Softmax attention, while
preserving global context modelling capability. Interestingly, we show that the
proposed agent attention is equivalent to a generalized form of linear
attention. Therefore, agent attention seamlessly integrates the powerful
Softmax attention and the highly efficient linear attention. Extensive
experiments demonstrate the effectiveness of agent attention with various
vision Transformers and across diverse vision tasks, including image
classification, object detection, semantic segmentation and image generation.
Notably, agent attention has shown remarkable performance in high-resolution
scenarios, owning to its linear attention nature. For instance, when applied to
Stable Diffusion, our agent attention accelerates generation and substantially
enhances image generation quality without any additional training. Code is
available at https://github.com/LeapLabTHU/Agent-Attention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1&quot;&gt;Dongchen Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1&quot;&gt;Tianzhu Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yizeng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1&quot;&gt;Zhuofan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shiji Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Gao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08875">
<title>What, How, and When Should Object Detectors Update in Continually Changing Test Domains?. (arXiv:2312.08875v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08875</link>
<description rdf:parseType="Literal">&lt;p&gt;It is a well-known fact that the performance of deep learning models
deteriorates when they encounter a distribution shift at test time. Test-time
adaptation (TTA) algorithms have been proposed to adapt the model online while
inferring test data. However, existing research predominantly focuses on
classification tasks through the optimization of batch normalization layers or
classification heads, but this approach limits its applicability to various
model architectures like Transformers and makes it challenging to apply to
other tasks, such as object detection. In this paper, we propose a novel online
adaption approach for object detection in continually changing test domains,
considering which part of the model to update, how to update it, and when to
perform the update. By introducing architecture-agnostic and lightweight
adaptor modules and only updating these while leaving the pre-trained backbone
unchanged, we can rapidly adapt to new test domains in an efficient way and
prevent catastrophic forgetting. Furthermore, we present a practical and
straightforward class-wise feature aligning method for object detection to
resolve domain shifts. Additionally, we enhance efficiency by determining when
the model is sufficiently adapted or when additional adaptation is needed due
to changes in the test distribution. Our approach surpasses baselines on widely
used benchmarks, achieving improvements of up to 4.9\%p and 7.9\%p in mAP for
COCO $\rightarrow$ COCO-corrupted and SHIFT, respectively, while maintaining
about 20 FPS or higher.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1&quot;&gt;Jayeon Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dongkwan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_I/0/1/0/all/0/1&quot;&gt;Inseop Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Donghyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1&quot;&gt;Nojun Kwak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08876">
<title>OpenSight: A Simple Open-Vocabulary Framework for LiDAR-Based Object Detection. (arXiv:2312.08876v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08876</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional LiDAR-based object detection research primarily focuses on
closed-set scenarios, which falls short in complex real-world applications.
Directly transferring existing 2D open-vocabulary models with some known LiDAR
classes for open-vocabulary ability, however, tends to suffer from over-fitting
problems: The obtained model will detect the known objects, even presented with
a novel category. In this paper, we propose OpenSight, a more advanced 2D-3D
modeling framework for LiDAR-based open-vocabulary detection. OpenSight
utilizes 2D-3D geometric priors for the initial discernment and localization of
generic objects, followed by a more specific semantic interpretation of the
detected objects. The process begins by generating 2D boxes for generic objects
from the accompanying camera images of LiDAR. These 2D boxes, together with
LiDAR points, are then lifted back into the LiDAR space to estimate
corresponding 3D boxes. For better generic object perception, our framework
integrates both temporal and spatial-aware constraints. Temporal awareness
correlates the predicted 3D boxes across consecutive timestamps, recalibrating
the missed or inaccurate boxes. The spatial awareness randomly places some
``precisely&apos;&apos; estimated 3D boxes at varying distances, increasing the
visibility of generic objects. To interpret the specific semantics of detected
objects, we develop a cross-modal alignment and fusion module to first align 3D
features with 2D image embeddings and then fuse the aligned 3D-2D features for
semantic decoding. Our experiments indicate that our method establishes
state-of-the-art open-vocabulary performance on widely used 3D detection
benchmarks and effectively identifies objects for new categories of interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jianhua Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1&quot;&gt;Tao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Haiyang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Kaicheng Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08877">
<title>May the Noise be with you: Adversarial Training without Adversarial Examples. (arXiv:2312.08877v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08877</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate the following question: Can we obtain
adversarially-trained models without training on adversarial examples? Our
intuition is that training a model with inherent stochasticity, i.e.,
optimizing the parameters by minimizing a stochastic loss function, yields a
robust expectation function that is non-stochastic. In contrast to related
methods that introduce noise at the input level, our proposed approach
incorporates inherent stochasticity by embedding Gaussian noise within the
layers of the NN model at training time. We model the propagation of noise
through the layers, introducing a closed-form stochastic loss function that
encapsulates a noise variance parameter. Additionally, we contribute a
formalized noise-aware gradient, enabling the optimization of model parameters
while accounting for stochasticity. Our experimental results confirm that the
expectation model of a stochastic architecture trained on benign distribution
is adversarially robust. Interestingly, we find that the impact of the applied
Gaussian noise&apos;s standard deviation on both robustness and baseline accuracy
closely mirrors the impact of the noise magnitude employed in adversarial
training. Our work contributes adversarially trained networks using a
completely different approach, with empirically similar robustness to
adversarial training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arous_A/0/1/0/all/0/1&quot;&gt;Ayoub Arous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_Lopera_A/0/1/0/all/0/1&quot;&gt;Andres F Lopez-Lopera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abu_Ghazaleh_N/0/1/0/all/0/1&quot;&gt;Nael Abu-Ghazaleh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alouani_I/0/1/0/all/0/1&quot;&gt;Ihsen Alouani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08878">
<title>Domain Prompt Learning with Quaternion Networks. (arXiv:2312.08878v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08878</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt learning has emerged as an effective and data-efficient technique in
large Vision-Language Models (VLMs). However, when adapting VLMs to specialized
domains such as remote sensing and medical imaging, domain prompt learning
remains underexplored. While large-scale domain-specific foundation models can
help tackle this challenge, their concentration on a single vision level makes
it challenging to prompt both vision and language modalities. To overcome this,
we propose to leverage domain-specific knowledge from domain-specific
foundation models to transfer the robust recognition ability of VLMs from
generalized to specialized domains, using quaternion networks. Specifically,
the proposed method involves using domain-specific vision features from
domain-specific foundation models to guide the transformation of generalized
contextual embeddings from the language branch into a specialized space within
the quaternion networks. Moreover, we present a hierarchical approach that
generates vision prompt features by analyzing intermodal relationships between
hierarchical language prompt features and domain-specific vision features. In
this way, quaternion networks can effectively mine the intermodal relationships
in the specific domain, facilitating domain-specific vision-language
contrastive learning. Extensive experiments on domain-specific datasets show
that our proposed method achieves new state-of-the-art results in prompt
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1&quot;&gt;Qinglong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhengqin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuntian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaokang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08879">
<title>Regularizing Self-supervised 3D Scene Flows with Surface Awareness and Cyclic Consistency. (arXiv:2312.08879v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08879</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning without supervision how to predict 3D scene flows from point clouds
is central to many vision systems. We propose a novel learning framework for
this task which improves the necessary regularization. Relying on the
assumption that scene elements are mostly rigid, current smoothness losses are
built on the definition of ``rigid clusters&quot; in the input point clouds. The
definition of these clusters is challenging and has a major impact on the
quality of predicted flows. We introduce two new consistency losses that
enlarge clusters while preventing them from spreading over distinct objects. In
particular, we enforce \emph{temporal} consistency with a forward-backward
cyclic loss and \emph{spatial} consistency by considering surface orientation
similarity in addition to spatial proximity. The proposed losses are
model-independent and can thus be used in a plug-and-play fashion to
significantly improve the performance of existing models, as demonstrated on
two top-performing ones. We also showcase the effectiveness and generalization
capability of our framework on four standard sensor-unique driving datasets,
achieving state-of-the-art performance in 3D scene flow estimation. Our codes
are available anonymously on \url{https://github.com/vacany/sac-flow}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vacek_P/0/1/0/all/0/1&quot;&gt;Patrik Vacek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hurych_D/0/1/0/all/0/1&quot;&gt;David Hurych&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmermann_K/0/1/0/all/0/1&quot;&gt;Karel Zimmermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1&quot;&gt;Patrick Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Svoboda_T/0/1/0/all/0/1&quot;&gt;Tomas Svoboda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08880">
<title>GenDet: Towards Good Generalizations for AI-Generated Image Detection. (arXiv:2312.08880v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08880</link>
<description rdf:parseType="Literal">&lt;p&gt;The misuse of AI imagery can have harmful societal effects, prompting the
creation of detectors to combat issues like the spread of fake news. Existing
methods can effectively detect images generated by seen generators, but it is
challenging to detect those generated by unseen generators. They do not
concentrate on amplifying the output discrepancy when detectors process real
versus fake images. This results in a close output distribution of real and
fake samples, increasing classification difficulty in detecting unseen
generators. This paper addresses the unseen-generator detection problem by
considering this task from the perspective of anomaly detection and proposes an
adversarial teacher-student discrepancy-aware framework. Our method encourages
smaller output discrepancies between the student and the teacher models for
real images while aiming for larger discrepancies for fake images. We employ
adversarial learning to train a feature augmenter, which promotes smaller
discrepancies between teacher and student networks when the inputs are fake
images. Our method has achieved state-of-the-art on public benchmarks, and the
visualization results show that a large output discrepancy is maintained when
faced with various types of generators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Mingjian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1&quot;&gt;Mouxiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hailin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jie Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhe Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08881">
<title>AdaptIR: Parameter Efficient Multi-task Adaptation for Pre-trained Image Restoration Models. (arXiv:2312.08881v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08881</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-training has shown promising results on various image restoration tasks,
which is usually followed by full fine-tuning for each specific downstream task
(e.g., image denoising). However, such full fine-tuning usually suffers from
the problems of heavy computational cost in practice, due to the massive
parameters of pre-trained restoration models, thus limiting its real-world
applications. Recently, Parameter Efficient Transfer Learning (PETL) offers an
efficient alternative solution to full fine-tuning, yet still faces great
challenges for pre-trained image restoration models, due to the diversity of
different degradations. To address these issues, we propose AdaptIR, a novel
parameter efficient transfer learning method for adapting pre-trained
restoration models. Specifically, the proposed method consists of a
multi-branch inception structure to orthogonally capture local spatial, global
spatial, and channel interactions. In this way, it allows powerful
representations under a very low parameter budget. Extensive experiments
demonstrate that the proposed method can achieve comparable or even better
performance than full fine-tuning, while only using 0.6% parameters. Code is
available at https://github.com/csguoh/AdaptIR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Hang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1&quot;&gt;Tao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yuanchao Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shu-Tao Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zexuan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08882">
<title>Neural Video Fields Editing. (arXiv:2312.08882v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08882</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have revolutionized text-driven video editing. However,
applying these methods to real-world editing encounters two significant
challenges: (1) the rapid increase in graphics memory demand as the number of
frames grows, and (2) the inter-frame inconsistency in edited videos. To this
end, we propose NVEdit, a novel text-driven video editing framework designed to
mitigate memory overhead and improve consistent editing for real-world long
videos. Specifically, we construct a neural video field, powered by tri-plane
and sparse grid, to enable encoding long videos with hundreds of frames in a
memory-efficient manner. Next, we update the video field through off-the-shelf
Text-to-Image (T2I) models to impart text-driven editing effects. A progressive
optimization strategy is developed to preserve original temporal priors.
Importantly, both the neural video field and T2I model are adaptable and
replaceable, thus inspiring future research. Experiments demonstrate that our
approach successfully edits hundreds of frames with impressive inter-frame
consistency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shuzhou Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mou_C/0/1/0/all/0/1&quot;&gt;Chong Mou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jiwen Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuhan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xiandong Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jian Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08883">
<title>EditGuard: Versatile Image Watermarking for Tamper Localization and Copyright Protection. (arXiv:2312.08883v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08883</link>
<description rdf:parseType="Literal">&lt;p&gt;In the era where AI-generated content (AIGC) models can produce stunning and
lifelike images, the lingering shadow of unauthorized reproductions and
malicious tampering poses imminent threats to copyright integrity and
information security. Current image watermarking methods, while widely accepted
for safeguarding visual content, can only protect copyright and ensure
traceability. They fall short in localizing increasingly realistic image
tampering, potentially leading to trust crises, privacy violations, and legal
disputes. To solve this challenge, we propose an innovative proactive forensics
framework EditGuard, to unify copyright protection and tamper-agnostic
localization, especially for AIGC-based editing methods. It can offer a
meticulous embedding of imperceptible watermarks and precise decoding of
tampered areas and copyright information. Leveraging our observed fragility and
locality of image-into-image steganography, the realization of EditGuard can be
converted into a united image-bit steganography issue, thus completely
decoupling the training process from the tampering types. Extensive experiments
demonstrate that our EditGuard balances the tamper localization accuracy,
copyright recovery precision, and generalizability to various AIGC-based
tampering methods, especially for image forgery that is difficult for the naked
eye to detect. The project page is available at
https://xuanyuzhang21.github.io/project/editguard/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuanyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Runyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jiwen Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Youmin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weiqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jian Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08885">
<title>SceneWiz3D: Towards Text-guided 3D Scene Composition. (arXiv:2312.08885v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08885</link>
<description rdf:parseType="Literal">&lt;p&gt;We are witnessing significant breakthroughs in the technology for generating
3D objects from text. Existing approaches either leverage large text-to-image
models to optimize a 3D representation or train 3D generators on object-centric
datasets. Generating entire scenes, however, remains very challenging as a
scene contains multiple 3D objects, diverse and scattered. In this work, we
introduce SceneWiz3D, a novel approach to synthesize high-fidelity 3D scenes
from text. We marry the locality of objects with globality of scenes by
introducing a hybrid 3D representation: explicit for objects and implicit for
scenes. Remarkably, an object, being represented explicitly, can be either
generated from text using conventional text-to-3D approaches, or provided by
users. To configure the layout of the scene and automatically place objects, we
apply the Particle Swarm Optimization technique during the optimization
process. Furthermore, it is difficult for certain parts of the scene (e.g.,
corners, occlusion) to receive multi-view supervision, leading to inferior
geometry. We incorporate an RGBD panorama diffusion model to mitigate it,
resulting in high-quality geometry. Extensive evaluation supports that our
approach achieves superior quality over previous approaches, enabling the
generation of detailed and view-consistent 3D scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qihang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaoyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siarohin_A/0/1/0/all/0/1&quot;&gt;Aliaksandr Siarohin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_P/0/1/0/all/0/1&quot;&gt;Peiye Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yinghao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Ceyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bolei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1&quot;&gt;Sergey Tulyakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hsin-Ying Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08886">
<title>Diffusion-based Blind Text Image Super-Resolution. (arXiv:2312.08886v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08886</link>
<description rdf:parseType="Literal">&lt;p&gt;Recovering degraded low-resolution text images is challenging, especially for
Chinese text images with complex strokes and severe degradation in real-world
scenarios. Ensuring both text fidelity and style realness is crucial for
high-quality text image super-resolution. Recently, diffusion models have
achieved great success in natural image synthesis and restoration due to their
powerful data distribution modeling abilities and data generation capabilities.
In this work, we propose an Image Diffusion Model (IDM) to restore text images
with realistic styles. For diffusion models, they are not only suitable for
modeling realistic image distribution but also appropriate for learning text
distribution. Since text prior is important to guarantee the correctness of the
restored text structure according to existing arts, we also propose a Text
Diffusion Model (TDM) for text recognition which can guide IDM to generate text
images with correct structures. We further propose a Mixture of Multi-modality
module (MoM) to make these two diffusion models cooperate with each other in
all the diffusion steps. Extensive experiments on synthetic and real-world
datasets demonstrate that our Diffusion-based Blind Text Image Super-Resolution
(DiffTSR) can restore text images with more accurate text structures as well as
more realistic appearances simultaneously.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiawei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhouxia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1&quot;&gt;Luwei Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1&quot;&gt;Dongqing Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_L/0/1/0/all/0/1&quot;&gt;Liheng Bian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08887">
<title>SpeedUpNet: A Plug-and-Play Hyper-Network for Accelerating Text-to-Image Diffusion Models. (arXiv:2312.08887v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08887</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image diffusion models (SD) exhibit significant advancements while
requiring extensive computational resources. Though many acceleration methods
have been proposed, they suffer from generation quality degradation or extra
training cost generalizing to new fine-tuned models. To address these
limitations, we propose a novel and universal Stable-Diffusion (SD)
acceleration module called SpeedUpNet(SUN). SUN can be directly plugged into
various fine-tuned SD models without extra training. This technique utilizes
cross-attention layers to learn the relative offsets in the generated image
results between negative and positive prompts achieving classifier-free
guidance distillation with negative prompts controllable, and introduces a
Multi-Step Consistency (MSC) loss to ensure a harmonious balance between
reducing inference steps and maintaining consistency in the generated output.
Consequently, SUN significantly reduces the number of inference steps to just 4
steps and eliminates the need for classifier-free guidance. It leads to an
overall speedup of more than 10 times for SD models compared to the
state-of-the-art 25-step DPM-solver++, and offers two extra advantages: (1)
classifier-free guidance distillation with controllable negative prompts and
(2) seamless integration into various fine-tuned Stable-Diffusion models
without training. The effectiveness of the SUN has been verified through
extensive experimentation. Project Page:
https://williechai.github.io/speedup-plugin-for-stable-diffusions.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_W/0/1/0/all/0/1&quot;&gt;Weilong Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1&quot;&gt;DanDan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jiajiong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhiquan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changbao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chenguang Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08888">
<title>Read Between the Layers: Leveraging Intra-Layer Representations for Rehearsal-Free Continual Learning with Pre-Trained Models. (arXiv:2312.08888v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08888</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the Continual Learning (CL) problem, where a model has to learn a
sequence of tasks from non-stationary distributions while preserving prior
knowledge as it encounters new experiences. With the advancement of foundation
models, CL research has shifted focus from the initial learning-from-scratch
paradigm to the use of generic features from large-scale pre-training. However,
existing approaches to CL with pre-trained models only focus on separating the
class-specific features from the final representation layer and neglect the
power of intermediate representations that capture low- and mid-level features
naturally more invariant to domain shifts. In this work, we propose LayUP, a
new class-prototype-based approach to continual learning that leverages
second-order feature statistics from multiple intermediate layers of a
pre-trained network. Our method is conceptually simple, does not require any
replay buffer, and works out of the box with any foundation model. LayUP
improves over the state-of-the-art on four of the seven class-incremental
learning settings at a considerably reduced memory and computational footprint
compared with the next best baseline. Our results demonstrate that fully
exhausting the representational capacities of pre-trained models in CL goes far
beyond their final embeddings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahrens_K/0/1/0/all/0/1&quot;&gt;Kyra Ahrens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehmann_H/0/1/0/all/0/1&quot;&gt;Hans Hergen Lehmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jae Hee Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1&quot;&gt;Stefan Wermter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08889">
<title>SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained Geometry and Appearance. (arXiv:2312.08889v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08889</link>
<description rdf:parseType="Literal">&lt;p&gt;Powered by large-scale text-to-image generation models, text-to-3D avatar
generation has made promising progress. However, most methods fail to produce
photorealistic results, limited by imprecise geometry and low-quality
appearance. Towards more practical avatar generation, we present SEEAvatar, a
method for generating photorealistic 3D avatars from text with SElf-Evolving
constraints for decoupled geometry and appearance. For geometry, we propose to
constrain the optimized avatar in a decent global shape with a template avatar.
The template avatar is initialized with human prior and can be updated by the
optimized avatar periodically as an evolving template, which enables more
flexible shape generation. Besides, the geometry is also constrained by the
static human prior in local parts like face and hands to maintain the delicate
structures. For appearance generation, we use diffusion model enhanced by
prompt engineering to guide a physically based rendering pipeline to generate
realistic textures. The lightness constraint is applied on the albedo texture
to suppress incorrect lighting effect. Experiments show that our method
outperforms previous methods on both global and local geometry and appearance
quality by a large margin. Since our method can produce high-quality meshes and
textures, such assets can be directly applied in classic graphics pipeline for
realistic rendering under any lighting condition. Project page at:
https://seeavatar3d.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuanyou Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zongxin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08890">
<title>Defenses in Adversarial Machine Learning: A Survey. (arXiv:2312.08890v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08890</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial phenomenon has been widely observed in machine learning (ML)
systems, especially in those using deep neural networks, describing that ML
systems may produce inconsistent and incomprehensible predictions with humans
at some particular cases. This phenomenon poses a serious security threat to
the practical application of ML systems, and several advanced attack paradigms
have been developed to explore it, mainly including backdoor attacks, weight
attacks, and adversarial examples. For each individual attack paradigm, various
defense paradigms have been developed to improve the model robustness against
the corresponding attack paradigm. However, due to the independence and
diversity of these defense paradigms, it is difficult to examine the overall
robustness of an ML system against different kinds of attacks.This survey aims
to build a systematic review of all existing defense paradigms from a unified
perspective. Specifically, from the life-cycle perspective, we factorize a
complete machine learning system into five stages, including pre-training,
training, post-training, deployment, and inference stages, respectively. Then,
we present a clear taxonomy to categorize and review representative defense
methods at each individual stage. The unified perspective and presented
taxonomies not only facilitate the analysis of the mechanism of each defense
paradigm but also help us to understand connections and differences among
different defense paradigms, which may inspire future research to develop more
advanced, comprehensive defenses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1&quot;&gt;Shaokui Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Mingli Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1&quot;&gt;Meixi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zihao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingda Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hongrui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_D/0/1/0/all/0/1&quot;&gt;Danni Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Li Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qingshan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08892">
<title>VaLID: Variable-Length Input Diffusion for Novel View Synthesis. (arXiv:2312.08892v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08892</link>
<description rdf:parseType="Literal">&lt;p&gt;Novel View Synthesis (NVS), which tries to produce a realistic image at the
target view given source view images and their corresponding poses, is a
fundamental problem in 3D Vision. As this task is heavily under-constrained,
some recent work, like Zero123, tries to solve this problem with generative
modeling, specifically using pre-trained diffusion models. Although this
strategy generalizes well to new scenes, compared to neural radiance
field-based methods, it offers low levels of flexibility. For example, it can
only accept a single-view image as input, despite realistic applications often
offering multiple input images. This is because the source-view images and
corresponding poses are processed separately and injected into the model at
different stages. Thus it is not trivial to generalize the model into
multi-view source images, once they are available. To solve this issue, we try
to process each pose image pair separately and then fuse them as a unified
visual representation which will be injected into the model to guide image
synthesis at the target-views. However, inconsistency and computation costs
increase as the number of input source-view images increases. To solve these
issues, the Multi-view Cross Former module is proposed which maps
variable-length input data to fix-size output data. A two-stage training
strategy is introduced to further improve the efficiency during training time.
Qualitative and quantitative evaluation over multiple datasets demonstrates the
effectiveness of the proposed method against previous approaches. The code will
be released according to the acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shijie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zanjani_F/0/1/0/all/0/1&quot;&gt;Farhad G. Zanjani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yahia_H/0/1/0/all/0/1&quot;&gt;Haitam Ben Yahia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1&quot;&gt;Yuki M. Asano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1&quot;&gt;Juergen Gall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habibian_A/0/1/0/all/0/1&quot;&gt;Amirhossein Habibian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08894">
<title>HAROOD: Human Activity Classification and Out-of-Distribution Detection with Short-Range FMCW Radar. (arXiv:2312.08894v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08894</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose HAROOD as a short-range FMCW radar-based human activity classifier
and out-of-distribution (OOD) detector. It aims to classify human sitting,
standing, and walking activities and to detect any other moving or stationary
object as OOD. We introduce a two-stage network. The first stage is trained
with a novel loss function that includes intermediate reconstruction loss,
intermediate contrastive loss, and triplet loss. The second stage uses the
first stage&apos;s output as its input and is trained with cross-entropy loss. It
creates a simple classifier that performs the activity classification. On our
dataset collected by 60 GHz short-range FMCW radar, we achieve an average
classification accuracy of 96.51%. Also, we achieve an average AUROC of 95.04%
as an OOD detector. Additionally, our extensive evaluations demonstrate the
superiority of HAROOD over the state-of-the-art OOD detection methods in terms
of standard OOD detection metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahya_S/0/1/0/all/0/1&quot;&gt;Sabri Mustafa Kahya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yavuz_M/0/1/0/all/0/1&quot;&gt;Muhammet Sami Yavuz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinbach_E/0/1/0/all/0/1&quot;&gt;Eckehard Steinbach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08895">
<title>Motion Flow Matching for Human Motion Synthesis and Editing. (arXiv:2312.08895v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08895</link>
<description rdf:parseType="Literal">&lt;p&gt;Human motion synthesis is a fundamental task in computer animation. Recent
methods based on diffusion models or GPT structure demonstrate commendable
performance but exhibit drawbacks in terms of slow sampling speeds and error
accumulation. In this paper, we propose \emph{Motion Flow Matching}, a novel
generative model designed for human motion generation featuring efficient
sampling and effectiveness in motion editing applications. Our method reduces
the sampling complexity from thousand steps in previous diffusion models to
just ten steps, while achieving comparable performance in text-to-motion and
action-to-motion generation benchmarks. Noticeably, our approach establishes a
new state-of-the-art Fr\&apos;echet Inception Distance on the KIT-ML dataset. What
is more, we tailor a straightforward motion editing paradigm named
\emph{sampling trajectory rewriting} leveraging the ODE-style generative models
and apply it to various editing scenarios including motion prediction, motion
in-between prediction, motion interpolation, and upper-body editing. Our code
will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_V/0/1/0/all/0/1&quot;&gt;Vincent Tao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1&quot;&gt;Wenzhe Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1&quot;&gt;Pingchuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yunlu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1&quot;&gt;Basura Fernando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1&quot;&gt;Yuki M Asano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1&quot;&gt;Efstratios Gavves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mettes_P/0/1/0/all/0/1&quot;&gt;Pascal Mettes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1&quot;&gt;Bjorn Ommer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1&quot;&gt;Cees G. M. Snoek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08912">
<title>Dataset Distillation via Adversarial Prediction Matching. (arXiv:2312.08912v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08912</link>
<description rdf:parseType="Literal">&lt;p&gt;Dataset distillation is the technique of synthesizing smaller condensed
datasets from large original datasets while retaining necessary information to
persist the effect. In this paper, we approach the dataset distillation problem
from a novel perspective: we regard minimizing the prediction discrepancy on
the real data distribution between models, which are respectively trained on
the large original dataset and on the small distilled dataset, as a conduit for
condensing information from the raw data into the distilled version. An
adversarial framework is proposed to solve the problem efficiently. In contrast
to existing distillation methods involving nested optimization or long-range
gradient unrolling, our approach hinges on single-level optimization. This
ensures the memory efficiency of our method and provides a flexible tradeoff
between time and memory budgets, allowing us to distil ImageNet-1K using a
minimum of only 6.5GB of GPU memory. Under the optimal tradeoff strategy, it
requires only 2.5$\times$ less memory and 5$\times$ less runtime compared to
the state-of-the-art. Empirically, our method can produce synthetic datasets
just 10% the size of the original, yet achieve, on average, 94% of the test
accuracy of models trained on the full original datasets including ImageNet-1K,
significantly surpassing state-of-the-art. Additionally, extensive tests reveal
that our distilled datasets excel in cross-architecture generalization
capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Bo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Junda Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Minhao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08914">
<title>CogAgent: A Visual Language Model for GUI Agents. (arXiv:2312.08914v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08914</link>
<description rdf:parseType="Literal">&lt;p&gt;People are spending an enormous amount of time on digital devices through
graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large
language models (LLMs) such as ChatGPT can assist people in tasks like writing
emails, but struggle to understand and interact with GUIs, thus limiting their
potential to increase automation levels. In this paper, we introduce CogAgent,
an 18-billion-parameter visual language model (VLM) specializing in GUI
understanding and navigation. By utilizing both low-resolution and
high-resolution image encoders, CogAgent supports input at a resolution of
1120*1120, enabling it to recognize tiny page elements and text. As a
generalist visual language model, CogAgent achieves the state of the art on
five text-rich and four general VQA benchmarks, including VQAv2, OK-VQA,
Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using
only screenshots as input, outperforms LLM-based methods that consume extracted
HTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW,
advancing the state of the art. The model and codes are available at
\url{https://github.com/THUDM/CogVLM}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1&quot;&gt;Wenyi Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weihan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1&quot;&gt;Qingsong Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiazheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wenmeng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Junhui Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Ming Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jie Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08915">
<title>Attribute Regularized Soft Introspective Variational Autoencoder for Interpretable Cardiac Disease Classification. (arXiv:2312.08915v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.08915</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretability is essential in medical imaging to ensure that clinicians
can comprehend and trust artificial intelligence models. In this paper, we
propose a novel interpretable approach that combines attribute regularization
of the latent space within the framework of an adversarially trained
variational autoencoder. Comparative experiments on a cardiac MRI dataset
demonstrate the ability of the proposed method to address blurry reconstruction
issues of variational autoencoder methods and improve latent space
interpretability. Additionally, our analysis of a downstream task reveals that
the classification of cardiac disease using the regularized latent space
heavily relies on attribute regularized dimensions, demonstrating great
interpretability by connecting the used attributes for prediction with clinical
observations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Folco_M/0/1/0/all/0/1&quot;&gt;Maxime Di Folco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bercea_C/0/1/0/all/0/1&quot;&gt;Cosmin I. Bercea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schnabel_J/0/1/0/all/0/1&quot;&gt;Julia A. Schnabel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08916">
<title>Progressive Uncertain Feature Self-reinforcement for Weakly Supervised Semantic Segmentation. (arXiv:2312.08916v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08916</link>
<description rdf:parseType="Literal">&lt;p&gt;Compared to conventional semantic segmentation with pixel-level supervision,
Weakly Supervised Semantic Segmentation (WSSS) with image-level labels poses
the challenge that it always focuses on the most discriminative regions,
resulting in a disparity between fully supervised conditions. A typical
manifestation is the diminished precision on the object boundaries, leading to
a deteriorated accuracy of WSSS. To alleviate this issue, we propose to
adaptively partition the image content into deterministic regions (e.g.,
confident foreground and background) and uncertain regions (e.g., object
boundaries and misclassified categories) for separate processing. For uncertain
cues, we employ an activation-based masking strategy and seek to recover the
local information with self-distilled knowledge. We further assume that the
unmasked confident regions should be robust enough to preserve the global
semantics. Building upon this, we introduce a complementary self-enhancement
method that constrains the semantic consistency between these confident regions
and an augmented image with the same class labels. Extensive experiments
conducted on PASCAL VOC 2012 and MS COCO 2014 demonstrate that our proposed
single-stage approach for WSSS not only outperforms state-of-the-art benchmarks
remarkably but also surpasses multi-stage methodologies that trade complexity
for accuracy. The code can be found at
https://github.com/Jessie459/feature-self-reinforcement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jingxuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lechao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1&quot;&gt;Chaowei Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zunlei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1&quot;&gt;Tingting Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1&quot;&gt;Mingli Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08917">
<title>An Incremental Unified Framework for Small Defect Inspection. (arXiv:2312.08917v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08917</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence (AI)-driven defect inspection is pivotal in
industrial manufacturing. Yet, many methods, tailored to specific pipelines,
grapple with diverse product portfolios and evolving processes. Addressing
this, we present the Incremental Unified Framework (IUF) that can reduce the
feature conflict problem when continuously integrating new objects in the
pipeline, making it advantageous in object-incremental learning scenarios.
Employing a state-of-the-art transformer, we introduce Object-Aware
Self-Attention (OASA) to delineate distinct semantic boundaries. Semantic
Compression Loss (SCL) is integrated to optimize non-primary semantic space,
enhancing network adaptability for novel objects. Additionally, we prioritize
retaining the features of established objects during weight updates.
Demonstrating prowess in both image and pixel-level defect inspection, our
approach achieves state-of-the-art performance, proving indispensable for
dynamic and scalable industrial inspections. Our code will be released at
https://github.com/jqtangust/IUF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaogang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruizheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Sixing Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1&quot;&gt;Tsz Wa Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_M/0/1/0/all/0/1&quot;&gt;Ming Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Ying-Cong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsung_F/0/1/0/all/0/1&quot;&gt;Fugee Tsung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08924">
<title>Training-free Zero-shot Composed Image Retrieval with Local Concept Reranking. (arXiv:2312.08924v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08924</link>
<description rdf:parseType="Literal">&lt;p&gt;Composed image retrieval attempts to retrieve an image of interest from
gallery images through a composed query of a reference image and its
corresponding modified text. It has recently attracted attention due to the
collaboration of information-rich images and concise language to precisely
express the requirements of target images. Most of the existing composed image
retrieval methods follow a supervised learning paradigm to perform training on
a costly triplet dataset composed of a reference image, modified text, and a
corresponding target image. To alleviate the demand for difficult-to-obtain
labeled triplet data, recent methods have introduced zero-shot composed image
retrieval (ZS-CIR), which aims to retrieve the target image without the
supervision of human-labeled triplets but instead relies on image-text pairs or
self-generated triplets. However, these methods are less computationally
efficient due to the requirement of training and also less understandable,
assuming that the interaction between image and text is conducted with implicit
query embedding. In this work, we present a new Training-Free zero-shot
Composed Image Retrieval (TFCIR) method which translates the query into
explicit human-understandable text. This helps improve computation efficiency
while maintaining the generalization of foundation models. Further, we
introduce a Local Concept Reranking (LCR) mechanism to focus on discriminative
local information extracted from the modified instruction. Extensive
experiments on three ZS-CIR benchmarks show that the proposed approach can
achieve comparable performances with state-of-the-art methods and significantly
outperforms other training-free methods on the open domain datasets, CIRR and
CIRCO, as well as the fashion domain dataset, FashionIQ.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shitong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1&quot;&gt;Fanghua Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1&quot;&gt;Shaogang Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08932">
<title>Influence of Prompting Strategies on Segment Anything Model (SAM) for Short-axis Cardiac MRI segmentation. (arXiv:2312.08932v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08932</link>
<description rdf:parseType="Literal">&lt;p&gt;The Segment Anything Model (SAM) has recently emerged as a significant
breakthrough in foundation models, demonstrating remarkable zero-shot
performance in object segmentation tasks. While SAM is designed for
generalization, it exhibits limitations in handling specific medical imaging
tasks that require fine-structure segmentation or precise boundaries. In this
paper, we focus on the task of cardiac magnetic resonance imaging (cMRI)
short-axis view segmentation using the SAM foundation model. We conduct a
comprehensive investigation of the impact of different prompting strategies
(including bounding boxes, positive points, negative points, and their
combinations) on segmentation performance. We evaluate on two public datasets
using the baseline model and models fine-tuned with varying amounts of
annotated data, ranging from a limited number of volumes to a fully annotated
dataset. Our findings indicate that prompting strategies significantly
influence segmentation performance. Combining positive points with either
bounding boxes or negative points shows substantial benefits, but little to no
benefit when combined simultaneously. We further observe that fine-tuning SAM
with a few annotated volumes improves segmentation performance when properly
prompted. Specifically, fine-tuning with bounding boxes has a positive impact,
while fine-tuning without bounding boxes leads to worse results compared to
baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stein_J/0/1/0/all/0/1&quot;&gt;Josh Stein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Folco_M/0/1/0/all/0/1&quot;&gt;Maxime Di Folco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1&quot;&gt;Julia A. Schnabel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08939">
<title>EAT: Towards Long-Tailed Out-of-Distribution Detection. (arXiv:2312.08939v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08939</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite recent advancements in out-of-distribution (OOD) detection, most
current studies assume a class-balanced in-distribution training dataset, which
is rarely the case in real-world scenarios. This paper addresses the
challenging task of long-tailed OOD detection, where the in-distribution data
follows a long-tailed class distribution. The main difficulty lies in
distinguishing OOD data from samples belonging to the tail classes, as the
ability of a classifier to detect OOD instances is not strongly correlated with
its accuracy on the in-distribution classes. To overcome this issue, we propose
two simple ideas: (1) Expanding the in-distribution class space by introducing
multiple abstention classes. This approach allows us to build a detector with
clear decision boundaries by training on OOD data using virtual labels. (2)
Augmenting the context-limited tail classes by overlaying images onto the
context-rich OOD data. This technique encourages the model to pay more
attention to the discriminative features of the tail classes. We provide a clue
for separating in-distribution and OOD data by analyzing gradient noise.
Through extensive experiments, we demonstrate that our method outperforms the
current state-of-the-art on various benchmark datasets. Moreover, our method
can be used as an add-on for existing long-tail learning approaches,
significantly enhancing their OOD detection performance. Code is available at:
https://github.com/Stomach-ache/Long-Tailed-OOD-Detection .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1&quot;&gt;Tong Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo-Lin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min-Ling Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08951">
<title>Multi-Scene Generalized Trajectory Global Graph Solver with Composite Nodes for Multiple Object Tracking. (arXiv:2312.08951v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08951</link>
<description rdf:parseType="Literal">&lt;p&gt;The global multi-object tracking (MOT) system can consider interaction,
occlusion, and other ``visual blur&apos;&apos; scenarios to ensure effective object
tracking in long videos. Among them, graph-based tracking-by-detection
paradigms achieve surprising performance. However, their fully-connected nature
poses storage space requirements that challenge algorithm handling long videos.
Currently, commonly used methods are still generated trajectories by building
one-forward associations across frames. Such matches produced under the
guidance of first-order similarity information may not be optimal from a
longer-time perspective. Moreover, they often lack an end-to-end scheme for
correcting mismatches. This paper proposes the Composite Node Message Passing
Network (CoNo-Link), a multi-scene generalized framework for modeling
ultra-long frames information for association. CoNo-Link&apos;s solution is a
low-storage overhead method for building constrained connected graphs. In
addition to the previous method of treating objects as nodes, the network
innovatively treats object trajectories as nodes for information interaction,
improving the graph neural network&apos;s feature representation capability.
Specifically, we formulate the graph-building problem as a top-k selection task
for some reliable objects or trajectories. Our model can learn better
predictions on longer-time scales by adding composite nodes. As a result, our
method outperforms the state-of-the-art in several commonly used datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haojun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nannan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08952">
<title>UCMCTrack: Multi-Object Tracking with Uniform Camera Motion Compensation. (arXiv:2312.08952v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08952</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-object tracking (MOT) in video sequences remains a challenging task,
especially in scenarios with significant camera movements. This is because
targets can drift considerably on the image plane, leading to erroneous
tracking outcomes. Addressing such challenges typically requires supplementary
appearance cues or Camera Motion Compensation (CMC). While these strategies are
effective, they also introduce a considerable computational burden, posing
challenges for real-time MOT. In response to this, we introduce UCMCTrack, a
novel motion model-based tracker robust to camera movements. Unlike
conventional CMC that computes compensation parameters frame-by-frame,
UCMCTrack consistently applies the same compensation parameters throughout a
video sequence. It employs a Kalman filter on the ground plane and introduces
the Mapped Mahalanobis Distance (MMD) as an alternative to the traditional
Intersection over Union (IoU) distance measure. By leveraging projected
probability distributions on the ground plane, our approach efficiently
captures motion patterns and adeptly manages uncertainties introduced by
homography projections. Remarkably, UCMCTrack, relying solely on motion cues,
achieves state-of-the-art performance across a variety of challenging datasets,
including MOT17, MOT20, DanceTrack and KITTI, with an exceptional speed of over
1000 FPS on a single CPU. More details and code are available at
https://github.com/corfyi/UCMCTrack
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1&quot;&gt;Kefu Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1&quot;&gt;Kai Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiaolei Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiangui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Rongdong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1&quot;&gt;Wei Hao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08962">
<title>Depicting Beyond Scores: Advancing Image Quality Assessment through Multi-modal Language Models. (arXiv:2312.08962v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08962</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a Depicted image Quality Assessment method (DepictQA),
overcoming the constraints of traditional score-based approaches. DepictQA
leverages Multi-modal Large Language Models (MLLMs), allowing for detailed,
language-based, human-like evaluation of image quality. Unlike conventional
Image Quality Assessment (IQA) methods relying on scores, DepictQA interprets
image content and distortions descriptively and comparatively, aligning closely
with humans&apos; reasoning process. To build the DepictQA model, we establish a
hierarchical task framework, and collect a multi-modal IQA training dataset,
named M-BAPPS. To navigate the challenges in limited training data and
processing multiple images, we propose to use multi-source training data and
specialized image tags. Our DepictQA demonstrates a better performance than
score-based methods on the BAPPS benchmark. Moreover, compared with general
MLLMs, our DepictQA can generate more accurate reasoning descriptive languages.
Our research indicates that language-based IQA methods have the potential to be
customized for individual preferences. Datasets and codes will be released
publicly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zheyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jinjin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhenfei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1&quot;&gt;Tianfan Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1&quot;&gt;Chao Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08963">
<title>LEMON: Learning 3D Human-Object Interaction Relation from 2D Images. (arXiv:2312.08963v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08963</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning 3D human-object interaction relation is pivotal to embodied AI and
interaction modeling. Most existing methods approach the goal by learning to
predict isolated interaction elements, e.g., human contact, object affordance,
and human-object spatial relation, primarily from the perspective of either the
human or the object. Which underexploit certain correlations between the
interaction counterparts (human and object), and struggle to address the
uncertainty in interactions. Actually, objects&apos; functionalities potentially
affect humans&apos; interaction intentions, which reveals what the interaction is.
Meanwhile, the interacting humans and objects exhibit matching geometric
structures, which presents how to interact. In light of this, we propose
harnessing these inherent correlations between interaction counterparts to
mitigate the uncertainty and jointly anticipate the above interaction elements
in 3D space. To achieve this, we present LEMON (LEarning 3D huMan-Object
iNteraction relation), a unified model that mines interaction intentions of the
counterparts and employs curvatures to guide the extraction of geometric
correlations, combining them to anticipate the interaction elements. Besides,
the 3D Interaction Relation dataset (3DIR) is collected to serve as the test
bed for training and evaluation. Extensive experiments demonstrate the
superiority of LEMON over methods estimating each element in isolation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuhang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1&quot;&gt;Wei Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1&quot;&gt;Hongchen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1&quot;&gt;Zheng-Jun Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08975">
<title>On Mask-based Image Set Desensitization with Recognition Support. (arXiv:2312.08975v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08975</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, Deep Neural Networks (DNN) have emerged as a practical
method for image recognition. The raw data, which contain sensitive
information, are generally exploited within the training process. However, when
the training process is outsourced to a third-party organization, the raw data
should be desensitized before being transferred to protect sensitive
information. Although masks are widely applied to hide important sensitive
information, preventing inpainting masked images is critical, which may restore
the sensitive information. The corresponding models should be adjusted for the
masked images to reduce the degradation of the performance for recognition or
classification tasks due to the desensitization of images. In this paper, we
propose a mask-based image desensitization approach while supporting
recognition. This approach consists of a mask generation algorithm and a model
adjustment method. We propose exploiting an interpretation algorithm to
maintain critical information for the recognition task in the mask generation
algorithm. In addition, we propose a feature selection masknet as the model
adjustment method to improve the performance based on the masked images.
Extensive experimentation results based on multiple image datasets reveal
significant advantages (up to 9.34% in terms of accuracy) of our approach for
image desensitization while supporting recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qilong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Ji Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chongsheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1&quot;&gt;Dejing Dou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08977">
<title>Weighted Ensemble Models Are Strong Continual Learners. (arXiv:2312.08977v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08977</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we study the problem of continual learning (CL) where the goal
is to learn a model on a sequence of tasks, such that the data from the
previous tasks becomes unavailable while learning on the current task data. CL
is essentially a balancing act between being able to learn on the new task
(i.e., plasticity) and maintaining the performance on the previously learned
concepts (i.e., stability). With an aim to address the stability-plasticity
trade-off, we propose to perform weight-ensembling of the model parameters of
the previous and current task. This weight-ensembled model, which we call
Continual Model Averaging (or CoMA), attains high accuracy on the current task
by leveraging plasticity, while not deviating too far from the previous weight
configuration, ensuring stability. We also propose an improved variant of CoMA,
named Continual Fisher-weighted Model Averaging (or CoFiMA), that selectively
weighs each parameter in the weight ensemble by leveraging the Fisher
information of the weights of the model. Both the variants are conceptually
simple, easy to implement, and effective in attaining state-of-the-art
performance on several standard CL benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marouf_I/0/1/0/all/0/1&quot;&gt;Imad Eddine Marouf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Subhankar Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tartaglione_E/0/1/0/all/0/1&quot;&gt;Enzo Tartaglione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Lathuili&amp;#xe8;re&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08983">
<title>Interactive Humanoid: Online Full-Body Motion Reaction Synthesis with Social Affordance Canonicalization and Forecasting. (arXiv:2312.08983v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08983</link>
<description rdf:parseType="Literal">&lt;p&gt;We focus on the human-humanoid interaction task optionally with an object. We
propose a new task named online full-body motion reaction synthesis, which
generates humanoid reactions based on the human actor&apos;s motions. The previous
work only focuses on human interaction without objects and generates body
reactions without hand. Besides, they also do not consider the task as an
online setting, which means the inability to observe information beyond the
current moment in practical situations. To support this task, we construct two
datasets named HHI and CoChair and propose a unified method. Specifically, we
propose to construct a social affordance representation. We first select a
social affordance carrier and use SE(3)-Equivariant Neural Networks to learn
the local frame for the carrier, then we canonicalize the social affordance.
Besides, we propose a social affordance forecasting scheme to enable the
reactor to predict based on the imagined future. Experiments demonstrate that
our approach can effectively generate high-quality reactions on HHI and
CoChair. Furthermore, we also validate our method on existing human interaction
datasets Interhuman and Chi3D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yunze Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changxi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1&quot;&gt;Li Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08984">
<title>CL2CM: Improving Cross-Lingual Cross-Modal Retrieval via Cross-Lingual Knowledge Transfer. (arXiv:2312.08984v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08984</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-lingual cross-modal retrieval has garnered increasing attention
recently, which aims to achieve the alignment between vision and target
language (V-T) without using any annotated V-T data pairs. Current methods
employ machine translation (MT) to construct pseudo-parallel data pairs, which
are then used to learn a multi-lingual and multi-modal embedding space that
aligns visual and target-language representations. However, the large
heterogeneous gap between vision and text, along with the noise present in
target language translations, poses significant challenges in effectively
aligning their representations. To address these challenges, we propose a
general framework, Cross-Lingual to Cross-Modal (CL2CM), which improves the
alignment between vision and target language using cross-lingual transfer. This
approach allows us to fully leverage the merits of multi-lingual pre-trained
models (e.g., mBERT) and the benefits of the same modality structure, i.e.,
smaller gap, to provide reliable and comprehensive semantic correspondence
(knowledge) for the cross-modal network. We evaluate our proposed approach on
two multilingual image-text datasets, Multi30K and MSCOCO, and one video-text
dataset, VATEX. The results clearly demonstrate the effectiveness of our
proposed method and its high potential for large-scale retrieval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yabing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jianfeng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1&quot;&gt;Hao Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08985">
<title>OMG: Towards Open-vocabulary Motion Generation via Mixture of Controllers. (arXiv:2312.08985v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08985</link>
<description rdf:parseType="Literal">&lt;p&gt;We have recently seen tremendous progress in realistic text-to-motion
generation. Yet, the existing methods often fail or produce implausible motions
with unseen text inputs, which limits the applications. In this paper, we
present OMG, a novel framework, which enables compelling motion generation from
zero-shot open-vocabulary text prompts. Our key idea is to carefully tailor the
pretrain-then-finetune paradigm into the text-to-motion generation. At the
pre-training stage, our model improves the generation ability by learning the
rich out-of-domain inherent motion traits. To this end, we scale up a large
unconditional diffusion model up to 1B parameters, so as to utilize the massive
unlabeled motion data up to over 20M motion instances. At the subsequent
fine-tuning stage, we introduce motion ControlNet, which incorporates text
prompts as conditioning information, through a trainable copy of the
pre-trained model and the proposed novel Mixture-of-Controllers (MoC) block.
MoC block adaptively recognizes various ranges of the sub-motions with a
cross-attention mechanism and processes them separately with the
text-token-specific experts. Such a design effectively aligns the CLIP token
embeddings of text prompts to various ranges of compact and expressive motion
features. Extensive experiments demonstrate that our OMG achieves significant
improvements over the state-of-the-art methods on zero-shot text-to-motion
generation. Project page: https://tr3e.github.io/omg-page.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Han Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1&quot;&gt;Jiacheng Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruichi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Sihan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuecheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Sibei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jingyi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09005">
<title>Scene 3-D Reconstruction System in Scattering Medium. (arXiv:2312.09005v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09005</link>
<description rdf:parseType="Literal">&lt;p&gt;The research on neural radiance fields for new view synthesis has experienced
explosive growth with the development of new models and extensions. The NERF
algorithm, suitable for underwater scenes or scattering media, is also
evolving. Existing underwater 3D reconstruction systems still face challenges
such as extensive training time and low rendering efficiency. This paper
proposes an improved underwater 3D reconstruction system to address these
issues and achieve rapid, high-quality 3D reconstruction.To begin with, we
enhance underwater videos captured by a monocular camera to correct the poor
image quality caused by the physical properties of the water medium while
ensuring consistency in enhancement across adjacent frames. Subsequently, we
perform keyframe selection on the video frames to optimize resource utilization
and eliminate the impact of dynamic objects on the reconstruction results. The
selected keyframes, after pose estimation using COLMAP, undergo a
three-dimensional reconstruction improvement process using neural radiance
fields based on multi-resolution hash coding for model construction and
rendering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhuoyifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haoming Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09008">
<title>Style Injection in Diffusion: A Training-free Approach for Adapting Large-scale Diffusion Models for Style Transfer. (arXiv:2312.09008v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09008</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the impressive generative capabilities of diffusion models, existing
diffusion model-based style transfer methods require inference-stage
optimization (e.g. fine-tuning or textual inversion of style) which is
time-consuming, or fails to leverage the generative ability of large-scale
diffusion models. To address these issues, we introduce a novel artistic style
transfer method based on a pre-trained large-scale diffusion model without any
optimization. Specifically, we manipulate the features of self-attention layers
as the way the cross-attention mechanism works; in the generation process,
substituting the key and value of content with those of style image. This
approach provides several desirable characteristics for style transfer
including 1) preservation of content by transferring similar styles into
similar image patches and 2) transfer of style based on similarity of local
texture (e.g. edge) between content and style images. Furthermore, we introduce
query preservation and attention temperature scaling to mitigate the issue of
disruption of original content, and initial latent Adaptive Instance
Normalization (AdaIN) to deal with the disharmonious color (failure to transfer
the colors of style). Our experimental results demonstrate that our proposed
method surpasses state-of-the-art methods in both conventional and
diffusion-based style transfer baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1&quot;&gt;Jiwoo Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hyun_S/0/1/0/all/0/1&quot;&gt;Sangeek Hyun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1&quot;&gt;Jae-Pil Heo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09020">
<title>Exploring Transferability for Randomized Smoothing. (arXiv:2312.09020v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09020</link>
<description rdf:parseType="Literal">&lt;p&gt;Training foundation models on extensive datasets and then finetuning them on
specific tasks has emerged as the mainstream approach in artificial
intelligence. However, the model robustness, which is a critical aspect for
safety, is often optimized for each specific task rather than at the
pretraining stage. In this paper, we propose a method for pretraining
certifiably robust models that can be readily finetuned for adaptation to a
particular task. A key challenge is dealing with the compromise between
semantic learning and robustness. We address this with a simple yet highly
effective strategy based on significantly broadening the pretraining data
distribution, which is shown to greatly benefit finetuning for downstream
tasks. Through pretraining on a mixture of clean and various noisy images, we
find that surprisingly strong certified accuracy can be achieved even when
finetuning on only clean images. Furthermore, this strategy requires just a
single model to deal with various noise levels, thus substantially reducing
computational costs in relation to previous works that employ multiple models.
Despite using just one model, our method can still yield results that are on
par with, or even superior to, existing multi-model methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_K/0/1/0/all/0/1&quot;&gt;Kai Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huishuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhirong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Stephen Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09022">
<title>Brain Diffuser with Hierarchical Transformer for MCI Causality Analysis. (arXiv:2312.09022v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.09022</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective connectivity estimation plays a crucial role in understanding the
interactions and information flow between different brain regions. However, the
functional time series used for estimating effective connentivity is derived
from certain software, which may lead to large computing errors because of
different parameter settings and degrade the ability to model complex causal
relationships between brain regions. In this paper, a brain diffuser with
hierarchical transformer (BDHT) is proposed to estimate effective connectivity
for mild cognitive impairment (MCI) analysis. To our best knowledge, the
proposed brain diffuer is the first generative model to apply diffusion models
in the application of generating and analyzing multimodal brain networks.
Specifically, the BDHT leverages the structural connectivity to guide the
reverse processes in an efficient way. It makes the denoising process more
reliable and guarantees effective connectivity estimation accuracy. To improve
denoising quality, the hierarchical denoising transformer is designed to learn
multi-scale features in topological space. Furthermore, the GraphConFormer
block can concentrate on both global and adjacent connectivity information. By
stacking the multi-head attention and graph convolutional network, the proposed
model enhances structure-function complementarity and improves the ability in
noise estimation. Experimental evaluations of the denoising diffusion model
demonstrate its effectiveness in estimating effective connectivity. The method
achieves superior performance in terms of accuracy and robustness compared to
existing approaches. It can captures both unidirectal and bidirectional
interactions between brain regions, providing a comprehensive understanding of
the brain&apos;s information processing mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zuo_Q/0/1/0/all/0/1&quot;&gt;Qiankun Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Ling Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuqiang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09028">
<title>Design Space Exploration of Low-Bit Quantized Neural Networks for Visual Place Recognition. (arXiv:2312.09028v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09028</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual Place Recognition (VPR) is a critical task for performing global
re-localization in visual perception systems. It requires the ability to
accurately recognize a previously visited location under variations such as
illumination, occlusion, appearance and viewpoint. In the case of robotic
systems and augmented reality, the target devices for deployment are battery
powered edge devices. Therefore whilst the accuracy of VPR methods is important
so too is memory consumption and latency. Recently new works have focused on
the recall@1 metric as a performance measure with limited focus on resource
utilization. This has resulted in methods that use deep learning models too
large to deploy on low powered edge devices. We hypothesize that these large
models are highly over-parameterized and can be optimized to satisfy the
constraints of a low powered embedded system whilst maintaining high recall
performance. Our work studies the impact of compact convolutional network
architecture design in combination with full-precision and mixed-precision
post-training quantization on VPR performance. Importantly we not only measure
performance via the recall@1 score but also measure memory consumption and
latency. We characterize the design implications on memory, latency and recall
scores and provide a number of design recommendations for VPR systems under
these resource limitations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grainge_O/0/1/0/all/0/1&quot;&gt;Oliver Grainge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1&quot;&gt;Michael Milford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bodala_I/0/1/0/all/0/1&quot;&gt;Indu Bodala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramchurn_S/0/1/0/all/0/1&quot;&gt;Sarvapali D. Ramchurn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehsan_S/0/1/0/all/0/1&quot;&gt;Shoaib Ehsan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09030">
<title>Dual Branch Network Towards Accurate Printed Mathematical Expression Recognition. (arXiv:2312.09030v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09030</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past years, Printed Mathematical Expression Recognition (PMER) has
progressed rapidly. However, due to the insufficient context information
captured by Convolutional Neural Networks, some mathematical symbols might be
incorrectly recognized or missed. To tackle this problem, in this paper, a Dual
Branch transformer-based Network (DBN) is proposed to learn both local and
global context information for accurate PMER. In our DBN, local and global
features are extracted simultaneously, and a Context Coupling Module (CCM) is
developed to complement the features between the global and local contexts. CCM
adopts an interactive manner so that the coupled context clues are highly
correlated to each expression symbol. Additionally, we design a Dynamic Soft
Target (DST) strategy to utilize the similarities among symbol categories for
reasonable label generation. Our experimental results have demonstrated that
DBN can accurately recognize mathematical expressions and has achieved
state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhaokun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shuaijian Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1&quot;&gt;Zhongjie Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuesheng Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09031">
<title>iComMa: Inverting 3D Gaussians Splatting for Camera Pose Estimation via Comparing and Matching. (arXiv:2312.09031v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09031</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a method named iComMa to address the 6D pose estimation problem in
computer vision. The conventional pose estimation methods typically rely on the
target&apos;s CAD model or necessitate specific network training tailored to
particular object classes. Some existing methods address mesh-free 6D pose
estimation by employing the inversion of a Neural Radiance Field (NeRF), aiming
to overcome the aforementioned constraints. However, it still suffers from
adverse initializations. By contrast, we model the pose estimation as the
problem of inverting the 3D Gaussian Splatting (3DGS) with both the comparing
and matching loss. In detail, a render-and-compare strategy is adopted for the
precise estimation of poses. Additionally, a matching module is designed to
enhance the model&apos;s robustness against adverse initializations by minimizing
the distances between 2D keypoints. This framework systematically incorporates
the distinctive characteristics and inherent rationale of render-and-compare
and matching-based approaches. This comprehensive consideration equips the
framework to effectively address a broader range of intricate and challenging
scenarios, including instances with substantial angular deviations, all while
maintaining a high level of prediction accuracy. Experimental results
demonstrate the superior precision and robustness of our proposed jointly
optimized framework when evaluated on synthetic and complex real-world data in
challenging scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunfan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Caigui Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09037">
<title>Impact of Ground Truth Quality on Handwriting Recognition. (arXiv:2312.09037v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09037</link>
<description rdf:parseType="Literal">&lt;p&gt;Handwriting recognition is a key technology for accessing the content of old
manuscripts, helping to preserve cultural heritage. Deep learning shows an
impressive performance in solving this task. However, to achieve its full
potential, it requires a large amount of labeled data, which is difficult to
obtain for ancient languages and scripts. Often, a trade-off has to be made
between ground truth quantity and quality, as is the case for the recently
introduced Bullinger database. It contains an impressive amount of over a
hundred thousand labeled text line images of mostly premodern German and Latin
texts that were obtained by automatically aligning existing page-level
transcriptions with text line images. However, the alignment process introduces
systematic errors, such as wrongly hyphenated words. In this paper, we
investigate the impact of such errors on training and evaluation and suggest
means to detect and correct typical alignment errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jungo_M/0/1/0/all/0/1&quot;&gt;Michael Jungo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogtlin_L/0/1/0/all/0/1&quot;&gt;Lars V&amp;#xf6;gtlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fakhari_A/0/1/0/all/0/1&quot;&gt;Atefeh Fakhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wegmann_N/0/1/0/all/0/1&quot;&gt;Nathan Wegmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ingold_R/0/1/0/all/0/1&quot;&gt;Rolf Ingold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1&quot;&gt;Andreas Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scius_Bertrand_A/0/1/0/all/0/1&quot;&gt;Anna Scius-Bertrand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09038">
<title>Object Recognition from Scientific Document based on Compartment Refinement Framework. (arXiv:2312.09038v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09038</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of the internet in the past decade, it has become
increasingly important to extract valuable information from vast resources
efficiently, which is crucial for establishing a comprehensive digital
ecosystem, particularly in the context of research surveys and comprehension.
The foundation of these tasks focuses on accurate extraction and deep mining of
data from scientific documents, which are essential for building a robust data
infrastructure. However, parsing raw data or extracting data from complex
scientific documents have been ongoing challenges. Current data extraction
methods for scientific documents typically use rule-based (RB) or machine
learning (ML) approaches. However, using rule-based methods can incur high
coding costs for articles with intricate typesetting. Conversely, relying
solely on machine learning methods necessitates annotation work for complex
content types within the scientific document, which can be costly.
Additionally, few studies have thoroughly defined and explored the hierarchical
layout within scientific documents. The lack of a comprehensive definition of
the internal structure and elements of the documents indirectly impacts the
accuracy of text classification and object recognition tasks. From the
perspective of analyzing the standard layout and typesetting used in the
specified publication, we propose a new document layout analysis framework
called CTBR(Compartment &amp;amp; Text Blocks Refinement). Firstly, we define
scientific documents into hierarchical divisions: base domain, compartment, and
text blocks. Next, we conduct an in-depth exploration and classification of the
meanings of text blocks. Finally, we utilize the results of text block
classification to implement object recognition within scientific documents
based on rule-based compartment segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinghong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1&quot;&gt;Wen Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ota_K/0/1/0/all/0/1&quot;&gt;Koichi Ota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasegawa_S/0/1/0/all/0/1&quot;&gt;Shinobu Hasegawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.04720">
<title>Amicable Aid: Perturbing Images to Improve Classification Performance. (arXiv:2112.04720v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2112.04720</link>
<description rdf:parseType="Literal">&lt;p&gt;While adversarial perturbation of images to attack deep image classification
models pose serious security concerns in practice, this paper suggests a novel
paradigm where the concept of image perturbation can benefit classification
performance, which we call amicable aid. We show that by taking the opposite
search direction of perturbation, an image can be modified to yield higher
classification confidence and even a misclassified image can be made correctly
classified. This can be also achieved with a large amount of perturbation by
which the image is made unrecognizable by human eyes. The mechanism of the
amicable aid is explained in the viewpoint of the underlying natural image
manifold. Furthermore, we investigate the universal amicable aid, i.e., a fixed
perturbation can be applied to multiple images to improve their classification
results. While it is challenging to find such perturbations, we show that
making the decision boundary as perpendicular to the image manifold as possible
via training with modified data is effective to obtain a model for which
universal amicable perturbations are more easily found.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Juyeop Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jun-Ho Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1&quot;&gt;Soobeom Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jong-Seok Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.17255">
<title>A Cognitive Architecture for Machine Consciousness and Artificial Superintelligence: Thought Is Structured by the Iterative Updating of Working Memory. (arXiv:2203.17255v6 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2203.17255</link>
<description rdf:parseType="Literal">&lt;p&gt;This article provides an analytical framework for how to simulate human-like
thought processes within a computer. It describes how attention and memory
should be structured, updated, and utilized to search for associative additions
to the stream of thought. The focus is on replicating the dynamics of the
mammalian working memory system, which features two forms of persistent
activity: sustained firing (preserving information on the order of seconds) and
synaptic potentiation (preserving information from minutes to hours). The
article uses a series of over 40 original figures to systematically demonstrate
how the iterative updating of these working memory stores provides functional
structure to behavior, cognition, and consciousness.
&lt;/p&gt;
&lt;p&gt;In an AI implementation, these two memory stores should be updated
continuously and in an iterative fashion, meaning each state should preserve a
proportion of the coactive representations from the state before it. Thus, the
set of concepts in working memory will evolve gradually and incrementally over
time. This makes each state a revised iteration of the preceding state and
causes successive states to overlap and blend with respect to the information
they contain. Transitions between states happen as persistent activity spreads
activation energy throughout the hierarchical network searching long-term
memory for the most appropriate representation to be added to the global
workspace. The result is a chain of associatively linked intermediate states
capable of advancing toward a solution or goal. Iterative updating is
conceptualized here as an information processing strategy, a model of working
memory, a theory of consciousness, and an algorithm for designing and
programming artificial general intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Reser_J/0/1/0/all/0/1&quot;&gt;Jared Edward Reser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.10347">
<title>Diverse super-resolution with pretrained deep hiererarchical VAEs. (arXiv:2205.10347v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.10347</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the problem of producing diverse solutions to an image
super-resolution problem. From a probabilistic perspective, this can be done by
sampling from the posterior distribution of an inverse problem, which requires
the definition of a prior distribution on the high-resolution images. In this
work, we propose to use a pretrained hierarchical variational autoencoder
(HVAE) as a prior. We train a lightweight stochastic encoder to encode
low-resolution images in the latent space of a pretrained HVAE. At inference,
we combine the low-resolution encoder and the pretrained generative model to
super-resolve an image. We demonstrate on the task of face super-resolution
that our method provides an advantageous trade-off between the computational
efficiency of conditional normalizing flows techniques and the sample quality
of diffusion based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prost_J/0/1/0/all/0/1&quot;&gt;Jean Prost&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houdard_A/0/1/0/all/0/1&quot;&gt;Antoine Houdard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almansa_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Almansa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadakis_N/0/1/0/all/0/1&quot;&gt;Nicolas Papadakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.05575">
<title>MammoFL: Mammographic Breast Density Estimation using Federated Learning. (arXiv:2206.05575v5 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.05575</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we automate quantitative mammographic breast density
estimation with neural networks and show that this tool is a strong use case
for federated learning on multi-institutional datasets. Our dataset included
bilateral CC-view and MLO-view mammographic images from two separate
institutions. Two U-Nets were separately trained on algorithm-generated labels
to perform segmentation of the breast and dense tissue from these images and
subsequently calculate breast percent density (PD). The networks were trained
with federated learning and compared to three non-federated baselines, one
trained on each single-institution dataset and one trained on the aggregated
multi-institution dataset. We demonstrate that training on multi-institution
datasets is critical to algorithm generalizability. We further show that
federated learning on multi-institutional datasets improves model
generalization to unseen data at nearly the same level as centralized training
on multi-institutional datasets, indicating that federated learning can be
applied to our method to improve algorithm generalizability while maintaining
patient privacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Muthukrishnan_R/0/1/0/all/0/1&quot;&gt;Ramya Muthukrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Heyler_A/0/1/0/all/0/1&quot;&gt;Angelina Heyler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Katti_K/0/1/0/all/0/1&quot;&gt;Keshava Katti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pati_S/0/1/0/all/0/1&quot;&gt;Sarthak Pati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mankowski_W/0/1/0/all/0/1&quot;&gt;Walter Mankowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alahari_A/0/1/0/all/0/1&quot;&gt;Aprupa Alahari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sanborn_M/0/1/0/all/0/1&quot;&gt;Michael Sanborn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Conant_E/0/1/0/all/0/1&quot;&gt;Emily F. Conant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Scott_C/0/1/0/all/0/1&quot;&gt;Christopher Scott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Winham_S/0/1/0/all/0/1&quot;&gt;Stacey Winham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vachon_C/0/1/0/all/0/1&quot;&gt;Celine Vachon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chaudhari_P/0/1/0/all/0/1&quot;&gt;Pratik Chaudhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kontos_D/0/1/0/all/0/1&quot;&gt;Despina Kontos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bakas_S/0/1/0/all/0/1&quot;&gt;Spyridon Bakas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.07207">
<title>Beyond Grounding: Extracting Fine-Grained Event Hierarchies Across Modalities. (arXiv:2206.07207v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.07207</link>
<description rdf:parseType="Literal">&lt;p&gt;Events describe happenings in our world that are of importance. Naturally,
understanding events mentioned in multimedia content and how they are related
forms an important way of comprehending our world. Existing literature can
infer if events across textual and visual (video) domains are identical (via
grounding) and thus, on the same semantic level. However, grounding fails to
capture the intricate cross-event relations that exist due to the same events
being referred to on many semantic levels. For example, in Figure 1, the
abstract event of &quot;war&quot; manifests at a lower semantic level through subevents
&quot;tanks firing&quot; (in video) and airplane &quot;shot&quot; (in text), leading to a
hierarchical, multimodal relationship between the events.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose the task of extracting event hierarchies from
multimodal (video and text) data to capture how the same event manifests itself
in different modalities at different semantic levels. This reveals the
structure of events and is critical to understanding them. To support research
on this task, we introduce the Multimodal Hierarchical Events (MultiHiEve)
dataset. Unlike prior video-language datasets, MultiHiEve is composed of news
video-article pairs, which makes it rich in event hierarchies. We densely
annotate a part of the dataset to construct the test benchmark. We show the
limitations of state-of-the-art unimodal and multimodal baselines on this task.
Further, we address these limitations via a new weakly supervised model,
leveraging only unannotated video-article pairs from MultiHiEve. We perform a
thorough evaluation of our proposed method which demonstrates improved
performance on this task and highlight opportunities for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayyubi_H/0/1/0/all/0/1&quot;&gt;Hammad A. Ayyubi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomas_C/0/1/0/all/0/1&quot;&gt;Christopher Thomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chum_L/0/1/0/all/0/1&quot;&gt;Lovish Chum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lokesh_R/0/1/0/all/0/1&quot;&gt;Rahul Lokesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Long Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1&quot;&gt;Yulei Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xudong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1&quot;&gt;Xuande Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koo_J/0/1/0/all/0/1&quot;&gt;Jaywon Koo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1&quot;&gt;Sounak Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shih-Fu Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.09753">
<title>Visualizing and Understanding Contrastive Learning. (arXiv:2206.09753v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.09753</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning has revolutionized the field of computer vision,
learning rich representations from unlabeled data, which generalize well to
diverse vision tasks. Consequently, it has become increasingly important to
explain these approaches and understand their inner workings mechanisms. Given
that contrastive models are trained with interdependent and interacting inputs
and aim to learn invariance through data augmentation, the existing methods for
explaining single-image systems (e.g., image classification models) are
inadequate as they fail to account for these factors and typically assume
independent inputs. Additionally, there is a lack of evaluation metrics
designed to assess pairs of explanations, and no analytical studies have been
conducted to investigate the effectiveness of different techniques used to
explaining contrastive learning. In this work, we design visual explanation
methods that contribute towards understanding similarity learning tasks from
pairs of images. We further adapt existing metrics, used to evaluate visual
explanations of image classification systems, to suit pairs of explanations and
evaluate our proposed methods with these metrics. Finally, we present a
thorough analysis of visual explainability methods for contrastive learning,
establish their correlation with downstream tasks and demonstrate the potential
of our approaches to investigate their merits and drawbacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sammani_F/0/1/0/all/0/1&quot;&gt;Fawaz Sammani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joukovsky_B/0/1/0/all/0/1&quot;&gt;Boris Joukovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1&quot;&gt;Nikos Deligiannis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.02529">
<title>Metadata-enhanced contrastive learning from retinal optical coherence tomography images. (arXiv:2208.02529v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.02529</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has potential to automate screening, monitoring and grading of
disease in medical images. Pretraining with contrastive learning enables models
to extract robust and generalisable features from natural image datasets,
facilitating label-efficient downstream image analysis. However, the direct
application of conventional contrastive methods to medical datasets introduces
two domain-specific issues. Firstly, several image transformations which have
been shown to be crucial for effective contrastive learning do not translate
from the natural image to the medical image domain. Secondly, the assumption
made by conventional methods, that any two images are dissimilar, is
systematically misleading in medical datasets depicting the same anatomy and
disease. This is exacerbated in longitudinal image datasets that repeatedly
image the same patient cohort to monitor their disease progression over time.
In this paper we tackle these issues by extending conventional contrastive
frameworks with a novel metadata-enhanced strategy. Our approach employs widely
available patient metadata to approximate the true set of inter-image
contrastive relationships. To this end we employ records for patient identity,
eye position (i.e. left or right) and time series information. In experiments
using two large longitudinal datasets containing 170,427 retinal OCT images of
7,912 patients with age-related macular degeneration (AMD), we evaluate the
utility of using metadata to incorporate the temporal dynamics of disease
progression into pretraining. Our metadata-enhanced approach outperforms both
standard contrastive methods and a retinal image foundation model in five out
of six image-level downstream tasks related to AMD. Due to its modularity, our
method can be quickly and cost-effectively tested to establish the potential
benefits of including available metadata in contrastive pretraining.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holland_R/0/1/0/all/0/1&quot;&gt;Robbie Holland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leingang_O/0/1/0/all/0/1&quot;&gt;Oliver Leingang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogunovic_H/0/1/0/all/0/1&quot;&gt;Hrvoje Bogunovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedl_S/0/1/0/all/0/1&quot;&gt;Sophie Riedl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fritsche_L/0/1/0/all/0/1&quot;&gt;Lars Fritsche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prevost_T/0/1/0/all/0/1&quot;&gt;Toby Prevost&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholl_H/0/1/0/all/0/1&quot;&gt;Hendrik P. N. Scholl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_Erfurth_U/0/1/0/all/0/1&quot;&gt;Ursula Schmidt-Erfurth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sivaprasad_S/0/1/0/all/0/1&quot;&gt;Sobha Sivaprasad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lotery_A/0/1/0/all/0/1&quot;&gt;Andrew J. Lotery&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menten_M/0/1/0/all/0/1&quot;&gt;Martin J. Menten&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.05788">
<title>Semantic Self-adaptation: Enhancing Generalization with a Single Sample. (arXiv:2208.05788v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.05788</link>
<description rdf:parseType="Literal">&lt;p&gt;The lack of out-of-domain generalization is a critical weakness of deep
networks for semantic segmentation. Previous studies relied on the assumption
of a static model, i. e., once the training process is complete, model
parameters remain fixed at test time. In this work, we challenge this premise
with a self-adaptive approach for semantic segmentation that adjusts the
inference process to each input sample. Self-adaptation operates on two levels.
First, it fine-tunes the parameters of convolutional layers to the input image
using consistency regularization. Second, in Batch Normalization layers,
self-adaptation interpolates between the training and the reference
distribution derived from a single test sample. Despite both techniques being
well known in the literature, their combination sets new state-of-the-art
accuracy on synthetic-to-real generalization benchmarks. Our empirical study
suggests that self-adaptation may complement the established practice of model
regularization at training time for improving deep network generalization to
out-of-domain data. Our code and pre-trained models are available at
https://github.com/visinf/self-adaptive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bahmani_S/0/1/0/all/0/1&quot;&gt;Sherwin Bahmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hahn_O/0/1/0/all/0/1&quot;&gt;Oliver Hahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zamfir_E/0/1/0/all/0/1&quot;&gt;Eduard Zamfir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araslanov_N/0/1/0/all/0/1&quot;&gt;Nikita Araslanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1&quot;&gt;Daniel Cremers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_S/0/1/0/all/0/1&quot;&gt;Stefan Roth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.05724">
<title>Concealing Sensitive Samples against Gradient Leakage in Federated Learning. (arXiv:2209.05724v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.05724</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) is a distributed learning paradigm that enhances
users privacy by eliminating the need for clients to share raw, private data
with the server. Despite the success, recent studies expose the vulnerability
of FL to model inversion attacks, where adversaries reconstruct users private
data via eavesdropping on the shared gradient information. We hypothesize that
a key factor in the success of such attacks is the low entanglement among
gradients per data within the batch during stochastic optimization. This
creates a vulnerability that an adversary can exploit to reconstruct the
sensitive data. Building upon this insight, we present a simple, yet effective
defense strategy that obfuscates the gradients of the sensitive data with
concealed samples. To achieve this, we propose synthesizing concealed samples
to mimic the sensitive data at the gradient level while ensuring their visual
dissimilarity from the actual sensitive data. Compared to the previous art, our
empirical evaluations suggest that the proposed technique provides the
strongest protection while simultaneously maintaining the FL performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jing Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1&quot;&gt;Munawar Hayat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1&quot;&gt;Mehrtash Harandi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.05742">
<title>Curved Representation Space of Vision Transformers. (arXiv:2210.05742v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.05742</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks with self-attention (a.k.a. Transformers) like ViT and Swin
have emerged as a better alternative to traditional convolutional neural
networks (CNNs). However, our understanding of how the new architecture works
is still limited. In this paper, we focus on the phenomenon that Transformers
show higher robustness against corruptions than CNNs, while not being
overconfident. This is contrary to the intuition that robustness increases with
confidence. We resolve this contradiction by empirically investigating how the
output of the penultimate layer moves in the representation space as the input
data moves linearly within a small area. In particular, we show the following.
(1) While CNNs exhibit fairly linear relationship between the input and output
movements, Transformers show nonlinear relationship for some data. For those
data, the output of Transformers moves in a curved trajectory as the input
moves linearly. (2) When a data is located in a curved region, it is hard to
move it out of the decision region since the output moves along a curved
trajectory instead of a straight line to the decision boundary, resulting in
high robustness of Transformers. (3) If a data is slightly modified to jump out
of the curved region, the movements afterwards become linear and the output
goes to the decision boundary directly. In other words, there does exist a
decision boundary near the data, which is hard to find only because of the
curved representation space. This explains the underconfident prediction of
Transformers. Also, we examine mathematical properties of the attention
operation that induce nonlinear response to linear perturbation. Finally, we
share our additional findings, regarding what contributes to the curved
representation space of Transformers, and how the curvedness evolves during
training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Juyeop Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Junha Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Songkuk Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jong-Seok Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.02997">
<title>3DGazeNet: Generalizing Gaze Estimation with Weak-Supervision from Synthetic Views. (arXiv:2212.02997v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.02997</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing gaze estimation models that generalize well to unseen domains and
in-the-wild conditions remains a challenge with no known best solution. This is
mostly due to the difficulty of acquiring ground truth data that cover the
distribution of faces, head poses, and environments that exist in the real
world. Most recent methods attempt to close the gap between specific source and
target domains using domain adaptation. In this work, we propose to train
general gaze estimation models which can be directly employed in novel
environments without adaptation. To do so, we leverage the observation that
head, body, and hand pose estimation benefit from revising them as dense 3D
coordinate prediction, and similarly express gaze estimation as regression of
dense 3D eye meshes. To close the gap between image domains, we create a
large-scale dataset of diverse faces with gaze pseudo-annotations, which we
extract based on the 3D geometry of the scene, and design a multi-view
supervision framework to balance their effect during training. We test our
method in the task of gaze generalization, in which we demonstrate improvement
of up to 30% compared to state-of-the-art when no ground truth data are
available, and up to 10% when they are. The project material are available for
research purposes at https://github.com/Vagver/3DGazeNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ververas_E/0/1/0/all/0/1&quot;&gt;Evangelos Ververas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gkagkos_P/0/1/0/all/0/1&quot;&gt;Polydefkis Gkagkos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jiankang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doukas_M/0/1/0/all/0/1&quot;&gt;Michail Christos Doukas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jia Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1&quot;&gt;Stefanos Zafeiriou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.04246">
<title>ViTPose++: Vision Transformer for Generic Body Pose Estimation. (arXiv:2212.04246v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.04246</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we show the surprisingly good properties of plain vision
transformers for body pose estimation from various aspects, namely simplicity
in model structure, scalability in model size, flexibility in training
paradigm, and transferability of knowledge between models, through a simple
baseline model dubbed ViTPose. Specifically, ViTPose employs the plain and
non-hierarchical vision transformer as an encoder to encode features and a
lightweight decoder to decode body keypoints in either a top-down or a
bottom-up manner. It can be scaled up from about 20M to 1B parameters by taking
advantage of the scalable model capacity and high parallelism of the vision
transformer, setting a new Pareto front for throughput and performance.
Besides, ViTPose is very flexible regarding the attention type, input
resolution, and pre-training and fine-tuning strategy. Based on the
flexibility, a novel ViTPose+ model is proposed to deal with heterogeneous body
keypoint categories in different types of body pose estimation tasks via
knowledge factorization, i.e., adopting task-agnostic and task-specific
feed-forward networks in the transformer. We also empirically demonstrate that
the knowledge of large ViTPose models can be easily transferred to small ones
via a simple knowledge token. Experimental results show that our ViTPose model
outperforms representative methods on the challenging MS COCO Human Keypoint
Detection benchmark at both top-down and bottom-up settings. Furthermore, our
ViTPose+ model achieves state-of-the-art performance simultaneously on a series
of body pose estimation tasks, including MS COCO, AI Challenger, OCHuman, MPII
for human keypoint detection, COCO-Wholebody for whole-body keypoint detection,
as well as AP-10K and APT-36K for animal keypoint detection, without
sacrificing inference speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yufei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.06244">
<title>PathFusion: Path-consistent Lidar-Camera Deep Feature Fusion. (arXiv:2212.06244v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.06244</link>
<description rdf:parseType="Literal">&lt;p&gt;Fusing 3D LiDAR features with 2D camera features is a promising technique for
enhancing the accuracy of 3D detection, thanks to their complementary physical
properties. While most of the existing methods focus on directly fusing camera
features with raw LiDAR point clouds or shallow-level 3D features, it is
observed that directly combining 2D and 3D features in deeper layers actually
leads to a decrease in accuracy due to feature misalignment. The misalignment,
which stems from the aggregation of features learned from large receptive
fields, becomes increasingly more severe as we delve into deeper layers. In
this paper, we propose PathFusion as a solution to enable the alignment of
semantically coherent LiDAR-camera deep feature fusion. PathFusion introduces a
path consistency loss at multiple stages within the network, encouraging the 2D
backbone and its fusion path to transform 2D features in a way that aligns
semantically with the transformation of the 3D backbone. This ensures semantic
consistency between 2D and 3D features, even in deeper layers, and amplifies
the usage of the network&apos;s learning capacity. We apply PathFusion to improve a
prior-art fusion baseline, Focals Conv, and observe an improvement of over 1.6%
in mAP on the nuScenes test split consistently with and without testing-time
data augmentations, and moreover, PathFusion also improves KITTI
$\text{AP}_{\text{3D}}$ (R11) by about 0.6% on the moderate level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lemeng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dilin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Meng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yunyang Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamoorthi_R/0/1/0/all/0/1&quot;&gt;Raghuraman Krishnamoorthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1&quot;&gt;Vikas Chandra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13319">
<title>ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from Micro CT Images in Mineral Processing and Recycling. (arXiv:2301.13319v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13319</link>
<description rdf:parseType="Literal">&lt;p&gt;Minerals, metals, and plastics are indispensable for a functioning modern
society. Yet, their supply is limited causing a need for optimizing ore
extraction and recuperation from recyclable materials.Typically, those
processes must be meticulously adapted to the precise properties of the
processed materials. Advancing our understanding of these materials is thus
vital and can be achieved by crushing them into particles of micrometer size
followed by their characterization. Current imaging approaches perform this
analysis based on segmentation and characterization of particles imaged with
computed tomography (CT), and rely on rudimentary postprocessing techniques to
separate touching particles. However, their inability to reliably perform this
separation as well as the need to retrain methods for each new image, these
approaches leave untapped potential to be leveraged. Here, we propose
ParticleSeg3D, an instance segmentation method able to extract individual
particles from large CT images of particle samples containing different
materials. Our approach is based on the powerful nnU-Net framework, introduces
a particle size normalization, uses a border-core representation to enable
instance segmentation, and is trained with a large dataset containing particles
of numerous different sizes, shapes, and compositions of various materials. We
demonstrate that ParticleSeg3D can be applied out-of-the-box to a large variety
of particle types, including materials and appearances that have not been part
of the training set. Thus, no further manual annotations and retraining are
required when applying the method to new particle samples, enabling
substantially higher scalability of experiments than existing methods. Our code
and dataset are made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gotkowski_K/0/1/0/all/0/1&quot;&gt;Karol Gotkowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Shuvam Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Godinho_J/0/1/0/all/0/1&quot;&gt;Jose R. A. Godinho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tochtrop_C/0/1/0/all/0/1&quot;&gt;Camila G. S. Tochtrop&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maier_Hein_K/0/1/0/all/0/1&quot;&gt;Klaus H. Maier-Hein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isensee_F/0/1/0/all/0/1&quot;&gt;Fabian Isensee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01903">
<title>Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering. (arXiv:2303.01903v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01903</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge-based visual question answering (VQA) requires external knowledge
beyond the image to answer the question. Early studies retrieve required
knowledge from explicit knowledge bases (KBs), which often introduces
irrelevant information to the question, hence restricting the performance of
their models. Recent works have resorted to using a powerful large language
model (LLM) as an implicit knowledge engine to acquire the necessary knowledge
for answering. Despite the encouraging results achieved by these methods, we
argue that they have not fully activated the capacity of the blind LLM as the
provided textual input is insufficient to depict the required visual
information to answer the question. In this paper, we present Prophet -- a
conceptually simple, flexible, and general framework designed to prompt LLM
with answer heuristics for knowledge-based VQA. Specifically, we first train a
vanilla VQA model on a specific knowledge-based VQA dataset without external
knowledge. After that, we extract two types of complementary answer heuristics
from the VQA model: answer candidates and answer-aware examples. Finally, the
two types of answer heuristics are jointly encoded into a formatted prompt to
facilitate the LLM&apos;s understanding of both the image and question, thus
generating a more accurate answer. By incorporating the state-of-the-art LLM
GPT-3, Prophet significantly outperforms existing state-of-the-art methods on
four challenging knowledge-based VQA datasets. To demonstrate the generality of
our approach, we instantiate Prophet with the combinations of different VQA
models (i.e., both discriminative and generative ones) and different LLMs
(i.e., both commercial and open-source ones).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhou Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_X/0/1/0/all/0/1&quot;&gt;Xuecheng Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1&quot;&gt;Zhenwei Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jun Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05164">
<title>Reliability-Adaptive Consistency Regularization for Weakly-Supervised Point Cloud Segmentation. (arXiv:2303.05164v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05164</link>
<description rdf:parseType="Literal">&lt;p&gt;Weakly-supervised point cloud segmentation with extremely limited labels is
highly desirable to alleviate the expensive costs of collecting densely
annotated 3D points. This paper explores applying the consistency
regularization that is commonly used in weakly-supervised learning, for its
point cloud counterpart with multiple data-specific augmentations, which has
not been well studied. We observe that the straightforward way of applying
consistency constraints to weakly-supervised point cloud segmentation has two
major limitations: noisy pseudo labels due to the conventional confidence-based
selection and insufficient consistency constraints due to discarding unreliable
pseudo labels. Therefore, we propose a novel Reliability-Adaptive Consistency
Network (RAC-Net) to use both prediction confidence and model uncertainty to
measure the reliability of pseudo labels and apply consistency training on all
unlabeled points while with different consistency constraints for different
points based on the reliability of corresponding pseudo labels. Experimental
results on the S3DIS and ScanNet-v2 benchmark datasets show that our model
achieves superior performance in weakly-supervised point cloud segmentation.
The code will be released publicly at https://github.com/wu-zhonghua/RAC-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhonghua Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yicheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guosheng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jianfei Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05451">
<title>Multiscale Augmented Normalizing Flows for Image Compression. (arXiv:2305.05451v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05451</link>
<description rdf:parseType="Literal">&lt;p&gt;Most learning-based image compression methods lack efficiency for high image
quality due to their non-invertible design. The decoding function of the
frequently applied compressive autoencoder architecture is only an approximated
inverse of the encoding transform. This issue can be resolved by using
invertible latent variable models, which allow a perfect reconstruction if no
quantization is performed. Furthermore, many traditional image and video coders
apply dynamic block partitioning to vary the compression of certain image
regions depending on their content. Inspired by this approach, hierarchical
latent spaces have been applied to learning-based compression networks. In this
paper, we present a novel concept, which adapts the hierarchical latent space
for augmented normalizing flows, an invertible latent variable model. Our best
performing model achieved average rate savings of more than 7% over comparable
single-scale models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Windsheimer_M/0/1/0/all/0/1&quot;&gt;Marc Windsheimer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brand_F/0/1/0/all/0/1&quot;&gt;Fabian Brand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kaup_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Kaup&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10547">
<title>Rethinking Multimodal Content Moderation from an Asymmetric Angle with Mixed-modality. (arXiv:2305.10547v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10547</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a rapidly growing need for multimodal content moderation (CM) as
more and more content on social media is multimodal in nature. Existing
unimodal CM systems may fail to catch harmful content that crosses modalities
(e.g., memes or videos), which may lead to severe consequences. In this paper,
we present a novel CM model, Asymmetric Mixed-Modal Moderation (AM3), to target
multimodal and unimodal CM tasks. Specifically, to address the asymmetry in
semantics between vision and language, AM3 has a novel asymmetric fusion
architecture that is designed to not only fuse the common knowledge in both
modalities but also to exploit the unique information in each modality. Unlike
previous works that focus on representing the two modalities into a similar
feature space while overlooking the intrinsic difference between the
information conveyed in multimodality and in unimodality (asymmetry in
modalities), we propose a novel cross-modality contrastive loss to learn the
unique knowledge that only appears in multimodality. This is critical as some
harmful intent may only be conveyed through the intersection of both
modalities. With extensive experiments, we show that AM3 outperforms all
existing state-of-the-art methods on both multimodal and unimodal CM
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jialin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Ye Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_G/0/1/0/all/0/1&quot;&gt;Gaurav Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hall_M/0/1/0/all/0/1&quot;&gt;Matthew Hall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajeev_S/0/1/0/all/0/1&quot;&gt;Sandra Sajeev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mei Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13921">
<title>Compositional Text-to-Image Synthesis with Attention Map Control of Diffusion Models. (arXiv:2305.13921v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13921</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent text-to-image (T2I) diffusion models show outstanding performance in
generating high-quality images conditioned on textual prompts. However, they
fail to semantically align the generated images with the prompts due to their
limited compositional capabilities, leading to attribute leakage, entity
leakage, and missing entities. In this paper, we propose a novel attention mask
control strategy based on predicted object boxes to address these issues. In
particular, we first train a BoxNet to predict a box for each entity that
possesses the attribute specified in the prompt. Then, depending on the
predicted boxes, a unique mask control is applied to the cross- and
self-attention maps. Our approach produces a more semantically accurate
synthesis by constraining the attention regions of each token in the prompt to
the image. In addition, the proposed method is straightforward and effective
and can be readily integrated into existing cross-attention-based T2I
generators. We compare our approach to competing methods and demonstrate that
it can faithfully convey the semantics of the original text to the generated
content and achieve high availability as a ready-to-use plugin. Please refer to
https://github.com/OPPOMente-Lab/attention-mask-control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruichen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zekang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Haonan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xiaodong Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14141">
<title>Learning Remote Sensing Object Detection with Single Point Supervision. (arXiv:2305.14141v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14141</link>
<description rdf:parseType="Literal">&lt;p&gt;Pointly Supervised Object Detection (PSOD) has attracted considerable
interests due to its lower labeling cost as compared to box-level supervised
object detection. However, the complex scenes, densely packed and dynamic-scale
objects in Remote Sensing (RS) images hinder the development of PSOD methods in
RS field. In this paper, we make the first attempt to achieve RS object
detection with single point supervision, and propose a PSOD method tailored for
RS images. Specifically, we design a point label upgrader (PLUG) to generate
pseudo box labels from single point labels, and then use the pseudo boxes to
supervise the optimization of existing detectors. Moreover, to handle the
challenge of the densely packed objects in RS images, we propose a sparse
feature guided semantic prediction module which can generate high-quality
semantic maps by fully exploiting informative cues from sparse objects.
Extensive ablation studies on the DOTA dataset have validated the effectiveness
of our method. Our method can achieve significantly better performance as
compared to state-of-the-art image-level and point-level supervised detection
methods, and reduce the performance gap between PSOD and box-level supervised
object detection. Code is available at https://github.com/heshitian/PLUG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shitian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1&quot;&gt;Huanxin Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yingqian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Boyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_N/0/1/0/all/0/1&quot;&gt;Ning Jing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16321">
<title>Eclipse: Disambiguating Illumination and Materials using Unintended Shadows. (arXiv:2305.16321v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16321</link>
<description rdf:parseType="Literal">&lt;p&gt;Decomposing an object&apos;s appearance into representations of its materials and
the surrounding illumination is difficult, even when the object&apos;s 3D shape is
known beforehand. This problem is especially challenging for diffuse objects:
it is ill-conditioned because diffuse materials severely blur incoming light,
and it is ill-posed because diffuse materials under high-frequency lighting can
be indistinguishable from shiny materials under low-frequency lighting. We show
that it is possible to recover precise materials and illumination -- even from
diffuse objects -- by exploiting unintended shadows, like the ones cast onto an
object by the photographer who moves around it. These shadows are a nuisance in
most previous inverse rendering pipelines, but here we exploit them as signals
that improve conditioning and help resolve material-lighting ambiguities. We
present a method based on differentiable Monte Carlo ray tracing that uses
images of an object to jointly recover its spatially-varying materials, the
surrounding illumination environment, and the shapes of the unseen light
occluders who inadvertently cast shadows upon it.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbin_D/0/1/0/all/0/1&quot;&gt;Dor Verbin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mildenhall_B/0/1/0/all/0/1&quot;&gt;Ben Mildenhall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hedman_P/0/1/0/all/0/1&quot;&gt;Peter Hedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1&quot;&gt;Jonathan T. Barron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zickler_T/0/1/0/all/0/1&quot;&gt;Todd Zickler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1&quot;&gt;Pratul P. Srinivasan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06388">
<title>From NeRFLiX to NeRFLiX++: A General NeRF-Agnostic Restorer Paradigm. (arXiv:2306.06388v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06388</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural radiance fields (NeRF) have shown great success in novel view
synthesis. However, recovering high-quality details from real-world scenes is
still challenging for the existing NeRF-based approaches, due to the potential
imperfect calibration information and scene representation inaccuracy. Even
with high-quality training frames, the synthetic novel views produced by NeRF
models still suffer from notable rendering artifacts, such as noise and blur.
To address this, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm
that learns a degradation-driven inter-viewpoint mixer. Specially, we design a
NeRF-style degradation modeling approach and construct large-scale training
data, enabling the possibility of effectively removing NeRF-native rendering
artifacts for deep neural networks. Moreover, beyond the degradation removal,
we propose an inter-viewpoint aggregation framework that fuses highly related
high-quality training images, pushing the performance of cutting-edge NeRF
models to entirely new levels and producing highly photo-realistic synthetic
views. Based on this paradigm, we further present NeRFLiX++ with a stronger
two-stage NeRF degradation simulator and a faster inter-viewpoint mixer,
achieving superior performance with significantly improved computational
efficiency. Notably, NeRFLiX++ is capable of restoring photo-realistic
ultra-high-resolution outputs from noisy low-resolution NeRF-rendered views.
Extensive experiments demonstrate the excellent restoration ability of
NeRFLiX++ on various novel view synthesis benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenbo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1&quot;&gt;Nianjuan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiaoguang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiangbo Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06963">
<title>Feature Fusion from Head to Tail for Long-Tailed Visual Recognition. (arXiv:2306.06963v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06963</link>
<description rdf:parseType="Literal">&lt;p&gt;The imbalanced distribution of long-tailed data presents a considerable
challenge for deep learning models, as it causes them to prioritize the
accurate classification of head classes but largely disregard tail classes. The
biased decision boundary caused by inadequate semantic information in tail
classes is one of the key factors contributing to their low recognition
accuracy. To rectify this issue, we propose to augment tail classes by grafting
the diverse semantic information from head classes, referred to as head-to-tail
fusion (H2T). We replace a portion of feature maps from tail classes with those
belonging to head classes. These fused features substantially enhance the
diversity of tail classes. Both theoretical analysis and practical
experimentation demonstrate that H2T can contribute to a more optimized
solution for the decision boundary. We seamlessly integrate H2T in the
classifier adjustment stage, making it a plug-and-play module. Its simplicity
and ease of implementation allow for smooth integration with existing
long-tailed recognition methods, facilitating a further performance boost.
Extensive experiments on various long-tailed benchmarks demonstrate the
effectiveness of the proposed H2T. The source code is available at
https://github.com/Keke921/H2T.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mengke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhikai Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_W/0/1/0/all/0/1&quot;&gt;Weichao Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_Y/0/1/0/all/0/1&quot;&gt;Yiu-ming Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hui Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10007">
<title>Robot Learning with Sensorimotor Pre-training. (arXiv:2306.10007v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10007</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a self-supervised sensorimotor pre-training approach for robotics.
Our model, called RPT, is a Transformer that operates on sequences of
sensorimotor tokens. Given a sequence of camera images, proprioceptive robot
states, and actions, we encode the sequence into tokens, mask out a subset, and
train a model to predict the missing content from the rest. We hypothesize that
if a robot can predict the masked-out content it will have acquired a good
model of the physical world that can enable it to act. RPT is designed to
operate on latent visual representations which makes prediction tractable,
enables scaling to larger models, and allows fast inference on a real robot. To
evaluate our approach, we collected a dataset of 20,000 real-world trajectories
over 9 months using a combination of motion planning and grasping algorithms.
We find that sensorimotor pre-training consistently outperforms training from
scratch, has favorable scaling properties, and enables transfer across
different tasks, environments, and robots.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radosavovic_I/0/1/0/all/0/1&quot;&gt;Ilija Radosavovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Baifeng Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1&quot;&gt;Letian Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1&quot;&gt;Ken Goldberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1&quot;&gt;Jitendra Malik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10311">
<title>Joint Denoising and Fusion with Short- and Long-exposure Raw Pairs. (arXiv:2306.10311v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10311</link>
<description rdf:parseType="Literal">&lt;p&gt;Denoising and high dynamic range (HDR) imaging are significant yet
challenging problems due to the small aperture and sensor size of generic image
sensors. Current methods predominantly generate HDR images from a set of
bracketed exposure sRGB images. However, they overlook the computational and
memory inefficiencies of the Image Signal Processor (ISP) when processing a set
of sRGB images with different exposures. Furthermore, the absence of
large-scale raw-based HDR datasets limits the research on HDR imaging. Unlike
existing methods, the core idea of this work is to utilize the difference
between short- and long-exposure images of signal-to-noise ratios to generate
HDR images and denoising. To this end, we propose a model tailor-made for
double-exposure HDR sensors, leveraging the unique features of the raw data to
facilitate raw-to-HDR mapping and raw denoising. Our key insights are
threefold: (1) a new computational raw LDR-HDR pair formation pipeline is
designed to construct a real-world raw HDR dataset called RealRaw-HDR; (2) a
lightweight-efficient HDR model, RepUNet, is developed using the structural
reparameterization technique; (3) a plug-and-play alignment-free and
motion-aware short-exposure-first selection loss and a colorfulness loss are
proposed to mitigate ghost artifacts and color cast. Our empirical evaluation
validates the effectiveness of the proposed LDR-HDR formation pipeline, as well
as experiments show that our method achieves comparable performance to the
state-of-the-art methods with less computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qirui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yihao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qihua Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jingyu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14408">
<title>Decompose and Realign: Tackling Condition Misalignment in Text-to-Image Diffusion Models. (arXiv:2306.14408v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14408</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image diffusion models have advanced towards more controllable
generation via supporting various additional conditions (e.g., depth map,
bounding box) beyond text. However, these models are learned based on the
premise of perfect alignment between the text and extra conditions. If this
alignment is not satisfied, the final output could be either dominated by one
condition, or ambiguity may arise, failing to meet user expectations.To address
this issue, we present a training-free approach called ``Decompose and
Realign&apos;&apos; to further improve the controllability of existing models when
provided with partially aligned conditions. The ``Decompose&apos;&apos; phase separates
conditions based on pair relationships, computing the result individually for
each pair. This ensures that each pair no longer has conflicting conditions.
The ``Realign&apos;&apos; phase aligns these independently calculated results via a
cross-attention mechanism to avoid new conflicts when combining them back. Both
qualitative and quantitative results demonstrate the effectiveness of our
approach in handling unaligned conditions, which performs favorably against
recent methods and more importantly adds flexibility to the controllable image
generation process. Our code will be available at:
https://github.com/EnVision-Research/Decompose-and-Realign.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Luozhou Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_G/0/1/0/all/0/1&quot;&gt;Guibao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1&quot;&gt;Wenhang Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guangyong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yijun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Ying-cong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00293">
<title>AutoST: Training-free Neural Architecture Search for Spiking Transformers. (arXiv:2307.00293v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00293</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking Transformers have gained considerable attention because they achieve
both the energy efficiency of Spiking Neural Networks (SNNs) and the high
capacity of Transformers. However, the existing Spiking Transformer
architectures, derived from Artificial Neural Networks (ANNs), exhibit a
notable architectural gap, resulting in suboptimal performance compared to
their ANN counterparts. Manually discovering optimal architectures is
time-consuming. To address these limitations, we introduce AutoST, a
training-free NAS method for Spiking Transformers, to rapidly identify
high-performance Spiking Transformer architectures. Unlike existing
training-free NAS methods, which struggle with the non-differentiability and
high sparsity inherent in SNNs, we propose to utilize Floating-Point Operations
(FLOPs) as a performance metric, which is independent of model computations and
training dynamics, leading to a stronger correlation with performance. Our
extensive experiments show that AutoST models outperform state-of-the-art
manually or automatically designed SNN architectures on static and neuromorphic
datasets. Full code, model, and data are released for reproduction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qidong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jinku Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dongkuan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01146">
<title>AVSegFormer: Audio-Visual Segmentation with Transformer. (arXiv:2307.01146v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01146</link>
<description rdf:parseType="Literal">&lt;p&gt;The combination of audio and vision has long been a topic of interest in the
multi-modal community. Recently, a new audio-visual segmentation (AVS) task has
been introduced, aiming to locate and segment the sounding objects in a given
video. This task demands audio-driven pixel-level scene understanding for the
first time, posing significant challenges. In this paper, we propose
AVSegFormer, a novel framework for AVS tasks that leverages the transformer
architecture. Specifically, we introduce audio queries and learnable queries
into the transformer decoder, enabling the network to selectively attend to
interested visual features. Besides, we present an audio-visual mixer, which
can dynamically adjust visual features by amplifying relevant and suppressing
irrelevant spatial channels. Additionally, we devise an intermediate mask loss
to enhance the supervision of the decoder, encouraging the network to produce
more accurate intermediate predictions. Extensive experiments demonstrate that
AVSegFormer achieves state-of-the-art results on the AVS benchmark. The code is
available at https://github.com/vvvb-github/AVSegFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shengyi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Tong Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01200">
<title>ProxyCap: Real-time Monocular Full-body Capture in World Space via Human-Centric Proxy-to-Motion Learning. (arXiv:2307.01200v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01200</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning-based approaches to monocular motion capture have recently shown
promising results by learning to regress in a data-driven manner. However, due
to the challenges in data collection and network designs, it remains
challenging for existing solutions to achieve real-time full-body capture while
being accurate in world space. In this work, we introduce ProxyCap, a
human-centric proxy-to-motion learning scheme to learn world-space motions from
a proxy dataset of 2D skeleton sequences and 3D rotational motions. Such proxy
data enables us to build a learning-based network with accurate world-space
supervision while also mitigating the generalization issues. For more accurate
and physically plausible predictions in world space, our network is designed to
learn human motions from a human-centric perspective, which enables the
understanding of the same motion captured with different camera trajectories.
Moreover, a contact-aware neural motion descent module is proposed in our
network so that it can be aware of foot-ground contact and motion misalignment
with the proxy observations. With the proposed learning-based solution, we
demonstrate the first real-time monocular full-body capture system with
plausible foot-ground contact in world space even using hand-held moving
cameras.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongwen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1&quot;&gt;Liangxiao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiajun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_H/0/1/0/all/0/1&quot;&gt;Hongwei Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shengping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yebin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12618">
<title>Attribute Regularized Soft Introspective VAE: Towards Cardiac Attribute Regularization Through MRI Domains. (arXiv:2307.12618v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12618</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models have emerged as influential instruments for data
generation and manipulation. Enhancing the controllability of these models by
selectively modifying data attributes has been a recent focus. Variational
Autoencoders (VAEs) have shown promise in capturing hidden attributes but often
produce blurry reconstructions. Controlling these attributes through different
imaging domains is difficult in medical imaging. Recently, Soft Introspective
VAE leverage the benefits of both VAEs and Generative Adversarial Networks
(GANs), which have demonstrated impressive image synthesis capabilities, by
incorporating an adversarial loss into VAE training. In this work, we propose
the Attributed Soft Introspective VAE (Attri-SIVAE) by incorporating an
attribute regularized loss, into the Soft-Intro VAE framework. We evaluate
experimentally the proposed method on cardiac MRI data from different domains,
such as various scanner vendors and acquisition centers. The proposed method
achieves similar performance in terms of reconstruction and regularization
compared to the state-of-the-art Attributed regularized VAE but additionally
also succeeds in keeping the same regularization level when tested on a
different dataset, unlike the compared method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Folco_M/0/1/0/all/0/1&quot;&gt;Maxime Di Folco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bercea_C/0/1/0/all/0/1&quot;&gt;Cosmin Bercea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schnabel_J/0/1/0/all/0/1&quot;&gt;Julia A. Schnabel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01890">
<title>DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition with Limited Annotations. (arXiv:2308.01890v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01890</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-label image recognition in the low-label regime is a task of great
challenge and practical significance. Previous works have focused on learning
the alignment between textual and visual spaces to compensate for limited image
labels, yet may suffer from reduced accuracy due to the scarcity of
high-quality multi-label annotations. In this research, we leverage the
powerful alignment between textual and visual features pretrained with millions
of auxiliary image-text pairs. We introduce an efficient and effective
framework called Evidence-guided Dual Context Optimization (DualCoOp++), which
serves as a unified approach for addressing partial-label and zero-shot
multi-label recognition. In DualCoOp++ we separately encode evidential,
positive, and negative contexts for target classes as parametric components of
the linguistic input (i.e., prompts). The evidential context aims to discover
all the related visual content for the target class, and serves as guidance to
aggregate positive and negative contexts from the spatial domain of the image,
enabling better distinguishment between similar categories. Additionally, we
introduce a Winner-Take-All module that promotes inter-class interaction during
training, while avoiding the need for extra parameters and costs. As DualCoOp++
imposes minimal additional learnable overhead on the pretrained vision-language
framework, it enables rapid adaptation to multi-label recognition tasks with
limited annotations and even unseen classes. Experiments on standard
multi-label recognition benchmarks across two challenging low-label settings
demonstrate the superior performance of our approach compared to
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1&quot;&gt;Ping Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Ximeng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1&quot;&gt;Stan Sclaroff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1&quot;&gt;Kate Saenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02668">
<title>Guided Distillation for Semi-Supervised Instance Segmentation. (arXiv:2308.02668v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02668</link>
<description rdf:parseType="Literal">&lt;p&gt;Although instance segmentation methods have improved considerably, the
dominant paradigm is to rely on fully-annotated training images, which are
tedious to obtain. To alleviate this reliance, and boost results,
semi-supervised approaches leverage unlabeled data as an additional training
signal that limits overfitting to the labeled samples. In this context, we
present novel design choices to significantly improve teacher-student
distillation models. In particular, we (i) improve the distillation approach by
introducing a novel &quot;guided burn-in&quot; stage, and (ii) evaluate different
instance segmentation architectures, as well as backbone networks and
pre-training strategies. Contrary to previous work which uses only supervised
data for the burn-in period of the student model, we also use guidance of the
teacher model to exploit unlabeled data in the burn-in period. Our improved
distillation approach leads to substantial improvements over previous
state-of-the-art results. For example, on the Cityscapes dataset we improve
mask-AP from 23.7 to 33.9 when using labels for 10\% of images, and on the COCO
dataset we improve mask-AP from 18.3 to 34.1 when using labels for only 1\% of
the training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berrada_T/0/1/0/all/0/1&quot;&gt;Tariq Berrada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couprie_C/0/1/0/all/0/1&quot;&gt;Camille Couprie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1&quot;&gt;Karteek Alahari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1&quot;&gt;Jakob Verbeek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03793">
<title>ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation. (arXiv:2308.03793v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03793</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale Pre-Training Vision-Language Model such as CLIP has demonstrated
outstanding performance in zero-shot classification, e.g. achieving 76.3% top-1
accuracy on ImageNet without seeing any example, which leads to potential
benefits to many tasks that have no labeled data. However, while applying CLIP
to a downstream target domain, the presence of visual and text domain gaps and
cross-modality misalignment can greatly impact the model performance. To
address such challenges, we propose ReCLIP, the first source-free domain
adaptation method for vision-language models, which does not require any source
data or target labeled data. ReCLIP first learns a projection space to mitigate
the misaligned visual-text embeddings and learns pseudo labels, and then
deploys cross-modality self-training with the pseudo labels, to update visual
and text encoders, refine labels and reduce domain gaps and misalignments
iteratively. With extensive experiments, we demonstrate ReCLIP reduces the
average error rate of CLIP from 30.17% to 25.06% on 22 image classification
benchmarks. Code available at https://github.com/michiganleon/ReCLIP_WACV.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xuefeng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Ke Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1&quot;&gt;Lu Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Albert Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jiajia Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuyin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Ken Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_N/0/1/0/all/0/1&quot;&gt;Nan Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xiao Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Min Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1&quot;&gt;Cheng-Hao Kuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nevatia_R/0/1/0/all/0/1&quot;&gt;Ram Nevatia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09300">
<title>V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models. (arXiv:2308.09300v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09300</link>
<description rdf:parseType="Literal">&lt;p&gt;Building artificial intelligence (AI) systems on top of a set of foundation
models (FMs) is becoming a new paradigm in AI research. Their representative
and generative abilities learnt from vast amounts of data can be easily adapted
and transferred to a wide range of downstream tasks without extra training from
scratch. However, leveraging FMs in cross-modal generation remains
under-researched when audio modality is involved. On the other hand,
automatically generating semantically-relevant sound from visual input is an
important problem in cross-modal generation studies. To solve this
vision-to-audio (V2A) generation problem, existing methods tend to design and
build complex systems from scratch using modestly sized datasets. In this
paper, we propose a lightweight solution to this problem by leveraging
foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate
the domain gap between the latent space of the visual CLIP and the auditory
CLAP models. Then we propose a simple yet effective mapper mechanism
(V2A-Mapper) to bridge the domain gap by translating the visual input between
CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained
audio generative FM AudioLDM is adopted to produce high-fidelity and
visually-aligned sound. Compared to previous approaches, our method only
requires a quick training of the V2A-Mapper. We further analyze and conduct
extensive experiments on the choice of the V2A-Mapper and show that a
generative mapper is better at fidelity and variability (FD) while a regression
mapper is slightly better at relevance (CS). Both objective and subjective
evaluation on two V2A datasets demonstrate the superiority of our proposed
method compared to current state-of-the-art approaches - trained with 86% fewer
parameters but achieving 53% and 19% improvement in FD and CS, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Heng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jianbo Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascual_S/0/1/0/all/0/1&quot;&gt;Santiago Pascual&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cartwright_R/0/1/0/all/0/1&quot;&gt;Richard Cartwright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Weidong Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09591">
<title>O^2-Recon: Completing 3D Reconstruction of Occluded Objects in the Scene with a Pre-trained 2D Diffusion Model. (arXiv:2308.09591v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09591</link>
<description rdf:parseType="Literal">&lt;p&gt;Occlusion is a common issue in 3D reconstruction from RGB-D videos, often
blocking the complete reconstruction of objects and presenting an ongoing
problem. In this paper, we propose a novel framework, empowered by a 2D
diffusion-based in-painting model, to reconstruct complete surfaces for the
hidden parts of objects. Specifically, we utilize a pre-trained diffusion model
to fill in the hidden areas of 2D images. Then we use these in-painted images
to optimize a neural implicit surface representation for each instance for 3D
reconstruction. Since creating the in-painting masks needed for this process is
tricky, we adopt a human-in-the-loop strategy that involves very little human
engagement to generate high-quality masks. Moreover, some parts of objects can
be totally hidden because the videos are usually shot from limited
perspectives. To ensure recovering these invisible areas, we develop a cascaded
network architecture for predicting signed distance field, making use of
different frequency bands of positional encoding and maintaining overall
smoothness. Besides the commonly used rendering loss, Eikonal loss, and
silhouette loss, we adopt a CLIP-based semantic consistency loss to guide the
surface from unseen camera angles. Experiments on ScanNet scenes show that our
proposed framework achieves state-of-the-art accuracy and completeness in
object-level reconstruction from scene-level RGB-D videos. Code:
https://github.com/THU-LYJ-Lab/O2-Recon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yubin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1&quot;&gt;Sheng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Matthieu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuze He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yu-Hui Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Ying He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong-Jin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10273">
<title>Turning Waste into Wealth: Leveraging Low-Quality Samples for Enhancing Continuous Conditional Generative Adversarial Networks. (arXiv:2308.10273v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10273</link>
<description rdf:parseType="Literal">&lt;p&gt;Continuous Conditional Generative Adversarial Networks (CcGANs) enable
generative modeling conditional on continuous scalar variables (termed
regression labels). However, they can produce subpar fake images due to limited
training data. Although Negative Data Augmentation (NDA) effectively enhances
unconditional and class-conditional GANs by introducing anomalies into real
training images, guiding the GANs away from low-quality outputs, its impact on
CcGANs is limited, as it fails to replicate negative samples that may occur
during the CcGAN sampling. We present a novel NDA approach called Dual-NDA
specifically tailored for CcGANs to address this problem. Dual-NDA employs two
types of negative samples: visually unrealistic images generated from a
pre-trained CcGAN and label-inconsistent images created by manipulating real
images&apos; labels. Leveraging these negative samples, we introduce a novel
discriminator objective alongside a modified CcGAN training algorithm.
Empirical analysis on UTKFace and Steering Angle reveals that Dual-NDA
consistently enhances the visual fidelity and label consistency of fake images
generated by CcGANs, exhibiting a substantial performance gain over the vanilla
NDA. Moreover, by applying Dual-NDA, CcGANs demonstrate a remarkable
advancement beyond the capabilities of state-of-the-art conditional GANs and
diffusion models, establishing a new pinnacle of performance. Our codes can be
found at https://github.com/UBCDingXin/Dual-NDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xin Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yongwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zuheng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11561">
<title>Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog Navigation. (arXiv:2308.11561v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11561</link>
<description rdf:parseType="Literal">&lt;p&gt;This report details the methods of the winning entry of the AVDN Challenge in
ICCV CLVL 2023. The competition addresses the Aerial Navigation from Dialog
History (ANDH) task, which requires a drone agent to associate dialog history
with aerial observations to reach the destination. For better cross-modal
grounding abilities of the drone agent, we propose a Target-Grounded
Graph-Aware Transformer (TG-GAT) framework. Concretely, TG-GAT first leverages
a graph-aware transformer to capture spatiotemporal dependency, which benefits
navigation state tracking and robust action planning. In addition,an auxiliary
visual grounding task is devised to boost the agent&apos;s awareness of referred
landmarks. Moreover, a hybrid augmentation strategy based on large language
models is utilized to mitigate data scarcity limitations. Our TG-GAT framework
won the AVDN Challenge, with 2.2% and 3.0% absolute improvements over the
baseline on SPL and SR metrics, respectively. The code is available at
https://github.com/yifeisu/TG-GAT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yifei Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1&quot;&gt;Dong An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kehan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yan Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11918">
<title>AMSP-UOD: When Vortex Convolution and Stochastic Perturbation Meet Underwater Object Detection. (arXiv:2308.11918v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11918</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a novel Amplitude-Modulated Stochastic Perturbation
and Vortex Convolutional Network, AMSP-UOD, designed for underwater object
detection. AMSP-UOD specifically addresses the impact of non-ideal imaging
factors on detection accuracy in complex underwater environments. To mitigate
the influence of noise on object detection performance, we propose AMSP Vortex
Convolution (AMSP-VConv) to disrupt the noise distribution, enhance feature
extraction capabilities, effectively reduce parameters, and improve network
robustness. We design the Feature Association Decoupling Cross Stage Partial
(FAD-CSP) module, which strengthens the association of long and short range
features, improving the network performance in complex underwater environments.
Additionally, our sophisticated post-processing method, based on Non-Maximum
Suppression (NMS) with aspect-ratio similarity thresholds, optimizes detection
in dense scenes, such as waterweed and schools of fish, improving object
detection accuracy. Extensive experiments on the URPC and RUOD datasets
demonstrate that our method outperforms existing state-of-the-art methods in
terms of accuracy and noise immunity. AMSP-UOD proposes an innovative solution
with the potential for real-world applications. Our code is available at
https://github.com/zhoujingchun03/AMSP-UOD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingchun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zongxin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1&quot;&gt;Kin-Man Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yudong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weishi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;ChunLe Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongyi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11932">
<title>Synergistic Multiscale Detail Refinement via Intrinsic Supervision for Underwater Image Enhancement. (arXiv:2308.11932v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11932</link>
<description rdf:parseType="Literal">&lt;p&gt;Visually restoring underwater scenes primarily involves mitigating
interference from underwater media. Existing methods ignore the inherent
scale-related characteristics in underwater scenes. Therefore, we present the
synergistic multi-scale detail refinement via intrinsic supervision (SMDR-IS)
for enhancing underwater scene details, which contain multi-stages. The
low-degradation stage from the original images furnishes the original stage
with multi-scale details, achieved through feature propagation using the
Adaptive Selective Intrinsic Supervised Feature (ASISF) module. By using
intrinsic supervision, the ASISF module can precisely control and guide feature
transmission across multi-degradation stages, enhancing multi-scale detail
refinement and minimizing the interference from irrelevant information in the
low-degradation stage. In multi-degradation encoder-decoder framework of
SMDR-IS, we introduce the Bifocal Intrinsic-Context Attention Module (BICA).
Based on the intrinsic supervision principles, BICA efficiently exploits
multi-scale scene information in images. BICA directs higher-resolution spaces
by tapping into the insights of lower-resolution ones, underscoring the pivotal
role of spatial contextual relationships in underwater image restoration.
Throughout training, the inclusion of a multi-degradation loss function can
enhance the network, allowing it to adeptly extract information across diverse
scales. When benchmarked against state-of-the-art methods, SMDR-IS consistently
showcases superior performance. The code is publicly available at:
https://github.com/zhoujingchun03/SMDR-IS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dehuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingchun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;ChunLe Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weishi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongyi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12608">
<title>HR-Pro: Point-supervised Temporal Action Localization via Hierarchical Reliability Propagation. (arXiv:2308.12608v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12608</link>
<description rdf:parseType="Literal">&lt;p&gt;Point-supervised Temporal Action Localization (PSTAL) is an emerging research
direction for label-efficient learning. However, current methods mainly focus
on optimizing the network either at the snippet-level or the instance-level,
neglecting the inherent reliability of point annotations at both levels. In
this paper, we propose a Hierarchical Reliability Propagation (HR-Pro)
framework, which consists of two reliability-aware stages: Snippet-level
Discrimination Learning and Instance-level Completeness Learning, both stages
explore the efficient propagation of high-confidence cues in point annotations.
For snippet-level learning, we introduce an online-updated memory to store
reliable snippet prototypes for each class. We then employ a Reliability-aware
Attention Block to capture both intra-video and inter-video dependencies of
snippets, resulting in more discriminative and robust snippet representation.
For instance-level learning, we propose a point-based proposal generation
approach as a means of connecting snippets and instances, which produces
high-confidence proposals for further optimization at the instance level.
Through multi-level reliability-aware learning, we obtain more reliable
confidence scores and more accurate temporal boundaries of predicted proposals.
Our HR-Pro achieves state-of-the-art performance on multiple challenging
benchmarks, including an impressive average mAP of 60.3% on THUMOS14. Notably,
our HR-Pro largely surpasses all previous point-supervised methods, and even
outperforms several competitive fully supervised methods. Code will be
available at https://github.com/pipixin321/HR-Pro.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huaxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaohao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1&quot;&gt;Zhiwu Qing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Changxin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1&quot;&gt;Nong Sang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13388">
<title>Direction-aware Video Demoireing with Temporal-guided Bilateral Learning. (arXiv:2308.13388v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13388</link>
<description rdf:parseType="Literal">&lt;p&gt;Moire patterns occur when capturing images or videos on screens, severely
degrading the quality of the captured images or videos. Despite the recent
progresses, existing video demoireing methods neglect the physical
characteristics and formation process of moire patterns, significantly limiting
the effectiveness of video recovery. This paper presents a unified framework,
DTNet, a direction-aware and temporal-guided bilateral learning network for
video demoireing. DTNet effectively incorporates the process of moire pattern
removal, alignment, color correction, and detail refinement. Our proposed DTNet
comprises two primary stages: Frame-level Direction-aware Demoireing and
Alignment (FDDA) and Tone and Detail Refinement (TDR). In FDDA, we employ
multiple directional DCT modes to perform the moire pattern removal process in
the frequency domain, effectively detecting the prominent moire edges. Then,
the coarse and fine-grained alignment is applied on the demoired features for
facilitating the utilization of neighboring information. In TDR, we propose a
temporal-guided bilateral learning pipeline to mitigate the degradation of
color and details caused by the moire patterns while preserving the restored
frequency information in FDDA. Guided by the aligned temporal features from
FDDA, the affine transformations for the recovery of the ultimate clean frames
are learned in TDR. Extensive experiments demonstrate that our video demoireing
method outperforms state-of-the-art approaches by 2.3 dB in PSNR, and also
delivers a superior visual experience. Our code is available at
https://github.com/rebeccaeexu/DTNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuning Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1&quot;&gt;Binbin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiangyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiantao Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13772">
<title>Boosting Residual Networks with Group Knowledge. (arXiv:2308.13772v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13772</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research understands the residual networks from a new perspective of
the implicit ensemble model. From this view, previous methods such as
stochastic depth and stimulative training have further improved the performance
of the residual network by sampling and training of its subnets. However, they
both use the same supervision for all subnets of different capacities and
neglect the valuable knowledge generated by subnets during training. In this
manuscript, we mitigate the significant knowledge distillation gap caused by
using the same kind of supervision and advocate leveraging the subnets to
provide diverse knowledge. Based on this motivation, we propose a group
knowledge based training framework for boosting the performance of residual
networks. Specifically, we implicitly divide all subnets into hierarchical
groups by subnet-in-subnet sampling, aggregate the knowledge of different
subnets in each group during training, and exploit upper-level group knowledge
to supervise lower-level subnet groups. Meanwhile, We also develop a subnet
sampling strategy that naturally samples larger subnets, which are found to be
more helpful than smaller subnets in boosting performance for hierarchical
groups. Compared with typical subnet training and other methods, our method
achieves the best efficiency and performance trade-offs on multiple datasets
and network structures. The code is at https://github.com/tsj-001/AAAI24-GKT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shengji Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1&quot;&gt;Peng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Baopu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weihao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14256">
<title>FaceChain: A Playground for Human-centric Artificial Intelligence Generated Content. (arXiv:2308.14256v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14256</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancement in personalized image generation have unveiled the
intriguing capability of pre-trained text-to-image models on learning identity
information from a collection of portrait images. However, existing solutions
are vulnerable in producing truthful details, and usually suffer from several
defects such as (i) The generated face exhibit its own unique characteristics,
\ie facial shape and facial feature positioning may not resemble key
characteristics of the input, and (ii) The synthesized face may contain warped,
blurred or corrupted regions. In this paper, we present FaceChain, a
personalized portrait generation framework that combines a series of customized
image-generation model and a rich set of face-related perceptual understanding
models (\eg, face detection, deep face embedding extraction, and facial
attribute recognition), to tackle aforementioned challenges and to generate
truthful personalized portraits, with only a handful of portrait images as
input. Concretely, we inject several SOTA face models into the generation
procedure, achieving a more efficient label-tagging, data-processing, and model
post-processing compared to previous solutions, such as DreamBooth
~\cite{ruiz2023dreambooth} , InstantBooth ~\cite{shi2023instantbooth} , or
other LoRA-only approaches ~\cite{hu2021lora} . Besides, based on FaceChain, we
further develop several applications to build a broader playground for better
showing its value, including virtual try on and 2D talking head. We hope it can
grow to serve the burgeoning needs from the communities. Note that this is an
ongoing work that will be consistently refined and improved upon. FaceChain is
open-sourced under Apache-2.0 license at
\url{https://github.com/modelscope/facechain}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Cheng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1&quot;&gt;Lei Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yongyi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Ziheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xingjun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1&quot;&gt;Haoyu Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weida Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yuze Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Chen Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weitao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuan Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wenmeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yingda Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xuansong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Baigui Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01141">
<title>VGDiffZero: Text-to-image Diffusion Models Can Be Zero-shot Visual Grounders. (arXiv:2309.01141v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01141</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale text-to-image diffusion models have shown impressive capabilities
for generative tasks by leveraging strong vision-language alignment from
pre-training. However, most vision-language discriminative tasks require
extensive fine-tuning on carefully-labeled datasets to acquire such alignment,
with great cost in time and computing resources. In this work, we explore
directly applying a pre-trained generative diffusion model to the challenging
discriminative task of visual grounding without any fine-tuning and additional
training dataset. Specifically, we propose VGDiffZero, a simple yet effective
zero-shot visual grounding framework based on text-to-image diffusion models.
We also design a comprehensive region-scoring method considering both global
and local contexts of each isolated proposal. Extensive experiments on RefCOCO,
RefCOCO+, and RefCOCOg show that VGDiffZero achieves strong performance on
zero-shot visual grounding. Our code is available at
https://github.com/xuyang-liu16/VGDiffZero.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Siteng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yachen Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Honggang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Donglin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03661">
<title>Prompt-based Context- and Domain-aware Pretraining for Vision and Language Navigation. (arXiv:2309.03661v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03661</link>
<description rdf:parseType="Literal">&lt;p&gt;Pretrained visual-language models have extensive world knowledge and are
widely used in visual and language navigation (VLN). However, they are not
sensitive to indoor scenarios for VLN tasks. Another challenge for VLN is how
the agent understands the contextual relations between actions on a path and
performs cross-modal alignment sequentially. In this paper, we propose a novel
Prompt-bAsed coNtext- and inDoor-Aware (PANDA) pretraining framework to address
these problems. It performs prompting in two stages. In the indoor-aware stage,
we apply an efficient tuning paradigm to learn deep visual prompts from an
indoor dataset, in order to augment pretrained models with inductive biases
towards indoor environments. This can enable more sample-efficient adaptation
for VLN agents. Furthermore, in the context-aware stage, we design a set of
hard context prompts to capture the sequence-level semantics in the
instruction. They enable further tuning of the pretrained models via
contrastive learning. Experimental results on both R2R and REVERIE show the
superiority of PANDA compared to existing state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yue Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wansen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Youkai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1&quot;&gt;Quanjun Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13672">
<title>Deep Reinforcement Learning for Image-to-Image Translation. (arXiv:2309.13672v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13672</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing Image-to-Image Translation (I2IT) methods generate images in a
single run of a deep learning (DL) model. However, designing such a single-step
model is always challenging, requiring a huge number of parameters and easily
falling into bad global minimums and overfitting. In this work, we reformulate
I2IT as a step-wise decision-making problem via deep reinforcement learning
(DRL) and propose a novel framework that performs RL-based I2IT (RL-I2IT). The
key feature in the RL-I2IT framework is to decompose a monolithic learning
process into small steps with a lightweight model to progressively transform a
source image successively to a target image. Considering that it is challenging
to handle high dimensional continuous state and action spaces in the
conventional RL framework, we introduce meta policy with a new concept Plan to
the standard Actor-Critic model, which is of a lower dimension than the
original image and can facilitate the actor to generate a tractable high
dimensional action. In the RL-I2IT framework, we also employ a task-specific
auxiliary learning strategy to stabilize the training process and improve the
performance of the corresponding task. Experiments on several I2IT tasks
demonstrate the effectiveness and robustness of the proposed method when facing
high-dimensional continuous action space problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Ziwei Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jing Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1&quot;&gt;Chengming Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Bin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Siwei Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15112">
<title>InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition. (arXiv:2309.15112v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15112</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose InternLM-XComposer, a vision-language large model that enables
advanced image-text comprehension and composition. The innovative nature of our
model is highlighted by three appealing properties: 1) Interleaved Text-Image
Composition: InternLM-XComposer can effortlessly generate coherent and
contextual articles that seamlessly integrate images, providing a more engaging
and immersive reading experience. Simply provide a writing instruction, and our
system will generate the corresponding manuscript. It can intelligently
identify the areas in the text where images would enhance the content and
automatically insert the most appropriate visual candidates. 2) Comprehension
with Rich Multilingual Knowledge: The text-image comprehension is empowered by
training on an extensive multi-modal multilingual database with carefully
crafted strategies, resulting in a deep understanding of visual content. 3)
State-of-the-art Performance: Our model consistently achieves state-of-the-art
results across various mainstream benchmarks for vision-language foundational
models, including MME Benchmark, MMBench, MMBench-CN, Seed-Bench, CCBench
(Chinese Cultural Benchmark), QBench and Tiny LVLM. Owing to the absence of
established metrics for quantitatively assessing text-image composition, we
have devised a robust evaluation procedure that comprises both human and
GPT4-Vision (GPT4-V) to ensure reliability. Notably, our InternLM-XComposer
achieves competitive text-image composition scores compared to public
solutions, including GPT4-V and GPT3.5. Collectively, InternLM-XComposer
seamlessly blends advanced text-image comprehension and composition,
revolutionizing vision-language interaction and offering new insights and
opportunities. The InternLM-XComposer model series are publicly available at
https://github.com/InternLM/InternLM-XComposer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xiaoyi Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yuhang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_L/0/1/0/all/0/1&quot;&gt;Linke Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1&quot;&gt;Haodong Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Songyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1&quot;&gt;Shuangrui Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jingwen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1&quot;&gt;Conghui He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xingcheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04086">
<title>End-to-End Chess Recognition. (arXiv:2310.04086v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04086</link>
<description rdf:parseType="Literal">&lt;p&gt;Chess recognition is the task of extracting the chess piece configuration
from a chessboard image. Current approaches use a pipeline of separate,
independent, modules such as chessboard detection, square localization, and
piece classification. Instead, we follow the deep learning philosophy and
explore an end-to-end approach to directly predict the configuration from the
image, thus avoiding the error accumulation of the sequential approaches and
eliminating the need for intermediate annotations. Furthermore, we introduce a
new dataset, Chess Recognition Dataset (ChessReD), that consists of 10,800 real
photographs and their corresponding annotations. In contrast to existing
datasets that are synthetically rendered and have only limited angles, ChessReD
has photographs captured from various angles using smartphone cameras; a sensor
choice made to ensure real-world applicability. Our approach in chess
recognition on the introduced challenging benchmark dataset outperforms related
approaches, successfully recognizing the chess pieces&apos; configuration in 15.26%
of ChessReD&apos;s test images. This accuracy may seem low, but it is ~7x better
than the current state-of-the-art and reflects the difficulty of the problem.
The code and data are available through:
https://github.com/ThanosM97/end-to-end-chess-recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masouris_A/0/1/0/all/0/1&quot;&gt;Athanasios Masouris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1&quot;&gt;Jan van Gemert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05804">
<title>Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis. (arXiv:2310.05804v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05804</link>
<description rdf:parseType="Literal">&lt;p&gt;Though Multimodal Sentiment Analysis (MSA) proves effective by utilizing rich
information from multiple sources (e.g., language, video, and audio), the
potential sentiment-irrelevant and conflicting information across modalities
may hinder the performance from being further improved. To alleviate this, we
present Adaptive Language-guided Multimodal Transformer (ALMT), which
incorporates an Adaptive Hyper-modality Learning (AHL) module to learn an
irrelevance/conflict-suppressing representation from visual and audio features
under the guidance of language features at different scales. With the obtained
hyper-modality representation, the model can obtain a complementary and joint
representation through multimodal fusion for effective MSA. In practice, ALMT
achieves state-of-the-art performance on several popular datasets (e.g., MOSI,
MOSEI and CH-SIMS) and an abundance of ablation demonstrates the validity and
necessity of our irrelevance/conflict suppression mechanism.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haoyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1&quot;&gt;Guanghao Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kejun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuanyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tianshu Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06372">
<title>Leveraging Diffusion-Based Image Variations for Robust Training on Poisoned Data. (arXiv:2310.06372v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06372</link>
<description rdf:parseType="Literal">&lt;p&gt;Backdoor attacks pose a serious security threat for training neural networks
as they surreptitiously introduce hidden functionalities into a model. Such
backdoors remain silent during inference on clean inputs, evading detection due
to inconspicuous behavior. However, once a specific trigger pattern appears in
the input data, the backdoor activates, causing the model to execute its
concealed function. Detecting such poisoned samples within vast datasets is
virtually impossible through manual inspection. To address this challenge, we
propose a novel approach that enables model training on potentially poisoned
datasets by utilizing the power of recent diffusion models. Specifically, we
create synthetic variations of all training samples, leveraging the inherent
resilience of diffusion models to potential trigger patterns in the data. By
combining this generative approach with knowledge distillation, we produce
student models that maintain their general performance on the task while
exhibiting robust resistance to backdoor triggers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1&quot;&gt;Lukas Struppek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hentschel_M/0/1/0/all/0/1&quot;&gt;Martin B. Hentschel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poth_C/0/1/0/all/0/1&quot;&gt;Clifton Poth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1&quot;&gt;Dominik Hintersdorf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06470">
<title>Focus on Local Regions for Query-based Object Detection. (arXiv:2310.06470v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06470</link>
<description rdf:parseType="Literal">&lt;p&gt;Query-based methods have garnered significant attention in object detection
since the advent of DETR, the pioneering query-based detector. However, these
methods face challenges like slow convergence and suboptimal performance.
Notably, self-attention in object detection often hampers convergence due to
its global focus. To address these issues, we propose FoLR, a transformer-like
architecture with only decoders. We improve the self-attention by isolating
connections between irrelevant objects that makes it focus on local regions but
not global regions. We also design the adaptive sampling method to extract
effective features based on queries&apos; local regions from feature maps.
Additionally, we employ a look-back strategy for decoders to retain previous
information, followed by the Feature Mixer module to fuse features and queries.
Experimental results demonstrate FoLR&apos;s state-of-the-art performance in
query-based detectors, excelling in convergence speed and computational
efficiency.
&lt;/p&gt;
&lt;p&gt;Index Terms: Local regions, Attention mechanism, Object detection
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hongbin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yamei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shuai Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1&quot;&gt;Bo Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08820">
<title>Learning to Adapt SAM for Segmenting Cross-domain Point Clouds. (arXiv:2310.08820v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08820</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain adaptation (UDA) in 3D segmentation tasks presents a
formidable challenge, primarily stemming from the sparse and unordered nature
of point cloud data. Especially for LiDAR point clouds, the domain discrepancy
becomes obvious across varying capture scenes, fluctuating weather conditions,
and the diverse array of LiDAR devices in use. While previous UDA methodologies
have often sought to mitigate this gap by aligning features between source and
target domains, this approach falls short when applied to 3D segmentation due
to the substantial domain variations. Inspired by the remarkable generalization
capabilities exhibited by the vision foundation model, SAM, in the realm of
image segmentation, our approach leverages the wealth of general knowledge
embedded within SAM to unify feature representations across diverse 3D domains
and further solves the 3D domain adaptation problem. Specifically, we harness
the corresponding images associated with point clouds to facilitate knowledge
transfer and propose an innovative hybrid feature augmentation methodology,
which significantly enhances the alignment between the 3D feature space and
SAM&apos;s feature space, operating at both the scene and instance levels. Our
method is evaluated on many widely-recognized datasets and achieves
state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xidong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Runnan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_F/0/1/0/all/0/1&quot;&gt;Feng Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1&quot;&gt;Lingdong Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Youquan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xinge Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuexin Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20092">
<title>Beyond U: Making Diffusion Models Faster &amp; Lighter. (arXiv:2310.20092v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20092</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models are a family of generative models that yield record-breaking
performance in tasks such as image synthesis, video generation, and molecule
design. Despite their capabilities, their efficiency, especially in the reverse
denoising process, remains a challenge due to slow convergence rates and high
computational costs. In this work, we introduce an approach that leverages
continuous dynamical systems to design a novel denoising network for diffusion
models that is more parameter-efficient, exhibits faster convergence, and
demonstrates increased noise robustness. Experimenting with denoising
probabilistic diffusion models, our framework operates with approximately a
quarter of the parameters and $\sim 30\%$ of the Floating Point Operations
(FLOPs) compared to standard U-Nets in Denoising Diffusion Probabilistic Models
(DDPMs). Furthermore, our model is faster in inference than the baseline models
when measured in equal conditions while converging to better quality solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calvo_Ordonez_S/0/1/0/all/0/1&quot;&gt;Sergio Calvo-Ordonez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiahao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lipei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1&quot;&gt;Carola-Bibiane Schonlieb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1&quot;&gt;Angelica I Aviles-Rivero&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00961">
<title>Concatenated Masked Autoencoders as Spatial-Temporal Learner. (arXiv:2311.00961v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00961</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning representations from videos requires understanding continuous motion
and visual correspondences between frames. In this paper, we introduce the
Concatenated Masked Autoencoders (CatMAE) as a spatial-temporal learner for
self-supervised video representation learning. For the input sequence of video
frames, CatMAE keeps the initial frame unchanged while applying substantial
masking (95%) to subsequent frames. The encoder in CatMAE is responsible for
encoding visible patches for each frame individually; subsequently, for each
masked frame, the decoder leverages visible patches from both previous and
current frames to reconstruct the original image. Our proposed method enables
the model to estimate the motion information between visible patches, match the
correspondences between preceding and succeeding frames, and ultimately learn
the evolution of scenes. Furthermore, we propose a new data augmentation
strategy, Video-Reverse (ViRe), which uses reversed video frames as the model&apos;s
reconstruction targets. This further encourages the model to utilize continuous
motion details and correspondences to complete the reconstruction, thereby
enhancing the model&apos;s capabilities. Compared to the most advanced pre-training
methods, CatMAE achieves a leading level in video segmentation tasks and action
recognition tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhouqiang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bowen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tong Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1&quot;&gt;Zhaofeng Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guangshun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liangzhi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02558">
<title>Multi-Agent 3D Map Reconstruction and Change Detection in Microgravity with Free-Flying Robots. (arXiv:2311.02558v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02558</link>
<description rdf:parseType="Literal">&lt;p&gt;Assistive free-flyer robots autonomously caring for future crewed outposts --
such as NASA&apos;s Astrobee robots on the International Space Station (ISS) -- must
be able to detect day-to-day interior changes to track inventory, detect and
diagnose faults, and monitor the outpost status. This work presents a framework
for multi-agent cooperative mapping and change detection to enable robotic
maintenance of space outposts. One agent is used to reconstruct a 3D model of
the environment from sequences of images and corresponding depth information.
Another agent is used to periodically scan the environment for inconsistencies
against the 3D model. Change detection is validated after completing the
surveys using real image and pose data collected by Astrobee robots in a ground
testing environment and from microgravity aboard the ISS. This work outlines
the objectives, requirements, and algorithmic modules for the multi-agent
reconstruction system, including recommendations for its use by assistive
free-flyers aboard future microgravity outposts.
&lt;/p&gt;
&lt;p&gt;*Denotes Equal Contribution
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dinkel_H/0/1/0/all/0/1&quot;&gt;Holly Dinkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Di_J/0/1/0/all/0/1&quot;&gt;Julia Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1&quot;&gt;Jamie Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albee_K/0/1/0/all/0/1&quot;&gt;Keenan Albee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borges_P/0/1/0/all/0/1&quot;&gt;Paulo Borges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreira_M/0/1/0/all/0/1&quot;&gt;Marina Moreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexandrov_O/0/1/0/all/0/1&quot;&gt;Oleg Alexandrov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coltin_B/0/1/0/all/0/1&quot;&gt;Brian Coltin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_T/0/1/0/all/0/1&quot;&gt;Trey Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03828">
<title>Multi-view Information Integration and Propagation for Occluded Person Re-identification. (arXiv:2311.03828v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03828</link>
<description rdf:parseType="Literal">&lt;p&gt;Occluded person re-identification (re-ID) presents a challenging task due to
occlusion perturbations. Although great efforts have been made to prevent the
model from being disturbed by occlusion noise, most current solutions only
capture information from a single image, disregarding the rich complementary
information available in multiple images depicting the same pedestrian. In this
paper, we propose a novel framework called Multi-view Information Integration
and Propagation (MVI$^{2}$P). Specifically, realizing the potential of
multi-view images in effectively characterizing the occluded target pedestrian,
we integrate feature maps of which to create a comprehensive representation.
During this process, to avoid introducing occlusion noise, we develop a
CAMs-aware Localization module that selectively integrates information
contributing to the identification. Additionally, considering the divergence in
the discriminative nature of different images, we design a probability-aware
Quantification module to emphatically integrate highly reliable information.
Moreover, as multiple images with the same identity are not accessible in the
testing stage, we devise an Information Propagation (IP) mechanism to distill
knowledge from the comprehensive representation to that of a single occluded
image. Extensive experiments and analyses have unequivocally demonstrated the
effectiveness and superiority of the proposed MVI$^{2}$P. The code will be
released at \url{https://github.com/nengdong96/MVIIP}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1&quot;&gt;Neng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shuanglin Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jinhui Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liyan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06497">
<title>DRUformer: Enhancing the driving scene Important object detection with driving relationship self-understanding. (arXiv:2311.06497v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06497</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic accidents frequently lead to fatal injuries, contributing to over 50
million deaths until 2023. To mitigate driving hazards and ensure personal
safety, it is crucial to assist vehicles in anticipating important objects
during travel. Previous research on important object detection primarily
assessed the importance of individual participants, treating them as
independent entities and frequently overlooking the connections between these
participants. Unfortunately, this approach has proven less effective in
detecting important objects in complex scenarios. In response, we introduce
Driving scene Relationship self-Understanding transformer (DRUformer), designed
to enhance the important object detection task. The DRUformer is a
transformer-based multi-modal important object detection model that takes into
account the relationships between all the participants in the driving scenario.
Recognizing that driving intention also significantly affects the detection of
important objects during driving, we have incorporated a module for embedding
driving intention. To assess the performance of our approach, we conducted a
comparative experiment on the DRAMA dataset, pitting our model against other
state-of-the-art (SOTA) models. The results demonstrated a noteworthy 16.2\%
improvement in mIoU and a substantial 12.3\% boost in ACC compared to SOTA
methods. Furthermore, we conducted a qualitative analysis of our model&apos;s
ability to detect important objects across different road scenarios and
classes, highlighting its effectiveness in diverse contexts. Finally, we
conducted various ablation studies to assess the efficiency of the proposed
modules in our DRUformer model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1&quot;&gt;Yingjie Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Ming Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1&quot;&gt;Keisuke Fujii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohtani_K/0/1/0/all/0/1&quot;&gt;Kento Ohtani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carballo_A/0/1/0/all/0/1&quot;&gt;Alexander Carballo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1&quot;&gt;Kazuya Takeda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08141">
<title>GMTR: Graph Matching Transformers. (arXiv:2311.08141v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08141</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers (ViTs) have recently been used for visual matching beyond
object detection and segmentation. However, the original grid dividing strategy
of ViTs neglects the spatial information of the keypoints, limiting the
sensitivity to local information. Therefore, we propose QueryTrans (Query
Transformer), which adopts a cross-attention module and keypoints-based center
crop strategy for better spatial information extraction. We further integrate
the graph attention module and devise a transformer-based graph matching
approach GMTR (Graph Matching TRansformers) whereby the combinatorial nature of
GM is addressed by a graph transformer neural GM solver. On standard GM
benchmarks, GMTR shows competitive performance against the SOTA frameworks.
Specifically, on Pascal VOC, GMTR achieves $\mathbf{83.6\%}$ accuracy,
$\mathbf{0.9\%}$ higher than the SOTA framework. On Spair-71k, GMTR shows great
potential and outperforms most of the previous works. Meanwhile, on Pascal VOC,
QueryTrans improves the accuracy of NGMv2 from $80.1\%$ to $\mathbf{83.3\%}$,
and BBGM from $79.0\%$ to $\mathbf{84.5\%}$. On Spair-71k, QueryTrans improves
NGMv2 from $80.6\%$ to $\mathbf{82.5\%}$, and BBGM from $82.1\%$ to
$\mathbf{83.9\%}$. Source code will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jinpei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaofeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Runzhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09118">
<title>WildlifeDatasets: An open-source toolkit for animal re-identification. (arXiv:2311.09118v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09118</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present WildlifeDatasets
(https://github.com/WildlifeDatasets/wildlife-datasets) - an open-source
toolkit intended primarily for ecologists and computer-vision /
machine-learning researchers. The WildlifeDatasets is written in Python, allows
straightforward access to publicly available wildlife datasets, and provides a
wide variety of methods for dataset pre-processing, performance analysis, and
model fine-tuning. We showcase the toolkit in various scenarios and baseline
experiments, including, to the best of our knowledge, the most comprehensive
experimental comparison of datasets and methods for wildlife re-identification,
including both local descriptors and deep learning approaches. Furthermore, we
provide the first-ever foundation model for individual re-identification within
a wide range of species - MegaDescriptor - that provides state-of-the-art
performance on animal re-identification datasets and outperforms other
pre-trained models such as CLIP and DINOv2 by a significant margin. To make the
model available to the general public and to allow easy integration with any
existing wildlife monitoring applications, we provide multiple MegaDescriptor
flavors (i.e., Small, Medium, and Large) through the HuggingFace hub
(https://huggingface.co/BVRA).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cermak_V/0/1/0/all/0/1&quot;&gt;Vojt&amp;#x11b;ch &amp;#x10c;erm&amp;#xe1;k&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picek_L/0/1/0/all/0/1&quot;&gt;Lukas Picek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adam_L/0/1/0/all/0/1&quot;&gt;Luk&amp;#xe1;&amp;#x161; Adam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papafitsoros_K/0/1/0/all/0/1&quot;&gt;Kostas Papafitsoros&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12342">
<title>LoCo: Locally Constrained Training-Free Layout-to-Image Synthesis. (arXiv:2311.12342v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12342</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent text-to-image diffusion models have reached an unprecedented level in
generating high-quality images. However, their exclusive reliance on textual
prompts often falls short in accurately conveying fine-grained spatial
compositions. In this paper, we propose LoCo, a training-free approach for
layout-to-image synthesis that excels in producing high-quality images aligned
with both textual prompts and spatial layouts. Our method introduces a
Localized Attention Constraint to refine cross-attention for individual
objects, ensuring their precise placement in designated regions. We further
propose a Padding Token Constraint to leverage the semantic information
embedded in previously neglected padding tokens, thereby preventing the
undesired fusion of synthesized objects. LoCo seamlessly integrates into
existing text-to-image and layout-to-image models, significantly amplifying
their performance and effectively addressing semantic failures observed in
prior methods. Through extensive experiments, we showcase the superiority of
our approach, surpassing existing state-of-the-art training-free
layout-to-image methods both qualitatively and quantitatively across multiple
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Peiang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Han Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1&quot;&gt;Ruiyang Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;S. Kevin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13018">
<title>GeoLocator: a location-integrated large multimodal model for inferring geo-privacy. (arXiv:2311.13018v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13018</link>
<description rdf:parseType="Literal">&lt;p&gt;Geographic privacy or geo-privacy refers to the keeping private of one&apos;s
geographic location, especially the restriction of geographical data maintained
by personal electronic equipment. Geo-privacy is a crucial aspect of personal
security, however often goes unnoticed in daily activities. With the surge in
the use of Large Multimodal Models (LMM), such as GPT-4, for Open Source
Intelligence (OSINT), the potential risks associated with geo-privacy breaches
have intensified. This study develops a location-integrated GPT-4 based model
named GeoLocator and designed four-dimensional experiments to demonstrate its
capability in inferring and identifying the locational information of input
imageries and/or social media contents. Our experiments reveal that GeoLocator
generates specific geographic details with high accuracy and consequently
embeds the risk of the model users exposing geospatial information to the
public unintentionally, highlighting the thread of online data sharing,
information gathering technologies and LLM on geo-privacy. We conclude with the
broader implications of GeoLocator and our findings for individuals and the
community at large, by emphasizing the urgency for enhanced awareness and
protective measures against geo-privacy leakage in the era of advanced AI and
widespread social media usage.
&lt;/p&gt;
&lt;p&gt;Keywords: geoprivacy, GPT-4, image comprehension, Large Multimodal Model
(LMM), Open Source Intelligence (OSINT)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yifan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yixian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Daoyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shuju Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Junhong Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junzhou He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qingyang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14294">
<title>Decouple Content and Motion for Conditional Image-to-Video Generation. (arXiv:2311.14294v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14294</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of conditional image-to-video (cI2V) generation is to create a
believable new video by beginning with the condition, i.e., one image and
text.The previous cI2V generation methods conventionally perform in RGB pixel
space, with limitations in modeling motion consistency and visual continuity.
Additionally, the efficiency of generating videos in pixel space is quite
low.In this paper, we propose a novel approach to address these challenges by
disentangling the target RGB pixels into two distinct components: spatial
content and temporal motions. Specifically, we predict temporal motions which
include motion vector and residual based on a 3D-UNet diffusion model. By
explicitly modeling temporal motions and warping them to the starting image, we
improve the temporal consistency of generated videos. This results in a
reduction of spatial redundancy, emphasizing temporal details. Our proposed
method achieves performance improvements by disentangling content and motion,
all without introducing new structural complexities to the model. Extensive
experiments on various datasets confirm our approach&apos;s superior performance
over the majority of state-of-the-art methods in both effectiveness and
efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Cuifeng Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1&quot;&gt;Yulu Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiongwei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lele Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1&quot;&gt;Tingting Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinzhi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14388">
<title>A Parameterized Generative Adversarial Network Using Cyclic Projection for Explainable Medical Image Classification. (arXiv:2311.14388v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14388</link>
<description rdf:parseType="Literal">&lt;p&gt;Although current data augmentation methods are successful to alleviate the
data insufficiency, conventional augmentation are primarily intra-domain while
advanced generative adversarial networks (GANs) generate images remaining
uncertain, particularly in small-scale datasets. In this paper, we propose a
parameterized GAN (ParaGAN) that effectively controls the changes of synthetic
samples among domains and highlights the attention regions for downstream
classification. Specifically, ParaGAN incorporates projection distance
parameters in cyclic projection and projects the source images to the decision
boundary to obtain the class-difference maps. Our experiments show that ParaGAN
can consistently outperform the existing augmentation methods with explainable
classification on two small-scale medical datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_X/0/1/0/all/0/1&quot;&gt;Xiangyu Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yue Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaohong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_C/0/1/0/all/0/1&quot;&gt;Chan-Tong Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_T/0/1/0/all/0/1&quot;&gt;Tong Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1&quot;&gt;Qinquan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_W/0/1/0/all/0/1&quot;&gt;Wei Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1&quot;&gt;Tao Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15939">
<title>Unleashing the Power of Prompt-driven Nucleus Instance Segmentation. (arXiv:2311.15939v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15939</link>
<description rdf:parseType="Literal">&lt;p&gt;Nuclear instance segmentation in histology images is crucial for a broad
spectrum of clinical applications. Current prevailing nuclear instance
segmentation algorithms rely on regression of nuclei contours, distance maps,
watershed markers or a proxy nuclear representation of star-convex polygons.
Consequently, these methods necessitate sophisticated post-processing
operations to distinguish nuclei instances, which are commonly acknowledged to
be error-prone and parameter-sensitive. Recently, the segment anything model
(SAM) has earned attracted huge attention within the domain of medical image
segmentation due to its impressive generalization ability and promptable
property. Nevertheless, its potential on nuclear instance segmentation remains
largely underexplored. In this paper, we present a novel prompt-driven
framework that consists of a point prompter and a SAM for automatic nuclei
instance segmentation. Specifically, the prompter learns to generate a unique
point prompt for each nucleus while the SAM is fine tuned to output the
corresponding mask of the cued nucleus. Furthermore, we propose to add adjacent
nuclei as negative prompts to promote the model&apos;s ability to recognize
overlapping nuclei. Without bells and whistles, our proposed method sets a new
state-of-the-art performance on three challenging benchmarks. Our code is
available at \url{https://github.com/windygoo/PromptNucSeg}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shui_Z/0/1/0/all/0/1&quot;&gt;Zhongyi Shui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunlong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1&quot;&gt;Kai Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chenglu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16516">
<title>Segment Every Out-of-Distribution Object. (arXiv:2311.16516v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16516</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation models, while effective for in-distribution categories,
face challenges in real-world deployment due to encountering
out-of-distribution (OoD) objects. Detecting these OoD objects is crucial for
safety-critical applications. Existing methods rely on anomaly scores, but
choosing a suitable threshold for generating masks presents difficulties and
can lead to fragmentation and inaccuracy. This paper introduces a method to
convert anomaly \textbf{S}core \textbf{T}o segmentation \textbf{M}ask, called
S2M, a simple and effective framework for OoD detection in semantic
segmentation. Unlike assigning anomaly scores to pixels, S2M directly segments
the entire OoD object. By transforming anomaly scores into prompts for a
promptable segmentation model, S2M eliminates the need for threshold selection.
Extensive experiments demonstrate that S2M outperforms the state-of-the-art by
approximately 10% in IoU and 30% in mean F1 score, on average, across various
benchmarks including Fishyscapes, Segment-Me-If-You-Can, and RoadAnomaly
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wenjie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1&quot;&gt;Yu Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yunhui Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17135">
<title>TLControl: Trajectory and Language Control for Human Motion Synthesis. (arXiv:2311.17135v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17135</link>
<description rdf:parseType="Literal">&lt;p&gt;Controllable human motion synthesis is essential for applications in AR/VR,
gaming, movies, and embodied AI. Existing methods often focus solely on either
language or full trajectory control, lacking precision in synthesizing motions
aligned with user-specified trajectories, especially for multi-joint control.
To address these issues, we present TLControl, a new method for realistic human
motion synthesis, incorporating both low-level trajectory and high-level
language semantics controls. Specifically, we first train a VQ-VAE to learn a
compact latent motion space organized by body parts. We then propose a Masked
Trajectories Transformer to make coarse initial predictions of full
trajectories of joints based on the learned latent motion space, with
user-specified partial trajectories and text descriptions as conditioning.
Finally, we introduce an efficient test-time optimization to refine these
coarse predictions for accurate trajectory control. Experiments demonstrate
that TLControl outperforms the state-of-the-art in trajectory accuracy and time
efficiency, making it practical for interactive and high-quality animation
generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1&quot;&gt;Weilin Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1&quot;&gt;Zhiyang Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1&quot;&gt;Taku Komura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenping Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1&quot;&gt;Dinesh Jayaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingjie Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17516">
<title>MMA-Diffusion: MultiModal Attack on Diffusion Models. (arXiv:2311.17516v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17516</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, Text-to-Image (T2I) models have seen remarkable
advancements, gaining widespread adoption. However, this progress has
inadvertently opened avenues for potential misuse, particularly in generating
inappropriate or Not-Safe-For-Work (NSFW) content. Our work introduces
MMA-Diffusion, a framework that presents a significant and realistic threat to
the security of T2I models by effectively circumventing current defensive
measures in both open-source models and commercial online services. Unlike
previous approaches, MMA-Diffusion leverages both textual and visual modalities
to bypass safeguards like prompt filters and post-hoc safety checkers, thus
exposing and highlighting the vulnerabilities in existing defense mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yijun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Ruiyuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaosen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1&quot;&gt;Tsung-Yi Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1&quot;&gt;Nan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17804">
<title>Aggregation Model Hyperparameters Matter in Digital Pathology. (arXiv:2311.17804v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17804</link>
<description rdf:parseType="Literal">&lt;p&gt;Digital pathology has significantly advanced disease detection and
pathologist efficiency through the analysis of gigapixel whole-slide images
(WSI). In this process, WSIs are first divided into patches, for which a
feature extractor model is applied to obtain feature vectors, which are
subsequently processed by an aggregation model to predict the respective WSI
label. With the rapid evolution of representation learning, numerous new
feature extractor models, often termed foundational models, have emerged.
Traditional evaluation methods, however, rely on fixed aggregation model
hyperparameters, a framework we identify as potentially biasing the results.
Our study uncovers a co-dependence between feature extractor models and
aggregation model hyperparameters, indicating that performance comparability
can be skewed based on the chosen hyperparameters. By accounting for this
co-dependency, we find that the performance of many current feature extractor
models is notably similar. We support this insight by evaluating seven feature
extractor models across three different datasets with 162 different aggregation
model configurations. This comprehensive approach provides a more nuanced
understanding of the relationship between feature extractors and aggregation
models, leading to a fairer and more accurate assessment of feature extractor
models in digital pathology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bredell_G/0/1/0/all/0/1&quot;&gt;Gustav Bredell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1&quot;&gt;Marcel Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szostak_P/0/1/0/all/0/1&quot;&gt;Przemyslaw Szostak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbasi_Sureshjani_S/0/1/0/all/0/1&quot;&gt;Samaneh Abbasi-Sureshjani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomariz_A/0/1/0/all/0/1&quot;&gt;Alvaro Gomariz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18259">
<title>Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives. (arXiv:2311.18259v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18259</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Ego-Exo4D, a diverse, large-scale multimodal multiview video
dataset and benchmark challenge. Ego-Exo4D centers around
simultaneously-captured egocentric and exocentric video of skilled human
activities (e.g., sports, music, dance, bike repair). More than 800
participants from 13 cities worldwide performed these activities in 131
different natural scene contexts, yielding long-form captures from 1 to 42
minutes each and 1,422 hours of video combined. The multimodal nature of the
dataset is unprecedented: the video is accompanied by multichannel audio, eye
gaze, 3D point clouds, camera poses, IMU, and multiple paired language
descriptions -- including a novel &quot;expert commentary&quot; done by coaches and
teachers and tailored to the skilled-activity domain. To push the frontier of
first-person video understanding of skilled human activity, we also present a
suite of benchmark tasks and their annotations, including fine-grained activity
understanding, proficiency estimation, cross-view translation, and 3D hand/body
pose. All resources will be open sourced to fuel new research in the community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1&quot;&gt;Kristen Grauman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Westbury_A/0/1/0/all/0/1&quot;&gt;Andrew Westbury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1&quot;&gt;Lorenzo Torresani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1&quot;&gt;Kris Kitani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1&quot;&gt;Jitendra Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afouras_T/0/1/0/all/0/1&quot;&gt;Triantafyllos Afouras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashutosh_K/0/1/0/all/0/1&quot;&gt;Kumar Ashutosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baiyya_V/0/1/0/all/0/1&quot;&gt;Vijay Baiyya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1&quot;&gt;Siddhant Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boote_B/0/1/0/all/0/1&quot;&gt;Bikram Boote&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byrne_E/0/1/0/all/0/1&quot;&gt;Eugene Byrne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chavis_Z/0/1/0/all/0/1&quot;&gt;Zach Chavis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Joya Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1&quot;&gt;Feng Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_F/0/1/0/all/0/1&quot;&gt;Fu-Jen Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crane_S/0/1/0/all/0/1&quot;&gt;Sean Crane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1&quot;&gt;Avijit Dasgupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jing Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Escobar_M/0/1/0/all/0/1&quot;&gt;Maria Escobar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forigua_C/0/1/0/all/0/1&quot;&gt;Cristhian Forigua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gebreselasie_A/0/1/0/all/0/1&quot;&gt;Abrham Gebreselasie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haresh_S/0/1/0/all/0/1&quot;&gt;Sanjay Haresh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Md Mohaiminul Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Suyog Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khirodkar_R/0/1/0/all/0/1&quot;&gt;Rawal Khirodkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kukreja_D/0/1/0/all/0/1&quot;&gt;Devansh Kukreja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Kevin J Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jia-Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumder_S/0/1/0/all/0/1&quot;&gt;Sagnik Majumder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yongsen Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_M/0/1/0/all/0/1&quot;&gt;Miguel Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mavroudi_E/0/1/0/all/0/1&quot;&gt;Effrosyni Mavroudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagarajan_T/0/1/0/all/0/1&quot;&gt;Tushar Nagarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ragusa_F/0/1/0/all/0/1&quot;&gt;Francesco Ragusa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramakrishnan_S/0/1/0/all/0/1&quot;&gt;Santhosh Kumar Ramakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seminara_L/0/1/0/all/0/1&quot;&gt;Luigi Seminara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Somayazulu_A/0/1/0/all/0/1&quot;&gt;Arjun Somayazulu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yale Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Shan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1&quot;&gt;Zihui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Edward Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jinxu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castillo_A/0/1/0/all/0/1&quot;&gt;Angela Castillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xinzhu Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furuta_R/0/1/0/all/0/1&quot;&gt;Ryosuke Furuta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_C/0/1/0/all/0/1&quot;&gt;Cristina Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1&quot;&gt;Prince Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jiabo Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yifei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiming Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khoo_W/0/1/0/all/0/1&quot;&gt;Weslie Khoo&lt;/a&gt;, et al. (48 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00856">
<title>QAFE-Net: Quality Assessment of Facial Expressions with Landmark Heatmaps. (arXiv:2312.00856v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00856</link>
<description rdf:parseType="Literal">&lt;p&gt;Facial expression recognition (FER) methods have made great inroads in
categorising moods and feelings in humans. Beyond FER, pain estimation methods
assess levels of intensity in pain expressions, however assessing the quality
of all facial expressions is of critical value in health-related applications.
In this work, we address the quality of five different facial expressions in
patients affected by Parkinson&apos;s disease. We propose a novel landmark-guided
approach, QAFE-Net, that combines temporal landmark heatmaps with RGB data to
capture small facial muscle movements that are encoded and mapped to severity
scores. The proposed approach is evaluated on a new Parkinson&apos;s Disease Facial
Expression dataset (PFED5), as well as on the pain estimation benchmark, the
UNBC-McMaster Shoulder Pain Expression Archive Database. Our comparative
experiments demonstrate that the proposed method outperforms SOTA action
quality assessment works on PFED5 and achieves lower mean absolute error than
the SOTA pain estimation methods on UNBC-McMaster. Our code and the new PFED5
dataset are available at https://github.com/shuchaoduan/QAFE-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1&quot;&gt;Shuchao Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dadashzadeh_A/0/1/0/all/0/1&quot;&gt;Amirhossein Dadashzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whone_A/0/1/0/all/0/1&quot;&gt;Alan Whone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirmehdi_M/0/1/0/all/0/1&quot;&gt;Majid Mirmehdi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00878">
<title>Grounding Everything: Emerging Localization Properties in Vision-Language Transformers. (arXiv:2312.00878v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00878</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-language foundation models have shown remarkable performance in
various zero-shot settings such as image retrieval, classification, or
captioning. But so far, those models seem to fall behind when it comes to
zero-shot localization of referential expressions and objects in images. As a
result, they need to be fine-tuned for this task. In this paper, we show that
pretrained vision-language (VL) models allow for zero-shot open-vocabulary
object localization without any fine-tuning. To leverage those capabilities, we
propose a Grounding Everything Module (GEM) that generalizes the idea of
value-value attention introduced by CLIPSurgery to a self-self attention path.
We show that the concept of self-self attention corresponds to clustering, thus
enforcing groups of tokens arising from the same object to be similar while
preserving the alignment with the language space. To further guide the group
formation, we propose a set of regularizations that allows the model to finally
generalize across datasets and backbones. We evaluate the proposed GEM
framework on various benchmark tasks and datasets for semantic segmentation. It
shows that GEM not only outperforms other training-free open-vocabulary
localization methods, but also achieves state-of-the-art results on the
recently proposed OpenImagesV7 large-scale segmentation benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bousselham_W/0/1/0/all/0/1&quot;&gt;Walid Bousselham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petersen_F/0/1/0/all/0/1&quot;&gt;Felix Petersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1&quot;&gt;Vittorio Ferrari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1&quot;&gt;Hilde Kuehne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01397">
<title>Visual Prompting Upgrades Neural Network Sparsification: A Data-Model Perspective. (arXiv:2312.01397v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01397</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid development of large-scale deep learning models questions the
affordability of hardware platforms, which necessitates the pruning to reduce
their computational and memory footprints. Sparse neural networks as the
product, have demonstrated numerous favorable benefits like low complexity,
undamaged generalization, etc. Most of the prominent pruning strategies are
invented from a model-centric perspective, focusing on searching and preserving
crucial weights by analyzing network topologies. However, the role of data and
its interplay with model-centric pruning has remained relatively unexplored. In
this research, we introduce a novel data-model co-design perspective: to
promote superior weight sparsity by learning important model topology and
adequate input data in a synergetic manner. Specifically, customized Visual
Prompts are mounted to upgrade neural Network sparsification in our proposed
VPNs framework. As a pioneering effort, this paper conducts systematic
investigations about the impact of different visual prompts on model pruning
and suggests an effective joint optimization approach. Extensive experiments
with 3 network architectures and 8 datasets evidence the substantial
performance improvements from VPNs over existing start-of-the-art pruning
algorithms. Furthermore, we find that subnetworks discovered by VPNs from
pre-trained models enjoy better transferability across diverse downstream
scenarios. These insights shed light on new promising possibilities of
data-model co-designs for vision model sparsification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Can Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tianjin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yihua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1&quot;&gt;Mykola Pechenizkiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sijia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shiwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianlong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02252">
<title>StoryGPT-V: Large Language Models as Consistent Story Visualizers. (arXiv:2312.02252v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02252</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent generative models have demonstrated impressive capabilities in
generating realistic and visually pleasing images grounded on textual prompts.
Nevertheless, a significant challenge remains in applying these models for the
more intricate task of story visualization. Since it requires resolving
pronouns (he, she, they) in the frame descriptions, i.e., anaphora resolution,
and ensuring consistent characters and background synthesis across frames. Yet,
the emerging Large Language Model (LLM) showcases robust reasoning abilities to
navigate through ambiguous references and process extensive sequences.
Therefore, we introduce \textbf{StoryGPT-V}, which leverages the merits of the
latent diffusion (LDM) and LLM to produce images with consistent and
high-quality characters grounded on given story descriptions. First, we train a
character-aware LDM, which takes character-augmented semantic embedding as
input and includes the supervision of the cross-attention map using character
segmentation masks, aiming to enhance character generation accuracy and
faithfulness. In the second stage, we enable an alignment between the output of
LLM and the character-augmented embedding residing in the input space of the
first-stage model. This harnesses the reasoning ability of LLM to address
ambiguous references and the comprehension capability to memorize the context.
We conduct comprehensive experiments on two visual story visualization
benchmarks. Our model reports superior quantitative results and consistently
generates accurate characters of remarkable quality with low memory
consumption. Our code will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xiaoqian Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1&quot;&gt;Mohamed Elhoseiny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03818">
<title>Alpha-CLIP: A CLIP Model Focusing on Wherever You Want. (arXiv:2312.03818v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03818</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive Language-Image Pre-training (CLIP) plays an essential role in
extracting valuable content information from images across diverse tasks. It
aligns textual and visual modalities to comprehend the entire image, including
all the details, even those irrelevant to specific tasks. However, for a finer
understanding and controlled editing of images, it becomes crucial to focus on
specific regions of interest, which can be indicated as points, masks, or boxes
by humans or perception models. To fulfill the requirements, we introduce
Alpha-CLIP, an enhanced version of CLIP with an auxiliary alpha channel to
suggest attentive regions and fine-tuned with constructed millions of RGBA
region-text pairs. Alpha-CLIP not only preserves the visual recognition ability
of CLIP but also enables precise control over the emphasis of image contents.
It demonstrates effectiveness in various tasks, including but not limited to
open-world recognition, multimodal large language models, and conditional 2D /
3D generation. It has a strong potential to serve as a versatile tool for
image-related tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zeyi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Ye Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zang_Y/0/1/0/all/0/1&quot;&gt;Yuhang Zang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1&quot;&gt;Shu Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yuanjun Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04265">
<title>Stronger, Fewer, &amp; Superior: Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation. (arXiv:2312.04265v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04265</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we first assess and harness various Vision Foundation Models
(VFMs) in the context of Domain Generalized Semantic Segmentation (DGSS).
Driven by the motivation that Leveraging Stronger pre-trained models and Fewer
trainable parameters for Superior generalizability, we introduce a robust
fine-tuning approach, namely Rein, to parameter-efficiently harness VFMs for
DGSS. Built upon a set of trainable tokens, each linked to distinct instances,
Rein precisely refines and forwards the feature maps from each layer to the
next layer within the backbone. This process produces diverse refinements for
different categories within a single image. With fewer trainable parameters,
Rein efficiently fine-tunes VFMs for DGSS tasks, surprisingly surpassing full
parameter fine-tuning. Extensive experiments across various settings
demonstrate that Rein significantly outperforms state-of-the-art methods.
Remarkably, with just an extra 1% of trainable parameters within the frozen
backbone, Rein achieves a mIoU of 68.1% on the Cityscapes, without accessing
any real urban-scene datasets.Code is available at
https://github.com/w1oves/Rein.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhixiang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianle Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1&quot;&gt;Pengyang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Ben Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huaian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jinjin Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05284">
<title>0.1% Data Makes Segment Anything Slim. (arXiv:2312.05284v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05284</link>
<description rdf:parseType="Literal">&lt;p&gt;The formidable model size and demanding computational requirements of Segment
Anything Model (SAM) have rendered it cumbersome for deployment on
resource-constrained devices. Existing approaches for SAM compression typically
involve training a new network from scratch, posing a challenging trade-off
between compression costs and model performance. To address this issue, this
paper introduces SlimSAM, a novel SAM compression method that achieves superior
performance with remarkably low training costs. This is achieved by the
efficient reuse of pre-trained SAMs through a unified pruning-distillation
framework. To enhance knowledge inheritance from the original SAM, we employ an
innovative alternate slimming strategy that partitions the compression process
into a progressive procedure. Diverging from prior pruning techniques, we
meticulously prune and distill decoupled model structures in an alternating
fashion. Furthermore, a novel label-free pruning criterion is also proposed to
align the pruning objective with the optimization target, thereby boosting the
post-distillation after pruning. SlimSAM yields significant performance
improvements while demanding over 10 times less training costs than any other
existing methods. Even when compared to the original SAM-H, SlimSAM achieves
approaching performance while reducing parameter counts to merely 0.9% (5.7M),
MACs to 0.8% (21G), and requiring only 0.1% (10k) of the SAM training data.
Code is available at url{&lt;a href=&quot;http://github.com/czg1225/SlimSAM&quot;&gt;this http URL&lt;/a&gt;}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zigeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_G/0/1/0/all/0/1&quot;&gt;Gongfan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xinyin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinchao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05476">
<title>Exploring the Naturalness of AI-Generated Images. (arXiv:2312.05476v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05476</link>
<description rdf:parseType="Literal">&lt;p&gt;The proliferation of Artificial Intelligence-Generated Images (AGIs) has
greatly expanded the Image Naturalness Assessment (INA) problem. Different from
early definitions that mainly focus on tone-mapped images with limited
distortions (e.g., exposure, contrast, and color reproduction), INA on
AI-generated images is especially challenging as it has more diverse contents
and could be affected by factors from multiple perspectives, including
low-level technical distortions and high-level rationality distortions. In this
paper, we take the first step to benchmark and assess the visual naturalness of
AI-generated images. First, we construct the AI-Generated Image Naturalness
(AGIN) database by conducting a large-scale subjective study to collect human
opinions on the overall naturalness as well as perceptions from technical and
rationality perspectives. AGIN verifies that naturalness is universally and
disparately affected by both technical and rationality distortions. Second, we
propose the Joint Objective Image Naturalness evaluaTor (JOINT), to
automatically learn the naturalness of AGIs that aligns human ratings.
Specifically, JOINT imitates human reasoning in naturalness evaluation by
jointly learning both technical and rationality perspectives. Experimental
results show our proposed JOINT significantly surpasses baselines for providing
more subjectively consistent results on naturalness assessment. Our database
and code will be released in https://github.com/zijianchen98/AGIN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zijian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haoning Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zicheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jun Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1&quot;&gt;Xiongkuo Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1&quot;&gt;Guangtao Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenjun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06720">
<title>Audio-Visual LLM for Video Understanding. (arXiv:2312.06720v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06720</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents Audio-Visual LLM, a Multimodal Large Language Model that
takes both visual and auditory inputs for holistic video understanding. A key
design is the modality-augmented training, which involves the integration of
modality-specific tokens engineered to activate the appropriate visual and/or
auditory encoder selectively. This mechanism is pivotal in enabling end-to-end
joint training with video data at different modalities, including visual-only,
audio-only, and audio-visual formats. Moreover, we introduce a high-quality
video instruction dataset, derived from GPT-4. This dataset allows Audio-Visual
LLM to adeptly process a variety of task-oriented video instructions, ranging
from multi-turn conversations and audio-visual narratives to complex reasoning
tasks. Extensive experiments demonstrate that Audio-Visual LLM impressively
achieves strong zero-shot results across a range of video understanding tasks.
For example, Audio-Visual LLM achieves an accuracy of 53.7% on MSRVTT-QA,
outperforming non-LLM-based InterVideo by 6.6% and LLM-based Valley by 4.4%,
respectively. Additionally, our Audio-Visual LLM also achieves competitive
performance on audio tasks (e.g., AudioCaps).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_F/0/1/0/all/0/1&quot;&gt;Fangxun Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Cihang Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06723">
<title>Learning to See Low-Light Images via Feature Domain Adaptation. (arXiv:2312.06723v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06723</link>
<description rdf:parseType="Literal">&lt;p&gt;Raw low light image enhancement (LLIE) has achieved much better performance
than the sRGB domain enhancement methods due to the merits of raw data.
However, the ambiguity between noisy to clean and raw to sRGB mappings may
mislead the single-stage enhancement networks. The two-stage networks avoid
ambiguity by decoupling the two mappings but usually have large computing
complexity. To solve this problem, we propose a single-stage network empowered
by Feature Domain Adaptation (FDA) to decouple the denoising and color mapping
tasks in raw LLIE. The denoising encoder is supervised by the clean raw image,
and then the denoised features are adapted for the color mapping task by an FDA
module. We propose a Lineformer to serve as the FDA, which can well explore the
global and local correlations with fewer line buffers (friendly to the
line-based imaging process). During inference, the raw supervision branch is
removed. In this way, our network combines the advantage of a two-stage
enhancement process with the efficiency of single-stage inference. Experiments
on four benchmark datasets demonstrate that our method achieves
state-of-the-art performance with fewer computing costs (60% FLOPs of the
two-stage method DNF). Our codes will be released after the acceptance of this
work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qirui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1&quot;&gt;Qihua Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_H/0/1/0/all/0/1&quot;&gt;Huanjing Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Le Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yihao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jingyu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06726">
<title>Compress &amp; Align: Curating Image-Text Data with Human Knowledge. (arXiv:2312.06726v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06726</link>
<description rdf:parseType="Literal">&lt;p&gt;The massive growth of image-text data through web crawling inherently
presents the challenge of variability in data quality. This paper introduces a
novel algorithm, rooted in human knowledge, to compress this vast corpus of
web-crawled image-text datasets to a compact and high-quality form. Our method
unfolds in three major steps. First, we collect an image-text dataset, wherein
each image is associated with multiple captions sourced from diverse origins.
Then, to systemically capture human preferences regarding the best caption
paired with each image, we establish a comprehensive set of both subjective and
objective criteria for critically guiding the alignment assessment from
labelers. Lastly, we train a reward model on the annotated dataset to
internalize the nuanced human understanding of image-text alignment. The
resulting reward model thus can act as a human-like referee to filter
misaligned/low-quality image-text pairs. Extensive experiments demonstrate that
we are able to secure (or even improve) model performance by compressing the
image-text datasets up to ~90%. An impressive example is that, by aggressively
reducing the total training sample from 130M to 15.5M (e.g., ~9x smaller), our
BLIP-B/16 models still consistently show superior performance compared with the
full-size-dataset counterpart on image-text retrieval (Flickr30K, COCO) by
~2.5% in Recall@1, and on image-captioning (Nocaps, COCO) by ~10.0% in CIDEr
and ~2.7% in SPICE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_F/0/1/0/all/0/1&quot;&gt;Fangxun Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Sucheng Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bingchen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Cihang Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06733">
<title>TULIP: Transformer for Upsampling of LiDAR Point Cloud. (arXiv:2312.06733v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06733</link>
<description rdf:parseType="Literal">&lt;p&gt;LiDAR Upsampling is a challenging task for the perception systems of robots
and autonomous vehicles, due to the sparse and irregular structure of
large-scale scene contexts. Recent works propose to solve this problem by
converting LiDAR data from 3D Euclidean space into an image super-resolution
problem in 2D image space. Although their methods can generate high-resolution
range images with fine-grained details, the resulting 3D point clouds often
blur out details and predict invalid points. In this paper, we propose TULIP, a
new method to reconstruct high-resolution LiDAR point clouds from
low-resolution LiDAR input. We also follow a range image-based approach but
specifically modify the patch and window geometries of a Swin-Transformer-based
network to better fit the characteristics of range images. We conducted several
experiments on three different public real-world and simulated datasets. TULIP
outperforms state-of-the-art methods in all relevant metrics and generates
robust and more realistic point clouds than prior works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfreundschuh_P/0/1/0/all/0/1&quot;&gt;Patrick Pfreundschuh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siegwart_R/0/1/0/all/0/1&quot;&gt;Roland Siegwart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1&quot;&gt;Marco Hutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1&quot;&gt;Peyman Moghadam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patil_V/0/1/0/all/0/1&quot;&gt;Vaishakh Patil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06988">
<title>MWSIS: Multimodal Weakly Supervised Instance Segmentation with 2D Box Annotations for Autonomous Driving. (arXiv:2312.06988v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06988</link>
<description rdf:parseType="Literal">&lt;p&gt;Instance segmentation is a fundamental research in computer vision,
especially in autonomous driving. However, manual mask annotation for instance
segmentation is quite time-consuming and costly. To address this problem, some
prior works attempt to apply weakly supervised manner by exploring 2D or 3D
boxes. However, no one has ever successfully segmented 2D and 3D instances
simultaneously by only using 2D box annotations, which could further reduce the
annotation cost by an order of magnitude. Thus, we propose a novel framework
called Multimodal Weakly Supervised Instance Segmentation (MWSIS), which
incorporates various fine-grained label generation and correction modules for
both 2D and 3D modalities to improve the quality of pseudo labels, along with a
new multimodal cross-supervision approach, named Consistency Sparse Cross-modal
Supervision (CSCS), to reduce the inconsistency of multimodal predictions by
response distillation. Particularly, transferring the 3D backbone to downstream
tasks not only improves the performance of the 3D detectors, but also
outperforms fully supervised instance segmentation with only 5% fully
supervised annotations. On the Waymo dataset, the proposed framework
demonstrates significant improvements over the baseline, especially achieving
2.59% mAP and 12.75% mAP increases for 2D and 3D instance segmentation tasks,
respectively. The code is available at
https://github.com/jiangxb98/mwsis-plugin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1&quot;&gt;Guangfeng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuzhi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1&quot;&gt;Wenlong Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1&quot;&gt;Pai Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07062">
<title>ThinkBot: Embodied Instruction Following with Thought Chain Reasoning. (arXiv:2312.07062v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07062</link>
<description rdf:parseType="Literal">&lt;p&gt;Embodied Instruction Following (EIF) requires agents to complete human
instruction by interacting objects in complicated surrounding environments.
Conventional methods directly consider the sparse human instruction to generate
action plans for agents, which usually fail to achieve human goals because of
the instruction incoherence in action descriptions. On the contrary, we propose
ThinkBot that reasons the thought chain in human instruction to recover the
missing action descriptions, so that the agent can successfully complete human
goals by following the coherent instruction. Specifically, we first design an
instruction completer based on large language models to recover the missing
actions with interacted objects between consecutive human instruction, where
the perceived surrounding environments and the completed sub-goals are
considered for instruction completion. Based on the partially observed scene
semantic maps, we present an object localizer to infer the position of
interacted objects for agents to achieve complex human goals. Extensive
experiments in the simulated environment show that our ThinkBot outperforms the
state-of-the-art EIF methods by a sizable margin in both success rate and
execution efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1&quot;&gt;Guanxing Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Changliu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yansong Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07180">
<title>Context-Aware Iteration Policy Network for Efficient Optical Flow Estimation. (arXiv:2312.07180v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07180</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing recurrent optical flow estimation networks are computationally
expensive since they use a fixed large number of iterations to update the flow
field for each sample. An efficient network should skip iterations when the
flow improvement is limited. In this paper, we develop a Context-Aware
Iteration Policy Network for efficient optical flow estimation, which
determines the optimal number of iterations per sample. The policy network
achieves this by learning contextual information to realize whether flow
improvement is bottlenecked or minimal. On the one hand, we use iteration
embedding and historical hidden cell, which include previous iterations
information, to convey how flow has changed from previous iterations. On the
other hand, we use the incremental loss to make the policy network implicitly
perceive the magnitude of optical flow improvement in the subsequent iteration.
Furthermore, the computational complexity in our dynamic network is
controllable, allowing us to satisfy various resource preferences with a single
trained model. Our policy network can be easily integrated into
state-of-the-art optical flow networks. Extensive experiments show that our
method maintains performance while reducing FLOPs by about 40%/20% for the
Sintel/KITTI datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1&quot;&gt;Ri Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ruian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xuhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shili Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1&quot;&gt;Weimin Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Bo Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07353">
<title>CLIP in Medical Imaging: A Comprehensive Survey. (arXiv:2312.07353v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07353</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive Language-Image Pre-training (CLIP), a simple yet effective
pre-training paradigm, successfully introduces text supervision to vision
models. It has shown promising results across various tasks, attributable to
its generalizability and interpretability. The use of CLIP has recently gained
increasing interest in the medical imaging domain, serving both as a
pre-training paradigm for aligning medical vision and language, and as a
critical component in diverse clinical tasks. With the aim of facilitating a
deeper understanding of this promising direction, this survey offers an
in-depth exploration of the CLIP paradigm within the domain of medical imaging,
regarding both refined CLIP pre-training and CLIP-driven applications. In this
study, We (1) start with a brief introduction to the fundamentals of CLIP
methodology. (2) Then, we investigate the adaptation of CLIP pre-training in
the medical domain, focusing on how to optimize CLIP given characteristics of
medical images and reports. (3) Furthermore, we explore the practical
utilization of CLIP pre-trained models in various tasks, including
classification, dense prediction, and cross-modal tasks. (4) Finally, we
discuss existing limitations of CLIP in the context of medical imaging and
propose forward-looking directions to address the demands of medical imaging
domain. We expect that this comprehensive survey will provide researchers in
the field of medical image analysis with a holistic understanding of the CLIP
paradigm and its potential implications. The project page can be found on
https://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zihao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Han Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yonghao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_L/0/1/0/all/0/1&quot;&gt;Lin Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Disheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1&quot;&gt;Zhiming Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dinggang Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07374">
<title>Relax Image-Specific Prompt Requirement in SAM: A Single Generic Prompt for Segmenting Camouflaged Objects. (arXiv:2312.07374v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07374</link>
<description rdf:parseType="Literal">&lt;p&gt;Camouflaged object detection (COD) approaches heavily rely on pixel-level
annotated datasets. Weakly-supervised COD (WSCOD) approaches use sparse
annotations like scribbles or points to reduce annotation effort, but this can
lead to decreased accuracy. The Segment Anything Model (SAM) shows remarkable
segmentation ability with sparse prompts like points. However, manual prompt is
not always feasible, as it may not be accessible in real-world application.
Additionally, it only provides localization information instead of semantic
one, which can intrinsically cause ambiguity in interpreting the targets. In
this work, we aim to eliminate the need for manual prompt. The key idea is to
employ Cross-modal Chains of Thought Prompting (CCTP) to reason visual prompts
using the semantic information given by a generic text prompt. To that end, we
introduce a test-time adaptation per-instance mechanism called Generalizable
SAM (GenSAM) to automatically enerate and optimize visual prompts the generic
task prompt for WSCOD. In particular, CCTP maps a single generic text prompt
onto image-specific consensus foreground and background heatmaps using
vision-language models, acquiring reliable visual prompts. Moreover, to
test-time adapt the visual prompts, we further propose Progressive Mask
Generation (PMG) to iteratively reweight the input image, guiding the model to
focus on the targets in a coarse-to-fine manner. Crucially, all network
parameters are fixed, avoiding the need for additional training. Experiments
demonstrate the superiority of GenSAM. Experiments on three benchmarks
demonstrate that GenSAM outperforms point supervision approaches and achieves
comparable results to scribble supervision ones, solely relying on general task
descriptions as prompts. our codes is in: https://lwpyh.github.io/GenSAM/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jian Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jiayi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Weitong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1&quot;&gt;Shaogang Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07533">
<title>VILA: On Pre-training for Visual Language Models. (arXiv:2312.07533v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07533</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual language models (VLMs) rapidly progressed with the recent success of
large language models. There have been growing efforts on visual instruction
tuning to extend the LLM with visual inputs, but lacks an in-depth study of the
visual language pre-training process, where the model learns to perform joint
modeling on both modalities. In this work, we examine the design options for
VLM pre-training by augmenting LLM towards VLM through step-by-step
controllable comparisons. We introduce three main findings: (1) freezing LLMs
during pre-training can achieve decent zero-shot performance, but lack
in-context learning capability, which requires unfreezing the LLM; (2)
interleaved pre-training data is beneficial whereas image-text pairs alone are
not optimal; (3) re-blending text-only instruction data to image-text data
during instruction fine-tuning not only remedies the degradation of text-only
tasks, but also boosts VLM task accuracy. With an enhanced pre-training recipe
we build VILA, a Visual Language model family that consistently outperforms the
state-of-the-art models, e.g., LLaVA-1.5, across main benchmarks without bells
and whistles. Multi-modal pre-training also helps unveil appealing properties
of VILA, including multi-image reasoning, enhanced in-context learning, and
better world knowledge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Ji Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Hongxu Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1&quot;&gt;Wei Ping&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1&quot;&gt;Pavlo Molchanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_A/0/1/0/all/0/1&quot;&gt;Andrew Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1&quot;&gt;Huizi Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1&quot;&gt;Jan Kautz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1&quot;&gt;Mohammad Shoeybi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Song Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07853">
<title>High-Order Structure Based Middle-Feature Learning for Visible-Infrared Person Re-Identification. (arXiv:2312.07853v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07853</link>
<description rdf:parseType="Literal">&lt;p&gt;Visible-infrared person re-identification (VI-ReID) aims to retrieve images
of the same persons captured by visible (VIS) and infrared (IR) cameras.
Existing VI-ReID methods ignore high-order structure information of features
while being relatively difficult to learn a reasonable common feature space due
to the large modality discrepancy between VIS and IR images. To address the
above problems, we propose a novel high-order structure based middle-feature
learning network (HOS-Net) for effective VI-ReID. Specifically, we first
leverage a short- and long-range feature extraction (SLE) module to effectively
exploit both short-range and long-range features. Then, we propose a high-order
structure learning (HSL) module to successfully model the high-order
relationship across different local features of each person image based on a
whitened hypergraph network.This greatly alleviates model collapse and enhances
feature representations. Finally, we develop a common feature space learning
(CFL) module to learn a discriminative and reasonable common feature space
based on middle features generated by aligning features from different
modalities and ranges. In particular, a modality-range identity-center
contrastive (MRIC) loss is proposed to reduce the distances between the VIS,
IR, and middle features, smoothing the training process. Extensive experiments
on the SYSU-MM01, RegDB, and LLCM datasets show that our HOS-Net achieves
superior state-of-the-art performance. Our code is available at
\url{https://github.com/Jaulaucoeng/HOS-Net}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Liuxiang Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Si Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yan Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1&quot;&gt;Jing-Hao Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Da-Han Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Shunzhi Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07871">
<title>MLNet: Mutual Learning Network with Neighborhood Invariance for Universal Domain Adaptation. (arXiv:2312.07871v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07871</link>
<description rdf:parseType="Literal">&lt;p&gt;Universal domain adaptation (UniDA) is a practical but challenging problem,
in which information about the relation between the source and the target
domains is not given for knowledge transfer. Existing UniDA methods may suffer
from the problems of overlooking intra-domain variations in the target domain
and difficulty in separating between the similar known and unknown class. To
address these issues, we propose a novel Mutual Learning Network (MLNet) with
neighborhood invariance for UniDA. In our method, confidence-guided invariant
feature learning with self-adaptive neighbor selection is designed to reduce
the intra-domain variations for more generalizable feature representation. By
using the cross-domain mixup scheme for better unknown-class identification,
the proposed method compensates for the misidentified known-class errors by
mutual learning between the closed-set and open-set classifiers. Extensive
experiments on three publicly available benchmarks demonstrate that our method
achieves the best results compared to the state-of-the-arts in most cases and
significantly outperforms the baseline across all the four settings in UniDA.
Code is available at https://github.com/YanzuoLu/MLNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yanzuo Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1&quot;&gt;Meng Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1&quot;&gt;Andy J Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohua Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1&quot;&gt;Jian-Huang Lai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07976">
<title>Challenges of YOLO Series for Object Detection in Extremely Heavy Rain: CALRA Simulator based Synthetic Evaluation Dataset. (arXiv:2312.07976v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07976</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, as many studies of autonomous vehicles have been achieved for
levels 4 and 5, there has been also increasing interest in the advancement of
perception, decision, and control technologies, which are the three major
aspects of autonomous vehicles. As for the perception technologies achieving
reliable maneuvering of autonomous vehicles, object detection by using diverse
sensors (e.g., LiDAR, radar, and camera) should be prioritized. These sensors
require to detect objects accurately and quickly in diverse weather conditions,
but they tend to have challenges to consistently detect objects in bad weather
conditions with rain, snow, or fog. Thus, in this study, based on the
experimentally obtained raindrop data from precipitation conditions, we
constructed a novel dataset that could test diverse network model in various
precipitation conditions through the CARLA simulator. Consequently, based on
our novel dataset, YOLO series, a one-stage-detector, was used to
quantitatively verify how much object detection performance could be decreased
under various precipitation conditions from normal to extreme heavy rain
situations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;T. Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1&quot;&gt;H. Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_Y/0/1/0/all/0/1&quot;&gt;Y. Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08009">
<title>Semi-Supervised Class-Agnostic Motion Prediction with Pseudo Label Regeneration and BEVMix. (arXiv:2312.08009v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08009</link>
<description rdf:parseType="Literal">&lt;p&gt;Class-agnostic motion prediction methods aim to comprehend motion within
open-world scenarios, holding significance for autonomous driving systems.
However, training a high-performance model in a fully-supervised manner always
requires substantial amounts of manually annotated data, which can be both
expensive and time-consuming to obtain. To address this challenge, our study
explores the potential of semi-supervised learning (SSL) for class-agnostic
motion prediction. Our SSL framework adopts a consistency-based self-training
paradigm, enabling the model to learn from unlabeled data by generating pseudo
labels through test-time inference. To improve the quality of pseudo labels, we
propose a novel motion selection and re-generation module. This module
effectively selects reliable pseudo labels and re-generates unreliable ones.
Furthermore, we propose two data augmentation strategies: temporal sampling and
BEVMix. These strategies facilitate consistency regularization in SSL.
Experiments conducted on nuScenes demonstrate that our SSL method can surpass
the self-supervised approach by a large margin by utilizing only a tiny
fraction of labeled data. Furthermore, our method exhibits comparable
performance to weakly and some fully supervised methods. These results
highlight the ability of our method to strike a favorable balance between
annotation costs and performance. Code will be available at
https://github.com/kwwcv/SSMP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kewei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yizheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zhiyu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xingyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xian_K/0/1/0/all/0/1&quot;&gt;Ke Xian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhiguo Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guosheng Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08048">
<title>Compositional Inversion for Stable Diffusion Models. (arXiv:2312.08048v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08048</link>
<description rdf:parseType="Literal">&lt;p&gt;Inversion methods, such as Textual Inversion, generate personalized images by
incorporating concepts of interest provided by user images. However, existing
methods often suffer from overfitting issues, where the dominant presence of
inverted concepts leads to the absence of other desired concepts. It stems from
the fact that during inversion, the irrelevant semantics in the user images are
also encoded, forcing the inverted concepts to occupy locations far from the
core distribution in the embedding space. To address this issue, we propose a
method that guides the inversion process towards the core distribution for
compositional embeddings. Additionally, we introduce a spatial regularization
approach to balance the attention on the concepts being composed. Our method is
designed as a post-training approach and can be seamlessly integrated with
other inversion methods. Experimental results demonstrate the effectiveness of
our proposed approach in mitigating the overfitting problem and generating more
diverse and balanced compositions of concepts in the synthesized images. The
source code is available at
https://github.com/zhangxulu1996/Compositional-Inversion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xu-Lu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiao-Yong Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jin-Lin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tian-Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhaoxiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1&quot;&gt;Zhen Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qing Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08078">
<title>Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation. (arXiv:2312.08078v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08078</link>
<description rdf:parseType="Literal">&lt;p&gt;To address these issues, we propose a novel Adaptive patch-word Matching
(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in
medical reports and apply it to CXR-report generation to provide explainability
for the generation process. AdaMatch exploits the fine-grained relation between
adaptive patches and words to provide explanations of specific image regions
with corresponding words. To capture the abnormal regions of varying sizes and
positions, we introduce the Adaptive Patch extraction (AdaPatch) module to
acquire the adaptive patches for these regions adaptively. In order to provide
explicit explainability for CXR-report generation task, we propose an
AdaMatch-based bidirectional large language model for Cyclic CXR-report
generation (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords
for CXR images and `keypatches&apos; for medical reports as hints to guide
CXR-report generation. Extensive experiments on two publicly available CXR
datasets prove the effectiveness of our method and its superior performance to
existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Linlin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yixuan Yuan&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>