<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-28T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16174" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16180" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16184" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16223" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16230" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16248" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16257" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16264" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16336" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16338" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16351" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16392" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16418" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16423" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16427" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16429" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16438" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16450" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16476" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16490" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16497" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16511" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16554" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16563" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16600" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16626" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16659" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16707" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16715" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16760" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1907.05861" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.15677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.14370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.09141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.08303" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.08843" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.02288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.07629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.02862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.14582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.00140" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03236" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08777" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09480" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03502" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04047" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04607" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13165" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16082" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14165" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15645" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10062" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15202" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02594" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12990" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13326" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17431" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00201" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01697" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02366" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08078" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08935" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10572" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10967" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11285" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13156" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14183" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14852" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15692" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15840" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.03447" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14654" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10482" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.16174">
<title>Industrial Internet of Things Intelligence Empowering Smart Manufacturing: A Literature Review. (arXiv:2312.16174v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.16174</link>
<description rdf:parseType="Literal">&lt;p&gt;The fiercely competitive business environment and increasingly personalized
customization needs are driving the digital transformation and upgrading of the
manufacturing industry. IIoT intelligence, which can provide innovative and
efficient solutions for various aspects of the manufacturing value chain,
illuminates the path of transformation for the manufacturing industry. It is
time to provide a systematic vision of IIoT intelligence. However, existing
surveys often focus on specific areas of IIoT intelligence, leading researchers
and readers to have biases in their understanding of IIoT intelligence, that
is, believing that research in one direction is the most important for the
development of IIoT intelligence, while ignoring contributions from other
directions. Therefore, this paper provides a comprehensive overview of IIoT
intelligence. We first conduct an in-depth analysis of the inevitability of
manufacturing transformation and study the successful experiences from the
practices of Chinese enterprises. Then we give our definition of IIoT
intelligence and demonstrate the value of IIoT intelligence for industries in
fucntions, operations, deployments, and application. Afterwards, we propose a
hierarchical development architecture for IIoT intelligence, which consists of
five layers. The practical values of technical upgrades at each layer are
illustrated by a close look on lighthouse factories. Following that, we
identify seven kinds of technologies that accelerate the transformation of
manufacturing, and clarify their contributions. Finally, we explore the open
challenges and development trends from four aspects to inspire future
researches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yujiao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1&quot;&gt;Qingmin Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuao Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Mengjie Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chenyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiaomao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1&quot;&gt;Renchao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;F. Richard Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16180">
<title>Investigating salient representations and label Variance in Dimensional Speech Emotion Analysis. (arXiv:2312.16180v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2312.16180</link>
<description rdf:parseType="Literal">&lt;p&gt;Representations derived from models such as BERT (Bidirectional Encoder
Representations from Transformers) and HuBERT (Hidden units BERT), have helped
to achieve state-of-the-art performance in dimensional speech emotion
recognition. Despite their large dimensionality, and even though these
representations are not tailored for emotion recognition tasks, they are
frequently used to train large speech emotion models with high memory and
computational costs. In this work, we show that there exist lower-dimensional
subspaces within the these pre-trained representational spaces that offer a
reduction in downstream model complexity without sacrificing performance on
emotion estimation. In addition, we model label uncertainty in the form of
grader opinion variance, and demonstrate that such information can improve the
models generalization capacity and robustness. Finally, we compare the
robustness of the emotion models against acoustic degradations and observed
that the reduced dimensional representations were able to retain the
performance similar to the full-dimensional representations without significant
regression in dimensional emotion performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_V/0/1/0/all/0/1&quot;&gt;Vikramjit Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1&quot;&gt;Jingping Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azemi_E/0/1/0/all/0/1&quot;&gt;Erdrin Azemi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16184">
<title>Dynamic Knowledge Injection for AIXI Agents. (arXiv:2312.16184v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.16184</link>
<description rdf:parseType="Literal">&lt;p&gt;Prior approximations of AIXI, a Bayesian optimality notion for general
reinforcement learning, can only approximate AIXI&apos;s Bayesian environment model
using an a-priori defined set of models. This is a fundamental source of
epistemic uncertainty for the agent in settings where the existence of
systematic bias in the predefined model class cannot be resolved by simply
collecting more data from the environment. We address this issue in the context
of Human-AI teaming by considering a setup where additional knowledge for the
agent in the form of new candidate models arrives from a human operator in an
online fashion. We introduce a new agent called DynamicHedgeAIXI that maintains
an exact Bayesian mixture over dynamically changing sets of models via a
time-adaptive prior constructed from a variant of the Hedge algorithm. The
DynamicHedgeAIXI agent is the richest direct approximation of AIXI known to
date and comes with good performance guarantees. Experimental results on
epidemic control on contact networks validates the agent&apos;s practical utility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Zhao_S/0/1/0/all/0/1&quot;&gt;Samuel Yang-Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_K/0/1/0/all/0/1&quot;&gt;Kee Siong Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1&quot;&gt;Marcus Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16189">
<title>OpenRL: A Unified Reinforcement Learning Framework. (arXiv:2312.16189v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16189</link>
<description rdf:parseType="Literal">&lt;p&gt;We present OpenRL, an advanced reinforcement learning (RL) framework designed
to accommodate a diverse array of tasks, from single-agent challenges to
complex multi-agent systems. OpenRL&apos;s robust support for self-play training
empowers agents to develop advanced strategies in competitive settings.
Notably, OpenRL integrates Natural Language Processing (NLP) with RL, enabling
researchers to address a combination of RL training and language-centric tasks
effectively. Leveraging PyTorch&apos;s robust capabilities, OpenRL exemplifies
modularity and a user-centric approach. It offers a universal interface that
simplifies the user experience for beginners while maintaining the flexibility
experts require for innovation and algorithm development. This equilibrium
enhances the framework&apos;s practicality, adaptability, and scalability,
establishing a new standard in RL research. To delve into OpenRL&apos;s features, we
invite researchers and enthusiasts to explore our GitHub repository at
https://github.com/OpenRL-Lab/openrl and access our comprehensive documentation
at https://openrl-docs.readthedocs.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shiyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wentse Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yiwen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bie_F/0/1/0/all/0/1&quot;&gt;Fuqing Bie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1&quot;&gt;Wei-Wei Tu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16191">
<title>SoK: Taming the Triangle -- On the Interplays between Fairness, Interpretability and Privacy in Machine Learning. (arXiv:2312.16191v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16191</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning techniques are increasingly used for high-stakes
decision-making, such as college admissions, loan attribution or recidivism
prediction. Thus, it is crucial to ensure that the models learnt can be audited
or understood by human users, do not create or reproduce discrimination or
bias, and do not leak sensitive information regarding their training data.
Indeed, interpretability, fairness and privacy are key requirements for the
development of responsible machine learning, and all three have been studied
extensively during the last decade. However, they were mainly considered in
isolation, while in practice they interplay with each other, either positively
or negatively. In this Systematization of Knowledge (SoK) paper, we survey the
literature on the interactions between these three desiderata. More precisely,
for each pairwise interaction, we summarize the identified synergies and
tensions. These findings highlight several fundamental theoretical and
empirical conflicts, while also demonstrating that jointly considering these
different requirements is challenging when one aims at preserving a high level
of utility. To solve this issue, we also discuss possible conciliation
mechanisms, showing that a careful design can enable to successfully handle
these different concerns in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferry_J/0/1/0/all/0/1&quot;&gt;Julien Ferry&lt;/a&gt; (LAAS-ROC), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aivodji_U/0/1/0/all/0/1&quot;&gt;Ulrich A&amp;#xef;vodji&lt;/a&gt; (ETS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gambs_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Gambs&lt;/a&gt; (UQAM), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huguet_M/0/1/0/all/0/1&quot;&gt;Marie-Jos&amp;#xe9; Huguet&lt;/a&gt; (LAAS-ROC), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siala_M/0/1/0/all/0/1&quot;&gt;Mohamed Siala&lt;/a&gt; (LAAS-ROC)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16211">
<title>An Explainable AI Approach to Large Language Model Assisted Causal Model Auditing and Development. (arXiv:2312.16211v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.16211</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal networks are widely used in many fields, including epidemiology,
social science, medicine, and engineering, to model the complex relationships
between variables. While it can be convenient to algorithmically infer these
models directly from observational data, the resulting networks are often
plagued with erroneous edges. Auditing and correcting these networks may
require domain expertise frequently unavailable to the analyst. We propose the
use of large language models such as ChatGPT as an auditor for causal networks.
Our method presents ChatGPT with a causal network, one edge at a time, to
produce insights about edge directionality, possible confounders, and mediating
variables. We ask ChatGPT to reflect on various aspects of each causal link and
we then produce visualizations that summarize these viewpoints for the human
analyst to direct the edge, gather more data, or test further hypotheses. We
envision a system where large language models, automated causal inference, and
the human analyst and domain expert work hand in hand as a team to derive
holistic and comprehensive causal models for any given case scenario. This
paper presents first results obtained with an emerging prototype.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fitzgibbon_B/0/1/0/all/0/1&quot;&gt;Brette Fitzgibbon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garofolo_D/0/1/0/all/0/1&quot;&gt;Dino Garofolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kota_A/0/1/0/all/0/1&quot;&gt;Akshith Kota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papenhausen_E/0/1/0/all/0/1&quot;&gt;Eric Papenhausen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mueller_K/0/1/0/all/0/1&quot;&gt;Klaus Mueller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16223">
<title>Increasing Profitability and Confidence by using Interpretable Model for Investment Decisions. (arXiv:2312.16223v1 [q-fin.ST])</title>
<link>http://arxiv.org/abs/2312.16223</link>
<description rdf:parseType="Literal">&lt;p&gt;Financial forecasting plays an important role in making informed decisions
for financial stakeholders, specifically in the stock exchange market. In a
traditional setting, investors commonly rely on the equity research department
for valuable reports on market insights and investment recommendations. The
equity research department, however, faces challenges in effectuating
decision-making due to the demanding cognitive effort required for analyzing
the inherently volatile nature of market dynamics. Furthermore, financial
forecasting systems employed by analysts pose potential risks in terms of
interpretability and gaining the trust of all stakeholders. This paper presents
an interpretable decision-making model leveraging the SHAP-based explainability
technique to forecast investment recommendations. The proposed solution not
only provides valuable insights into the factors that influence forecasted
recommendations but also caters to investors of varying types, including those
interested in daily and short-term investment opportunities. To ascertain the
efficacy of the proposed model, a case study is devised that demonstrates a
notable enhancement in investor&apos;s portfolio value, employing our trading
strategies. The results highlight the significance of incorporating
interpretability in forecasting models to boost stakeholders&apos; confidence and
foster transparency in the stock exchange domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Arshad_S/0/1/0/all/0/1&quot;&gt;Sahar Arshad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Latif_S/0/1/0/all/0/1&quot;&gt;Seemab Latif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Salman_A/0/1/0/all/0/1&quot;&gt;Ahmad Salman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Irfan_S/0/1/0/all/0/1&quot;&gt;Saadia Irfan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16230">
<title>Navigating Decision Landscapes: The Impact of Principals on Decision-Making Dynamics. (arXiv:2312.16230v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.16230</link>
<description rdf:parseType="Literal">&lt;p&gt;We explored decision-making dynamics in social systems, referencing the &apos;herd
behavior&apos; from prior studies where individuals follow preceding choices without
understanding the underlying reasons. While previous research highlighted a
preference for the optimal choice without external influences, our study
introduced principals or external guides, adding complexity to the
decision-making process. The reliability of these principals significantly
influenced decisions. Notably, even occasional trust in an unreliable principal
could alter decision outcomes. Furthermore, when a principal&apos;s advice was
purely random, heightened trust led to more decision errors. Our findings
emphasize the need for caution when placing trust in decision-making contexts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huangxing Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16240">
<title>Merging Vision Transformers from Different Tasks and Domains. (arXiv:2312.16240v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.16240</link>
<description rdf:parseType="Literal">&lt;p&gt;This work targets to merge various Vision Transformers (ViTs) trained on
different tasks (i.e., datasets with different object categories) or domains
(i.e., datasets with the same categories but different environments) into one
unified model, yielding still good performance on each task or domain. Previous
model merging works focus on either CNNs or NLP models, leaving the ViTs
merging research untouched. To fill this gap, we first explore and find that
existing model merging methods cannot well handle the merging of the whole ViT
models and still have improvement space. To enable the merging of the whole
ViT, we propose a simple-but-effective gating network that can both merge all
kinds of layers (e.g., Embedding, Norm, Attention, and MLP) and select the
suitable classifier. Specifically, the gating network is trained by unlabeled
datasets from all the tasks (domains), and predicts the probability of which
task (domain) the input belongs to for merging the models during inference. To
further boost the performance of the merged model, especially when the
difficulty of merging tasks increases, we design a novel metric of model weight
similarity, and utilize it to realize controllable and combined weight merging.
Comprehensive experiments on kinds of newly established benchmarks, validate
the superiority of the proposed ViT merging framework for different tasks and
domains. Our method can even merge beyond 10 ViT models from different vision
tasks with a negligible effect on the performance of each task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1&quot;&gt;Peng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chenyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1&quot;&gt;Mingzhu Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yongqi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16248">
<title>XuanCe: A Comprehensive and Unified Deep Reinforcement Learning Library. (arXiv:2312.16248v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16248</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present XuanCe, a comprehensive and unified deep
reinforcement learning (DRL) library designed to be compatible with PyTorch,
TensorFlow, and MindSpore. XuanCe offers a wide range of functionalities,
including over 40 classical DRL and multi-agent DRL algorithms, with the
flexibility to easily incorporate new algorithms and environments. It is a
versatile DRL library that supports CPU, GPU, and Ascend, and can be executed
on various operating systems such as Ubuntu, Windows, MacOS, and EulerOS.
Extensive benchmarks conducted on popular environments including MuJoCo, Atari,
and StarCraftII multi-agent challenge demonstrate the library&apos;s impressive
performance. XuanCe is open-source and can be accessed at
https://github.com/agi-brain/xuance.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenzhang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Wenzhe Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1&quot;&gt;Kun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1&quot;&gt;Guangran Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuanda Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiawei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jingyu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lele Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_C/0/1/0/all/0/1&quot;&gt;Chaoxu Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Changyin Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16251">
<title>MetaScript: Few-Shot Handwritten Chinese Content Generation via Generative Adversarial Networks. (arXiv:2312.16251v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.16251</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose MetaScript, a novel Chinese content generation
system designed to address the diminishing presence of personal handwriting
styles in the digital representation of Chinese characters. Our approach
harnesses the power of few-shot learning to generate Chinese characters that
not only retain the individual&apos;s unique handwriting style but also maintain the
efficiency of digital typing. Trained on a diverse dataset of handwritten
styles, MetaScript is adept at producing high-quality stylistic imitations from
minimal style references and standard fonts. Our work demonstrates a practical
solution to the challenges of digital typography in preserving the personal
touch in written communication, particularly in the context of Chinese script.
Notably, our system has demonstrated superior performance in various
evaluations, including recognition accuracy, inception score, and Frechet
inception distance. At the same time, the training conditions of our model are
easy to meet and facilitate generalization to real applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1&quot;&gt;Xiangyuan Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kailing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1&quot;&gt;Jiazi Bu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qirui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16256">
<title>DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision. (arXiv:2312.16256v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.16256</link>
<description rdf:parseType="Literal">&lt;p&gt;We have witnessed significant progress in deep learning-based 3D vision,
ranging from neural radiance field (NeRF) based 3D representation learning to
applications in novel view synthesis (NVS). However, existing scene-level
datasets for deep learning-based 3D vision, limited to either synthetic
environments or a narrow selection of real-world scenes, are quite
insufficient. This insufficiency not only hinders a comprehensive benchmark of
existing methods but also caps what could be explored in deep learning-based 3D
analysis. To address this critical gap, we present DL3DV-10K, a large-scale
scene dataset, featuring 51.2 million frames from 10,510 videos captured from
65 types of point-of-interest (POI) locations, covering both bounded and
unbounded scenes, with different levels of reflection, transparency, and
lighting. We conducted a comprehensive benchmark of recent NVS methods on
DL3DV-10K, which revealed valuable insights for future research in NVS. In
addition, we have obtained encouraging results in a pilot study to learn
generalizable NeRF from DL3DV-10K, which manifests the necessity of a
large-scale scene-level dataset to forge a path toward a foundation model for
learning 3D representation. Our DL3DV-10K dataset, benchmark results, and
models will be publicly accessible at https://dl3dv-10k.github.io/DL3DV-10K/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_L/0/1/0/all/0/1&quot;&gt;Lu Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1&quot;&gt;Yichen Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhi Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wentian Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_C/0/1/0/all/0/1&quot;&gt;Cheng Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_K/0/1/0/all/0/1&quot;&gt;Kun Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lantao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qianyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zixun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yawen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuanmao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xingpeng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashok_R/0/1/0/all/0/1&quot;&gt;Rohan Ashok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1&quot;&gt;Aniruddha Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1&quot;&gt;Hao Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1&quot;&gt;Xiangrui Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1&quot;&gt;Gang Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianti Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benes_B/0/1/0/all/0/1&quot;&gt;Bedrich Benes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1&quot;&gt;Aniket Bera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16257">
<title>More than Correlation: Do Large Language Models Learn Causal Representations of Space?. (arXiv:2312.16257v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.16257</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work found high mutual information between the learned representations
of large language models (LLMs) and the geospatial property of its input,
hinting an emergent internal model of space. However, whether this internal
space model has any causal effects on the LLMs&apos; behaviors was not answered by
that work, led to criticism of these findings as mere statistical correlation.
Our study focused on uncovering the causality of the spatial representations in
LLMs. In particular, we discovered the potential spatial representations in
DeBERTa, GPT-Neo using representational similarity analysis and linear and
non-linear probing. Our casual intervention experiments showed that the spatial
representations influenced the model&apos;s performance on next word prediction and
a downstream task that relies on geospatial information. Our experiments
suggested that the LLMs learn and use an internal model of space in solving
geospatial related tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yida Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1&quot;&gt;Yixian Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sijia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1&quot;&gt;Li Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiaohan Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16262">
<title>Dynamic In-Context Learning from Nearest Neighbors for Bundle Generation. (arXiv:2312.16262v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2312.16262</link>
<description rdf:parseType="Literal">&lt;p&gt;Product bundling has evolved into a crucial marketing strategy in e-commerce.
However, current studies are limited to generating (1) fixed-size or single
bundles, and most importantly, (2) bundles that do not reflect consistent user
intents, thus being less intelligible or useful to users. This paper explores
two interrelated tasks, i.e., personalized bundle generation and the underlying
intent inference based on users&apos; interactions in a session, leveraging the
logical reasoning capability of large language models. We introduce a dynamic
in-context learning paradigm, which enables ChatGPT to seek tailored and
dynamic lessons from closely related sessions as demonstrations while
performing tasks in the target session. Specifically, it first harnesses
retrieval augmented generation to identify nearest neighbor sessions for each
target session. Then, proper prompts are designed to guide ChatGPT to perform
the two tasks on neighbor sessions. To enhance reliability and mitigate the
hallucination issue, we develop (1) a self-correction strategy to foster mutual
improvement in both tasks without supervision signals; and (2) an auto-feedback
mechanism to recurrently offer dynamic supervision based on the distinct
mistakes made by ChatGPT on various neighbor sessions. Thus, the target session
can receive customized and dynamic lessons for improved performance by
observing the demonstrations of its neighbor sessions. Finally, experimental
results on three real-world datasets verify the effectiveness of our methods on
both tasks. Additionally, the inferred intents can prove beneficial for other
intriguing downstream tasks, such as crafting appealing bundle names.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1&quot;&gt;Kaidong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xinghua Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1&quot;&gt;Hui Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1&quot;&gt;Yew-Soon Ong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenyuan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16264">
<title>SPnet: Estimating Garment Sewing Patterns from a Single Image. (arXiv:2312.16264v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.16264</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel method for reconstructing 3D garment models from
a single image of a posed user. Previous studies that have primarily focused on
accurately reconstructing garment geometries to match the input garment image
may often result in unnatural-looking garments when deformed for new poses. To
overcome this limitation, our approach takes a different approach by inferring
the fundamental shape of the garment through sewing patterns from a single
image, rather than directly reconstructing 3D garments. Our method consists of
two stages. Firstly, given a single image of a posed user, it predicts the
garment image worn on a T-pose, representing the baseline form of the garment.
Then, it estimates the sewing pattern parameters based on the T-pose garment
image. By simulating the stitching and draping of the sewing pattern using
physics simulation, we can generate 3D garments that can adaptively deform to
arbitrary poses. The effectiveness of our method is validated through ablation
studies on the major components and a comparison with other approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Seungchan Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sumin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sung-Hee Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16331">
<title>Early and Accurate Detection of Tomato Leaf Diseases Using TomFormer. (arXiv:2312.16331v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.16331</link>
<description rdf:parseType="Literal">&lt;p&gt;Tomato leaf diseases pose a significant challenge for tomato farmers,
resulting in substantial reductions in crop productivity. The timely and
precise identification of tomato leaf diseases is crucial for successfully
implementing disease management strategies. This paper introduces a
transformer-based model called TomFormer for the purpose of tomato leaf disease
detection. The paper&apos;s primary contributions include the following: Firstly, we
present a novel approach for detecting tomato leaf diseases by employing a
fusion model that combines a visual transformer and a convolutional neural
network. Secondly, we aim to apply our proposed methodology to the Hello
Stretch robot to achieve real-time diagnosis of tomato leaf diseases. Thirdly,
we assessed our method by comparing it to models like YOLOS, DETR, ViT, and
Swin, demonstrating its ability to achieve state-of-the-art outcomes. For the
purpose of the experiment, we used three datasets of tomato leaf diseases,
namely KUTomaDATA, PlantDoc, and PlanVillage, where KUTomaDATA is being
collected from a greenhouse in Abu Dhabi, UAE. Finally, we present a
comprehensive analysis of the performance of our model and thoroughly discuss
the limitations inherent in our approach. TomFormer performed well on the
KUTomaDATA, PlantDoc, and PlantVillage datasets, with mean average accuracy
(mAP) scores of 87%, 81%, and 83%, respectively. The comparative results in
terms of mAP demonstrate that our method exhibits robustness, accuracy,
efficiency, and scalability. Furthermore, it can be readily adapted to new
datasets. We are confident that our work holds the potential to significantly
influence the tomato industry by effectively mitigating crop losses and
enhancing crop yields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Asim Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nawaz_U/0/1/0/all/0/1&quot;&gt;Umair Nawaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kshetrimayum_L/0/1/0/all/0/1&quot;&gt;Lochan Kshetrimayum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Seneviratne_L/0/1/0/all/0/1&quot;&gt;Lakmal Seneviratne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hussain_I/0/1/0/all/0/1&quot;&gt;Irfan Hussain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16336">
<title>Learning temporal formulas from examples is hard. (arXiv:2312.16336v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16336</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of learning linear temporal logic (LTL) formulas from
examples, as a first step towards expressing a property separating positive and
negative instances in a way that is comprehensible for humans. In this paper we
initiate the study of the computational complexity of the problem. Our main
results are hardness results: we show that the LTL learning problem is
NP-complete, both for the full logic and for almost all of its fragments. This
motivates the search for efficient heuristics, and highlights the complexity of
expressing separating properties in concise natural language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mascle_C/0/1/0/all/0/1&quot;&gt;Corto Mascle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fijalkow_N/0/1/0/all/0/1&quot;&gt;Nathana&amp;#xeb;l Fijalkow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lagarde_G/0/1/0/all/0/1&quot;&gt;Guillaume Lagarde&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16338">
<title>State-of-the-Art in Nudity Classification: A Comparative Analysis. (arXiv:2312.16338v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.16338</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a comparative analysis of existing nudity classification
techniques for classifying images based on the presence of nudity, with a focus
on their application in content moderation. The evaluation focuses on CNN-based
models, vision transformer, and popular open-source safety checkers from Stable
Diffusion and Large-scale Artificial Intelligence Open Network (LAION). The
study identifies the limitations of current evaluation datasets and highlights
the need for more diverse and challenging datasets. The paper discusses the
potential implications of these findings for developing more accurate and
effective image classification systems on online platforms. Overall, the study
emphasizes the importance of continually improving image classification models
to ensure the safety and well-being of platform users. The project page,
including the demonstrations and results is publicly available at
https://github.com/fcakyon/content-moderation-deep-learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akyon_F/0/1/0/all/0/1&quot;&gt;Fatih Cagatay Akyon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Temizel_A/0/1/0/all/0/1&quot;&gt;Alptekin Temizel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16351">
<title>LLMs with User-defined Prompts as Generic Data Operators for Reliable Data Processing. (arXiv:2312.16351v1 [cs.DB])</title>
<link>http://arxiv.org/abs/2312.16351</link>
<description rdf:parseType="Literal">&lt;p&gt;Data processing is one of the fundamental steps in machine learning pipelines
to ensure data quality. Majority of the applications consider the user-defined
function (UDF) design pattern for data processing in databases. Although the
UDF design pattern introduces flexibility, reusability and scalability, the
increasing demand on machine learning pipelines brings three new challenges to
this design pattern -- not low-code, not dependency-free and not
knowledge-aware. To address these challenges, we propose a new design pattern
that large language models (LLMs) could work as a generic data operator
(LLM-GDO) for reliable data cleansing, transformation and modeling with their
human-compatible performance. In the LLM-GDO design pattern, user-defined
prompts (UDPs) are used to represent the data processing logic rather than
implementations with a specific programming language. LLMs can be centrally
maintained so users don&apos;t have to manage the dependencies at the run-time.
Fine-tuning LLMs with domain-specific data could enhance the performance on the
domain-specific tasks which makes data processing knowledge-aware. We
illustrate these advantages with examples in different data processing tasks.
Furthermore, we summarize the challenges and opportunities introduced by LLMs
to provide a complete view of this design pattern for more discussions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Luyi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakurdesai_N/0/1/0/all/0/1&quot;&gt;Nikhil Thakurdesai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jianpeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korpeoglu_E/0/1/0/all/0/1&quot;&gt;Evren Korpeoglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sushant Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achan_K/0/1/0/all/0/1&quot;&gt;Kannan Achan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16364">
<title>Robustness Verification for Knowledge-Based Logic of Risky Driving Scenes. (arXiv:2312.16364v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.16364</link>
<description rdf:parseType="Literal">&lt;p&gt;Many decision-making scenarios in modern life benefit from the decision
support of artificial intelligence algorithms, which focus on a data-driven
philosophy and automated programs or systems. However, crucial decision issues
related to security, fairness, and privacy should consider more human knowledge
and principles to supervise such AI algorithms to reach more proper solutions
and to benefit society more effectively. In this work, we extract
knowledge-based logic that defines risky driving formats learned from public
transportation accident datasets, which haven&apos;t been analyzed in detail to the
best of our knowledge. More importantly, this knowledge is critical for
recognizing traffic hazards and could supervise and improve AI models in
safety-critical systems. Then we use automated verification methods to verify
the robustness of such logic. More specifically, we gather 72 accident datasets
from Data.gov and organize them by state. Further, we train Decision Tree and
XGBoost models on each state&apos;s dataset, deriving accident judgment logic.
Finally, we deploy robustness verification on these tree-based models under
multiple parameter combinations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_A/0/1/0/all/0/1&quot;&gt;Anda Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sprinkle_J/0/1/0/all/0/1&quot;&gt;Jonathan Sprinkle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_T/0/1/0/all/0/1&quot;&gt;Taylor T. Johnson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16365">
<title>Active Third-Person Imitation Learning. (arXiv:2312.16365v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16365</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of third-person imitation learning with the
additional challenge that the learner must select the perspective from which
they observe the expert. In our setting, each perspective provides only limited
information about the expert&apos;s behavior, and the learning agent must carefully
select and combine information from different perspectives to achieve
competitive performance. This setting is inspired by real-world imitation
learning applications, e.g., in robotics, a robot might observe a human
demonstrator via camera and receive information from different perspectives
depending on the camera&apos;s position. We formalize the aforementioned active
third-person imitation learning problem, theoretically analyze its
characteristics, and propose a generative adversarial network-based active
learning approach. Empirically, we demstrate that our proposed approach can
effectively learn from expert demonstrations and explore the importance of
different architectural choices for the learner&apos;s performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_T/0/1/0/all/0/1&quot;&gt;Timo Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinberger_S/0/1/0/all/0/1&quot;&gt;Susanna Weinberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1&quot;&gt;Adish Singla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tschiatschek_S/0/1/0/all/0/1&quot;&gt;Sebastian Tschiatschek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16374">
<title>LLM Polygraph: Uncovering LLMs&apos; Factual Discernment through Intermediate Data Analysis. (arXiv:2312.16374v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.16374</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have revolutionized various domains with
extensive knowledge and creative capabilities. However, a critical issue with
LLMs is their tendency to produce outputs that diverge from factual reality.
This phenomenon is particularly concerning in sensitive applications such as
medical consultation and legal advice, where accuracy is paramount. In this
paper, we introduce the LLM factoscope, a novel Siamese network-based model
that leverages the inner states of LLMs for factual detection. Our
investigation reveals distinguishable patterns in LLMs&apos; inner states when
generating factual versus non-factual content. We demonstrate the LLM
factoscope&apos;s effectiveness across various architectures, achieving over 96%
accuracy in factual detection. Our work opens a new avenue for utilizing LLMs&apos;
inner states for factual detection and encourages further exploration into
LLMs&apos; inner workings for enhanced reliability and transparency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jinwen He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yujia Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zijin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1&quot;&gt;Chengan Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yue Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16378">
<title>Automating Knowledge Acquisition for Content-Centric Cognitive Agents Using LLMs. (arXiv:2312.16378v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.16378</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper describes a system that uses large language model (LLM) technology
to support the automatic learning of new entries in an intelligent agent&apos;s
semantic lexicon. The process is bootstrapped by an existing non-toy lexicon
and a natural language generator that converts formal, ontologically-grounded
representations of meaning into natural language sentences. The learning method
involves a sequence of LLM requests and includes an automatic quality control
step. To date, this learning method has been applied to learning multiword
expressions whose meanings are equivalent to those of transitive verbs in the
agent&apos;s lexicon. The experiment demonstrates the benefits of a hybrid learning
architecture that integrates knowledge-based methods and resources with both
traditional data analytics and LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oruganti_S/0/1/0/all/0/1&quot;&gt;Sanjay Oruganti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nirenburg_S/0/1/0/all/0/1&quot;&gt;Sergei Nirenburg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+English_J/0/1/0/all/0/1&quot;&gt;Jesse English&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McShane_M/0/1/0/all/0/1&quot;&gt;Marjorie McShane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16383">
<title>Frame-level emotional state alignment method for speech emotion recognition. (arXiv:2312.16383v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2312.16383</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech emotion recognition (SER) systems aim to recognize human emotional
state during human-computer interaction. Most existing SER systems are trained
based on utterance-level labels. However, not all frames in an audio have
affective states consistent with utterance-level label, which makes it
difficult for the model to distinguish the true emotion of the audio and
perform poorly. To address this problem, we propose a frame-level emotional
state alignment method for SER. First, we fine-tune HuBERT model to obtain a
SER system with task-adaptive pretraining (TAPT) method, and extract embeddings
from its transformer layers to form frame-level pseudo-emotion labels with
clustering. Then, the pseudo labels are used to pretrain HuBERT. Hence, the
each frame output of HuBERT has corresponding emotional information. Finally,
we fine-tune the above pretrained HuBERT for SER by adding an attention layer
on the top of it, which can focus only on those frames that are emotionally
more consistent with utterance-level label. The experimental results performed
on IEMOCAP indicate that our proposed method performs better than
state-of-the-art (SOTA) methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qifei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yingming Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yayue Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1&quot;&gt;Jinlong Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yichen Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Ya Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16388">
<title>Gaussian Mixture Proposals with Pull-Push Learning Scheme to Capture Diverse Events for Weakly Supervised Temporal Video Grounding. (arXiv:2312.16388v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.16388</link>
<description rdf:parseType="Literal">&lt;p&gt;In the weakly supervised temporal video grounding study, previous methods use
predetermined single Gaussian proposals which lack the ability to express
diverse events described by the sentence query. To enhance the expression
ability of a proposal, we propose a Gaussian mixture proposal (GMP) that can
depict arbitrary shapes by learning importance, centroid, and range of every
Gaussian in the mixture. In learning GMP, each Gaussian is not trained in a
feature space but is implemented over a temporal location. Thus the
conventional feature-based learning for Gaussian mixture model is not valid for
our case. In our special setting, to learn moderately coupled Gaussian mixture
capturing diverse events, we newly propose a pull-push learning scheme using
pulling and pushing losses, each of which plays an opposite role to the other.
The effects of components in our scheme are verified in-depth with extensive
ablation studies and the overall scheme achieves state-of-the-art performance.
Our code is available at https://github.com/sunoh-kim/pps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sunoh Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Jungchan Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Joonsang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1&quot;&gt;YoungJoon Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jin Young Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16392">
<title>Adaptive Depth Networks with Skippable Sub-Paths. (arXiv:2312.16392v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.16392</link>
<description rdf:parseType="Literal">&lt;p&gt;Systematic adaptation of network depths at runtime can be an effective way to
control inference latency and meet the resource condition of various devices.
However, previous depth adaptive networks do not provide general principles and
a formal explanation on why and which layers can be skipped, and, hence, their
approaches are hard to be generalized and require long and complex training
steps. In this paper, we present an architectural pattern and training method
for adaptive depth networks that can provide flexible accuracy-efficiency
trade-offs in a single network. In our approach, every residual stage is
divided into 2 consecutive sub-paths with different properties. While the first
sub-path is mandatory for hierarchical feature learning, the other is optimized
to incur minimal performance degradation even if it is skipped. Unlike previous
adaptive networks, our approach does not iteratively self-distill a fixed set
of sub-networks, resulting in significantly shorter training time. However,
once deployed on devices, it can instantly construct sub-networks of varying
depths to provide various accuracy-efficiency trade-offs in a single model. We
provide a formal rationale for why the proposed architectural pattern and
training method can reduce overall prediction errors while minimizing the
impact of skipping selected sub-paths. We also demonstrate the generality and
effectiveness of our approach with various residual networks, both from
convolutional neural networks and vision transformers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1&quot;&gt;Woochul Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16403">
<title>Learning Time-aware Graph Structures for Spatially Correlated Time Series Forecasting. (arXiv:2312.16403v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16403</link>
<description rdf:parseType="Literal">&lt;p&gt;Spatio-temporal forecasting of future values of spatially correlated time
series is important across many cyber-physical systems (CPS). Recent studies
offer evidence that the use of graph neural networks to capture latent
correlations between time series holds a potential for enhanced forecasting.
However, most existing methods rely on pre-defined or self-learning graphs,
which are either static or unintentionally dynamic, and thus cannot model the
time-varying correlations that exhibit trends and periodicities caused by the
regularity of the underlying processes in CPS. To tackle such limitation, we
propose Time-aware Graph Structure Learning (TagSL), which extracts time-aware
correlations among time series by measuring the interaction of node and time
representations in high-dimensional spaces. Notably, we introduce time
discrepancy learning that utilizes contrastive learning with distance-based
regularization terms to constrain learned spatial correlations to a trend
sequence. Additionally, we propose a periodic discriminant function to enable
the capture of periodic changes from the state of nodes. Next, we present a
Graph Convolution-based Gated Recurrent Unit (GCGRU) that jointly captures
spatial and temporal dependencies while learning time-aware and node-specific
patterns. Finally, we introduce a unified framework named Time-aware Graph
Convolutional Recurrent Network (TGCRN), combining TagSL, and GCGRU in an
encoder-decoder architecture for multi-step spatio-temporal forecasting. We
report on experiments with TGCRN and popular existing approaches on five
real-world datasets, thus providing evidence that TGCRN is capable of advancing
the state-of-the-art. We also cover a detailed ablation study and visualization
analysis, offering detailed insight into the effectiveness of time-aware
structure learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Minbo Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jilin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jensen_C/0/1/0/all/0/1&quot;&gt;Christian S. Jensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_F/0/1/0/all/0/1&quot;&gt;Fei Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_P/0/1/0/all/0/1&quot;&gt;Peng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianrui Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16418">
<title>Refining Latent Homophilic Structures over Heterophilic Graphs for Robust Graph Convolution Networks. (arXiv:2312.16418v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16418</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph convolution networks (GCNs) are extensively utilized in various graph
tasks to mine knowledge from spatial data. Our study marks the pioneering
attempt to quantitatively investigate the GCN robustness over omnipresent
heterophilic graphs for node classification. We uncover that the predominant
vulnerability is caused by the structural out-of-distribution (OOD) issue. This
finding motivates us to present a novel method that aims to harden GCNs by
automatically learning Latent Homophilic Structures over heterophilic graphs.
We term such a methodology as LHS. To elaborate, our initial step involves
learning a latent structure by employing a novel self-expressive technique
based on multi-node interactions. Subsequently, the structure is refined using
a pairwisely constrained dual-view contrastive learning approach. We
iteratively perform the above procedure, enabling a GCN model to aggregate
information in a homophilic way on heterophilic graphs. Armed with such an
adaptable structure, we can properly mitigate the structural OOD threats over
heterophilic graphs. Experiments on various benchmarks show the effectiveness
of the proposed LHS approach for robust GCNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1&quot;&gt;Chenyang Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nan_G/0/1/0/all/0/1&quot;&gt;Guoshun Nan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_T/0/1/0/all/0/1&quot;&gt;Tianyu Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1&quot;&gt;Wendi Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Di Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1&quot;&gt;Zhiyang Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lijuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1&quot;&gt;Qimei Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16423">
<title>General Method for Solving Four Types of SAT Problems. (arXiv:2312.16423v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.16423</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing methods provide varying algorithms for different types of Boolean
satisfiability problems (SAT), lacking a general solution framework.
Accordingly, this study proposes a unified framework DCSAT based on integer
programming and reinforcement learning (RL) algorithm to solve different types
of SAT problems such as MaxSAT, Weighted MaxSAT, PMS, WPMS. Specifically, we
first construct a consolidated integer programming representation for four
types of SAT problems by adjusting objective function coefficients. Secondly,
we construct an appropriate reinforcement learning models based on the 0-1
integer programming for SAT problems. Based on the binary tree search
structure, we apply the Monte Carlo tree search (MCTS) method on SAT problems.
Finally, we prove that this method can find all optimal Boolean assignments
based on Wiener-khinchin law of large Numbers. We experimentally verify that
this paradigm can prune the unnecessary search space to find the optimal
Boolean assignments for the problem. Furthermore, the proposed method can
provide diverse labels for supervised learning methods for SAT problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Anqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Congying Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1&quot;&gt;Tiande Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bonan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16424">
<title>Soft Contrastive Learning for Time Series. (arXiv:2312.16424v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16424</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning has shown to be effective to learn representations from
time series in a self-supervised way. However, contrasting similar time series
instances or values from adjacent timestamps within a time series leads to
ignore their inherent correlations, which results in deteriorating the quality
of learned representations. To address this issue, we propose SoftCLT, a simple
yet effective soft contrastive learning strategy for time series. This is
achieved by introducing instance-wise and temporal contrastive loss with soft
assignments ranging from zero to one. Specifically, we define soft assignments
for 1) instance-wise contrastive loss by the distance between time series on
the data space, and 2) temporal contrastive loss by the difference of
timestamps. SoftCLT is a plug-and-play method for time series contrastive
learning that improves the quality of learned representations without bells and
whistles. In experiments, we demonstrate that SoftCLT consistently improves the
performance in various downstream tasks including classification,
semi-supervised learning, transfer learning, and anomaly detection, showing
state-of-the-art performance. Code is available at this repository:
https://github.com/seunghan96/softclt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seunghan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1&quot;&gt;Taeyoung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kibok Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16427">
<title>Learning to Embed Time Series Patches Independently. (arXiv:2312.16427v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16427</link>
<description rdf:parseType="Literal">&lt;p&gt;Masked time series modeling has recently gained much attention as a
self-supervised representation learning strategy for time series. Inspired by
masked image modeling in computer vision, recent works first patchify and
partially mask out time series, and then train Transformers to capture the
dependencies between patches by predicting masked patches from unmasked
patches. However, we argue that capturing such patch dependencies might not be
an optimal strategy for time series representation learning; rather, learning
to embed patches independently results in better time series representations.
Specifically, we propose to use 1) the simple patch reconstruction task, which
autoencode each patch without looking at other patches, and 2) the simple
patch-wise MLP that embeds each patch independently. In addition, we introduce
complementary contrastive learning to hierarchically capture adjacent time
series information efficiently. Our proposed method improves time series
forecasting and classification performance compared to state-of-the-art
Transformer-based models, while it is more efficient in terms of the number of
parameters and training/inference time. Code is available at this repository:
https://github.com/seunghan96/pits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seunghan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1&quot;&gt;Taeyoung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kibok Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16429">
<title>GAD-PVI: A General Accelerated Dynamic-Weight Particle-Based Variational Inference Framework. (arXiv:2312.16429v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16429</link>
<description rdf:parseType="Literal">&lt;p&gt;Particle-based Variational Inference (ParVI) methods approximate the target
distribution by iteratively evolving finite weighted particle systems. Recent
advances of ParVI methods reveal the benefits of accelerated position update
strategies and dynamic weight adjustment approaches. In this paper, we propose
the first ParVI framework that possesses both accelerated position update and
dynamical weight adjustment simultaneously, named the General Accelerated
Dynamic-Weight Particle-based Variational Inference (GAD-PVI) framework.
Generally, GAD-PVI simulates the semi-Hamiltonian gradient flow on a novel
Information-Fisher-Rao space, which yields an additional decrease on the local
functional dissipation. GAD-PVI is compatible with different dissimilarity
functionals and associated smoothing approaches under three information
metrics. Experiments on both synthetic and real-world data demonstrate the
faster convergence and reduced approximation error of GAD-PVI methods over the
state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fangyikang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Huminhao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hanbin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1&quot;&gt;Hui Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16430">
<title>Preference as Reward, Maximum Preference Optimization with Importance Sampling. (arXiv:2312.16430v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16430</link>
<description rdf:parseType="Literal">&lt;p&gt;Preference learning is a key technology for aligning language models with
human values. Reinforcement Learning from Human Feedback (RLHF) is a model
based algorithm to optimize preference learning, which first fitting a reward
model for preference score, and then optimizing generating policy with
on-policy PPO algorithm to maximize the reward. The processing of RLHF is
complex, time-consuming and unstable. Direct Preference Optimization (DPO)
algorithm using off-policy algorithm to direct optimize generating policy and
eliminating the need for reward model, which is data efficient and stable. DPO
use Bradley-Terry model and log-loss which leads to over-fitting to the
preference data at the expense of ignoring KL-regularization term when
preference near deterministic. IPO uses a root-finding pairwise MSE loss to
solve the ignoring KL-regularization problem, and learning an optimal policy.
But IPO&apos;s pairwise loss still can&apos;t s make the KL-regularization to work. In
this paper, we design a simple and intuitive off-policy preferences
optimization algorithm from an importance sampling view, and add an off-policy
KL-regularization term which makes KL-regularization truly effective. To
simplify the learning process and save memory usage, we can generate
regularization data in advance, which eliminate the needs for both reward model
and reference policy in the stage of optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zaifan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1&quot;&gt;Chao Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16438">
<title>Visual Spatial Attention and Proprioceptive Data-Driven Reinforcement Learning for Robust Peg-in-Hole Task Under Variable Conditions. (arXiv:2312.16438v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.16438</link>
<description rdf:parseType="Literal">&lt;p&gt;Anchor-bolt insertion is a peg-in-hole task performed in the construction
field for holes in concrete. Efforts have been made to automate this task, but
the variable lighting and hole surface conditions, as well as the requirements
for short setup and task execution time make the automation challenging. In
this study, we introduce a vision and proprioceptive data-driven robot control
model for this task that is robust to challenging lighting and hole surface
conditions. This model consists of a spatial attention point network (SAP) and
a deep reinforcement learning (DRL) policy that are trained jointly end-to-end
to control the robot. The model is trained in an offline manner, with a
sample-efficient framework designed to reduce training time and minimize the
reality gap when transferring the model to the physical world. Through
evaluations with an industrial robot performing the task in 12 unknown holes,
starting from 16 different initial positions, and under three different
lighting conditions (two with misleading shadows), we demonstrate that SAP can
generate relevant attention points of the image even in challenging lighting
conditions. We also show that the proposed model enables task execution with
higher success rate and shorter task completion time than various baselines.
Due to the proposed model&apos;s high effectiveness even in severe lighting, initial
positions, and hole conditions, and the offline training framework&apos;s high
sample-efficiency and short training time, this approach can be easily applied
to construction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yasutomi_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Yuji Yasutomi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichiwara_H/0/1/0/all/0/1&quot;&gt;Hideyuki Ichiwara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ito_H/0/1/0/all/0/1&quot;&gt;Hiroshi Ito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mori_H/0/1/0/all/0/1&quot;&gt;Hiroki Mori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ogata_T/0/1/0/all/0/1&quot;&gt;Tetsuya Ogata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16448">
<title>Randomized Signature Methods in Optimal Portfolio Selection. (arXiv:2312.16448v1 [q-fin.PM])</title>
<link>http://arxiv.org/abs/2312.16448</link>
<description rdf:parseType="Literal">&lt;p&gt;We present convincing empirical results on the application of Randomized
Signature Methods for non-linear, non-parametric drift estimation for a
multi-variate financial market. Even though drift estimation is notoriously ill
defined due to small signal to noise ratio, one can still try to learn optimal
non-linear maps from data to future returns for the purposes of portfolio
optimization. Randomized Signatures, in contrast to classical signatures, allow
for high dimensional market dimension and provide features on the same scale.
We do not contribute to the theory of Randomized Signatures here, but rather
present our empirical findings on portfolio selection in real world settings
including real market data and transaction costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Akyildirim_E/0/1/0/all/0/1&quot;&gt;Erdinc Akyildirim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Gambara_M/0/1/0/all/0/1&quot;&gt;Matteo Gambara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Teichmann_J/0/1/0/all/0/1&quot;&gt;Josef Teichmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Syang Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16450">
<title>FCDNet: Frequency-Guided Complementary Dependency Modeling for Multivariate Time-Series Forecasting. (arXiv:2312.16450v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16450</link>
<description rdf:parseType="Literal">&lt;p&gt;Multivariate time-series (MTS) forecasting is a challenging task in many
real-world non-stationary dynamic scenarios. In addition to intra-series
temporal signals, the inter-series dependency also plays a crucial role in
shaping future trends. How to enable the model&apos;s awareness of dependency
information has raised substantial research attention. Previous approaches have
either presupposed dependency constraints based on domain knowledge or imposed
them using real-time feature similarity. However, MTS data often exhibit both
enduring long-term static relationships and transient short-term interactions,
which mutually influence their evolving states. It is necessary to recognize
and incorporate the complementary dependencies for more accurate MTS
prediction. The frequency information in time series reflects the evolutionary
rules behind complex temporal dynamics, and different frequency components can
be used to well construct long-term and short-term interactive dependency
structures between variables. To this end, we propose FCDNet, a concise yet
effective framework for multivariate time-series forecasting. Specifically,
FCDNet overcomes the above limitations by applying two light-weight dependency
constructors to help extract long- and short-term dependency information
adaptively from multi-level frequency patterns. With the growth of input
variables, the number of trainable parameters in FCDNet only increases
linearly, which is conducive to the model&apos;s scalability and avoids
over-fitting. Additionally, adopting a frequency-based perspective can
effectively mitigate the influence of noise within MTS data, which helps
capture more genuine dependencies. The experimental results on six real-world
datasets from multiple fields show that FCDNet significantly exceeds strong
baselines, with an average improvement of 6.82% on MAE, 4.98% on RMSE, and
4.91% on MAPE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weijun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Heyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Ye Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1&quot;&gt;Shijie Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ning Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16451">
<title>Domain Generalization with Vital Phase Augmentation. (arXiv:2312.16451v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.16451</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have shown remarkable performance in image
classification. However, their performance significantly deteriorates with
corrupted input data. Domain generalization methods have been proposed to train
robust models against out-of-distribution data. Data augmentation in the
frequency domain is one of such approaches that enable a model to learn phase
features to establish domain-invariant representations. This approach changes
the amplitudes of the input data while preserving the phases. However, using
fixed phases leads to susceptibility to phase fluctuations because amplitudes
and phase fluctuations commonly occur in out-of-distribution. In this study, to
address this problem, we introduce an approach using finite variation of the
phases of input data rather than maintaining fixed phases. Based on the
assumption that the degree of domain-invariant features varies for each phase,
we propose a method to distinguish phases based on this degree. In addition, we
propose a method called vital phase augmentation (VIPAug) that applies the
variation to the phases differently according to the degree of domain-invariant
features of given phases. The model depends more on the vital phases that
contain more domain-invariant features for attaining robustness to amplitude
and phase fluctuations. We present experimental evaluations of our proposed
approach, which exhibited improved performance for both clean and corrupted
data. VIPAug achieved SOTA performance on the benchmark CIFAR-10 and CIFAR-100
datasets, as well as near-SOTA performance on the ImageNet-100 and ImageNet
datasets. Our code is available at https://github.com/excitedkid/vipaug.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;Ingyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1&quot;&gt;Wooju Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1&quot;&gt;Hyun Myung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16470">
<title>ReSynthDetect: A Fundus Anomaly Detection Network with Reconstruction and Synthetic Features. (arXiv:2312.16470v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.16470</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting anomalies in fundus images through unsupervised methods is a
challenging task due to the similarity between normal and abnormal tissues, as
well as their indistinct boundaries. The current methods have limitations in
accurately detecting subtle anomalies while avoiding false positives. To
address these challenges, we propose the ReSynthDetect network which utilizes a
reconstruction network for modeling normal images, and an anomaly generator
that produces synthetic anomalies consistent with the appearance of fundus
images. By combining the features of consistent anomaly generation and image
reconstruction, our method is suited for detecting fundus abnormalities. The
proposed approach has been extensively tested on benchmark datasets such as
EyeQ and IDRiD, demonstrating state-of-the-art performance in both image-level
and pixel-level anomaly detection. Our experiments indicate a substantial 9%
improvement in AUROC on EyeQ and a significant 17.1% improvement in AUPR on
IDRiD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1&quot;&gt;Jingqi Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qinji Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1&quot;&gt;Shiwen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zilong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_K/0/1/0/all/0/1&quot;&gt;Kang Dang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xiaowei Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16475">
<title>Federated Continual Learning via Knowledge Fusion: A Survey. (arXiv:2312.16475v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16475</link>
<description rdf:parseType="Literal">&lt;p&gt;Data privacy and silos are nontrivial and greatly challenging in many
real-world applications. Federated learning is a decentralized approach to
training models across multiple local clients without the exchange of raw data
from client devices to global servers. However, existing works focus on a
static data environment and ignore continual learning from streaming data with
incremental tasks. Federated Continual Learning (FCL) is an emerging paradigm
to address model learning in both federated and continual learning
environments. The key objective of FCL is to fuse heterogeneous knowledge from
different clients and retain knowledge of previous tasks while learning on new
ones. In this work, we delineate federated learning and continual learning
first and then discuss their integration, i.e., FCL, and particular FCL via
knowledge fusion. In summary, our motivations are four-fold: we (1) raise a
fundamental problem called &apos;&apos;spatial-temporal catastrophic forgetting&apos;&apos; and
evaluate its impact on the performance using a well-known method called
federated averaging (FedAvg), (2) integrate most of the existing FCL methods
into two generic frameworks, namely synchronous FCL and asynchronous FCL, (3)
categorize a large number of methods according to the mechanism involved in
knowledge fusion, and finally (4) showcase an outlook on the future work of
FCL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junbo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianrui Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16476">
<title>SVGDreamer: Text Guided SVG Generation with Diffusion Model. (arXiv:2312.16476v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.16476</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, text-guided scalable vector graphics (SVGs) synthesis has shown
promise in domains such as iconography and sketch. However, existing
text-to-SVG generation methods lack editability and struggle with visual
quality and result diversity. To address these limitations, we propose a novel
text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer
incorporates a semantic-driven image vectorization (SIVE) process that enables
the decomposition of synthesis into foreground objects and background, thereby
enhancing editability. Specifically, the SIVE process introduce attention-based
primitive control and an attention-mask loss function for effective control and
manipulation of individual elements. Additionally, we propose a Vectorized
Particle-based Score Distillation (VPSD) approach to tackle the challenges of
color over-saturation, vector primitives over-smoothing, and limited result
diversity in existing text-to-SVG generation methods. Furthermore, on the basis
of VPSD, we introduce Reward Feedback Learning (ReFL) to accelerate VPSD
convergence and improve aesthetic appeal. Extensive experiments have been
conducted to validate the effectiveness of SVGDreamer, demonstrating its
superiority over baseline methods in terms of editability, visual quality, and
diversity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1&quot;&gt;Ximing Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Haitao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chuang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qian Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16477">
<title>Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding. (arXiv:2312.16477v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.16477</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the results of view-based 3D shape recognition methods have
saturated, and models with excellent performance cannot be deployed on
memory-limited devices due to their huge size of parameters. To address this
problem, we introduce a compression method based on knowledge distillation for
this field, which largely reduces the number of parameters while preserving
model performance as much as possible. Specifically, to enhance the
capabilities of smaller models, we design a high-performing large model called
Group Multi-view Vision Transformer (GMViT). In GMViT, the view-level ViT first
establishes relationships between view-level features. Additionally, to capture
deeper features, we employ the grouping module to enhance view-level features
into group-level features. Finally, the group-level ViT aggregates group-level
features into complete, well-formed 3D shape descriptors. Notably, in both
ViTs, we introduce spatial encoding of camera coordinates as innovative
position embeddings. Furthermore, we propose two compressed versions based on
GMViT, namely GMViT-simple and GMViT-mini. To enhance the training
effectiveness of the small models, we introduce a knowledge distillation method
throughout the GMViT process, where the key outputs of each GMViT component
serve as distillation targets. Extensive experiments demonstrate the efficacy
of the proposed method. The large model GMViT achieves excellent 3D
classification and retrieval results on the benchmark datasets ModelNet,
ShapeNetCore55, and MCB. The smaller models, GMViT-simple and GMViT-mini,
reduce the parameter size by 8 and 17.6 times, respectively, and improve shape
recognition speed by 1.5 times on average, while preserving at least 90% of the
classification and retrieval performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lixiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1&quot;&gt;Qingzhe Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1&quot;&gt;Richang Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1&quot;&gt;Enhong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yuanyan Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16486">
<title>PanGu-Draw: Advancing Resource-Efficient Text-to-Image Synthesis with Time-Decoupled Training and Reusable Coop-Diffusion. (arXiv:2312.16486v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.16486</link>
<description rdf:parseType="Literal">&lt;p&gt;Current large-scale diffusion models represent a giant leap forward in
conditional image synthesis, capable of interpreting diverse cues like text,
human poses, and edges. However, their reliance on substantial computational
resources and extensive data collection remains a bottleneck. On the other
hand, the integration of existing diffusion models, each specialized for
different controls and operating in unique latent spaces, poses a challenge due
to incompatible image resolutions and latent space embedding structures,
hindering their joint use. Addressing these constraints, we present
&quot;PanGu-Draw&quot;, a novel latent diffusion model designed for resource-efficient
text-to-image synthesis that adeptly accommodates multiple control signals. We
first propose a resource-efficient Time-Decoupling Training Strategy, which
splits the monolithic text-to-image model into structure and texture
generators. Each generator is trained using a regimen that maximizes data
utilization and computational efficiency, cutting data preparation by 48% and
reducing training resources by 51%. Secondly, we introduce &quot;Coop-Diffusion&quot;, an
algorithm that enables the cooperative use of various pre-trained diffusion
models with different latent spaces and predefined resolutions within a unified
denoising process. This allows for multi-control image synthesis at arbitrary
resolutions without the necessity for additional data or retraining. Empirical
validations of Pangu-Draw show its exceptional prowess in text-to-image and
multi-control image generation, suggesting a promising direction for future
model training efficiencies and generation versatility. The largest 5B T2I
PanGu-Draw model is released on the Ascend platform. Project page:
https://pangu-draw.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1&quot;&gt;Guansong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuanfan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jianhua Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_M/0/1/0/all/0/1&quot;&gt;Minzhe Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yihan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Songcen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zeyi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1&quot;&gt;Zhao Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16489">
<title>Best-of-Both-Worlds Linear Contextual Bandits. (arXiv:2312.16489v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16489</link>
<description rdf:parseType="Literal">&lt;p&gt;This study investigates the problem of $K$-armed linear contextual bandits,
an instance of the multi-armed bandit problem, under an adversarial corruption.
At each round, a decision-maker observes an independent and identically
distributed context and then selects an arm based on the context and past
observations. After selecting an arm, the decision-maker incurs a loss
corresponding to the selected arm. The decision-maker aims to minimize the
cumulative loss over the trial. The goal of this study is to develop a strategy
that is effective in both stochastic and adversarial environments, with
theoretical guarantees. We first formulate the problem by introducing a novel
setting of bandits with adversarial corruption, referred to as the contextual
adversarial regime with a self-bounding constraint. We assume linear models for
the relationship between the loss and the context. Then, we propose a strategy
that extends the RealLinExp3 by Neu &amp;amp; Olkhovskaya (2020) and the
Follow-The-Regularized-Leader (FTRL). The regret of our proposed algorithm is
shown to be upper-bounded by $O\left(\min\left\{\frac{(\log(T))^3}{\Delta_{*}}
+ \sqrt{\frac{C(\log(T))^3}{\Delta_{*}}},\ \
\sqrt{T}(\log(T))^2\right\}\right)$, where $T \in\mathbb{N}$ is the number of
rounds, $\Delta_{*} &amp;gt; 0$ is the constant minimum gap between the best and
suboptimal arms for any context, and $C\in[0, T] $ is an adversarial corruption
parameter. This regret upper bound implies
$O\left(\frac{(\log(T))^3}{\Delta_{*}}\right)$ in a stochastic environment and
by $O\left( \sqrt{T}(\log(T))^2\right)$ in an adversarial environment. We refer
to our strategy as the Best-of-Both-Worlds (BoBW) RealFTRL, due to its
theoretical guarantees in both stochastic and adversarial regimes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1&quot;&gt;Masahiro Kato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ito_S/0/1/0/all/0/1&quot;&gt;Shinji Ito&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16490">
<title>Understanding News Creation Intents: Frame, Dataset, and Method. (arXiv:2312.16490v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.16490</link>
<description rdf:parseType="Literal">&lt;p&gt;As the disruptive changes in the media economy and the proliferation of
alternative news media outlets, news intent has progressively deviated from
ethical standards that serve the public interest. News intent refers to the
purpose or intention behind the creation of a news article. While the
significance of research on news intent has been widely acknowledged, the
absence of a systematic news intent understanding framework hinders further
exploration of news intent and its downstream applications. To bridge this gap,
we propose News INTent (NINT) frame, the first component-aware formalism for
understanding the news creation intent based on research in philosophy,
psychology, and cognitive science. Within this frame, we define the news intent
identification task and provide a benchmark dataset with fine-grained labels
along with an efficient benchmark method. Experiments demonstrate that NINT is
beneficial in both the intent identification task and downstream tasks that
demand a profound understanding of news. This work marks a foundational step
towards a more systematic exploration of news creation intents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhengjia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Danding Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1&quot;&gt;Qiang Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Juan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Silong Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Beizhe Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Siyuan Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16497">
<title>Mobility and Cost Aware Inference Accelerating Algorithm for Edge Intelligence. (arXiv:2312.16497v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2312.16497</link>
<description rdf:parseType="Literal">&lt;p&gt;The edge intelligence (EI) has been widely applied recently. Spliting the
model between device, edge server, and cloud can improve the performance of EI
greatly. The model segmentation without user mobility has been investigated
deeply by previous works. However, in most use cases of EI, the end devices are
mobile. Only a few works have been carried out on this aspect. These works
still have many issues, such as ignoring the energy consumption of mobile
device, inappropriate network assumption, and low effectiveness on adaptiving
user mobility, etc. Therefore, for addressing the disadvantages of model
segmentation and resource allocation in previous works, we propose mobility and
cost aware model segmentation and resource allocation algorithm for
accelerating the inference at edge (MCSA). Specfically, in the scenario without
user mobility, the loop interation gradient descent (Li-GD) algorithm is
provided. When the mobile user has a large model inference task needs to be
calculated, it will take the energy consumption of mobile user, the
communication and computing resource renting cost, and the inference delay into
account to find the optimal model segmentation and resource allocation
strategy. In the scenario with user mobility, the mobiity aware Li-GD (MLi-GD)
algorithm is proposed to calculate the optimal strategy. Then, the properties
of the proposed algorithms are investigated, including convergence, complexity,
and approximation ratio. The experimental results demonstrate the effectiveness
of the proposed algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1&quot;&gt;Ning Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_k/0/1/0/all/0/1&quot;&gt;kang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wenchao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Quan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Song Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16511">
<title>S2M: Converting Single-Turn to Multi-Turn Datasets for Conversational Question Answering. (arXiv:2312.16511v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.16511</link>
<description rdf:parseType="Literal">&lt;p&gt;Supplying data augmentation to conversational question answering (CQA) can
effectively improve model performance. However, there is less improvement from
single-turn datasets in CQA due to the distribution gap between single-turn and
multi-turn datasets. On the other hand, while numerous single-turn datasets are
available, we have not utilized them effectively. To solve this problem, we
propose a novel method to convert single-turn datasets to multi-turn datasets.
The proposed method consists of three parts, namely, a QA pair Generator, a QA
pair Reassembler, and a question Rewriter. Given a sample consisting of context
and single-turn QA pairs, the Generator obtains candidate QA pairs and a
knowledge graph based on the context. The Reassembler utilizes the knowledge
graph to get sequential QA pairs, and the Rewriter rewrites questions from a
conversational perspective to obtain a multi-turn dataset S2M. Our experiments
show that our method can synthesize effective training resources for CQA.
Notably, S2M ranks 1st place on the QuAC leaderboard at the time of submission
(Aug 24th, 2022).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Baokui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wangshu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yicheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Changlin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Sen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Teng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+liu_S/0/1/0/all/0/1&quot;&gt;Siye liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiwei Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16542">
<title>FALCON: Feature-Label Constrained Graph Net Collapse for Memory Efficient GNNs. (arXiv:2312.16542v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16542</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Network (GNN) ushered in a new era of machine learning with
interconnected datasets. While traditional neural networks can only be trained
on independent samples, GNN allows for the inclusion of inter-sample
interactions in the training process. This gain, however, incurs additional
memory cost, rendering most GNNs unscalable for real-world applications
involving vast and complicated networks with tens of millions of nodes (e.g.,
social circles, web graphs, and brain graphs). This means that storing the
graph in the main memory can be difficult, let alone training the GNN model
with significantly less GPU memory. While much of the recent literature has
focused on either mini-batching GNN methods or quantization, graph reduction
methods remain largely scarce. Furthermore, present graph reduction approaches
have several drawbacks. First, most graph reduction focuses only on the
inference stage (e.g., condensation and distillation) and requires full graph
GNN training, which does not reduce training memory footprint. Second, many
methods focus solely on the graph&apos;s structural aspect, ignoring the initial
population feature-label distribution, resulting in a skewed post-reduction
label distribution. Here, we propose a Feature-Label COnstrained graph Net
collapse, FALCON, to address these limitations. Our three core contributions
lie in (i) designing FALCON, a topology-aware graph reduction technique that
preserves feature-label distribution; (ii) implementation of FALCON with other
memory reduction methods (i.e., mini-batched GNN and quantization) for further
memory reduction; (iii) extensive benchmarking and ablation studies against
SOTA methods to evaluate FALCON memory reduction. Our extensive results show
that FALCON can significantly collapse various public datasets while achieving
equal prediction quality across GNN models. Code:
https://github.com/basiralab/FALCON
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adnel_C/0/1/0/all/0/1&quot;&gt;Christopher Adnel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rekik_I/0/1/0/all/0/1&quot;&gt;Islem Rekik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16548">
<title>A proposed new metric for the conceptual diversity of a text. (arXiv:2312.16548v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.16548</link>
<description rdf:parseType="Literal">&lt;p&gt;A word may contain one or more hidden concepts. While the &quot;animal&quot; word
evokes many images in our minds and encapsulates many concepts (birds, dogs,
cats, crocodiles, etc.), the `parrot&apos; word evokes a single image (a colored
bird with a short, hooked beak and the ability to mimic sounds). In spoken or
written texts, we use some words in a general sense and some in a detailed way
to point to a specific object. Until now, a text&apos;s conceptual diversity value
cannot be determined using a standard and precise technique. This research
contributes to the natural language processing field of AI by offering a
standardized method and a generic metric for evaluating and comparing concept
diversity in different texts and domains. It also contributes to the field of
semantic research of languages. If we give examples for the diversity score of
two sentences, &quot;He discovered an unknown entity.&quot; has a high conceptual
diversity score (16.6801), and &quot;The endoplasmic reticulum forms a series of
flattened sacs within the cytoplasm of eukaryotic cells.&quot; sentence has a low
conceptual diversity score which is 3.9068.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phd_I/0/1/0/all/0/1&quot;&gt;&amp;#x130;lknur D&amp;#xf6;nmez Phd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phd_M/0/1/0/all/0/1&quot;&gt;Mehmet Hakl&amp;#x131;d&amp;#x131;r Phd&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16549">
<title>How Robust are LLMs to In-Context Majority Label Bias?. (arXiv:2312.16549v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16549</link>
<description rdf:parseType="Literal">&lt;p&gt;In the In-Context Learning (ICL) setup, various forms of label biases can
manifest. One such manifestation is majority label bias, which arises when the
distribution of labeled examples in the in-context samples is skewed towards
one or more specific classes making Large Language Models (LLMs) more prone to
predict those labels. Such discrepancies can arise from various factors,
including logistical constraints, inherent biases in data collection methods,
limited access to diverse data sources, etc. which are unavoidable in a
real-world industry setup. In this work, we study the robustness of in-context
learning in LLMs to shifts that occur due to majority label bias within the
purview of text classification tasks. Prior works have shown that in-context
learning with LLMs is susceptible to such biases. In our study, we go one level
deeper and show that the robustness boundary varies widely for different models
and tasks, with certain LLMs being highly robust (~90%) to majority label bias.
Additionally, our findings also highlight the impact of model size and the
richness of instructional prompts contributing towards model robustness. We
restrict our study to only publicly available open-source models to ensure
transparency and reproducibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1&quot;&gt;Karan Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roychowdhury_S/0/1/0/all/0/1&quot;&gt;Sumegh Roychowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasa_S/0/1/0/all/0/1&quot;&gt;Siva Rajesh Kasa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasa_S/0/1/0/all/0/1&quot;&gt;Santhosh Kumar Kasa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhanushali_A/0/1/0/all/0/1&quot;&gt;Anish Bhanushali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pattisapu_N/0/1/0/all/0/1&quot;&gt;Nikhil Pattisapu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murthy_P/0/1/0/all/0/1&quot;&gt;Prasanna Srinivasa Murthy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16554">
<title>A Theoretical Analysis of Efficiency Constrained Utility-Privacy Bi-Objective Optimization in Federated Learning. (arXiv:2312.16554v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16554</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) enables multiple clients to collaboratively learn a
shared model without sharing their individual data. Concerns about utility,
privacy, and training efficiency in FL have garnered significant research
attention. Differential privacy has emerged as a prevalent technique in FL,
safeguarding the privacy of individual user data while impacting utility and
training efficiency. Within Differential Privacy Federated Learning (DPFL),
previous studies have primarily focused on the utility-privacy trade-off,
neglecting training efficiency, which is crucial for timely completion.
Moreover, differential privacy achieves privacy by introducing controlled
randomness (noise) on selected clients in each communication round. Previous
work has mainly examined the impact of noise level ($\sigma$) and communication
rounds ($T$) on the privacy-utility dynamic, overlooking other influential
factors like the sample ratio ($q$, the proportion of selected clients). This
paper systematically formulates an efficiency-constrained utility-privacy
bi-objective optimization problem in DPFL, focusing on $\sigma$, $T$, and $q$.
We provide a comprehensive theoretical analysis, yielding analytical solutions
for the Pareto front. Extensive empirical experiments verify the validity and
efficacy of our analysis, offering valuable guidance for low-cost parameter
design in DPFL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1&quot;&gt;Hanlin Gu&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xinyuan Zhao&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yuxing Han&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yan Kang&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Lixin Fan&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiang Yang&lt;/a&gt; (1 and 3) ((1) WeBank, China, (2) Tsinghua University, China, (3) Hong Kong University of Science and Technology, China)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16563">
<title>RDGCL: Reaction-Diffusion Graph Contrastive Learning for Recommendation. (arXiv:2312.16563v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2312.16563</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning (CL) has emerged as a promising technique for improving
recommender systems, addressing the challenge of data sparsity by leveraging
self-supervised signals from raw data. Integration of CL with graph
convolutional network (GCN)-based collaborative filterings (CFs) has been
explored in recommender systems. However, current CL-based recommendation
models heavily rely on low-pass filters and graph augmentations. In this paper,
we propose a novel CL method for recommender systems called the
reaction-diffusion graph contrastive learning model (RDGCL). We design our own
GCN for CF based on both the diffusion, i.e., low-pass filter, and the
reaction, i.e., high-pass filter, equations. Our proposed CL-based training
occurs between reaction and diffusion-based embeddings, so there is no need for
graph augmentations. Experimental evaluation on 6 benchmark datasets
demonstrates that our proposed method outperforms state-of-the-art CL-based
recommendation models. By enhancing recommendation accuracy and diversity, our
method brings an advancement in CL for recommender systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jeongwhan Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wi_H/0/1/0/all/0/1&quot;&gt;Hyowon Wi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chaejeong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1&quot;&gt;Sung-Bae Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dongha Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1&quot;&gt;Noseong Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16600">
<title>scRNA-seq Data Clustering by Cluster-aware Iterative Contrastive Learning. (arXiv:2312.16600v1 [q-bio.GN])</title>
<link>http://arxiv.org/abs/2312.16600</link>
<description rdf:parseType="Literal">&lt;p&gt;Single-cell RNA sequencing (scRNA-seq) enables researchers to analyze gene
expression at single-cell level. One important task in scRNA-seq data analysis
is unsupervised clustering, which helps identify distinct cell types, laying
down the foundation for other downstream analysis tasks. In this paper, we
propose a novel method called Cluster-aware Iterative Contrastive Learning
(CICL in short) for scRNA-seq data clustering, which utilizes an iterative
representation learning and clustering framework to progressively learn the
clustering structure of scRNA-seq data with a cluster-aware contrastive loss.
CICL consists of a Transformer encoder, a clustering head, a projection head
and a contrastive loss module. First, CICL extracts the feature vectors of the
original and augmented data by the Transformer encoder. Then, it computes the
clustering centroids by K-means and employs the student t-distribution to
assign pseudo-labels to all cells in the clustering head. The projection-head
uses a Multi-Layer Perceptron (MLP) to obtain projections of the augmented
data. At last, both pseudo-labels and projections are used in the contrastive
loss to guide the model training. Such a process goes iteratively so that the
clustering result becomes better and better. Extensive experiments on 25 real
world scRNA-seq datasets show that CICL outperforms the SOTA methods.
Concretely, CICL surpasses the existing methods by from 14% to 280%, and from
5% to 133% on average in terms of performance metrics ARI and NMI respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Weikang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinxian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Guan_J/0/1/0/all/0/1&quot;&gt;Jihong Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shuigeng Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16626">
<title>Sorting of Smartphone Components for Recycling Through Convolutional Neural Networks. (arXiv:2312.16626v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.16626</link>
<description rdf:parseType="Literal">&lt;p&gt;The recycling of waste electrical and electronic equipment is an essential
tool in allowing for a circular economy, presenting the potential for
significant environmental and economic gain. However, traditional material
separation techniques, based on physical and chemical processes, require
substantial investment and do not apply to all cases. In this work, we
investigate using an image classification neural network as a potential means
to control an automated material separation process in treating smartphone
waste, acting as a more efficient, less costly, and more widely applicable
alternative to existing tools. We produced a dataset with 1,127 images of
pyrolyzed smartphone components, which was then used to train and assess a
VGG-16 image classification model. The model achieved 83.33% accuracy, lending
credence to the viability of using such a neural network in material
separation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Becker_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;lvaro G. Becker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cenci_M/0/1/0/all/0/1&quot;&gt;Marcelo P. Cenci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silveira_T/0/1/0/all/0/1&quot;&gt;Thiago L. T. da Silveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veit_H/0/1/0/all/0/1&quot;&gt;Hugo M. Veit&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16659">
<title>A Large Language Model-based Computational Approach to Improve Identity-Related Write-Ups. (arXiv:2312.16659v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.16659</link>
<description rdf:parseType="Literal">&lt;p&gt;Creating written products is essential to modern life, including writings
about one&apos;s identity and personal experiences. However, writing is often a
difficult activity that requires extensive effort to frame the central ideas,
the pursued approach to communicate the central ideas, e.g., using analogies,
metaphors, or other possible means, the needed presentation structure, and the
actual verbal expression. Large Language Models, a recently emerged approach in
Machine Learning, can offer a significant help in reducing the effort and
improving the quality of written products. This paper proposes a new
computational approach to explore prompts that given as inputs to a Large
Language Models can generate cues to improve the considered written products.
Two case studies on improving write-ups, one based on an analogy and one on a
metaphor, are also presented in the paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doboli_A/0/1/0/all/0/1&quot;&gt;Alex Doboli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16682">
<title>Some things are more CRINGE than others: Preference Optimization with the Pairwise Cringe Loss. (arXiv:2312.16682v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.16682</link>
<description rdf:parseType="Literal">&lt;p&gt;Practitioners commonly align large language models using pairwise
preferences, i.e., given labels of the type response A is preferred to response
B for a given input. Perhaps less commonly, methods have also been developed
for binary feedback, i.e. training models given labels of type response A is
good or bad. We show how an existing performant binary feedback method, the
Cringe Loss (Adolphs et al., 2022), can be generalized to the pairwise
preference setting using a simple soft margin extension. Pairwise Cringe Loss
is straightforward to implement and efficient to train, and we find it
outperforms state-of-the-art preference optimization algorithms such as PPO and
DPO on the AlpacaFarm benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1&quot;&gt;Andrew Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1&quot;&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1&quot;&gt;Jason Weston&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16695">
<title>Performance Comparison of Session-based Recommendation Algorithms based on GNNs. (arXiv:2312.16695v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2312.16695</link>
<description rdf:parseType="Literal">&lt;p&gt;In session-based recommendation settings, a recommender system has to base
its suggestions on the user interactions that are ob served in an ongoing
session. Since such sessions can consist of only a small set of interactions,
various approaches based on Graph Neural Networks (GNN) were recently proposed,
as they allow us to integrate various types of side information about the items
in a natural way. Unfortunately, a variety of evaluation settings are used in
the literature, e.g., in terms of protocols, metrics and baselines, making it
difficult to assess what represents the state of the art. In this work, we
present the results of an evaluation of eight recent GNN-based approaches that
were published in high-quality outlets. For a fair comparison, all models are
systematically tuned and tested under identical conditions using three common
datasets. We furthermore include k-nearest-neighbor and sequential rules-based
models as baselines, as such models have previously exhibited competitive
performance results for similar settings. To our surprise, the evaluation
showed that the simple models outperform all recent GNN models in terms of the
Mean Reciprocal Rank, which we used as an optimization criterion, and were only
outperformed in three cases in terms of the Hit Rate. Additional analyses
furthermore reveal that several other factors that are often not deeply
discussed in papers, e.g., random seeds, can markedly impact the performance of
GNN-based models. Our results therefore (a) point to continuing issues in the
community in terms of research methodology and (b) indicate that there is ample
room for improvement in session-based recommendation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shehzad_F/0/1/0/all/0/1&quot;&gt;Faisal Shehzad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jannach_D/0/1/0/all/0/1&quot;&gt;Dietmar Jannach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16702">
<title>Rethinking Tabular Data Understanding with Large Language Models. (arXiv:2312.16702v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.16702</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown to be capable of various tasks, yet
their capability in interpreting and reasoning over tabular data remains an
underexplored area. In this context, this study investigates from three core
perspectives: the robustness of LLMs to structural perturbations in tables, the
comparative analysis of textual and symbolic reasoning on tables, and the
potential of boosting model performance through the aggregation of multiple
reasoning pathways. We discover that structural variance of tables presenting
the same content reveals a notable performance decline, particularly in
symbolic reasoning tasks. This prompts the proposal of a method for table
structure normalization. Moreover, textual reasoning slightly edges out
symbolic reasoning, and a detailed error analysis reveals that each exhibits
different strengths depending on the specific tasks. Notably, the aggregation
of textual and symbolic reasoning pathways, bolstered by a mix self-consistency
mechanism, resulted in achieving SOTA performance, with an accuracy of 73.6% on
WIKITABLEQUESTIONS, representing a substantial advancement over previous
existing table processing paradigms of LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Muhao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16704">
<title>On the Granular Representation of Fuzzy Quantifier-Based Fuzzy Rough Sets. (arXiv:2312.16704v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.16704</link>
<description rdf:parseType="Literal">&lt;p&gt;Rough set theory is a well-known mathematical framework that can deal with
inconsistent data by providing lower and upper approximations of concepts. A
prominent property of these approximations is their granular representation:
that is, they can be written as unions of simple sets, called granules. The
latter can be identified with &quot;if. . . , then. . . &quot; rules, which form the
backbone of rough set rule induction. It has been shown previously that this
property can be maintained for various fuzzy rough set models, including those
based on ordered weighted average (OWA) operators. In this paper, we will focus
on some instances of the general class of fuzzy quantifier-based fuzzy rough
sets (FQFRS). In these models, the lower and upper approximations are evaluated
using binary and unary fuzzy quantifiers, respectively. One of the main targets
of this study is to examine the granular representation of different models of
FQFRS. The main findings reveal that Choquet-based fuzzy rough sets can be
represented granularly under the same conditions as OWA-based fuzzy rough sets,
whereas Sugeno-based FRS can always be represented granularly. This observation
highlights the potential of these models for resolving data inconsistencies and
managing noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theerens_A/0/1/0/all/0/1&quot;&gt;Adnan Theerens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cornelis_C/0/1/0/all/0/1&quot;&gt;Chris Cornelis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16707">
<title>Modeling Systemic Risk: A Time-Varying Nonparametric Causal Inference Framework. (arXiv:2312.16707v1 [econ.EM])</title>
<link>http://arxiv.org/abs/2312.16707</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a nonparametric and time-varying directed information graph
(TV-DIG) framework to estimate the evolving causal structure in time series
networks, thereby addressing the limitations of traditional econometric models
in capturing high-dimensional, nonlinear, and time-varying interconnections
among series. This framework employs an information-theoretic measure rooted in
a generalized version of Granger-causality, which is applicable to both linear
and nonlinear dynamics. Our framework offers advancements in measuring systemic
risk and establishes meaningful connections with established econometric
models, including vector autoregression and switching models. We evaluate the
efficacy of our proposed model through simulation experiments and empirical
analysis, reporting promising results in recovering simulated time-varying
networks with nonlinear and multivariate structures. We apply this framework to
identify and monitor the evolution of interconnectedness and systemic risk
among major assets and industrial sectors within the financial network. We
focus on cryptocurrencies&apos; potential systemic risks to financial stability,
including spillover effects on other sectors during crises like the COVID-19
pandemic and the Federal Reserve&apos;s 2020 emergency response. Our findings
reveals significant, previously underrecognized pre-2020 influences of
cryptocurrencies on certain financial sectors, highlighting their potential
systemic risks and offering a systematic approach in tracking evolving
cross-sector interactions within financial networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Etesami_J/0/1/0/all/0/1&quot;&gt;Jalal Etesami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Habibnia_A/0/1/0/all/0/1&quot;&gt;Ali Habibnia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Kiyavash_N/0/1/0/all/0/1&quot;&gt;Negar Kiyavash&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16713">
<title>Knowledge Enhanced Conditional Imputation for Healthcare Time-series. (arXiv:2312.16713v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16713</link>
<description rdf:parseType="Literal">&lt;p&gt;This study presents a novel approach to addressing the challenge of missing
data in multivariate time series, with a particular focus on the complexities
of healthcare data. Our Conditional Self-Attention Imputation (CSAI) model,
grounded in a transformer-based framework, introduces a conditional hidden
state initialization tailored to the intricacies of medical time series data.
This methodology diverges from traditional imputation techniques by
specifically targeting the imbalance in missing data distribution, a crucial
aspect often overlooked in healthcare datasets. By integrating advanced
knowledge embedding and a non-uniform masking strategy, CSAI adeptly adjusts to
the distinct patterns of missing data in Electronic Health Records (EHRs).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1&quot;&gt;Linglong Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibrahim_Z/0/1/0/all/0/1&quot;&gt;Zina Ibrahim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellis_H/0/1/0/all/0/1&quot;&gt;Hugh Logan Ellis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Ao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuezhou Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dobson_R/0/1/0/all/0/1&quot;&gt;Richard Dobson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16715">
<title>Adversarial Attacks on LoRa Device Identification and Rogue Signal Detection with Deep Learning. (arXiv:2312.16715v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.16715</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-Power Wide-Area Network (LPWAN) technologies, such as LoRa, have gained
significant attention for their ability to enable long-range, low-power
communication for Internet of Things (IoT) applications. However, the security
of LoRa networks remains a major concern, particularly in scenarios where
device identification and classification of legitimate and spoofed signals are
crucial. This paper studies a deep learning framework to address these
challenges, considering LoRa device identification and legitimate vs. rogue
LoRa device classification tasks. A deep neural network (DNN), either a
convolutional neural network (CNN) or feedforward neural network (FNN), is
trained for each task by utilizing real experimental I/Q data for LoRa signals,
while rogue signals are generated by using kernel density estimation (KDE) of
received signals by rogue devices. Fast Gradient Sign Method (FGSM)-based
adversarial attacks are considered for LoRa signal classification tasks using
deep learning models. The impact of these attacks is assessed on the
performance of two tasks, namely device identification and legitimate vs. rogue
device classification, by utilizing separate or common perturbations against
these signal classification tasks. Results presented in this paper quantify the
level of transferability of adversarial attacks on different LoRa signal
classification tasks as a major vulnerability and highlight the need to make
IoT applications robust to adversarial attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sagduyu_Y/0/1/0/all/0/1&quot;&gt;Yalin E. Sagduyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erpek_T/0/1/0/all/0/1&quot;&gt;Tugba Erpek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16726">
<title>FairCompass: Operationalising Fairness in Machine Learning. (arXiv:2312.16726v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16726</link>
<description rdf:parseType="Literal">&lt;p&gt;As artificial intelligence (AI) increasingly becomes an integral part of our
societal and individual activities, there is a growing imperative to develop
responsible AI solutions. Despite a diverse assortment of machine learning
fairness solutions is proposed in the literature, there is reportedly a lack of
practical implementation of these tools in real-world applications. Industry
experts have participated in thorough discussions on the challenges associated
with operationalising fairness in the development of machine learning-empowered
solutions, in which a shift toward human-centred approaches is promptly
advocated to mitigate the limitations of existing techniques. In this work, we
propose a human-in-the-loop approach for fairness auditing, presenting a mixed
visual analytical system (hereafter referred to as &apos;FairCompass&apos;), which
integrates both subgroup discovery technique and the decision tree-based schema
for end users. Moreover, we innovatively integrate an Exploration, Guidance and
Informed Analysis loop, to facilitate the use of the Knowledge Generation Model
for Visual Analytics in FairCompass. We evaluate the effectiveness of
FairCompass for fairness auditing in a real-world scenario, and the findings
demonstrate the system&apos;s potential for real-world deployability. We anticipate
this work will address the current gaps in research for fairness and facilitate
the operationalisation of fairness in machine learning systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jessica Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huaming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jun Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choo_K/0/1/0/all/0/1&quot;&gt;Kim-Kwang Raymond Choo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16760">
<title>The Fourth International Verification of Neural Networks Competition (VNN-COMP 2023): Summary and Results. (arXiv:2312.16760v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16760</link>
<description rdf:parseType="Literal">&lt;p&gt;This report summarizes the 4th International Verification of Neural Networks
Competition (VNN-COMP 2023), held as a part of the 6th Workshop on Formal
Methods for ML-Enabled Autonomous Systems (FoMLAS), that was collocated with
the 35th International Conference on Computer-Aided Verification (CAV).
VNN-COMP is held annually to facilitate the fair and objective comparison of
state-of-the-art neural network verification tools, encourage the
standardization of tool interfaces, and bring together the neural network
verification community. To this end, standardized formats for networks (ONNX)
and specification (VNN-LIB) were defined, tools were evaluated on equal-cost
hardware (using an automatic evaluation pipeline based on AWS instances), and
tool parameters were chosen by the participants before the final test sets were
made public. In the 2023 iteration, 7 teams participated on a diverse set of 10
scored and 4 unscored benchmarks. This report summarizes the rules, benchmarks,
participating tools, results, and lessons learned from this iteration of this
competition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brix_C/0/1/0/all/0/1&quot;&gt;Christopher Brix&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bak_S/0/1/0/all/0/1&quot;&gt;Stanley Bak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Changliu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_T/0/1/0/all/0/1&quot;&gt;Taylor T. Johnson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16767">
<title>Adaptive Anytime Multi-Agent Path Finding Using Bandit-Based Large Neighborhood Search. (arXiv:2312.16767v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.16767</link>
<description rdf:parseType="Literal">&lt;p&gt;Anytime multi-agent path finding (MAPF) is a promising approach to scalable
path optimization in large-scale multi-agent systems. State-of-the-art anytime
MAPF is based on Large Neighborhood Search (LNS), where a fast initial solution
is iteratively optimized by destroying and repairing a fixed number of parts,
i.e., the neighborhood, of the solution, using randomized destroy heuristics
and prioritized planning. Despite their recent success in various MAPF
instances, current LNS-based approaches lack exploration and flexibility due to
greedy optimization with a fixed neighborhood size which can lead to low
quality solutions in general. So far, these limitations have been addressed
with extensive prior effort in tuning or offline machine learning beyond actual
planning. In this paper, we focus on online learning in LNS and propose
Bandit-based Adaptive LArge Neighborhood search Combined with Exploration
(BALANCE). BALANCE uses a bi-level multi-armed bandit scheme to adapt the
selection of destroy heuristics and neighborhood sizes on the fly during
search. We evaluate BALANCE on multiple maps from the MAPF benchmark set and
empirically demonstrate cost improvements of at least 50% compared to
state-of-the-art anytime MAPF in large-scale scenarios. We find that Thompson
Sampling performs particularly well compared to alternative multi-armed bandit
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phan_T/0/1/0/all/0/1&quot;&gt;Thomy Phan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Taoan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1&quot;&gt;Bistra Dilkina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koenig_S/0/1/0/all/0/1&quot;&gt;Sven Koenig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16788">
<title>Mitigating Degree Biases in Message Passing Mechanism by Utilizing Community Structures. (arXiv:2312.16788v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.16788</link>
<description rdf:parseType="Literal">&lt;p&gt;This study utilizes community structures to address node degree biases in
message-passing (MP) via learnable graph augmentations and novel graph
transformers. Recent augmentation-based methods showed that MP neural networks
often perform poorly on low-degree nodes, leading to degree biases due to a
lack of messages reaching low-degree nodes. Despite their success, most methods
use heuristic or uniform random augmentations, which are non-differentiable and
may not always generate valuable edges for learning representations. In this
paper, we propose Community-aware Graph Transformers, namely CGT, to learn
degree-unbiased representations based on learnable augmentations and graph
transformers by extracting within community structures. We first design a
learnable graph augmentation to generate more within-community edges connecting
low-degree nodes through edge perturbation. Second, we propose an improved
self-attention to learn underlying proximity and the roles of nodes within the
community. Third, we propose a self-supervised learning task that could learn
the representations to preserve the global graph structure and regularize the
graph augmentations. Extensive experiments on various benchmark datasets showed
CGT outperforms state-of-the-art baselines and significantly improves the node
degree biases. The source code is available at
https://github.com/NSLab-CUK/Community-aware-Graph-Transformer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_V/0/1/0/all/0/1&quot;&gt;Van Thuy Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_O/0/1/0/all/0/1&quot;&gt;O-Joun Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1907.05861">
<title>Adaptive Thompson Sampling Stacks for Memory Bounded Open-Loop Planning. (arXiv:1907.05861v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1907.05861</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Stable Yet Memory Bounded Open-Loop (SYMBOL) planning, a general
memory bounded approach to partially observable open-loop planning. SYMBOL
maintains an adaptive stack of Thompson Sampling bandits, whose size is bounded
by the planning horizon and can be automatically adapted according to the
underlying domain without any prior domain knowledge beyond a generative model.
We empirically test SYMBOL in four large POMDP benchmark problems to
demonstrate its effectiveness and robustness w.r.t. the choice of
hyperparameters and evaluate its adaptive memory consumption. We also compare
its performance with other open-loop planning algorithms and POMCP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phan_T/0/1/0/all/0/1&quot;&gt;Thomy Phan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabor_T/0/1/0/all/0/1&quot;&gt;Thomas Gabor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_R/0/1/0/all/0/1&quot;&gt;Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roch_C/0/1/0/all/0/1&quot;&gt;Christoph Roch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linnhoff_Popien_C/0/1/0/all/0/1&quot;&gt;Claudia Linnhoff-Popien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.15677">
<title>Augmentation-Aware Self-Supervision for Data-Efficient GAN Training. (arXiv:2205.15677v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.15677</link>
<description rdf:parseType="Literal">&lt;p&gt;Training generative adversarial networks (GANs) with limited data is
challenging because the discriminator is prone to overfitting. Previously
proposed differentiable augmentation demonstrates improved data efficiency of
training GANs. However, the augmentation implicitly introduces undesired
invariance to augmentation for the discriminator since it ignores the change of
semantics in the label space caused by data transformation, which may limit the
representation learning ability of the discriminator and ultimately affect the
generative modeling performance of the generator. To mitigate the negative
impact of invariance while inheriting the benefits of data augmentation, we
propose a novel augmentation-aware self-supervised discriminator that predicts
the augmentation parameter of the augmented data. Particularly, the prediction
targets of real data and generated data are required to be distinguished since
they are different during training. We further encourage the generator to
adversarially learn from the self-supervised discriminator by generating
augmentation-predictable real and not fake data. This formulation connects the
learning objective of the generator and the arithmetic $-$ harmonic mean
divergence under certain assumptions. We compare our method with
state-of-the-art (SOTA) methods using the class-conditional BigGAN and
unconditional StyleGAN2 architectures on data-limited CIFAR-10, CIFAR-100,
FFHQ, LSUN-Cat, and five low-shot datasets. Experimental results demonstrate
significant improvements of our method over SOTA methods in training
data-efficient GANs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1&quot;&gt;Liang Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1&quot;&gt;Qi Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yige Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Songtao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chongyang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Siyuan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1&quot;&gt;Pengfei Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhongyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Huawei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xueqi Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.14370">
<title>Few-shot News Recommendation via Cross-lingual Transfer. (arXiv:2207.14370v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2207.14370</link>
<description rdf:parseType="Literal">&lt;p&gt;The cold-start problem has been commonly recognized in recommendation systems
and studied by following a general idea to leverage the abundant interaction
records of warm users to infer the preference of cold users. However, the
performance of these solutions is limited by the amount of records available
from warm users to use. Thus, building a recommendation system based on few
interaction records from a few users still remains a challenging problem for
unpopular or early-stage recommendation platforms. This paper focuses on
solving the few-shot recommendation problem for news recommendation based on
two observations. First, news at different platforms (even in different
languages) may share similar topics. Second, the user preference over these
topics is transferable across different platforms. Therefore, we propose to
solve the few-shot news recommendation problem by transferring the user-news
preference from a many-shot source domain to a few-shot target domain. To
bridge two domains that are even in different languages and without any
overlapping users and news, we propose a novel unsupervised cross-lingual
transfer model as the news encoder that aligns semantically similar news in two
domains. A user encoder is constructed on top of the aligned news encoding and
transfers the user preference from the source to target domain. Experimental
results on two real-world news recommendation datasets show the superior
performance of our proposed method on addressing few-shot news recommendation,
comparing to the baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1&quot;&gt;Taicheng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lu Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shihada_B/0/1/0/all/0/1&quot;&gt;Basem Shihada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangliang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.09141">
<title>&quot;Guess what I&apos;m doing&quot;: Extending legibility to sequential decision tasks. (arXiv:2209.09141v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2209.09141</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we investigate the notion of legibility in sequential decision
tasks under uncertainty. Previous works that extend legibility to scenarios
beyond robot motion either focus on deterministic settings or are
computationally too expensive. Our proposed approach, dubbed PoL-MDP, is able
to handle uncertainty while remaining computationally tractable. We establish
the advantages of our approach against state-of-the-art approaches in several
simulated scenarios of different complexity. We also showcase the use of our
legible policies as demonstrations for an inverse reinforcement learning agent,
establishing their superiority against the commonly used demonstrations based
on the optimal policy. Finally, we assess the legibility of our computed
policies through a user study where people are asked to infer the goal of a
mobile robot following a legible policy by observing its actions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faria_M/0/1/0/all/0/1&quot;&gt;Miguel Faria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melo_F/0/1/0/all/0/1&quot;&gt;Francisco S. Melo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paiva_A/0/1/0/all/0/1&quot;&gt;Ana Paiva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.08303">
<title>Improving Radiology Summarization with Radiograph and Anatomy Prompts. (arXiv:2210.08303v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.08303</link>
<description rdf:parseType="Literal">&lt;p&gt;The impression is crucial for the referring physicians to grasp key
information since it is concluded from the findings and reasoning of
radiologists. To alleviate the workload of radiologists and reduce repetitive
human labor in impression writing, many researchers have focused on automatic
impression generation. However, recent works on this task mainly summarize the
corresponding findings and pay less attention to the radiology images. In
clinical, radiographs can provide more detailed valuable observations to
enhance radiologists&apos; impression writing, especially for complicated cases.
Besides, each sentence in findings usually focuses on single anatomy, so they
only need to be matched to corresponding anatomical regions instead of the
whole image, which is beneficial for textual and visual features alignment.
Therefore, we propose a novel anatomy-enhanced multimodal model to promote
impression generation. In detail, we first construct a set of rules to extract
anatomies and put these prompts into each sentence to highlight anatomy
characteristics. Then, two separate encoders are applied to extract features
from the radiograph and findings. Afterward, we utilize a contrastive learning
module to align these two representations at the overall level and use a
co-attention to fuse them at the sentence level with the help of
anatomy-enhanced sentence representation. Finally, the decoder takes the fused
information as the input to generate impressions. The experimental results on
two benchmark datasets confirm the effectiveness of the proposed method, which
achieves state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jinpeng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhihong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xiang Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1&quot;&gt;Tsung-Hui Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.08843">
<title>Improving Speech Emotion Recognition with Unsupervised Speaking Style Transfer. (arXiv:2211.08843v3 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2211.08843</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans can effortlessly modify various prosodic attributes, such as the
placement of stress and the intensity of sentiment, to convey a specific
emotion while maintaining consistent linguistic content. Motivated by this
capability, we propose EmoAug, a novel style transfer model designed to enhance
emotional expression and tackle the data scarcity issue in speech emotion
recognition tasks. EmoAug consists of a semantic encoder and a paralinguistic
encoder that represent verbal and non-verbal information respectively.
Additionally, a decoder reconstructs speech signals by conditioning on the
aforementioned two information flows in an unsupervised fashion. Once training
is completed, EmoAug enriches expressions of emotional speech with different
prosodic attributes, such as stress, rhythm and intensity, by feeding different
styles into the paralinguistic encoder. EmoAug enables us to generate similar
numbers of samples for each class to tackle the data imbalance issue as well.
Experimental results on the IEMOCAP dataset demonstrate that EmoAug can
successfully transfer different speaking styles while retaining the speaker
identity and semantic content. Furthermore, we train a SER model with data
augmented by EmoAug and show that the augmented model not only surpasses the
state-of-the-art supervised and self-supervised methods but also overcomes
overfitting problems caused by data imbalance. Some audio samples can be found
on our demo website.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1&quot;&gt;Leyuan Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1&quot;&gt;Cornelius Weber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_P/0/1/0/all/0/1&quot;&gt;Pengcheng Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Taihao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1&quot;&gt;Stefan Wermter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.02288">
<title>gRoMA: a Tool for Measuring the Global Robustness of Deep Neural Networks. (arXiv:2301.02288v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.02288</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) are at the forefront of cutting-edge technology,
and have been achieving remarkable performance in a variety of complex tasks.
Nevertheless, their integration into safety-critical systems, such as in the
aerospace or automotive domains, poses a significant challenge due to the
threat of adversarial inputs: perturbations in inputs that might cause the DNN
to make grievous mistakes. Multiple studies have demonstrated that even modern
DNNs are susceptible to adversarial inputs, and this risk must thus be measured
and mitigated to allow the deployment of DNNs in critical settings. Here, we
present gRoMA (global Robustness Measurement and Assessment), an innovative and
scalable tool that implements a probabilistic approach to measure the global
categorial robustness of a DNN. Specifically, gRoMA measures the probability of
encountering adversarial inputs for a specific output category. Our tool
operates on pre-trained, black-box classification DNNs, and generates input
samples belonging to an output category of interest. It measures the DNN&apos;s
susceptibility to adversarial inputs around these inputs, and aggregates the
results to infer the overall global categorial robustness of the DNN up to some
small bounded statistical error.
&lt;/p&gt;
&lt;p&gt;We evaluate our tool on the popular Densenet DNN model over the CIFAR10
dataset. Our results reveal significant gaps in the robustness of the different
output categories. This experiment demonstrates the usefulness and scalability
of our approach and its potential for allowing DNNs to be deployed within
critical systems of interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_N/0/1/0/all/0/1&quot;&gt;Natan Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yerushalmi_R/0/1/0/all/0/1&quot;&gt;Raz Yerushalmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1&quot;&gt;Guy Katz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.07629">
<title>Generalisation Through Negation and Predicate Invention. (arXiv:2301.07629v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2301.07629</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to generalise from a small number of examples is a fundamental
challenge in machine learning. To tackle this challenge, we introduce an
inductive logic programming (ILP) approach that combines negation and predicate
invention. Combining these two features allows an ILP system to generalise
better by learning rules with universally quantified body-only variables. We
implement our idea in NOPI, which can learn normal logic programs with
predicate invention, including Datalog programs with stratified negation. Our
experimental results on multiple domains show that our approach can improve
predictive accuracies and learning times.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cerna_D/0/1/0/all/0/1&quot;&gt;David M. Cerna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cropper_A/0/1/0/all/0/1&quot;&gt;Andrew Cropper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10405">
<title>Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v8 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10405</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently decades have witnessed the empirical success of framing Knowledge
Graph (KG) embeddings via language models. However, language model-based KG
embeddings are usually deployed as static artifacts, making them difficult to
modify post-deployment without re-training after deployment. To address this
issue, we propose a new task of editing language model-based KG embeddings in
this paper. This task is designed to facilitate rapid, data-efficient updates
to KG embeddings without compromising the performance of other aspects. We
build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and
evaluate several knowledge editing baselines demonstrating the limited ability
of previous models to handle the proposed challenging task. We further propose
a simple yet strong baseline dubbed KGEditor, which utilizes additional
parametric layers of the hypernetwork to edit/add facts. Our comprehensive
experimental results reveal that KGEditor excels in updating specific facts
without impacting the overall performance, even when faced with limited
training resources. Code and datasets are available in
https://github.com/zjunlp/PromptKG/tree/main/deltaKG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Siyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1&quot;&gt;Bozhong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qingbing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11048">
<title>Adversarial Model for Offline Reinforcement Learning. (arXiv:2302.11048v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11048</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel model-based offline Reinforcement Learning (RL) framework,
called Adversarial Model for Offline Reinforcement Learning (ARMOR), which can
robustly learn policies to improve upon an arbitrary reference policy
regardless of data coverage. ARMOR is designed to optimize policies for the
worst-case performance relative to the reference policy through adversarially
training a Markov decision process model. In theory, we prove that ARMOR, with
a well-tuned hyperparameter, can compete with the best policy within data
coverage when the reference policy is supported by the data. At the same time,
ARMOR is robust to hyperparameter choices: the policy learned by ARMOR, with
&quot;any&quot; admissible hyperparameter, would never degrade the performance of the
reference policy, even when the reference policy is not covered by the dataset.
To validate these properties in practice, we design a scalable implementation
of ARMOR, which by adversarial training, can optimize policies without using
model ensembles in contrast to typical model-based methods. We show that ARMOR
achieves competent performance with both state-of-the-art offline model-free
and model-based RL algorithms and can robustly improve the reference policy
over various hyperparameter choices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhardwaj_M/0/1/0/all/0/1&quot;&gt;Mohak Bhardwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1&quot;&gt;Tengyang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boots_B/0/1/0/all/0/1&quot;&gt;Byron Boots&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1&quot;&gt;Nan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Ching-An Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01234">
<title>Frauds Bargain Attack: Generating Adversarial Text Samples via Word Manipulation Process. (arXiv:2303.01234v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01234</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research has revealed that natural language processing (NLP) models
are vulnerable to adversarial examples. However, the current techniques for
generating such examples rely on deterministic heuristic rules, which fail to
produce optimal adversarial examples. In response, this study proposes a new
method called the Fraud&apos;s Bargain Attack (FBA), which uses a randomization
mechanism to expand the search space and produce high-quality adversarial
examples with a higher probability of success. FBA uses the Metropolis-Hasting
sampler, a type of Markov Chain Monte Carlo sampler, to improve the selection
of adversarial examples from all candidates generated by a customized
stochastic process called the Word Manipulation Process (WMP). The WMP method
modifies individual words in a contextually-aware manner through insertion,
removal, or substitution. Through extensive experiments, this study
demonstrates that FBA outperforms other methods in terms of attack success
rate, imperceptibility and sentence quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_M/0/1/0/all/0/1&quot;&gt;Mingze Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhensu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01325">
<title>A Pathway Towards Responsible AI Generated Content. (arXiv:2303.01325v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01325</link>
<description rdf:parseType="Literal">&lt;p&gt;AI Generated Content (AIGC) has received tremendous attention within the past
few years, with content generated in the format of image, text, audio, video,
etc. Meanwhile, AIGC has become a double-edged sword and recently received much
criticism regarding its responsible usage. In this article, we focus on 8 main
concerns that may hinder the healthy development and deployment of AIGC in
practice, including risks from (1) privacy; (2) bias, toxicity, misinformation;
(3) intellectual property (IP); (4) robustness; (5) open source and
explanation; (6) technology abuse; (7) consent, credit, and compensation; (8)
environment. Additionally, we provide insights into the promising directions
for tackling these risks while constructing generative models, enabling AIGC to
be used more responsibly to truly benefit society.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1&quot;&gt;Lingjuan Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.02862">
<title>EvHandPose: Event-based 3D Hand Pose Estimation with Sparse Supervision. (arXiv:2303.02862v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.02862</link>
<description rdf:parseType="Literal">&lt;p&gt;Event camera shows great potential in 3D hand pose estimation, especially
addressing the challenges of fast motion and high dynamic range in a low-power
way. However, due to the asynchronous differential imaging mechanism, it is
challenging to design event representation to encode hand motion information
especially when the hands are not moving (causing motion ambiguity), and it is
infeasible to fully annotate the temporally dense event stream. In this paper,
we propose EvHandPose with novel hand flow representations in Event-to-Pose
module for accurate hand pose estimation and alleviating the motion ambiguity
issue. To solve the problem under sparse annotation, we design contrast
maximization and hand-edge constraints in Pose-to-IWE (Image with Warped
Events) module and formulate EvHandPose in a weakly-supervision framework. We
further build EvRealHands, the first large-scale real-world event-based hand
pose dataset on several challenging scenes to bridge the real-synthetic domain
gap. Experiments on EvRealHands demonstrate that EvHandPose outperforms
previous event-based methods under all evaluation scenes, achieves accurate and
stable hand pose estimation with high temporal resolution in fast motion and
strong light scenes compared with RGB-based methods, generalizes well to
outdoor scenes and another type of event camera, and shows the potential for
the hand gesture recognition task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jianping Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiahe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Baowen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1&quot;&gt;Xiaoming Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Boxin Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12040">
<title>Roots and Requirements for Collaborative AIs. (arXiv:2303.12040v6 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12040</link>
<description rdf:parseType="Literal">&lt;p&gt;The vision of AI collaborators is a staple of mythology and science fiction,
where artificial agents with special talents assist human partners and teams.
In this dream, sophisticated AIs understand nuances of collaboration and human
communication. The AI as collaborator dream is different from computer tools
that augment human intelligence (IA) or intermediate human collaboration. Such
tools have their roots in the 1960s and helped to drive an information
technology revolution. They can be useful but they are not intelligent and do
not collaborate as effectively as skilled people. With the increase of hybrid
and remote work since the COVID pandemic, the benefits and requirements for
better coordination, collaboration, and communication are becoming a hot topic
in the workplace. Employers and workers face choices and trade-offs as they
negotiate the options for working from home versus working at the office. Many
factors such as the high costs of homes near employers are impeding a mass
return to the office. Government advisory groups and leaders in AI have
advocated for years that AIs should be transparent and effective collaborators.
Nonetheless, robust AIs that collaborate like talented people remain out of
reach. Are AI teammates part of a solution? How artificially intelligent (AI)
could and should they be? This position paper reviews the arc of technology and
public calls for human-machine teaming. It draws on earlier research in
psychology and the social sciences about what human-like collaboration
requires. This paper sets a context for a second science-driven paper that
advocates a radical shift in technology and methodology for creating resilient,
intelligent, and human-compatible AIs (Stefik &amp;amp; Price, 2023). The aspirational
goal is that such AIs would learn, share what they learn, and collaborate to
achieve high capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stefik_M/0/1/0/all/0/1&quot;&gt;Mark Stefik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.14582">
<title>Identification of Negative Transfers in Multitask Learning Using Surrogate Models. (arXiv:2303.14582v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.14582</link>
<description rdf:parseType="Literal">&lt;p&gt;Multitask learning is widely used in practice to train a low-resource target
task by augmenting it with multiple related source tasks. Yet, naively
combining all the source tasks with a target task does not always improve the
prediction performance for the target task due to negative transfers. Thus, a
critical problem in multitask learning is identifying subsets of source tasks
that would benefit the target task. This problem is computationally challenging
since the number of subsets grows exponentially with the number of source
tasks; efficient heuristics for subset selection do not always capture the
relationship between task subsets and multitask learning performances. In this
paper, we introduce an efficient procedure to address this problem via
surrogate modeling. In surrogate modeling, we sample (random) subsets of source
tasks and precompute their multitask learning performances. Then, we
approximate the precomputed performances with a linear regression model that
can also predict the multitask performance of unseen task subsets. We show
theoretically and empirically that fitting this model only requires sampling
linearly many subsets in the number of source tasks. The fitted model provides
a relevance score between each source and target task. We use the relevance
scores to perform subset selection for multitask learning by thresholding.
Through extensive experiments, we show that our approach predicts negative
transfers from multiple source tasks to target tasks much more accurately than
existing task affinity measures. Additionally, we demonstrate that for several
weak supervision datasets, our approach consistently improves upon existing
optimization methods for multitask learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dongyue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Huy L. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongyang R. Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09870">
<title>Heterogeneous-Agent Reinforcement Learning. (arXiv:2304.09870v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09870</link>
<description rdf:parseType="Literal">&lt;p&gt;The necessity for cooperation among intelligent machines has popularised
cooperative multi-agent reinforcement learning (MARL) in AI research. However,
many research endeavours heavily rely on parameter sharing among agents, which
confines them to only homogeneous-agent setting and leads to training
instability and lack of convergence guarantees. To achieve effective
cooperation in the general heterogeneous-agent setting, we propose
Heterogeneous-Agent Reinforcement Learning (HARL) algorithms that resolve the
aforementioned issues. Central to our findings are the multi-agent advantage
decomposition lemma and the sequential update scheme. Based on these, we
develop the provably correct Heterogeneous-Agent Trust Region Learning (HATRL),
and derive HATRPO and HAPPO by tractable approximations. Furthermore, we
discover a novel framework named Heterogeneous-Agent Mirror Learning (HAML),
which strengthens theoretical guarantees for HATRPO and HAPPO and provides a
general template for cooperative MARL algorithmic designs. We prove that all
algorithms derived from HAML inherently enjoy monotonic improvement of joint
return and convergence to Nash Equilibrium. As its natural outcome, HAML
validates more novel algorithms in addition to HATRPO and HAPPO, including
HAA2C, HADDPG, and HATD3, which generally outperform their existing
MA-counterparts. We comprehensively test HARL algorithms on six challenging
benchmarks and demonstrate their superior effectiveness and stability for
coordinating heterogeneous agents compared to strong baselines such as MAPPO
and QMIX.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuba_J/0/1/0/all/0/1&quot;&gt;Jakub Grudzien Kuba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1&quot;&gt;Xidong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Siyi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jiaming Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yaodong Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.00140">
<title>Space reduction techniques for the $3$-wise Kemeny problem. (arXiv:2305.00140v2 [cs.DM] UPDATED)</title>
<link>http://arxiv.org/abs/2305.00140</link>
<description rdf:parseType="Literal">&lt;p&gt;Kemeny&apos;s rule is one of the most studied and well-known voting schemes with
various important applications in computational social choice and biology.
Recently, Kemeny&apos;s rule was generalized via a set-wise approach by Gilbert et.
al. This paradigm presents interesting advantages in comparison with Kemeny&apos;s
rule since not only pairwise comparisons but also the discordance between the
winners of subsets of three alternatives are also taken into account in the
definition of the $3$-wise Kendall-tau distance between two rankings. In spite
of the NP-hardness of the 3-wise Kemeny problem which consists of computing the
set of $3$-wise consensus rankings, namely rankings whose total $3$-wise
Kendall-tau distance to a given voting profile is minimized, we establish in
this paper several generalizations of the Major Order Theorems, as obtained by
Milosz and Hamel for Kemeny&apos;s rule, for the $3$-wise Kemeny voting schemes to
achieve a substantial search space reduction by efficiently determining in
polynomial time the relative orders of pairs of alternatives. Essentially, our
theorems quantify precisely the nontrivial property that if the preference for
an alternative over another one in an election is strong enough, not only in
the head-to-head competition but even when taking into account one or two more
alternatives, then the relative order of these two alternatives in all $3$-wise
consensus rankings must be as expected. As an application, we also obtain an
improvement of the Major Order Theorems for Kememy&apos;s rule. Moreover, we show
that the well-known $3/4$-majority rule of Betzler et al. for Kemeny&apos;s rule is
only valid in general for elections with no more than $5$ alternatives with
respect to the $3$-wise Kemeny scheme. Several simulations and tests of our
algorithms on real-world and uniform data are provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phung_X/0/1/0/all/0/1&quot;&gt;Xuan Kien Phung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamel_S/0/1/0/all/0/1&quot;&gt;Sylvie Hamel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03236">
<title>A Survey on Out-of-Distribution Detection in NLP. (arXiv:2305.03236v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03236</link>
<description rdf:parseType="Literal">&lt;p&gt;Out-of-distribution (OOD) detection is essential for the reliable and safe
deployment of machine learning systems in the real world. Great progress has
been made over the past years. This paper presents the first review of recent
advances in OOD detection with a particular focus on natural language
processing approaches. First, we provide a formal definition of OOD detection
and discuss several related fields. We then categorize recent algorithms into
three classes according to the data they used: (1) OOD data available, (2) OOD
data unavailable + in-distribution (ID) label available, and (3) OOD data
unavailable + ID label unavailable. Third, we introduce datasets, applications,
and metrics. Finally, we summarize existing work and present potential future
research topics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1&quot;&gt;Hao Lang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yinhe Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yixuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yongbin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08777">
<title>Question-Answering System Extracts Information on Injection Drug Use from Clinical Notes. (arXiv:2305.08777v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08777</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: Injection drug use (IDU) is a dangerous health behavior that
increases mortality and morbidity. Identifying IDU early and initiating harm
reduction interventions can benefit individuals at risk. However, extracting
IDU behaviors from patients&apos; electronic health records (EHR) is difficult
because there is no International Classification of Disease (ICD) code and the
only place IDU information can be indicated is unstructured free-text clinical
notes. Although natural language processing can efficiently extract this
information from unstructured data, there are no validated tools. Methods: To
address this gap in clinical information, we design and demonstrate a
question-answering (QA) framework to extract information on IDU from clinical
notes. Our framework involves two main steps: (1) generating a gold-standard QA
dataset and (2) developing and testing the QA model. We utilize 2323 clinical
notes of 1145 patients sourced from the VA Corporate Data Warehouse to
construct the gold-standard dataset for developing and evaluating the QA model.
We also demonstrate the QA model&apos;s ability to extract IDU-related information
on temporally out-of-distribution data. Results: Here we show that for a strict
match between gold-standard and predicted answers, the QA model achieves 51.65%
F1 score. For a relaxed match between the gold-standard and predicted answers,
the QA model obtains 78.03% F1 score, along with 85.38% Precision and 79.02%
Recall scores. Moreover, the QA model demonstrates consistent performance when
subjected to temporally out-of-distribution data. Conclusions: Our study
introduces a QA framework designed to extract IDU information from clinical
notes, aiming to enhance the accurate and efficient detection of people who
inject drugs, extract relevant information, and ultimately facilitate informed
patient care.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahbub_M/0/1/0/all/0/1&quot;&gt;Maria Mahbub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goethert_I/0/1/0/all/0/1&quot;&gt;Ian Goethert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danciu_I/0/1/0/all/0/1&quot;&gt;Ioana Danciu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knight_K/0/1/0/all/0/1&quot;&gt;Kathryn Knight&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1&quot;&gt;Sudarshan Srinivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamang_S/0/1/0/all/0/1&quot;&gt;Suzanne Tamang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rozenberg_Ben_Dror_K/0/1/0/all/0/1&quot;&gt;Karine Rozenberg-Ben-Dror&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solares_H/0/1/0/all/0/1&quot;&gt;Hugo Solares&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martins_S/0/1/0/all/0/1&quot;&gt;Susana Martins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trafton_J/0/1/0/all/0/1&quot;&gt;Jodie Trafton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Begoli_E/0/1/0/all/0/1&quot;&gt;Edmon Begoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peterson_G/0/1/0/all/0/1&quot;&gt;Gregory Peterson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09480">
<title>Cross-Gate MLP with Protein Complex Invariant Embedding is A One-Shot Antibody Designer. (arXiv:2305.09480v4 [q-bio.BM] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09480</link>
<description rdf:parseType="Literal">&lt;p&gt;Antibodies are crucial proteins produced by the immune system in response to
foreign substances or antigens. The specificity of an antibody is determined by
its complementarity-determining regions (CDRs), which are located in the
variable domains of the antibody chains and form the antigen-binding site.
Previous studies have utilized complex techniques to generate CDRs, but they
suffer from inadequate geometric modeling. Moreover, the common iterative
refinement strategies lead to an inefficient inference. In this paper, we
propose a \textit{simple yet effective} model that can co-design 1D sequences
and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple
the antibody CDR design problem into two stages: (i) geometric modeling of
protein complex structures and (ii) sequence-structure co-learning. We develop
a novel macromolecular structure invariant embedding, typically for protein
complexes, that captures both intra- and inter-component interactions among the
backbone atoms, including C$\alpha$, N, C, and O atoms, to achieve
comprehensive geometric modeling. Then, we introduce a simple cross-gate MLP
for sequence-structure co-learning, allowing sequence and structure
representations to implicitly refine each other. This enables our model to
design desired sequences and structures in a one-shot manner. Extensive
experiments are conducted to evaluate our results at both the sequence and
structure levels, which demonstrate that our model achieves superior
performance compared to the state-of-the-art antibody CDR design methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Cheng Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lirong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xia_J/0/1/0/all/0/1&quot;&gt;Jun Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jiangbin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xihong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Bozhen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Stan Z. Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18365">
<title>What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks. (arXiv:2305.18365v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18365</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) with strong abilities in natural language
processing tasks have emerged and have been applied in various kinds of areas
such as science, finance and software engineering. However, the capability of
LLMs to advance the field of chemistry remains unclear. In this paper, rather
than pursuing state-of-the-art performance, we aim to evaluate capabilities of
LLMs in a wide range of tasks across the chemistry domain. We identify three
key chemistry-related capabilities including understanding, reasoning and
explaining to explore in LLMs and establish a benchmark containing eight
chemistry tasks. Our analysis draws on widely recognized datasets facilitating
a broad exploration of the capacities of LLMs within the context of practical
chemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are
evaluated for each chemistry task in zero-shot and few-shot in-context learning
settings with carefully selected demonstration examples and specially crafted
prompts. Our investigation found that GPT-4 outperformed other models and LLMs
exhibit different competitive levels in eight chemistry tasks. In addition to
the key findings from the comprehensive benchmark analysis, our work provides
insights into the limitation of current LLMs and the impact of in-context
learning settings on LLMs&apos; performance across various chemistry tasks. The code
and datasets used in this study are available at
https://github.com/ChemFoundationModels/ChemLLMBench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1&quot;&gt;Taicheng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1&quot;&gt;Kehan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nan_B/0/1/0/all/0/1&quot;&gt;Bozhao Nan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1&quot;&gt;Zhenwen Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhichun Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chawla_N/0/1/0/all/0/1&quot;&gt;Nitesh V. Chawla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiest_O/0/1/0/all/0/1&quot;&gt;Olaf Wiest&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangliang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00789">
<title>Cross-Lingual Transfer Learning for Low-Resource Speech Translation. (arXiv:2306.00789v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00789</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper presents a novel three-step transfer learning framework for
enhancing cross-lingual transfer from high- to low-resource languages in the
downstream application of Automatic Speech Translation. The approach integrates
a semantic knowledge-distillation step into the existing two-step cross-lingual
transfer learning framework XLS-R. This extra step aims to encode semantic
knowledge in the multilingual speech encoder pre-trained via Self-Supervised
Learning using unlabeled speech. Our proposed three-step cross-lingual transfer
learning framework addresses the large cross-lingual transfer gap (TRFGap)
observed in the XLS-R framework between high-resource and low-resource
languages. We validate our proposal through extensive experiments and
comparisons on the CoVoST-2 benchmark, showing significant improvements in
translation performance, especially for low-resource languages, and a notable
reduction in the TRFGap.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khurana_S/0/1/0/all/0/1&quot;&gt;Sameer Khurana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dawalatabad_N/0/1/0/all/0/1&quot;&gt;Nauman Dawalatabad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laurent_A/0/1/0/all/0/1&quot;&gt;Antoine Laurent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vicente_L/0/1/0/all/0/1&quot;&gt;Luis Vicente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gimeno_P/0/1/0/all/0/1&quot;&gt;Pablo Gimeno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mingote_V/0/1/0/all/0/1&quot;&gt;Victoria Mingote&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1&quot;&gt;Jonathan Le Roux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1&quot;&gt;James Glass&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03502">
<title>Russo-Ukrainian War: Prediction and explanation of Twitter suspension. (arXiv:2306.03502v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03502</link>
<description rdf:parseType="Literal">&lt;p&gt;On 24 February 2022, Russia invaded Ukraine, starting what is now known as
the Russo-Ukrainian War, initiating an online discourse on social media.
Twitter as one of the most popular SNs, with an open and democratic character,
enables a transparent discussion among its large user base. Unfortunately, this
often leads to Twitter&apos;s policy violations, propaganda, abusive actions, civil
integrity violation, and consequently to user accounts&apos; suspension and
deletion. This study focuses on the Twitter suspension mechanism and the
analysis of shared content and features of the user accounts that may lead to
this. Toward this goal, we have obtained a dataset containing 107.7M tweets,
originating from 9.8 million users, using Twitter API. We extract the
categories of shared content of the suspended accounts and explain their
characteristics, through the extraction of text embeddings in junction with
cosine similarity clustering. Our results reveal scam campaigns taking
advantage of trending topics regarding the Russia-Ukrainian conflict for
Bitcoin and Ethereum fraud, spam, and advertisement campaigns. Additionally, we
apply a machine learning methodology including a SHapley Additive
explainability model to understand and explain how user accounts get suspended.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shevtsov_A/0/1/0/all/0/1&quot;&gt;Alexander Shevtsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antonakaki_D/0/1/0/all/0/1&quot;&gt;Despoina Antonakaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamprou_I/0/1/0/all/0/1&quot;&gt;Ioannis Lamprou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kontogiorgakis_I/0/1/0/all/0/1&quot;&gt;Ioannis Kontogiorgakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pratikakis_P/0/1/0/all/0/1&quot;&gt;Polyvios Pratikakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ioannidis_S/0/1/0/all/0/1&quot;&gt;Sotiris Ioannidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04047">
<title>CAVEN: An Embodied Conversational Agent for Efficient Audio-Visual Navigation in Noisy Environments. (arXiv:2306.04047v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04047</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio-visual navigation of an agent towards locating an audio goal is a
challenging task especially when the audio is sporadic or the environment is
noisy. In this paper, we present CAVEN, a Conversation-based Audio-Visual
Embodied Navigation framework in which the agent may interact with a
human/oracle for solving the task of navigating to an audio goal. Specifically,
CAVEN is modeled as a budget-aware partially observable semi-Markov decision
process that implicitly learns the uncertainty in the audio-based navigation
policy to decide when and how the agent may interact with the oracle. Our CAVEN
agent can engage in fully-bidirectional natural language conversations by
producing relevant questions and interpret free-form, potentially noisy
responses from the oracle based on the audio-visual context. To enable such a
capability, CAVEN is equipped with: (i) a trajectory forecasting network that
is grounded in audio-visual cues to produce a potential trajectory to the
estimated goal, and (ii) a natural language based question generation and
reasoning network to pose an interactive question to the oracle or interpret
the oracle&apos;s response to produce navigation instructions. To train the
interactive modules, we present a large scale dataset: AVN-Instruct, based on
the Landmark-RxR dataset. To substantiate the usefulness of conversations, we
present experiments on the benchmark audio-goal task using the SoundSpaces
simulator under various noisy settings. Our results reveal that our
fully-conversational approach leads to nearly an order-of-magnitude improvement
in success rate, especially in localizing new sound sources and against methods
that only use uni-directional interaction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiulong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1&quot;&gt;Sudipta Paul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_M/0/1/0/all/0/1&quot;&gt;Moitreya Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1&quot;&gt;Anoop Cherian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04607">
<title>GeoDiffusion: Text-Prompted Geometric Control for Object Detection Data Generation. (arXiv:2306.04607v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04607</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have attracted significant attention due to the remarkable
ability to create content and generate data for tasks like image
classification. However, the usage of diffusion models to generate the
high-quality object detection data remains an underexplored area, where not
only image-level perceptual quality but also geometric conditions such as
bounding boxes and camera views are essential. Previous studies have utilized
either copy-paste synthesis or layout-to-image (L2I) generation with
specifically designed modules to encode the semantic layouts. In this paper, we
propose the GeoDiffusion, a simple framework that can flexibly translate
various geometric conditions into text prompts and empower pre-trained
text-to-image (T2I) diffusion models for high-quality detection data
generation. Unlike previous L2I methods, our GeoDiffusion is able to encode not
only the bounding boxes but also extra geometric conditions such as camera
views in self-driving scenes. Extensive experiments demonstrate GeoDiffusion
outperforms previous L2I methods while maintaining 4x training time faster. To
the best of our knowledge, this is the first work to adopt diffusion models for
layout-to-image generation with geometric conditions and demonstrate that
L2I-generated images can be beneficial for improving the performance of object
detectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yibo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1&quot;&gt;Lanqing Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1&quot;&gt;Dit-Yan Yeung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17052">
<title>Safe Model-Based Multi-Agent Mean-Field Reinforcement Learning. (arXiv:2306.17052v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17052</link>
<description rdf:parseType="Literal">&lt;p&gt;Many applications, e.g., in shared mobility, require coordinating a large
number of agents. Mean-field reinforcement learning addresses the resulting
scalability challenge by optimizing the policy of a representative agent
interacting with the infinite population of identical agents instead of
considering individual pairwise interactions. In this paper, we address an
important generalization where there exist global constraints on the
distribution of agents (e.g., requiring capacity constraints or minimum
coverage requirements to be met). We propose Safe-M$^3$-UCRL, the first
model-based mean-field reinforcement learning algorithm that attains safe
policies even in the case of unknown transitions. As a key ingredient, it uses
epistemic uncertainty in the transition model within a log-barrier approach to
ensure pessimistic constraints satisfaction with high probability. Beyond the
synthetic swarm motion benchmark, we showcase Safe-M$^3$-UCRL on the vehicle
repositioning problem faced by many shared mobility operators and evaluate its
performance through simulations built on vehicle trajectory data from a service
provider in Shenzhen. Our algorithm effectively meets the demand in critical
areas while ensuring service accessibility in regions with low demand.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jusup_M/0/1/0/all/0/1&quot;&gt;Matej Jusup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pasztor_B/0/1/0/all/0/1&quot;&gt;Barna P&amp;#xe1;sztor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janik_T/0/1/0/all/0/1&quot;&gt;Tadeusz Janik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kenan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corman_F/0/1/0/all/0/1&quot;&gt;Francesco Corman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1&quot;&gt;Andreas Krause&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogunovic_I/0/1/0/all/0/1&quot;&gt;Ilija Bogunovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10422">
<title>PreDiff: Precipitation Nowcasting with Latent Diffusion Models. (arXiv:2307.10422v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10422</link>
<description rdf:parseType="Literal">&lt;p&gt;Earth system forecasting has traditionally relied on complex physical models
that are computationally expensive and require significant domain expertise. In
the past decade, the unprecedented increase in spatiotemporal Earth observation
data has enabled data-driven forecasting models using deep learning techniques.
These models have shown promise for diverse Earth system forecasting tasks but
either struggle with handling uncertainty or neglect domain-specific prior
knowledge, resulting in averaging possible futures to blurred forecasts or
generating physically implausible predictions. To address these limitations, we
propose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1)
We develop PreDiff, a conditional latent diffusion model capable of
probabilistic forecasts. 2) We incorporate an explicit knowledge alignment
mechanism to align forecasts with domain-specific physical constraints. This is
achieved by estimating the deviation from imposed constraints at each denoising
step and adjusting the transition distribution accordingly. We conduct
empirical studies on two datasets: N-body MNIST, a synthetic dataset with
chaotic behavior, and SEVIR, a real-world precipitation nowcasting dataset.
Specifically, we impose the law of conservation of energy in N-body MNIST and
anticipated precipitation intensity in SEVIR. Experiments demonstrate the
effectiveness of PreDiff in handling uncertainty, incorporating domain-specific
prior knowledge, and generating forecasts that exhibit high operational
utility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zhihan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xingjian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Boran Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xiaoyong Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maddix_D/0/1/0/all/0/1&quot;&gt;Danielle Maddix&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuyang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13165">
<title>Investigating the Robustness of Sequential Recommender Systems Against Training Data Perturbations. (arXiv:2307.13165v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13165</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequential Recommender Systems (SRSs) are widely employed to model user
behavior over time. However, their robustness in the face of perturbations in
training data remains a largely understudied yet critical issue. A fundamental
challenge emerges in previous studies aimed at assessing the robustness of
SRSs: the Rank-Biased Overlap (RBO) similarity is not particularly suited for
this task as it is designed for infinite rankings of items and thus shows
limitations in real-world scenarios. For instance, it fails to achieve a
perfect score of 1 for two identical finite-length rankings. To address this
challenge, we introduce a novel contribution: Finite Rank-Biased Overlap
(FRBO), an enhanced similarity tailored explicitly for finite rankings. This
innovation facilitates a more intuitive evaluation in practical settings. In
pursuit of our goal, we empirically investigate the impact of removing items at
different positions within a temporally ordered sequence. We evaluate two
distinct SRS models across multiple datasets, measuring their performance using
metrics such as Normalized Discounted Cumulative Gain (NDCG) and Rank List
Sensitivity. Our results demonstrate that removing items at the end of the
sequence has a statistically significant impact on performance, with NDCG
decreasing up to 60%. Conversely, removing items from the beginning or middle
has no significant effect. These findings underscore the criticality of the
position of perturbed items in the training data. As we spotlight the
vulnerabilities inherent in current SRSs, we fervently advocate for intensified
research efforts to fortify their robustness against adversarial perturbations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Betello_F/0/1/0/all/0/1&quot;&gt;Filippo Betello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siciliano_F/0/1/0/all/0/1&quot;&gt;Federico Siciliano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1&quot;&gt;Pushkar Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1&quot;&gt;Fabrizio Silvestri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16082">
<title>EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction. (arXiv:2307.16082v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16082</link>
<description rdf:parseType="Literal">&lt;p&gt;Social platforms have emerged as crucial platforms for disseminating
information and discussing real-life social events, offering researchers an
excellent opportunity to design and implement novel event detection frameworks.
However, most existing approaches only exploit keyword burstiness or network
structures to detect unspecified events. Thus, they often need help identifying
unknown events regarding the challenging nature of events and social data.
Social data, e.g., tweets, is characterized by misspellings, incompleteness,
word sense ambiguation, irregular language, and variation in aspects of
opinions. Moreover, extracting discriminative features and patterns for
evolving events by exploiting the limited structural knowledge is almost
infeasible. To address these challenges, in this paper, we propose a novel
framework, namely EnrichEvent, that leverages the linguistic and contextual
representations of streaming social data. In particular, we leverage contextual
and linguistic knowledge to detect semantically related tweets and enhance the
effectiveness of the event detection approaches. Eventually, our proposed
framework produces cluster chains for each event to show the evolving variation
of the event through time. We conducted extensive experiments to evaluate our
framework, validating its high performance and effectiveness in detecting and
distinguishing unspecified social events.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esfahani_M/0/1/0/all/0/1&quot;&gt;Mohammadali Sefidi Esfahani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akbari_M/0/1/0/all/0/1&quot;&gt;Mohammad Akbari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04586">
<title>Bootstrapping Developmental AIs: From Simple Competences to Intelligent Human-Compatible AIs. (arXiv:2308.04586v14 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04586</link>
<description rdf:parseType="Literal">&lt;p&gt;Mainstream approaches for creating AIs include deep learning and generative
approaches (e.g., large language models) and manually constructed symbolic
approaches. These approaches have led to valuable AI systems and impressive
feats, but they can create risks when their operations affect people. Manually
constructed AIs are brittle even in circumscribed domains. Generative AIs make
strange mistakes and do not notice them. Today, these AIs cannot be instructed
easily, fail to use common sense, lack curiosity, and lack social alignment.
Developmental AI is a bootstrapping approach that uses embodied AIs. The AIs
start with innate competences and learn by interacting with their environment.
The AIs develop abilities in small steps along a bio-inspired trajectory.
Developmental AIs have shown capabilities for multimodal perception, object
recognition, and manipulation. Computational models for hierarchical planning,
abstraction discovery, curiosity, and language acquisition exist but need to be
adapted to an embodied approach. This research aims to produce AIs that learn
to communicate, establish common ground, read critically, consider the
provenance of information, test hypotheses, and collaborate. However,
developmental AI systems have not yet passed the abilities of young children.
They need to bridge competence gaps involving nonverbal communication, speech,
reading, and writing. Scaling to practical applications also requires reducing
hardware costs. This position paper lays out prospects, gaps, and challenges
for this approach. The ambition is to create data-rich experientially based
foundation models for human-compatible, resilient, and trustworthy AIs. The AIs
would learn, share what they learn, and collaborate to achieve high standards.
The approach would make AI technology more democratic and enable more people to
train, test, build on, and replicate AIs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stefik_M/0/1/0/all/0/1&quot;&gt;Mark Stefik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Price_R/0/1/0/all/0/1&quot;&gt;Robert Price&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11234">
<title>Traffic Flow Optimisation for Lifelong Multi-Agent Path Finding. (arXiv:2308.11234v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11234</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Agent Path Finding (MAPF) is a fundamental problem in robotics that
asks us to compute collision-free paths for a team of agents, all moving across
a shared map. Although many works appear on this topic, all current algorithms
struggle as the number of agents grows. The principal reason is that existing
approaches typically plan free-flow optimal paths, which creates congestion. To
tackle this issue we propose a new approach for MAPF where agents are guided to
their destination by following congestion-avoiding paths. We evaluate the idea
in two large-scale settings: one-shot MAPF, where each agent has a single
destination, and lifelong MAPF, where agents are continuously assigned new
tasks. For one-shot MAPF we show that our approach substantially improves
solution quality. For Lifelong MAPF we report large improvements in overall
throughput.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harabor_D/0/1/0/all/0/1&quot;&gt;Daniel Harabor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiaoyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stuckey_P/0/1/0/all/0/1&quot;&gt;Peter J. Stuckey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14165">
<title>Distributional Off-Policy Evaluation for Slate Recommendations. (arXiv:2308.14165v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14165</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommendation strategies are typically evaluated by using previously logged
data, employing off-policy evaluation methods to estimate their expected
performance. However, for strategies that present users with slates of multiple
items, the resulting combinatorial action space renders many of these methods
impractical. Prior work has developed estimators that leverage the structure in
slates to estimate the expected off-policy performance, but the estimation of
the entire performance distribution remains elusive. Estimating the complete
distribution allows for a more comprehensive evaluation of recommendation
strategies, particularly along the axes of risk and fairness that employ
metrics computable from the distribution. In this paper, we propose an
estimator for the complete off-policy performance distribution for slates and
establish conditions under which the estimator is unbiased and consistent. This
builds upon prior work on off-policy evaluation for slates and off-policy
distribution estimation in reinforcement learning. We validate the efficacy of
our method empirically on synthetic data as well as on a slate recommendation
simulator constructed from real-world data (MovieLens-20M). Our results show a
significant reduction in estimation variance and improved sample efficiency
over prior work across a range of slate structures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhari_S/0/1/0/all/0/1&quot;&gt;Shreyas Chaudhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arbour_D/0/1/0/all/0/1&quot;&gt;David Arbour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theocharous_G/0/1/0/all/0/1&quot;&gt;Georgios Theocharous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vlassis_N/0/1/0/all/0/1&quot;&gt;Nikos Vlassis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15030">
<title>SwapMoE: Efficient Memory-Constrained Serving of Large Sparse MoE Models via Dynamic Expert Pruning and Swapping. (arXiv:2308.15030v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15030</link>
<description rdf:parseType="Literal">&lt;p&gt;Mixture of experts (MoE) is a popular technique to improve capacity of large
models with conditionally-activated parallel neural network modules (experts).
Due to its remarkable scaling performance with sparse computation, it is widely
used in modern Large Language Models (LLMs) and Large Vision Models (LVMs).
However, serving such large models on edge devices is challenging due to memory
constraints. Typical solutions like memory swapping or weight pruning may lead
to significantly higher latency or severe accuracy loss.
&lt;/p&gt;
&lt;p&gt;In this paper, we introduce SwapMoE, a framework for efficient continuous
MoE-based large models serving with tunable memory budgets. The main idea of
SwapMoE is to keep a small dynamic set of important experts, namely Virtual
Experts, in the main memory for inference, while seamlessly maintaining how the
Virtual Experts map to the actual experts. We use a profiling-guided planner to
allocate the resources for SwapMoE that can fully utilize the memory budgets
and bandwidth, and an importance-aware scheduler to efficiently identify,
update, and use the Virtual Experts for accurate inference.
&lt;/p&gt;
&lt;p&gt;To evaluate SwapMoE, we conduct experiments on multiple edge devices with
state-of-the-art MoE-based Large Language Models and Large Vision Models. The
results demonstrate remarkable performance of SwapMoE under various memory
constraints. Specifically, SwapMoE can enable running large MoE models under
tight memory budgets with similar latency to pruned compact models, while with
significantly higher accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_R/0/1/0/all/0/1&quot;&gt;Rui Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanchun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1&quot;&gt;Qingtian Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weijun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1&quot;&gt;Linghe Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yunxin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15645">
<title>AskIt: Unified Programming Interface for Programming with Large Language Models. (arXiv:2308.15645v2 [cs.PL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15645</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) exhibit a unique phenomenon known as emergent
abilities, demonstrating adeptness across numerous tasks, from text
summarization to code generation. While these abilities open up novel avenues
in software design and crafting, their incorporation presents substantial
challenges. Developers face decisions regarding the use of LLMs for directly
performing tasks within applications as well as for generating and executing
code to accomplish these tasks. Moreover, effective prompt design becomes a
critical concern, given the necessity of extracting data from natural language
outputs. To address these complexities, this paper introduces AskIt, a
domain-specific language (DSL) specifically designed for LLMs. AskIt simplifies
LLM integration by providing a unified interface that not only allows for
direct task execution using LLMs but also supports the entire cycle of code
generation and execution. This dual capability is achieved through (1)
type-guided output control, (2) template-based function definitions, and (3)
prompt generation for both usage modes. Our evaluations underscore AskIt&apos;s
effectiveness. Across 50 tasks, AskIt generated concise prompts, achieving a
16.14 % reduction in prompt length compared to benchmarks. Additionally, by
enabling a seamless transition between using LLMs directly in applications and
for generating code, AskIt achieved significant efficiency improvements, as
observed in our GSM8K benchmark experiments. The implementations of AskIt in
TypeScript and Python are available at https://github.com/katsumiok/ts-askit
and https://github.com/katsumiok/pyaskit, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okuda_K/0/1/0/all/0/1&quot;&gt;Katsumi Okuda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amarasinghe_S/0/1/0/all/0/1&quot;&gt;Saman Amarasinghe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01824">
<title>Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon Decision-Making in Embodied AI. (arXiv:2310.01824v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01824</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Mini-BEHAVIOR, a novel benchmark for embodied AI that challenges
agents to use reasoning and decision-making skills to solve complex activities
that resemble everyday human challenges. The Mini-BEHAVIOR environment is a
fast, realistic Gridworld environment that offers the benefits of rapid
prototyping and ease of use while preserving a symbolic level of physical
realism and complexity found in complex embodied AI benchmarks. We introduce
key features such as procedural generation, to enable the creation of countless
task variations and support open-ended learning. Mini-BEHAVIOR provides
implementations of various household tasks from the original BEHAVIOR
benchmark, along with starter code for data collection and reinforcement
learning agent training. In essence, Mini-BEHAVIOR offers a fast, open-ended
benchmark for evaluating decision-making and planning solutions in embodied AI.
It serves as a user-friendly entry point for research and facilitates the
evaluation and development of solutions, simplifying their assessment and
development while advancing the field of embodied AI. Code is publicly
available at https://github.com/StanfordVL/mini_behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_E/0/1/0/all/0/1&quot;&gt;Emily Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jiaheng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhuoyi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruohan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1&quot;&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1&quot;&gt;Roberto Mart&amp;#xed;n-Mart&amp;#xed;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03149">
<title>Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03149</link>
<description rdf:parseType="Literal">&lt;p&gt;By now there is substantial evidence that deep learning models learn certain
human-interpretable features as part of their internal representations of data.
As having the right (or wrong) concepts is critical to trustworthy machine
learning systems, it is natural to ask which inputs from the model&apos;s original
training set were most important for learning a concept at a given layer. To
answer this, we combine data attribution methods with methods for probing the
concepts learned by a model. Training network and probe ensembles for two
concept datasets on a range of network layers, we use the recently developed
TRAK method for large-scale data attribution. We find some evidence for
convergence, where removing the 10,000 top attributing images for a concept and
retraining the model does not change the location of the concept in the network
nor the probing sparsity of the concept. This suggests that rather than being
highly dependent on a few specific examples, the features that inform the
development of a concept are spread in a more diffuse manner across its
exemplars, implying robustness in concept formation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konz_N/0/1/0/all/0/1&quot;&gt;Nicholas Konz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Godfrey_C/0/1/0/all/0/1&quot;&gt;Charles Godfrey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shapiro_M/0/1/0/all/0/1&quot;&gt;Madelyn Shapiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1&quot;&gt;Jonathan Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1&quot;&gt;Henry Kvinge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1&quot;&gt;Davis Brown&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08475">
<title>Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08475</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we focus on editing Multimodal Large Language Models (MLLMs).
Compared to editing single-modal LLMs, multimodal model editing is more
challenging, which demands a higher level of scrutiny and careful consideration
in the editing process. To facilitate research in this area, we construct a new
benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite
of innovative metrics for evaluation. We conduct comprehensive experiments
involving various model editing baselines and analyze the impact of editing
different components for multimodal LLMs. Empirically, we notice that previous
baselines can implement editing multimodal LLMs to some extent, but the effect
is still barely satisfactory, indicating the potential difficulty of this task.
We hope that our work can provide the NLP community with insights. Code and
dataset are available in https://github.com/zjunlp/EasyEdit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Siyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1&quot;&gt;Bozhong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qingbin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yongheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10062">
<title>A Comprehensive Evaluation of Tool-Assisted Generation Strategies. (arXiv:2310.10062v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10062</link>
<description rdf:parseType="Literal">&lt;p&gt;A growing area of research investigates augmenting language models with tools
(e.g., search engines, calculators) to overcome their shortcomings (e.g.,
missing or incorrect knowledge, incorrect logical inferences). Various few-shot
tool-usage strategies have been proposed. However, there is no systematic and
fair comparison across different strategies, or between these strategies and
strong baselines that do not leverage tools. We conduct an extensive empirical
analysis, finding that (1) across various datasets, example difficulty levels,
and models, strong no-tool baselines are competitive to tool-assisted
strategies, implying that effectively using tools with in-context
demonstrations is a difficult unsolved problem; (2) for knowledge-retrieval
tasks, strategies that *refine* incorrect outputs with tools outperform
strategies that retrieve relevant information *ahead of* or *during
generation*; (3) tool-assisted strategies are expensive in the number of tokens
they require to work -- incurring additional costs by orders of magnitude --
which does not translate into significant improvement in performance. Overall,
our findings suggest that few-shot tool integration is still an open challenge,
emphasizing the need for comprehensive evaluations of future strategies to
accurately assess their *benefits* and *costs*.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacovi_A/0/1/0/all/0/1&quot;&gt;Alon Jacovi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1&quot;&gt;Avi Caciularu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1&quot;&gt;Jonathan Herzig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1&quot;&gt;Roee Aharoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohnet_B/0/1/0/all/0/1&quot;&gt;Bernd Bohnet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1&quot;&gt;Mor Geva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10477">
<title>Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10477</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid development of large language models (LLMs) has not only provided
numerous opportunities but also presented significant challenges. This becomes
particularly evident when LLMs inadvertently generate harmful or toxic content,
either unintentionally or because of intentional inducement. Existing alignment
methods usually direct LLMs toward favorable outcomes by utilizing
human-annotated, flawless instruction-response pairs. Conversely, this study
proposes a novel alignment technique based on mistake analysis, which
deliberately exposes LLMs to erroneous content to learn the reasons for
mistakes and how to avoid them. In this case, mistakes are repurposed into
valuable data for alignment, effectively helping to avoid the production of
erroneous responses. Without external models or human annotations, our method
leverages a model&apos;s intrinsic ability to discern undesirable mistakes and
improves the safety of its generated responses. Experimental results reveal
that our method outperforms existing alignment approaches in enhancing model
safety while maintaining the overall utility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chunwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kuo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jianhua Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1&quot;&gt;Lanqing Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1&quot;&gt;Fei Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenyong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1&quot;&gt;Dit-Yan Yeung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1&quot;&gt;Lifeng Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qun Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15202">
<title>Predicting Transcription Factor Binding Sites using Transformer based Capsule Network. (arXiv:2310.15202v2 [q-bio.GN] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15202</link>
<description rdf:parseType="Literal">&lt;p&gt;Prediction of binding sites for transcription factors is important to
understand how they regulate gene expression and how this regulation can be
modulated for therapeutic purposes. Although in the past few years there are
significant works addressing this issue, there is still space for improvement.
In this regard, a transformer based capsule network viz. DNABERT-Cap is
proposed in this work to predict transcription factor binding sites mining
ChIP-seq datasets. DNABERT-Cap is a bidirectional encoder pre-trained with
large number of genomic DNA sequences, empowered with a capsule layer
responsible for the final prediction. The proposed model builds a predictor for
transcription factor binding sites using the joint optimisation of features
encompassing both bidirectional encoder and capsule layer, along with
convolutional and bidirectional long-short term memory layers. To evaluate the
efficiency of the proposed approach, we use a benchmark ChIP-seq datasets of
five cell lines viz. A549, GM12878, Hep-G2, H1-hESC and Hela, available in the
ENCODE repository. The results show that the average area under the receiver
operating characteristic curve score exceeds 0.91 for all such five cell lines.
DNABERT-Cap is also compared with existing state-of-the-art deep learning based
predictors viz. DeepARC, DeepTF, CNN-Zeng and DeepBind, and is seen to
outperform them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ghosh_N/0/1/0/all/0/1&quot;&gt;Nimisha Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Santoni_D/0/1/0/all/0/1&quot;&gt;Daniele Santoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Saha_I/0/1/0/all/0/1&quot;&gt;Indrajit Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Felici_G/0/1/0/all/0/1&quot;&gt;Giovanni Felici&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16221">
<title>Hierarchical Randomized Smoothing. (arXiv:2310.16221v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16221</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world data is complex and often consists of objects that can be
decomposed into multiple entities (e.g. images into pixels, graphs into
interconnected nodes). Randomized smoothing is a powerful framework for making
models provably robust against small changes to their inputs - by guaranteeing
robustness of the majority vote when randomly adding noise before
classification. Yet, certifying robustness on such complex data via randomized
smoothing is challenging when adversaries do not arbitrarily perturb entire
objects (e.g. images) but only a subset of their entities (e.g. pixels). As a
solution, we introduce hierarchical randomized smoothing: We partially smooth
objects by adding random noise only on a randomly selected subset of their
entities. By adding noise in a more targeted manner than existing methods we
obtain stronger robustness guarantees while maintaining high accuracy. We
initialize hierarchical smoothing using different noising distributions,
yielding novel robustness certificates for discrete and continuous domains. We
experimentally demonstrate the importance of hierarchical smoothing in image
and node classification, where it yields superior robustness-accuracy
trade-offs. Overall, hierarchical smoothing is an important contribution
towards models that are both - certifiably robust to perturbations and
accurate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholten_Y/0/1/0/all/0/1&quot;&gt;Yan Scholten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuchardt_J/0/1/0/all/0/1&quot;&gt;Jan Schuchardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bojchevski_A/0/1/0/all/0/1&quot;&gt;Aleksandar Bojchevski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1&quot;&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18446">
<title>A Novel Skip Orthogonal List for Dynamic Optimal Transport Problem. (arXiv:2310.18446v3 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18446</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimal transport is a fundamental topic that has attracted a great amount of
attention from the optimization community in the past decades. In this paper,
we consider an interesting discrete dynamic optimal transport problem: can we
efficiently update the optimal transport plan when the weights or the locations
of the data points change? This problem is naturally motivated by several
applications in machine learning. For example, we often need to compute the
optimal transport cost between two different data sets; if some changes happen
to a few data points, should we re-compute the high complexity cost function or
update the cost by some efficient dynamic data structure? We are aware that
several dynamic maximum flow algorithms have been proposed before, however, the
research on dynamic minimum cost flow problem is still quite limited, to the
best of our knowledge. We propose a novel 2D Skip Orthogonal List together with
some dynamic tree techniques. Although our algorithm is based on the
conventional simplex method, it can efficiently find the variable to pivot
within expected $O(1)$ time, and complete each pivoting operation within
expected $O(|V|)$ time where $V$ is the set of all supply and demand nodes.
Since dynamic modifications typically do not introduce significant changes, our
algorithm requires only a few simplex iterations in practice. So our algorithm
is more efficient than re-computing the optimal transport cost that needs at
least one traversal over all $|E| = O(|V|^2)$ variables, where $|E|$ denotes
the number of edges in the network. Our experiments demonstrate that our
algorithm significantly outperforms existing algorithms in the dynamic
scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Hu Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20704">
<title>Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders. (arXiv:2310.20704v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20704</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) have become ubiquitous in computer vision. Despite
their success, ViTs lack inductive biases, which can make it difficult to train
them with limited data. To address this challenge, prior studies suggest
training ViTs with self-supervised learning (SSL) and fine-tuning sequentially.
However, we observe that jointly optimizing ViTs for the primary task and a
Self-Supervised Auxiliary Task (SSAT) is surprisingly beneficial when the
amount of training data is limited. We explore the appropriate SSL tasks that
can be optimized alongside the primary task, the training schemes for these
tasks, and the data scale at which they can be most effective. Our findings
reveal that SSAT is a powerful technique that enables ViTs to leverage the
unique characteristics of both the self-supervised and primary tasks, achieving
better performance than typical ViTs pre-training with SSL and fine-tuning
sequentially. Our experiments, conducted on 10 datasets, demonstrate that SSAT
significantly improves ViT performance while reducing carbon footprint. We also
confirm the effectiveness of SSAT in the video domain for deepfake detection,
showcasing its generalizability. Our code is available at
https://github.com/dominickrei/Limited-data-vits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1&quot;&gt;Srijan Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_T/0/1/0/all/0/1&quot;&gt;Tanmay Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reilly_D/0/1/0/all/0/1&quot;&gt;Dominick Reilly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balaji_P/0/1/0/all/0/1&quot;&gt;Pranav Balaji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karmakar_S/0/1/0/all/0/1&quot;&gt;Soumyajit Karmakar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marjit_S/0/1/0/all/0/1&quot;&gt;Shyam Marjit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Abhijit Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1&quot;&gt;Michael S. Ryoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02594">
<title>scBeacon: single-cell biomarker extraction via identifying paired cell clusters across biological conditions with contrastive siamese networks. (arXiv:2311.02594v2 [q-bio.GN] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02594</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the breakthroughs in biomarker discovery facilitated by differential
gene analysis, challenges remain, particularly at the single-cell level.
Traditional methodologies heavily rely on user-supplied cell annotations,
focusing on individually expressed data, often neglecting the critical
interactions between biological conditions, such as healthy versus diseased
states. In response, here we introduce scBeacon, an innovative framework built
upon a deep contrastive siamese network. scBeacon pioneers an unsupervised
approach, adeptly identifying matched cell populations across varied
conditions, enabling a refined differential gene analysis. By utilizing a
VQ-VAE framework, a contrastive siamese network, and a greedy iterative
strategy, scBeacon effectively pinpoints differential genes that hold potential
as key biomarkers. Comprehensive evaluations on a diverse array of datasets
validate scBeacon&apos;s superiority over existing single-cell differential gene
analysis tools. Its precision and adaptability underscore its significant role
in enhancing diagnostic accuracy in biomarker discovery. With the emphasis on
the importance of biomarkers in diagnosis, scBeacon is positioned to be a
pivotal asset in the evolution of personalized medicine and targeted
treatments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kweon_Y/0/1/0/all/0/1&quot;&gt;Yong Jin Kweon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ding_J/0/1/0/all/0/1&quot;&gt;Jun Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10319">
<title>Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification. (arXiv:2311.10319v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10319</link>
<description rdf:parseType="Literal">&lt;p&gt;Advancements in clinical treatment are increasingly constrained by the
limitations of supervised learning techniques, which depend heavily on large
volumes of annotated data. The annotation process is not only costly but also
demands substantial time from clinical specialists. Addressing this issue, we
introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging)
pipeline, a novel approach that leverages the advancements in self-supervised
and semi-supervised learning. These techniques engage in auxiliary tasks that
do not require labeling, thus simplifying the scaling of machine supervision
compared to fully-supervised methods. Our study benchmarks these techniques on
three distinct medical imaging datasets to evaluate their effectiveness in
classification and segmentation tasks. Remarkably, we observed that
self-supervised learning with only 10% of the annotation surpassed the
performance of full annotation in the classification of most datasets.
Similarly, the semi-supervised approach demonstrated superior outcomes in
segmentation, outperforming fully-supervised methods with 50% fewer labels
across all datasets. In line with our commitment to contributing to the
scientific community, we have made the S4MI code openly accessible, allowing
for broader application and further development of these methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1&quot;&gt;Pranav Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chukkapalli_R/0/1/0/all/0/1&quot;&gt;Raviteja Chukkapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhari_S/0/1/0/all/0/1&quot;&gt;Shravan Chaudhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Luoyao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jinqian Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smuda_C/0/1/0/all/0/1&quot;&gt;Craig Smuda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cirrone_J/0/1/0/all/0/1&quot;&gt;Jacopo Cirrone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12893">
<title>A Safer Vision-based Autonomous Planning System for Quadrotor UAVs with Dynamic Obstacle Trajectory Prediction and Its Application with LLMs. (arXiv:2311.12893v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12893</link>
<description rdf:parseType="Literal">&lt;p&gt;For intelligent quadcopter UAVs, a robust and reliable autonomous planning
system is crucial. Most current trajectory planning methods for UAVs are
suitable for static environments but struggle to handle dynamic obstacles,
which can pose challenges and even dangers to flight. To address this issue,
this paper proposes a vision-based planning system that combines tracking and
trajectory prediction of dynamic obstacles to achieve efficient and reliable
autonomous flight. We use a lightweight object detection algorithm to identify
dynamic obstacles and then use Kalman Filtering to track and estimate their
motion states. During the planning phase, we not only consider static obstacles
but also account for the potential movements of dynamic obstacles. For
trajectory generation, we use a B-spline-based trajectory search algorithm,
which is further optimized with various constraints to enhance safety and
alignment with the UAV&apos;s motion characteristics. We conduct experiments in both
simulation and real-world environments, and the results indicate that our
approach can successfully detect and avoid obstacles in dynamic environments in
real-time, offering greater reliability compared to existing approaches.
Furthermore, with the advancements in Natural Language Processing (NLP)
technology demonstrating exceptional zero-shot generalization capabilities,
more user-friendly human-machine interactions have become feasible, and this
study also explores the integration of autonomous planning systems with Large
Language Models (LLMs).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1&quot;&gt;Jiageng Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Ming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yinliang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zihang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Haoran Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12990">
<title>NERIF: GPT-4V for Automatic Scoring of Drawn Models. (arXiv:2311.12990v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12990</link>
<description rdf:parseType="Literal">&lt;p&gt;Scoring student-drawn models is time-consuming. Recently released GPT-4V
provides a unique opportunity to advance scientific modeling practices by
leveraging the powerful image processing capability. To test this ability
specifically for automatic scoring, we developed a method NERIF
(Notation-Enhanced Rubric Instruction for Few-shot Learning) employing
instructional note and rubrics to prompt GPT-4V to score students&apos; drawn models
for science phenomena. We randomly selected a set of balanced data (N = 900)
that includes student-drawn models for six modeling assessment tasks. Each
model received a score from GPT-4V ranging at three levels: &apos;Beginning,&apos;
&apos;Developing,&apos; or &apos;Proficient&apos; according to scoring rubrics. GPT-4V scores were
compared with human experts&apos; scores to calculate scoring accuracy. Results show
that GPT-4V&apos;s average scoring accuracy was mean =.51, SD = .037. Specifically,
average scoring accuracy was .64 for the &apos;Beginning&apos; class, .62 for the
&apos;Developing&apos; class, and .26 for the &apos;Proficient&apos; class, indicating that more
proficient models are more challenging to score. Further qualitative study
reveals how GPT-4V retrieves information from image input, including problem
context, example evaluations provided by human coders, and students&apos; drawing
models. We also uncovered how GPT-4V catches the characteristics of
student-drawn models and narrates them in natural language. At last, we
demonstrated how GPT-4V assigns scores to student-drawn models according to the
given scoring rubric and instructional notes. Our findings suggest that the
NERIF is an effective approach for employing GPT-4V to score drawn models. Even
though there is space for GPT-4V to improve scoring accuracy, some mis-assigned
scores seemed interpretable to experts. The results of this study show that
utilizing GPT-4V for automatic scoring of student-drawn models is promising.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gyeong-Geon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1&quot;&gt;Xiaoming Zhai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13326">
<title>Curriculum Learning and Imitation Learning for Model-free Control on Financial Time-series. (arXiv:2311.13326v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13326</link>
<description rdf:parseType="Literal">&lt;p&gt;Curriculum learning and imitation learning have been leveraged extensively in
the robotics domain. However, minimal research has been done on leveraging
these ideas on control tasks over highly stochastic time-series data. Here, we
theoretically and empirically explore these approaches in a representative
control task over complex time-series data. We implement the fundamental ideas
of curriculum learning via data augmentation, while imitation learning is
implemented via policy distillation from an oracle. Our findings reveal that
curriculum learning should be considered a novel direction in improving
control-task performance over complex time-series. Our ample random-seed
out-sample empirics and ablation studies are highly encouraging for curriculum
learning for time-series control. These findings are especially encouraging as
we tune all overlapping hyperparameters on the baseline -- giving an advantage
to the baseline. On the other hand, we find that imitation learning should be
used with caution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koh_W/0/1/0/all/0/1&quot;&gt;Woosung Koh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_I/0/1/0/all/0/1&quot;&gt;Insu Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1&quot;&gt;Yuntae Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1&quot;&gt;Gimin Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1&quot;&gt;Woo Chang Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17431">
<title>Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17431</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and
powerful emergent abilities have achieved remarkable success in various natural
language processing and computer vision tasks. Grounding FMs by adapting them
to domain-specific tasks or augmenting them with domain-specific knowledge
enables us to exploit the full potential of FMs. However, grounding FMs faces
several challenges, stemming primarily from constrained computing resources,
data privacy, model heterogeneity, and model ownership. Federated Transfer
Learning (FTL), the combination of federated learning and transfer learning,
provides promising solutions to address these challenges. In recent years, the
need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in
both academia and industry. Motivated by the strong growth in FTL-FM research
and the potential impact of FTL-FM on industrial applications, we propose an
FTL-FM framework that formulates problems of grounding FMs in the federated
learning setting, construct a detailed taxonomy based on the FTL-FM framework
to categorize state-of-the-art FTL-FM works, and comprehensively overview
FTL-FM works based on the proposed taxonomy. We also establish correspondences
between FTL-FM and conventional phases of adapting FM so that FM practitioners
can align their research works with FTL-FM. In addition, we overview advanced
efficiency-improving and privacy-preserving techniques because efficiency and
privacy are critical concerns in FTL-FM. Last, we discuss opportunities and
future research directions of FTL-FM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yan Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1&quot;&gt;Tao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1&quot;&gt;Hanlin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaojin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Lixin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00201">
<title>An integrated framework for developing and evaluating an automated lecture style assessment system. (arXiv:2312.00201v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00201</link>
<description rdf:parseType="Literal">&lt;p&gt;The aim of the work presented in this paper is to develop and evaluate an
integrated system that provides automated lecture style evaluation, allowing
teachers to get instant feedback related to the goodness of their lecturing
style. The proposed system aims to promote improvement of lecture quality, that
could upgrade the overall student learning experience. The proposed application
utilizes specific measurable biometric characteristics, such as facial
expressions, body activity, speech rate and intonation, hand movement, and
facial pose, extracted from a video showing the lecturer from the audience
point of view. Measurable biometric features extracted during a lecture are
combined to provide teachers with a score reflecting lecture style quality both
at frame rate and by providing lecture quality metrics for the whole lecture.
The acceptance of the proposed lecture style evaluation system was evaluated by
chief education officers, teachers and students regarding the functionality,
usefulness of the application, and possible improvements. The results indicate
that participants found the application novel and useful in providing automated
feedback regarding lecture quality. Furthermore, the performance evaluation of
the proposed system was compared with the performance of humans in the task of
lecture style evaluation. Results indicate that the proposed system not only
achieves similar performance to human observers, but in some cases, it
outperforms them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimitriadou_E/0/1/0/all/0/1&quot;&gt;Eleni Dimitriadou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanitis_A/0/1/0/all/0/1&quot;&gt;Andreas Lanitis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01697">
<title>Hulk: A Universal Knowledge Translator for Human-Centric Tasks. (arXiv:2312.01697v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01697</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-centric perception tasks, e.g., pedestrian detection, skeleton-based
action recognition, and pose estimation, have wide industrial applications,
such as metaverse and sports analysis. There is a recent surge to develop
human-centric foundation models that can benefit a broad range of human-centric
perception tasks. While many human-centric foundation models have achieved
success, they did not explore 3D and vision-language tasks for human-centric
and required task-specific finetuning. These limitations restrict their
application to more downstream tasks and situations. To tackle these problems,
we present Hulk, the first multimodal human-centric generalist model, capable
of addressing 2D vision, 3D vision, skeleton-based, and vision-language tasks
without task-specific finetuning. The key to achieving this is condensing
various task-specific heads into two general heads, one for discrete
representations, e.g., languages, and the other for continuous representations,
e.g., location coordinates. The outputs of two heads can be further stacked
into four distinct input and output modalities. This uniform representation
enables Hulk to treat diverse human-centric tasks as modality translation,
integrating knowledge across a wide range of tasks. Comprehensive evaluations
of Hulk on 12 benchmarks covering 8 human-centric tasks demonstrate the
superiority of our proposed method, achieving state-of-the-art performance in
11 benchmarks. The code will be available on
https://github.com/OpenGVLab/HumanBench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhou Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yixuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shixiang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Weizhen He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xun Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Feng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Lei Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02366">
<title>Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks. (arXiv:2312.02366v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02366</link>
<description rdf:parseType="Literal">&lt;p&gt;The integration of deep learning systems into healthcare has been hindered by
the resource-intensive process of data annotation and the inability of these
systems to generalize to different data distributions. Foundation models, which
are models pre-trained on large datasets, have emerged as a solution to reduce
reliance on annotated data and enhance model generalizability and robustness.
DINOv2 is an open-source foundation model pre-trained with self-supervised
learning on 142 million curated natural images that exhibits promising
capabilities across various vision tasks. Nevertheless, a critical question
remains unanswered regarding DINOv2&apos;s adaptability to radiological imaging, and
whether its features are sufficiently general to benefit radiology image
analysis. Therefore, this study comprehensively evaluates DINOv2 for radiology,
conducting over 100 experiments across diverse modalities (X-ray, CT, and MRI).
To measure the effectiveness and generalizability of DINOv2&apos;s feature
representations, we analyze the model across medical image analysis tasks
including disease classification and organ segmentation on both 2D and 3D
images, and under different settings like kNN, few-shot learning,
linear-probing, end-to-end fine-tuning, and parameter-efficient fine-tuning.
Comparative analyses with established supervised, self-supervised, and
weakly-supervised models reveal DINOv2&apos;s superior performance and cross-task
generalizability. The findings contribute insights to potential avenues for
optimizing pre-training strategies for medical imaging and enhancing the
broader understanding of DINOv2&apos;s role in bridging the gap between natural and
radiological image analysis. Our code is available at
https://github.com/MohammedSB/DINOv2ForRadiology
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baharoon_M/0/1/0/all/0/1&quot;&gt;Mohammed Baharoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qureshi_W/0/1/0/all/0/1&quot;&gt;Waseem Qureshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1&quot;&gt;Jiahong Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yanwu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aljouie_A/0/1/0/all/0/1&quot;&gt;Abdulrhman Aljouie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1&quot;&gt;Wei Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03719">
<title>Assessing AI Chatbots Performance in Comprehensive Standardized Test Preparation; A Case Study with GRE. (arXiv:2312.03719v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03719</link>
<description rdf:parseType="Literal">&lt;p&gt;This research paper presents an analysis of how well three artificial
intelligence chatbots, Bing, ChatGPT, and GPT-4, perform when answering
questions from standardized tests. The Graduate Record Examination (GRE) is
used in this paper as a case study. A total of 137 questions with different
forms of quantitative reasoning and 157 questions with verbal categories were
used to assess their capabilities. This paper presents the performance of each
chatbot across various skills and styles tested in the exam. This paper also
explores the proficiency of these chatbots in addressing image-based questions
and illustrates the uncertainty level of each chatbot. The results show varying
degrees of success across the chatbots, where GPT-4 served as the most
proficient, especially in complex language understanding tasks and image-based
questions. Results highlight the ability of these chatbots to pass the GRE with
a high score, which encourages the use of these chatbots in test preparation.
The results also show how important it is to ensure that, if the test is
administered online, as it was during COVID, the test taker is segregated from
these resources for a fair competition on higher education opportunities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abu_Haifa_M/0/1/0/all/0/1&quot;&gt;Mohammad Abu-Haifa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Etawi_B/0/1/0/all/0/1&quot;&gt;Bara&amp;#x27;a Etawi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alkhatatbeh_H/0/1/0/all/0/1&quot;&gt;Huthaifa Alkhatatbeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ababneh_A/0/1/0/all/0/1&quot;&gt;Ayman Ababneh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04316">
<title>Towards Knowledge-driven Autonomous Driving. (arXiv:2312.04316v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04316</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the emerging knowledge-driven autonomous driving
technologies. Our investigation highlights the limitations of current
autonomous driving systems, in particular their sensitivity to data bias,
difficulty in handling long-tail scenarios, and lack of interpretability.
Conversely, knowledge-driven methods with the abilities of cognition,
generalization and life-long learning emerge as a promising way to overcome
these challenges. This paper delves into the essence of knowledge-driven
autonomous driving and examines its core components: dataset \&amp;amp; benchmark,
environment, and driver agent. By leveraging large language models, world
models, neural rendering, and other advanced artificial intelligence
techniques, these components collectively contribute to a more holistic,
adaptive, and intelligent autonomous driving system. The paper systematically
organizes and reviews previous research efforts in this area, and provides
insights and guidance for future research and practical applications of
autonomous driving. We will continually share the latest updates on
cutting-edge developments in knowledge-driven autonomous driving along with the
relevant valuable open-source resources at:
\url{https://github.com/PJLab-ADG/awesome-knowledge-driven-AD}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yeqi Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1&quot;&gt;Pinlong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1&quot;&gt;Licheng Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1&quot;&gt;Daocheng Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xuemeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1&quot;&gt;Xinyu Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jianfei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xing Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_M/0/1/0/all/0/1&quot;&gt;Min Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yikang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Botian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Liang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06681">
<title>Steering Llama 2 via Contrastive Activation Addition. (arXiv:2312.06681v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06681</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Contrastive Activation Addition (CAA), an innovative method for
steering language models by modifying activations during their forward passes.
CAA computes ``steering vectors&apos;&apos; by averaging the difference in residual
stream activations between pairs of positive and negative examples of a
particular behavior such as factual versus hallucinatory responses. During
inference, these steering vectors are added at all token positions after the
user&apos;s prompt with either a positive or negative coefficient, allowing precise
control over the degree of the targeted behavior. We evaluate CAA&apos;s
effectiveness on Llama 2 Chat using both multiple-choice behavioral question
datasets and open-ended generation tasks. We demonstrate that CAA significantly
alters model behavior, outperforms traditional methods like finetuning and
few-shot prompting, and minimally reduces capabilities. Moreover, by employing
various activation space interpretation methods, we gain deeper insights into
CAA&apos;s mechanisms. CAA both accurately steers model outputs and also sheds light
on how high-level concepts are represented in Large Language Models (LLMs).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rimsky_N/0/1/0/all/0/1&quot;&gt;Nina Rimsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabrieli_N/0/1/0/all/0/1&quot;&gt;Nick Gabrieli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_J/0/1/0/all/0/1&quot;&gt;Julian Schulz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1&quot;&gt;Meg Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hubinger_E/0/1/0/all/0/1&quot;&gt;Evan Hubinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turner_A/0/1/0/all/0/1&quot;&gt;Alexander Matt Turner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07492">
<title>SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models. (arXiv:2312.07492v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07492</link>
<description rdf:parseType="Literal">&lt;p&gt;Current datasets for unwanted social bias auditing are limited to studying
protected demographic features such as race and gender. In this work, we
introduce a comprehensive benchmark that is meant to capture the amplification
of social bias, via stigmas, in generative language models. Taking inspiration
from social science research, we start with a documented list of 93 US-centric
stigmas and curate a question-answering (QA) dataset which involves simple
social situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts,
with a variety of prompt styles, carefully constructed to systematically test
for both social bias and model robustness. We present results for
SocialStigmaQA with two open source generative language models and we find that
the proportion of socially biased output ranges from 45% to 59% across a
variety of decoding strategies and prompting styles. We demonstrate that the
deliberate design of the templates in our benchmark (e.g., adding biasing text
to the prompt or using different verbs that change the answer that indicates
bias) impacts the model tendencies to generate socially biased output.
Additionally, through manual evaluation, we discover problematic patterns in
the generated chain-of-thought output that range from subtle bias to lack of
reasoning.
&lt;/p&gt;
&lt;p&gt;Warning: This paper contains examples of text which are toxic, biased, and
potentially harmful.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagireddy_M/0/1/0/all/0/1&quot;&gt;Manish Nagireddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiazor_L/0/1/0/all/0/1&quot;&gt;Lamogha Chiazor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Moninder Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1&quot;&gt;Ioana Baldini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08078">
<title>Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation. (arXiv:2312.08078v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08078</link>
<description rdf:parseType="Literal">&lt;p&gt;To address these issues, we propose a novel Adaptive patch-word Matching
(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in
medical reports and apply it to CXR-report generation to provide explainability
for the generation process. AdaMatch exploits the fine-grained relation between
adaptive patches and words to provide explanations of specific image regions
with corresponding words. To capture the abnormal regions of varying sizes and
positions, we introduce the Adaptive Patch extraction (AdaPatch) module to
acquire the adaptive patches for these regions adaptively. In order to provide
explicit explainability for CXR-report generation task, we propose an
AdaMatch-based bidirectional large language model for Cyclic CXR-report
generation (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords
for CXR images and `keypatches&apos; for medical reports as hints to guide
CXR-report generation. Extensive experiments on two publicly available CXR
datasets prove the effectiveness of our method and its superior performance to
existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Linlin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yixuan Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08935">
<title>Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations. (arXiv:2312.08935v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08935</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present an innovative process-oriented math process reward
model called \textbf{Math-Shepherd}, which assigns a reward score to each step
of math problem solutions. The training of Math-Shepherd is achieved using
automatically constructed process-wise supervision data, breaking the
bottleneck of heavy reliance on manual annotation in existing work. We explore
the effectiveness of Math-Shepherd in two scenarios: 1) \textit{Verification}:
Math-Shepherd is utilized for reranking multiple outputs generated by Large
Language Models (LLMs); 2) \textit{Reinforcement Learning}: Math-Shepherd is
employed to reinforce LLMs with step-by-step Proximal Policy Optimization
(PPO). With Math-Shepherd, a series of open-source LLMs demonstrates
exceptional performance. For instance, the step-by-step PPO with Math-Shepherd
significantly improves the accuracy of Mistral-7B (77.9\%$\to$84.1\% on GSM8K
and 28.6\%$\to$33.0\% on MATH). The accuracy can be further enhanced to 89.1\%
and 43.5\% on GSM8K and MATH with the verification of Math-Shepherd,
respectively. We believe that automatic process supervision holds significant
potential for the future evolution of LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peiyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1&quot;&gt;Zhihong Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;R.X. Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1&quot;&gt;Damai Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yifei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Deli Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Y.Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1&quot;&gt;Zhifang Sui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09507">
<title>WAVER: Writing-style Agnostic Video Retrieval via Distilling Vision-Language Models Through Open-Vocabulary Knowledge. (arXiv:2312.09507v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09507</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-video retrieval, a prominent sub-field within the domain of multimodal
information retrieval, has witnessed remarkable growth in recent years.
However, existing methods assume video scenes are consistent with unbiased
descriptions. These limitations fail to align with real-world scenarios since
descriptions can be influenced by annotator biases, diverse writing styles, and
varying textual perspectives. To overcome the aforementioned problems, we
introduce WAVER, a cross-domain knowledge distillation framework via
vision-language models through open-vocabulary knowledge designed to tackle the
challenge of handling different writing styles in video descriptions. WAVER
capitalizes on the open-vocabulary properties that lie in pre-trained
vision-language models and employs an implicit knowledge distillation approach
to transfer text-based knowledge from a teacher model to a vision-based
student. Empirical studies conducted across four standard benchmark datasets,
encompassing various settings, provide compelling evidence that WAVER can
achieve state-of-the-art performance in text-video retrieval task while
handling writing-style variations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1&quot;&gt;Huy Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kieu_T/0/1/0/all/0/1&quot;&gt;Tung Kieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngan Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10572">
<title>Improved Anonymous Multi-Agent Path Finding Algorithm. (arXiv:2312.10572v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10572</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider an Anonymous Multi-Agent Path-Finding (AMAPF) problem where the
set of agents is confined to a graph, a set of goal vertices is given and each
of these vertices has to be reached by some agent. The problem is to find an
assignment of the goals to the agents as well as the collision-free paths, and
we are interested in finding the solution with the optimal makespan. A
well-established approach to solve this problem is to reduce it to a special
type of a graph search problem, i.e. to the problem of finding a maximum flow
on an auxiliary graph induced by the input one. The size of the former graph
may be very large and the search on it may become a bottleneck. To this end, we
suggest a specific search algorithm that leverages the idea of exploring the
search space not through considering separate search states but rather bulks of
them simultaneously. That is, we implicitly compress, store and expand bulks of
the search states as single states, which results in high reduction in runtime
and memory. Empirically, the resultant AMAPF solver demonstrates superior
performance compared to the state-of-the-art competitor and is able to solve
all publicly available MAPF instances from the well-known MovingAI benchmark in
less than 30 seconds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_Z/0/1/0/all/0/1&quot;&gt;Zain Alabedeen Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yakovlev_K/0/1/0/all/0/1&quot;&gt;Konstantin Yakovlev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10967">
<title>Knowledge Graphs and Pre-trained Language Models enhanced Representation Learning for Conversational Recommender Systems. (arXiv:2312.10967v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10967</link>
<description rdf:parseType="Literal">&lt;p&gt;Conversational recommender systems (CRS) utilize natural language
interactions and dialogue history to infer user preferences and provide
accurate recommendations. Due to the limited conversation context and
background knowledge, existing CRSs rely on external sources such as knowledge
graphs to enrich the context and model entities based on their inter-relations.
However, these methods ignore the rich intrinsic information within entities.
To address this, we introduce the Knowledge-Enhanced Entity Representation
Learning (KERL) framework, which leverages both the knowledge graph and a
pre-trained language model to improve the semantic understanding of entities
for CRS. In our KERL framework, entity textual descriptions are encoded via a
pre-trained language model, while a knowledge graph helps reinforce the
representation of these entities. We also employ positional encoding to
effectively capture the temporal information of entities in a conversation. The
enhanced entity representation is then used to develop a recommender component
that fuses both entity and contextual representations for more informed
recommendations, as well as a dialogue component that generates informative
entity-related information in the response text. A high-quality knowledge graph
with aligned entity descriptions is constructed to facilitate our study, namely
the Wiki Movie Knowledge Graph (WikiMKG). The experimental results show that
KERL achieves state-of-the-art results in both recommendation and response
generation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1&quot;&gt;Zhangchi Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1&quot;&gt;Ye Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shirui Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liew_A/0/1/0/all/0/1&quot;&gt;Alan Wee-Chung Liew&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11193">
<title>&quot;Paraphrasing The Original Text&quot; Makes High Accuracy Long-Context QA. (arXiv:2312.11193v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11193</link>
<description rdf:parseType="Literal">&lt;p&gt;Most open-source LLMs still have a context window of no more than 4k,
limiting their ability to handle long-context problems. Meanwhile, even those
with a long context window still lack satisfactory accuracy. To address this
issue, we explore from the perspective of training data and theoretically prove
training the capability to handle long contexts requires &quot;effective&quot; rather
than &quot;long&quot; data. Based on this, we propose using the &quot;original text
paraphrase&quot; task, and successfully extend the context window of the existing
model to 32k by a low-cost and effective method, achieving the SOTA accuracy in
multi-document-QA among models of the same scale. The model and training data
have been open-sourced on
HuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) and
WiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yijiong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11285">
<title>Adv-Diffusion: Imperceptible Adversarial Face Identity Attack via Latent Diffusion Model. (arXiv:2312.11285v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11285</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial attacks involve adding perturbations to the source image to cause
misclassification by the target model, which demonstrates the potential of
attacking face recognition models. Existing adversarial face image generation
methods still can&apos;t achieve satisfactory performance because of low
transferability and high detectability. In this paper, we propose a unified
framework Adv-Diffusion that can generate imperceptible adversarial identity
perturbations in the latent space but not the raw pixel space, which utilizes
strong inpainting capabilities of the latent diffusion model to generate
realistic adversarial images. Specifically, we propose the identity-sensitive
conditioned diffusion generative model to generate semantic perturbations in
the surroundings. The designed adaptive strength-based adversarial perturbation
algorithm can ensure both attack transferability and stealthiness. Extensive
qualitative and quantitative experiments on the public FFHQ and CelebA-HQ
datasets prove the proposed method achieves superior performance compared with
the state-of-the-art methods without an extra generative model training
process. The source code is available at
https://github.com/kopper-xdu/Adv-Diffusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Decheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xijun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1&quot;&gt;Chunlei Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nannan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Ruiming Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11508">
<title>Rethinking the Instruction Quality: LIFT is What You Need. (arXiv:2312.11508v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11508</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction tuning, a specialized technique to enhance large language model
(LLM) performance via instruction datasets, relies heavily on the quality of
employed data. Existing quality improvement methods alter instruction data
through dataset expansion or curation. However, the expansion method risks data
redundancy, potentially compromising LLM performance, while the curation
approach confines the LLM&apos;s potential to the original dataset. Our aim is to
surpass the original data quality without encountering these shortcomings. To
achieve this, we propose LIFT (LLM Instruction Fusion Transfer), a novel and
versatile paradigm designed to elevate the instruction quality to new heights.
LIFT strategically broadens data distribution to encompass more high-quality
subspaces and eliminates redundancy, concentrating on high-quality segments
across overall data subspaces. Experimental results demonstrate that, even with
a limited quantity of high-quality instruction data selected by our paradigm,
LLMs not only consistently uphold robust performance across various tasks but
also surpass some state-of-the-art results, highlighting the significant
improvement in instruction quality achieved by our paradigm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yongqiang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yufan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1&quot;&gt;Mengnan Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Maoquan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1&quot;&gt;Bin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1&quot;&gt;Neel Sundaresan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12343">
<title>LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction. (arXiv:2312.12343v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12343</link>
<description rdf:parseType="Literal">&lt;p&gt;Data contamination in evaluation is getting increasingly prevalent with the
emergence of language models pre-trained on super large, automatically crawled
corpora. This problem leads to significant challenges in the accurate
assessment of model capabilities and generalisations. In this paper, we propose
LatestEval, an automatic method that leverages the most recent texts to create
uncontaminated reading comprehension evaluations. LatestEval avoids data
contamination by only using texts published within a recent time window,
ensuring no overlap with the training corpora of pre-trained language models.
We develop the LatestEval automated pipeline to 1) gather the latest texts; 2)
identify key information, and 3) construct questions targeting the information
while removing the existing answers from the context. This encourages models to
infer the answers themselves based on the remaining context, rather than just
copy-paste. Our experiments demonstrate that language models exhibit negligible
memorisation behaviours on LatestEval as opposed to previous benchmarks,
suggesting a significantly reduced risk of data contamination and leading to a
more robust evaluation. Data and code are publicly available at:
https://github.com/liyucheng09/LatestEval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yucheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1&quot;&gt;Frank Guerin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chenghua Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12679">
<title>Towards Efficient Verification of Quantized Neural Networks. (arXiv:2312.12679v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12679</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantization replaces floating point arithmetic with integer arithmetic in
deep neural network models, providing more efficient on-device inference with
less power and memory. In this work, we propose a framework for formally
verifying properties of quantized neural networks. Our baseline technique is
based on integer linear programming which guarantees both soundness and
completeness. We then show how efficiency can be improved by utilizing
gradient-based heuristic search methods and also bound-propagation techniques.
We evaluate our approach on perception networks quantized with PyTorch. Our
results show that we can verify quantized networks with better scalability and
efficiency than the previous state of the art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Pei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haoze Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuting Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daukantas_I/0/1/0/all/0/1&quot;&gt;Ieva Daukantas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Min Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yedi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrett_C/0/1/0/all/0/1&quot;&gt;Clark Barrett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13156">
<title>AccidentGPT: Accident Analysis and Prevention from V2X Environmental Perception with Multi-modal Large Model. (arXiv:2312.13156v2 [cs.CE] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13156</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic accidents, being a significant contributor to both human casualties
and property damage, have long been a focal point of research for many scholars
in the field of traffic safety. However, previous studies, whether focusing on
static environmental assessments or dynamic driving analyses, as well as
pre-accident predictions or post-accident rule analyses, have typically been
conducted in isolation. There has been a lack of an effective framework for
developing a comprehensive understanding and application of traffic safety. To
address this gap, this paper introduces AccidentGPT, a comprehensive accident
analysis and prevention multi-modal large model. AccidentGPT establishes a
multi-modal information interaction framework grounded in multi-sensor
perception, thereby enabling a holistic approach to accident analysis and
prevention in the field of traffic safety. Specifically, our capabilities can
be categorized as follows: for autonomous driving vehicles, we provide
comprehensive environmental perception and understanding to control the vehicle
and avoid collisions. For human-driven vehicles, we offer proactive long-range
safety warnings and blind-spot alerts while also providing safety driving
recommendations and behavioral norms through human-machine dialogue and
interaction. Additionally, for traffic police and management agencies, our
framework supports intelligent and real-time analysis of traffic safety,
encompassing pedestrian, vehicles, roads, and the environment through
collaborative perception from multiple vehicles and road testing devices. The
system is also capable of providing a thorough analysis of accident causes and
liability after vehicle collisions. Our framework stands as the first large
model to integrate comprehensive scene understanding into traffic safety
studies. Project page: https://accidentgpt.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lening Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Han Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1&quot;&gt;Pinlong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1&quot;&gt;Daocheng Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yilong Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haiyang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuesong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hanchu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Helai Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yinhai Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14149">
<title>TagAlign: Improving Vision-Language Alignment with Multi-Tag Classification. (arXiv:2312.14149v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14149</link>
<description rdf:parseType="Literal">&lt;p&gt;The crux of learning vision-language models is to extract semantically
aligned information from visual and linguistic data. Existing attempts usually
face the problem of coarse alignment, e.g., the vision encoder struggles in
localizing an attribute-specified object. In this work, we propose an
embarrassingly simple approach to better align image and text features with no
need of additional data formats other than image-text pairs. Concretely, given
an image and its paired text, we manage to parse objects (e.g., cat) and
attributes (e.g., black) from the description, which are highly likely to exist
in the image. It is noteworthy that the parsing pipeline is fully automatic and
thus enjoys good scalability. With these parsed semantics as supervision
signals, we can complement the commonly used image-text contrastive loss with
the multi-tag classification loss. Extensive experimental results on a broad
suite of semantic segmentation datasets substantiate the average 3.65\%
improvement of our framework over existing alternatives. Furthermore, the
visualization results indicate that attribute supervision makes vision-language
models accurately localize attribute-specified objects. Project page and code
can be found at https://qinying-liu.github.io/Tag-Align.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qinying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1&quot;&gt;Kecheng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1&quot;&gt;Zhan Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zilei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yujun Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14183">
<title>On Early Detection of Hallucinations in Factual Question Answering. (arXiv:2312.14183v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14183</link>
<description rdf:parseType="Literal">&lt;p&gt;While large language models (LLMs) have taken great strides towards helping
humans with a plethora of tasks like search and summarization, hallucinations
remain a major impediment towards gaining user trust. The fluency and coherence
of model generations even when hallucinating makes it difficult to detect
whether or not a model is hallucinating. In this work, we explore if the
artifacts associated with the model generations can provide hints that the
generation will contain hallucinations. Specifically, we probe LLMs at 1) the
inputs via Integrated Gradients based token attribution, 2) the outputs via the
Softmax probabilities, and 3) the internal state via self-attention and
fully-connected layer activations for signs of hallucinations on open-ended
question answering tasks. Our results show that the distributions of these
artifacts differ between hallucinated and non-hallucinated generations.
Building on this insight, we train binary classifiers that use these artifacts
as input features to classify model generations into hallucinations and
non-hallucinations. These hallucination classifiers achieve up to 0.80 AUROC.
We further show that tokens preceding a hallucination can predict the
subsequent hallucination before it occurs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snyder_B/0/1/0/all/0/1&quot;&gt;Ben Snyder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moisescu_M/0/1/0/all/0/1&quot;&gt;Marius Moisescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1&quot;&gt;Muhammad Bilal Zafar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14232">
<title>Parrot Captions Teach CLIP to Spot Text. (arXiv:2312.14232v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14232</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite CLIP being the foundation model in numerous vision-language
applications, the CLIP suffers from a severe text spotting bias. Such bias
causes CLIP models to &apos;Parrot&apos; the visual text embedded within images while
disregarding the authentic visual semantics. We uncover that in the most
popular image-text dataset LAION-2B, the captions also densely parrot (spell)
the text embedded in images. Our analysis shows that around 50% of images are
embedded with visual text content, and 90% of their captions more or less
parrot the visual text. Based on such observation, we thoroughly inspect the
different released versions of CLIP models and verify that the visual text is
the dominant factor in measuring the LAION-style image-text similarity for
these models. To examine whether these parrot captions shape the text spotting
bias, we train a series of CLIP models with LAION subsets curated by different
parrot-caption-oriented criteria. We show that training with parrot captions
easily shapes such bias but harms the expected visual-language representation
learning in CLIP models. This suggests that it is urgent to revisit either the
design of CLIP-like models or the existing image-text dataset curation pipeline
built on CLIP score filtering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yiqi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1&quot;&gt;Conghui He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Alex Jinpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weijia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14852">
<title>TACO: Topics in Algorithmic COde generation dataset. (arXiv:2312.14852v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14852</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce TACO, an open-source, large-scale code generation dataset, with
a focus on the optics of algorithms, designed to provide a more challenging
training dataset and evaluation benchmark in the field of code generation
models. TACO includes competition-level programming questions that are more
challenging, to enhance or evaluate problem understanding and reasoning
abilities in real-world programming scenarios. There are 25433 and 1000 coding
problems in training and test set, as well as up to 1.55 million diverse
solution answers. Moreover, each TACO problem includes several fine-grained
labels such as task topics, algorithms, programming skills, and difficulty
levels, providing a more precise reference for the training and evaluation of
code generation models. The dataset and evaluation scripts are available on
Hugging Face Hub (https://huggingface.co/datasets/BAAI/TACO) and Github
(https://github.com/FlagOpen/TACO).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Rongao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo-Wen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhihong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1&quot;&gt;Chen Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zhi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Ge Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14998">
<title>Synthetic images aid the recognition of human-made art forgeries. (arXiv:2312.14998v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14998</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous research has shown that Artificial Intelligence is capable of
distinguishing between authentic paintings by a given artist and human-made
forgeries with remarkable accuracy, provided sufficient training. However, with
the limited amount of existing known forgeries, augmentation methods for
forgery detection are highly desirable. In this work, we examine the potential
of incorporating synthetic artworks into training datasets to enhance the
performance of forgery detection. Our investigation focuses on paintings by
Vincent van Gogh, for which we release the first dataset specialized for
forgery detection. To reinforce our results, we conduct the same analyses on
the artists Amedeo Modigliani and Raphael. We train a classifier to distinguish
original artworks from forgeries. For this, we use human-made forgeries and
imitations in the style of well-known artists and augment our training sets
with images in a similar style generated by Stable Diffusion and StyleGAN. We
find that the additional synthetic forgeries consistently improve the detection
of human-made forgeries. In addition, we find that, in line with previous
research, the inclusion of synthetic forgeries in the training also enables the
detection of AI-generated forgeries, especially if created using a similar
generator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ostmeyer_J/0/1/0/all/0/1&quot;&gt;Johann Ostmeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaerf_L/0/1/0/all/0/1&quot;&gt;Ludovica Schaerf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buividovich_P/0/1/0/all/0/1&quot;&gt;Pavel Buividovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charles_T/0/1/0/all/0/1&quot;&gt;Tessa Charles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Postma_E/0/1/0/all/0/1&quot;&gt;Eric Postma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popovici_C/0/1/0/all/0/1&quot;&gt;Carina Popovici&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15646">
<title>A graph-based multimodal framework to predict gentrification. (arXiv:2312.15646v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15646</link>
<description rdf:parseType="Literal">&lt;p&gt;Gentrification--the transformation of a low-income urban area caused by the
influx of affluent residents--has many revitalizing benefits. However, it also
poses extremely concerning challenges to low-income residents. To help
policymakers take targeted and early action in protecting low-income residents,
researchers have recently proposed several machine learning models to predict
gentrification using socioeconomic and image features. Building upon previous
studies, we propose a novel graph-based multimodal deep learning framework to
predict gentrification based on urban networks of tracts and essential
facilities (e.g., schools, hospitals, and subway stations). We train and test
the proposed framework using data from Chicago, New York City, and Los Angeles.
The model successfully predicts census-tract level gentrification with 0.9
precision on average. Moreover, the framework discovers a previously unexamined
strong relationship between schools and gentrification, which provides a basis
for further exploration of social factors affecting gentrification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eshtiyagh_J/0/1/0/all/0/1&quot;&gt;Javad Eshtiyagh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Baotong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yujing Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Linhui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15661">
<title>Unlocking the Potential of Large Language Models for Explainable Recommendations. (arXiv:2312.15661v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15661</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating user-friendly explanations regarding why an item is recommended
has become increasingly common, largely due to advances in language generation
technology, which can enhance user trust and facilitate more informed
decision-making when using online services. However, existing explainable
recommendation systems focus on using small-size language models. It remains
uncertain what impact replacing the explanation generator with the recently
emerging large language models (LLMs) would have. Can we expect unprecedented
results?
&lt;/p&gt;
&lt;p&gt;In this study, we propose LLMXRec, a simple yet effective two-stage
explainable recommendation framework aimed at further boosting the explanation
quality by employing LLMs. Unlike most existing LLM-based recommendation works,
a key characteristic of LLMXRec is its emphasis on the close collaboration
between previous recommender models and LLM-based explanation generators.
Specifically, by adopting several key fine-tuning techniques, including
parameter-efficient instructing tuning and personalized prompt techniques,
controllable and fluent explanations can be well generated to achieve the goal
of explanation recommendation. Most notably, we provide three different
perspectives to evaluate the effectiveness of the explanations. Finally, we
conduct extensive experiments over several benchmark recommender models and
publicly available datasets. The experimental results not only yield positive
results in terms of effectiveness and efficiency but also uncover some
previously unknown outcomes. To facilitate further explorations in this area,
the full code and detailed original results are open-sourced at
https://anonymous.4open.science/r/LLM_rec_explanation-7028/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yucong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Mingyue Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Junyu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1&quot;&gt;Enhong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15692">
<title>Instruction Fusion: Advancing Prompt Evolution through Hybridization. (arXiv:2312.15692v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15692</link>
<description rdf:parseType="Literal">&lt;p&gt;The fine-tuning of Large Language Models (LLMs) specialized in code
generation has seen notable advancements through the use of open-domain coding
queries. Despite the successes, existing methodologies like Evol-Instruct
encounter performance limitations, impeding further enhancements in code
generation tasks. This paper examines the constraints of existing prompt
evolution techniques and introduces a novel approach, Instruction Fusion (IF).
IF innovatively combines two distinct prompts through a hybridization process,
thereby enhancing the evolution of training prompts for code LLMs. Our
experimental results reveal that the proposed novel method effectively
addresses the shortcomings of prior methods, significantly improving the
performance of Code LLMs across five code generation benchmarks, namely
HumanEval, HumanEval+, MBPP, MBPP+ and MultiPL-E, which underscore the
effectiveness of Instruction Fusion in advancing the capabilities of LLMs in
code generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1&quot;&gt;Weidong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiuding Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kaitong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_Z/0/1/0/all/0/1&quot;&gt;Zhuwei Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1&quot;&gt;Di Niu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15840">
<title>Masked Contrastive Reconstruction for Cross-modal Medical Image-Report Retrieval. (arXiv:2312.15840v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15840</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-modal medical image-report retrieval task plays a significant role in
clinical diagnosis and various medical generative tasks. Eliminating
heterogeneity between different modalities to enhance semantic consistency is
the key challenge of this task. The current Vision-Language Pretraining (VLP)
models, with cross-modal contrastive learning and masked reconstruction as
joint training tasks, can effectively enhance the performance of cross-modal
retrieval. This framework typically employs dual-stream inputs, using unmasked
data for cross-modal contrastive learning and masked data for reconstruction.
However, due to task competition and information interference caused by
significant differences between the inputs of the two proxy tasks, the
effectiveness of representation learning for intra-modal and cross-modal
features is limited. In this paper, we propose an efficient VLP framework named
Masked Contrastive and Reconstruction (MCR), which takes masked data as the
sole input for both tasks. This enhances task connections, reducing information
interference and competition between them, while also substantially decreasing
the required GPU memory and training time. Moreover, we introduce a new
modality alignment strategy named Mapping before Aggregation (MbA). Unlike
previous methods, MbA maps different modalities to a common feature space
before conducting local feature aggregation, thereby reducing the loss of
fine-grained semantic information necessary for improved modality alignment.
Qualitative and quantitative experiments conducted on the MIMIC-CXR dataset
validate the effectiveness of our approach, demonstrating state-of-the-art
performance in medical cross-modal retrieval tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zeqiang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1&quot;&gt;Kai Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiuzhuang Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16154">
<title>Clustered Orienteering Problem with Subgroups. (arXiv:2312.16154v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16154</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces an extension to the Orienteering Problem (OP), called
Clustered Orienteering Problem with Subgroups (COPS). In this variant, nodes
are arranged into subgroups, and the subgroups are organized into clusters. A
reward is associated with each subgroup and is gained only if all of its nodes
are visited; however, at most one subgroup can be visited per cluster. The
objective is to maximize the total collected reward while attaining a travel
budget. We show that our new formulation has the ability to model and solve two
previous well-known variants, the Clustered Orienteering Problem (COP) and the
Set Orienteering Problem (SOP), in addition to other scenarios introduced here.
An Integer Linear Programming (ILP) formulation and a Tabu Search-based
heuristic are proposed to solve the problem. Experimental results indicate that
the ILP method can yield optimal solutions at the cost of time, whereas the
metaheuristic produces comparable solutions within a more reasonable
computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almeida_L/0/1/0/all/0/1&quot;&gt;Luciano E. Almeida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macharet_D/0/1/0/all/0/1&quot;&gt;Douglas G. Macharet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.03447">
<title>Using Set Covering to Generate Databases for Holistic Steganalysis. (arXiv:2211.03447v2 [cs.MM] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2211.03447</link>
<description rdf:parseType="Literal">&lt;p&gt;Within an operational framework, covers used by a steganographer are likely
to come from different sensors and different processing pipelines than the ones
used by researchers for training their steganalysis models. Thus, a performance
gap is unavoidable when it comes to out-of-distributions covers, an extremely
frequent scenario called Cover Source Mismatch (CSM). Here, we explore a grid
of processing pipelines to study the origins of CSM, to better understand it,
and to better tackle it. A set-covering greedy algorithm is used to select
representative pipelines minimizing the maximum regret between the
representative and the pipelines within the set. Our main contribution is a
methodology for generating relevant bases able to tackle operational CSM.
Experimental validation highlights that, for a given number of training
samples, our set covering selection is a better strategy than selecting random
pipelines or using all the available pipelines. Our analysis also shows that
parameters as denoising, sharpening, and downsampling are very important to
foster diversity. Finally, different benchmarks for classical and wild
databases show the good generalization property of the extracted databases.
Additional resources are available at
github.com/RonyAbecidan/HolisticSteganalysisWithSetCovering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abecidan_R/0/1/0/all/0/1&quot;&gt;Rony Abecidan&lt;/a&gt; (CRIStAL, CNRS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Itier_V/0/1/0/all/0/1&quot;&gt;Vincent Itier&lt;/a&gt; (CRIStAL, IMT Nord Europe, CNRS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boulanger_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xe9;mie Boulanger&lt;/a&gt; (CRIStAL, CNRS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bas_P/0/1/0/all/0/1&quot;&gt;Patrick Bas&lt;/a&gt; (CRIStAL, CNRS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pevny_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Pevn&amp;#xfd;&lt;/a&gt; (CTU)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14654">
<title>Joint Multiple Intent Detection and Slot Filling with Supervised Contrastive Learning and Self-Distillation. (arXiv:2308.14654v1 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2308.14654</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiple intent detection and slot filling are two fundamental and crucial
tasks in spoken language understanding. Motivated by the fact that the two
tasks are closely related, joint models that can detect intents and extract
slots simultaneously are preferred to individual models that perform each task
independently. The accuracy of a joint model depends heavily on the ability of
the model to transfer information between the two tasks so that the result of
one task can correct the result of the other. In addition, since a joint model
has multiple outputs, how to train the model effectively is also challenging.
In this paper, we present a method for multiple intent detection and slot
filling by addressing these challenges. First, we propose a bidirectional joint
model that explicitly employs intent information to recognize slots and slot
features to detect intents. Second, we introduce a novel method for training
the proposed joint model using supervised contrastive learning and
self-distillation. Experimental results on two benchmark datasets MixATIS and
MixSNIPS show that our method outperforms state-of-the-art models in both
tasks. The results also demonstrate the contributions of both bidirectional
design and the training method to the accuracy improvement. Our source code is
available at https://github.com/anhtunguyen98/BiSLU
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_N/0/1/0/all/0/1&quot;&gt;Nguyen Anh Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uyen_H/0/1/0/all/0/1&quot;&gt;Hoang Thi Thu Uyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phuong_T/0/1/0/all/0/1&quot;&gt;Tu Minh Phuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bach_N/0/1/0/all/0/1&quot;&gt;Ngo Xuan Bach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10482">
<title>A new method color MS-BSIF Features learning for the robust kinship verification. (arXiv:2312.10482v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2312.10482</link>
<description rdf:parseType="Literal">&lt;p&gt;the paper presents a new method color MS-BSIF learning and MS-LBP for the
kinship verification is the machine&apos;s ability to identify the genetic and blood
the relationship and its degree between the facial images of humans. Facial
verification of kinship refers to the task of training a machine to recognize
the blood relationship between a pair of faces parent and non-parent
(verification) based on features extracted from facial images, and determining
the exact type or degree of this genetic relationship. We use the LBP and color
BSIF learning features for the comparison and the TXQDA method for
dimensionality reduction and data classification. We let&apos;s test the kinship
facial verification application is namely the kinface Cornell database. This
system improves the robustness of learning while controlling efficiency. The
experimental results obtained and compared to other methods have proven the
reliability of our framework and surpass the performance of other
state-of-the-art techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aliradi_R/0/1/0/all/0/1&quot;&gt;Rachid Aliradi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouamane_A/0/1/0/all/0/1&quot;&gt;Abdealmalik Ouamane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amrane_A/0/1/0/all/0/1&quot;&gt;Abdeslam Amrane&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>