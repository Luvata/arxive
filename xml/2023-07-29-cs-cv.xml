<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-27T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14352" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14354" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14382" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14392" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14433" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14460" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14482" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14487" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14523" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14570" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14571" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14575" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14593" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14603" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14605" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14612" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14624" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14637" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14638" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14659" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14710" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14735" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14748" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14768" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14777" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14825" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14859" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14864" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14889" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14901" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14907" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14959" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15007" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15029" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15042" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15049" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15058" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15061" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15063" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15064" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2005.00695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.10206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.04812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.00052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.11860" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.12300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.08944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11220" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.12292" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.16762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.13381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.13504" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01226" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01669" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.00570" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04186" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05077" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09160" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02082" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01524" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09715" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10853" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11411" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13957" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14127" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.14343">
<title>Pruning Distorted Images in MNIST Handwritten Digits. (arXiv:2307.14343v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14343</link>
<description rdf:parseType="Literal">&lt;p&gt;Recognizing handwritten digits is a challenging task primarily due to the
diversity of writing styles and the presence of noisy images. The widely used
MNIST dataset, which is commonly employed as a benchmark for this task,
includes distorted digits with irregular shapes, incomplete strokes, and
varying skew in both the training and testing datasets. Consequently, these
factors contribute to reduced accuracy in digit recognition. To overcome this
challenge, we propose a two-stage deep learning approach. In the first stage,
we create a simple neural network to identify distorted digits within the
training set. This model serves to detect and filter out such distorted and
ambiguous images. In the second stage, we exclude these identified images from
the training dataset and proceed to retrain the model using the filtered
dataset. This process aims to improve the classification accuracy and
confidence levels while mitigating issues of underfitting and overfitting. Our
experimental results demonstrate the effectiveness of the proposed approach,
achieving an accuracy rate of over 99.5% on the testing dataset. This
significant improvement showcases the potential of our method in enhancing
digit classification accuracy. In our future work, we intend to explore the
scalability of this approach and investigate techniques to further enhance
accuracy by reducing the size of the training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+R_A/0/1/0/all/0/1&quot;&gt;Amarnath R&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+V_V/0/1/0/all/0/1&quot;&gt;Vinay Kumar V&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14352">
<title>General Image-to-Image Translation with One-Shot Image Guidance. (arXiv:2307.14352v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14352</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale text-to-image models pre-trained on massive text-image pairs show
excellent performance in image synthesis recently. However, image can provide
more intuitive visual concepts than plain text. People may ask: how can we
integrate the desired visual concept into an existing image, such as our
portrait? Current methods are inadequate in meeting this demand as they lack
the ability to preserve content or translate visual concepts effectively.
Inspired by this, we propose a novel framework named visual concept translator
(VCT) with the ability to preserve content in the source image and translate
the visual concepts guided by a single reference image. The proposed VCT
contains a content-concept inversion (CCI) process to extract contents and
concepts, and a content-concept fusion (CCF) process to gather the extracted
information to obtain the target image. Given only one reference image, the
proposed VCT can complete a wide range of general image-to-image translation
tasks with excellent results. Extensive experiments are conducted to prove the
superiority and effectiveness of the proposed methods. Codes are available at
https://github.com/CrystalNeuro/visual-concept-translator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1&quot;&gt;Bin Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zuhao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yunbo Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yue Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14354">
<title>Learned Gridification for Efficient Point Cloud Processing. (arXiv:2307.14354v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14354</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural operations that rely on neighborhood information are much more
expensive when deployed on point clouds than on grid data due to the irregular
distances between points in a point cloud. In a grid, on the other hand, we can
compute the kernel only once and reuse it for all query positions. As a result,
operations that rely on neighborhood information scale much worse for point
clouds than for grid data, specially for large inputs and large neighborhoods.
&lt;/p&gt;
&lt;p&gt;In this work, we address the scalability issue of point cloud methods by
tackling its root cause: the irregularity of the data. We propose learnable
gridification as the first step in a point cloud processing pipeline to
transform the point cloud into a compact, regular grid. Thanks to
gridification, subsequent layers can use operations defined on regular grids,
e.g., Conv3D, which scale much better than native point cloud methods. We then
extend gridification to point cloud to point cloud tasks, e.g., segmentation,
by adding a learnable de-gridification step at the end of the point cloud
processing pipeline to map the compact, regular grid back to its original point
cloud form. Through theoretical and empirical analysis, we show that gridified
networks scale better in terms of memory and time than networks directly
applied on raw point cloud data, while being able to achieve competitive
results. Our code is publicly available at
https://github.com/computri/gridifier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linden_P/0/1/0/all/0/1&quot;&gt;Putri A. van der Linden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_D/0/1/0/all/0/1&quot;&gt;David W. Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1&quot;&gt;Erik J. Bekkers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14382">
<title>When Multi-Task Learning Meets Partial Supervision: A Computer Vision Review. (arXiv:2307.14382v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.14382</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Task Learning (MTL) aims to learn multiple tasks simultaneously while
exploiting their mutual relationships. By using shared resources to
simultaneously calculate multiple outputs, this learning paradigm has the
potential to have lower memory requirements and inference times compared to the
traditional approach of using separate methods for each task. Previous work in
MTL has mainly focused on fully-supervised methods, as task relationships can
not only be leveraged to lower the level of data-dependency of those methods
but they can also improve performance. However, MTL introduces a set of
challenges due to a complex optimisation scheme and a higher labeling
requirement. This review focuses on how MTL could be utilised under different
partial supervision settings to address these challenges. First, this review
analyses how MTL traditionally uses different parameter sharing techniques to
transfer knowledge in between tasks. Second, it presents the different
challenges arising from such a multi-objective optimisation scheme. Third, it
introduces how task groupings can be achieved by analysing task relationships.
Fourth, it focuses on how partially supervised methods applied to MTL can
tackle the aforementioned challenges. Lastly, this review presents the
available datasets, tools and benchmarking results of such methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fontana_M/0/1/0/all/0/1&quot;&gt;Maxime Fontana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spratling_M/0/1/0/all/0/1&quot;&gt;Michael Spratling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1&quot;&gt;Miaojing Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14392">
<title>Human-centric Scene Understanding for 3D Large-scale Scenarios. (arXiv:2307.14392v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14392</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-centric scene understanding is significant for real-world applications,
but it is extremely challenging due to the existence of diverse human poses and
actions, complex human-environment interactions, severe occlusions in crowds,
etc. In this paper, we present a large-scale multi-modal dataset for
human-centric scene understanding, dubbed HuCenLife, which is collected in
diverse daily-life scenarios with rich and fine-grained annotations. Our
HuCenLife can benefit many 3D perception tasks, such as segmentation,
detection, action recognition, etc., and we also provide benchmarks for these
tasks to facilitate related research. In addition, we design novel modules for
LiDAR-based segmentation and action recognition, which are more applicable for
large-scale human-centric scenarios and achieve state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yiteng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_P/0/1/0/all/0/1&quot;&gt;Peishan Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yichen Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Runnan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1&quot;&gt;Yuenan Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xinge Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xuming He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jingyi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuexin Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14397">
<title>A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot. (arXiv:2307.14397v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14397</link>
<description rdf:parseType="Literal">&lt;p&gt;In machine learning, generative modeling aims to learn to generate new data
statistically similar to the training data distribution. In this paper, we
survey learning generative models under limited data, few shots and zero shot,
referred to as Generative Modeling under Data Constraint (GM-DC). This is an
important topic when data acquisition is challenging, e.g. healthcare
applications. We discuss background, challenges, and propose two taxonomies:
one on GM-DC tasks and another on GM-DC approaches. Importantly, we study
interactions between different GM-DC tasks and approaches. Furthermore, we
highlight research gaps, research trends, and potential avenues for future
exploration. Project website: https://gmdc-survey.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdollahzadeh_M/0/1/0/all/0/1&quot;&gt;Milad Abdollahzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malekzadeh_T/0/1/0/all/0/1&quot;&gt;Touba Malekzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teo_C/0/1/0/all/0/1&quot;&gt;Christopher T. H. Teo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandrasegaran_K/0/1/0/all/0/1&quot;&gt;Keshigeyan Chandrasegaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guimeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1&quot;&gt;Ngai-Man Cheung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14403">
<title>Unsupervised Deep Learning-based Pansharpening with Jointly-Enhanced Spectral and Spatial Fidelity. (arXiv:2307.14403v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.14403</link>
<description rdf:parseType="Literal">&lt;p&gt;In latest years, deep learning has gained a leading role in the pansharpening
of multiresolution images. Given the lack of ground truth data, most deep
learning-based methods carry out supervised training in a reduced-resolution
domain. However, models trained on downsized images tend to perform poorly on
high-resolution target images. For this reason, several research groups are now
turning to unsupervised training in the full-resolution domain, through the
definition of appropriate loss functions and training paradigms. In this
context, we have recently proposed a full-resolution training framework which
can be applied to many existing architectures.
&lt;/p&gt;
&lt;p&gt;Here, we propose a new deep learning-based pansharpening model that fully
exploits the potential of this approach and provides cutting-edge performance.
Besides architectural improvements with respect to previous work, such as the
use of residual attention modules, the proposed model features a novel loss
function that jointly promotes the spectral and spatial quality of the
pansharpened data. In addition, thanks to a new fine-tuning strategy, it
improves inference-time adaptation to target images. Experiments on a large
variety of test images, performed in challenging scenarios, demonstrate that
the proposed method compares favorably with the state of the art both in terms
of numerical results and visual output. Code is available online at
https://github.com/matciotola/Lambda-PNN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ciotola_M/0/1/0/all/0/1&quot;&gt;Matteo Ciotola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Poggi_G/0/1/0/all/0/1&quot;&gt;Giovanni Poggi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Scarpa_G/0/1/0/all/0/1&quot;&gt;Giuseppe Scarpa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14433">
<title>ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography. (arXiv:2307.14433v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14433</link>
<description rdf:parseType="Literal">&lt;p&gt;Aortic stenosis (AS) is a common heart valve disease that requires accurate
and timely diagnosis for appropriate treatment. Most current automatic AS
severity detection methods rely on black-box models with a low level of
trustworthiness, which hinders clinical adoption. To address this issue, we
propose ProtoASNet, a prototypical network that directly detects AS from B-mode
echocardiography videos, while making interpretable predictions based on the
similarity between the input and learned spatio-temporal prototypes. This
approach provides supporting evidence that is clinically relevant, as the
prototypes typically highlight markers such as calcification and restricted
movement of aortic valve leaflets. Moreover, ProtoASNet utilizes abstention
loss to estimate aleatoric uncertainty by defining a set of prototypes that
capture ambiguity and insufficient information in the observed data. This
provides a reliable system that can detect and explain when it may fail. We
evaluate ProtoASNet on a private dataset and the publicly available TMED-2
dataset, where it outperforms existing state-of-the-art methods with an
accuracy of 80.0% and 79.7%, respectively. Furthermore, ProtoASNet provides
interpretability and an uncertainty measure for each prediction, which can
improve transparency and facilitate the interactive usage of deep networks to
aid clinical decision-making. Our source code is available at:
https://github.com/hooman007/ProtoASNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaseli_H/0/1/0/all/0/1&quot;&gt;Hooman Vaseli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1&quot;&gt;Ang Nan Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amiri_S/0/1/0/all/0/1&quot;&gt;S. Neda Ahmadi Amiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsang_M/0/1/0/all/0/1&quot;&gt;Michael Y. Tsang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fung_A/0/1/0/all/0/1&quot;&gt;Andrea Fung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondori_N/0/1/0/all/0/1&quot;&gt;Nima Kondori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saadat_A/0/1/0/all/0/1&quot;&gt;Armin Saadat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abolmaesumi_P/0/1/0/all/0/1&quot;&gt;Purang Abolmaesumi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsang_T/0/1/0/all/0/1&quot;&gt;Teresa S. M. Tsang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14436">
<title>Phenotype-preserving metric design for high-content image reconstruction by generative inpainting. (arXiv:2307.14436v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.14436</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past decades, automated high-content microscopy demonstrated its
ability to deliver large quantities of image-based data powering the
versatility of phenotypic drug screening and systems biology applications.
However, as the sizes of image-based datasets grew, it became infeasible for
humans to control, avoid and overcome the presence of imaging and sample
preparation artefacts in the images. While novel techniques like machine
learning and deep learning may address these shortcomings through generative
image inpainting, when applied to sensitive research data this may come at the
cost of undesired image manipulation. Undesired manipulation may be caused by
phenomena such as neural hallucinations, to which some artificial neural
networks are prone. To address this, here we evaluate the state-of-the-art
inpainting methods for image restoration in a high-content fluorescence
microscopy dataset of cultured cells with labelled nuclei. We show that
architectures like DeepFill V2 and Edge Connect can faithfully restore
microscopy images upon fine-tuning with relatively little data. Our results
demonstrate that the area of the region to be restored is of higher importance
than shape. Furthermore, to control for the quality of restoration, we propose
a novel phenotype-preserving metric design strategy. In this strategy, the size
and count of the restored biological phenotypes like cell nuclei are quantified
to penalise undesirable manipulation. We argue that the design principles of
our approach may also generalise to other applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sharma_V/0/1/0/all/0/1&quot;&gt;Vaibhav Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yakimovich_A/0/1/0/all/0/1&quot;&gt;Artur Yakimovich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14446">
<title>Self-supervised Few-shot Learning for Semantic Segmentation: An Annotation-free Approach. (arXiv:2307.14446v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14446</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot semantic segmentation (FSS) offers immense potential in the field of
medical image analysis, enabling accurate object segmentation with limited
training data. However, existing FSS techniques heavily rely on annotated
semantic classes, rendering them unsuitable for medical images due to the
scarcity of annotations. To address this challenge, multiple contributions are
proposed: First, inspired by spectral decomposition methods, the problem of
image decomposition is reframed as a graph partitioning task. The eigenvectors
of the Laplacian matrix, derived from the feature affinity matrix of
self-supervised networks, are analyzed to estimate the distribution of the
objects of interest from the support images. Secondly, we propose a novel
self-supervised FSS framework that does not rely on any annotation. Instead, it
adaptively estimates the query mask by leveraging the eigenvectors obtained
from the support images. This approach eliminates the need for manual
annotation, making it particularly suitable for medical images with limited
annotated data. Thirdly, to further enhance the decoding of the query image
based on the information provided by the support image, we introduce a
multi-scale large kernel attention module. By selectively emphasizing relevant
features and details, this module improves the segmentation process and
contributes to better object delineation. Evaluations on both natural and
medical image datasets demonstrate the efficiency and effectiveness of our
method. Moreover, the proposed approach is characterized by its generality and
model-agnostic nature, allowing for seamless integration with various deep
architectures. The code is publicly available at
\href{https://github.com/mindflow-institue/annotation_free_fewshot}{\textcolor{magenta}{GitHub}}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karimijafarbigloo_S/0/1/0/all/0/1&quot;&gt;Sanaz Karimijafarbigloo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azad_R/0/1/0/all/0/1&quot;&gt;Reza Azad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merhof_D/0/1/0/all/0/1&quot;&gt;Dorit Merhof&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14460">
<title>MiDaS v3.1 -- A Model Zoo for Robust Monocular Relative Depth Estimation. (arXiv:2307.14460v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14460</link>
<description rdf:parseType="Literal">&lt;p&gt;We release MiDaS v3.1 for monocular depth estimation, offering a variety of
new models based on different encoder backbones. This release is motivated by
the success of transformers in computer vision, with a large variety of
pretrained vision transformers now available. We explore how using the most
promising vision transformers as image encoders impacts depth estimation
quality and runtime of the MiDaS architecture. Our investigation also includes
recent convolutional approaches that achieve comparable quality to vision
transformers in image classification tasks. While the previous release MiDaS
v3.0 solely leverages the vanilla vision transformer ViT, MiDaS v3.1 offers
additional models based on BEiT, Swin, SwinV2, Next-ViT and LeViT. These models
offer different performance-runtime tradeoffs. The best model improves the
depth estimation quality by 28% while efficient models enable downstream tasks
requiring high frame rates. We also describe the general process for
integrating new backbones. A video summarizing the work can be found at
https://youtu.be/UjaeNNFf9sE and the code is available at
https://github.com/isl-org/MiDaS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Birkl_R/0/1/0/all/0/1&quot;&gt;Reiner Birkl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wofk_D/0/1/0/all/0/1&quot;&gt;Diana Wofk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1&quot;&gt;Matthias M&amp;#xfc;ller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14482">
<title>Role of Image Acquisition and Patient Phenotype Variations in Automatic Segmentation Model Generalization. (arXiv:2307.14482v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.14482</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: This study evaluated the out-of-domain performance and
generalization capabilities of automated medical image segmentation models,
with a particular focus on adaptation to new image acquisitions and disease
type.
&lt;/p&gt;
&lt;p&gt;Materials: Datasets from both non-contrast and contrast-enhanced abdominal CT
scans of healthy patients and those with polycystic kidney disease (PKD) were
used. A total of 400 images (100 non-contrast controls, 100 contrast controls,
100 non-contrast PKD, 100 contrast PKD) were utilized for training/validation
of models to segment kidneys, livers, and spleens, and the final models were
then tested on 100 non-contrast CT images of patients affected by PKD.
Performance was evaluated using Dice, Jaccard, TPR, and Precision.
&lt;/p&gt;
&lt;p&gt;Results: Models trained on a diverse range of data showed no worse
performance than models trained exclusively on in-domain data when tested on
in-domain data. For instance, the Dice similarity of the model trained on 25%
from each dataset was found to be non-inferior to the model trained purely on
in-domain data.
&lt;/p&gt;
&lt;p&gt;Conclusions: The results indicate that broader training examples
significantly enhances model generalization and out-of-domain performance,
thereby improving automated segmentation tools&apos; applicability in clinical
settings. The study&apos;s findings provide a roadmap for future research to adopt a
data-centric approach in medical image AI model development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kline_T/0/1/0/all/0/1&quot;&gt;Timothy L. Kline&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ramanathan_S/0/1/0/all/0/1&quot;&gt;Sumana Ramanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gottlich_H/0/1/0/all/0/1&quot;&gt;Harrison C. Gottlich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Korfiatis_P/0/1/0/all/0/1&quot;&gt;Panagiotis Korfiatis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gregory_A/0/1/0/all/0/1&quot;&gt;Adriana V. Gregory&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14487">
<title>Technical note: ShinyAnimalCV: open-source cloud-based web application for object detection, segmentation, and three-dimensional visualization of animals using computer vision. (arXiv:2307.14487v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14487</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer vision (CV), a non-intrusive and cost-effective technology, has
furthered the development of precision livestock farming by enabling optimized
decision-making through timely and individualized animal care. The availability
of affordable two- and three-dimensional camera sensors, combined with various
machine learning and deep learning algorithms, has provided a valuable
opportunity to improve livestock production systems. However, despite the
availability of various CV tools in the public domain, applying these tools to
animal data can be challenging, often requiring users to have programming and
data analysis skills, as well as access to computing resources. Moreover, the
rapid expansion of precision livestock farming is creating a growing need to
educate and train animal science students in CV. This presents educators with
the challenge of efficiently demonstrating the complex algorithms involved in
CV. Thus, the objective of this study was to develop ShinyAnimalCV, an
open-source cloud-based web application. This application provides a
user-friendly interface for performing CV tasks, including object segmentation,
detection, three-dimensional surface visualization, and extraction of two- and
three-dimensional morphological features. Nine pre-trained CV models using
top-view animal data are included in the application. ShinyAnimalCV has been
deployed online using cloud computing platforms. The source code of
ShinyAnimalCV is available on GitHub, along with detailed documentation on
training CV models using custom data and deploying ShinyAnimalCV locally to
allow users to fully leverage the capabilities of the application.
ShinyAnimalCV can contribute to CV research and teaching in the animal science
community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1&quot;&gt;Lirong Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morota_G/0/1/0/all/0/1&quot;&gt;Gota Morota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brooks_S/0/1/0/all/0/1&quot;&gt;Samantha A. Brooks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wickens_C/0/1/0/all/0/1&quot;&gt;Carissa L. Wickens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_Cushon_E/0/1/0/all/0/1&quot;&gt;Emily K. Miller-Cushon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haipeng Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14489">
<title>SuperInpaint: Learning Detail-Enhanced Attentional Implicit Representation for Super-resolutional Image Inpainting. (arXiv:2307.14489v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14489</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we introduce a challenging image restoration task, referred to
as SuperInpaint, which aims to reconstruct missing regions in low-resolution
images and generate completed images with arbitrarily higher resolutions. We
have found that this task cannot be effectively addressed by stacking
state-of-the-art super-resolution and image inpainting methods as they amplify
each other&apos;s flaws, leading to noticeable artifacts. To overcome these
limitations, we propose the detail-enhanced attentional implicit representation
(DEAR) that can achieve SuperInpaint with a single model, resulting in
high-quality completed images with arbitrary resolutions. Specifically, we use
a deep convolutional network to extract the latent embedding of an input image
and then enhance the high-frequency components of the latent embedding via an
adaptive high-pass filter. This leads to detail-enhanced semantic embedding. We
further feed the semantic embedding into an unmask-attentional module that
suppresses embeddings from ineffective masked pixels. Additionally, we extract
a pixel-wise importance map that indicates which pixels should be used for
image reconstruction. Given the coordinates of a pixel we want to reconstruct,
we first collect its neighboring pixels in the input image and extract their
detail-enhanced semantic embeddings, unmask-attentional semantic embeddings,
importance values, and spatial distances to the desired pixel. Then, we feed
all the above terms into an implicit representation and generate the color of
the specified pixel. To evaluate our method, we extend three existing datasets
for this new task and build 18 meaningful baselines using SOTA inpainting and
super-resolution methods. Extensive experimental results demonstrate that our
method outperforms all existing methods by a significant margin on four widely
used metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Canyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qing Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoguang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_R/0/1/0/all/0/1&quot;&gt;Renjie Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hongkai Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1&quot;&gt;Ivor Tsang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Song Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14517">
<title>The Co-12 Recipe for Evaluating Interpretable Part-Prototype Image Classifiers. (arXiv:2307.14517v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14517</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretable part-prototype models are computer vision models that are
explainable by design. The models learn prototypical parts and recognise these
components in an image, thereby combining classification and explanation.
Despite the recent attention for intrinsically interpretable models, there is
no comprehensive overview on evaluating the explanation quality of
interpretable part-prototype models. Based on the Co-12 properties for
explanation quality as introduced in &lt;a href=&quot;/abs/2201.08164&quot;&gt;arXiv:2201.08164&lt;/a&gt; (e.g., correctness,
completeness, compactness), we review existing work that evaluates
part-prototype models, reveal research gaps and outline future approaches for
evaluation of the explanation quality of part-prototype models. This paper,
therefore, contributes to the progression and maturity of this relatively new
research field on interpretable part-prototype models. We additionally provide
a ``Co-12 cheat sheet&apos;&apos; that acts as a concise summary of our findings on
evaluating part-prototype models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nauta_M/0/1/0/all/0/1&quot;&gt;Meike Nauta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seifert_C/0/1/0/all/0/1&quot;&gt;Christin Seifert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14520">
<title>FocalErrorNet: Uncertainty-aware focal modulation network for inter-modal registration error estimation in ultrasound-guided neurosurgery. (arXiv:2307.14520v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.14520</link>
<description rdf:parseType="Literal">&lt;p&gt;In brain tumor resection, accurate removal of cancerous tissues while
preserving eloquent regions is crucial to the safety and outcomes of the
treatment. However, intra-operative tissue deformation (called brain shift) can
move the surgical target and render the pre-surgical plan invalid.
Intra-operative ultrasound (iUS) has been adopted to provide real-time images
to track brain shift, and inter-modal (i.e., MRI-iUS) registration is often
required to update the pre-surgical plan. Quality control for the registration
results during surgery is important to avoid adverse outcomes, but manual
verification faces great challenges due to difficult 3D visualization and the
low contrast of iUS. Automatic algorithms are urgently needed to address this
issue, but the problem was rarely attempted. Therefore, we propose a novel deep
learning technique based on 3D focal modulation in conjunction with uncertainty
estimation to accurately assess MRI-iUS registration errors for brain tumor
surgery. Developed and validated with the public RESECT clinical database, the
resulting algorithm can achieve an estimation error of 0.59+-0.57 mm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Salari_S/0/1/0/all/0/1&quot;&gt;Soorena Salari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rasoulian_A/0/1/0/all/0/1&quot;&gt;Amirhossein Rasoulian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rivaz_H/0/1/0/all/0/1&quot;&gt;Hassan Rivaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yiming Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14521">
<title>Patterns of Vehicle Lights: Addressing Complexities in Curation and Annotation of Camera-Based Vehicle Light Datasets and Metrics. (arXiv:2307.14521v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14521</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the representation of vehicle lights in computer vision
and its implications for various tasks in the field of autonomous driving.
Different specifications for representing vehicle lights, including bounding
boxes, center points, corner points, and segmentation masks, are discussed in
terms of their strengths and weaknesses. Three important tasks in autonomous
driving that can benefit from vehicle light detection are identified: nighttime
vehicle detection, 3D vehicle orientation estimation, and dynamic trajectory
cues. Each task may require a different representation of the light. The
challenges of collecting and annotating large datasets for training data-driven
models are also addressed, leading to introduction of the LISA Vehicle Lights
Dataset and associated Light Visibility Model, which provides light annotations
specifically designed for downstream applications in vehicle detection, intent
and trajectory prediction, and safe path planning. A comparison of existing
vehicle light datasets is provided, highlighting the unique features and
limitations of each dataset. Overall, this paper provides insights into the
representation of vehicle lights and the importance of accurate annotations for
training effective detection models in autonomous driving applications. Our
dataset and model are made available at
https://cvrr.ucsd.edu/vehicle-lights-dataset
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greer_R/0/1/0/all/0/1&quot;&gt;Ross Greer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalkrishnan_A/0/1/0/all/0/1&quot;&gt;Akshay Gopalkrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keskar_M/0/1/0/all/0/1&quot;&gt;Maitrayee Keskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1&quot;&gt;Mohan Trivedi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14523">
<title>Towards multi-modal anatomical landmark detection for ultrasound-guided brain tumor resection with contrastive learning. (arXiv:2307.14523v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14523</link>
<description rdf:parseType="Literal">&lt;p&gt;Homologous anatomical landmarks between medical scans are instrumental in
quantitative assessment of image registration quality in various clinical
applications, such as MRI-ultrasound registration for tissue shift correction
in ultrasound-guided brain tumor resection. While manually identified landmark
pairs between MRI and ultrasound (US) have greatly facilitated the validation
of different registration algorithms for the task, the procedure requires
significant expertise, labor, and time, and can be prone to inter- and
intra-rater inconsistency. So far, many traditional and machine learning
approaches have been presented for anatomical landmark detection, but they
primarily focus on mono-modal applications. Unfortunately, despite the clinical
needs, inter-modal/contrast landmark detection has very rarely been attempted.
Therefore, we propose a novel contrastive learning framework to detect
corresponding landmarks between MRI and intra-operative US scans in
neurosurgery. Specifically, two convolutional neural networks were trained
jointly to encode image features in MRI and US scans to help match the US image
patch that contain the corresponding landmarks in the MRI. We developed and
validated the technique using the public RESECT database. With a mean landmark
detection accuracy of 5.88+-4.79 mm against 18.78+-4.77 mm with SIFT features,
the proposed method offers promising results for MRI-US landmark detection in
neurosurgical applications for the first time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salari_S/0/1/0/all/0/1&quot;&gt;Soorena Salari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasoulian_A/0/1/0/all/0/1&quot;&gt;Amirhossein Rasoulian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivaz_H/0/1/0/all/0/1&quot;&gt;Hassan Rivaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yiming Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14527">
<title>Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad. (arXiv:2307.14527v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14527</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper details the challenges in applying two computer vision systems, an
EfficientDET supervised learning model and the unsupervised RX spectral
classifier, to 98.9 GB of drone imagery from the Wu-Murad wilderness search and
rescue (WSAR) effort in Japan and identifies 3 directions for future research.
There have been at least 19 proposed approaches and 3 datasets aimed at
locating missing persons in drone imagery, but only 3 approaches (2
unsupervised and 1 of an unknown structure) are referenced in the literature as
having been used in an actual WSAR operation. Of these proposed approaches, the
EfficientDET architecture and the unsupervised spectral RX classifier were
selected as the most appropriate for this setting. The EfficientDET model was
applied to the HERIDAL dataset and despite achieving performance that is
statistically equivalent to the state-of-the-art, the model fails to translate
to the real world in terms of false positives (e.g., identifying tree limbs and
rocks as people), and false negatives (e.g., failing to identify members of the
search team). The poor results in practice for algorithms that showed good
results on datasets suggest 3 areas of future research: more realistic datasets
for wilderness SAR, computer vision models that are capable of seamlessly
handling the variety of imagery that can be collected during actual WSAR
operations, and better alignment on performance measures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manzini_T/0/1/0/all/0/1&quot;&gt;Thomas Manzini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphy_R/0/1/0/all/0/1&quot;&gt;Robin Murphy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14570">
<title>Physically Plausible 3D Human-Scene Reconstruction from Monocular RGB Image using an Adversarial Learning Approach. (arXiv:2307.14570v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14570</link>
<description rdf:parseType="Literal">&lt;p&gt;Holistic 3D human-scene reconstruction is a crucial and emerging research
area in robot perception. A key challenge in holistic 3D human-scene
reconstruction is to generate a physically plausible 3D scene from a single
monocular RGB image. The existing research mainly proposes optimization-based
approaches for reconstructing the scene from a sequence of RGB frames with
explicitly defined physical laws and constraints between different scene
elements (humans and objects). However, it is hard to explicitly define and
model every physical law in every scenario. This paper proposes using an
implicit feature representation of the scene elements to distinguish a
physically plausible alignment of humans and objects from an implausible one.
We propose using a graph-based holistic representation with an encoded physical
representation of the scene to analyze the human-object and object-object
interactions within the scene. Using this graphical representation, we
adversarially train our model to learn the feasible alignments of the scene
elements from the training data itself without explicitly defining the laws and
constraints between them. Unlike the existing inference-time optimization-based
approaches, we use this adversarially trained model to produce a per-frame 3D
reconstruction of the scene that abides by the physical laws and constraints.
Our learning-based method achieves comparable 3D reconstruction quality to
existing optimization-based holistic human-scene reconstruction methods and
does not need inference time optimization. This makes it better suited when
compared to existing methods, for potential use in robotic applications, such
as robot navigation, etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1&quot;&gt;Sandika Biswas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kejie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1&quot;&gt;Biplab Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1&quot;&gt;Subhasis Chaudhuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezatofighi_H/0/1/0/all/0/1&quot;&gt;Hamid Rezatofighi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14571">
<title>Robust Detection, Assocation, and Localization of Vehicle Lights: A Context-Based Cascaded CNN Approach and Evaluations. (arXiv:2307.14571v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14571</link>
<description rdf:parseType="Literal">&lt;p&gt;Vehicle light detection is required for important downstream safe autonomous
driving tasks, such as predicting a vehicle&apos;s light state to determine if the
vehicle is making a lane change or turning. Currently, many vehicle light
detectors use single-stage detectors which predict bounding boxes to identify a
vehicle light, in a manner decoupled from vehicle instances. In this paper, we
present a method for detecting a vehicle light given an upstream vehicle
detection and approximation of a visible light&apos;s center. Our method predicts
four approximate corners associated with each vehicle light. We experiment with
CNN architectures, data augmentation, and contextual preprocessing methods
designed to reduce surrounding-vehicle confusion. We achieve an average
distance error from the ground truth corner of 5.09 pixels, about 17.24% of the
size of the vehicle light on average. We train and evaluate our model on the
LISA Lights dataset, allowing us to thoroughly evaluate our vehicle light
corner detection model on a large variety of vehicle light shapes and lighting
conditions. We propose that this model can be integrated into a pipeline with
vehicle detection and vehicle light center detection to make a fully-formed
vehicle light detection network, valuable to identifying trajectory-informative
signals in driving scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalkrishnan_A/0/1/0/all/0/1&quot;&gt;Akshay Gopalkrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greer_R/0/1/0/all/0/1&quot;&gt;Ross Greer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keskar_M/0/1/0/all/0/1&quot;&gt;Maitrayee Keskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1&quot;&gt;Mohan Trivedi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14575">
<title>A Memory-Augmented Multi-Task Collaborative Framework for Unsupervised Traffic Accident Detection in Driving Videos. (arXiv:2307.14575v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14575</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying traffic accidents in driving videos is crucial to ensuring the
safety of autonomous driving and driver assistance systems. To address the
potential danger caused by the long-tailed distribution of driving events,
existing traffic accident detection (TAD) methods mainly rely on unsupervised
learning. However, TAD is still challenging due to the rapid movement of
cameras and dynamic scenes in driving scenarios. Existing unsupervised TAD
methods mainly rely on a single pretext task, i.e., an appearance-based or
future object localization task, to detect accidents. However, appearance-based
approaches are easily disturbed by the rapid movement of the camera and changes
in illumination, which significantly reduce the performance of traffic accident
detection. Methods based on future object localization may fail to capture
appearance changes in video frames, making it difficult to detect ego-involved
accidents (e.g., out of control of the ego-vehicle). In this paper, we propose
a novel memory-augmented multi-task collaborative framework (MAMTCF) for
unsupervised traffic accident detection in driving videos. Different from
previous approaches, our method can more accurately detect both ego-involved
and non-ego accidents by simultaneously modeling appearance changes and object
motions in video frames through the collaboration of optical flow
reconstruction and future object localization tasks. Further, we introduce a
memory-augmented motion representation mechanism to fully explore the
interrelation between different types of motion representations and exploit the
high-level features of normal traffic patterns stored in memory to augment
motion representations, thus enlarging the difference from anomalies.
Experimental results on recently published large-scale dataset demonstrate that
our method achieves better performance compared to previous state-of-the-art
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1&quot;&gt;Rongqin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanman Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_Y/0/1/0/all/0/1&quot;&gt;Yingxin Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiantao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xia Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14578">
<title>GADER: GAit DEtection and Recognition in the Wild. (arXiv:2307.14578v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14578</link>
<description rdf:parseType="Literal">&lt;p&gt;Gait recognition holds the promise of robustly identifying subjects based on
their walking patterns instead of color information. While previous approaches
have performed well for curated indoor scenes, they have significantly impeded
applicability in unconstrained situations, e.g. outdoor, long distance scenes.
We propose an end-to-end GAit DEtection and Recognition (GADER) algorithm for
human authentication in challenging outdoor scenarios. Specifically, GADER
leverages a Double Helical Signature to detect the fragment of human movement
and incorporates a novel gait recognition method, which learns representations
by distilling from an auxiliary RGB recognition model. At inference time, GADER
only uses the silhouette modality but benefits from a more robust
representation. Extensive experiments on indoor and outdoor datasets
demonstrate that the proposed method outperforms the State-of-The-Arts for gait
recognition and verification, with a significant 20.6% improvement on
unconstrained, long distance scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1&quot;&gt;Cheng Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhakar_R/0/1/0/all/0/1&quot;&gt;Ram Prabhakar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lau_C/0/1/0/all/0/1&quot;&gt;Chun Pong Lau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1&quot;&gt;Rama Chellappa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14579">
<title>Neural Representation-Based Method for Metal-induced Artifact Reduction in Dental CBCT Imaging. (arXiv:2307.14579v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14579</link>
<description rdf:parseType="Literal">&lt;p&gt;This study introduces a novel reconstruction method for dental cone-beam
computed tomography (CBCT), focusing on effectively reducing metal-induced
artifacts commonly encountered in the presence of prevalent metallic implants.
Despite significant progress in metal artifact reduction techniques, challenges
persist owing to the intricate physical interactions between polychromatic
X-ray beams and metal objects, which are further compounded by the additional
effects associated with metal-tooth interactions and factors specific to the
dental CBCT data environment. To overcome these limitations, we propose an
implicit neural network that generates two distinct and informative tomographic
images. One image represents the monochromatic attenuation distribution at a
specific energy level, whereas the other captures the nonlinear beam-hardening
factor resulting from the polychromatic nature of X-ray beams. In contrast to
existing CT reconstruction techniques, the proposed method relies exclusively
on the Beer--Lambert law, effectively preventing the generation of
metal-induced artifacts during the backprojection process commonly implemented
in conventional methods. Extensive experimental evaluations demonstrate that
the proposed method effectively reduces metal artifacts while providing
high-quality image reconstructions, thus emphasizing the significance of the
second image in capturing the nonlinear beam-hardening factor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1&quot;&gt;Hyoung Suk Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_K/0/1/0/all/0/1&quot;&gt;Kiwan Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Jin Keun Seo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14588">
<title>MCPA: Multi-scale Cross Perceptron Attention Network for 2D Medical Image Segmentation. (arXiv:2307.14588v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.14588</link>
<description rdf:parseType="Literal">&lt;p&gt;The UNet architecture, based on Convolutional Neural Networks (CNN), has
demonstrated its remarkable performance in medical image analysis. However, it
faces challenges in capturing long-range dependencies due to the limited
receptive fields and inherent bias of convolutional operations. Recently,
numerous transformer-based techniques have been incorporated into the UNet
architecture to overcome this limitation by effectively capturing global
feature correlations. However, the integration of the Transformer modules may
result in the loss of local contextual information during the global feature
fusion process. To overcome these challenges, we propose a 2D medical image
segmentation model called Multi-scale Cross Perceptron Attention Network
(MCPA). The MCPA consists of three main components: an encoder, a decoder, and
a Cross Perceptron. The Cross Perceptron first captures the local correlations
using multiple Multi-scale Cross Perceptron modules, facilitating the fusion of
features across scales. The resulting multi-scale feature vectors are then
spatially unfolded, concatenated, and fed through a Global Perceptron module to
model global dependencies. Furthermore, we introduce a Progressive Dual-branch
Structure to address the semantic segmentation of the image involving finer
tissue structures. This structure gradually shifts the segmentation focus of
MCPA network training from large-scale structural features to more
sophisticated pixel-level features. We evaluate our proposed MCPA model on
several publicly available medical image datasets from different tasks and
devices, including the open large-scale dataset of CT (Synapse), MRI (ACDC),
fundus camera (DRIVE, CHASE_DB1, HRF), and OCTA (ROSE). The experimental
results show that our MCPA model achieves state-of-the-art performance. The
code is available at
https://github.com/simonustc/MCPA-for-2D-Medical-Image-Segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Liang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingxiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shao_P/0/1/0/all/0/1&quot;&gt;Pengfei Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_S/0/1/0/all/0/1&quot;&gt;Shuwei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yao_P/0/1/0/all/0/1&quot;&gt;Peng Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Ronald X.Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14591">
<title>The detection and rectification for identity-switch based on unfalsified control. (arXiv:2307.14591v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14591</link>
<description rdf:parseType="Literal">&lt;p&gt;The purpose of multi-object tracking (MOT) is to continuously track and
identify objects detected in videos. Currently, most methods for multi-object
tracking model the motion information and combine it with appearance
information to determine and track objects. In this paper, unfalsified control
is employed to address the ID-switch problem in multi-object tracking. We
establish sequences of appearance information variations for the trajectories
during the tracking process and design a detection and rectification module
specifically for ID-switch detection and recovery. We also propose a simple and
effective strategy to address the issue of ambiguous matching of appearance
information during the data association process. Experimental results on
publicly available MOT datasets demonstrate that the tracker exhibits excellent
effectiveness and robustness in handling tracking errors caused by occlusions
and rapid movements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Junchao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaoqi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Sheng Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14593">
<title>FakeTracer: Proactively Defending Against Face-swap DeepFakes via Implanting Traces in Training. (arXiv:2307.14593v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14593</link>
<description rdf:parseType="Literal">&lt;p&gt;Face-swap DeepFake is an emerging AI-based face forgery technique that can
replace the original face in a video with a generated face of the target
identity while retaining consistent facial attributes such as expression and
orientation. Due to the high privacy of faces, the misuse of this technique can
raise severe social concerns, drawing tremendous attention to defend against
DeepFakes recently. In this paper, we describe a new proactive defense method
called FakeTracer to expose face-swap DeepFakes via implanting traces in
training. Compared to general face-synthesis DeepFake, the face-swap DeepFake
is more complex as it involves identity change, is subjected to the
encoding-decoding process, and is trained unsupervised, increasing the
difficulty of implanting traces into the training phase. To effectively defend
against face-swap DeepFake, we design two types of traces, sustainable trace
(STrace) and erasable trace (ETrace), to be added to training faces. During the
training, these manipulated faces affect the learning of the face-swap DeepFake
model, enabling it to generate faces that only contain sustainable traces. In
light of these two traces, our method can effectively expose DeepFakes by
identifying them. Extensive experiments are conducted on the Celeb-DF dataset,
compared with recent passive and proactive defense methods, and are studied
thoroughly regarding various factors, corroborating the efficacy of our method
on defending against face-swap DeepFake.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1&quot;&gt;Pu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1&quot;&gt;Honggang Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuezun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Siwei Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14603">
<title>A Weakly Supervised Segmentation Network Embedding Cross-scale Attention Guidance and Noise-sensitive Constraint for Detecting Tertiary Lymphoid Structures of Pancreatic Tumors. (arXiv:2307.14603v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.14603</link>
<description rdf:parseType="Literal">&lt;p&gt;The presence of tertiary lymphoid structures (TLSs) on pancreatic
pathological images is an important prognostic indicator of pancreatic tumors.
Therefore, TLSs detection on pancreatic pathological images plays a crucial
role in diagnosis and treatment for patients with pancreatic tumors. However,
fully supervised detection algorithms based on deep learning usually require a
large number of manual annotations, which is time-consuming and
labor-intensive. In this paper, we aim to detect the TLSs in a manner of
few-shot learning by proposing a weakly supervised segmentation network. We
firstly obtain the lymphocyte density maps by combining a pretrained model for
nuclei segmentation and a domain adversarial network for lymphocyte nuclei
recognition. Then, we establish a cross-scale attention guidance mechanism by
jointly learning the coarse-scale features from the original histopathology
images and fine-scale features from our designed lymphocyte density attention.
A noise-sensitive constraint is introduced by an embedding signed distance
function loss in the training procedure to reduce tiny prediction errors.
Experimental results on two collected datasets demonstrate that our proposed
method significantly outperforms the state-of-the-art segmentation-based
algorithms in terms of TLSs detection accuracy. Additionally, we apply our
method to study the congruent relationship between the density of TLSs and
peripancreatic vascular invasion and obtain some clinically statistical
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bingxue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zou_L/0/1/0/all/0/1&quot;&gt;Liwen Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yingying Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zhenghua Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qiu_Y/0/1/0/all/0/1&quot;&gt;Yudong Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mao_L/0/1/0/all/0/1&quot;&gt;Liang Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhongqiu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jingya Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gui_L/0/1/0/all/0/1&quot;&gt;Luying Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaoping Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14605">
<title>Clustering based Point Cloud Representation Learning for 3D Analysis. (arXiv:2307.14605v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14605</link>
<description rdf:parseType="Literal">&lt;p&gt;Point cloud analysis (such as 3D segmentation and detection) is a challenging
task, because of not only the irregular geometries of many millions of
unordered points, but also the great variations caused by depth, viewpoint,
occlusion, etc. Current studies put much focus on the adaption of neural
networks to the complex geometries of point clouds, but are blind to a
fundamental question: how to learn an appropriate point embedding space that is
aware of both discriminative semantics and challenging variations? As a
response, we propose a clustering based supervised learning scheme for point
cloud analysis. Unlike current de-facto, scene-wise training paradigm, our
algorithm conducts within-class clustering on the point embedding space for
automatically discovering subclass patterns which are latent yet representative
across scenes. The mined patterns are, in turn, used to repaint the embedding
space, so as to respect the underlying distribution of the entire training
dataset and improve the robustness to the variations. Our algorithm is
principled and readily pluggable to modern point cloud segmentation networks
during training, without extra overhead during testing. With various 3D network
architectures (i.e., voxel-based, point-based, Transformer-based, automatically
searched), our algorithm shows notable improvements on famous point cloud
segmentation datasets (i.e.,2.0-2.6% on single-scan and 2.0-2.2% multi-scan of
SemanticKITTI, 1.8-1.9% on S3DIS, in terms of mIoU). Our algorithm also
demonstrates utility in 3D detection, showing 2.0-3.4% mAP gains on KITTI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1&quot;&gt;Tuo Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenguan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaohan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qinghua Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14611">
<title>TextManiA: Enriching Visual Feature by Text-driven Manifold Augmentation. (arXiv:2307.14611v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14611</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent label mix-based augmentation methods have shown their effectiveness in
generalization despite their simplicity, and their favorable effects are often
attributed to semantic-level augmentation. However, we found that they are
vulnerable to highly skewed class distribution, because scarce data classes are
rarely sampled for inter-class perturbation. We propose TextManiA, a
text-driven manifold augmentation method that semantically enriches visual
feature spaces, regardless of data distribution. TextManiA augments visual data
with intra-class semantic perturbation by exploiting easy-to-understand
visually mimetic words, i.e., attributes. To this end, we bridge between the
text representation and a target visual feature space, and propose an efficient
vector augmentation. To empirically support the validity of our design, we
devise two visualization-based analyses and show the plausibility of the bridge
between two different modality spaces. Our experiments demonstrate that
TextManiA is powerful in scarce samples with class imbalance as well as even
distribution. We also show compatibility with the label mix-based approaches in
evenly distributed scarce data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Bin_M/0/1/0/all/0/1&quot;&gt;Moon Ye-Bin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jisoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hongyeob Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Son_K/0/1/0/all/0/1&quot;&gt;Kilho Son&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1&quot;&gt;Tae-Hyun Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14612">
<title>GenCo: An Auxiliary Generator from Contrastive Learning for Enhanced Few-Shot Learning in Remote Sensing. (arXiv:2307.14612v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14612</link>
<description rdf:parseType="Literal">&lt;p&gt;Classifying and segmenting patterns from a limited number of examples is a
significant challenge in remote sensing and earth observation due to the
difficulty in acquiring accurately labeled data in large quantities. Previous
studies have shown that meta-learning, which involves episodic training on
query and support sets, is a promising approach. However, there has been little
attention paid to direct fine-tuning techniques. This paper repurposes
contrastive learning as a pre-training method for few-shot learning for
classification and semantic segmentation tasks. Specifically, we introduce a
generator-based contrastive learning framework (GenCo) that pre-trains
backbones and simultaneously explores variants of feature samples. In
fine-tuning, the auxiliary generator can be used to enrich limited labeled data
samples in feature space. We demonstrate the effectiveness of our method in
improving few-shot learning performance on two key remote sensing datasets:
Agriculture-Vision and EuroSAT. Empirically, our approach outperforms purely
supervised training on the nearly 95,000 images in Agriculture-Vision for both
classification and semantic segmentation tasks. Similarly, the proposed
few-shot method achieves better results on the land-cover classification task
on EuroSAT compared to the results obtained from fully supervised model
training on the dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jing Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hovakimyan_N/0/1/0/all/0/1&quot;&gt;Naira Hovakimyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hobbs_J/0/1/0/all/0/1&quot;&gt;Jennifer Hobbs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14617">
<title>Multiscale Dynamic Graph Representation for Biometric Recognition with Occlusions. (arXiv:2307.14617v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14617</link>
<description rdf:parseType="Literal">&lt;p&gt;Occlusion is a common problem with biometric recognition in the wild. The
generalization ability of CNNs greatly decreases due to the adverse effects of
various occlusions. To this end, we propose a novel unified framework
integrating the merits of both CNNs and graph models to overcome occlusion
problems in biometric recognition, called multiscale dynamic graph
representation (MS-DGR). More specifically, a group of deep features reflected
on certain subregions is recrafted into a feature graph (FG). Each node inside
the FG is deemed to characterize a specific local region of the input sample,
and the edges imply the co-occurrence of non-occluded regions. By analyzing the
similarities of the node representations and measuring the topological
structures stored in the adjacent matrix, the proposed framework leverages
dynamic graph matching to judiciously discard the nodes corresponding to the
occluded parts. The multiscale strategy is further incorporated to attain more
diverse nodes representing regions of various sizes. Furthermore, the proposed
framework exhibits a more illustrative and reasonable inference by showing the
paired nodes. Extensive experiments demonstrate the superiority of the proposed
framework, which boosts the accuracy in both natural and occlusion-simulated
cases by a large margin compared with that of baseline methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1&quot;&gt;Min Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunlong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kunbo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhenan Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14620">
<title>NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection. (arXiv:2307.14620v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14620</link>
<description rdf:parseType="Literal">&lt;p&gt;We present NeRF-Det, a novel method for indoor 3D detection with posed RGB
images as input. Unlike existing indoor 3D detection methods that struggle to
model scene geometry, our method makes novel use of NeRF in an end-to-end
manner to explicitly estimate 3D geometry, thereby improving 3D detection
performance. Specifically, to avoid the significant extra latency associated
with per-scene optimization of NeRF, we introduce sufficient geometry priors to
enhance the generalizability of NeRF-MLP. Furthermore, we subtly connect the
detection and NeRF branches through a shared MLP, enabling an efficient
adaptation of NeRF to detection and yielding geometry-aware volumetric
representations for 3D detection. Our method outperforms state-of-the-arts by
3.9 mAP and 3.1 mAP on the ScanNet and ARKITScenes benchmarks, respectively. We
provide extensive analysis to shed light on how NeRF-Det works. As a result of
our joint-training design, NeRF-Det is able to generalize well to unseen scenes
for object detection, view synthesis, and depth estimation tasks without
requiring per-scene optimization. Code is available at
\url{https://github.com/facebookresearch/NeRF-Det}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chenfeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bichen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Ji Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsai_S/0/1/0/all/0/1&quot;&gt;Sam Tsai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruilong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jialiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1&quot;&gt;Wei Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zijian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1&quot;&gt;Peter Vajda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1&quot;&gt;Kurt Keutzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1&quot;&gt;Masayoshi Tomizuka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14624">
<title>FS-Depth: Focal-and-Scale Depth Estimation from a Single Image in Unseen Indoor Scene. (arXiv:2307.14624v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14624</link>
<description rdf:parseType="Literal">&lt;p&gt;It has long been an ill-posed problem to predict absolute depth maps from
single images in real (unseen) indoor scenes. We observe that it is essentially
due to not only the scale-ambiguous problem but also the focal-ambiguous
problem that decreases the generalization ability of monocular depth
estimation. That is, images may be captured by cameras of different focal
lengths in scenes of different scales. In this paper, we develop a
focal-and-scale depth estimation model to well learn absolute depth maps from
single images in unseen indoor scenes. First, a relative depth estimation
network is adopted to learn relative depths from single images with diverse
scales/semantics. Second, multi-scale features are generated by mapping a
single focal length value to focal length features and concatenating them with
intermediate features of different scales in relative depth estimation.
Finally, relative depths and multi-scale features are jointly fed into an
absolute depth estimation network. In addition, a new pipeline is developed to
augment the diversity of focal lengths of public datasets, which are often
captured with cameras of the same or similar focal lengths. Our model is
trained on augmented NYUDv2 and tested on three unseen datasets. Our model
considerably improves the generalization ability of depth estimation by 41%/13%
(RMSE) with/without data augmentation compared with five recent SOTAs and well
alleviates the deformation problem in 3D reconstruction. Notably, our model
well maintains the accuracy of depth estimation on original NYUDv2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1&quot;&gt;Chengrui Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Meng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Lei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1&quot;&gt;Nanning Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14630">
<title>360VOT: A New Benchmark Dataset for Omnidirectional Visual Object Tracking. (arXiv:2307.14630v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14630</link>
<description rdf:parseType="Literal">&lt;p&gt;360{\deg} images can provide an omnidirectional field of view which is
important for stable and long-term scene perception. In this paper, we explore
360{\deg} images for visual object tracking and perceive new challenges caused
by large distortion, stitching artifacts, and other unique attributes of
360{\deg} images. To alleviate these problems, we take advantage of novel
representations of target localization, i.e., bounding field-of-view, and then
introduce a general 360 tracking framework that can adopt typical trackers for
omnidirectional tracking. More importantly, we propose a new large-scale
omnidirectional tracking benchmark dataset, 360VOT, in order to facilitate
future research. 360VOT contains 120 sequences with up to 113K high-resolution
frames in equirectangular projection. The tracking targets cover 32 categories
in diverse scenarios. Moreover, we provide 4 types of unbiased ground truth,
including (rotated) bounding boxes and (rotated) bounding field-of-views, as
well as new metrics tailored for 360{\deg} images which allow for the accurate
evaluation of omnidirectional tracking performance. Finally, we extensively
evaluated 20 state-of-the-art visual trackers and provided a new baseline for
future comparisons. Homepage: https://360vot.hkustvgd.com
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Huajian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yinzhe Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yingshu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1&quot;&gt;Sai-Kit Yeung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14634">
<title>Fact-Checking of AI-Generated Reports. (arXiv:2307.14634v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.14634</link>
<description rdf:parseType="Literal">&lt;p&gt;With advances in generative artificial intelligence (AI), it is now possible
to produce realistic-looking automated reports for preliminary reads of
radiology images. This can expedite clinical workflows, improve accuracy and
reduce overall costs. However, it is also well-known that such models often
hallucinate, leading to false findings in the generated reports. In this paper,
we propose a new method of fact-checking of AI-generated reports using their
associated images. Specifically, the developed examiner differentiates real and
fake sentences in reports by learning the association between an image and
sentences describing real or potentially fake findings. To train such an
examiner, we first created a new dataset of fake reports by perturbing the
findings in the original ground truth radiology reports associated with images.
Text encodings of real and fake sentences drawn from these reports are then
paired with image encodings to learn the mapping to real/fake labels. The
utility of such an examiner is demonstrated for verifying automatically
generated reports by detecting and removing fake sentences. Future generative
AI approaches can use the resulting tool to validate their reports leading to a
more responsible use of AI in expediting clinical workflows.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmood_R/0/1/0/all/0/1&quot;&gt;Razi Mahmood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Ge Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalra_M/0/1/0/all/0/1&quot;&gt;Mannudeep Kalra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1&quot;&gt;Pingkun Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14637">
<title>HTNet for micro-expression recognition. (arXiv:2307.14637v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14637</link>
<description rdf:parseType="Literal">&lt;p&gt;Facial expression is related to facial muscle contractions and different
muscle movements correspond to different emotional states. For micro-expression
recognition, the muscle movements are usually subtle, which has a negative
impact on the performance of current facial emotion recognition algorithms.
Most existing methods use self-attention mechanisms to capture relationships
between tokens in a sequence, but they do not take into account the inherent
spatial relationships between facial landmarks. This can result in sub-optimal
performance on micro-expression recognition tasks.Therefore, learning to
recognize facial muscle movements is a key challenge in the area of
micro-expression recognition. In this paper, we propose a Hierarchical
Transformer Network (HTNet) to identify critical areas of facial muscle
movement. HTNet includes two major components: a transformer layer that
leverages the local temporal features and an aggregation layer that extracts
local and global semantical facial features. Specifically, HTNet divides the
face into four different facial areas: left lip area, left eye area, right eye
area and right lip area. The transformer layer is used to focus on representing
local minor muscle movement with local self-attention in each area. The
aggregation layer is used to learn the interactions between eye areas and lip
areas. The experiments on four publicly available micro-expression datasets
show that the proposed approach outperforms previous methods by a large margin.
The codes and models are available at:
\url{https://github.com/wangzhifengharrison/HTNet}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaihao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1&quot;&gt;Wenhan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankaranarayana_R/0/1/0/all/0/1&quot;&gt;Ramesh Sankaranarayana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14638">
<title>EqGAN: Feature Equalization Fusion for Few-shot Image Generation. (arXiv:2307.14638v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14638</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the absence of fine structure and texture information, existing
fusion-based few-shot image generation methods suffer from unsatisfactory
generation quality and diversity. To address this problem, we propose a novel
feature Equalization fusion Generative Adversarial Network (EqGAN) for few-shot
image generation. Unlike existing fusion strategies that rely on either deep
features or local representations, we design two separate branches to fuse
structures and textures by disentangling encoded features into shallow and deep
contents. To refine image contents at all feature levels, we equalize the fused
structure and texture semantics at different scales and supplement the decoder
with richer information by skip connections. Since the fused structures and
textures may be inconsistent with each other, we devise a consistent
equalization loss between the equalized features and the intermediate output of
the decoder to further align the semantics. Comprehensive experiments on three
public datasets demonstrate that, EqGAN not only significantly improves
generation performance with FID score (by up to 32.7%) and LPIPS score (by up
to 4.19%), but also outperforms the state-of-the-arts in terms of accuracy (by
up to 1.97%) for downstream classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yingbo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1&quot;&gt;Zhihao Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yutong Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pengyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xian Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingsong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14648">
<title>Spatial-Frequency U-Net for Denoising Diffusion Probabilistic Models. (arXiv:2307.14648v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14648</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the denoising diffusion probabilistic model (DDPM) in
wavelet space, instead of pixel space, for visual synthesis. Considering the
wavelet transform represents the image in spatial and frequency domains, we
carefully design a novel architecture SFUNet to effectively capture the
correlation for both domains. Specifically, in the standard denoising U-Net for
pixel data, we supplement the 2D convolutions and spatial-only attention layers
with our spatial frequency-aware convolution and attention modules to jointly
model the complementary information from spatial and frequency domains in
wavelet data. Our new architecture can be used as a drop-in replacement to the
pixel-based network and is compatible with the vanilla DDPM training process.
By explicitly modeling the wavelet signals, we find our model is able to
generate images with higher quality on CIFAR-10, FFHQ, LSUN-Bedroom, and
LSUN-Church datasets, than the pixel-based counterpart.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhengyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kevin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zicheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lijuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14659">
<title>LLDiffusion: Learning Degradation Representations in Diffusion Models for Low-Light Image Enhancement. (arXiv:2307.14659v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14659</link>
<description rdf:parseType="Literal">&lt;p&gt;Current deep learning methods for low-light image enhancement (LLIE)
typically rely on pixel-wise mapping learned from paired data. However, these
methods often overlook the importance of considering degradation
representations, which can lead to sub-optimal outcomes. In this paper, we
address this limitation by proposing a degradation-aware learning scheme for
LLIE using diffusion models, which effectively integrates degradation and image
priors into the diffusion process, resulting in improved image enhancement. Our
proposed degradation-aware learning scheme is based on the understanding that
degradation representations play a crucial role in accurately modeling and
capturing the specific degradation patterns present in low-light images. To
this end, First, a joint learning framework for both image generation and image
enhancement is presented to learn the degradation representations. Second, to
leverage the learned degradation representations, we develop a Low-Light
Diffusion model (LLDiffusion) with a well-designed dynamic diffusion module.
This module takes into account both the color map and the latent degradation
representations to guide the diffusion process. By incorporating these
conditioning factors, the proposed LLDiffusion can effectively enhance
low-light images, considering both the inherent degradation patterns and the
desired color fidelity. Finally, we evaluate our proposed method on several
well-known benchmark datasets, including synthetic and real-world unpaired
datasets. Extensive experiments on public benchmarks demonstrate that our
LLDiffusion outperforms state-of-the-art LLIE methods both quantitatively and
qualitatively. The source code and pre-trained models are available at
https://github.com/TaoWangzj/LLDiffusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaihao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1&quot;&gt;Ziqian Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1&quot;&gt;Wenhan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stenger_B/0/1/0/all/0/1&quot;&gt;Bjorn Stenger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Tae-Kyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongdong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14682">
<title>Unified Adversarial Patch for Visible-Infrared Cross-modal Attacks in the Physical World. (arXiv:2307.14682v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14682</link>
<description rdf:parseType="Literal">&lt;p&gt;Physical adversarial attacks have put a severe threat to DNN-based object
detectors. To enhance security, a combination of visible and infrared sensors
is deployed in various scenarios, which has proven effective in disabling
existing single-modal physical attacks. To further demonstrate the potential
risks in such cases, we design a unified adversarial patch that can perform
cross-modal physical attacks, achieving evasion in both modalities
simultaneously with a single patch. Given the different imaging mechanisms of
visible and infrared sensors, our work manipulates patches&apos; shape features,
which can be captured in different modalities when they undergo changes. To
deal with challenges, we propose a novel boundary-limited shape optimization
approach that aims to achieve compact and smooth shapes for the adversarial
patch, making it easy to implement in the physical world. And a score-aware
iterative evaluation method is also introduced to balance the fooling degree
between visible and infrared detectors during optimization, which guides the
adversarial patch to iteratively reduce the predicted scores of the multi-modal
sensors. Furthermore, we propose an Affine-Transformation-based enhancement
strategy that makes the learnable shape robust to various angles, thus
mitigating the issue of shape deformation caused by different shooting angles
in the real world. Our method is evaluated against several state-of-the-art
object detectors, achieving an Attack Success Rate (ASR) of over 80%. We also
demonstrate the effectiveness of our approach in physical-world scenarios under
various settings, including different angles, distances, postures, and scenes
for both visible and infrared sensors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xingxing Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yitong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jie Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14701">
<title>MIM-OOD: Generative Masked Image Modelling for Out-of-Distribution Detection in Medical Images. (arXiv:2307.14701v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14701</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised Out-of-Distribution (OOD) detection consists in identifying
anomalous regions in images leveraging only models trained on images of healthy
anatomy. An established approach is to tokenize images and model the
distribution of tokens with Auto-Regressive (AR) models. AR models are used to
1) identify anomalous tokens and 2) in-paint anomalous representations with
in-distribution tokens. However, AR models are slow at inference time and prone
to error accumulation issues which negatively affect OOD detection performance.
Our novel method, MIM-OOD, overcomes both speed and error accumulation issues
by replacing the AR model with two task-specific networks: 1) a transformer
optimized to identify anomalous tokens and 2) a transformer optimized to
in-paint anomalous tokens using masked image modelling (MIM). Our experiments
with brain MRI anomalies show that MIM-OOD substantially outperforms AR models
(DICE 0.458 vs 0.301) while achieving a nearly 25x speedup (9.5s vs 244s).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marimont%7D_S/0/1/0/all/0/1&quot;&gt;Sergio {Naval Marimont}&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siomos_V/0/1/0/all/0/1&quot;&gt;Vasilis Siomos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarroni_G/0/1/0/all/0/1&quot;&gt;Giacomo Tarroni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14705">
<title>High Dynamic Range Imaging via Visual Attention Modules. (arXiv:2307.14705v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14705</link>
<description rdf:parseType="Literal">&lt;p&gt;Thanks to High Dynamic Range (HDR) imaging methods, the scope of photography
has seen profound changes recently. To be more specific, such methods try to
reconstruct the lost luminosity of the real world caused by the limitation of
regular cameras from the Low Dynamic Range (LDR) images. Additionally, although
the State-Of-The-Art methods in this topic perform well, they mainly
concentrate on combining different exposures and have less attention to
extracting the informative parts of the images. Thus, this paper aims to
introduce a new model capable of incorporating information from the most
visible areas of each image extracted by a visual attention module (VAM), which
is a result of a segmentation strategy. In particular, the model, based on a
deep learning architecture, utilizes the extracted areas to produce the final
HDR image. The results demonstrate that our method outperformed most of the
State-Of-The-Art algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omrani_A/0/1/0/all/0/1&quot;&gt;Ali Reza Omrani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moroni_D/0/1/0/all/0/1&quot;&gt;Davide Moroni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14709">
<title>Taxonomy Adaptive Cross-Domain Adaptation in Medical Imaging via Optimization Trajectory Distillation. (arXiv:2307.14709v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14709</link>
<description rdf:parseType="Literal">&lt;p&gt;The success of automated medical image analysis depends on large-scale and
expert-annotated training sets. Unsupervised domain adaptation (UDA) has been
raised as a promising approach to alleviate the burden of labeled data
collection. However, they generally operate under the closed-set adaptation
setting assuming an identical label set between the source and target domains,
which is over-restrictive in clinical practice where new classes commonly exist
across datasets due to taxonomic inconsistency. While several methods have been
presented to tackle both domain shifts and incoherent label sets, none of them
take into account the common characteristics of the two issues and consider the
learning dynamics along network training. In this work, we propose optimization
trajectory distillation, a unified approach to address the two technical
challenges from a new perspective. It exploits the low-rank nature of gradient
space and devises a dual-stream distillation algorithm to regularize the
learning dynamics of insufficiently annotated domain and classes with the
external guidance obtained from reliable sources. Our approach resolves the
issue of inadequate navigation along network optimization, which is the major
obstacle in the taxonomy adaptive cross-domain adaptation scenario. We evaluate
the proposed method extensively on several tasks towards various endpoints with
clinical and open-world significance. The results demonstrate its effectiveness
and improvements over previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jianan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dongnan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Hang Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Heng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Weidong Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14710">
<title>Pre-training Vision Transformers with Very Limited Synthesized Images. (arXiv:2307.14710v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14710</link>
<description rdf:parseType="Literal">&lt;p&gt;Formula-driven supervised learning (FDSL) is a pre-training method that
relies on synthetic images generated from mathematical formulae such as
fractals. Prior work on FDSL has shown that pre-training vision transformers on
such synthetic datasets can yield competitive accuracy on a wide range of
downstream tasks. These synthetic images are categorized according to the
parameters in the mathematical formula that generate them. In the present work,
we hypothesize that the process for generating different instances for the same
category in FDSL, can be viewed as a form of data augmentation. We validate
this hypothesis by replacing the instances with data augmentation, which means
we only need a single image per category. Our experiments shows that this
one-instance fractal database (OFDB) performs better than the original dataset
where instances were explicitly generated. We further scale up OFDB to 21,000
categories and show that it matches, or even surpasses, the model pre-trained
on ImageNet-21k in ImageNet-1k fine-tuning. The number of images in OFDB is
21k, whereas ImageNet-21k has 14M. This opens new possibilities for
pre-training vision transformers with much smaller datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakamura1_R/0/1/0/all/0/1&quot;&gt;Ryo Nakamura1&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kataoka_H/0/1/0/all/0/1&quot;&gt;Hirokatsu Kataoka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takashima_S/0/1/0/all/0/1&quot;&gt;Sora Takashima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noriega_E/0/1/0/all/0/1&quot;&gt;Edgar Josafat Martinez Noriega&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yokota_R/0/1/0/all/0/1&quot;&gt;Rio Yokota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inoue_N/0/1/0/all/0/1&quot;&gt;Nakamasa Inoue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14713">
<title>GaitMorph: Transforming Gait by Optimally Transporting Discrete Codes. (arXiv:2307.14713v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14713</link>
<description rdf:parseType="Literal">&lt;p&gt;Gait, the manner of walking, has been proven to be a reliable biometric with
uses in surveillance, marketing and security. A promising new direction for the
field is training gait recognition systems without explicit human annotations,
through self-supervised learning approaches. Such methods are heavily reliant
on strong augmentations for the same walking sequence to induce more data
variability and to simulate additional walking variations. Current data
augmentation schemes are heuristic and cannot provide the necessary data
variation as they are only able to provide simple temporal and spatial
distortions. In this work, we propose GaitMorph, a novel method to modify the
walking variation for an input gait sequence. Our method entails the training
of a high-compression model for gait skeleton sequences that leverages
unlabelled data to construct a discrete and interpretable latent space, which
preserves identity-related features. Furthermore, we propose a method based on
optimal transport theory to learn latent transport maps on the discrete
codebook that morph gait sequences between variations. We perform extensive
experiments and show that our method is suitable to synthesize additional views
for an input sequence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1&quot;&gt;Adrian Cosma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radoi_E/0/1/0/all/0/1&quot;&gt;Emilian Radoi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14723">
<title>EFLNet: Enhancing Feature Learning for Infrared Small Target Detection. (arXiv:2307.14723v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14723</link>
<description rdf:parseType="Literal">&lt;p&gt;Single-frame infrared small target detection is considered to be a
challenging task, due to the extreme imbalance between target and background,
bounding box regression is extremely sensitive to infrared small targets, and
small target information is easy to lose in the high-level semantic layer. In
this paper, we propose an enhancing feature learning network (EFLNet) based on
YOLOv7 framework to solve these problems. First, we notice that there is an
extremely imbalance between the target and the background in the infrared
image, which makes the model pay more attention to the background features,
resulting in missed detection. To address this problem, we propose a new
adaptive threshold focal loss function that adjusts the loss weight
automatically, compelling the model to allocate greater attention to target
features. Second, we introduce the normalized Gaussian Wasserstein distance to
alleviate the difficulty of model convergence caused by the extreme sensitivity
of the bounding box regression to infrared small targets. Finally, we
incorporate a dynamic head mechanism into the network to enable adaptive
learning of the relative importance of each semantic layer. Experimental
results demonstrate our method can achieve better performance in the detection
performance of infrared small targets compared to state-of-the-art
deep-learning based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jiahao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_D/0/1/0/all/0/1&quot;&gt;Dongjian Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jun Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingliang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pi_Y/0/1/0/all/0/1&quot;&gt;Yangjun Pi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14725">
<title>vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-level Representations in Medical Images. (arXiv:2307.14725v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14725</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces vox2vec - a contrastive method for self-supervised
learning (SSL) of voxel-level representations. vox2vec representations are
modeled by a Feature Pyramid Network (FPN): a voxel representation is a
concatenation of the corresponding feature vectors from different pyramid
levels. The FPN is pre-trained to produce similar representations for the same
voxel in different augmented contexts and distinctive representations for
different voxels. This results in unified multi-scale representations that
capture both global semantics (e.g., body part) and local semantics (e.g.,
different small organs or healthy versus tumor tissue). We use vox2vec to
pre-train a FPN on more than 6500 publicly available computed tomography
images. We evaluate the pre-trained representations by attaching simple heads
on top of them and training the resulting models for 22 segmentation tasks. We
show that vox2vec outperforms existing medical imaging SSL techniques in three
evaluation setups: linear and non-linear probing and end-to-end fine-tuning.
Moreover, a non-linear head trained on top of the frozen vox2vec
representations achieves competitive performance with the FPN trained from
scratch while having 50 times fewer trainable parameters. The code is available
at https://github.com/mishgon/vox2vec .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goncharov_M/0/1/0/all/0/1&quot;&gt;Mikhail Goncharov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soboleva_V/0/1/0/all/0/1&quot;&gt;Vera Soboleva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurmukov_A/0/1/0/all/0/1&quot;&gt;Anvar Kurmukov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pisov_M/0/1/0/all/0/1&quot;&gt;Maxim Pisov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belyaev_M/0/1/0/all/0/1&quot;&gt;Mikhail Belyaev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14726">
<title>P2C: Self-Supervised Point Cloud Completion from Single Partial Clouds. (arXiv:2307.14726v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14726</link>
<description rdf:parseType="Literal">&lt;p&gt;Point cloud completion aims to recover the complete shape based on a partial
observation. Existing methods require either complete point clouds or multiple
partial observations of the same object for learning. In contrast to previous
approaches, we present Partial2Complete (P2C), the first self-supervised
framework that completes point cloud objects using training samples consisting
of only a single incomplete point cloud per object. Specifically, our framework
groups incomplete point clouds into local patches as input and predicts masked
patches by learning prior information from different partial objects. We also
propose Region-Aware Chamfer Distance to regularize shape mismatch without
limiting completion capability, and devise the Normal Consistency Constraint to
incorporate a local planarity assumption, encouraging the recovered shape
surface to be continuous and complete. In this way, P2C no longer needs
multiple observations or complete point clouds as ground truth. Instead,
structural cues are learned from a category-specific dataset to complete
partial point clouds of objects. We demonstrate the effectiveness of our
approach on both synthetic ShapeNet data and real-world ScanNet data, showing
that P2C produces comparable results to methods trained with complete shapes,
and outperforms methods learned with multiple partial observations. Code is
available at https://github.com/CuiRuikai/Partial2Complete.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1&quot;&gt;Ruikai Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1&quot;&gt;Shi Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1&quot;&gt;Saeed Anwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiawei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1&quot;&gt;Chaoyue Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1&quot;&gt;Nick Barnes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14729">
<title>Understanding Silent Failures in Medical Image Classification. (arXiv:2307.14729v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.14729</link>
<description rdf:parseType="Literal">&lt;p&gt;To ensure the reliable use of classification systems in medical applications,
it is crucial to prevent silent failures. This can be achieved by either
designing classifiers that are robust enough to avoid failures in the first
place, or by detecting remaining failures using confidence scoring functions
(CSFs). A predominant source of failures in image classification is
distribution shifts between training data and deployment data. To understand
the current state of silent failure prevention in medical imaging, we conduct
the first comprehensive analysis comparing various CSFs in four biomedical
tasks and a diverse range of distribution shifts. Based on the result that none
of the benchmarked CSFs can reliably prevent silent failures, we conclude that
a deeper understanding of the root causes of failures in the data is required.
To facilitate this, we introduce SF-Visuals, an interactive analysis tool that
uses latent space clustering to visualize shifts and failures. On the basis of
various examples, we demonstrate how this tool can help researchers gain
insight into the requirements for safe application of classification systems in
the medical domain. The open-source benchmark and tool are at:
https://github.com/IML-DKFZ/sf-visuals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bungert_T/0/1/0/all/0/1&quot;&gt;Till J. Bungert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kobelke_L/0/1/0/all/0/1&quot;&gt;Levin Kobelke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jaeger_P/0/1/0/all/0/1&quot;&gt;Paul F. Jaeger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14735">
<title>Test Time Adaptation for Blind Image Quality Assessment. (arXiv:2307.14735v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14735</link>
<description rdf:parseType="Literal">&lt;p&gt;While the design of blind image quality assessment (IQA) algorithms has
improved significantly, the distribution shift between the training and testing
scenarios often leads to a poor performance of these methods at inference time.
This motivates the study of test time adaptation (TTA) techniques to improve
their performance at inference time. Existing auxiliary tasks and loss
functions used for TTA may not be relevant for quality-aware adaptation of the
pre-trained model. In this work, we introduce two novel quality-relevant
auxiliary tasks at the batch and sample levels to enable TTA for blind IQA. In
particular, we introduce a group contrastive loss at the batch level and a
relative rank loss at the sample level to make the model quality aware and
adapt to the target data. Our experiments reveal that even using a small batch
of images from the test distribution helps achieve significant improvement in
performance by updating the batch normalization statistics of the source model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Subhadeep Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1&quot;&gt;Shankhanil Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1&quot;&gt;Soma Biswas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soundararajan_R/0/1/0/all/0/1&quot;&gt;Rajiv Soundararajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14748">
<title>Semantic Image Completion and Enhancement using GANs. (arXiv:2307.14748v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14748</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic inpainting or image completion alludes to the task of inferring
arbitrary large missing regions in images based on image semantics. Since the
prediction of image pixels requires an indication of high-level context, this
makes it significantly tougher than image completion, which is often more
concerned with correcting data corruption and removing entire objects from the
input image. On the other hand, image enhancement attempts to eliminate
unwanted noise and blur from the image, along with sustaining most of the image
details. Efficient image completion and enhancement model should be able to
recover the corrupted and masked regions in images and then refine the image
further to increase the quality of the output image. Generative Adversarial
Networks (GAN), have turned out to be helpful in picture completion tasks. In
this chapter, we will discuss the underlying GAN architecture and how they can
be used used for image completion tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxena_P/0/1/0/all/0/1&quot;&gt;Priyansh Saxena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1&quot;&gt;Raahat Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1&quot;&gt;Akshat Maheshwari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maheshwari_S/0/1/0/all/0/1&quot;&gt;Saumil Maheshwari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14750">
<title>Exploring Annotation-free Image Captioning with Retrieval-augmented Pseudo Sentence Generation. (arXiv:2307.14750v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14750</link>
<description rdf:parseType="Literal">&lt;p&gt;Training an image captioner without annotated image-sentence pairs has gained
traction in recent years. Previous approaches can be categorized into two
strategies: crawling sentences from mismatching corpora and aligning them with
the given images as pseudo annotations, or pre-training the captioner using
external image-text pairs. However, the aligning setting seems to reach its
performance limit due to the quality problem of pairs, and pre-training
requires significant computational resources. To address these challenges, we
propose a new strategy ``LPM + retrieval-augmented learning&quot; where the prior
knowledge from large pre-trained models (LPMs) is leveraged as supervision, and
a retrieval process is integrated to further reinforce its effectiveness.
Specifically, we introduce Retrieval-augmented Pseudo Sentence Generation
(RaPSG), which adopts an efficient approach to retrieve highly relevant short
region descriptions from the mismatching corpora and use them to generate a
variety of pseudo sentences with distinct representations as well as high
quality via LPMs. In addition, a fluency filter and a CLIP-guided training
objective are further introduced to facilitate model optimization. Experimental
results demonstrate that our method surpasses the SOTA pre-training model
(Flamingo3B) by achieving a CIDEr score of 78.1 (+5.1) while utilizing only
0.3% of its trainable parameters (1.3B VS 33M). Importantly, our approach
eliminates the need of computationally expensive pre-training processes on
external datasets (e.g., the requirement of 312M image-text pairs for
Flamingo3B). We further show that with a simple extension, the generated pseudo
sentences can be deployed as weak supervision to boost the 1% semi-supervised
image caption benchmark up to 93.4 CIDEr score (+8.9) which showcases the
versatility and effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dongnan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Heng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaoyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Weidong Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14768">
<title>Gloss-free Sign Language Translation: Improving from Visual-Language Pretraining. (arXiv:2307.14768v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14768</link>
<description rdf:parseType="Literal">&lt;p&gt;Sign Language Translation (SLT) is a challenging task due to its cross-domain
nature, involving the translation of visual-gestural language to text. Many
previous methods employ an intermediate representation, i.e., gloss sequences,
to facilitate SLT, thus transforming it into a two-stage task of sign language
recognition (SLR) followed by sign language translation (SLT). However, the
scarcity of gloss-annotated sign language data, combined with the information
bottleneck in the mid-level gloss representation, has hindered the further
development of the SLT task. To address this challenge, we propose a novel
Gloss-Free SLT based on Visual-Language Pretraining (GFSLT-VLP), which improves
SLT by inheriting language-oriented prior knowledge from pre-trained models,
without any gloss annotation assistance. Our approach involves two stages: (i)
integrating Contrastive Language-Image Pre-training (CLIP) with masked
self-supervised learning to create pre-tasks that bridge the semantic gap
between visual and textual representations and restore masked sentences, and
(ii) constructing an end-to-end architecture with an encoder-decoder-like
structure that inherits the parameters of the pre-trained Visual Encoder and
Text Decoder from the first stage. The seamless combination of these novel
designs forms a robust sign language representation and significantly improves
gloss-free sign language translation. In particular, we have achieved
unprecedented improvements in terms of BLEU-4 score on the PHOENIX14T dataset
(&amp;gt;+5) and the CSL-Daily dataset (&amp;gt;+3) compared to state-of-the-art gloss-free
SLT methods. Furthermore, our approach also achieves competitive results on the
PHOENIX14T dataset when compared with most of the gloss-based methods. Our code
is available at https://github.com/zhoubenjia/GFSLT-VLP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Benjia Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhigang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clapes_A/0/1/0/all/0/1&quot;&gt;Albert Clap&amp;#xe9;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1&quot;&gt;Jun Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yanyan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1&quot;&gt;Sergio Escalera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1&quot;&gt;Zhen Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Du Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14770">
<title>Learning Full-Head 3D GANs from a Single-View Portrait Dataset. (arXiv:2307.14770v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14770</link>
<description rdf:parseType="Literal">&lt;p&gt;33D-aware face generators are commonly trained on 2D real-life face image
datasets. Nevertheless, existing facial recognition methods often struggle to
extract face data captured from various camera angles. Furthermore, in-the-wild
images with diverse body poses introduce a high-dimensional challenge for
3D-aware generators, making it difficult to utilize data that contains complete
neck and shoulder regions. Consequently, these face image datasets often
contain only near-frontal face data, which poses challenges for 3D-aware face
generators to construct \textit{full-head} 3D portraits. To this end, we first
create the dataset {$\it{360}^{\circ}$}-\textit{Portrait}-\textit{HQ}
(\textit{$\it{360}^{\circ}$PHQ}), which consists of high-quality single-view
real portraits annotated with a variety of camera parameters {(the yaw angles
span the entire $360^{\circ}$ range)} and body poses. We then propose
\textit{3DPortraitGAN}, the first 3D-aware full-head portrait generator that
learns a canonical 3D avatar distribution from the body-pose-various
\textit{$\it{360}^{\circ}$PHQ} dataset with body pose self-learning. Our model
can generate view-consistent portrait images from all camera angles
(${360}^{\circ}$) with a full-head 3D representation. We incorporate a
mesh-guided deformation field into volumetric rendering to produce deformed
results to generate portrait images that conform to the body pose distribution
of the dataset using our canonical generator. We integrate two pose predictors
into our framework to predict more accurate body poses to address the issue of
inaccurately estimated body poses in our dataset. Our experiments show that the
proposed framework can generate view-consistent, realistic portrait images with
complete geometry from all camera angles and accurately predict portrait body
pose.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yiqian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xiangjun Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Hongbo Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xiaogang Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14777">
<title>pCTFusion: Point Convolution-Transformer Fusion with Semantic Aware Loss for Outdoor LiDAR Point Cloud Segmentation. (arXiv:2307.14777v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14777</link>
<description rdf:parseType="Literal">&lt;p&gt;LiDAR-generated point clouds are crucial for perceiving outdoor environments.
The segmentation of point clouds is also essential for many applications.
Previous research has focused on using self-attention and convolution (local
attention) mechanisms individually in semantic segmentation architectures.
However, there is limited work on combining the learned representations of
these attention mechanisms to improve performance. Additionally, existing
research that combines convolution with self-attention relies on global
attention, which is not practical for processing large point clouds. To address
these challenges, this study proposes a new architecture, pCTFusion, which
combines kernel-based convolutions and self-attention mechanisms for better
feature learning and capturing local and global dependencies in segmentation.
The proposed architecture employs two types of self-attention mechanisms, local
and global, based on the hierarchical positions of the encoder blocks.
Furthermore, the existing loss functions do not consider the semantic and
position-wise importance of the points, resulting in reduced accuracy,
particularly at sharp class boundaries. To overcome this, the study models a
novel attention-based loss function called Pointwise Geometric Anisotropy
(PGA), which assigns weights based on the semantic distribution of points in a
neighborhood. The proposed architecture is evaluated on SemanticKITTI outdoor
dataset and showed a 5-7% improvement in performance compared to the
state-of-the-art architectures. The results are particularly encouraging for
minor classes, often misclassified due to class imbalance, lack of space, and
neighbor-aware feature encoding. These developed methods can be leveraged for
the segmentation of complex datasets and can drive real-world applications of
LiDAR point cloud.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuriyal_A/0/1/0/all/0/1&quot;&gt;Abhishek Kuriyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Vaibhav Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lohani_B/0/1/0/all/0/1&quot;&gt;Bharat Lohani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14781">
<title>Contrastive Knowledge Amalgamation for Unsupervised Image Classification. (arXiv:2307.14781v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14781</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge amalgamation (KA) aims to learn a compact student model to handle
the joint objective from multiple teacher models that are are specialized for
their own tasks respectively. Current methods focus on coarsely aligning
teachers and students in the common representation space, making it difficult
for the student to learn the proper decision boundaries from a set of
heterogeneous teachers. Besides, the KL divergence in previous works only
minimizes the probability distribution difference between teachers and the
student, ignoring the intrinsic characteristics of teachers. Therefore, we
propose a novel Contrastive Knowledge Amalgamation (CKA) framework, which
introduces contrastive losses and an alignment loss to achieve intra-class
cohesion and inter-class separation.Contrastive losses intra- and inter- models
are designed to widen the distance between representations of different
classes. The alignment loss is introduced to minimize the sample-level
distribution differences of teacher-student models in the common representation
space.Furthermore, the student learns heterogeneous unsupervised classification
tasks through soft targets efficiently and flexibly in the task-level
amalgamation. Extensive experiments on benchmarks demonstrate the
generalization capability of CKA in the amalgamation of specific task as well
as multiple tasks. Comprehensive ablation studies provide a further insight
into our CKA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shangde Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yichao Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Ke Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yuqiang Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14786">
<title>Towards Deeply Unified Depth-aware Panoptic Segmentation with Bi-directional Guidance Learning. (arXiv:2307.14786v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14786</link>
<description rdf:parseType="Literal">&lt;p&gt;Depth-aware panoptic segmentation is an emerging topic in computer vision
which combines semantic and geometric understanding for more robust scene
interpretation. Recent works pursue unified frameworks to tackle this challenge
but mostly still treat it as two individual learning tasks, which limits their
potential for exploring cross-domain information. We propose a deeply unified
framework for depth-aware panoptic segmentation, which performs joint
segmentation and depth estimation both in a per-segment manner with identical
object queries. To narrow the gap between the two tasks, we further design a
geometric query enhancement method, which is able to integrate scene geometry
into object queries using latent representations. In addition, we propose a
bi-directional guidance learning approach to facilitate cross-task feature
learning by taking advantage of their mutual relations. Our method sets the new
state of the art for depth-aware panoptic segmentation on both Cityscapes-DVPS
and SemKITTI-DVPS datasets. Moreover, our guidance learning approach is shown
to deliver performance improvement even under incomplete supervision labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junwen He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lijun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Huchuan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jun-Yan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_J/0/1/0/all/0/1&quot;&gt;Jin-Peng Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Bin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1&quot;&gt;Yifeng Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xuansong Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14823">
<title>Fading memory as inductive bias in residual recurrent networks. (arXiv:2307.14823v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.14823</link>
<description rdf:parseType="Literal">&lt;p&gt;Residual connections have been proposed as architecture-based inductive bias
to mitigate the problem of exploding and vanishing gradients and increase task
performance in both feed-forward and recurrent networks (RNNs) when trained
with the backpropagation algorithm. Yet, little is known about how residual
connections in RNNs influence their dynamics and fading memory properties.
Here, we introduce weakly coupled residual recurrent networks (WCRNNs) in which
residual connections result in well-defined Lyapunov exponents and allow for
studying properties of fading memory. We investigate how the residual
connections of WCRNNs influence their performance, network dynamics, and memory
properties on a set of benchmark tasks. We show that several distinct forms of
residual connections yield effective inductive biases that result in increased
network expressivity. In particular, residual connections that (i) result in
network dynamics at the proximity of the edge of chaos, (ii) allow networks to
capitalize on characteristic spectral properties of the data, and (iii) result
in heterogeneous memory properties are shown to increase practical
expressivity. In addition, we demonstrate how our results can be extended to
non-linear residuals and introduce a weakly coupled residual initialization
scheme that can be used for Elman RNNs
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubinin_I/0/1/0/all/0/1&quot;&gt;Igor Dubinin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Effenberger_F/0/1/0/all/0/1&quot;&gt;Felix Effenberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14825">
<title>Simplified Concrete Dropout -- Improving the Generation of Attribution Masks for Fine-grained Classification. (arXiv:2307.14825v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14825</link>
<description rdf:parseType="Literal">&lt;p&gt;Fine-grained classification is a particular case of a classification problem,
aiming to classify objects that share the visual appearance and can only be
distinguished by subtle differences. Fine-grained classification models are
often deployed to determine animal species or individuals in automated animal
monitoring systems. Precise visual explanations of the model&apos;s decision are
crucial to analyze systematic errors. Attention- or gradient-based methods are
commonly used to identify regions in the image that contribute the most to the
classification decision. These methods deliver either too coarse or too noisy
explanations, unsuitable for identifying subtle visual differences reliably.
However, perturbation-based methods can precisely identify pixels causally
responsible for the classification result. Fill-in of the dropout (FIDO)
algorithm is one of those methods. It utilizes the concrete dropout (CD) to
sample a set of attribution masks and updates the sampling parameters based on
the output of the classification model. A known problem of the algorithm is a
high variance in the gradient estimates, which the authors have mitigated until
now by mini-batch updates of the sampling parameters. This paper presents a
solution to circumvent these computational instabilities by simplifying the CD
sampling and reducing reliance on large mini-batch sizes. First, it allows
estimating the parameters with smaller mini-batch sizes without losing the
quality of the estimates but with a reduced computational effort. Furthermore,
our solution produces finer and more coherent attribution masks. Finally, we
use the resulting attribution masks to improve the classification performance
of a trained model without additional fine-tuning of the model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korsch_D/0/1/0/all/0/1&quot;&gt;Dimitri Korsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shadaydeh_M/0/1/0/all/0/1&quot;&gt;Maha Shadaydeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1&quot;&gt;Joachim Denzler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14859">
<title>Comparative Evaluation of Digital and Analog Chest Radiographs to Identify Tuberculosis using Deep Learning Model. (arXiv:2307.14859v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14859</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose:Chest X-ray (CXR) is an essential tool and one of the most prescribed
imaging to detect pulmonary abnormalities, with a yearly estimate of over 2
billion imaging performed worldwide. However, the accurate and timely diagnosis
of TB remains an unmet goal. The prevalence of TB is highest in
low-middle-income countries, and the requirement of a portable, automated, and
reliable solution is required. In this study, we compared the performance of
DL-based devices on digital and analog CXR. The evaluated DL-based device can
be used in resource-constraint settings. Methods: A total of 10,000 CXR
DICOMs(.dcm) and printed photos of the films acquired with three different
cellular phones - Samsung S8, iPhone 8, and iPhone XS along with their
radiological report were retrospectively collected from various sites across
India from April 2020 to March 2021. Results: 10,000 chest X-rays were utilized
to evaluate the DL-based device in identifying radiological signs of TB. The
AUC of qXR for detecting signs of tuberculosis on the original DICOMs dataset
was 0.928 with a sensitivity of 0.841 at a specificity of 0.806. At an optimal
threshold, the difference in the AUC of three cellular smartphones with the
original DICOMs is 0.024 (2.55%), 0.048 (5.10%), and 0.038 (1.91%). The minimum
difference demonstrates the robustness of the DL-based device in identifying
radiological signs of TB in both digital and analog CXR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chattoraj_S/0/1/0/all/0/1&quot;&gt;Subhankar Chattoraj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_B/0/1/0/all/0/1&quot;&gt;Bhargava Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tadepalli_M/0/1/0/all/0/1&quot;&gt;Manoj Tadepalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Putha_P/0/1/0/all/0/1&quot;&gt;Preetham Putha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14863">
<title>IML-ViT: Image Manipulation Localization by Vision Transformer. (arXiv:2307.14863v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14863</link>
<description rdf:parseType="Literal">&lt;p&gt;Advanced image tampering techniques are increasingly challenging the
trustworthiness of multimedia, leading to the development of Image Manipulation
Localization (IML). But what makes a good IML model? The answer lies in the way
to capture artifacts. Exploiting artifacts requires the model to extract
non-semantic discrepancies between the manipulated and authentic regions, which
needs to compare differences between these two areas explicitly. With the
self-attention mechanism, naturally, the Transformer is the best candidate.
Besides, artifacts are sensitive to image resolution, amplified under
multi-scale features, and massive at the manipulation border. Therefore, we
formulate the answer to the former question as building a ViT with
high-resolution capacity, multi-scale feature extraction capability, and
manipulation edge supervision. We term this simple but effective ViT paradigm
as the IML-ViT, which has great potential to become a new benchmark for IML.
Extensive experiments on five benchmark datasets verified our model outperforms
the state-of-the-art manipulation localization methods. Code and models are
available at \url{https://github.com/SunnyHaze/IML-ViT}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaochen Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bo Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianggen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hammadi_A/0/1/0/all/0/1&quot;&gt;Ahmed Y. Al Hammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jizhe Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14864">
<title>A full-resolution training framework for Sentinel-2 image fusion. (arXiv:2307.14864v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.14864</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents a new unsupervised framework for training deep learning
models for super-resolution of Sentinel-2 images by fusion of its 10-m and 20-m
bands. The proposed scheme avoids the resolution downgrade process needed to
generate training data in the supervised case. On the other hand, a proper loss
that accounts for cycle-consistency between the network prediction and the
input components to be fused is proposed. Despite its unsupervised nature, in
our preliminary experiments the proposed scheme has shown promising results in
comparison to the supervised approach. Besides, by construction of the proposed
loss, the resulting trained network can be ascribed to the class of
multi-resolution analysis methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ciotola_M/0/1/0/all/0/1&quot;&gt;Matteo Ciotola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ragosta_M/0/1/0/all/0/1&quot;&gt;Mario Ragosta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Poggi_G/0/1/0/all/0/1&quot;&gt;Giovanni Poggi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Scarpa_G/0/1/0/all/0/1&quot;&gt;Giuseppe Scarpa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14866">
<title>Sample Less, Learn More: Efficient Action Recognition via Frame Feature Restoration. (arXiv:2307.14866v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14866</link>
<description rdf:parseType="Literal">&lt;p&gt;Training an effective video action recognition model poses significant
computational challenges, particularly under limited resource budgets. Current
methods primarily aim to either reduce model size or utilize pre-trained
models, limiting their adaptability to various backbone architectures. This
paper investigates the issue of over-sampled frames, a prevalent problem in
many approaches yet it has received relatively little attention. Despite the
use of fewer frames being a potential solution, this approach often results in
a substantial decline in performance. To address this issue, we propose a novel
method to restore the intermediate features for two sparsely sampled and
adjacent video frames. This feature restoration technique brings a negligible
increase in computational requirements compared to resource-intensive image
encoders, such as ViT. To evaluate the effectiveness of our method, we conduct
extensive experiments on four public datasets, including Kinetics-400,
ActivityNet, UCF-101, and HMDB-51. With the integration of our method, the
efficiency of three commonly used baselines has been improved by over 50%, with
a mere 0.5% reduction in recognition accuracy. In addition, our method also
surprisingly helps improve the generalization ability of the models under
zero-shot settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Harry Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yangyang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1&quot;&gt;Liqiang Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1&quot;&gt;Mohan Kankanhalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14889">
<title>Weakly Supervised Multi-Modal 3D Human Body Pose Estimation for Autonomous Driving. (arXiv:2307.14889v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14889</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate 3D human pose estimation (3D HPE) is crucial for enabling autonomous
vehicles (AVs) to make informed decisions and respond proactively in critical
road scenarios. Promising results of 3D HPE have been gained in several domains
such as human-computer interaction, robotics, sports and medical analytics,
often based on data collected in well-controlled laboratory environments.
Nevertheless, the transfer of 3D HPE methods to AVs has received limited
research attention, due to the challenges posed by obtaining accurate 3D pose
annotations and the limited suitability of data from other domains.
&lt;/p&gt;
&lt;p&gt;We present a simple yet efficient weakly supervised approach for 3D HPE in
the AV context by employing a high-level sensor fusion between camera and LiDAR
data. The weakly supervised setting enables training on the target datasets
without any 2D/3D keypoint labels by using an off-the-shelf 2D joint extractor
and pseudo labels generated from LiDAR to image projections. Our approach
outperforms state-of-the-art results by up to $\sim$ 13% on the Waymo Open
Dataset in the weakly supervised setting and achieves state-of-the-art results
in the supervised setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauer_P/0/1/0/all/0/1&quot;&gt;Peter Bauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouazizi_A/0/1/0/all/0/1&quot;&gt;Arij Bouazizi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kressel_U/0/1/0/all/0/1&quot;&gt;Ulrich Kressel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flohr_F/0/1/0/all/0/1&quot;&gt;Fabian B. Flohr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14897">
<title>Mixture of Self-Supervised Learning. (arXiv:2307.14897v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14897</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning is popular method because of its ability to learn
features in images without using its labels and is able to overcome limited
labeled datasets used in supervised learning. Self-supervised learning works by
using a pretext task which will be trained on the model before being applied to
a specific task. There are some examples of pretext tasks used in
self-supervised learning in the field of image recognition, namely rotation
prediction, solving jigsaw puzzles, and predicting relative positions on image.
Previous studies have only used one type of transformation as a pretext task.
This raises the question of how it affects if more than one pretext task is
used and to use a gating network to combine all pretext tasks. Therefore, we
propose the Gated Self-Supervised Learning method to improve image
classification which use more than one transformation as pretext task and uses
the Mixture of Expert architecture as a gating network in combining each
pretext task so that the model automatically can study and focus more on the
most useful augmentations for classification. We test performance of the
proposed method in several scenarios, namely CIFAR imbalance dataset
classification, adversarial perturbations, Tiny-Imagenet dataset
classification, and semi-supervised learning. Moreover, there are Grad-CAM and
T-SNE analysis that are used to see the proposed method for identifying
important features that influence image classification and representing data
for each class and separating different classes properly. Our code is in
https://github.com/aristorenaldo/G-SSL
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruslim_A/0/1/0/all/0/1&quot;&gt;Aristo Renaldo Ruslim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1&quot;&gt;Novanto Yudistira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Setiawan_B/0/1/0/all/0/1&quot;&gt;Budi Darma Setiawan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14901">
<title>Text-guided Foundation Model Adaptation for Pathological Image Classification. (arXiv:2307.14901v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14901</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent surge of foundation models in computer vision and natural language
processing opens up perspectives in utilizing multi-modal clinical data to
train large models with strong generalizability. Yet pathological image
datasets often lack biomedical text annotation and enrichment. Guiding
data-efficient image diagnosis from the use of biomedical text knowledge
becomes a substantial interest. In this paper, we propose to Connect Image and
Text Embeddings (CITE) to enhance pathological image classification. CITE
injects text insights gained from language models pre-trained with a broad
range of biomedical texts, leading to adapt foundation models towards
pathological image understanding. Through extensive experiments on the
PatchGastric stomach tumor pathological image dataset, we demonstrate that CITE
achieves leading performance compared with various baselines especially when
training data is scarce. CITE offers insights into leveraging in-domain text
knowledge to reinforce data-efficient pathological image classification. Code
is available at https://github.com/Yunkun-Zhang/CITE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunkun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaosong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaoting Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dequan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14907">
<title>Weakly Supervised AI for Efficient Analysis of 3D Pathology Samples. (arXiv:2307.14907v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.14907</link>
<description rdf:parseType="Literal">&lt;p&gt;Human tissue and its constituent cells form a microenvironment that is
fundamentally three-dimensional (3D). However, the standard-of-care in
pathologic diagnosis involves selecting a few two-dimensional (2D) sections for
microscopic evaluation, risking sampling bias and misdiagnosis. Diverse methods
for capturing 3D tissue morphologies have been developed, but they have yet had
little translation to clinical practice; manual and computational evaluations
of such large 3D data have so far been impractical and/or unable to provide
patient-level clinical insights. Here we present Modality-Agnostic Multiple
instance learning for volumetric Block Analysis (MAMBA), a deep-learning-based
platform for processing 3D tissue images from diverse imaging modalities and
predicting patient outcomes. Archived prostate cancer specimens were imaged
with open-top light-sheet microscopy or microcomputed tomography and the
resulting 3D datasets were used to train risk-stratification networks based on
5-year biochemical recurrence outcomes via MAMBA. With the 3D block-based
approach, MAMBA achieves an area under the receiver operating characteristic
curve (AUC) of 0.86 and 0.74, superior to 2D traditional single-slice-based
prognostication (AUC of 0.79 and 0.57), suggesting superior prognostication
with 3D morphological features. Further analyses reveal that the incorporation
of greater tissue volume improves prognostic performance and mitigates risk
prediction variability from sampling bias, suggesting the value of capturing
larger extents of heterogeneous 3D morphology. With the rapid growth and
adoption of 3D spatial biology and pathology techniques by researchers and
clinicians, MAMBA provides a general and efficient framework for 3D weakly
supervised learning for clinical decision support and can help to reveal novel
3D morphological biomarkers for prognosis and therapeutic response.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_A/0/1/0/all/0/1&quot;&gt;Andrew H. Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Williams_M/0/1/0/all/0/1&quot;&gt;Mane Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Williamson_D/0/1/0/all/0/1&quot;&gt;Drew F.K. Williamson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jaume_G/0/1/0/all/0/1&quot;&gt;Guillaume Jaume&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Andrew Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bowen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Serafin_R/0/1/0/all/0/1&quot;&gt;Robert Serafin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jonathan T.C. Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baras_A/0/1/0/all/0/1&quot;&gt;Alex Baras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Parwani_A/0/1/0/all/0/1&quot;&gt;Anil V. Parwani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mahmood_F/0/1/0/all/0/1&quot;&gt;Faisal Mahmood&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14917">
<title>NSA: Naturalistic Support Artifact to Boost Network Confidence. (arXiv:2307.14917v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14917</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual AI systems are vulnerable to natural and synthetic physical corruption
in the real-world. Such corruption often arises unexpectedly and alters the
model&apos;s performance. In recent years, the primary focus has been on adversarial
attacks. However, natural corruptions (e.g., snow, fog, dust) are an
omnipresent threat to visual AI systems and should be considered equally
important. Many existing works propose interesting solutions to train robust
models against natural corruption. These works either leverage image
augmentations, which come with the additional cost of model training, or place
suspicious patches in the scene to design unadversarial examples. In this work,
we propose the idea of naturalistic support artifacts (NSA) for robust
prediction. The NSAs are shown to be beneficial in scenarios where model
parameters are inaccessible and adding artifacts in the scene is feasible. The
NSAs are natural looking objects generated through artifact training using
DC-GAN to have high visual fidelity in the scene. We test against natural
corruptions on the Imagenette dataset and observe the improvement in prediction
confidence score by four times. We also demonstrate NSA&apos;s capability to
increase adversarial accuracy by 8\% on average. Lastly, we qualitatively
analyze NSAs using saliency maps to understand how they help improve prediction
confidence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Abhijith Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munz_P/0/1/0/all/0/1&quot;&gt;Phil Munz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayan_A/0/1/0/all/0/1&quot;&gt;Apurva Narayan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14918">
<title>GET3D--: Learning GET3D from Unconstrained Image Collections. (arXiv:2307.14918v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14918</link>
<description rdf:parseType="Literal">&lt;p&gt;The demand for efficient 3D model generation techniques has grown
exponentially, as manual creation of 3D models is time-consuming and requires
specialized expertise. While generative models have shown potential in creating
3D textured shapes from 2D images, their applicability in 3D industries is
limited due to the lack of a well-defined camera distribution in real-world
scenarios, resulting in low-quality shapes. To overcome this limitation, we
propose GET3D--, the first method that directly generates textured 3D shapes
from 2D images with unknown pose and scale. GET3D-- comprises a 3D shape
generator and a learnable camera sampler that captures the 6D external changes
on the camera. In addition, We propose a novel training schedule to stably
optimize both the shape generator and camera sampler in a unified framework. By
controlling external variations using the learnable camera sampler, our method
can generate aligned shapes with clear textures. Extensive experiments
demonstrate the efficacy of GET3D--, which precisely fits the 6D camera pose
distribution and generates high-quality shapes on both synthetic and realistic
unconstrained datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fanghua Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zheyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yan-Pei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1&quot;&gt;Chao Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14959">
<title>Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification. (arXiv:2307.14959v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14959</link>
<description rdf:parseType="Literal">&lt;p&gt;In the medical field, federated learning commonly deals with highly
imbalanced datasets, including skin lesions and gastrointestinal images.
Existing federated methods under highly imbalanced datasets primarily focus on
optimizing a global model without incorporating the intra-class variations that
can arise in medical imaging due to different populations, findings, and
scanners. In this paper, we study the inter-client intra-class variations with
publicly available self-supervised auxiliary networks. Specifically, we find
that employing a shared auxiliary pre-trained model, like MoCo-V2, locally on
every client yields consistent divergence measurements. Based on these
findings, we derive a dynamic balanced model aggregation via self-supervised
priors (MAS) to guide the global model optimization. Fed-MAS can be utilized
with different local learning methods for effective model aggregation toward a
highly robust and unbiased global model. Our code is available at
\url{https://github.com/xmed-lab/Fed-MAS}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elbatel_M/0/1/0/all/0/1&quot;&gt;Marawan Elbatel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hualiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marti_R/0/1/0/all/0/1&quot;&gt;Robert Mart&amp;#xed;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Huazhu Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14971">
<title>Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models. (arXiv:2307.14971v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14971</link>
<description rdf:parseType="Literal">&lt;p&gt;With the overwhelming trend of mask image modeling led by MAE, generative
pre-training has shown a remarkable potential to boost the performance of
fundamental models in 2D vision. However, in 3D vision, the over-reliance on
Transformer-based backbones and the unordered nature of point clouds have
restricted the further development of generative pre-training. In this paper,
we propose a novel 3D-to-2D generative pre-training method that is adaptable to
any point cloud model. We propose to generate view images from different
instructed poses via the cross-attention mechanism as the pre-training scheme.
Generating view images has more precise supervision than its point cloud
counterpart, thus assisting 3D backbones to have a finer comprehension of the
geometrical structure and stereoscopic relations of the point cloud.
Experimental results have proved the superiority of our proposed 3D-to-2D
generative pre-training over previous pre-training methods. Our method is also
effective in boosting the performance of architecture-oriented approaches,
achieving state-of-the-art performance when fine-tuning on ScanObjectNN
classification and ShapeNetPart segmentation tasks. Code is available at
https://github.com/wangzy22/TAP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xumin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1&quot;&gt;Yongming Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14981">
<title>MapNeRF: Incorporating Map Priors into Neural Radiance Fields for Driving View Simulation. (arXiv:2307.14981v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14981</link>
<description rdf:parseType="Literal">&lt;p&gt;Simulating camera sensors is a crucial task in autonomous driving. Although
neural radiance fields are exceptional at synthesizing photorealistic views in
driving simulations, they still fail in generating extrapolated views. This
paper proposes to incorporate map priors into neural radiance fields to
synthesize out-of-trajectory driving views with semantic road consistency. The
key insight is that map information can be utilized as a prior to guide the
training of the radiance fields with uncertainty. Specifically, we utilize the
coarse ground surface as uncertain information to supervise the density field
and warp depth with uncertainty from unknown camera poses to ensure multi-view
consistency. Experimental results demonstrate that our approach can produce
semantic consistency in deviated views for vehicle camera simulation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chenming Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiadai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhelun Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liangjun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15007">
<title>Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability. (arXiv:2307.15007v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15007</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increased deployment of machine learning models in various
real-world applications, researchers and practitioners alike have emphasized
the need for explanations of model behaviour. To this end, two broad strategies
have been outlined in prior literature to explain models. Post hoc explanation
methods explain the behaviour of complex black-box models by highlighting
features that are critical to model predictions; however, prior work has shown
that these explanations may not be faithful, and even more concerning is our
inability to verify them. Specifically, it is nontrivial to evaluate if a given
attribution is correct with respect to the underlying model. Inherently
interpretable models, on the other hand, circumvent these issues by explicitly
encoding explanations into model architecture, meaning their explanations are
naturally faithful and verifiable, but they often exhibit poor predictive
performance due to their limited expressive power. In this work, we aim to
bridge the gap between the aforementioned strategies by proposing Verifiability
Tuning (VerT), a method that transforms black-box models into models that
naturally yield faithful and verifiable feature attributions. We begin by
introducing a formal theoretical framework to understand verifiability and show
that attributions produced by standard models cannot be verified. We then
leverage this framework to propose a method to build verifiable models and
feature attributions out of fully trained black-box models. Finally, we perform
extensive experiments on semi-synthetic and real-world datasets, and show that
VerT produces models that (1) yield explanations that are correct and
verifiable and (2) are faithful to the original black-box models they are meant
to explain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhalla_U/0/1/0/all/0/1&quot;&gt;Usha Bhalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivas_S/0/1/0/all/0/1&quot;&gt;Suraj Srinivas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1&quot;&gt;Himabindu Lakkaraju&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15016">
<title>How Good is Google Bard&apos;s Visual Understanding? An Empirical Study on Open Challenges. (arXiv:2307.15016v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15016</link>
<description rdf:parseType="Literal">&lt;p&gt;Google&apos;s Bard has emerged as a formidable competitor to OpenAI&apos;s ChatGPT in
the field of conversational AI. Notably, Bard has recently been updated to
handle visual inputs alongside text prompts during conversations. Given Bard&apos;s
impressive track record in handling textual inputs, we explore its capabilities
in understanding and interpreting visual data (images) conditioned by text
questions. This exploration holds the potential to unveil new insights and
challenges for Bard and other forthcoming multi-modal Generative models,
especially in addressing complex computer vision problems that demand accurate
visual and language understanding. Specifically, in this study, we focus on 15
diverse task scenarios encompassing regular, camouflaged, medical, under-water
and remote sensing data to comprehensively evaluate Bard&apos;s performance. Our
primary finding indicates that Bard still struggles in these vision scenarios,
highlighting the significant gap in vision-based understanding that needs to be
bridged in future developments. We expect that this empirical study will prove
valuable in advancing future models, leading to enhanced capabilities in
comprehending and interpreting fine-grained visual data. Our project is
released on https://github.com/htqin/GoogleBard-VisUnderstand
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1&quot;&gt;Haotong Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1&quot;&gt;Ge-Peng Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1&quot;&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15019">
<title>Self-Supervised Graph Transformer for Deepfake Detection. (arXiv:2307.15019v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15019</link>
<description rdf:parseType="Literal">&lt;p&gt;Deepfake detection methods have shown promising results in recognizing
forgeries within a given dataset, where training and testing take place on the
in-distribution dataset. However, their performance deteriorates significantly
when presented with unseen samples. As a result, a reliable deepfake detection
system must remain impartial to forgery types, appearance, and quality for
guaranteed generalizable detection performance. Despite various attempts to
enhance cross-dataset generalization, the problem remains challenging,
particularly when testing against common post-processing perturbations, such as
video compression or blur. Hence, this study introduces a deepfake detection
framework, leveraging a self-supervised pre-training model that delivers
exceptional generalization ability, withstanding common corruptions and
enabling feature explainability. The framework comprises three key components:
a feature extractor based on vision Transformer architecture that is
pre-trained via self-supervised contrastive learning methodology, a graph
convolution network coupled with a Transformer discriminator, and a graph
Transformer relevancy map that provides a better understanding of manipulated
regions and further explains the model&apos;s decision. To assess the effectiveness
of the proposed framework, several challenging experiments are conducted,
including in-data distribution performance, cross-dataset, cross-manipulation
generalization, and robustness against common post-production perturbations.
The results achieved demonstrate the remarkable effectiveness of the proposed
deepfake detection framework, surpassing the current state-of-the-art
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khormali_A/0/1/0/all/0/1&quot;&gt;Aminollah Khormali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jiann-Shiun Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15029">
<title>Adaptive Segmentation Network for Scene Text Detection. (arXiv:2307.15029v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15029</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by deep convolution segmentation algorithms, scene text detectors
break the performance ceiling of datasets steadily. However, these methods
often encounter threshold selection bottlenecks and have poor performance on
text instances with extreme aspect ratios. In this paper, we propose to
automatically learn the discriminate segmentation threshold, which
distinguishes text pixels from background pixels for segmentation-based scene
text detectors and then further reduces the time-consuming manual parameter
adjustment. Besides, we design a Global-information Enhanced Feature Pyramid
Network (GE-FPN) for capturing text instances with macro size and extreme
aspect ratios. Following the GE-FPN, we introduce a cascade optimization
structure to further refine the text instances. Finally, together with the
proposed threshold learning strategy and text detection structure, we design an
Adaptive Segmentation Network (ASNet) for scene text detection. Extensive
experiments are carried out to demonstrate that the proposed ASNet can achieve
the state-of-the-art performance on four text detection benchmarks, i.e., ICDAR
2015, MSRA-TD500, ICDAR 2017 MLT and CTW1500. The ablation experiments also
verify the effectiveness of our contributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1&quot;&gt;Guiqin Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15033">
<title>Diverse Inpainting and Editing with GAN Inversion. (arXiv:2307.15033v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15033</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent inversion methods have shown that real images can be inverted into
StyleGAN&apos;s latent space and numerous edits can be achieved on those images
thanks to the semantically rich feature representations of well-trained GAN
models. However, extensive research has also shown that image inversion is
challenging due to the trade-off between high-fidelity reconstruction and
editability. In this paper, we tackle an even more difficult task, inverting
erased images into GAN&apos;s latent space for realistic inpaintings and editings.
Furthermore, by augmenting inverted latent codes with different latent samples,
we achieve diverse inpaintings. Specifically, we propose to learn an encoder
and mixing network to combine encoded features from erased images with
StyleGAN&apos;s mapped features from random samples. To encourage the mixing network
to utilize both inputs, we train the networks with generated data via a novel
set-up. We also utilize higher-rate features to prevent color inconsistencies
between the inpainted and unerased parts. We run extensive experiments and
compare our method with state-of-the-art inversion and inpainting methods.
Qualitative metrics and visual comparisons show significant improvements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yildirim_A/0/1/0/all/0/1&quot;&gt;Ahmet Burak Yildirim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pehlivan_H/0/1/0/all/0/1&quot;&gt;Hamza Pehlivan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilecen_B/0/1/0/all/0/1&quot;&gt;Bahri Batuhan Bilecen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dundar_A/0/1/0/all/0/1&quot;&gt;Aysegul Dundar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15042">
<title>TEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis. (arXiv:2307.15042v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15042</link>
<description rdf:parseType="Literal">&lt;p&gt;The gradual nature of a diffusion process that synthesizes samples in small
increments constitutes a key ingredient of Denoising Diffusion Probabilistic
Models (DDPM), which have presented unprecedented quality in image synthesis
and been recently explored in the motion domain. In this work, we propose to
adapt the gradual diffusion concept (operating along a diffusion time-axis)
into the temporal-axis of the motion sequence. Our key idea is to extend the
DDPM framework to support temporally varying denoising, thereby entangling the
two axes. Using our special formulation, we iteratively denoise a motion buffer
that contains a set of increasingly-noised poses, which auto-regressively
produces an arbitrarily long stream of frames. With a stationary diffusion
time-axis, in each diffusion step we increment only the temporal-axis of the
motion such that the framework produces a new, clean frame which is removed
from the beginning of the buffer, followed by a newly drawn noise vector that
is appended to it. This new mechanism paves the way towards a new framework for
long-term motion synthesis with applications to character animation and other
domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zihan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Richard Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1&quot;&gt;Kfir Aberman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanocka_R/0/1/0/all/0/1&quot;&gt;Rana Hanocka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15045">
<title>A Transformer-based Approach for Arabic Offline Handwritten Text Recognition. (arXiv:2307.15045v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15045</link>
<description rdf:parseType="Literal">&lt;p&gt;Handwriting recognition is a challenging and critical problem in the fields
of pattern recognition and machine learning, with applications spanning a wide
range of domains. In this paper, we focus on the specific issue of recognizing
offline Arabic handwritten text. Existing approaches typically utilize a
combination of convolutional neural networks for image feature extraction and
recurrent neural networks for temporal modeling, with connectionist temporal
classification used for text generation. However, these methods suffer from a
lack of parallelization due to the sequential nature of recurrent neural
networks. Furthermore, these models cannot account for linguistic rules,
necessitating the use of an external language model in the post-processing
stage to boost accuracy. To overcome these issues, we introduce two alternative
architectures, namely the Transformer Transducer and the standard
sequence-to-sequence Transformer, and compare their performance in terms of
accuracy and speed. Our approach can model language dependencies and relies
only on the attention mechanism, thereby making it more parallelizable and less
complex. We employ pre-trained Transformers for both image understanding and
language modeling. Our evaluation on the Arabic KHATT dataset demonstrates that
our proposed method outperforms the current state-of-the-art approaches for
recognizing offline Arabic handwritten text.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Momeni_S/0/1/0/all/0/1&quot;&gt;Saleh Momeni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+BabaAli_B/0/1/0/all/0/1&quot;&gt;Bagher BabaAli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15049">
<title>Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models. (arXiv:2307.15049v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15049</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt tuning and adapter tuning have shown great potential in transferring
pre-trained vision-language models (VLMs) to various downstream tasks. In this
work, we design a new type of tuning method, termed as regularized mask tuning,
which masks the network parameters through a learnable selection. Inspired by
neural pathways, we argue that the knowledge required by a downstream task
already exists in the pre-trained weights but just gets concealed in the
upstream pre-training stage. To bring the useful knowledge back into light, we
first identify a set of parameters that are important to a given downstream
task, then attach a binary mask to each parameter, and finally optimize these
masks on the downstream data with the parameters frozen. When updating the
mask, we introduce a novel gradient dropout strategy to regularize the
parameter selection, in order to prevent the model from forgetting old
knowledge and overfitting the downstream data. Experimental results on 11
datasets demonstrate the consistent superiority of our method over previous
alternatives. It is noteworthy that we manage to deliver 18.73% performance
improvement compared to the zero-shot CLIP via masking an average of only 2.56%
parameters. Furthermore, our method is synergistic with most existing
parameter-efficient tuning methods and can boost the performance on top of
them. Project page can be found here (https://wuw2019.github.io/RMT/).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1&quot;&gt;Kecheng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1&quot;&gt;Ruili Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kai Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiawei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Deli Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1&quot;&gt;Zheng-Jun Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yujun Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15052">
<title>Learning Depth Estimation for Transparent and Mirror Surfaces. (arXiv:2307.15052v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15052</link>
<description rdf:parseType="Literal">&lt;p&gt;Inferring the depth of transparent or mirror (ToM) surfaces represents a hard
challenge for either sensors, algorithms, or deep networks. We propose a simple
pipeline for learning to estimate depth properly for such surfaces with neural
networks, without requiring any ground-truth annotation. We unveil how to
obtain reliable pseudo labels by in-painting ToM objects in images and
processing them with a monocular depth estimation model. These labels can be
used to fine-tune existing monocular or stereo networks, to let them learn how
to deal with ToM surfaces. Experimental results on the Booster dataset show the
dramatic improvements enabled by our remarkably simple proposal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costanzino_A/0/1/0/all/0/1&quot;&gt;Alex Costanzino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramirez_P/0/1/0/all/0/1&quot;&gt;Pierluigi Zama Ramirez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poggi_M/0/1/0/all/0/1&quot;&gt;Matteo Poggi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tosi_F/0/1/0/all/0/1&quot;&gt;Fabio Tosi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mattoccia_S/0/1/0/all/0/1&quot;&gt;Stefano Mattoccia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stefano_L/0/1/0/all/0/1&quot;&gt;Luigi Di Stefano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15055">
<title>PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking. (arXiv:2307.15055v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15055</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce PointOdyssey, a large-scale synthetic dataset, and data
generation framework, for the training and evaluation of long-term fine-grained
tracking algorithms. Our goal is to advance the state-of-the-art by placing
emphasis on long videos with naturalistic motion. Toward the goal of
naturalism, we animate deformable characters using real-world motion capture
data, we build 3D scenes to match the motion capture environments, and we
render camera viewpoints using trajectories mined via structure-from-motion on
real videos. We create combinatorial diversity by randomizing character
appearance, motion profiles, materials, lighting, 3D assets, and atmospheric
effects. Our dataset currently includes 104 videos, averaging 2,000 frames
long, with orders of magnitude more correspondence annotations than prior work.
We show that existing methods can be trained from scratch in our dataset and
outperform the published variants. Finally, we introduce modifications to the
PIPs point tracking method, greatly widening its temporal receptive field,
which improves its performance on PointOdyssey as well as on two real-world
benchmarks. Our data and code are publicly available at:
https://pointodyssey.com
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1&quot;&gt;Adam W. Harley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1&quot;&gt;Bokui Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1&quot;&gt;Gordon Wetzstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1&quot;&gt;Leonidas J. Guibas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15058">
<title>MARS: An Instance-aware, Modular and Realistic Simulator for Autonomous Driving. (arXiv:2307.15058v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15058</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays, autonomous cars can drive smoothly in ordinary cases, and it is
widely recognized that realistic sensor simulation will play a critical role in
solving remaining corner cases by simulating them. To this end, we propose an
autonomous driving simulator based upon neural radiance fields (NeRFs).
Compared with existing works, ours has three notable features: (1)
Instance-aware. Our simulator models the foreground instances and background
environments separately with independent networks so that the static (e.g.,
size and appearance) and dynamic (e.g., trajectory) properties of instances can
be controlled separately. (2) Modular. Our simulator allows flexible switching
between different modern NeRF-related backbones, sampling strategies, input
modalities, etc. We expect this modular design to boost academic progress and
industrial deployment of NeRF-based autonomous driving simulation. (3)
Realistic. Our simulator set new state-of-the-art photo-realism results given
the best module selection. Our simulator will be open-sourced while most of our
counterparts are not. Project page: https://open-air-sun.github.io/mars/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zirui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Liyi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1&quot;&gt;Zhide Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianteng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1&quot;&gt;Hongmin Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_C/0/1/0/all/0/1&quot;&gt;Chao Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_H/0/1/0/all/0/1&quot;&gt;Haozhe Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuantao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Runyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zike Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yongliang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1&quot;&gt;Yiyi Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hao Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15061">
<title>The RoboDepth Challenge: Methods and Advancements Towards Robust Depth Estimation. (arXiv:2307.15061v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15061</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate depth estimation under out-of-distribution (OoD) scenarios, such as
adverse weather conditions, sensor failure, and noise contamination, is
desirable for safety-critical applications. Existing depth estimation systems,
however, suffer inevitably from real-world corruptions and perturbations and
are struggled to provide reliable depth predictions under such cases. In this
paper, we summarize the winning solutions from the RoboDepth Challenge -- an
academic competition designed to facilitate and advance robust OoD depth
estimation. This challenge was developed based on the newly established KITTI-C
and NYUDepth2-C benchmarks. We hosted two stand-alone tracks, with an emphasis
on robust self-supervised and robust fully-supervised depth estimation,
respectively. Out of more than two hundred participants, nine unique and
top-performing solutions have appeared, with novel designs ranging from the
following aspects: spatial- and frequency-domain augmentations, masked image
modeling, image restoration and super-resolution, adversarial training,
diffusion-based noise suppression, vision-language pre-training, learned model
ensembling, and hierarchical feature enhancement. Extensive experimental
analyses along with insightful observations are drawn to better understand the
rationale behind each design. We hope this challenge could lay a solid
foundation for future research on robust and reliable depth estimation and
beyond. The datasets, competition toolkit, workshop recordings, and source code
from the winning teams are publicly available on the challenge website.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1&quot;&gt;Lingdong Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1&quot;&gt;Yaru Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Shaoyuan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hanjiang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_L/0/1/0/all/0/1&quot;&gt;Lai Xing Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cottereau_B/0/1/0/all/0/1&quot;&gt;Benoit R. Cottereau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Ding Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liangjun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hesheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ooi_W/0/1/0/all/0/1&quot;&gt;Wei Tsang Ooi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1&quot;&gt;Ruijie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Ziyang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Li Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianzhu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_M/0/1/0/all/0/1&quot;&gt;Mohan Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Pengwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xiaohua Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Cheng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yingfeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Jie Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kan_Z/0/1/0/all/0/1&quot;&gt;Zhen Kan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_Q/0/1/0/all/0/1&quot;&gt;Qiang Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1&quot;&gt;Liang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Minglei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Di Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Changpeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuanqi Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1&quot;&gt;Gang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuai_J/0/1/0/all/0/1&quot;&gt;Jian Kuai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Junjun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiamian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Baojun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiale Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ao_S/0/1/0/all/0/1&quot;&gt;Sun Ao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Runze Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1&quot;&gt;Haiyong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1&quot;&gt;Fang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jingze Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15063">
<title>To Adapt or Not to Adapt? Real-Time Adaptation for Semantic Segmentation. (arXiv:2307.15063v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15063</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of Online Domain Adaptation for semantic segmentation is to handle
unforeseeable domain changes that occur during deployment, like sudden weather
events. However, the high computational costs associated with brute-force
adaptation make this paradigm unfeasible for real-world applications. In this
paper we propose HAMLET, a Hardware-Aware Modular Least Expensive Training
framework for real-time domain adaptation. Our approach includes a
hardware-aware back-propagation orchestration agent (HAMT) and a dedicated
domain-shift detector that enables active control over when and how the model
is adapted (LT). Thanks to these advancements, our approach is capable of
performing semantic segmentation while simultaneously adapting at more than
29FPS on a single consumer-grade GPU. Our framework&apos;s encouraging accuracy and
speed trade-off is demonstrated on OnDA and SHIFT benchmarks through
experimental results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colomer_M/0/1/0/all/0/1&quot;&gt;Marc Botet Colomer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dovesi_P/0/1/0/all/0/1&quot;&gt;Pier Luigi Dovesi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panagiotakopoulos_T/0/1/0/all/0/1&quot;&gt;Theodoros Panagiotakopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carvalho_J/0/1/0/all/0/1&quot;&gt;Joao Frederico Carvalho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harenstam_Nielsen_L/0/1/0/all/0/1&quot;&gt;Linus H&amp;#xe4;renstam-Nielsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azizpour_H/0/1/0/all/0/1&quot;&gt;Hossein Azizpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1&quot;&gt;Hedvig Kjellstr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1&quot;&gt;Daniel Cremers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poggi_M/0/1/0/all/0/1&quot;&gt;Matteo Poggi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15064">
<title>Self-Supervised Visual Acoustic Matching. (arXiv:2307.15064v1 [cs.MM])</title>
<link>http://arxiv.org/abs/2307.15064</link>
<description rdf:parseType="Literal">&lt;p&gt;Acoustic matching aims to re-synthesize an audio clip to sound as if it were
recorded in a target acoustic environment. Existing methods assume access to
paired training data, where the audio is observed in both source and target
environments, but this limits the diversity of training data or requires the
use of simulated data or heuristics to create paired samples. We propose a
self-supervised approach to visual acoustic matching where training samples
include only the target scene image and audio -- without acoustically
mismatched source audio for reference. Our approach jointly learns to
disentangle room acoustics and re-synthesize audio into the target environment,
via a conditional GAN framework and a novel metric that quantifies the level of
residual acoustic information in the de-biased audio. Training with either
in-the-wild web data or simulated data, we demonstrate it outperforms the
state-of-the-art on multiple challenging datasets and a wide variety of
real-world audio and environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Somayazulu_A/0/1/0/all/0/1&quot;&gt;Arjun Somayazulu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1&quot;&gt;Kristen Grauman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2005.00695">
<title>On the Generalization Effects of Linear Transformations in Data Augmentation. (arXiv:2005.00695v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2005.00695</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation is a powerful technique to improve performance in
applications such as image and text classification tasks. Yet, there is little
rigorous understanding of why and how various augmentations work. In this work,
we consider a family of linear transformations and study their effects on the
ridge estimator in an over-parametrized linear regression setting. First, we
show that transformations that preserve the labels of the data can improve
estimation by enlarging the span of the training data. Second, we show that
transformations that mix data can improve estimation by playing a
regularization effect. Finally, we validate our theoretical insights on MNIST.
Based on the insights, we propose an augmentation scheme that searches over the
space of transformations by how uncertain the model is about the transformed
data. We validate our proposed scheme on image and text datasets. For example,
our method outperforms random sampling methods by 1.24% on CIFAR-100 using
Wide-ResNet-28-10. Furthermore, we achieve comparable accuracy to the SoTA
Adversarial AutoAugment on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Sen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongyang R. Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valiant_G/0/1/0/all/0/1&quot;&gt;Gregory Valiant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1&quot;&gt;Christopher R&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.10206">
<title>DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer. (arXiv:2103.10206v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2103.10206</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating 3D dances from music is an emerged research task that benefits a
lot of applications in vision and graphics. Previous works treat this task as
sequence generation, however, it is challenging to render a music-aligned
long-term sequence with high kinematic complexity and coherent movements. In
this paper, we reformulate it by a two-stage process, ie, a key pose generation
and then an in-between parametric motion curve prediction, where the key poses
are easier to be synchronized with the music beats and the parametric curves
can be efficiently regressed to render fluent rhythm-aligned movements. We
named the proposed method as DanceFormer, which includes two cascading
kinematics-enhanced transformer-guided networks (called DanTrans) that tackle
each stage, respectively. Furthermore, we propose a large-scale music
conditioned 3D dance dataset, called PhantomDance, that is accurately labeled
by experienced animators rather than reconstruction or motion capture. This
dataset also encodes dances as key poses and parametric motion curves apart
from pose sequences, thus benefiting the training of our DanceFormer. Extensive
experiments demonstrate that the proposed method, even trained by existing
datasets, can generate fluent, performative, and music-matched 3D dances that
surpass previous works quantitatively and qualitatively. Moreover, the proposed
DanceFormer, together with the PhantomDance dataset
(https://github.com/libuyu/PhantomDanceDataset), are seamlessly compatible with
industrial animation software, thus facilitating the adaptation for various
downstream applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Buyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yongchi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhelun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1&quot;&gt;Lu Sheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.04812">
<title>The Impact of Partial Occlusion on Pedestrian Detectability. (arXiv:2205.04812v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.04812</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust detection of vulnerable road users is a safety critical requirement
for the deployment of autonomous vehicles in heterogeneous traffic. One of the
most complex outstanding challenges is that of partial occlusion where a target
object is only partially available to the sensor due to obstruction by another
foreground object. A number of leading pedestrian detection benchmarks provide
annotation for partial occlusion, however each benchmark varies greatly in
their definition of the occurrence and severity of occlusion. Recent research
demonstrates that a high degree of subjectivity is used to classify occlusion
level in these cases and occlusion is typically categorized into 2 to 3 broad
categories such as partially and heavily occluded. This can lead to inaccurate
or inconsistent reporting of pedestrian detection model performance depending
on which benchmark is used. This research introduces a novel, objective
benchmark for partially occluded pedestrian detection to facilitate the
objective characterization of pedestrian detection models. Characterization is
carried out on seven popular pedestrian detection models for a range of
occlusion levels from 0-99%, in order to demonstrate the efficacy and increased
analysis capabilities of the proposed characterization method. Results
demonstrate that pedestrian detection performance degrades, and the number of
false negative detections increase as pedestrian occlusion level increases. Of
the seven popular pedestrian detection routines characterized, CenterNet has
the greatest overall performance, followed by SSDlite. RetinaNet has the lowest
overall detection performance across the range of occlusion levels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilroy_S/0/1/0/all/0/1&quot;&gt;Shane Gilroy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mullins_D/0/1/0/all/0/1&quot;&gt;Darragh Mullins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_E/0/1/0/all/0/1&quot;&gt;Edward Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parsi_A/0/1/0/all/0/1&quot;&gt;Ashkan Parsi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glavin_M/0/1/0/all/0/1&quot;&gt;Martin Glavin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.00052">
<title>Visual Pre-training for Navigation: What Can We Learn from Noise?. (arXiv:2207.00052v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.00052</link>
<description rdf:parseType="Literal">&lt;p&gt;One powerful paradigm in visual navigation is to predict actions from
observations directly. Training such an end-to-end system allows
representations useful for downstream tasks to emerge automatically. However,
the lack of inductive bias makes this system data inefficient. We hypothesize a
sufficient representation of the current view and the goal view for a
navigation policy can be learned by predicting the location and size of a crop
of the current view that corresponds to the goal. We further show that training
such random crop prediction in a self-supervised fashion purely on synthetic
noise images transfers well to natural home images. The learned representation
can then be bootstrapped to learn a navigation policy efficiently with little
interaction data. The code is available at https://yanweiw.github.io/noise2ptz
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_C/0/1/0/all/0/1&quot;&gt;Ching-Yun Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1&quot;&gt;Pulkit Agrawal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.11860">
<title>Behind Every Domain There is a Shift: Adapting Distortion-aware Vision Transformers for Panoramic Semantic Segmentation. (arXiv:2207.11860v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.11860</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we address panoramic semantic segmentation which is
under-explored due to two critical challenges: (1) image distortions and object
deformations on panoramas; (2) lack of semantic annotations in the 360-degree
imagery. To tackle these problems, first, we propose the upgraded Transformer
for Panoramic Semantic Segmentation, i.e., Trans4PASS+, equipped with
Deformable Patch Embedding (DPE) and Deformable MLP (DMLPv2) modules for
handling object deformations and image distortions whenever (before or after
adaptation) and wherever (shallow or deep levels). Second, we enhance the
Mutual Prototypical Adaptation (MPA) strategy via pseudo-label rectification
for unsupervised domain adaptive panoramic segmentation. Third, aside from
Pinhole-to-Panoramic (Pin2Pan) adaptation, we create a new dataset (SynPASS)
with 9,080 panoramic images, facilitating Synthetic-to-Real (Syn2Real)
adaptation scheme in 360-degree imagery. Extensive experiments are conducted,
which cover indoor and outdoor scenarios, and each of them is investigated with
Pin2Pan and Syn2Real regimens. Trans4PASS+ achieves state-of-the-art
performances on four domain adaptive panoramic semantic segmentation
benchmarks. Code is available at https://github.com/jamycheung/Trans4PASS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kailun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Hao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reiss_S/0/1/0/all/0/1&quot;&gt;Simon Rei&amp;#xdf;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1&quot;&gt;Kunyu Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chaoxiang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Haodong Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip H. S. Torr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaiwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1&quot;&gt;Rainer Stiefelhagen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.12300">
<title>A Deep Perceptual Measure for Lens and Camera Calibration. (arXiv:2208.12300v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.12300</link>
<description rdf:parseType="Literal">&lt;p&gt;Image editing and compositing have become ubiquitous in entertainment, from
digital art to AR and VR experiences. To produce beautiful composites, the
camera needs to be geometrically calibrated, which can be tedious and requires
a physical calibration target. In place of the traditional multi-image
calibration process, we propose to infer the camera calibration parameters such
as pitch, roll, field of view, and lens distortion directly from a single image
using a deep convolutional neural network. We train this network using
automatically generated samples from a large-scale panorama dataset, yielding
competitive accuracy in terms of standard `2 error. However, we argue that
minimizing such standard error metrics might not be optimal for many
applications. In this work, we investigate human sensitivity to inaccuracies in
geometric camera calibration. To this end, we conduct a large-scale human
perception study where we ask participants to judge the realism of 3D objects
composited with correct and biased camera calibration parameters. Based on this
study, we develop a new perceptual measure for camera calibration and
demonstrate that our deep calibration network outperforms previous single-image
based calibration methods both on standard metrics as well as on this novel
perceptual measure. Finally, we demonstrate the use of our calibration network
for several applications, including virtual object insertion, image retrieval,
and compositing. A demonstration of our approach is available at
https://lvsn.github.io/deepcalib .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hold_Geoffroy_Y/0/1/0/all/0/1&quot;&gt;Yannick Hold-Geoffroy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piche_Meunier_D/0/1/0/all/0/1&quot;&gt;Dominique Pich&amp;#xe9;-Meunier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1&quot;&gt;Kalyan Sunkavalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bazin_J/0/1/0/all/0/1&quot;&gt;Jean-Charles Bazin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rameau_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Rameau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lalonde_J/0/1/0/all/0/1&quot;&gt;Jean-Fran&amp;#xe7;ois Lalonde&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.08944">
<title>Reasons for the Superiority of Stochastic Estimators over Deterministic Ones: Robustness, Consistency and Perceptual Quality. (arXiv:2211.08944v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.08944</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic restoration algorithms allow to explore the space of solutions
that correspond to the degraded input. In this paper we reveal additional
fundamental advantages of stochastic methods over deterministic ones, which
further motivate their use. First, we prove that any restoration algorithm that
attains perfect perceptual quality and whose outputs are consistent with the
input must be a posterior sampler, and is thus required to be stochastic.
Second, we illustrate that while deterministic restoration algorithms may
attain high perceptual quality, this can be achieved only by filling up the
space of all possible source images using an extremely sensitive mapping, which
makes them highly vulnerable to adversarial attacks. Indeed, we show that
enforcing deterministic models to be robust to such attacks profoundly hinders
their perceptual quality, while robustifying stochastic models hardly
influences their perceptual quality, and improves their output variability.
These findings provide a motivation to foster progress in stochastic
restoration methods, paving the way to better recovery algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ohayon_G/0/1/0/all/0/1&quot;&gt;Guy Ohayon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Adrai_T/0/1/0/all/0/1&quot;&gt;Theo Adrai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Elad_M/0/1/0/all/0/1&quot;&gt;Michael Elad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Michaeli_T/0/1/0/all/0/1&quot;&gt;Tomer Michaeli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11220">
<title>STGlow: A Flow-based Generative Framework with Dual Graphormer for Pedestrian Trajectory Prediction. (arXiv:2211.11220v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11220</link>
<description rdf:parseType="Literal">&lt;p&gt;The pedestrian trajectory prediction task is an essential component of
intelligent systems. Its applications include but are not limited to autonomous
driving, robot navigation, and anomaly detection of monitoring systems. Due to
the diversity of motion behaviors and the complex social interactions among
pedestrians, accurately forecasting their future trajectory is challenging.
Existing approaches commonly adopt GANs or CVAEs to generate diverse
trajectories. However, GAN-based methods do not directly model data in a latent
space, which may make them fail to have full support over the underlying data
distribution; CVAE-based methods optimize a lower bound on the log-likelihood
of observations, which may cause the learned distribution to deviate from the
underlying distribution. The above limitations make existing approaches often
generate highly biased or inaccurate trajectories. In this paper, we propose a
novel generative flow based framework with dual graphormer for pedestrian
trajectory prediction (STGlow). Different from previous approaches, our method
can more precisely model the underlying data distribution by optimizing the
exact log-likelihood of motion behaviors. Besides, our method has clear
physical meanings for simulating the evolution of human motion behaviors. The
forward process of the flow gradually degrades complex motion behavior into
simple behavior, while its reverse process represents the evolution of simple
behavior into complex motion behavior. Further, we introduce a dual graphormer
combining with the graph structure to more adequately model the temporal
dependencies and the mutual spatial interactions. Experimental results on
several benchmarks demonstrate that our method achieves much better performance
compared to previous state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1&quot;&gt;Rongqin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanman Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiantao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xia Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.12292">
<title>Exemplar-free Continual Learning of Vision Transformers via Gated Class-Attention and Cascaded Feature Drift Compensation. (arXiv:2211.12292v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.12292</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new method for exemplar-free class incremental training of ViTs.
The main challenge of exemplar-free continual learning is maintaining
plasticity of the learner without causing catastrophic forgetting of previously
learned tasks. This is often achieved via exemplar replay which can help
recalibrate previous task classifiers to the feature drift which occurs when
learning new tasks. Exemplar replay, however, comes at the cost of retaining
samples from previous tasks which for many applications may not be possible. To
address the problem of continual ViT training, we first propose gated
class-attention to minimize the drift in the final ViT transformer block. This
mask-based gating is applied to class-attention mechanism of the last
transformer block and strongly regulates the weights crucial for previous
tasks. Importantly, gated class-attention does not require the task-ID during
inference, which distinguishes it from other parameter isolation methods.
Secondly, we propose a new method of feature drift compensation that
accommodates feature drift in the backbone when learning new tasks. The
combination of gated class-attention and cascaded feature drift compensation
allows for plasticity towards new tasks while limiting forgetting of previous
ones. Extensive experiments performed on CIFAR-100, Tiny-ImageNet and
ImageNet100 demonstrate that our exemplar-free method obtains competitive
results when compared to rehearsal based ViT methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotogni_M/0/1/0/all/0/1&quot;&gt;Marco Cotogni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cusano_C/0/1/0/all/0/1&quot;&gt;Claudio Cusano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagdanov_A/0/1/0/all/0/1&quot;&gt;Andrew D. Bagdanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1&quot;&gt;Joost van de Weijer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.16762">
<title>GeoUDF: Surface Reconstruction from 3D Point Clouds via Geometry-guided Distance Representation. (arXiv:2211.16762v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.16762</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a learning-based method, namely GeoUDF,to tackle the long-standing
and challenging problem of reconstructing a discrete surface from a sparse
point cloud.To be specific, we propose a geometry-guided learning method for
UDF and its gradient estimation that explicitly formulates the unsigned
distance of a query point as the learnable affine averaging of its distances to
the tangent planes of neighboring points on the surface. Besides,we model the
local geometric structure of the input point clouds by explicitly learning a
quadratic polynomial for each point. This not only facilitates upsampling the
input sparse point cloud but also naturally induces unoriented normal, which
further augments UDF estimation. Finally, to extract triangle meshes from the
predicted UDF we propose a customized edge-based marching cube module. We
conduct extensive experiments and ablation studies to demonstrate the
significant advantages of our method over state-of-the-art methods in terms of
reconstruction accuracy, efficiency, and generality. The source code is
publicly available at https://github.com/rsy6318/GeoUDF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Siyu Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Junhui Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaodong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Ying He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.13381">
<title>MixupE: Understanding and Improving Mixup from Directional Derivative Perspective. (arXiv:2212.13381v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.13381</link>
<description rdf:parseType="Literal">&lt;p&gt;Mixup is a popular data augmentation technique for training deep neural
networks where additional samples are generated by linearly interpolating pairs
of inputs and their labels. This technique is known to improve the
generalization performance in many learning paradigms and applications. In this
work, we first analyze Mixup and show that it implicitly regularizes infinitely
many directional derivatives of all orders. Based on this new insight, we
propose an improved version of Mixup, theoretically justified to deliver better
generalization performance than the vanilla Mixup. To demonstrate the
effectiveness of the proposed method, we conduct experiments across various
domains such as images, tabular data, speech, and graphs. Our results show that
the proposed method improves Mixup across multiple datasets using a variety of
architectures, for instance, exhibiting an improvement over Mixup by 0.8% in
ImageNet top-1 accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1&quot;&gt;Yingtian Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1&quot;&gt;Vikas Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1&quot;&gt;Sarthak Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1&quot;&gt;Wai Hoh Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1&quot;&gt;Hieu Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1&quot;&gt;Juho Kannala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1&quot;&gt;Arno Solin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1&quot;&gt;Kenji Kawaguchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.13504">
<title>DAE-Former: Dual Attention-guided Efficient Transformer for Medical Image Segmentation. (arXiv:2212.13504v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.13504</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have recently gained attention in the computer vision domain due
to their ability to model long-range dependencies. However, the self-attention
mechanism, which is the core part of the Transformer model, usually suffers
from quadratic computational complexity with respect to the number of tokens.
Many architectures attempt to reduce model complexity by limiting the
self-attention mechanism to local regions or by redesigning the tokenization
process. In this paper, we propose DAE-Former, a novel method that seeks to
provide an alternative perspective by efficiently designing the self-attention
mechanism. More specifically, we reformulate the self-attention mechanism to
capture both spatial and channel relations across the whole feature dimension
while staying computationally efficient. Furthermore, we redesign the skip
connection path by including the cross-attention module to ensure the feature
reusability and enhance the localization power. Our method outperforms
state-of-the-art methods on multi-organ cardiac and skin lesion segmentation
datasets without requiring pre-training weights. The code is publicly available
at https://github.com/mindflow-institue/DAEFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azad_R/0/1/0/all/0/1&quot;&gt;Reza Azad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arimond_R/0/1/0/all/0/1&quot;&gt;Ren&amp;#xe9; Arimond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aghdam_E/0/1/0/all/0/1&quot;&gt;Ehsan Khodapanah Aghdam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazerouni_A/0/1/0/all/0/1&quot;&gt;Amirhossein Kazerouni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merhof_D/0/1/0/all/0/1&quot;&gt;Dorit Merhof&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01226">
<title>Factor Fields: A Unified Framework for Neural Fields and Beyond. (arXiv:2302.01226v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01226</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Factor Fields, a novel framework for modeling and representing
signals. Factor Fields decomposes a signal into a product of factors, each
represented by a classical or neural field representation which operates on
transformed input coordinates. This decomposition results in a unified
framework that accommodates several recent signal representations including
NeRF, Plenoxels, EG3D, Instant-NGP, and TensoRF. Additionally, our framework
allows for the creation of powerful new signal representations, such as the
&quot;Dictionary Field&quot; (DiF) which is a second contribution of this paper. Our
experiments show that DiF leads to improvements in approximation quality,
compactness, and training time when compared to previous fast reconstruction
methods. Experimentally, our representation achieves better image approximation
quality on 2D image regression tasks, higher geometric quality when
reconstructing 3D signed distance fields, and higher compactness for radiance
field reconstruction tasks. Furthermore, DiF enables generalization to unseen
images/3D scenes by sharing bases across signals during training which greatly
benefits use cases such as image regression from sparse observations and
few-shot radiance field reconstruction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Anpei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zexiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xinyue Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Siyu Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1&quot;&gt;Andreas Geiger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08720">
<title>Algorithmic Hallucinations of Near-Surface Winds: Statistical Downscaling with Generative Adversarial Networks to Convection-Permitting Scales. (arXiv:2302.08720v2 [physics.ao-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08720</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the application of emerging machine learning methods from
image super-resolution (SR) to the task of statistical downscaling. We
specifically focus on convolutional neural network-based Generative Adversarial
Networks (GANs). Our GANs are conditioned on low-resolution (LR) inputs to
generate high-resolution (HR) surface winds emulating Weather Research and
Forecasting (WRF) model simulations over North America. Unlike traditional SR
models, where LR inputs are idealized coarsened versions of the HR images, WRF
emulation involves using non-idealized LR and HR pairs resulting in
shared-scale mismatches due to internal variability. Our study builds upon
current SR-based statistical downscaling by experimenting with a novel
frequency-separation (FS) approach from the computer vision field. To assess
the skill of SR models, we carefully select evaluation metrics, and focus on
performance measures based on spatial power spectra. Our analyses reveal how
GAN configurations influence spatial structures in the generated fields,
particularly biases in spatial variability spectra. Using power spectra to
evaluate the FS experiments reveals that successful applications of FS in
computer vision do not translate to climate fields. However, the FS experiments
demonstrate the sensitivity of power spectra to a commonly used GAN-based SR
objective function, which helps interpret and understand its role in
determining spatial structures. This result motivates the development of a
novel partial frequency-separation scheme as a promising configuration option.
We also quantify the influence on GAN performance of non-idealized LR fields
resulting from internal variability. Furthermore, we conduct a spectra-based
feature-importance experiment allowing us to explore the dependence of the
spatial structure of generated fields on different physically relevant LR
covariates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Annau_N/0/1/0/all/0/1&quot;&gt;Nicolaas J. Annau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cannon_A/0/1/0/all/0/1&quot;&gt;Alex J. Cannon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Monahan_A/0/1/0/all/0/1&quot;&gt;Adam H. Monahan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01669">
<title>Learning Common Rationale to Improve Self-Supervised Representation for Fine-Grained Visual Recognition Problems. (arXiv:2303.01669v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01669</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning (SSL) strategies have demonstrated remarkable
performance in various recognition tasks. However, both our preliminary
investigation and recent studies suggest that they may be less effective in
learning representations for fine-grained visual recognition (FGVR) since many
features helpful for optimizing SSL objectives are not suitable for
characterizing the subtle differences in FGVR. To overcome this issue, we
propose learning an additional screening mechanism to identify discriminative
clues commonly seen across instances and classes, dubbed as common rationales
in this paper. Intuitively, common rationales tend to correspond to the
discriminative patterns from the key parts of foreground objects. We show that
a common rationale detector can be learned by simply exploiting the GradCAM
induced from the SSL objective without using any pre-trained object parts or
saliency detectors, making it seamlessly to be integrated with the existing SSL
process. Specifically, we fit the GradCAM with a branch with limited fitting
capacity, which allows the branch to capture the common rationales and discard
the less common discriminative patterns. At the test stage, the branch
generates a set of spatial weights to selectively aggregate features
representing an instance. Extensive experimental results on four visual tasks
demonstrate that the proposed method can lead to a significant improvement in
different evaluation settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1&quot;&gt;Yangyang Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1&quot;&gt;Anton van den Hengel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingqiao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.00570">
<title>FedFTN: Personalized Federated Learning with Deep Feature Transformation Network for Multi-institutional Low-count PET Denoising. (arXiv:2304.00570v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.00570</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-count PET is an efficient way to reduce radiation exposure and
acquisition time, but the reconstructed images often suffer from low
signal-to-noise ratio (SNR), thus affecting diagnosis and other downstream
tasks. Recent advances in deep learning have shown great potential in improving
low-count PET image quality, but acquiring a large, centralized, and diverse
dataset from multiple institutions for training a robust model is difficult due
to privacy and security concerns of patient data. Moreover, low-count PET data
at different institutions may have different data distribution, thus requiring
personalized models. While previous federated learning (FL) algorithms enable
multi-institution collaborative training without the need of aggregating local
data, addressing the large domain shift in the application of
multi-institutional low-count PET denoising remains a challenge and is still
highly under-explored. In this work, we propose FedFTN, a personalized
federated learning strategy that addresses these challenges. FedFTN uses a
local deep feature transformation network (FTN) to modulate the feature outputs
of a globally shared denoising network, enabling personalized low-count PET
denoising for each institution. During the federated learning process, only the
denoising network&apos;s weights are communicated and aggregated, while the FTN
remains at the local institutions for feature transformation. We evaluated our
method using a large-scale dataset of multi-institutional low-count PET imaging
data from three medical centers located across three continents, and showed
that FedFTN provides high-quality low-count PET images, outperforming previous
baseline FL reconstruction methods across all low-count levels at all three
institutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xie_H/0/1/0/all/0/1&quot;&gt;Huidong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiongchao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xueqi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;S. Kevin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Biao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rominger_A/0/1/0/all/0/1&quot;&gt;Axel Rominger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_K/0/1/0/all/0/1&quot;&gt;Kuangyu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Duncan_J/0/1/0/all/0/1&quot;&gt;James S. Duncan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chi Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01986">
<title>USTC FLICAR: A Sensors Fusion Dataset of LiDAR-Inertial-Camera for Heavy-duty Autonomous Aerial Work Robots. (arXiv:2304.01986v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01986</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present the USTC FLICAR Dataset, which is dedicated to the
development of simultaneous localization and mapping and precise 3D
reconstruction of the workspace for heavy-duty autonomous aerial work robots.
In recent years, numerous public datasets have played significant roles in the
advancement of autonomous cars and unmanned aerial vehicles (UAVs). However,
these two platforms differ from aerial work robots: UAVs are limited in their
payload capacity, while cars are restricted to two-dimensional movements. To
fill this gap, we create the &quot;Giraffe&quot; mapping robot based on a bucket truck,
which is equipped with a variety of well-calibrated and synchronized sensors:
four 3D LiDARs, two stereo cameras, two monocular cameras, Inertial Measurement
Units (IMUs), and a GNSS/INS system. A laser tracker is used to record the
millimeter-level ground truth positions. We also make its ground twin, the
&quot;Okapi&quot; mapping robot, to gather data for comparison. The proposed dataset
extends the typical autonomous driving sensing suite to aerial scenes,
demonstrating the potential of combining autonomous driving perception systems
with bucket trucks to create a versatile autonomous aerial working platform.
Moreover, based on the Segment Anything Model (SAM), we produce the Semantic
FLICAR dataset, which provides fine-grained semantic segmentation annotations
for multimodal continuous data in both temporal and spatial dimensions. The
dataset is available for download at: https://ustc-flicar.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yujiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1&quot;&gt;Yifan Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xingchen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinran Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jianmin Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_E/0/1/0/all/0/1&quot;&gt;Erbao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanyong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06648">
<title>DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning. (arXiv:2304.06648v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06648</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have proven to be highly effective in generating
high-quality images. However, adapting large pre-trained diffusion models to
new domains remains an open challenge, which is critical for real-world
applications. This paper proposes DiffFit, a parameter-efficient strategy to
fine-tune large pre-trained diffusion models that enable fast adaptation to new
domains. DiffFit is embarrassingly simple that only fine-tunes the bias term
and newly-added scaling factors in specific layers, yet resulting in
significant training speed-up and reduced model storage costs. Compared with
full fine-tuning, DiffFit achieves 2$\times$ training speed-up and only needs
to store approximately 0.12\% of the total model parameters. Intuitive
theoretical analysis has been provided to justify the efficacy of scaling
factors on fast adaptation. On 8 downstream datasets, DiffFit achieves superior
or competitive performances compared to the full fine-tuning while being more
efficient. Remarkably, we show that DiffFit can adapt a pre-trained
low-resolution generative model to a high-resolution one by adding minimal
cost. Among diffusion-based methods, DiffFit sets a new state-of-the-art FID of
3.02 on ImageNet 512$\times$512 benchmark by fine-tuning only 25 epochs from a
public pre-trained ImageNet 256$\times$256 checkpoint while being 30$\times$
more training efficient than the closest competitor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1&quot;&gt;Lewei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Han Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhili Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Daquan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhaoqiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04186">
<title>Video-Specific Query-Key Attention Modeling for Weakly-Supervised Temporal Action Localization. (arXiv:2305.04186v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04186</link>
<description rdf:parseType="Literal">&lt;p&gt;Weakly-supervised temporal action localization aims to identify and localize
the action instances in the untrimmed videos with only video-level action
labels. When humans watch videos, we can adapt our abstract-level knowledge
about actions in different video scenarios and detect whether some actions are
occurring. In this paper, we mimic how humans do and bring a new perspective
for locating and identifying multiple actions in a video. We propose a network
named VQK-Net with a video-specific query-key attention modeling that learns a
unique query for each action category of each input video. The learned queries
not only contain the actions&apos; knowledge features at the abstract level but also
have the ability to fit this knowledge into the target video scenario, and they
will be used to detect the presence of the corresponding action along the
temporal dimension. To better learn these action category queries, we exploit
not only the features of the current input video but also the correlation
between different videos through a novel video-specific action category query
learner worked with a query similarity loss. Finally, we conduct extensive
experiments on three commonly used datasets (THUMOS14, ActivityNet1.2, and
ActivityNet1.3) and achieve state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xijun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katsaggelos_A/0/1/0/all/0/1&quot;&gt;Aggelos K. Katsaggelos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05077">
<title>Atmospheric Turbulence Correction via Variational Deep Diffusion. (arXiv:2305.05077v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05077</link>
<description rdf:parseType="Literal">&lt;p&gt;Atmospheric Turbulence (AT) correction is a challenging restoration task as
it consists of two distortions: geometric distortion and spatially variant
blur. Diffusion models have shown impressive accomplishments in photo-realistic
image synthesis and beyond. In this paper, we propose a novel deep conditional
diffusion model under a variational inference framework to solve the AT
correction problem. We use this framework to improve performance by learning
latent prior information from the input and degradation processes. We use the
learned information to further condition the diffusion model. Experiments are
conducted in a comprehensive synthetic AT dataset. We show that the proposed
framework achieves good quantitative and qualitative results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xijun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_Tapia_S/0/1/0/all/0/1&quot;&gt;Santiago L&amp;#xf3;pez-Tapia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katsaggelos_A/0/1/0/all/0/1&quot;&gt;Aggelos K. Katsaggelos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06716">
<title>Distracting Downpour: Adversarial Weather Attacks for Motion Estimation. (arXiv:2305.06716v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06716</link>
<description rdf:parseType="Literal">&lt;p&gt;Current adversarial attacks on motion estimation, or optical flow, optimize
small per-pixel perturbations, which are unlikely to appear in the real world.
In contrast, adverse weather conditions constitute a much more realistic threat
scenario. Hence, in this work, we present a novel attack on motion estimation
that exploits adversarially optimized particles to mimic weather effects like
snowflakes, rain streaks or fog clouds. At the core of our attack framework is
a differentiable particle rendering system that integrates particles (i)
consistently over multiple time steps (ii) into the 3D space (iii) with a
photo-realistic appearance. Through optimization, we obtain adversarial weather
that significantly impacts the motion estimation. Surprisingly, methods that
previously showed good robustness towards small per-pixel perturbations are
particularly vulnerable to adversarial weather. At the same time, augmenting
the training with non-optimized weather increases a method&apos;s robustness towards
weather effects and improves generalizability at almost no additional cost. Our
code will be available at https://github.com/cv-stuttgart/DistractingDownpour.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmalfuss_J/0/1/0/all/0/1&quot;&gt;Jenny Schmalfuss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehl_L/0/1/0/all/0/1&quot;&gt;Lukas Mehl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruhn_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Bruhn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09160">
<title>SUG: Single-dataset Unified Generalization for 3D Point Cloud Classification. (arXiv:2305.09160v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09160</link>
<description rdf:parseType="Literal">&lt;p&gt;Although Domain Generalization (DG) problem has been fast-growing in the 2D
image tasks, its exploration on 3D point cloud data is still insufficient and
challenged by more complex and uncertain cross-domain variances with uneven
inter-class modality distribution. In this paper, different from previous 2D DG
works, we focus on the 3D DG problem and propose a Single-dataset Unified
Generalization (SUG) framework that only leverages a single source dataset to
alleviate the unforeseen domain differences faced by a well-trained source
model. Specifically, we first design a Multi-grained Sub-domain Alignment (MSA)
method, which can constrain the learned representations to be domain-agnostic
and discriminative, by performing a multi-grained feature alignment process
between the splitted sub-domains from the single source dataset. Then, a
Sample-level Domain-aware Attention (SDA) strategy is presented, which can
selectively enhance easy-to-adapt samples from different sub-domains according
to the sample-level inter-domain distance to avoid the negative transfer.
Experiments demonstrate that our SUG can boost the generalization ability for
unseen target domains, even outperforming the existing unsupervised domain
adaptation methods that have to access extensive target domain data. Our code
is available at https://github.com/SiyuanHuang95/SUG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Siyuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Botian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1&quot;&gt;Peng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yikang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10132">
<title>Automatic 3D Registration of Dental CBCT and Face Scan Data using 2D Projection Images. (arXiv:2305.10132v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10132</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a fully automatic registration method of dental cone-beam
computed tomography (CBCT) and face scan data. It can be used for a digital
platform of 3D jaw-teeth-face models in a variety of applications, including 3D
digital treatment planning and orthognathic surgery. Difficulties in accurately
merging facial scans and CBCT images are due to the different image acquisition
methods and limited area of correspondence between the two facial surfaces. In
addition, it is difficult to use machine learning techniques because they use
face-related 3D medical data with radiation exposure, which are difficult to
obtain for training. The proposed method addresses these problems by reusing an
existing machine-learning-based 2D landmark detection algorithm in an
open-source library and developing a novel mathematical algorithm that
identifies paired 3D landmarks from knowledge of the corresponding 2D
landmarks. A main contribution of this study is that the proposed method does
not require annotated training data of facial landmarks because it uses a
pre-trained facial landmark detection algorithm that is known to be robust and
generalized to various 2D face image models. Note that this reduces a 3D
landmark detection problem to a 2D problem of identifying the corresponding
landmarks on two 2D projection images generated from two different projection
angles. Here, the 3D landmarks for registration were selected from the
sub-surfaces with the least geometric change under the CBCT and face scan
environments. For the final fine-tuning of the registration, the Iterative
Closest Point method was applied, which utilizes geometrical information around
the 3D landmarks. The experimental results show that the proposed method
achieved an averaged surface distance error of 0.74 mm for three pairs of CBCT
and face scan datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1&quot;&gt;Hyoung Suk Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hyun_C/0/1/0/all/0/1&quot;&gt;Chang Min Hyun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sang-Hwy Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Jin Keun Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_K/0/1/0/all/0/1&quot;&gt;Kiwan Jeon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13849">
<title>Gaussian Latent Representations for Uncertainty Estimation using Mahalanobis Distance in Deep Classifiers. (arXiv:2305.13849v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13849</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent works show that the data distribution in a network&apos;s latent space is
useful for estimating classification uncertainty and detecting
Out-of-distribution (OOD) samples. To obtain a well-regularized latent space
that is conducive for uncertainty estimation, existing methods bring in
significant changes to model architectures and training procedures. In this
paper, we present a lightweight, fast, and high-performance regularization
method for Mahalanobis distance-based uncertainty prediction, and that requires
minimal changes to the network&apos;s architecture. To derive Gaussian latent
representation favourable for Mahalanobis Distance calculation, we introduce a
self-supervised representation learning method that separates in-class
representations into multiple Gaussians. Classes with non-Gaussian
representations are automatically identified and dynamically clustered into
multiple new classes that are approximately Gaussian. Evaluation on standard
OOD benchmarks shows that our method achieves state-of-the-art results on OOD
detection with minimal inference time, and is very competitive on predictive
probability calibration. Finally, we show the applicability of our method to a
real-life computer vision use case on microorganism classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkataramanan_A/0/1/0/all/0/1&quot;&gt;Aishwarya Venkataramanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benbihi_A/0/1/0/all/0/1&quot;&gt;Assia Benbihi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laviale_M/0/1/0/all/0/1&quot;&gt;Martin Laviale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pradalier_C/0/1/0/all/0/1&quot;&gt;Cedric Pradalier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02082">
<title>Unsupervised Low Light Image Enhancement Using SNR-Aware Swin Transformer. (arXiv:2306.02082v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02082</link>
<description rdf:parseType="Literal">&lt;p&gt;Image captured under low-light conditions presents unpleasing artifacts,
which debilitate the performance of feature extraction for many upstream visual
tasks. Low-light image enhancement aims at improving brightness and contrast,
and further reducing noise that corrupts the visual quality. Recently, many
image restoration methods based on Swin Transformer have been proposed and
achieve impressive performance. However, on one hand, trivially employing Swin
Transformer for low-light image enhancement would expose some artifacts,
including over-exposure, brightness imbalance and noise corruption, etc. On the
other hand, it is impractical to capture image pairs of low-light images and
corresponding ground-truth, i.e. well-exposed image in same visual scene. In
this paper, we propose a dual-branch network based on Swin Transformer, guided
by a signal-to-noise ratio prior map which provides the spatial-varying
information for low-light image enhancement. Moreover, we leverage unsupervised
learning to construct the optimization objective based on Retinex model, to
guide the training of proposed network. Experimental results demonstrate that
the proposed model is competitive with the baseline models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhijian Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiahui Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1&quot;&gt;Yueen Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zihan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yanzeng Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03753">
<title>AI Art Curation: Re-imagining the city of Helsinki in occasion of its Biennial. (arXiv:2306.03753v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03753</link>
<description rdf:parseType="Literal">&lt;p&gt;Art curatorial practice is characterized by the presentation of an art
collection in a knowledgeable way. Machine processes are characterized by their
capacity to manage and analyze large amounts of data. This paper envisages AI
curation and audience interaction to explore the implications of contemporary
machine learning models for the curatorial world. This project was developed
for the occasion of the 2023 Helsinki Art Biennial, entitled New Directions May
Emerge. We use the Helsinki Art Museum (HAM) collection to re-imagine the city
of Helsinki through the lens of machine perception. We use visual-textual
models to place indoor artworks in public spaces, assigning fictional
coordinates based on similarity scores. We transform the space that each
artwork inhabits in the city by generating synthetic 360 art panoramas. We
guide the generation estimating depth values from 360 panoramas at each artwork
location, and machine-generated prompts of the artworks. The result of this
project is an AI curation that places the artworks in their imagined physical
space, blurring the lines of artwork, context, and machine perception. The work
is virtually presented as a web-based installation on this link
&lt;a href=&quot;http://newlyformedcity.net/&quot;&gt;this http URL&lt;/a&gt;, where users can navigate an alternative version of
the city while exploring and interacting with its cultural heritage at scale.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaerf_L/0/1/0/all/0/1&quot;&gt;Ludovica Schaerf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ballesteros_P/0/1/0/all/0/1&quot;&gt;Pepe Ballesteros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernasconi_V/0/1/0/all/0/1&quot;&gt;Valentine Bernasconi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neri_I/0/1/0/all/0/1&quot;&gt;Iacopo Neri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castillo_D/0/1/0/all/0/1&quot;&gt;Dario Negueruela del Castillo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08541">
<title>Fine-Tuned but Zero-Shot 3D Shape Sketch View Similarity and Retrieval. (arXiv:2306.08541v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08541</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, encoders like ViT (vision transformer) and ResNet have been trained
on vast datasets and utilized as perceptual metrics for comparing sketches and
images, as well as multi-domain encoders in a zero-shot setting. However, there
has been limited effort to quantify the granularity of these encoders. Our work
addresses this gap by focusing on multi-modal 2D projections of individual 3D
instances. This task holds crucial implications for retrieval and sketch-based
modeling. We show that in a zero-shot setting, the more abstract the sketch,
the higher the likelihood of incorrect image matches. Even within the same
sketch domain, sketches of the same object drawn in different styles, for
example by distinct individuals, might not be accurately matched. One of the
key findings of our research is that meticulous fine-tuning on one class of 3D
shapes can lead to improved performance on other shape classes, reaching or
surpassing the accuracy of supervised methods. We compare and discuss several
fine-tuning strategies. Additionally, we delve deeply into how the scale of an
object in a sketch influences the similarity of features at different network
layers, helping us identify which network layers provide the most accurate
matching. Significantly, we discover that ViT and ResNet perform best when
dealing with similar object scales. We believe that our work will have a
significant impact on research in the sketch domain, providing insights and
guidance on how to adopt large pretrained models as perceptual losses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berardi_G/0/1/0/all/0/1&quot;&gt;Gianluca Berardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gryaditskaya_Y/0/1/0/all/0/1&quot;&gt;Yulia Gryaditskaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01524">
<title>Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation. (arXiv:2307.01524v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01524</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous vehicles and Advanced Driving Assistance Systems (ADAS) have the
potential to radically change the way we travel. Many such vehicles currently
rely on segmentation and object detection algorithms to detect and track
objects around its surrounding. The data collected from the vehicles are often
sent to cloud servers to facilitate continual/life-long learning of these
algorithms. Considering the bandwidth constraints, the data is compressed
before sending it to servers, where it is typically decompressed for training
and analysis. In this work, we propose the use of a learning-based compression
Codec to reduce the overhead in latency incurred for the decompression
operation in the standard pipeline. We demonstrate that the learned compressed
representation can also be used to perform tasks like semantic segmentation in
addition to decompression to obtain the images. We experimentally validate the
proposed pipeline on the Cityscapes dataset, where we achieve a compression
factor up to $66 \times$ while preserving the information required to perform
segmentation with a dice coefficient of $0.84$ as compared to $0.88$ achieved
using decompressed images while reducing the overall compute by $11\%$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kakaiya_R/0/1/0/all/0/1&quot;&gt;Ravi Kakaiya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sathish_R/0/1/0/all/0/1&quot;&gt;Rakshith Sathish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sethuraman_R/0/1/0/all/0/1&quot;&gt;Ramanathan Sethuraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheet_D/0/1/0/all/0/1&quot;&gt;Debdoot Sheet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09715">
<title>Semantic-Aware Dual Contrastive Learning for Multi-label Image Classification. (arXiv:2307.09715v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09715</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracting image semantics effectively and assigning corresponding labels to
multiple objects or attributes for natural images is challenging due to the
complex scene contents and confusing label dependencies. Recent works have
focused on modeling label relationships with graph and understanding object
regions using class activation maps (CAM). However, these methods ignore the
complex intra- and inter-category relationships among specific semantic
features, and CAM is prone to generate noisy information. To this end, we
propose a novel semantic-aware dual contrastive learning framework that
incorporates sample-to-sample contrastive learning (SSCL) as well as
prototype-to-sample contrastive learning (PSCL). Specifically, we leverage
semantic-aware representation learning to extract category-related local
discriminative features and construct category prototypes. Then based on SSCL,
label-level visual representations of the same category are aggregated
together, and features belonging to distinct categories are separated.
Meanwhile, we construct a novel PSCL module to narrow the distance between
positive samples and category prototypes and push negative samples away from
the corresponding category prototypes. Finally, the discriminative label-level
features related to the image content are accurately captured by the joint
training of the above three parts. Experiments on five challenging large-scale
public datasets demonstrate that our proposed method is effective and
outperforms the state-of-the-art methods. Code and supplementary materials are
released on https://github.com/yu-gi-oh-leilei/SADCL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Leilei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1&quot;&gt;Dengdi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haifeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Bin Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10249">
<title>RCM-Fusion: Radar-Camera Multi-Level Fusion for 3D Object Detection. (arXiv:2307.10249v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10249</link>
<description rdf:parseType="Literal">&lt;p&gt;While LiDAR sensors have been succesfully applied to 3D object detection, the
affordability of radar and camera sensors has led to a growing interest in
fusiong radars and cameras for 3D object detection. However, previous
radar-camera fusion models have not been able to fully utilize radar
information in that initial 3D proposals were generated based on the camera
features only and the instance-level fusion is subsequently conducted. In this
paper, we propose radar-camera multi-level fusion (RCM-Fusion), which fuses
radar and camera modalities at both the feature-level and instance-level to
fully utilize radar information. At the feature-level, we propose a Radar
Guided BEV Encoder which utilizes radar Bird&apos;s-Eye-View (BEV) features to
transform image features into precise BEV representations and then adaptively
combines the radar and camera BEV features. At the instance-level, we propose a
Radar Grid Point Refinement module that reduces localization error by
considering the characteristics of the radar point clouds. The experiments
conducted on the public nuScenes dataset demonstrate that our proposed
RCM-Fusion offers 11.8% performance gain in nuScenes detection score (NDS) over
the camera-only baseline model and achieves state-of-the-art performaces among
radar-camera fusion methods in the nuScenes 3D object detection benchmark. Code
will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jisong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seong_M/0/1/0/all/0/1&quot;&gt;Minjae Seong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bang_G/0/1/0/all/0/1&quot;&gt;Geonho Bang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kum_D/0/1/0/all/0/1&quot;&gt;Dongsuk Kum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jun Won Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10705">
<title>TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars. (arXiv:2307.10705v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10705</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation is a common task in autonomous driving to understand
the surrounding environment. Driveable Area Segmentation and Lane Detection are
particularly important for safe and efficient navigation on the road. However,
original semantic segmentation models are computationally expensive and require
high-end hardware, which is not feasible for embedded systems in autonomous
vehicles. This paper proposes a lightweight model for the driveable area and
lane line segmentation. TwinLiteNet is designed cheaply but achieves accurate
and efficient segmentation results. We evaluate TwinLiteNet on the BDD100K
dataset and compare it with modern models. Experimental results show that our
TwinLiteNet performs similarly to existing approaches, requiring significantly
fewer computational resources. Specifically, TwinLiteNet achieves a mIoU score
of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task
with only 0.4 million parameters and achieves 415 FPS on GPU RTX A5000.
Furthermore, TwinLiteNet can run in real-time on embedded devices with limited
computing power, especially since it achieves 60FPS on Jetson Xavier NX, making
it an ideal solution for self-driving vehicles. Code is available:
url{https://github.com/chequanghuy/TwinLiteNet}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_Q/0/1/0/all/0/1&quot;&gt;Quang Huy Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Dinh Phuc Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1&quot;&gt;Minh Quan Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_D/0/1/0/all/0/1&quot;&gt;Duc Khai Lam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10853">
<title>Exploring Effective Priors and Efficient Models for Weakly-Supervised Change Detection. (arXiv:2307.10853v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10853</link>
<description rdf:parseType="Literal">&lt;p&gt;Weakly-supervised change detection (WSCD) aims to detect pixel-level changes
with only image-level annotations. Owing to its label efficiency, WSCD is
drawing increasing attention recently. However, current WSCD methods often
encounter the challenge of change missing and fabricating, i.e., the
inconsistency between image-level annotations and pixel-level predictions.
Specifically, change missing refer to the situation that the WSCD model fails
to predict any changed pixels, even though the image-level label indicates
changed, and vice versa for change fabricating. To address this challenge, in
this work, we leverage global-scale and local-scale priors in WSCD and propose
two components: a Dilated Prior (DP) decoder and a Label Gated (LG) constraint.
The DP decoder decodes samples with the changed image-level label, skips
samples with the unchanged label, and replaces them with an all-unchanged
pixel-level label. The LG constraint is derived from the correspondence between
changed representations and image-level labels, penalizing the model when it
mispredicts the change status. Additionally, we develop TransWCD, a simple yet
powerful transformer-based model, showcasing the potential of weakly-supervised
learning in change detection. By integrating the DP decoder and LG constraint
into TransWCD, we form TransWCD-DL. Our proposed TransWCD and TransWCD-DL
achieve significant +6.33% and +9.55% F1 score improvements over the
state-of-the-art methods on the WHU-CD dataset, respectively. Some performance
metrics even exceed several fully-supervised change detection (FSCD)
competitors. Code will be available at
https://github.com/zhenghuizhao/TransWCD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhenghui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ru_L/0/1/0/all/0/1&quot;&gt;Lixiang Ru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chen Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11411">
<title>Deep Directly-Trained Spiking Neural Networks for Object Detection. (arXiv:2307.11411v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.11411</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking neural networks (SNNs) are brain-inspired energy-efficient models
that encode information in spatiotemporal dynamics. Recently, deep SNNs trained
directly have shown great success in achieving high performance on
classification tasks with very few time steps. However, how to design a
directly-trained SNN for the regression task of object detection still remains
a challenging problem. To address this problem, we propose EMS-YOLO, a novel
directly-trained SNN framework for object detection, which is the first trial
to train a deep SNN with surrogate gradients for object detection rather than
ANN-SNN conversion strategies. Specifically, we design a full-spike residual
block, EMS-ResNet, which can effectively extend the depth of the
directly-trained SNN with low power consumption. Furthermore, we theoretically
analyze and prove the EMS-ResNet could avoid gradient vanishing or exploding.
The results demonstrate that our approach outperforms the state-of-the-art
ANN-SNN conversion methods (at least 500 time steps) in extremely fewer time
steps (only 4 time steps). It is shown that our model could achieve comparable
performance to the ANN with the same architecture while consuming 5.83 times
less energy on the frame-based COCO Dataset and the event-based Gen1 Dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1&quot;&gt;Qiaoyi Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1&quot;&gt;Yuhong Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yifan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1&quot;&gt;Shijie Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Ziyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guoqi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11934">
<title>LAMP: Leveraging Language Prompts for Multi-person Pose Estimation. (arXiv:2307.11934v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.11934</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-centric visual understanding is an important desideratum for effective
human-robot interaction. In order to navigate crowded public places, social
robots must be able to interpret the activity of the surrounding humans. This
paper addresses one key aspect of human-centric visual understanding,
multi-person pose estimation. Achieving good performance on multi-person pose
estimation in crowded scenes is difficult due to the challenges of occluded
joints and instance separation. In order to tackle these challenges and
overcome the limitations of image features in representing invisible body
parts, we propose a novel prompt-based pose inference strategy called LAMP
(Language Assisted Multi-person Pose estimation). By utilizing the text
representations generated by a well-trained language model (CLIP), LAMP can
facilitate the understanding of poses on the instance and joint levels, and
learn more robust visual representations that are less susceptible to
occlusion. This paper demonstrates that language-supervised training boosts the
performance of single-stage multi-person pose estimation, and both
instance-level and joint-level prompts are valuable for training. The code is
available at https://github.com/shengnanh20/LAMP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shengnan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Ce Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zixiang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sukthankar_G/0/1/0/all/0/1&quot;&gt;Gita Sukthankar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13717">
<title>A Comprehensive Analysis on the Leakage of Fuzzy Matchers. (arXiv:2307.13717v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13717</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides a comprehensive analysis of information leakage during
distance evaluation, with an emphasis on threshold-based obfuscated distance
(i.e., Fuzzy Matcher). Leakage can occur due to a malware infection or the use
of a weakly privacy-preserving matcher, exemplified by side channel attacks or
partially obfuscated designs. We provide an exhaustive catalog of information
leakage scenarios as well as their impacts on the security concerning data
privacy. Each of the scenarios leads to generic attacks whose impacts are
expressed in terms of computational costs, hence allowing the establishment of
upper bounds on the security level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durbet_A/0/1/0/all/0/1&quot;&gt;Axel Durbet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grollemund_P/0/1/0/all/0/1&quot;&gt;Paul-Marie Grollemund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiry_Atighehchi_K/0/1/0/all/0/1&quot;&gt;Kevin Thiry-Atighehchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13925">
<title>EasyNet: An Easy Network for 3D Industrial Anomaly Detection. (arXiv:2307.13925v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13925</link>
<description rdf:parseType="Literal">&lt;p&gt;3D anomaly detection is an emerging and vital computer vision task in
industrial manufacturing (IM). Recently many advanced algorithms have been
published, but most of them cannot meet the needs of IM. There are several
disadvantages: i) difficult to deploy on production lines since their
algorithms heavily rely on large pre-trained models; ii) hugely increase
storage overhead due to overuse of memory banks; iii) the inference speed
cannot be achieved in real-time. To overcome these issues, we propose an easy
and deployment-friendly network (called EasyNet) without using pre-trained
models and memory banks: firstly, we design a multi-scale multi-modality
feature encoder-decoder to accurately reconstruct the segmentation maps of
anomalous regions and encourage the interaction between RGB images and depth
images; secondly, we adopt a multi-modality anomaly segmentation network to
achieve a precise anomaly map; thirdly, we propose an attention-based
information entropy fusion module for feature fusion during inference, making
it suitable for real-time deployment. Extensive experiments show that EasyNet
achieves an anomaly detection AUROC of 92.6% without using pre-trained models
and memory banks. In addition, EasyNet is faster than existing methods, with a
high frame rate of 94.55 FPS on a Tesla V100 GPU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ruitao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1&quot;&gt;Guoyang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinbao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Ziqi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinfan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1&quot;&gt;Feng Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13957">
<title>Heterogeneous Embodied Multi-Agent Collaboration. (arXiv:2307.13957v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13957</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-agent embodied tasks have recently been studied in complex indoor
visual environments. Collaboration among multiple agents can improve work
efficiency and has significant practical value. However, most of the existing
research focuses on homogeneous multi-agent tasks. Compared with homogeneous
agents, heterogeneous agents can leverage their different capabilities to
allocate corresponding sub-tasks and cooperate to complete complex tasks.
Heterogeneous multi-agent tasks are common in real-world scenarios, and the
collaboration strategy among heterogeneous agents is a challenging and
important problem to be solved. To study collaboration among heterogeneous
agents, we propose the heterogeneous multi-agent tidying-up task, in which
multiple heterogeneous agents with different capabilities collaborate with each
other to detect misplaced objects and place them in reasonable locations. This
is a demanding task since it requires agents to make the best use of their
different capabilities to conduct reasonable task planning and complete the
whole task. To solve this task, we build a heterogeneous multi-agent tidying-up
benchmark dataset in a large number of houses with multiple rooms based on
ProcTHOR-10K. We propose the hierarchical decision model based on misplaced
object detection, reasonable receptacle prediction, as well as the
handshake-based group communication mechanism. Extensive experiments are
conducted to demonstrate the effectiveness of the proposed model. The project&apos;s
website and videos of experiments can be found at https://hetercol.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinzhu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1&quot;&gt;Di Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huaping Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14016">
<title>RPG-Palm: Realistic Pseudo-data Generation for Palmprint Recognition. (arXiv:2307.14016v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14016</link>
<description rdf:parseType="Literal">&lt;p&gt;Palmprint recently shows great potential in recognition applications as it is
a privacy-friendly and stable biometric. However, the lack of large-scale
public palmprint datasets limits further research and development of palmprint
recognition. In this paper, we propose a novel realistic pseudo-palmprint
generation (RPG) model to synthesize palmprints with massive identities. We
first introduce a conditional modulation generator to improve the intra-class
diversity. Then an identity-aware loss is proposed to ensure identity
consistency against unpaired training. We further improve the B\&apos;ezier palm
creases generation strategy to guarantee identity independence. Extensive
experimental results demonstrate that synthetic pretraining significantly
boosts the recognition model performance. For example, our model improves the
state-of-the-art B\&apos;ezierPalm by more than $5\%$ and $14\%$ in terms of
TAR@FAR=1e-6 under the $1:1$ and $1:3$ Open-set protocol. When accessing only
$10\%$ of the real training data, our method still outperforms ArcFace with
$100\%$ real training data, indicating that we are closer to real-data-free
palmprint recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Lei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jianlong Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruixin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huaen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Kai Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yingyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingyun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1&quot;&gt;Shouhong Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1&quot;&gt;Wei Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14051">
<title>3D Semantic Subspace Traverser: Empowering 3D Generative Model with Shape Editing Capability. (arXiv:2307.14051v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14051</link>
<description rdf:parseType="Literal">&lt;p&gt;Shape generation is the practice of producing 3D shapes as various
representations for 3D content creation. Previous studies on 3D shape
generation have focused on shape quality and structure, without or less
considering the importance of semantic information. Consequently, such
generative models often fail to preserve the semantic consistency of shape
structure or enable manipulation of the semantic attributes of shapes during
generation. In this paper, we proposed a novel semantic generative model named
3D Semantic Subspace Traverser that utilizes semantic attributes for
category-specific 3D shape generation and editing. Our method utilizes implicit
functions as the 3D shape representation and combines a novel latent-space GAN
with a linear subspace model to discover semantic dimensions in the local
latent space of 3D shapes. Each dimension of the subspace corresponds to a
particular semantic attribute, and we can edit the attributes of generated
shapes by traversing the coefficients of those dimensions. Experimental results
demonstrate that our method can produce plausible shapes with complex
structures and enable the editing of semantic attributes. The code and trained
models are available at
https://github.com/TrepangCat/3D_Semantic_Subspace_Traverser
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_P/0/1/0/all/0/1&quot;&gt;Pei Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qijun Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14066">
<title>Pre-Training with Diffusion models for Dental Radiography segmentation. (arXiv:2307.14066v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14066</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical radiography segmentation, and specifically dental radiography, is
highly limited by the cost of labeling which requires specific expertise and
labor-intensive annotations. In this work, we propose a straightforward
pre-training method for semantic segmentation leveraging Denoising Diffusion
Probabilistic Models (DDPM), which have shown impressive results for generative
modeling. Our straightforward approach achieves remarkable performance in terms
of label efficiency and does not require architectural modifications between
pre-training and downstream tasks. We propose to first pre-train a Unet by
exploiting the DDPM training objective, and then fine-tune the resulting model
on a segmentation task. Our experimental results on the segmentation of dental
radiographs demonstrate that the proposed method is competitive with
state-of-the-art pre-training methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rousseau_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xe9;my Rousseau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alaka_C/0/1/0/all/0/1&quot;&gt;Christian Alaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Covili_E/0/1/0/all/0/1&quot;&gt;Emma Covili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mayard_H/0/1/0/all/0/1&quot;&gt;Hippolyte Mayard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misrachi_L/0/1/0/all/0/1&quot;&gt;Laura Misrachi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Au_W/0/1/0/all/0/1&quot;&gt;Willy Au&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14127">
<title>Creative Birds: Self-Supervised Single-View 3D Style Transfer. (arXiv:2307.14127v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14127</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel method for single-view 3D style transfer
that generates a unique 3D object with both shape and texture transfer. Our
focus lies primarily on birds, a popular subject in 3D reconstruction, for
which no existing single-view 3D transfer methods have been developed.The
method we propose seeks to generate a 3D mesh shape and texture of a bird from
two single-view images. To achieve this, we introduce a novel shape transfer
generator that comprises a dual residual gated network (DRGNet), and a
multi-layer perceptron (MLP). DRGNet extracts the features of source and target
images using a shared coordinate gate unit, while the MLP generates spatial
coordinates for building a 3D mesh. We also introduce a semantic UV texture
transfer module that implements textural style transfer using semantic UV
segmentation, which ensures consistency in the semantic meaning of the
transferred regions. This module can be widely adapted to many existing
approaches. Finally, our method constructs a novel 3D bird using a
differentiable renderer. Experimental results on the CUB dataset verify that
our method achieves state-of-the-art performance on the single-view 3D style
transfer task. Code is available in https://github.com/wrk226/creative_birds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Renke Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Que_G/0/1/0/all/0/1&quot;&gt;Guimin Que&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>