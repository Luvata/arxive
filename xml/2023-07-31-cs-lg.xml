<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-30T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15073" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15076" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15088" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15089" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15098" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15099" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15105" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15150" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15164" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15175" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15196" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15199" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15285" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15299" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15326" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15339" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15367" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15377" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15396" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15398" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15428" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15429" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15438" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15456" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15546" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15557" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15593" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15621" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15625" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15638" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15644" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15645" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15647" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15663" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15678" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15690" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15710" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15712" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2104.00641" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.08221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.02419" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.01696" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.12672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.05606" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.07734" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.09189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.10043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.14272" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.05304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.10890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.04965" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13289" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10839" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.04253" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.08164" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.12717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.09394" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10909" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12202" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01170" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.00818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06987" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05966" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11164" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00607" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05965" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01995" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11465" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13124" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13692" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14935" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14643" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.15067">
<title>Set-Membership Inference Attacks using Data Watermarking. (arXiv:2307.15067v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15067</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a set-membership inference attack for generative
models using deep image watermarking techniques. In particular, we demonstrate
how conditional sampling from a generative model can reveal the watermark that
was injected into parts of the training data. Our empirical results demonstrate
that the proposed watermarking technique is a principled approach for detecting
the non-consensual use of image data in training generative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laszkiewicz_M/0/1/0/all/0/1&quot;&gt;Mike Laszkiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lukovnikov_D/0/1/0/all/0/1&quot;&gt;Denis Lukovnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lederer_J/0/1/0/all/0/1&quot;&gt;Johannes Lederer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1&quot;&gt;Asja Fischer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15072">
<title>Detecting the Presence of COVID-19 Vaccination Hesitancy from South African Twitter Data Using Machine Learning. (arXiv:2307.15072v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.15072</link>
<description rdf:parseType="Literal">&lt;p&gt;Very few social media studies have been done on South African user-generated
content during the COVID-19 pandemic and even fewer using hand-labelling over
automated methods. Vaccination is a major tool in the fight against the
pandemic, but vaccine hesitancy jeopardizes any public health effort. In this
study, sentiment analysis on South African tweets related to vaccine hesitancy
was performed, with the aim of training AI-mediated classification models and
assessing their reliability in categorizing UGC. A dataset of 30000 tweets from
South Africa were extracted and hand-labelled into one of three sentiment
classes: positive, negative, neutral. The machine learning models used were
LSTM, bi-LSTM, SVM, BERT-base-cased and the RoBERTa-base models, whereby their
hyperparameters were carefully chosen and tuned using the WandB platform. We
used two different approaches when we pre-processed our data for comparison:
one was semantics-based, while the other was corpus-based. The pre-processing
of the tweets in our dataset was performed using both methods, respectively.
All models were found to have low F1-scores within a range of 45$\%$-55$\%$,
except for BERT and RoBERTa which both achieved significantly better measures
with overall F1-scores of 60$\%$ and 61$\%$, respectively. Topic modelling
using an LDA was performed on the miss-classified tweets of the RoBERTa model
to gain insight on how to further improve model accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perikli_N/0/1/0/all/0/1&quot;&gt;Nicholas Perikli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1&quot;&gt;Srimoy Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ogbuokiri_B/0/1/0/all/0/1&quot;&gt;Blessing Ogbuokiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nia_Z/0/1/0/all/0/1&quot;&gt;Zahra Movahedi Nia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lieberman_B/0/1/0/all/0/1&quot;&gt;Benjamin Lieberman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tripathi_N/0/1/0/all/0/1&quot;&gt;Nidhi Tripathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahbi_S/0/1/0/all/0/1&quot;&gt;Salah-Eddine Dahbi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stevenson_F/0/1/0/all/0/1&quot;&gt;Finn Stevenson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bragazzi_N/0/1/0/all/0/1&quot;&gt;Nicola Bragazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_J/0/1/0/all/0/1&quot;&gt;Jude Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mellado_B/0/1/0/all/0/1&quot;&gt;Bruce Mellado&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15073">
<title>Drug Discovery under Covariate Shift with Domain-Informed Prior Distributions over Functions. (arXiv:2307.15073v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2307.15073</link>
<description rdf:parseType="Literal">&lt;p&gt;Accelerating the discovery of novel and more effective therapeutics is an
important pharmaceutical problem in which deep learning is playing an
increasingly significant role. However, real-world drug discovery tasks are
often characterized by a scarcity of labeled data and significant covariate
shift$\unicode{x2013}\unicode{x2013}$a setting that poses a challenge to
standard deep learning methods. In this paper, we present Q-SAVI, a
probabilistic model able to address these challenges by encoding explicit prior
knowledge of the data-generating process into a prior distribution over
functions, presenting researchers with a transparent and probabilistically
principled way to encode data-driven modeling preferences. Building on a novel,
gold-standard bioactivity dataset that facilitates a meaningful comparison of
models in an extrapolative regime, we explore different approaches to induce
data shift and construct a challenging evaluation setup. We then demonstrate
that using Q-SAVI to integrate contextualized prior knowledge of drug-like
chemical space into the modeling process affords substantial gains in
predictive accuracy and calibration, outperforming a broad range of
state-of-the-art self-supervised pre-training and domain adaptation techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Klarner_L/0/1/0/all/0/1&quot;&gt;Leo Klarner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rudner_T/0/1/0/all/0/1&quot;&gt;Tim G. J. Rudner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Reutlinger_M/0/1/0/all/0/1&quot;&gt;Michael Reutlinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Schindler_T/0/1/0/all/0/1&quot;&gt;Torsten Schindler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Morris_G/0/1/0/all/0/1&quot;&gt;Garrett M. Morris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Deane_C/0/1/0/all/0/1&quot;&gt;Charlotte Deane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Teh_Y/0/1/0/all/0/1&quot;&gt;Yee Whye Teh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15074">
<title>ISAC-NET: Model-driven Deep Learning for Integrated Passive Sensing and Communication. (arXiv:2307.15074v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2307.15074</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in wireless communication with the enormous demands of
sensing ability have given rise to the integrated sensing and communication
(ISAC) technology, among which passive sensing plays an important role. The
main challenge of passive sensing is how to achieve high sensing performance in
the condition of communication demodulation errors. In this paper, we propose
an ISAC network (ISAC-NET) that combines passive sensing with communication
signal detection by using model-driven deep learning (DL). Dissimilar to
existing passive sensing algorithms that first demodulate the transmitted
symbols and then obtain passive sensing results from the demodulated symbols,
ISAC-NET obtains passive sensing results and communication demodulated symbols
simultaneously. Different from the data-driven DL method, we adopt the
block-by-block signal processing method that divides the ISAC-NET into the
passive sensing module, signal detection module and channel reconstruction
module. From the simulation results, ISAC-NET obtains better communication
performance than the traditional signal demodulation algorithm, which is close
to OAMP-Net2. Compared to the 2D-DFT algorithm, ISAC-NET demonstrates
significantly enhanced sensing performance. In summary, ISAC-NET is a promising
tool for passive sensing and communication in wireless communications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wangjun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1&quot;&gt;Dingyou Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhiqing Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Ping Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15076">
<title>Knowledge Graph Enhanced Intelligent Tutoring System Based on Exercise Representativeness and Informativeness. (arXiv:2307.15076v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.15076</link>
<description rdf:parseType="Literal">&lt;p&gt;Presently, knowledge graph-based recommendation algorithms have garnered
considerable attention among researchers. However, these algorithms solely
consider knowledge graphs with single relationships and do not effectively
model exercise-rich features, such as exercise representativeness and
informativeness. Consequently, this paper proposes a framework, namely the
Knowledge-Graph-Exercise Representativeness and Informativeness Framework, to
address these two issues. The framework consists of four intricate components
and a novel cognitive diagnosis model called the Neural Attentive cognitive
diagnosis model. These components encompass the informativeness component,
exercise representation component, knowledge importance component, and exercise
representativeness component. The informativeness component evaluates the
informational value of each question and identifies the candidate question set
that exhibits the highest exercise informativeness. Furthermore, the skill
embeddings are employed as input for the knowledge importance component. This
component transforms a one-dimensional knowledge graph into a multi-dimensional
one through four class relations and calculates skill importance weights based
on novelty and popularity. Subsequently, the exercise representativeness
component incorporates exercise weight knowledge coverage to select questions
from the candidate question set for the tested question set. Lastly, the
cognitive diagnosis model leverages exercise representation and skill
importance weights to predict student performance on the test set and estimate
their knowledge state. To evaluate the effectiveness of our selection strategy,
extensive experiments were conducted on two publicly available educational
datasets. The experimental results demonstrate that our framework can recommend
appropriate exercises to students, leading to improved student performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linqing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15084">
<title>Mathematical Modeling of BCG-based Bladder Cancer Treatment Using Socio-Demographics. (arXiv:2307.15084v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15084</link>
<description rdf:parseType="Literal">&lt;p&gt;Cancer is one of the most widespread diseases around the world with millions
of new patients each year. Bladder cancer is one of the most prevalent types of
cancer affecting all individuals alike with no obvious prototypical patient.
The current standard treatment for BC follows a routine weekly Bacillus
Calmette-Guerin (BCG) immunotherapy-based therapy protocol which is applied to
all patients alike. The clinical outcomes associated with BCG treatment vary
significantly among patients due to the biological and clinical complexity of
the interaction between the immune system, treatments, and cancer cells. In
this study, we take advantage of the patient&apos;s socio-demographics to offer a
personalized mathematical model that describes the clinical dynamics associated
with BCG-based treatment. To this end, we adopt a well-established BCG
treatment model and integrate a machine learning component to temporally adjust
and reconfigure key parameters within the model thus promoting its
personalization. Using real clinical data, we show that our personalized model
favorably compares with the original one in predicting the number of cancer
cells at the end of the treatment, with 14.8% improvement, on average.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savchenko_E/0/1/0/all/0/1&quot;&gt;Elizaveta Savchenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenfeld_A/0/1/0/all/0/1&quot;&gt;Ariel Rosenfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bunimovich_Mendrazitsky_S/0/1/0/all/0/1&quot;&gt;Svetlana Bunimovich-Mendrazitsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15088">
<title>Equitable Time-Varying Pricing Tariff Design: A Joint Learning and Optimization Approach. (arXiv:2307.15088v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15088</link>
<description rdf:parseType="Literal">&lt;p&gt;Time-varying pricing tariffs incentivize consumers to shift their electricity
demand and reduce costs, but may increase the energy burden for consumers with
limited response capability. The utility must thus balance affordability and
response incentives when designing these tariffs by considering consumers&apos;
response expectations. This paper proposes a joint learning-based
identification and optimization method to design equitable time-varying
tariffs. Our proposed method encodes historical prices and demand response data
into a recurrent neural network (RNN) to capture high-dimensional and
non-linear consumer price response behaviors. We then embed the RNN into the
tariff design optimization, formulating a non-linear optimization problem with
a quadratic objective. We propose a gradient-based solution method that
achieves fast and scalable computation. Simulation using real-world consumer
data shows that our equitable tariffs protect low-income consumers from price
surges while effectively motivating consumers to reduce peak demand. The method
also ensures revenue recovery for the utility company and achieves robust
performance against demand response uncertainties and prediction errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liudong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Bolun Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15089">
<title>Information Gained Subgroup Discovery in Datasets. (arXiv:2307.15089v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15089</link>
<description rdf:parseType="Literal">&lt;p&gt;Lung cancer is the leading cause of cancer death. More than 238,340 new cases
of lung cancer patients are expected in 2023, with an estimation of more than
127,070 deaths. Choosing the correct treatment is an important element to
enhance the probability of survival and to improve patient&apos;s quality of life.
Cancer treatments might provoke secondary effects. These toxicities cause
different health problems that impact the patient&apos;s quality of life. Hence,
reducing treatments toxicities while maintaining or improving their
effectivenes is an important goal that aims to be pursued from the clinical
perspective. On the other hand, clinical guidelines include general knowledge
about cancer treatment recommendations to assist clinicians. Although they
provide treatment recommendations based on cancer disease aspects and
individual patient features, a statistical analysis taking into account
treatment outcomes is not provided here. Therefore, the comparison between
clinical guidelines with treatment patterns found in clinical data, would allow
to validate the patterns found, as well as discovering alternative treatment
patterns. In this work, we present Information Gained Subgroup Discovery, a
Subgroup Discovery algorithm that aims to find most relevant patterns taking
into account Information gain and Odds ratio. Thus, we analyze a dataset
containing lung cancer patients information including patients&apos; data,
prescribed treatments and their outcomes. Obtained results are validated
through clinicians and compared with clinical guidelines. We conclude that this
new algorithm achieves highest acceptance of found patterns in this dataset,
while also improving indices of Subgroup Discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_Bravo_D/0/1/0/all/0/1&quot;&gt;Daniel G&amp;#xf3;mez-Bravo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_A/0/1/0/all/0/1&quot;&gt;Aaron Garc&amp;#xed;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vigueras_G/0/1/0/all/0/1&quot;&gt;Guillermo Vigueras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rios_B/0/1/0/all/0/1&quot;&gt;Bel&amp;#xe9;n R&amp;#xed;os&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Provencio_M/0/1/0/all/0/1&quot;&gt;Mariano Provencio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_Gonzalez_A/0/1/0/all/0/1&quot;&gt;Alejandro Rodr&amp;#xed;guez-Gonz&amp;#xe1;lez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15090">
<title>Understanding Forward Process of Convolutional Neural Network. (arXiv:2307.15090v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15090</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper reveal the selective rotation in the CNNs&apos; forward processing. It
elucidates the activation function as a discerning mechanism that unifies and
quantizes the rotational aspects of the input data. Experiments show how this
defined methodology reflects the progress network distinguish inputs based on
statistical indicators, which can be comprehended or analyzed by applying
structured mathematical tools. Our findings also unveil the consistency between
artificial neural networks and the human brain in their data processing
pattern.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_P/0/1/0/all/0/1&quot;&gt;Peixin Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15092">
<title>A Survey on Reservoir Computing and its Interdisciplinary Applications Beyond Traditional Machine Learning. (arXiv:2307.15092v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2307.15092</link>
<description rdf:parseType="Literal">&lt;p&gt;Reservoir computing (RC), first applied to temporal signal processing, is a
recurrent neural network in which neurons are randomly connected. Once
initialized, the connection strengths remain unchanged. Such a simple structure
turns RC into a non-linear dynamical system that maps low-dimensional inputs
into a high-dimensional space. The model&apos;s rich dynamics, linear separability,
and memory capacity then enable a simple linear readout to generate adequate
responses for various applications. RC spans areas far beyond machine learning,
since it has been shown that the complex dynamics can be realized in various
physical hardware implementations and biological devices. This yields greater
flexibility and shorter computation time. Moreover, the neuronal responses
triggered by the model&apos;s dynamics shed light on understanding brain mechanisms
that also exploit similar dynamical processes. While the literature on RC is
vast and fragmented, here we conduct a unified review of RC&apos;s recent
developments from machine learning to physics, biology, and neuroscience. We
first review the early RC models, and then survey the state-of-the-art models
and their applications. We further introduce studies on modeling the brain&apos;s
mechanisms by RC. Finally, we offer new perspectives on RC development,
including reservoir design, coding frameworks unification, physical RC
implementations, and interaction between RC, cognitive neuroscience and
evolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Heng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vargas_D/0/1/0/all/0/1&quot;&gt;Danilo Vasconcellos Vargas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15097">
<title>Cascaded Cross-Modal Transformer for Request and Complaint Detection. (arXiv:2307.15097v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.15097</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel cascaded cross-modal transformer (CCMT) that combines
speech and text transcripts to detect customer requests and complaints in phone
conversations. Our approach leverages a multimodal paradigm by transcribing the
speech using automatic speech recognition (ASR) models and translating the
transcripts into different languages. Subsequently, we combine
language-specific BERT-based models with Wav2Vec2.0 audio features in a novel
cascaded cross-attention transformer model. We apply our system to the Requests
Sub-Challenge of the ACM Multimedia 2023 Computational Paralinguistics
Challenge, reaching unweighted average recalls (UAR) of 65.41% and 85.87% for
the complaint and request classes, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1&quot;&gt;Nicolae-Catalin Ristea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1&quot;&gt;Radu Tudor Ionescu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15098">
<title>Self-Supervised Learning for Improved Synthetic Aperture Sonar Target Recognition. (arXiv:2307.15098v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15098</link>
<description rdf:parseType="Literal">&lt;p&gt;This study explores the application of self-supervised learning (SSL) for
improved target recognition in synthetic aperture sonar (SAS) imagery. The
unique challenges of underwater environments make traditional computer vision
techniques, which rely heavily on optical camera imagery, less effective. SAS,
with its ability to generate high-resolution imagery, emerges as a preferred
choice for underwater imaging. However, the voluminous high-resolution SAS data
presents a significant challenge for labeling; a crucial step for training deep
neural networks (DNNs).
&lt;/p&gt;
&lt;p&gt;SSL, which enables models to learn features in data without the need for
labels, is proposed as a potential solution to the data labeling challenge in
SAS. The study evaluates the performance of two prominent SSL algorithms,
MoCov2 and BYOL, against the well-regarded supervised learning model, ResNet18,
for binary image classification tasks. The findings suggest that while both SSL
models can outperform a fully supervised model with access to a small number of
labels in a few-shot scenario, they do not exceed it when all the labels are
used.
&lt;/p&gt;
&lt;p&gt;The results underscore the potential of SSL as a viable alternative to
traditional supervised learning, capable of maintaining task performance while
reducing the time and costs associated with data labeling. The study also
contributes to the growing body of evidence supporting the use of SSL in remote
sensing and could stimulate further research in this area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheffield_B/0/1/0/all/0/1&quot;&gt;BW Sheffield&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15099">
<title>Clustering of illustrations by atmosphere using a combination of supervised and unsupervised learning. (arXiv:2307.15099v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15099</link>
<description rdf:parseType="Literal">&lt;p&gt;The distribution of illustrations on social media, such as Twitter and Pixiv
has increased with the growing popularity of animation, games, and animated
movies. The &quot;atmosphere&quot; of illustrations plays an important role in user
preferences. Classifying illustrations by atmosphere can be helpful for
recommendations and searches. However, assigning clear labels to the elusive
&quot;atmosphere&quot; and conventional supervised classification is not always
practical. Furthermore, even images with similar colors, edges, and low-level
features may not have similar atmospheres, making classification based on
low-level features challenging. In this paper, this problem is solved using
both supervised and unsupervised learning with pseudo-labels. The feature
vectors are obtained using the supervised method with pseudo-labels that
contribute to an ambiguous atmosphere. Further, clustering is performed based
on these feature vectors. Experimental analyses show that our method
outperforms conventional methods in human-like clustering on datasets manually
classified by humans.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kubota_K/0/1/0/all/0/1&quot;&gt;Keisuke Kubota&lt;/a&gt; (Doshisha University), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okuda_M/0/1/0/all/0/1&quot;&gt;Masahiro Okuda&lt;/a&gt; (Doshisha University)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15101">
<title>Detection of Children Abuse by Voice and Audio Classification by Short-Time Fourier Transform Machine Learning implemented on Nvidia Edge GPU device. (arXiv:2307.15101v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2307.15101</link>
<description rdf:parseType="Literal">&lt;p&gt;The safety of children in children home has become an increasing social
concern, and the purpose of this experiment is to use machine learning applied
to detect the scenarios of child abuse to increase the safety of children. This
experiment uses machine learning to classify and recognize a child&apos;s voice and
predict whether the current sound made by the child is crying, screaming or
laughing. If a child is found to be crying or screaming, an alert is
immediately sent to the relevant personnel so that they can perceive what the
child may be experiencing in a surveillance blind spot and respond in a timely
manner. Together with a hybrid use of video image classification, the accuracy
of child abuse detection can be significantly increased. This greatly reduces
the likelihood that a child will receive violent abuse in the nursery and
allows personnel to stop an imminent or incipient child abuse incident in time.
The datasets collected from this experiment is entirely from sounds recorded on
site at the children home, including crying, laughing, screaming sound and
background noises. These sound files are transformed into spectrograms using
Short-Time Fourier Transform, and then these image data are imported into a CNN
neural network for classification, and the final trained model can achieve an
accuracy of about 92% for sound detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Jiuqi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yingxian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fok_W/0/1/0/all/0/1&quot;&gt;W.W.T.Fok&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15105">
<title>Detecting Morphing Attacks via Continual Incremental Training. (arXiv:2307.15105v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15105</link>
<description rdf:parseType="Literal">&lt;p&gt;Scenarios in which restrictions in data transfer and storage limit the
possibility to compose a single dataset -- also exploiting different data
sources -- to perform a batch-based training procedure, make the development of
robust models particularly challenging. We hypothesize that the recent
Continual Learning (CL) paradigm may represent an effective solution to enable
incremental training, even through multiple sites. Indeed, a basic assumption
of CL is that once a model has been trained, old data can no longer be used in
successive training iterations and in principle can be deleted. Therefore, in
this paper, we investigate the performance of different Continual Learning
methods in this scenario, simulating a learning model that is updated every
time a new chunk of data, even of variable size, is available. Experimental
results reveal that a particular CL method, namely Learning without Forgetting
(LwF), is one of the best-performing algorithms. Then, we investigate its usage
and parametrization in Morphing Attack Detection and Object Classification
tasks, specifically with respect to the amount of new training data that became
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pellegrini_L/0/1/0/all/0/1&quot;&gt;Lorenzo Pellegrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borghi_G/0/1/0/all/0/1&quot;&gt;Guido Borghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franco_A/0/1/0/all/0/1&quot;&gt;Annalisa Franco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maltoni_D/0/1/0/all/0/1&quot;&gt;Davide Maltoni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15150">
<title>R-Block: Regularized Block of Dropout for convolutional networks. (arXiv:2307.15150v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15150</link>
<description rdf:parseType="Literal">&lt;p&gt;Dropout as a regularization technique is widely used in fully connected
layers while is less effective in convolutional layers. Therefore more
structured forms of dropout have been proposed to regularize convolutional
networks. The disadvantage of these methods is that the randomness introduced
causes inconsistency between training and inference. In this paper, we apply a
mutual learning training strategy for convolutional layer regularization,
namely R-Block, which forces two outputs of the generated difference maximizing
sub models to be consistent with each other. Concretely, R-Block minimizes the
losses between the output distributions of two sub models with different drop
regions for each sample in the training dataset. We design two approaches to
construct such sub models. Our experiments demonstrate that R-Block achieves
better performance than other existing structured dropout variants. We also
demonstrate that our approaches to construct sub models outperforms others.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1&quot;&gt;Qiya Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15154">
<title>A/B Testing and Best-arm Identification for Linear Bandits with Robustness to Non-stationarity. (arXiv:2307.15154v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15154</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the fixed-budget best-arm identification (BAI) problem for
linear bandits in a potentially non-stationary environment. Given a finite arm
set $\mathcal{X}\subset\mathbb{R}^d$, a fixed budget $T$, and an unpredictable
sequence of parameters $\left\lbrace\theta_t\right\rbrace_{t=1}^{T}$, an
algorithm will aim to correctly identify the best arm $x^* :=
\arg\max_{x\in\mathcal{X}}x^\top\sum_{t=1}^{T}\theta_t$ with probability as
high as possible. Prior work has addressed the stationary setting where
$\theta_t = \theta_1$ for all $t$ and demonstrated that the error probability
decreases as $\exp(-T /\rho^*)$ for a problem-dependent constant $\rho^*$. But
in many real-world $A/B/n$ multivariate testing scenarios that motivate our
work, the environment is non-stationary and an algorithm expecting a stationary
setting can easily fail. For robust identification, it is well-known that if
arms are chosen randomly and non-adaptively from a G-optimal design over
$\mathcal{X}$ at each time then the error probability decreases as
$\exp(-T\Delta^2_{(1)}/d)$, where $\Delta_{(1)} = \min_{x \neq x^*} (x^* -
x)^\top \frac{1}{T}\sum_{t=1}^T \theta_t$. As there exist environments where
$\Delta_{(1)}^2/ d \ll 1/ \rho^*$, we are motivated to propose a novel
algorithm $\mathsf{P1}$-$\mathsf{RAGE}$ that aims to obtain the best of both
worlds: robustness to non-stationarity and fast rates of identification in
benign settings. We characterize the error probability of
$\mathsf{P1}$-$\mathsf{RAGE}$ and demonstrate empirically that the algorithm
indeed never performs worse than G-optimal design but compares favorably to the
best algorithms in the stationary setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zhihan Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camilleri_R/0/1/0/all/0/1&quot;&gt;Romain Camilleri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fazel_M/0/1/0/all/0/1&quot;&gt;Maryam Fazel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_L/0/1/0/all/0/1&quot;&gt;Lalit Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1&quot;&gt;Kevin Jamieson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15157">
<title>R-LPIPS: An Adversarially Robust Perceptual Similarity Metric. (arXiv:2307.15157v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15157</link>
<description rdf:parseType="Literal">&lt;p&gt;Similarity metrics have played a significant role in computer vision to
capture the underlying semantics of images. In recent years, advanced
similarity metrics, such as the Learned Perceptual Image Patch Similarity
(LPIPS), have emerged. These metrics leverage deep features extracted from
trained neural networks and have demonstrated a remarkable ability to closely
align with human perception when evaluating relative image similarity. However,
it is now well-known that neural networks are susceptible to adversarial
examples, i.e., small perturbations invisible to humans crafted to deliberately
mislead the model. Consequently, the LPIPS metric is also sensitive to such
adversarial examples. This susceptibility introduces significant security
concerns, especially considering the widespread adoption of LPIPS in
large-scale applications. In this paper, we propose the Robust Learned
Perceptual Image Patch Similarity (R-LPIPS) metric, a new metric that leverages
adversarially trained deep features. Through a comprehensive set of
experiments, we demonstrate the superiority of R-LPIPS compared to the
classical LPIPS metric. The code is available at
\url{https://github.com/SaraGhazanfari/R-LPIPS}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghazanfari_S/0/1/0/all/0/1&quot;&gt;Sara Ghazanfari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Siddharth Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamurthy_P/0/1/0/all/0/1&quot;&gt;Prashanth Krishnamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khorrami_F/0/1/0/all/0/1&quot;&gt;Farshad Khorrami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araujo_A/0/1/0/all/0/1&quot;&gt;Alexandre Araujo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15164">
<title>VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories Leveraging BERT and Stacked Embeddings. (arXiv:2307.15164v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.15164</link>
<description rdf:parseType="Literal">&lt;p&gt;Our system, VISU, participated in the WASSA 2023 Shared Task (3) of Emotion
Classification from essays written in reaction to news articles. Emotion
detection from complex dialogues is challenging and often requires
context/domain understanding. Therefore in this research, we have focused on
developing deep learning (DL) models using the combination of word embedding
representations with tailored prepossessing strategies to capture the nuances
of emotions expressed. Our experiments used static and contextual embeddings
(individual and stacked) with Bidirectional Long short-term memory (BiLSTM) and
Transformer based models. We occupied rank tenth in the emotion detection task
by scoring a Macro F1-Score of 0.2717, validating the efficacy of our
implemented approaches for small and imbalanced datasets with mixed categories
of target emotions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Vivek Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sushmita Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiwari_P/0/1/0/all/0/1&quot;&gt;Prayag Tiwari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15168">
<title>PredictChain: Empowering Collaboration and Data Accessibility for AI in a Decentralized Blockchain-based Marketplace. (arXiv:2307.15168v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15168</link>
<description rdf:parseType="Literal">&lt;p&gt;Limited access to computing resources and training data poses significant
challenges for individuals and groups aiming to train and utilize predictive
machine learning models. Although numerous publicly available machine learning
models exist, they are often unhosted, necessitating end-users to establish
their computational infrastructure. Alternatively, these models may only be
accessible through paid cloud-based mechanisms, which can prove costly for
general public utilization. Moreover, model and data providers require a more
streamlined approach to track resource usage and capitalize on subsequent usage
by others, both financially and otherwise. An effective mechanism is also
lacking to contribute high-quality data for improving model performance. We
propose a blockchain-based marketplace called &quot;PredictChain&quot; for predictive
machine-learning models to address these issues. This marketplace enables users
to upload datasets for training predictive machine learning models, request
model training on previously uploaded datasets, or submit queries to trained
models. Nodes within the blockchain network, equipped with available computing
resources, will operate these models, offering a range of archetype machine
learning models with varying characteristics, such as cost, speed, simplicity,
power, and cost-effectiveness. This decentralized approach empowers users to
develop improved models accessible to the public, promotes data sharing, and
reduces reliance on centralized cloud providers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pisano_M/0/1/0/all/0/1&quot;&gt;Matthew T. Pisano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patterson_C/0/1/0/all/0/1&quot;&gt;Connor J. Patterson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seneviratne_O/0/1/0/all/0/1&quot;&gt;Oshani Seneviratne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15175">
<title>Causative Cyberattacks on Online Learning-based Automated Demand Response Systems. (arXiv:2307.15175v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2307.15175</link>
<description rdf:parseType="Literal">&lt;p&gt;Power utilities are adopting Automated Demand Response (ADR) to replace the
costly fuel-fired generators and to preempt congestion during peak electricity
demand. Similarly, third-party Demand Response (DR) aggregators are leveraging
controllable small-scale electrical loads to provide on-demand grid support
services to the utilities. Some aggregators and utilities have started
employing Artificial Intelligence (AI) to learn the energy usage patterns of
electricity consumers and use this knowledge to design optimal DR incentives.
Such AI frameworks use open communication channels between the
utility/aggregator and the DR customers, which are vulnerable to
\textit{causative} data integrity cyberattacks. This paper explores
vulnerabilities of AI-based DR learning and designs a data-driven attack
strategy informed by DR data collected from the New York University (NYU)
campus buildings. The case study demonstrates the feasibility and effects of
maliciously tampering with (i) real-time DR incentives, (ii) DR event data sent
to DR customers, and (iii) responses of DR customers to the DR incentives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Acharya_S/0/1/0/all/0/1&quot;&gt;Samrat Acharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dvorkin_Y/0/1/0/all/0/1&quot;&gt;Yury Dvorkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Karri_R/0/1/0/all/0/1&quot;&gt;Ramesh Karri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15176">
<title>RCT Rejection Sampling for Causal Estimation Evaluation. (arXiv:2307.15176v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.15176</link>
<description rdf:parseType="Literal">&lt;p&gt;Confounding is a significant obstacle to unbiased estimation of causal
effects from observational data. For settings with high-dimensional covariates
-- such as text data, genomics, or the behavioral social sciences --
researchers have proposed methods to adjust for confounding by adapting machine
learning methods to the goal of causal estimation. However, empirical
evaluation of these adjustment methods has been challenging and limited. In
this work, we build on a promising empirical evaluation strategy that
simplifies evaluation design and uses real data: subsampling randomized
controlled trials (RCTs) to create confounded observational datasets while
using the average causal effects from the RCTs as ground-truth. We contribute a
new sampling algorithm, which we call RCT rejection sampling, and provide
theoretical guarantees that causal identification holds in the observational
data to allow for valid comparisons to the ground-truth RCT. Using synthetic
data, we show our algorithm indeed results in low bias when oracle estimators
are evaluated on the confounded samples, which is not always the case for a
previously proposed algorithm. In addition to this identification result, we
highlight several finite data considerations for evaluation designers who plan
to use RCT rejection sampling on their own datasets. As a proof of concept, we
implement an example evaluation pipeline and walk through these finite data
considerations with a novel, real-world RCT -- which we release publicly --
consisting of approximately 70k observations and text data as high-dimensional
covariates. Together, these contributions build towards a broader agenda of
improved empirical evaluation for causal estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keith_K/0/1/0/all/0/1&quot;&gt;Katherine A. Keith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1&quot;&gt;Sergey Feldman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1&quot;&gt;David Jurgens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1&quot;&gt;Jonathan Bragg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_R/0/1/0/all/0/1&quot;&gt;Rohit Bhattacharya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15190">
<title>f-Divergence Minimization for Sequence-Level Knowledge Distillation. (arXiv:2307.15190v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.15190</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge distillation (KD) is the process of transferring knowledge from a
large model to a small one. It has gained increasing attention in the natural
language processing community, driven by the demands of compressing
ever-growing language models. In this work, we propose an f-DISTILL framework,
which formulates sequence-level knowledge distillation as minimizing a
generalized f-divergence function. We propose four distilling variants under
our framework and show that existing SeqKD and ENGINE approaches are
approximations of our f-DISTILL methods. We further derive step-wise
decomposition for our f-DISTILL, reducing intractable sequence-level divergence
to word-level losses that can be computed in a tractable manner. Experiments
across four datasets show that our methods outperform existing KD approaches,
and that our symmetric distilling losses can better force the student to learn
from the teacher distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yuqiao Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zichao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1&quot;&gt;Wenyu Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1&quot;&gt;Lili Mou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15193">
<title>Learning in Repeated Multi-Unit Pay-As-Bid Auctions. (arXiv:2307.15193v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2307.15193</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by Carbon Emissions Trading Schemes, Treasury Auctions, and
Procurement Auctions, which all involve the auctioning of homogeneous multiple
units, we consider the problem of learning how to bid in repeated multi-unit
pay-as-bid auctions. In each of these auctions, a large number of (identical)
items are to be allocated to the largest submitted bids, where the price of
each of the winning bids is equal to the bid itself. The problem of learning
how to bid in pay-as-bid auctions is challenging due to the combinatorial
nature of the action space. We overcome this challenge by focusing on the
offline setting, where the bidder optimizes their vector of bids while only
having access to the past submitted bids by other bidders. We show that the
optimal solution to the offline problem can be obtained using a polynomial time
dynamic programming (DP) scheme. We leverage the structure of the DP scheme to
design online learning algorithms with polynomial time and space complexity
under full information and bandit feedback settings. We achieve an upper bound
on regret of $O(M\sqrt{T\log |\mathcal{B}|})$ and $O(M\sqrt{|\mathcal{B}|T\log
|\mathcal{B}|})$ respectively, where $M$ is the number of units demanded by the
bidder, $T$ is the total number of auctions, and $|\mathcal{B}|$ is the size of
the discretized bid space. We accompany these results with a regret lower
bound, which match the linear dependency in $M$. Our numerical results suggest
that when all agents behave according to our proposed no regret learning
algorithms, the resulting market dynamics mainly converge to a welfare
maximizing equilibrium where bidders submit uniform bids. Lastly, our
experiments demonstrate that the pay-as-bid auction consistently generates
significantly higher revenue compared to its popular alternative, the uniform
price auction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galgana_R/0/1/0/all/0/1&quot;&gt;Rigel Galgana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golrezaei_N/0/1/0/all/0/1&quot;&gt;Negin Golrezaei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15196">
<title>The Marginal Value of Momentum for Small Learning Rate SGD. (arXiv:2307.15196v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15196</link>
<description rdf:parseType="Literal">&lt;p&gt;Momentum is known to accelerate the convergence of gradient descent in
strongly convex settings without stochastic gradient noise. In stochastic
optimization, such as training neural networks, folklore suggests that momentum
may help deep learning optimization by reducing the variance of the stochastic
gradient update, but previous theoretical analyses do not find momentum to
offer any provable acceleration. Theoretical results in this paper clarify the
role of momentum in stochastic settings where the learning rate is small and
gradient noise is the dominant source of instability, suggesting that SGD with
and without momentum behave similarly in the short and long time horizons.
Experiments show that momentum indeed has limited benefits for both
optimization and generalization in practical training regimes where the optimal
learning rate is not very large, including small- to medium-batch training from
scratch on ImageNet and fine-tuning language models on downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Runzhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malladi_S/0/1/0/all/0/1&quot;&gt;Sadhika Malladi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_K/0/1/0/all/0/1&quot;&gt;Kaifeng Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15199">
<title>PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization. (arXiv:2307.15199v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15199</link>
<description rdf:parseType="Literal">&lt;p&gt;In a joint vision-language space, a text feature (e.g., from &quot;a photo of a
dog&quot;) could effectively represent its relevant image features (e.g., from dog
photos). Inspired by this, we propose PromptStyler which simulates various
distribution shifts in the joint space by synthesizing diverse styles via
prompts without using any images to deal with source-free domain
generalization. Our method learns to generate a variety of style features (from
&quot;a S* style of a&quot;) via learnable style word vectors for pseudo-words S*. To
ensure that learned styles do not distort content information, we force
style-content features (from &quot;a S* style of a [class]&quot;) to be located nearby
their corresponding content features (from &quot;[class]&quot;) in the joint
vision-language space. After learning style word vectors, we train a linear
classifier using synthesized style-content features. PromptStyler achieves the
state of the art on PACS, VLCS, OfficeHome and DomainNet, although it does not
require any images and takes just ~30 minutes for training using a single GPU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Junhyeong Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_G/0/1/0/all/0/1&quot;&gt;Gilhyun Nam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sungyeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hunmin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1&quot;&gt;Suha Kwak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15217">
<title>Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. (arXiv:2307.15217v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.15217</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning from human feedback (RLHF) is a technique for training
AI systems to align with human goals. RLHF has emerged as the central method
used to finetune state-of-the-art large language models (LLMs). Despite this
popularity, there has been relatively little public work systematizing its
flaws. In this paper, we (1) survey open problems and fundamental limitations
of RLHF and related methods; (2) overview techniques to understand, improve,
and complement RLHF in practice; and (3) propose auditing and disclosure
standards to improve societal oversight of RLHF systems. Our work emphasizes
the limitations of RLHF and highlights the importance of a multi-faceted
approach to the development of safer AI systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1&quot;&gt;Stephen Casper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davies_X/0/1/0/all/0/1&quot;&gt;Xander Davies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1&quot;&gt;Claudia Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilbert_T/0/1/0/all/0/1&quot;&gt;Thomas Krendl Gilbert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scheurer_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xe9;my Scheurer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rando_J/0/1/0/all/0/1&quot;&gt;Javier Rando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freedman_R/0/1/0/all/0/1&quot;&gt;Rachel Freedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1&quot;&gt;Tomasz Korbak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindner_D/0/1/0/all/0/1&quot;&gt;David Lindner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freire_P/0/1/0/all/0/1&quot;&gt;Pedro Freire&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tony Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marks_S/0/1/0/all/0/1&quot;&gt;Samuel Marks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Segerie_C/0/1/0/all/0/1&quot;&gt;Charbel-Rapha&amp;#xeb;l Segerie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carroll_M/0/1/0/all/0/1&quot;&gt;Micah Carroll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1&quot;&gt;Andi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christoffersen_P/0/1/0/all/0/1&quot;&gt;Phillip Christoffersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damani_M/0/1/0/all/0/1&quot;&gt;Mehul Damani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slocum_S/0/1/0/all/0/1&quot;&gt;Stewart Slocum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anwar_U/0/1/0/all/0/1&quot;&gt;Usman Anwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siththaranjan_A/0/1/0/all/0/1&quot;&gt;Anand Siththaranjan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nadeau_M/0/1/0/all/0/1&quot;&gt;Max Nadeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michaud_E/0/1/0/all/0/1&quot;&gt;Eric J. Michaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfau_J/0/1/0/all/0/1&quot;&gt;Jacob Pfau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krasheninnikov_D/0/1/0/all/0/1&quot;&gt;Dmitrii Krasheninnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langosco_L/0/1/0/all/0/1&quot;&gt;Lauro Langosco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1&quot;&gt;Peter Hase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biyik_E/0/1/0/all/0/1&quot;&gt;Erdem B&amp;#x131;y&amp;#x131;k&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1&quot;&gt;Anca Dragan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1&quot;&gt;David Krueger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1&quot;&gt;Dorsa Sadigh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1&quot;&gt;Dylan Hadfield-Menell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15245">
<title>A Practical Recipe for Federated Learning Under Statistical Heterogeneity Experimental Design. (arXiv:2307.15245v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15245</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) has been an area of active research in recent years.
There have been numerous studies in FL to make it more successful in the
presence of data heterogeneity. However, despite the existence of many
publications, the state of progress in the field is unknown. Many of the works
use inconsistent experimental settings and there are no comprehensive studies
on the effect of FL-specific experimental variables on the results and
practical insights for a more comparable and consistent FL experimental setup.
Furthermore, the existence of several benchmarks and confounding variables has
further complicated the issue of inconsistency and ambiguity. In this work, we
present the first comprehensive study on the effect of FL-specific experimental
variables in relation to each other and performance results, bringing several
insights and recommendations for designing a meaningful and well-incentivized
FL experimental setup. We further aid the community by releasing FedZoo-Bench,
an open-source library based on PyTorch with pre-implementation of 22
state-of-the-art methods, and a broad set of standardized and customizable
features available at https://github.com/MMorafah/FedZoo-Bench. We also provide
a comprehensive comparison of several state-of-the-art (SOTA) methods to better
understand the current state of the field and existing limitations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morafah_M/0/1/0/all/0/1&quot;&gt;Mahdi Morafah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weijia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bill Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15247">
<title>Is this model reliable for everyone? Testing for strong calibration. (arXiv:2307.15247v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15247</link>
<description rdf:parseType="Literal">&lt;p&gt;In a well-calibrated risk prediction model, the average predicted probability
is close to the true event rate for any given subgroup. Such models are
reliable across heterogeneous populations and satisfy strong notions of
algorithmic fairness. However, the task of auditing a model for strong
calibration is well-known to be difficult -- particularly for machine learning
(ML) algorithms -- due to the sheer number of potential subgroups. As such,
common practice is to only assess calibration with respect to a few predefined
subgroups. Recent developments in goodness-of-fit testing offer potential
solutions but are not designed for settings with weak signal or where the
poorly calibrated subgroup is small, as they either overly subdivide the data
or fail to divide the data at all. We introduce a new testing procedure based
on the following insight: if we can reorder observations by their expected
residuals, there should be a change in the association between the predicted
and observed residuals along this sequence if a poorly calibrated subgroup
exists. This lets us reframe the problem of calibration testing into one of
changepoint detection, for which powerful methods already exist. We begin with
introducing a sample-splitting procedure where a portion of the data is used to
train a suite of candidate models for predicting the residual, and the
remaining data are used to perform a score-based cumulative sum (CUSUM) test.
To further improve power, we then extend this adaptive CUSUM test to
incorporate cross-validation, while maintaining Type I error control under
minimal assumptions. Compared to existing methods, the proposed procedure
consistently achieved higher power in simulation studies and more than doubled
the power when auditing a mortality risk prediction model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jean Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gossmann_A/0/1/0/all/0/1&quot;&gt;Alexej Gossmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pirracchio_R/0/1/0/all/0/1&quot;&gt;Romain Pirracchio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrick_N/0/1/0/all/0/1&quot;&gt;Nicholas Petrick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pennello_G/0/1/0/all/0/1&quot;&gt;Gene Pennello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahiner_B/0/1/0/all/0/1&quot;&gt;Berkman Sahiner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15273">
<title>Recovering high-quality FODs from a reduced number of diffusion-weighted images using a model-driven deep learning architecture. (arXiv:2307.15273v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15273</link>
<description rdf:parseType="Literal">&lt;p&gt;Fibre orientation distribution (FOD) reconstruction using deep learning has
the potential to produce accurate FODs from a reduced number of
diffusion-weighted images (DWIs), decreasing total imaging time. Diffusion
acquisition invariant representations of the DWI signals are typically used as
input to these methods to ensure that they can be applied flexibly to data with
different b-vectors and b-values; however, this means the network cannot
condition its output directly on the DWI signal. In this work, we propose a
spherical deconvolution network, a model-driven deep learning FOD
reconstruction architecture, that ensures intermediate and output FODs produced
by the network are consistent with the input DWI signals. Furthermore, we
implement a fixel classification penalty within our loss function, encouraging
the network to produce FODs that can subsequently be segmented into the correct
number of fixels and improve downstream fixel-based analysis. Our results show
that the model-based deep learning architecture achieves competitive
performance compared to a state-of-the-art FOD super-resolution network,
FOD-Net. Moreover, we show that the fixel classification penalty can be tuned
to offer improved performance with respect to metrics that rely on accurately
segmented of FODs. Our code is publicly available at
https://github.com/Jbartlett6/SDNet .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartlett_J/0/1/0/all/0/1&quot;&gt;J Bartlett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davey_C/0/1/0/all/0/1&quot;&gt;C E Davey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnston_L/0/1/0/all/0/1&quot;&gt;L A Johnston&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;J Duan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15285">
<title>Optimal Approximation of Zonoids and Uniform Approximation by Shallow Neural Networks. (arXiv:2307.15285v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.15285</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the following two related problems. The first is to determine to
what error an arbitrary zonoid in $\mathbb{R}^{d+1}$ can be approximated in the
Hausdorff distance by a sum of $n$ line segments. The second is to determine
optimal approximation rates in the uniform norm for shallow ReLU$^k$ neural
networks on their variation spaces. The first of these problems has been solved
for $d\neq 2,3$, but when $d=2,3$ a logarithmic gap between the best upper and
lower bounds remains. We close this gap, which completes the solution in all
dimensions. For the second problem, our techniques significantly improve upon
existing approximation rates when $k\geq 1$, and enable uniform approximation
of both the target function and its derivatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Siegel_J/0/1/0/all/0/1&quot;&gt;Jonathan W. Siegel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15288">
<title>Learning Nonlinear Projections for Reduced-Order Modeling of Dynamical Systems using Constrained Autoencoders. (arXiv:2307.15288v1 [math.DS])</title>
<link>http://arxiv.org/abs/2307.15288</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently developed reduced-order modeling techniques aim to approximate
nonlinear dynamical systems on low-dimensional manifolds learned from data.
This is an effective approach for modeling dynamics in a post-transient regime
where the effects of initial conditions and other disturbances have decayed.
However, modeling transient dynamics near an underlying manifold, as needed for
real-time control and forecasting applications, is complicated by the effects
of fast dynamics and nonnormal sensitivity mechanisms. To begin to address
these issues, we introduce a parametric class of nonlinear projections
described by constrained autoencoder neural networks in which both the manifold
and the projection fibers are learned from data. Our architecture uses
invertible activation functions and biorthogonal weight matrices to ensure that
the encoder is a left inverse of the decoder. We also introduce new
dynamics-aware cost functions that promote learning of oblique projection
fibers that account for fast dynamics and nonnormality. To demonstrate these
methods and the specific challenges they address, we provide a detailed case
study of a three-state model of vortex shedding in the wake of a bluff body
immersed in a fluid, which has a two-dimensional slow manifold that can be
computed analytically. In anticipation of future applications to
high-dimensional systems, we also propose several techniques for constructing
computationally efficient reduced-order models using our proposed nonlinear
projection framework. This includes a novel sparsity-promoting penalty for the
encoder that avoids detrimental weight matrix shrinkage via computation on the
Grassmann manifold.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Otto_S/0/1/0/all/0/1&quot;&gt;Samuel E. Otto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Macchio_G/0/1/0/all/0/1&quot;&gt;Gregory R. Macchio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Rowley_C/0/1/0/all/0/1&quot;&gt;Clarence W. Rowley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15299">
<title>Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting. (arXiv:2307.15299v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2307.15299</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate load forecasting plays a vital role in numerous sectors, but
accurately capturing the complex dynamics of dynamic power systems remains a
challenge for traditional statistical models. For these reasons, time-series
models (ARIMA) and deep-learning models (ANN, LSTM, GRU, etc.) are commonly
deployed and often experience higher success. In this paper, we analyze the
efficacy of the recently developed Transformer-based Neural Network model in
Load forecasting. Transformer models have the potential to improve Load
forecasting because of their ability to learn long-range dependencies derived
from their Attention Mechanism. We apply several metaheuristics namely
Differential Evolution to find the optimal hyperparameters of the
Transformer-based Neural Network to produce accurate forecasts. Differential
Evolution provides scalable, robust, global solutions to non-differentiable,
multi-objective, or constrained optimization problems. Our work compares the
proposed Transformer based Neural Network model integrated with different
metaheuristic algorithms by their performance in Load forecasting based on
numerical metrics such as Mean Squared Error (MSE) and Mean Absolute Percentage
Error (MAPE). Our findings demonstrate the potential of metaheuristic-enhanced
Transformer-based Neural Network models in Load forecasting accuracy and
provide optimal hyperparameters for each model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_A/0/1/0/all/0/1&quot;&gt;Anuvab Sen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazumder_A/0/1/0/all/0/1&quot;&gt;Arul Rhik Mazumder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_U/0/1/0/all/0/1&quot;&gt;Udayon Sen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15317">
<title>DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable Kendall&apos;s Rank Correlation. (arXiv:2307.15317v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15317</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot learning aims to adapt models trained on the base dataset to novel
tasks where the categories are not seen by the model before. This often leads
to a relatively uniform distribution of feature values across channels on novel
classes, posing challenges in determining channel importance for novel tasks.
Standard few-shot learning methods employ geometric similarity metrics such as
cosine similarity and negative Euclidean distance to gauge the semantic
relatedness between two features. However, features with high geometric
similarities may carry distinct semantics, especially in the context of
few-shot learning. In this paper, we demonstrate that the importance ranking of
feature channels is a more reliable indicator for few-shot learning than
geometric similarity metrics. We observe that replacing the geometric
similarity metric with Kendall&apos;s rank correlation only during inference is able
to improve the performance of few-shot learning across a wide range of datasets
with different domains. Furthermore, we propose a carefully designed
differentiable loss for meta-training to address the non-differentiability
issue of Kendall&apos;s rank correlation. Extensive experiments demonstrate that the
proposed rank-correlation-based approach substantially enhances few-shot
learning performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1&quot;&gt;Kaipeng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huishuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weiran Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15320">
<title>Robust Visual Sim-to-Real Transfer for Robotic Manipulation. (arXiv:2307.15320v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.15320</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning visuomotor policies in simulation is much safer and cheaper than in
the real world. However, due to discrepancies between the simulated and real
data, simulator-trained policies often fail when transferred to real robots.
One common approach to bridge the visual sim-to-real domain gap is domain
randomization (DR). While previous work mainly evaluates DR for disembodied
tasks, such as pose estimation and object detection, here we systematically
explore visual domain randomization methods and benchmark them on a rich set of
challenging robotic manipulation tasks. In particular, we propose an off-line
proxy task of cube localization to select DR parameters for texture
randomization, lighting randomization, variations of object colors and camera
parameters. Notably, we demonstrate that DR parameters have similar impact on
our off-line proxy task and on-line policies. We, hence, use off-line optimized
DR parameters to train visuomotor policies in simulation and directly apply
such policies to a real robot. Our approach achieves 93% success rate on
average when tested on a diverse set of challenging manipulation tasks.
Moreover, we evaluate the robustness of policies to visual variations in real
scenes and show that our simulator-trained policies outperform policies learned
using real but limited data. Code, simulation environment, real robot datasets
and trained models are available at
https://www.di.ens.fr/willow/research/robust_s2r/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_R/0/1/0/all/0/1&quot;&gt;Ricardo Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strudel_R/0/1/0/all/0/1&quot;&gt;Robin Strudel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shizhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arlaud_E/0/1/0/all/0/1&quot;&gt;Etienne Arlaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1&quot;&gt;Ivan Laptev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1&quot;&gt;Cordelia Schmid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15325">
<title>Partial observations, coarse graining and equivariance in Koopman operator theory for large-scale dynamical systems. (arXiv:2307.15325v1 [math.DS])</title>
<link>http://arxiv.org/abs/2307.15325</link>
<description rdf:parseType="Literal">&lt;p&gt;The Koopman operator has become an essential tool for data-driven analysis,
prediction and control of complex systems, the main reason being the enormous
potential of identifying linear function space representations of nonlinear
dynamics from measurements. Until now, the situation where for large-scale
systems, we (i) only have access to partial observations (i.e., measurements,
as is very common for experimental data) or (ii) deliberately perform coarse
graining (for efficiency reasons) has not been treated to its full extent. In
this paper, we address the pitfall associated with this situation, that the
classical EDMD algorithm does not automatically provide a Koopman operator
approximation for the underlying system if we do not carefully select the
number of observables. Moreover, we show that symmetries in the system dynamics
can be carried over to the Koopman operator, which allows us to massively
increase the model efficiency. We also briefly draw a connection to domain
decomposition techniques for partial differential equations and present
numerical evidence using the Kuramoto--Sivashinsky equation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Peitz_S/0/1/0/all/0/1&quot;&gt;Sebastian Peitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Harder_H/0/1/0/all/0/1&quot;&gt;Hans Harder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nuske_F/0/1/0/all/0/1&quot;&gt;Feliks N&amp;#xfc;ske&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Philipp_F/0/1/0/all/0/1&quot;&gt;Friedrich Philipp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Schaller_M/0/1/0/all/0/1&quot;&gt;Manuel Schaller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Worthmann_K/0/1/0/all/0/1&quot;&gt;Karl Worthmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15326">
<title>Staging E-Commerce Products for Online Advertising using Retrieval Assisted Image Generation. (arXiv:2307.15326v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15326</link>
<description rdf:parseType="Literal">&lt;p&gt;Online ads showing e-commerce products typically rely on the product images
in a catalog sent to the advertising platform by an e-commerce platform. In the
broader ads industry such ads are called dynamic product ads (DPA). It is
common for DPA catalogs to be in the scale of millions (corresponding to the
scale of products which can be bought from the e-commerce platform). However,
not all product images in the catalog may be appealing when directly
re-purposed as an ad image, and this may lead to lower click-through rates
(CTRs). In particular, products just placed against a solid background may not
be as enticing and realistic as a product staged in a natural environment. To
address such shortcomings of DPA images at scale, we propose a generative
adversarial network (GAN) based approach to generate staged backgrounds for
un-staged product images. Generating the entire staged background is a
challenging task susceptible to hallucinations. To get around this, we
introduce a simpler approach called copy-paste staging using retrieval assisted
GANs. In copy paste staging, we first retrieve (from the catalog) staged
products similar to the un-staged input product, and then copy-paste the
background of the retrieved product in the input image. A GAN based in-painting
model is used to fill the holes left after this copy-paste operation. We show
the efficacy of our copy-paste staging method via offline metrics, and human
evaluation. In addition, we show how our staging approach can enable animations
of moving products leading to a video ad from a product image.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ku_Y/0/1/0/all/0/1&quot;&gt;Yueh-Ning Ku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuznetsov_M/0/1/0/all/0/1&quot;&gt;Mikhail Kuznetsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Shaunak Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juan_P/0/1/0/all/0/1&quot;&gt;Paloma de Juan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15339">
<title>The Radon Signed Cumulative Distribution Transform and its applications in classification of Signed Images. (arXiv:2307.15339v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2307.15339</link>
<description rdf:parseType="Literal">&lt;p&gt;Here we describe a new image representation technique based on the
mathematics of transport and optimal transport. The method relies on the
combination of the well-known Radon transform for images and a recent signal
representation method called the Signed Cumulative Distribution Transform. The
newly proposed method generalizes previous transport-related image
representation methods to arbitrary functions (images), and thus can be used in
more applications. We describe the new transform, and some of its mathematical
properties and demonstrate its ability to partition image classes with real and
simulated data. In comparison to existing transport transform methods, as well
as deep learning-based classification methods, the new transform more
accurately represents the information content of signed images, and thus can be
used to obtain higher classification accuracies. The implementation of the
proposed method in Python language is integrated as a part of the software
package PyTransKit, available on Github.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_L/0/1/0/all/0/1&quot;&gt;Le Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shiying Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathan_N/0/1/0/all/0/1&quot;&gt;Naqib Sad Pathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shifat_E_Rabbi_M/0/1/0/all/0/1&quot;&gt;Mohammad Shifat-E-Rabbi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rohde_G/0/1/0/all/0/1&quot;&gt;Gustavo K. Rohde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubaiyat_A/0/1/0/all/0/1&quot;&gt;Abu Hasnat Mohammad Rubaiyat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thareja_S/0/1/0/all/0/1&quot;&gt;Sumati Thareja&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15343">
<title>Med-HALT: Medical Domain Hallucination Test for Large Language Models. (arXiv:2307.15343v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.15343</link>
<description rdf:parseType="Literal">&lt;p&gt;This research paper focuses on the challenges posed by hallucinations in
large language models (LLMs), particularly in the context of the medical
domain. Hallucination, wherein these models generate plausible yet unverified
or incorrect information, can have serious consequences in healthcare
applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain
Hallucination Test), designed specifically to evaluate and reduce
hallucinations. Med-HALT provides a diverse multinational dataset derived from
medical examinations across various countries and includes multiple innovative
testing modalities. Med-HALT includes two categories of tests reasoning and
memory-based hallucination tests, designed to assess LLMs&apos;s problem-solving and
information retrieval abilities.
&lt;/p&gt;
&lt;p&gt;Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2,
MPT, and Falcon, revealing significant differences in their performance. The
paper provides detailed insights into the dataset, promoting transparency and
reproducibility. Through this work, we aim to contribute to the development of
safer and more reliable language models in healthcare. Our benchmark can be
found at medhalt.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Umapathi_L/0/1/0/all/0/1&quot;&gt;Logesh Kumar Umapathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_A/0/1/0/all/0/1&quot;&gt;Ankit Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankarasubbu_M/0/1/0/all/0/1&quot;&gt;Malaikannan Sankarasubbu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15361">
<title>Confident Feature Ranking. (arXiv:2307.15361v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.15361</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretation of feature importance values often relies on the relative
order of the features rather than on the value itself, referred to as ranking.
However, the order may be unstable due to the small sample sizes used in
calculating the importance values. We propose that post-hoc importance methods
produce a ranking and simultaneous confident intervals for the rankings. Based
on pairwise comparisons of the feature importance values, our method is
guaranteed to include the ``true&apos;&apos; (infinite sample) ranking with high
probability and allows for selecting top-k sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Neuhof_B/0/1/0/all/0/1&quot;&gt;Bitya Neuhof&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Benjamini_Y/0/1/0/all/0/1&quot;&gt;Yuval Benjamini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15367">
<title>Toward Transparent Sequence Models with Model-Based Tree Markov Model. (arXiv:2307.15367v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15367</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we address the interpretability issue in complex, black-box
Machine Learning models applied to sequence data. We introduce the Model-Based
tree Hidden Semi-Markov Model (MOB-HSMM), an inherently interpretable model
aimed at detecting high mortality risk events and discovering hidden patterns
associated with the mortality risk in Intensive Care Units (ICU). This model
leverages knowledge distilled from Deep Neural Networks (DNN) to enhance
predictive performance while offering clear explanations. Our experimental
results indicate the improved performance of Model-Based trees (MOB trees) via
employing LSTM for learning sequential patterns, which are then transferred to
MOB trees. Integrating MOB trees with the Hidden Semi-Markov Model (HSMM) in
the MOB-HSMM enables uncovering potential and explainable sequences using
available information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1&quot;&gt;Chan Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei-Chun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jun-Ting Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chih-Yuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yihuang Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15373">
<title>Conflict-free joint decision by lag and zero-lag synchronization in laser network. (arXiv:2307.15373v1 [physics.optics])</title>
<link>http://arxiv.org/abs/2307.15373</link>
<description rdf:parseType="Literal">&lt;p&gt;With the end of Moore&apos;s Law and the increasing demand for computing, photonic
accelerators are garnering considerable attention. This is due to the physical
characteristics of light, such as high bandwidth and multiplicity, and the
various synchronization phenomena that emerge in the realm of laser physics.
These factors come into play as computer performance approaches its limits. In
this study, we explore the application of a laser network, acting as a photonic
accelerator, to the competitive multi-armed bandit problem. In this context,
conflict avoidance is key to maximizing environmental rewards. We
experimentally demonstrate cooperative decision-making using zero-lag and lag
synchronization within a network of four semiconductor lasers. Lag
synchronization of chaos realizes effective decision-making and zero-delay
synchronization is responsible for the realization of the collision avoidance
function. We experimentally verified a low collision rate and high reward in a
fundamental 2-player, 2-slot scenario, and showed the scalability of this
system. This system architecture opens up new possibilities for intelligent
functionalities in laser dynamics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ito_H/0/1/0/all/0/1&quot;&gt;Hisako Ito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Mihana_T/0/1/0/all/0/1&quot;&gt;Takatomo Mihana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Horisaki_R/0/1/0/all/0/1&quot;&gt;Ryoichi Horisaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Naruse_M/0/1/0/all/0/1&quot;&gt;Makoto Naruse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15377">
<title>Co-attention Graph Pooling for Efficient Pairwise Graph Interaction Learning. (arXiv:2307.15377v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15377</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have proven to be effective in processing and
learning from graph-structured data. However, previous works mainly focused on
understanding single graph inputs while many real-world applications require
pair-wise analysis for graph-structured data (e.g., scene graph matching, code
searching, and drug-drug interaction prediction). To this end, recent works
have shifted their focus to learning the interaction between pairs of graphs.
Despite their improved performance, these works were still limited in that the
interactions were considered at the node-level, resulting in high computational
costs and suboptimal performance. To address this issue, we propose a novel and
efficient graph-level approach for extracting interaction representations using
co-attention in graph pooling. Our method, Co-Attention Graph Pooling
(CAGPool), exhibits competitive performance relative to existing methods in
both classification and regression tasks using real-world datasets, while
maintaining lower computational complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junhyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Bumsoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1&quot;&gt;Minji Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;Jaewoo Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15388">
<title>Does Full Waveform Inversion Benefit from Big Data?. (arXiv:2307.15388v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15388</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates the impact of big data on deep learning models for
full waveform inversion (FWI). While it is well known that big data can boost
the performance of deep learning models in many tasks, its effectiveness has
not been validated for FWI. To address this gap, we present an empirical study
that investigates how deep learning models in FWI behave when trained on
OpenFWI, a collection of large-scale, multi-structural datasets published
recently. Particularly, we train and evaluate the FWI models on a combination
of 10 2D subsets in OpenFWI that contain 470K data pairs in total. Our
experiments demonstrate that larger datasets lead to better performance and
generalization of deep learning models for FWI. We further demonstrate that
model capacity needs to scale in accordance with data size for optimal
improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1&quot;&gt;Peng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yinan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shihang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanchen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yinpeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Consolvo_B/0/1/0/all/0/1&quot;&gt;Benjamin Consolvo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zicheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Youzuo Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15396">
<title>Noisy Interpolation Learning with Shallow Univariate ReLU Networks. (arXiv:2307.15396v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15396</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the asymptotic overfitting behavior of interpolation with minimum
norm ($\ell_2$ of the weights) two-layer ReLU networks for noisy univariate
regression. We show that overfitting is tempered for the $L_1$ loss, and any
$L_p$ loss for $p&amp;lt;2$, but catastrophic for $p\geq 2$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1&quot;&gt;Nirmit Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vardi_G/0/1/0/all/0/1&quot;&gt;Gal Vardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srebro_N/0/1/0/all/0/1&quot;&gt;Nathan Srebro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15398">
<title>The Initial Screening Order Problem. (arXiv:2307.15398v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15398</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we present the initial screening order problem, a crucial step
within candidate screening. It involves a human-like screener with an objective
to find the first k suitable candidates rather than the best k suitable
candidates in a candidate pool given an initial screening order. The initial
screening order represents the way in which the human-like screener arranges
the candidate pool prior to screening. The choice of initial screening order
has considerable effects on the selected set of k candidates. We prove that
under an unbalanced candidate pool (e.g., having more male than female
candidates), the human-like screener can suffer from uneven efforts that hinder
its decision-making over the protected, under-represented group relative to the
non-protected, over-represented group. Other fairness results are proven under
the human-like screener. This research is based on a collaboration with a large
company to better understand its hiring process for potential automation. Our
main contribution is the formalization of the initial screening order problem
which, we argue, opens the path for future extensions of the current works on
ranking algorithms, fairness, and automation for screening procedures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1&quot;&gt;Jose M. Alvarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruggieri_S/0/1/0/all/0/1&quot;&gt;Salvatore Ruggieri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15422">
<title>Is One Epoch All You Need For Multi-Fidelity Hyperparameter Optimization?. (arXiv:2307.15422v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15422</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperparameter optimization (HPO) is crucial for fine-tuning machine learning
models but can be computationally expensive. To reduce costs, Multi-fidelity
HPO (MF-HPO) leverages intermediate accuracy levels in the learning process and
discards low-performing models early on. We compared various representative
MF-HPO methods against a simple baseline on classical benchmark data. The
baseline involved discarding all models except the Top-K after training for
only one epoch, followed by further training to select the best model.
Surprisingly, this baseline achieved similar results to its counterparts, while
requiring an order of magnitude less computation. Upon analyzing the learning
curves of the benchmark data, we observed a few dominant learning curves, which
explained the success of our baseline. This suggests that researchers should
(1) always use the suggested baseline in benchmarks and (2) broaden the
diversity of MF-HPO benchmarks to include more complex cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Egele_R/0/1/0/all/0/1&quot;&gt;Romain Egele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1&quot;&gt;Isabelle Guyon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yixuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balaprakash_P/0/1/0/all/0/1&quot;&gt;Prasanna Balaprakash&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15424">
<title>Deep Generative Models, Synthetic Tabular Data, and Differential Privacy: An Overview and Synthesis. (arXiv:2307.15424v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15424</link>
<description rdf:parseType="Literal">&lt;p&gt;This article provides a comprehensive synthesis of the recent developments in
synthetic data generation via deep generative models, focusing on tabular
datasets. We specifically outline the importance of synthetic data generation
in the context of privacy-sensitive data. Additionally, we highlight the
advantages of using deep generative models over other methods and provide a
detailed explanation of the underlying concepts, including unsupervised
learning, neural networks, and generative models. The paper covers the
challenges and considerations involved in using deep generative models for
tabular datasets, such as data normalization, privacy concerns, and model
evaluation. This review provides a valuable resource for researchers and
practitioners interested in synthetic data generation and its applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassan_C/0/1/0/all/0/1&quot;&gt;Conor Hassan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salomone_R/0/1/0/all/0/1&quot;&gt;Robert Salomone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mengersen_K/0/1/0/all/0/1&quot;&gt;Kerrie Mengersen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15428">
<title>Implicit neural representation for change detection. (arXiv:2307.15428v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15428</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting changes that occurred in a pair of 3D airborne LiDAR point clouds,
acquired at two different times over the same geographical area, is a
challenging task because of unmatching spatial supports and acquisition system
noise. Most recent attempts to detect changes on point clouds are based on
supervised methods, which require large labelled data unavailable in real-world
applications. To address these issues, we propose an unsupervised approach that
comprises two components: Neural Field (NF) for continuous shape reconstruction
and a Gaussian Mixture Model for categorising changes. NF offer a grid-agnostic
representation to encode bi-temporal point clouds with unmatched spatial
support that can be regularised to increase high-frequency details and reduce
noise. The reconstructions at each timestamp are compared at arbitrary spatial
scales, leading to a significant increase in detection capabilities. We apply
our method to a benchmark dataset of simulated LiDAR point clouds for urban
sprawling. The dataset offers different challenging scenarios with different
resolutions, input modalities and noise levels, allowing a multi-scenario
comparison of our method with the current state-of-the-art. We boast the
previous methods on this dataset by a 10% margin in intersection over union
metric. In addition, we apply our methods to a real-world scenario to identify
illegal excavation (looting) of archaeological sites and confirm that they
match findings from field experts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naylor_P/0/1/0/all/0/1&quot;&gt;Peter Naylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carlo_D/0/1/0/all/0/1&quot;&gt;Diego Di Carlo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Traviglia_A/0/1/0/all/0/1&quot;&gt;Arianna Traviglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamada_M/0/1/0/all/0/1&quot;&gt;Makoto Yamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiorucci_M/0/1/0/all/0/1&quot;&gt;Marco Fiorucci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15429">
<title>Improvable Gap Balancing for Multi-Task Learning. (arXiv:2307.15429v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15429</link>
<description rdf:parseType="Literal">&lt;p&gt;In multi-task learning (MTL), gradient balancing has recently attracted more
research interest than loss balancing since it often leads to better
performance. However, loss balancing is much more efficient than gradient
balancing, and thus it is still worth further exploration in MTL. Note that
prior studies typically ignore that there exist varying improvable gaps across
multiple tasks, where the improvable gap per task is defined as the distance
between the current training progress and desired final training progress.
Therefore, after loss balancing, the performance imbalance still arises in many
cases. In this paper, following the loss balancing framework, we propose two
novel improvable gap balancing (IGB) algorithms for MTL: one takes a simple
heuristic, and the other (for the first time) deploys deep reinforcement
learning for MTL. Particularly, instead of directly balancing the losses in
MTL, both algorithms choose to dynamically assign task weights for improvable
gap balancing. Moreover, we combine IGB and gradient balancing to show the
complementarity between the two types of algorithms. Extensive experiments on
two benchmark datasets demonstrate that our IGB algorithms lead to the best
results in MTL via loss balancing and achieve further improvements when
combined with gradient balancing. Code is available at
https://github.com/YanqiDai/IGB4MTL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yanqi Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_N/0/1/0/all/0/1&quot;&gt;Nanyi Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhiwu Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15438">
<title>Autonomous Payload Thermal Control. (arXiv:2307.15438v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15438</link>
<description rdf:parseType="Literal">&lt;p&gt;In small satellites there is less room for heat control equipment, scientific
instruments, and electronic components. Furthermore, the near proximity of the
electronics makes power dissipation difficult, with the risk of not being able
to control the temperature appropriately, reducing component lifetime and
mission performance. To address this challenge, taking advantage of the advent
of increasing intelligence on board satellites, a deep reinforcement learning
based framework that uses Soft Actor-Critic algorithm is proposed for learning
the thermal control policy onboard. The framework is evaluated both in a naive
simulated environment and in a real space edge processing computer that will be
shipped in the future IMAGIN-e mission and hosted in the ISS. The experiment
results show that the proposed framework is able to learn to control the
payload processing power to maintain the temperature under operational ranges,
complementing traditional thermal control systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousist_A/0/1/0/all/0/1&quot;&gt;Alejandro D. Mousist&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15456">
<title>Worrisome Properties of Neural Network Controllers and Their Symbolic Representations. (arXiv:2307.15456v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15456</link>
<description rdf:parseType="Literal">&lt;p&gt;We raise concerns about controllers&apos; robustness in simple reinforcement
learning benchmark problems. We focus on neural network controllers and their
low neuron and symbolic abstractions. A typical controller reaching high mean
return values still generates an abundance of persistent low-return solutions,
which is a highly undesirable property, easily exploitable by an adversary. We
find that the simpler controllers admit more persistent bad solutions. We
provide an algorithm for a systematic robustness study and prove existence of
persistent solutions and, in some cases, periodic orbits, using a
computer-assisted proof methodology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cyranka_J/0/1/0/all/0/1&quot;&gt;Jacek Cyranka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Church_K/0/1/0/all/0/1&quot;&gt;Kevin E M Church&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lessard_J/0/1/0/all/0/1&quot;&gt;Jean-Philippe Lessard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15466">
<title>LUCID-GAN: Conditional Generative Models to Locate Unfairness. (arXiv:2307.15466v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15466</link>
<description rdf:parseType="Literal">&lt;p&gt;Most group fairness notions detect unethical biases by computing statistical
parity metrics on a model&apos;s output. However, this approach suffers from several
shortcomings, such as philosophical disagreement, mutual incompatibility, and
lack of interpretability. These shortcomings have spurred the research on
complementary bias detection methods that offer additional transparency into
the sources of discrimination and are agnostic towards an a priori decision on
the definition of fairness and choice of protected features. A recent proposal
in this direction is LUCID (Locating Unfairness through Canonical Inverse
Design), where canonical sets are generated by performing gradient descent on
the input space, revealing a model&apos;s desired input given a preferred output.
This information about the model&apos;s mechanisms, i.e., which feature values are
essential to obtain specific outputs, allows exposing potential unethical
biases in its internal logic. Here, we present LUCID-GAN, which generates
canonical inputs via a conditional generative model instead of gradient-based
inverse design. LUCID-GAN has several benefits, including that it applies to
non-differentiable models, ensures that canonical sets consist of realistic
inputs, and allows to assess proxy and intersectional discrimination. We
empirically evaluate LUCID-GAN on the UCI Adult and COMPAS data sets and show
that it allows for detecting unethical biases in black-box models without
requiring access to the training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Algaba_A/0/1/0/all/0/1&quot;&gt;Andres Algaba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazijn_C/0/1/0/all/0/1&quot;&gt;Carmen Mazijn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prunkl_C/0/1/0/all/0/1&quot;&gt;Carina Prunkl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danckaert_J/0/1/0/all/0/1&quot;&gt;Jan Danckaert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ginis_V/0/1/0/all/0/1&quot;&gt;Vincent Ginis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15475">
<title>FeedbackLogs: Recording and Incorporating Stakeholder Feedback into Machine Learning Pipelines. (arXiv:2307.15475v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.15475</link>
<description rdf:parseType="Literal">&lt;p&gt;Even though machine learning (ML) pipelines affect an increasing array of
stakeholders, there is little work on how input from stakeholders is recorded
and incorporated. We propose FeedbackLogs, addenda to existing documentation of
ML pipelines, to track the input of multiple stakeholders. Each log records
important details about the feedback collection process, the feedback itself,
and how the feedback is used to update the ML pipeline. In this paper, we
introduce and formalise a process for collecting a FeedbackLog. We also provide
concrete use cases where FeedbackLogs can be employed as evidence for
algorithmic auditing and as a tool to record updates based on stakeholder
feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barker_M/0/1/0/all/0/1&quot;&gt;Matthew Barker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kallina_E/0/1/0/all/0/1&quot;&gt;Emma Kallina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashok_D/0/1/0/all/0/1&quot;&gt;Dhananjay Ashok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_K/0/1/0/all/0/1&quot;&gt;Katherine M. Collins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casovan_A/0/1/0/all/0/1&quot;&gt;Ashley Casovan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1&quot;&gt;Adrian Weller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1&quot;&gt;Ameet Talwalkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1&quot;&gt;Valerie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatt_U/0/1/0/all/0/1&quot;&gt;Umang Bhatt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15496">
<title>From continuous-time formulations to discretization schemes: tensor trains and robust regression for BSDEs and parabolic PDEs. (arXiv:2307.15496v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15496</link>
<description rdf:parseType="Literal">&lt;p&gt;The numerical approximation of partial differential equations (PDEs) poses
formidable challenges in high dimensions since classical grid-based methods
suffer from the so-called curse of dimensionality. Recent attempts rely on a
combination of Monte Carlo methods and variational formulations, using neural
networks for function approximation. Extending previous work (Richter et al.,
2021), we argue that tensor trains provide an appealing framework for parabolic
PDEs: The combination of reformulations in terms of backward stochastic
differential equations and regression-type methods holds the promise of
leveraging latent low-rank structures, enabling both compression and efficient
computation. Emphasizing a continuous-time viewpoint, we develop iterative
schemes, which differ in terms of computational efficiency and robustness. We
demonstrate both theoretically and numerically that our methods can achieve a
favorable trade-off between accuracy and computational efficiency. While
previous methods have been either accurate or fast, we have identified a novel
numerical strategy that can often combine both of these aspects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richter_L/0/1/0/all/0/1&quot;&gt;Lorenz Richter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sallandt_L/0/1/0/all/0/1&quot;&gt;Leon Sallandt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nusken_N/0/1/0/all/0/1&quot;&gt;Nikolas N&amp;#xfc;sken&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15503">
<title>The Applicability of Federated Learning to Official Statistics. (arXiv:2307.15503v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15503</link>
<description rdf:parseType="Literal">&lt;p&gt;This work investigates the potential of Federated Learning (FL) for official
statistics and shows how well the performance of FL models can keep up with
centralized learning methods. At the same time, its utilization can safeguard
the privacy of data holders, thus facilitating access to a broader range of
data and ultimately enhancing official statistics. By simulating three
different use cases, important insights on the applicability of the technology
are gained. The use cases are based on a medical insurance data set, a fine
dust pollution data set and a mobile radio coverage data set - all of which are
from domains close to official statistics. We provide a detailed analysis of
the results, including a comparison of centralized and FL algorithm
performances for each simulation. In all three use cases, we were able to train
models via FL which reach a performance very close to the centralized model
benchmarks. Our key observations and their implications for transferring the
simulations into practice are summarized. We arrive at the conclusion that FL
has the potential to emerge as a pivotal technology in future use cases of
official statistics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stock_J/0/1/0/all/0/1&quot;&gt;Joshua Stock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hauke_O/0/1/0/all/0/1&quot;&gt;Oliver Hauke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weissmann_J/0/1/0/all/0/1&quot;&gt;Julius Wei&amp;#xdf;mann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Federrath_H/0/1/0/all/0/1&quot;&gt;Hannes Federrath&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15539">
<title>Backdoor Defense with Non-Adversarial Backdoor. (arXiv:2307.15539v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15539</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) are vulnerable to backdoor attack, which does not
affect the network&apos;s performance on clean data but would manipulate the network
behavior once a trigger pattern is added. Existing defense methods have greatly
reduced attack success rate, but their prediction accuracy on clean data still
lags behind a clean model by a large margin. Inspired by the stealthiness and
effectiveness of backdoor attack, we propose a simple but highly effective
defense framework which injects non-adversarial backdoors targeting poisoned
samples. Following the general steps in backdoor attack, we detect a small set
of suspected samples and then apply a poisoning strategy to them. The
non-adversarial backdoor, once triggered, suppresses the attacker&apos;s backdoor on
poisoned data, but has limited influence on clean data. The defense can be
carried out during data preprocessing, without any modification to the standard
end-to-end training pipeline. We conduct extensive experiments on multiple
benchmarks with different architectures and representative attacks. Results
demonstrate that our method achieves state-of-the-art defense effectiveness
with by far the lowest performance drop on clean data. Considering the
surprising defense ability displayed by our framework, we call for more
attention to utilizing backdoor for backdoor defense. Code is available at
https://github.com/damianliumin/non-adversarial_backdoor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Min Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sangiovanni_Vincentelli_A/0/1/0/all/0/1&quot;&gt;Alberto Sangiovanni-Vincentelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xiangyu Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15546">
<title>On the Trade-off Between Efficiency and Precision of Neural Abstraction. (arXiv:2307.15546v1 [cs.LO])</title>
<link>http://arxiv.org/abs/2307.15546</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural abstractions have been recently introduced as formal approximations of
complex, nonlinear dynamical models. They comprise a neural ODE and a certified
upper bound on the error between the abstract neural network and the concrete
dynamical model. So far neural abstractions have exclusively been obtained as
neural networks consisting entirely of $ReLU$ activation functions, resulting
in neural ODE models that have piecewise affine dynamics, and which can be
equivalently interpreted as linear hybrid automata. In this work, we observe
that the utility of an abstraction depends on its use: some scenarios might
require coarse abstractions that are easier to analyse, whereas others might
require more complex, refined abstractions. We therefore consider neural
abstractions of alternative shapes, namely either piecewise constant or
nonlinear non-polynomial (specifically, obtained via sigmoidal activations). We
employ formal inductive synthesis procedures to generate neural abstractions
that result in dynamical models with these semantics. Empirically, we
demonstrate the trade-off that these different neural abstraction templates
have vis-a-vis their precision and synthesis time, as well as the time required
for their safety verification (done via reachability computation). We improve
existing synthesis techniques to enable abstraction of higher-dimensional
models, and additionally discuss the abstraction of complex neural ODEs to
improve the efficiency of reachability analysis for these models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edwards_A/0/1/0/all/0/1&quot;&gt;Alec Edwards&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giacobbe_M/0/1/0/all/0/1&quot;&gt;Mirco Giacobbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1&quot;&gt;Alessandro Abate&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15557">
<title>Dynamic algorithms for k-center on graphs. (arXiv:2307.15557v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2307.15557</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we give the first efficient algorithms for the $k$-center
problem on dynamic graphs undergoing edge updates. In this problem, the goal is
to partition the input into $k$ sets by choosing $k$ centers such that the
maximum distance from any data point to the closest center is minimized. It is
known that it is NP-hard to get a better than $2$ approximation for this
problem.
&lt;/p&gt;
&lt;p&gt;While in many applications the input may naturally be modeled as a graph, all
prior works on $k$-center problem in dynamic settings are on metrics. In this
paper, we give a deterministic decremental $(2+\epsilon)$-approximation
algorithm and a randomized incremental $(4+\epsilon)$-approximation algorithm,
both with amortized update time $kn^{o(1)}$ for weighted graphs. Moreover, we
show a reduction that leads to a fully dynamic $(2+\epsilon)$-approximation
algorithm for the $k$-center problem, with worst-case update time that is
within a factor $k$ of the state-of-the-art upper bound for maintaining
$(1+\epsilon)$-approximate single-source distances in graphs. Matching this
bound is a natural goalpost because the approximate distances of each vertex to
its center can be used to maintain a $(2+\epsilon)$-approximation of the graph
diameter and the fastest known algorithms for such a diameter approximation
also rely on maintaining approximate single-source distances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruciani_E/0/1/0/all/0/1&quot;&gt;Emilio Cruciani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forster_S/0/1/0/all/0/1&quot;&gt;Sebastian Forster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goranci_G/0/1/0/all/0/1&quot;&gt;Gramoz Goranci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nazari_Y/0/1/0/all/0/1&quot;&gt;Yasamin Nazari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skarlatos_A/0/1/0/all/0/1&quot;&gt;Antonis Skarlatos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15593">
<title>Robust Distortion-free Watermarks for Language Models. (arXiv:2307.15593v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15593</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a methodology for planting watermarks in text from an
autoregressive language model that are robust to perturbations without changing
the distribution over text up to a certain maximum generation budget. We
generate watermarked text by mapping a sequence of random numbers -- which we
compute using a randomized watermark key -- to a sample from the language
model. To detect watermarked text, any party who knows the key can align the
text to the random number sequence. We instantiate our watermark methodology
with two sampling schemes: inverse transform sampling and exponential minimum
sampling. We apply these watermarks to three language models -- OPT-1.3B,
LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power
and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B
and LLaMA-7B models, we find we can reliably detect watermarked text ($p \leq
0.01$) from $35$ tokens even after corrupting between $40$-$50$\% of the tokens
via random edits (i.e., substitutions, insertions or deletions). For the
Alpaca-7B model, we conduct a case study on the feasibility of watermarking
responses to typical user instructions. Due to the lower entropy of the
responses, detection is more difficult: around $25\%$ of the responses -- whose
median length is around $100$ tokens -- are detectable with $p \leq 0.01$, and
the watermark is also less robust to certain automated paraphrasing attacks we
implement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuditipudi_R/0/1/0/all/0/1&quot;&gt;Rohith Kuditipudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thickstun_J/0/1/0/all/0/1&quot;&gt;John Thickstun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1&quot;&gt;Tatsunori Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Percy Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15621">
<title>Shrink-Perturb Improves Architecture Mixing during Population Based Training for Neural Architecture Search. (arXiv:2307.15621v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15621</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we show that simultaneously training and mixing neural networks
is a promising way to conduct Neural Architecture Search (NAS). For
hyperparameter optimization, reusing the partially trained weights allows for
efficient search, as was previously demonstrated by the Population Based
Training (PBT) algorithm. We propose PBT-NAS, an adaptation of PBT to NAS where
architectures are improved during training by replacing poorly-performing
networks in a population with the result of mixing well-performing ones and
inheriting the weights using the shrink-perturb technique. After PBT-NAS
terminates, the created networks can be directly used without retraining.
PBT-NAS is highly parallelizable and effective: on challenging tasks (image
generation and reinforcement learning) PBT-NAS achieves superior performance
compared to baselines (random search and mutation-based PBT).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chebykin_A/0/1/0/all/0/1&quot;&gt;Alexander Chebykin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dushatskiy_A/0/1/0/all/0/1&quot;&gt;Arkadiy Dushatskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alderliesten_T/0/1/0/all/0/1&quot;&gt;Tanja Alderliesten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosman_P/0/1/0/all/0/1&quot;&gt;Peter A. N. Bosman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15625">
<title>A Comparative Analysis of Machine Learning Methods for Lane Change Intention Recognition Using Vehicle Trajectory Data. (arXiv:2307.15625v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.15625</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately detecting and predicting lane change (LC)processes can help
autonomous vehicles better understand their surrounding environment, recognize
potential safety hazards, and improve traffic safety. This paper focuses on LC
processes and compares different machine learning methods&apos; performance to
recognize LC intention from high-dimensionality time series data. To validate
the performance of the proposed models, a total number of 1023 vehicle
trajectories is extracted from the CitySim dataset. For LC intention
recognition issues, the results indicate that with ninety-eight percent of
classification accuracy, ensemble methods reduce the impact of Type II and Type
III classification errors. Without sacrificing recognition accuracy, the
LightGBM demonstrates a sixfold improvement in model training efficiency than
the XGBoost algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yuan_R/0/1/0/all/0/1&quot;&gt;Renteng Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15638">
<title>TriadNet: Sampling-free predictive intervals for lesional volume in 3D brain MR images. (arXiv:2307.15638v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.15638</link>
<description rdf:parseType="Literal">&lt;p&gt;The volume of a brain lesion (e.g. infarct or tumor) is a powerful indicator
of patient prognosis and can be used to guide the therapeutic strategy.
Lesional volume estimation is usually performed by segmentation with deep
convolutional neural networks (CNN), currently the state-of-the-art approach.
However, to date, few work has been done to equip volume segmentation tools
with adequate quantitative predictive intervals, which can hinder their
usefulness and acceptation in clinical practice. In this work, we propose
TriadNet, a segmentation approach relying on a multi-head CNN architecture,
which provides both the lesion volumes and the associated predictive intervals
simultaneously, in less than a second. We demonstrate its superiority over
other solutions on BraTS 2021, a large-scale MRI glioblastoma image database.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lambert_B/0/1/0/all/0/1&quot;&gt;Benjamin Lambert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Forbes_F/0/1/0/all/0/1&quot;&gt;Florence Forbes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Doyle_S/0/1/0/all/0/1&quot;&gt;Senan Doyle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dojat_M/0/1/0/all/0/1&quot;&gt;Michel Dojat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15644">
<title>Scaling Data Generation in Vision-and-Language Navigation. (arXiv:2307.15644v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15644</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research in language-guided visual navigation has demonstrated a
significant demand for the diversity of traversable environments and the
quantity of supervision for training generalizable agents. To tackle the common
data scarcity issue in existing vision-and-language navigation datasets, we
propose an effective paradigm for generating large-scale data for learning,
which applies 1200+ photo-realistic environments from HM3D and Gibson datasets
and synthesizes 4.9 million instruction trajectory pairs using fully-accessible
resources on the web. Importantly, we investigate the influence of each
component in this paradigm on the agent&apos;s performance and study how to
adequately apply the augmented data to pre-train and fine-tune an agent. Thanks
to our large-scale dataset, the performance of an existing agent can be pushed
up (+11% absolute with regard to previous SoTA) to a significantly new best of
80% single-run success rate on the R2R test split by simple imitation learning.
The long-lasting generalization gap between navigating in seen and unseen
environments is also reduced to less than 1% (versus 8% in the previous best
method). Moreover, our paradigm also facilitates different models to achieve
new state-of-the-art navigation results on CVDN, REVERIE, and R2R in continuous
environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jialu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yicong Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1&quot;&gt;Stephen Gould&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1&quot;&gt;Hao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15645">
<title>Scale-aware Test-time Click Adaptation for Pulmonary Nodule and Mass Segmentation. (arXiv:2307.15645v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.15645</link>
<description rdf:parseType="Literal">&lt;p&gt;Pulmonary nodules and masses are crucial imaging features in lung cancer
screening that require careful management in clinical diagnosis. Despite the
success of deep learning-based medical image segmentation, the robust
performance on various sizes of lesions of nodule and mass is still
challenging. In this paper, we propose a multi-scale neural network with
scale-aware test-time adaptation to address this challenge. Specifically, we
introduce an adaptive Scale-aware Test-time Click Adaptation method based on
effortlessly obtainable lesion clicks as test-time cues to enhance segmentation
performance, particularly for large lesions. The proposed method can be
seamlessly integrated into existing networks. Extensive experiments on both
open-source and in-house datasets consistently demonstrate the effectiveness of
the proposed method over some CNN and Transformer-based segmentation methods.
Our code is available at https://github.com/SplinterLi/SaTTCA
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhihao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiancheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yongchao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_W/0/1/0/all/0/1&quot;&gt;Wenhui Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bo Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15647">
<title>Multi-layer Aggregation as a key to feature-based OOD detection. (arXiv:2307.15647v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15647</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning models are easily disturbed by variations in the input images
that were not observed during the training stage, resulting in unpredictable
predictions. Detecting such Out-of-Distribution (OOD) images is particularly
crucial in the context of medical image analysis, where the range of possible
abnormalities is extremely wide. Recently, a new category of methods has
emerged, based on the analysis of the intermediate features of a trained model.
These methods can be divided into 2 groups: single-layer methods that consider
the feature map obtained at a fixed, carefully chosen layer, and multi-layer
methods that consider the ensemble of the feature maps generated by the model.
While promising, a proper comparison of these algorithms is still lacking. In
this work, we compared various feature-based OOD detection methods on a large
spectra of OOD (20 types), representing approximately 7800 3D MRIs. Our
experiments shed the light on two phenomenons. First, multi-layer methods
consistently outperform single-layer approaches, which tend to have
inconsistent behaviour depending on the type of anomaly. Second, the OOD
detection performance highly depends on the architecture of the underlying
neural network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lambert_B/0/1/0/all/0/1&quot;&gt;Benjamin Lambert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forbes_F/0/1/0/all/0/1&quot;&gt;Florence Forbes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doyle_S/0/1/0/all/0/1&quot;&gt;Senan Doyle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dojat_M/0/1/0/all/0/1&quot;&gt;Michel Dojat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15663">
<title>CoRe Optimizer: An All-in-One Solution for Machine Learning. (arXiv:2307.15663v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15663</link>
<description rdf:parseType="Literal">&lt;p&gt;The optimization algorithm and its hyperparameters can significantly affect
the training speed and resulting model accuracy in machine learning
applications. The wish list for an ideal optimizer includes fast and smooth
convergence to low error, low computational demand, and general applicability.
Our recently introduced continual resilient (CoRe) optimizer has shown superior
performance compared to other state-of-the-art first-order gradient-based
optimizers for training lifelong machine learning potentials. In this work we
provide an extensive performance comparison of the CoRe optimizer and nine
other optimization algorithms including the Adam optimizer and resilient
backpropagation (RPROP) for diverse machine learning tasks. We analyze the
influence of different hyperparameters and provide generally applicable values.
The CoRe optimizer yields best or competitive performance in every investigated
application, while only one hyperparameter needs to be changed depending on
mini-batch or batch learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eckhoff_M/0/1/0/all/0/1&quot;&gt;Marco Eckhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reiher_M/0/1/0/all/0/1&quot;&gt;Markus Reiher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15672">
<title>Bayesian Time-Series Classifier for Decoding Simple Visual Stimuli from Intracranial Neural Activity. (arXiv:2307.15672v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15672</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding how external stimuli are encoded in distributed neural activity
is of significant interest in clinical and basic neuroscience. To address this
need, it is essential to develop analytical tools capable of handling limited
data and the intrinsic stochasticity present in neural data. In this study, we
propose a straightforward Bayesian time series classifier (BTsC) model that
tackles these challenges whilst maintaining a high level of interpretability.
We demonstrate the classification capabilities of this approach by utilizing
neural data to decode colors in a visual task. The model exhibits consistent
and reliable average performance of 75.55% on 4 patients&apos; dataset, improving
upon state-of-the-art machine learning techniques by about 3.0 percent. In
addition to its high classification accuracy, the proposed BTsC model provides
interpretable results, making the technique a valuable tool to study neural
activity in various tasks and categories. The proposed solution can be applied
to neural data recorded in various tasks, where there is a need for
interpretable results and accurate classification accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziaei_N/0/1/0/all/0/1&quot;&gt;Navid Ziaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saadatifard_R/0/1/0/all/0/1&quot;&gt;Reza Saadatifard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yousefi_A/0/1/0/all/0/1&quot;&gt;Ali Yousefi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nazari_B/0/1/0/all/0/1&quot;&gt;Behzad Nazari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cash_S/0/1/0/all/0/1&quot;&gt;Sydney S. Cash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paulk_A/0/1/0/all/0/1&quot;&gt;Angelique C. Paulk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15677">
<title>Adversarial training for tabular data with attack propagation. (arXiv:2307.15677v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15677</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial attacks are a major concern in security-centered applications,
where malicious actors continuously try to mislead Machine Learning (ML) models
into wrongly classifying fraudulent activity as legitimate, whereas system
maintainers try to stop them. Adversarially training ML models that are robust
against such attacks can prevent business losses and reduce the work load of
system maintainers. In such applications data is often tabular and the space
available for attackers to manipulate undergoes complex feature engineering
transformations, to provide useful signals for model training, to a space
attackers cannot access. Thus, we propose a new form of adversarial training
where attacks are propagated between the two spaces in the training loop. We
then test this method empirically on a real world dataset in the domain of
credit card fraud detection. We show that our method can prevent about 30%
performance drops under moderate attacks and is essential under very aggressive
attacks, with a trade-off loss in performance under no attacks smaller than 7%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melo_T/0/1/0/all/0/1&quot;&gt;Tiago Leon Melo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bravo_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Bravo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sampaio_M/0/1/0/all/0/1&quot;&gt;Marco O. P. Sampaio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romano_P/0/1/0/all/0/1&quot;&gt;Paolo Romano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_H/0/1/0/all/0/1&quot;&gt;Hugo Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ascensao_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Tiago Ascens&amp;#xe3;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bizarro_P/0/1/0/all/0/1&quot;&gt;Pedro Bizarro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15678">
<title>Case Studies of Causal Discovery from IT Monitoring Time Series. (arXiv:2307.15678v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15678</link>
<description rdf:parseType="Literal">&lt;p&gt;Information technology (IT) systems are vital for modern businesses, handling
data storage, communication, and process automation. Monitoring these systems
is crucial for their proper functioning and efficiency, as it allows collecting
extensive observational time series data for analysis. The interest in causal
discovery is growing in IT monitoring systems as knowing causal relations
between different components of the IT system helps in reducing downtime,
enhancing system performance and identifying root causes of anomalies and
incidents. It also allows proactive prediction of future issues through
historical data analysis. Despite its potential benefits, applying causal
discovery algorithms on IT monitoring data poses challenges, due to the
complexity of the data. For instance, IT monitoring data often contains
misaligned time series, sleeping time series, timestamp errors and missing
values. This paper presents case studies on applying causal discovery
algorithms to different IT monitoring datasets, highlighting benefits and
ongoing challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ait_Bachir_A/0/1/0/all/0/1&quot;&gt;Ali A&amp;#xef;t-Bachir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assaad_C/0/1/0/all/0/1&quot;&gt;Charles K. Assaad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bignicourt_C/0/1/0/all/0/1&quot;&gt;Christophe de Bignicourt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Devijver_E/0/1/0/all/0/1&quot;&gt;Emilie Devijver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_S/0/1/0/all/0/1&quot;&gt;Simon Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaussier_E/0/1/0/all/0/1&quot;&gt;Eric Gaussier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohanna_H/0/1/0/all/0/1&quot;&gt;Hosein Mohanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zan_L/0/1/0/all/0/1&quot;&gt;Lei Zan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15679">
<title>Dynamic Analysis and an Eigen Initializer for Recurrent Neural Networks. (arXiv:2307.15679v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15679</link>
<description rdf:parseType="Literal">&lt;p&gt;In recurrent neural networks, learning long-term dependency is the main
difficulty due to the vanishing and exploding gradient problem. Many
researchers are dedicated to solving this issue and they proposed many
algorithms. Although these algorithms have achieved great success,
understanding how the information decays remains an open problem. In this
paper, we study the dynamics of the hidden state in recurrent neural networks.
We propose a new perspective to analyze the hidden state space based on an
eigen decomposition of the weight matrix. We start the analysis by linear state
space model and explain the function of preserving information in activation
functions. We provide an explanation for long-term dependency based on the
eigen analysis. We also point out the different behavior of eigenvalues for
regression tasks and classification tasks. From the observations on
well-trained recurrent neural networks, we proposed a new initialization method
for recurrent neural networks, which improves consistently performance. It can
be applied to vanilla-RNN, LSTM, and GRU. We test on many datasets, such as
Tomita Grammars, pixel-by-pixel MNIST datasets, and machine translation
datasets (Multi30k). It outperforms the Xavier initializer and kaiming
initializer as well as other RNN-only initializers like IRNN and sp-RNN in
several tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_R/0/1/0/all/0/1&quot;&gt;Ran Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1&quot;&gt;Jose Principe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15682">
<title>A supervised hybrid quantum machine learning solution to the emergency escape routing problem. (arXiv:2307.15682v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2307.15682</link>
<description rdf:parseType="Literal">&lt;p&gt;Managing the response to natural disasters effectively can considerably
mitigate their devastating impact. This work explores the potential of using
supervised hybrid quantum machine learning to optimize emergency evacuation
plans for cars during natural disasters. The study focuses on earthquake
emergencies and models the problem as a dynamic computational graph where an
earthquake damages an area of a city. The residents seek to evacuate the city
by reaching the exit points where traffic congestion occurs. The situation is
modeled as a shortest-path problem on an uncertain and dynamically evolving
map. We propose a novel hybrid supervised learning approach and test it on
hypothetical situations on a concrete city graph. This approach uses a novel
quantum feature-wise linear modulation (FiLM) neural network parallel to a
classical FiLM network to imitate Dijkstra&apos;s node-wise shortest path algorithm
on a deterministic dynamic graph. Adding the quantum neural network in parallel
increases the overall model&apos;s expressivity by splitting the dataset&apos;s harmonic
and non-harmonic features between the quantum and classical components. The
hybrid supervised learning agent is trained on a dataset of Dijkstra&apos;s shortest
paths and can successfully learn the navigation task. The hybrid quantum
network improves over the purely classical supervised learning approach by 7%
in accuracy. We show that the quantum part has a significant contribution of
45.(3)% to the prediction and that the network could be executed on an
ion-based quantum computer. The results demonstrate the potential of supervised
hybrid quantum machine learning in improving emergency evacuation planning
during natural disasters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Haboury_N/0/1/0/all/0/1&quot;&gt;Nathan Haboury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kordzanganeh_M/0/1/0/all/0/1&quot;&gt;Mo Kordzanganeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Schmitt_S/0/1/0/all/0/1&quot;&gt;Sebastian Schmitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Joshi_A/0/1/0/all/0/1&quot;&gt;Ayush Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Tokarev_I/0/1/0/all/0/1&quot;&gt;Igor Tokarev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Abdallah_L/0/1/0/all/0/1&quot;&gt;Lukas Abdallah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kurkin_A/0/1/0/all/0/1&quot;&gt;Andrii Kurkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kyriacou_B/0/1/0/all/0/1&quot;&gt;Basil Kyriacou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Melnikov_A/0/1/0/all/0/1&quot;&gt;Alexey Melnikov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15690">
<title>Benchmarking Offline Reinforcement Learning on Real-Robot Hardware. (arXiv:2307.15690v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15690</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning policies from previously recorded data is a promising direction for
real-world robotics tasks, as online learning is often infeasible. Dexterous
manipulation in particular remains an open problem in its general form. The
combination of offline reinforcement learning with large diverse datasets,
however, has the potential to lead to a breakthrough in this challenging domain
analogously to the rapid progress made in supervised learning in recent years.
To coordinate the efforts of the research community toward tackling this
problem, we propose a benchmark including: i) a large collection of data for
offline learning from a dexterous manipulation platform on two tasks, obtained
with capable RL agents trained in simulation; ii) the option to execute learned
policies on a real-world robotic system and a simulation for efficient
debugging. We evaluate prominent open-sourced offline reinforcement learning
algorithms on the datasets and provide a reproducible experimental setup for
offline reinforcement learning on real systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurtler_N/0/1/0/all/0/1&quot;&gt;Nico G&amp;#xfc;rtler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blaes_S/0/1/0/all/0/1&quot;&gt;Sebastian Blaes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolev_P/0/1/0/all/0/1&quot;&gt;Pavel Kolev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Widmaier_F/0/1/0/all/0/1&quot;&gt;Felix Widmaier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wuthrich_M/0/1/0/all/0/1&quot;&gt;Manuel W&amp;#xfc;thrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1&quot;&gt;Stefan Bauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martius_G/0/1/0/all/0/1&quot;&gt;Georg Martius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15691">
<title>ODTlearn: A Package for Learning Optimal Decision Trees for Prediction and Prescription. (arXiv:2307.15691v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.15691</link>
<description rdf:parseType="Literal">&lt;p&gt;ODTLearn is an open-source Python package that provides methods for learning
optimal decision trees for high-stakes predictive and prescriptive tasks based
on the mixed-integer optimization (MIO) framework proposed in Aghaei et al.
(2019) and several of its extensions. The current version of the package
provides implementations for learning optimal classification trees, optimal
fair classification trees, optimal classification trees robust to distribution
shifts, and optimal prescriptive trees from observational data. We have
designed the package to be easy to maintain and extend as new optimal decision
tree problem classes, reformulation strategies, and solution algorithms are
introduced. To this end, the package follows object-oriented design principles
and supports both commercial (Gurobi) and open source (COIN-OR branch and cut)
solvers. The package documentation and an extensive user guide can be found at
https://d3m-research-group.github.io/odtlearn/. Additionally, users can view
the package source code and submit feature requests and bug reports by visiting
https://github.com/D3M-Research-Group/odtlearn.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vossler_P/0/1/0/all/0/1&quot;&gt;Patrick Vossler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aghaei_S/0/1/0/all/0/1&quot;&gt;Sina Aghaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Justin_N/0/1/0/all/0/1&quot;&gt;Nathan Justin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jo_N/0/1/0/all/0/1&quot;&gt;Nathanael Jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gomez_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s G&amp;#xf3;mez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vayanos_P/0/1/0/all/0/1&quot;&gt;Phebe Vayanos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15694">
<title>Universal Recurrent Event Memories for Streaming Data. (arXiv:2307.15694v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15694</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a new event memory architecture (MemNet) for
recurrent neural networks, which is universal for different types of time
series data such as scalar, multivariate or symbolic. Unlike other external
neural memory architectures, it stores key-value pairs, which separate the
information for addressing and for content to improve the representation, as in
the digital archetype. Moreover, the key-value pairs also avoid the compromise
between memory depth and resolution that applies to memories constructed by the
model state. One of the MemNet key characteristics is that it requires only
linear adaptive mapping functions while implementing a nonlinear operation on
the input data. MemNet architecture can be applied without modifications to
scalar time series, logic operators on strings, and also to natural language
processing, providing state-of-the-art results in all application domains such
as the chaotic time series, the symbolic operation tasks, and the
question-answering tasks (bAbI). Finally, controlled by five linear layers,
MemNet requires a much smaller number of training parameters than other
external memory networks as well as the transformer network. The space
complexity of MemNet equals a single self-attention layer. It greatly improves
the efficiency of the attention mechanism and opens the door for IoT
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_R/0/1/0/all/0/1&quot;&gt;Ran Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1&quot;&gt;Jose Principe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15703">
<title>Uncertainty in Natural Language Generation: From Theory to Applications. (arXiv:2307.15703v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.15703</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances of powerful Language Models have allowed Natural Language
Generation (NLG) to emerge as an important technology that can not only perform
traditional tasks like summarisation or translation, but also serve as a
natural language interface to a variety of applications. As such, it is crucial
that NLG systems are trustworthy and reliable, for example by indicating when
they are likely to be wrong; and supporting multiple views, backgrounds and
writing styles -- reflecting diverse human sub-populations. In this paper, we
argue that a principled treatment of uncertainty can assist in creating systems
and evaluation protocols better aligned with these goals. We first present the
fundamental theory, frameworks and vocabulary required to represent
uncertainty. We then characterise the main sources of uncertainty in NLG from a
linguistic perspective, and propose a two-dimensional taxonomy that is more
informative and faithful than the popular aleatoric/epistemic dichotomy.
Finally, we move from theory to applications and highlight exciting research
directions that exploit uncertainty to power decoding, controllable generation,
self-assessment, selective answering, active learning and more.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baan_J/0/1/0/all/0/1&quot;&gt;Joris Baan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daheim_N/0/1/0/all/0/1&quot;&gt;Nico Daheim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilia_E/0/1/0/all/0/1&quot;&gt;Evgenia Ilia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulmer_D/0/1/0/all/0/1&quot;&gt;Dennis Ulmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haau-Sing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_R/0/1/0/all/0/1&quot;&gt;Raquel Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1&quot;&gt;Barbara Plank&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1&quot;&gt;Rico Sennrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zerva_C/0/1/0/all/0/1&quot;&gt;Chrysoula Zerva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1&quot;&gt;Wilker Aziz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15710">
<title>Semi-Supervised Object Detection in the Open World. (arXiv:2307.15710v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15710</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing approaches for semi-supervised object detection assume a fixed set
of classes present in training and unlabeled datasets, i.e., in-distribution
(ID) data. The performance of these techniques significantly degrades when
these techniques are deployed in the open-world, due to the fact that the
unlabeled and test data may contain objects that were not seen during training,
i.e., out-of-distribution (OOD) data. The two key questions that we explore in
this paper are: can we detect these OOD samples and if so, can we learn from
them? With these considerations in mind, we propose the Open World
Semi-supervised Detection framework (OWSSD) that effectively detects OOD data
along with a semi-supervised learning pipeline that learns from both ID and OOD
data. We introduce an ensemble based OOD detector consisting of lightweight
auto-encoder networks trained only on ID data. Through extensive evalulation,
we demonstrate that our method performs competitively against state-of-the-art
OOD detection algorithms and also significantly boosts the semi-supervised
learning performance in open-world scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allabadi_G/0/1/0/all/0/1&quot;&gt;Garvita Allabadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucic_A/0/1/0/all/0/1&quot;&gt;Ana Lucic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pao_Huang_P/0/1/0/all/0/1&quot;&gt;Peter Pao-Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adve_V/0/1/0/all/0/1&quot;&gt;Vikram Adve&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15712">
<title>Quantum-noise-limited optical neural networks operating at a few quanta per activation. (arXiv:2307.15712v1 [physics.optics])</title>
<link>http://arxiv.org/abs/2307.15712</link>
<description rdf:parseType="Literal">&lt;p&gt;Analog physical neural networks, which hold promise for improved energy
efficiency and speed compared to digital electronic neural networks, are
nevertheless typically operated in a relatively high-power regime so that the
signal-to-noise ratio (SNR) is large (&amp;gt;10). What happens if an analog system is
instead operated in an ultra-low-power regime, in which the behavior of the
system becomes highly stochastic and the noise is no longer a small
perturbation on the signal? In this paper, we study this question in the
setting of optical neural networks operated in the limit where some layers use
only a single photon to cause a neuron activation. Neuron activations in this
limit are dominated by quantum noise from the fundamentally probabilistic
nature of single-photon detection of weak optical signals. We show that it is
possible to train stochastic optical neural networks to perform deterministic
image-classification tasks with high accuracy in spite of the extremely high
noise (SNR ~ 1) by using a training procedure that directly models the
stochastic behavior of photodetection. We experimentally demonstrated MNIST
classification with a test accuracy of 98% using an optical neural network with
a hidden layer operating in the single-photon regime; the optical energy used
to perform the classification corresponds to 0.008 photons per
multiply-accumulate (MAC) operation, which is equivalent to 0.003 attojoules of
optical energy per MAC. Our experiment used &amp;gt;40x fewer photons per inference
than previous state-of-the-art low-optical-energy demonstrations, to achieve
the same accuracy of &amp;gt;90%. Our work shows that some extremely stochastic analog
systems, including those operating in the limit where quantum noise dominates,
can nevertheless be used as layers in neural networks that deterministically
perform classification tasks with high accuracy if they are appropriately
trained.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shi-Yuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Laydevant_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xe9;mie Laydevant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wright_L/0/1/0/all/0/1&quot;&gt;Logan G. Wright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+McMahon_P/0/1/0/all/0/1&quot;&gt;Peter L. McMahon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2104.00641">
<title>Dynamic Silos: Increased Modularity in Intra-organizational Communication Networks during the Covid-19 Pandemic. (arXiv:2104.00641v6 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2104.00641</link>
<description rdf:parseType="Literal">&lt;p&gt;Workplace communications around the world were drastically altered by
Covid-19, related work-from-home orders, and the rise of remote work. To
understand these shifts, we analyzed aggregated, anonymized metadata from over
360 billion emails within 4,361 organizations worldwide. By comparing
month-to-month and year-over-year metrics, we examined changes in network
community structures over 24 months before and after Covid-19. We also examined
shifts across multiple communication media (email, instant messages, video
calls, and calendaring software) within a single global organization, and
compared them to communications shifts that were driven by changes in formal
organizational structure. We found that, in 2020, organizations around the
world became more siloed than in 2019, evidenced by increased modularity. This
shift was concurrent with decreased stability within silos. Collectively, our
analyses indicate that following the onset of Covid-19, employees began to
shift more dynamically between subcommunities (teams, workgroups or functional
areas). At the same time, once in a subcommunity, they limited their
communication to other members of that community. We term these network changes
dynamic silos. We provide initial insights into the meaning and implications of
dynamic silos for the future of work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zuzul_T/0/1/0/all/0/1&quot;&gt;Tiona Zuzul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pahnke_E/0/1/0/all/0/1&quot;&gt;Emily Cox Pahnke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Larson_J/0/1/0/all/0/1&quot;&gt;Jonathan Larson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bourke_P/0/1/0/all/0/1&quot;&gt;Patrick Bourke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Caurvina_N/0/1/0/all/0/1&quot;&gt;Nicholas Caurvina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Neha Parikh Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Amini_F/0/1/0/all/0/1&quot;&gt;Fereshteh Amini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Weston_J/0/1/0/all/0/1&quot;&gt;Jeffrey Weston&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Park_Y/0/1/0/all/0/1&quot;&gt;Youngser Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vogelstein_J/0/1/0/all/0/1&quot;&gt;Joshua Vogelstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+White_C/0/1/0/all/0/1&quot;&gt;Christopher White&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Priebe_C/0/1/0/all/0/1&quot;&gt;Carey E. Priebe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.08221">
<title>Fairness-aware Online Price Discrimination with Nonparametric Demand Models. (arXiv:2111.08221v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2111.08221</link>
<description rdf:parseType="Literal">&lt;p&gt;Price discrimination, which refers to the strategy of setting different
prices for different customer groups, has been widely used in online retailing.
Although it helps boost the collected revenue for online retailers, it might
create serious concerns about fairness, which even violates the regulation and
laws. This paper studies the problem of dynamic discriminatory pricing under
fairness constraints. In particular, we consider a finite selling horizon of
length $T$ for a single product with two groups of customers. Each group of
customers has its unknown demand function that needs to be learned. For each
selling period, the seller determines the price for each group and observes
their purchase behavior. While existing literature mainly focuses on maximizing
revenue, ensuring fairness among different customers has not been fully
explored in the dynamic pricing literature. This work adopts the fairness
notion from Cohen et al. (2022). For price fairness, we propose an optimal
dynamic pricing policy regarding regret, which enforces the strict price
fairness constraint. In contrast to the standard $\sqrt{T}$-type regret in
online learning, we show that the optimal regret in our case is
$\tilde{O}(T^{4/5})$. We further extend our algorithm to a more general notion
of fairness, which includes demand fairness as a special case. To handle this
general class, we propose a soft fairness constraint and develop a dynamic
pricing policy that achieves $\tilde{O}(T^{4/5})$ regret. We also demonstrate
that our algorithmic techniques can be adapted to more general scenarios such
as fairness among multiple groups of customers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1&quot;&gt;Jiameng Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.02419">
<title>Learning a Discrete Set of Optimal Allocation Rules in a Queueing System with Unknown Service Rate. (arXiv:2202.02419v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2202.02419</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by the wide range of modern applications of the Erlang-B blocking
model beyond communication networks and call centers to sizing and pricing in
design production systems, messaging systems, and app-based parking systems, we
study admission control for such a system but with unknown arrival and service
rates. In our model, at every job arrival, a dispatcher decides to assign the
job to an available server or block it. Every served job yields a fixed reward
for the dispatcher, but it also results in a cost per unit time of service. Our
goal is to design a dispatching policy that maximizes the long-term average
reward for the dispatcher based on observing only the arrival times and the
state of the system at each arrival that reflects a realistic sampling of such
systems. Critically, the dispatcher observes neither the service times nor
departure times so that standard reinforcement learning-based approaches that
use reward signals do not apply. Hence, we develop our learning-based dispatch
scheme as a parametric learning problem a&apos;la self-tuning adaptive control. In
our problem, certainty equivalent control switches between an always admit if
room policy (explore infinitely often) and a never admit policy (immediately
terminate learning), which is distinct from the adaptive control literature.
Hence, our learning scheme judiciously uses the always admit if room policy so
that learning doesn&apos;t stall. We prove that for all service rates, the proposed
policy asymptotically learns to take the optimal action and present finite-time
regret guarantees. The extreme contrast in the certainty equivalent optimal
control policies leads to difficulties in learning that show up in our regret
bounds for different parameter regimes: constant regret in one regime versus
regret growing logarithmically in the other.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Adler_S/0/1/0/all/0/1&quot;&gt;Saghar Adler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moharrami_M/0/1/0/all/0/1&quot;&gt;Mehrdad Moharrami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Subramanian_V/0/1/0/all/0/1&quot;&gt;Vijay Subramanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.01696">
<title>Fail-Safe Adversarial Generative Imitation Learning. (arXiv:2203.01696v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2203.01696</link>
<description rdf:parseType="Literal">&lt;p&gt;For flexible yet safe imitation learning (IL), we propose theory and a
modular method, with a safety layer that enables a closed-form probability
density/gradient of the safe generative continuous policy, end-to-end
generative adversarial training, and worst-case safety guarantees. The safety
layer maps all actions into a set of safe actions, and uses the
change-of-variables formula plus additivity of measures for the density. The
set of safe actions is inferred by first checking safety of a finite sample of
actions via adversarial reachability analysis of fallback maneuvers, and then
concluding on the safety of these actions&apos; neighborhoods using, e.g., Lipschitz
continuity. We provide theoretical analysis showing the robustness advantage of
using the safety layer already during training (imitation error linear in the
horizon) compared to only using it at test time (up to quadratic error). In an
experiment on real-world driver interaction data, we empirically demonstrate
tractability, safety and imitation performance of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_P/0/1/0/all/0/1&quot;&gt;Philipp Geiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Straehle_C/0/1/0/all/0/1&quot;&gt;Christoph-Nikolas Straehle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.12672">
<title>SKTR: Trace Recovery from Stochastically Known Logs. (arXiv:2206.12672v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.12672</link>
<description rdf:parseType="Literal">&lt;p&gt;Developments in machine learning together with the increasing usage of sensor
data challenge the reliance on deterministic logs, requiring new process mining
solutions for uncertain, and in particular stochastically known, logs. In this
work we formulate {trace recovery}, the task of generating a deterministic log
from stochastically known logs that is as faithful to reality as possible. An
effective trace recovery algorithm would be a powerful aid for maintaining
credible process mining tools for uncertain settings. We propose an algorithmic
framework for this task that recovers the best alignment between a
stochastically known log and a process model, with three innovative features.
Our algorithm, SKTR, 1) handles both Markovian and non-Markovian processes; 2)
offers a quality-based balance between a process model and a log, depending on
the available process information, sensor quality, and machine learning
predictiveness power; and 3) offers a novel use of a synchronous product
multigraph to create the log. An empirical analysis using five publicly
available datasets, three of which use predictive models over standard video
capturing benchmarks, shows an average relative accuracy improvement of more
than 10 over a common baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogdanov_E/0/1/0/all/0/1&quot;&gt;Eli Bogdanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1&quot;&gt;Izack Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gal_A/0/1/0/all/0/1&quot;&gt;Avigdor Gal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.05606">
<title>Multi-fidelity wavelet neural operator with application to uncertainty quantification. (arXiv:2208.05606v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.05606</link>
<description rdf:parseType="Literal">&lt;p&gt;Operator learning frameworks, because of their ability to learn nonlinear
maps between two infinite dimensional functional spaces and utilization of
neural networks in doing so, have recently emerged as one of the more pertinent
areas in the field of applied machine learning. Although these frameworks are
extremely capable when it comes to modeling complex phenomena, they require an
extensive amount of data for successful training which is often not available
or is too expensive. However, this issue can be alleviated with the use of
multi-fidelity learning, where a model is trained by making use of a large
amount of inexpensive low-fidelity data along with a small amount of expensive
high-fidelity data. To this end, we develop a new framework based on the
wavelet neural operator which is capable of learning from a multi-fidelity
dataset. The developed model&apos;s excellent learning capabilities are demonstrated
by solving different problems which require effective correlation learning
between the two fidelities for surrogate construction. Furthermore, we also
assess the application of the developed framework for uncertainty
quantification. The results obtained from this work illustrate the excellent
performance of the proposed framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1&quot;&gt;Akshay Thakur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tripura_T/0/1/0/all/0/1&quot;&gt;Tapas Tripura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1&quot;&gt;Souvik Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.07734">
<title>Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success. (arXiv:2208.07734v7 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.07734</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning (SSL) has emerged as a promising alternative to
create supervisory signals to real-world problems, avoiding the extensive cost
of manual labeling. SSL is particularly attractive for unsupervised tasks such
as anomaly detection (AD), where labeled anomalies are rare or often
nonexistent. A large catalog of augmentation functions has been used for
SSL-based AD (SSAD) on image data, and recent works have reported that the type
of augmentation has a significant impact on accuracy. Motivated by those, this
work sets out to put image-based SSAD under a larger lens and investigate the
role of data augmentation in SSAD. Through extensive experiments on 3 different
detector models and across 420 AD tasks, we provide comprehensive numerical and
visual evidences that the alignment between data augmentation and
anomaly-generating mechanism is the key to the success of SSAD, and in the lack
thereof, SSL may even impair accuracy. To the best of our knowledge, this is
the first meta-analysis on the role of data augmentation in SSAD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1&quot;&gt;Jaemin Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tiancheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akoglu_L/0/1/0/all/0/1&quot;&gt;Leman Akoglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.09189">
<title>Cross-Domain Evaluation of a Deep Learning-Based Type Inference System. (arXiv:2208.09189v4 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2208.09189</link>
<description rdf:parseType="Literal">&lt;p&gt;Optional type annotations allow for enriching dynamic programming languages
with static typing features like better Integrated Development Environment
(IDE) support, more precise program analysis, and early detection and
prevention of type-related runtime errors. Machine learning-based type
inference promises interesting results for automating this task. However, the
practical usage of such systems depends on their ability to generalize across
different domains, as they are often applied outside their training domain. In
this work, we investigate Type4Py as a representative of state-of-the-art deep
learning-based type inference systems, by conducting extensive cross-domain
experiments. Thereby, we address the following problems: class imbalances,
out-of-vocabulary words, dataset shifts, and unknown classes. To perform such
experiments, we use the datasets ManyTypes4Py and CrossDomainTypes4Py. The
latter we introduce in this paper. Our dataset enables the evaluation of type
inference systems in different domains of software projects and has over
1,000,000 type annotations mined on the platforms GitHub and Libraries. It
consists of data from the two domains web development and scientific
calculation. Through our experiments, we detect that the shifts in the dataset
and the long-tailed distribution with many rare and unknown data types decrease
the performance of the deep learning-based type inference system drastically.
In this context, we test unsupervised domain adaptation methods and fine-tuning
to overcome these issues. Moreover, we investigate the impact of
out-of-vocabulary words.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gruner_B/0/1/0/all/0/1&quot;&gt;Bernd Gruner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonnekalb_T/0/1/0/all/0/1&quot;&gt;Tim Sonnekalb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinze_T/0/1/0/all/0/1&quot;&gt;Thomas S. Heinze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brust_C/0/1/0/all/0/1&quot;&gt;Clemens-Alexander Brust&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.10043">
<title>SynthA1c: Towards Clinically Interpretable Patient Representations for Diabetes Risk Stratification. (arXiv:2209.10043v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.10043</link>
<description rdf:parseType="Literal">&lt;p&gt;Early diagnosis of Type 2 Diabetes Mellitus (T2DM) is crucial to enable
timely therapeutic interventions and lifestyle modifications. As the time
available for clinical office visits shortens and medical imaging data become
more widely available, patient image data could be used to opportunistically
identify patients for additional T2DM diagnostic workup by physicians. We
investigated whether image-derived phenotypic data could be leveraged in
tabular learning classifier models to predict T2DM risk in an automated fashion
to flag high-risk patients without the need for additional blood laboratory
measurements. In contrast to traditional binary classifiers, we leverage neural
networks and decision tree models to represent patient data as &apos;SynthA1c&apos;
latent variables, which mimic blood hemoglobin A1c empirical lab measurements,
that achieve sensitivities as high as 87.6%. To evaluate how SynthA1c models
may generalize to other patient populations, we introduce a novel generalizable
metric that uses vanilla data augmentation techniques to predict model
performance on input out-of-domain covariates. We show that image-derived
phenotypes and physical examination data together can accurately predict
diabetes risk as a means of opportunistic risk stratification enabled by
artificial intelligence and medical imaging. Our code is available at
https://github.com/allisonjchae/DMT2RiskAssessment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1&quot;&gt;Michael S. Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chae_A/0/1/0/all/0/1&quot;&gt;Allison Chae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacLean_M/0/1/0/all/0/1&quot;&gt;Matthew T. MacLean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1&quot;&gt;Anurag Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duda_J/0/1/0/all/0/1&quot;&gt;Jeffrey Duda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1&quot;&gt;James Gee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torigian_D/0/1/0/all/0/1&quot;&gt;Drew A. Torigian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rader_D/0/1/0/all/0/1&quot;&gt;Daniel Rader&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahn_C/0/1/0/all/0/1&quot;&gt;Charles Kahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witschey_W/0/1/0/all/0/1&quot;&gt;Walter R. Witschey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sagreiya_H/0/1/0/all/0/1&quot;&gt;Hersh Sagreiya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.14272">
<title>Towards Multimodal Prediction of Spontaneous Humour: A Novel Dataset and First Results. (arXiv:2209.14272v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.14272</link>
<description rdf:parseType="Literal">&lt;p&gt;Humour is a substantial element of human affect and cognition. Its automatic
understanding can facilitate a more naturalistic human-device interaction and
the humanisation of artificial intelligence. Current methods of humour
detection are solely based on staged data making them inadequate for
&apos;real-world&apos; applications. We address this deficiency by introducing the novel
Passau-Spontaneous Football Coach Humour (Passau-SFCH) dataset, comprising of
about 11 hours of recordings. The Passau-SFCH dataset is annotated for the
presence of humour and its dimensions (sentiment and direction) as proposed in
Martin&apos;s Humor Style Questionnaire. We conduct a series of experiments,
employing pretrained Transformers, convolutional neural networks, and
expert-designed features. The performance of each modality (text, audio, video)
for spontaneous humour recognition is analysed and their complementarity is
investigated. Our findings suggest that for the automatic analysis of humour
and its sentiment, facial expressions are most promising, while humour
direction can be best modelled via text-based features. The results reveal
considerable differences among various subjects, highlighting the individuality
of humour usage and style. Further, we observe that a decision-level fusion
yields the best recognition result. Finally, we make our code publicly
available at https://www.github.com/EIHW/passau-sfch. The Passau-SFCH dataset
is available upon request.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christ_L/0/1/0/all/0/1&quot;&gt;Lukas Christ&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amiriparian_S/0/1/0/all/0/1&quot;&gt;Shahin Amiriparian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kathan_A/0/1/0/all/0/1&quot;&gt;Alexander Kathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_N/0/1/0/all/0/1&quot;&gt;Niklas M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konig_A/0/1/0/all/0/1&quot;&gt;Andreas K&amp;#xf6;nig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn W. Schuller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.05304">
<title>Learning Provably Stabilizing Neural Controllers for Discrete-Time Stochastic Systems. (arXiv:2210.05304v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.05304</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of learning control policies in discrete-time
stochastic systems which guarantee that the system stabilizes within some
specified stabilization region with probability~$1$. Our approach is based on
the novel notion of stabilizing ranking supermartingales (sRSMs) that we
introduce in this work. Our sRSMs overcome the limitation of methods proposed
in previous works whose applicability is restricted to systems in which the
stabilizing region cannot be left once entered under any control policy. We
present a learning procedure that learns a control policy together with an sRSM
that formally certifies probability~$1$ stability, both learned as neural
networks. We show that this procedure can also be adapted to formally verifying
that, under a given Lipschitz continuous control policy, the stochastic system
stabilizes within some stabilizing region with probability~$1$. Our
experimental evaluation shows that our learning procedure can successfully
learn provably stabilizing policies in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ansaripour_M/0/1/0/all/0/1&quot;&gt;Matin Ansaripour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_K/0/1/0/all/0/1&quot;&gt;Krishnendu Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henzinger_T/0/1/0/all/0/1&quot;&gt;Thomas A. Henzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1&quot;&gt;Mathias Lechner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zikelic_%7B/0/1/0/all/0/1&quot;&gt;&amp;#x110;or&amp;#x111;e &amp;#x17d;ikeli&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.10890">
<title>Mitigating spectral bias for the multiscale operator learning with hierarchical attention. (arXiv:2210.10890v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.10890</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural operators have emerged as a powerful tool for learning the mapping
between infinite-dimensional parameter and solution spaces of partial
differential equations (PDEs). In this work, we focus on multiscale PDEs that
have important applications such as reservoir modeling and turbulence
prediction. We demonstrate that for such PDEs, the spectral bias towards
low-frequency components presents a significant challenge for existing neural
operators. To address this challenge, we propose a hierarchical attention
neural operator (HANO) inspired by the hierarchical matrix approach. HANO
features a scale-adaptive interaction range and self-attentions over a
hierarchy of levels, enabling nested feature computation with controllable
linear cost and encoding/decoding of multiscale solution space. We also
incorporate an empirical $H^1$ loss function to enhance the learning of
high-frequency components. Our numerical experiments demonstrate that HANO
outperforms state-of-the-art (SOTA) methods for representative multiscale
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Bo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.04965">
<title>Resource frugal optimizer for quantum machine learning. (arXiv:2211.04965v3 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2211.04965</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantum-enhanced data science, also known as quantum machine learning (QML),
is of growing interest as an application of near-term quantum computers.
Variational QML algorithms have the potential to solve practical problems on
real hardware, particularly when involving quantum data. However, training
these algorithms can be challenging and calls for tailored optimization
procedures. Specifically, QML applications can require a large shot-count
overhead due to the large datasets involved. In this work, we advocate for
simultaneous random sampling over both the dataset as well as the measurement
operators that define the loss function. We consider a highly general loss
function that encompasses many QML applications, and we show how to construct
an unbiased estimator of its gradient. This allows us to propose a shot-frugal
gradient descent optimizer called Refoqus (REsource Frugal Optimizer for
QUantum Stochastic gradient descent). Our numerics indicate that Refoqus can
save several orders of magnitude in shot cost, even relative to optimizers that
sample over measurement operators alone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Moussa_C/0/1/0/all/0/1&quot;&gt;Charles Moussa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Gordon_M/0/1/0/all/0/1&quot;&gt;Max Hunter Gordon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Baczyk_M/0/1/0/all/0/1&quot;&gt;Michal Baczyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Cerezo_M/0/1/0/all/0/1&quot;&gt;M. Cerezo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Cincio_L/0/1/0/all/0/1&quot;&gt;Lukasz Cincio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Coles_P/0/1/0/all/0/1&quot;&gt;Patrick J. Coles&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11891">
<title>A Bi-level Nonlinear Eigenvector Algorithm for Wasserstein Discriminant Analysis. (arXiv:2211.11891v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11891</link>
<description rdf:parseType="Literal">&lt;p&gt;Much like the classical Fisher linear discriminant analysis (LDA), the
recently proposed Wasserstein discriminant analysis (WDA) is a linear
dimensionality reduction method that seeks a projection matrix to maximize the
dispersion of different data classes and minimize the dispersion of same data
classes via a bi-level optimization. In contrast to LDA, WDA can account for
both global and local interconnections between data classes by using the
underlying principles of optimal transport. In this paper, a bi-level nonlinear
eigenvector algorithm (WDA-nepv) is presented to fully exploit the structures
of the bi-level optimization of WDA. The inner level of WDA-nepv for computing
the optimal transport matrices is formulated as an eigenvector-dependent
nonlinear eigenvalue problem (NEPv), and meanwhile, the outer level for trace
ratio optimizations is formulated as another NEPv. Both NEPvs can be computed
efficiently under the self-consistent field (SCF) framework. WDA-nepv is
derivative-free and surrogate-model-free when compared with existing
algorithms. Convergence analysis of the proposed WDA-nepv justifies the
utilization of the SCF for solving the bi-level optimization of WDA. Numerical
experiments with synthetic and real-life datasets demonstrate the
classification accuracy and scalability of WDA-nepv.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roh_D/0/1/0/all/0/1&quot;&gt;Dong Min Roh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bai_Z/0/1/0/all/0/1&quot;&gt;Zhaojun Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ren-Cang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13289">
<title>Shapley Curves: A Smoothing Perspective. (arXiv:2211.13289v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13289</link>
<description rdf:parseType="Literal">&lt;p&gt;Originating from cooperative game theory, Shapley values have become one of
the most widely used measures for variable importance in applied Machine
Learning. However, the statistical understanding of Shapley values is still
limited. In this paper, we take a nonparametric (or smoothing) perspective by
introducing Shapley curves as a local measure of variable importance. We
consider two estimation strategies and derive the consistency and asymptotic
normality both under independence and dependence among the features. We further
propose a novel version of the wild bootstrap procedure specifically adjusted
for Shapley curves. This allows us to construct confidence intervals and
conduct inference. The asymptotic results are validated in extensive
experiments. In an empirical application, we analyze which attributes drive the
prices of vehicles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Miftachov_R/0/1/0/all/0/1&quot;&gt;Ratmir Miftachov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Keilbar_G/0/1/0/all/0/1&quot;&gt;Georg Keilbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hardle_W/0/1/0/all/0/1&quot;&gt;Wolfgang Karl H&amp;#xe4;rdle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10839">
<title>Consistent Range Approximation for Fair Predictive Modeling. (arXiv:2212.10839v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10839</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel framework for certifying the fairness of
predictive models trained on biased data. It draws from query answering for
incomplete and inconsistent databases to formulate the problem of consistent
range approximation (CRA) of fairness queries for a predictive model on a
target population. The framework employs background knowledge of the data
collection process and biased data, working with or without limited statistics
about the target population, to compute a range of answers for fairness
queries. Using CRA, the framework builds predictive models that are certifiably
fair on the target population, regardless of the availability of external data
during training. The framework&apos;s efficacy is demonstrated through evaluations
on real data, showing substantial improvement over existing state-of-the-art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jiongli Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galhotra_S/0/1/0/all/0/1&quot;&gt;Sainyam Galhotra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabri_N/0/1/0/all/0/1&quot;&gt;Nazanin Sabri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salimi_B/0/1/0/all/0/1&quot;&gt;Babak Salimi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.04253">
<title>Towards Answering Climate Questionnaires from Unstructured Climate Reports. (arXiv:2301.04253v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2301.04253</link>
<description rdf:parseType="Literal">&lt;p&gt;The topic of Climate Change (CC) has received limited attention in NLP
despite its urgency. Activists and policymakers need NLP tools to effectively
process the vast and rapidly growing unstructured textual climate reports into
structured form. To tackle this challenge we introduce two new large-scale
climate questionnaire datasets and use their existing structure to train
self-supervised models. We conduct experiments to show that these models can
learn to generalize to climate disclosures of different organizations types
than seen during training. We then use these models to help align texts from
unstructured climate documents to the semi-structured questionnaires in a human
pilot study. Finally, to support further NLP research in the climate domain we
introduce a benchmark of existing climate text classification datasets to
better evaluate and compare existing models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spokoyny_D/0/1/0/all/0/1&quot;&gt;Daniel Spokoyny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laud_T/0/1/0/all/0/1&quot;&gt;Tanmay Laud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corringham_T/0/1/0/all/0/1&quot;&gt;Tom Corringham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1&quot;&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.08164">
<title>DiME: Maximizing Mutual Information by a Difference of Matrix-Based Entropies. (arXiv:2301.08164v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.08164</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce an information-theoretic quantity with similar properties to
mutual information that can be estimated from data without making explicit
assumptions on the underlying distribution. This quantity is based on a
recently proposed matrix-based entropy that uses the eigenvalues of a
normalized Gram matrix to compute an estimate of the eigenvalues of an
uncentered covariance operator in a reproducing kernel Hilbert space. We show
that a difference of matrix-based entropies (DiME) is well suited for problems
involving the maximization of mutual information between random variables.
While many methods for such tasks can lead to trivial solutions, DiME naturally
penalizes such outcomes. We compare DiME to several baseline estimators of
mutual information on a toy Gaussian dataset. We provide examples of use cases
for DiME, such as latent factor disentanglement and a multiview representation
learning problem where DiME is used to learn a shared representation among
views with high mutual information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skean_O/0/1/0/all/0/1&quot;&gt;Oscar Skean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osorio_J/0/1/0/all/0/1&quot;&gt;Jhoan Keider Hoyos Osorio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brockmeier_A/0/1/0/all/0/1&quot;&gt;Austin J. Brockmeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giraldo_L/0/1/0/all/0/1&quot;&gt;Luis Gonzalo Sanchez Giraldo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.12717">
<title>Automatic Intersection Management in Mixed Traffic Using Reinforcement Learning and Graph Neural Networks. (arXiv:2301.12717v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2301.12717</link>
<description rdf:parseType="Literal">&lt;p&gt;Connected automated driving has the potential to significantly improve urban
traffic efficiency, e.g., by alleviating issues due to occlusion. Cooperative
behavior planning can be employed to jointly optimize the motion of multiple
vehicles. Most existing approaches to automatic intersection management,
however, only consider fully automated traffic. In practice, mixed traffic,
i.e., the simultaneous road usage by automated and human-driven vehicles, will
be prevalent. The present work proposes to leverage reinforcement learning and
a graph-based scene representation for cooperative multi-agent planning. We
build upon our previous works that showed the applicability of such machine
learning methods to fully automated traffic. The scene representation is
extended for mixed traffic and considers uncertainty in the human drivers&apos;
intentions. In the simulation-based evaluation, we model measurement
uncertainties through noise processes that are tuned using real-world data. The
paper evaluates the proposed method against an enhanced first in - first out
scheme, our baseline for mixed traffic management. With increasing share of
automated vehicles, the learned planner significantly increases the vehicle
throughput and reduces the delay due to interaction. Non-automated vehicles
benefit virtually alike.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klimke_M/0/1/0/all/0/1&quot;&gt;Marvin Klimke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volz_B/0/1/0/all/0/1&quot;&gt;Benjamin V&amp;#xf6;lz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buchholz_M/0/1/0/all/0/1&quot;&gt;Michael Buchholz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08854">
<title>Post-Episodic Reinforcement Learning Inference. (arXiv:2302.08854v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08854</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider estimation and inference with data collected from episodic
reinforcement learning (RL) algorithms; i.e. adaptive experimentation
algorithms that at each period (aka episode) interact multiple times in a
sequential manner with a single treated unit. Our goal is to be able to
evaluate counterfactual adaptive policies after data collection and to estimate
structural parameters such as dynamic treatment effects, which can be used for
credit assignment (e.g. what was the effect of the first period action on the
final outcome). Such parameters of interest can be framed as solutions to
moment equations, but not minimizers of a population loss function, leading to
$Z$-estimation approaches in the case of static data. However, such estimators
fail to be asymptotically normal in the case of adaptive data collection. We
propose a re-weighted $Z$-estimation approach with carefully designed adaptive
weights to stabilize the episode-varying estimation variance, which results
from the nonstationary policy that typical episodic RL algorithms invoke. We
identify proper weighting schemes to restore the consistency and asymptotic
normality of the re-weighted Z-estimators for target parameters, which allows
for hypothesis testing and constructing uniform confidence regions for target
parameters of interest. Primary applications include dynamic treatment effect
estimation and dynamic off-policy evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Syrgkanis_V/0/1/0/all/0/1&quot;&gt;Vasilis Syrgkanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhan_R/0/1/0/all/0/1&quot;&gt;Ruohan Zhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.09394">
<title>Deep Neural Networks based Meta-Learning for Network Intrusion Detection. (arXiv:2302.09394v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.09394</link>
<description rdf:parseType="Literal">&lt;p&gt;The digitization of different components of industry and inter-connectivity
among indigenous networks have increased the risk of network attacks. Designing
an intrusion detection system to ensure security of the industrial ecosystem is
difficult as network traffic encompasses various attack types, including new
and evolving ones with minor changes. The data used to construct a predictive
model for computer networks has a skewed class distribution and limited
representation of attack types, which differ from real network traffic. These
limitations result in dataset shift, negatively impacting the machine learning
models&apos; predictive abilities and reducing the detection rate against novel
attacks. To address the challenges, we propose a novel deep neural network
based Meta-Learning framework; INformation FUsion and Stacking Ensemble
(INFUSE) for network intrusion detection. First, a hybrid feature space is
created by integrating decision and feature spaces. Five different classifiers
are utilized to generate a pool of decision spaces. The feature space is then
enriched through a deep sparse autoencoder that learns the semantic
relationships between attacks. Finally, the deep Meta-Learner acts as an
ensemble combiner to analyze the hybrid feature space and make a final
decision. Our evaluation on stringent benchmark datasets and comparison to
existing techniques showed the effectiveness of INFUSE with an F-Score of 0.91,
Accuracy of 91.6%, and Recall of 0.94 on the Test+ dataset, and an F-Score of
0.91, Accuracy of 85.6%, and Recall of 0.87 on the stringent Test-21 dataset.
These promising results indicate the strong generalization capability and the
potential to detect network attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohail_A/0/1/0/all/0/1&quot;&gt;Anabia Sohail&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayisha_B/0/1/0/all/0/1&quot;&gt;Bibi Ayisha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hameed_I/0/1/0/all/0/1&quot;&gt;Irfan Hameed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1&quot;&gt;Muhammad Mohsin Zafar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alquhayz_H/0/1/0/all/0/1&quot;&gt;Hani Alquhayz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Asifullah Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10909">
<title>Multi-modal Machine Learning in Engineering Design: A Review and Future Directions. (arXiv:2302.10909v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10909</link>
<description rdf:parseType="Literal">&lt;p&gt;In the rapidly advancing field of multi-modal machine learning (MMML), the
convergence of multiple data modalities has the potential to reshape various
applications. This paper presents a comprehensive overview of the current
state, advancements, and challenges of MMML within the sphere of engineering
design. The review begins with a deep dive into five fundamental concepts of
MMML:multi-modal information representation, fusion, alignment, translation,
and co-learning. Following this, we explore the cutting-edge applications of
MMML, placing a particular emphasis on tasks pertinent to engineering design,
such as cross-modal synthesis, multi-modal prediction, and cross-modal
information retrieval. Through this comprehensive overview, we highlight the
inherent challenges in adopting MMML in engineering design, and proffer
potential directions for future research. To spur on the continued evolution of
MMML in engineering design, we advocate for concentrated efforts to construct
extensive multi-modal design datasets, develop effective data-driven MMML
techniques tailored to design applications, and enhance the scalability and
interpretability of MMML models. MMML models, as the next generation of
intelligent design tools, hold a promising future to impact how products are
designed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1&quot;&gt;Binyang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1&quot;&gt;Rui Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1&quot;&gt;Faez Ahmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12202">
<title>A Definition of Non-Stationary Bandits. (arXiv:2302.12202v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12202</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the subject of non-stationary bandit learning having attracted much
recent attention, we have yet to identify a formal definition of
non-stationarity that can consistently distinguish non-stationary bandits from
stationary ones. Prior work has characterized non-stationary bandits as bandits
for which the reward distribution changes over time. We demonstrate that this
definition can ambiguously classify the same bandit as both stationary and
non-stationary; this ambiguity arises in the existing definition&apos;s dependence
on the latent sequence of reward distributions. Moreover, the definition has
given rise to two widely used notions of regret: the dynamic regret and the
weak regret. These notions are not indicative of qualitative agent performance
in some bandits. Additionally, this definition of non-stationary bandits has
led to the design of agents that explore excessively. We introduce a formal
definition of non-stationary bandits that resolves these issues. Our new
definition provides a unified approach, applicable seamlessly to both Bayesian
and frequentist formulations of bandits. Furthermore, our definition ensures
consistent classification of two bandits offering agents indistinguishable
experiences, categorizing them as either both stationary or both
non-stationary. This advancement provides a more robust framework for
non-stationary bandit learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yueyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_X/0/1/0/all/0/1&quot;&gt;Xu Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1&quot;&gt;Benjamin Van Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12247">
<title>Quantifying &amp; Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12247</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent explosion of interest in multimodal applications has resulted in a
wide selection of datasets and methods for representing and integrating
information from different modalities. Despite these empirical advances, there
remain fundamental research questions: How can we quantify the interactions
that are necessary to solve a multimodal task? Subsequently, what are the most
suitable multimodal models to capture these interactions? To answer these
questions, we propose an information-theoretic approach to quantify the degree
of redundancy, uniqueness, and synergy relating input modalities with an output
task. We term these three measures as the PID statistics of a multimodal
distribution (or PID for short), and introduce two new estimators for these PID
statistics that scale to high-dimensional distributions. To validate PID
estimation, we conduct extensive experiments on both synthetic datasets where
the PID is known and on large-scale multimodal benchmarks where PID estimations
are compared with human annotations. Finally, we demonstrate their usefulness
in (1) quantifying interactions within multimodal datasets, (2) quantifying
interactions captured by multimodal models, (3) principled approaches for model
selection, and (4) three real-world case studies engaging with domain experts
in pathology, mood prediction, and robotic perception where our framework helps
to recommend strong multimodal models for each application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yun Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xiang Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1&quot;&gt;Chun Kai Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1&quot;&gt;Suzanne Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Richard Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zihao Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1&quot;&gt;Nicholas Allen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1&quot;&gt;Randy Auerbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1&quot;&gt;Faisal Mahmood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1&quot;&gt;Louis-Philippe Morency&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01170">
<title>Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning. (arXiv:2303.01170v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01170</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning in Reinforcement Learning (RL) has been widely studied to
overcome training issues of Deep-RL, i.e., exploration cost, data availability
and convergence time, by introducing a way to enhance training phase with
external knowledge. Generally, knowledge is transferred from expert-agents to
novices. While this fixes the issue for a novice agent, a good understanding of
the task on expert agent is required for such transfer to be effective. As an
alternative, in this paper we propose Expert-Free Online Transfer Learning
(EF-OnTL), an algorithm that enables expert-free real-time dynamic transfer
learning in multi-agent system. No dedicated expert exists, and transfer source
agent and knowledge to be transferred are dynamically selected at each transfer
step based on agents&apos; performance and uncertainty. To improve uncertainty
estimation, we also propose State Action Reward Next-State Random Network
Distillation (sars-RND), an extension of RND that estimates uncertainty from RL
agent-environment interaction. We demonstrate EF-OnTL effectiveness against a
no-transfer scenario and advice-based baselines, with and without expert
agents, in three benchmark tasks: Cart-Pole, a grid-based Multi-Team
Predator-Prey (mt-pp) and Half Field Offense (HFO). Our results show that
EF-OnTL achieve overall comparable performance when compared against
advice-based baselines while not requiring any external input nor threshold
tuning. EF-OnTL outperforms no-transfer with an improvement related to the
complexity of the task addressed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castagna_A/0/1/0/all/0/1&quot;&gt;Alberto Castagna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dusparic_I/0/1/0/all/0/1&quot;&gt;Ivana Dusparic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07189">
<title>Optimizing Convolutional Neural Networks for Chronic Obstructive Pulmonary Disease Detection in Clinical Computed Tomography Imaging. (arXiv:2303.07189v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07189</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: To optimize the binary detection of Chronic Obstructive Pulmonary
Disease (COPD) based on emphysema presence in the lung with convolutional
neural networks (CNN) by exploring manually adjusted versus automated
window-setting optimization (WSO) on computed tomography (CT) images.
&lt;/p&gt;
&lt;p&gt;Methods: 7,194 CT images (3,597 with COPD; 3,597 healthy controls) from 78
subjects (43 with COPD; 35 healthy controls) were selected retrospectively
(10.2018-12.2019) and preprocessed. For each image, intensity values were
manually clipped to the emphysema window setting and a baseline &apos;full-range&apos;
window setting. Class-balanced train, validation, and test sets contained
3,392, 1,114, and 2,688 images. The network backbone was optimized by comparing
various CNN architectures. Furthermore, automated WSO was implemented by adding
a customized layer to the model. The image-level area under the Receiver
Operating Characteristics curve (AUC) [lower, upper limit 95% confidence] and
P-values calculated from one-sided Mann-Whitney U-test were utilized to compare
model variations.
&lt;/p&gt;
&lt;p&gt;Results: Repeated inference (n=7) on the test set showed that the DenseNet
was the most efficient backbone and achieved a mean AUC of 0.80 [0.76, 0.85]
without WSO. Comparably, with input images manually adjusted to the emphysema
window, the DenseNet model predicted COPD with a mean AUC of 0.86 [0.82, 0.89]
(P=0.03). By adding a customized WSO layer to the DenseNet, an optimal window
in the proximity of the emphysema window setting was learned automatically, and
a mean AUC of 0.82 [0.78, 0.86] was achieved.
&lt;/p&gt;
&lt;p&gt;Conclusion: Detection of COPD with DenseNet models was improved by WSO of CT
data to the emphysema window setting range.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dorosti_T/0/1/0/all/0/1&quot;&gt;Tina Dorosti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schultheiss_M/0/1/0/all/0/1&quot;&gt;Manuel Schultheiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hofmann_F/0/1/0/all/0/1&quot;&gt;Felix Hofmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thalhammer_J/0/1/0/all/0/1&quot;&gt;Johannes Thalhammer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kirchner_L/0/1/0/all/0/1&quot;&gt;Luisa Kirchner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Urban_T/0/1/0/all/0/1&quot;&gt;Theresa Urban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pfeiffer_F/0/1/0/all/0/1&quot;&gt;Franz Pfeiffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schaff_F/0/1/0/all/0/1&quot;&gt;Florian Schaff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lasser_T/0/1/0/all/0/1&quot;&gt;Tobias Lasser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pfeiffer_D/0/1/0/all/0/1&quot;&gt;Daniela Pfeiffer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.00818">
<title>Swarm Reinforcement Learning For Adaptive Mesh Refinement. (arXiv:2304.00818v2 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/2304.00818</link>
<description rdf:parseType="Literal">&lt;p&gt;The Finite Element Method, an important technique in engineering, is aided by
Adaptive Mesh Refinement (AMR), which dynamically refines mesh regions to allow
for a favorable trade-off between computational speed and simulation accuracy.
Classical methods for AMR depend on task-specific heuristics or expensive error
estimators, hindering their use for complex simulations. Recent learned AMR
methods tackle these problems, but so far scale only to simple toy examples. We
formulate AMR as a novel Adaptive Swarm Markov Decision Process in which a mesh
is modeled as a system of simple collaborating agents that may split into
multiple new agents. This framework allows for a spatial reward formulation
that simplifies the credit assignment problem, which we combine with Message
Passing Networks to propagate information between neighboring mesh elements. We
experimentally validate the effectiveness of our approach, Adaptive Swarm Mesh
Refinement (ASMR), showing that it learns reliable, scalable, and efficient
refinement strategies on a set of challenging problems. Our approach
significantly speeds up computation, achieving up to 30-fold improvement
compared to uniform refinements in complex simulations. Additionally, we
outperform learned baselines and achieve a refinement quality that is on par
with a traditional error-based AMR strategy without expensive oracle
information about the error signal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freymuth_N/0/1/0/all/0/1&quot;&gt;Niklas Freymuth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahlinger_P/0/1/0/all/0/1&quot;&gt;Philipp Dahlinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wurth_T/0/1/0/all/0/1&quot;&gt;Tobias W&amp;#xfc;rth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reisch_S/0/1/0/all/0/1&quot;&gt;Simon Reisch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karger_L/0/1/0/all/0/1&quot;&gt;Luise K&amp;#xe4;rger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1&quot;&gt;Gerhard Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06987">
<title>Unsupervised ANN-Based Equalizer and Its Trainable FPGA Implementation. (arXiv:2304.06987v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06987</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, communication engineers put strong emphasis on artificial
neural network (ANN)-based algorithms with the aim of increasing the
flexibility and autonomy of the system and its components. In this context,
unsupervised training is of special interest as it enables adaptation without
the overhead of transmitting pilot symbols. In this work, we present a novel
ANN-based, unsupervised equalizer and its trainable field programmable gate
array (FPGA) implementation. We demonstrate that our custom loss function
allows the ANN to adapt for varying channel conditions, approaching the
performance of a supervised baseline. Furthermore, as a first step towards a
practical communication system, we design an efficient FPGA implementation of
our proposed algorithm, which achieves a throughput in the order of Gbit/s,
outperforming a high-performance GPU by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ney_J/0/1/0/all/0/1&quot;&gt;Jonas Ney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lauinger_V/0/1/0/all/0/1&quot;&gt;Vincent Lauinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schmalen_L/0/1/0/all/0/1&quot;&gt;Laurent Schmalen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wehn_N/0/1/0/all/0/1&quot;&gt;Norbert Wehn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07810">
<title>VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping. (arXiv:2304.07810v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07810</link>
<description rdf:parseType="Literal">&lt;p&gt;In argumentative writing, writers must brainstorm hierarchical writing goals,
ensure the persuasiveness of their arguments, and revise and organize their
plans through drafting. Recent advances in large language models (LLMs) have
made interactive text generation through a chat interface (e.g., ChatGPT)
possible. However, this approach often neglects implicit writing context and
user intent, lacks support for user control and autonomy, and provides limited
assistance for sensemaking and revising writing plans. To address these
challenges, we introduce VISAR, an AI-enabled writing assistant system designed
to help writers brainstorm and revise hierarchical goals within their writing
context, organize argument structures through synchronized text editing and
visual programming, and enhance persuasiveness with argumentation spark
recommendations. VISAR allows users to explore, experiment with, and validate
their writing plans using automatic draft prototyping. A controlled lab study
confirmed the usability and effectiveness of VISAR in facilitating the
argumentative writing planning process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jie Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhaliwal_R/0/1/0/all/0/1&quot;&gt;Ranjodh Singh Dhaliwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Toby Jia-Jun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14831">
<title>Earning Extra Performance from Restrictive Feedbacks. (arXiv:2304.14831v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14831</link>
<description rdf:parseType="Literal">&lt;p&gt;Many machine learning applications encounter a situation where model
providers are required to further refine the previously trained model so as to
gratify the specific need of local users. This problem is reduced to the
standard model tuning paradigm if the target data is permissibly fed to the
model. However, it is rather difficult in a wide range of practical cases where
target data is not shared with model providers but commonly some evaluations
about the model are accessible. In this paper, we formally set up a challenge
named \emph{Earning eXtra PerformancE from restriCTive feEDdbacks} (EXPECTED)
to describe this form of model tuning problems. Concretely, EXPECTED admits a
model provider to access the operational performance of the candidate model
multiple times via feedback from a local user (or a group of users). The goal
of the model provider is to eventually deliver a satisfactory model to the
local user(s) by utilizing the feedbacks. Unlike existing model tuning methods
where the target data is always ready for calculating model gradients, the
model providers in EXPECTED only see some feedbacks which could be as simple as
scalars, such as inference accuracy or usage rate. To enable tuning in this
restrictive circumstance, we propose to characterize the geometry of the model
performance with regard to model parameters through exploring the parameters&apos;
distribution. In particular, for the deep models whose parameters distribute
across multiple layers, a more query-efficient algorithm is further
tailor-designed that conducts layerwise tuning with more attention to those
layers which pay off better. Extensive experiments on different applications
demonstrate that our work forges a sound solution to the EXPECTED problem. Code
is available via https://github.com/kylejingli/EXPECTED.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yuangang Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1&quot;&gt;Yueming Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yinghua Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1&quot;&gt;Yulei Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1&quot;&gt;Ivor W. Tsang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02757">
<title>Multi-Domain Learning From Insufficient Annotations. (arXiv:2305.02757v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02757</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-domain learning (MDL) refers to simultaneously constructing a model or
a set of models on datasets collected from different domains. Conventional
approaches emphasize domain-shared information extraction and domain-private
information preservation, following the shared-private framework (SP models),
which offers significant advantages over single-domain learning. However, the
limited availability of annotated data in each domain considerably hinders the
effectiveness of conventional supervised MDL approaches in real-world
applications. In this paper, we introduce a novel method called multi-domain
contrastive learning (MDCL) to alleviate the impact of insufficient annotations
by capturing both semantic and structural information from both labeled and
unlabeled data.Specifically, MDCL comprises two modules: inter-domain semantic
alignment and intra-domain contrast. The former aims to align annotated
instances of the same semantic category from distinct domains within a shared
hidden space, while the latter focuses on learning a cluster structure of
unlabeled instances in a private hidden space for each domain. MDCL is readily
compatible with many SP models, requiring no additional model parameters and
allowing for end-to-end training. Experimental results across five textual and
image multi-domain datasets demonstrate that MDCL brings noticeable improvement
over various SP models.Furthermore, MDCL can further be employed in
multi-domain active learning (MDAL) to achieve a superior initialization,
eventually leading to better overall performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Rui He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shengcai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiahao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1&quot;&gt;Ke Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03824">
<title>No-Regret Constrained Bayesian Optimization of Noisy and Expensive Hybrid Models using Differentiable Quantile Function Approximations. (arXiv:2305.03824v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03824</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates the problem of efficient constrained global
optimization of hybrid models that are a composition of a known white-box
function and an expensive multi-output black-box function subject to noisy
observations, which often arises in real-world science and engineering
applications. We propose a novel method, Constrained Upper Quantile Bound
(CUQB), to solve such problems that directly exploits the composite structure
of the objective and constraint functions that we show leads substantially
improved sampling efficiency. CUQB is a conceptually simple, deterministic
approach that avoid constraint approximations used by previous methods.
Although the CUQB acquisition function is not available in closed form, we
propose a novel differentiable sample average approximation that enables it to
be efficiently maximized. We further derive bounds on the cumulative regret and
constraint violation under a non-parametric Bayesian representation of the
black-box function. Since these bounds depend sublinearly on the number of
iterations under some regularity assumptions, we establis bounds on the
convergence rate to the optimal solution of the original constrained problem.
In contrast to most existing methods, CUQB further incorporates a simple
infeasibility detection scheme, which we prove triggers in a finite number of
iterations when the original problem is infeasible (with high probability given
the Bayesian model). Numerical experiments on several test problems, including
environmental model calibration and real-time optimization of a reactor system,
show that CUQB significantly outperforms traditional Bayesian optimization in
both constrained and unconstrained cases. Furthermore, compared to other
state-of-the-art methods that exploit composite structure, CUQB achieves
competitive empirical performance while also providing substantially improved
theoretical guarantees.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Congwen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Paulson_J/0/1/0/all/0/1&quot;&gt;Joel A. Paulson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05966">
<title>Graph Neural Networks and 3-Dimensional Topology. (arXiv:2305.05966v2 [math.GT] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05966</link>
<description rdf:parseType="Literal">&lt;p&gt;We test the efficiency of applying Geometric Deep Learning to the problems in
low-dimensional topology in a certain simple setting. Specifically, we consider
the class of 3-manifolds described by plumbing graphs and use Graph Neural
Networks (GNN) for the problem of deciding whether a pair of graphs give
homeomorphic 3-manifolds. We use supervised learning to train a GNN that
provides the answer to such a question with high accuracy. Moreover, we
consider reinforcement learning by a GNN to find a sequence of Neumann moves
that relates the pair of graphs if the answer is positive. The setting can be
understood as a toy model of the problem of deciding whether a pair of Kirby
diagrams give diffeomorphic 3- or 4-manifolds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Putrov_P/0/1/0/all/0/1&quot;&gt;Pavel Putrov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ri_S/0/1/0/all/0/1&quot;&gt;Song Jin Ri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11164">
<title>Exploring the Carbon Footprint of Hugging Face&apos;s ML Models: A Repository Mining Study. (arXiv:2305.11164v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11164</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise of machine learning (ML) systems has exacerbated their carbon
footprint due to increased capabilities and model sizes. However, there is
scarce knowledge on how the carbon footprint of ML models is actually measured,
reported, and evaluated. In light of this, the paper aims to analyze the
measurement of the carbon footprint of 1,417 ML models and associated datasets
on Hugging Face, which is the most popular repository for pretrained ML models.
The goal is to provide insights and recommendations on how to report and
optimize the carbon efficiency of ML models. The study includes the first
repository mining study on the Hugging Face Hub API on carbon emissions. This
study seeks to answer two research questions: (1) how do ML model creators
measure and report carbon emissions on Hugging Face Hub?, and (2) what aspects
impact the carbon emissions of training ML models? The study yielded several
key findings. These include a stalled proportion of carbon emissions-reporting
models, a slight decrease in reported carbon footprint on Hugging Face over the
past 2 years, and a continued dominance of NLP as the main application domain.
Furthermore, the study uncovers correlations between carbon emissions and
various attributes such as model size, dataset size, and ML application
domains. These results highlight the need for software measurements to improve
energy reporting practices and promote carbon-efficient model development
within the Hugging Face community. In response to this issue, two
classifications are proposed: one for categorizing models based on their carbon
emission reporting practices and another for their carbon efficiency. The aim
of these classification proposals is to foster transparency and sustainable
model development within the ML community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castano_J/0/1/0/all/0/1&quot;&gt;Joel Casta&amp;#xf1;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Fernandez_S/0/1/0/all/0/1&quot;&gt;Silverio Mart&amp;#xed;nez-Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franch_X/0/1/0/all/0/1&quot;&gt;Xavier Franch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogner_J/0/1/0/all/0/1&quot;&gt;Justus Bogner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00607">
<title>FACT: Federated Adversarial Cross Training. (arXiv:2306.00607v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00607</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) facilitates distributed model development to
aggregate multiple confidential data sources. The information transfer among
clients can be compromised by distributional differences, i.e., by non-i.i.d.
data. A particularly challenging scenario is the federated model adaptation to
a target client without access to annotated data. We propose Federated
Adversarial Cross Training (FACT), which uses the implicit domain differences
between source clients to identify domain shifts in the target domain. In each
round of FL, FACT cross initializes a pair of source clients to generate domain
specialized representations which are then used as a direct adversary to learn
a domain invariant data representation. We empirically show that FACT
outperforms state-of-the-art federated, non-federated and source-free domain
adaptation models on three popular multi-source-single-target benchmarks, and
state-of-the-art Unsupervised Domain Adaptation (UDA) models on
single-source-single-target experiments. We further study FACT&apos;s behavior with
respect to communication restrictions and the number of participating clients.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schrod_S/0/1/0/all/0/1&quot;&gt;Stefan Schrod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lippl_J/0/1/0/all/0/1&quot;&gt;Jonas Lippl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schafer_A/0/1/0/all/0/1&quot;&gt;Andreas Sch&amp;#xe4;fer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altenbuchinger_M/0/1/0/all/0/1&quot;&gt;Michael Altenbuchinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01874">
<title>SACSoN: Scalable Autonomous Control for Social Navigation. (arXiv:2306.01874v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01874</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning provides a powerful tool for building socially compliant
robotic systems that go beyond simple predictive models of human behavior. By
observing and understanding human interactions from past experiences, learning
can enable effective social navigation behaviors directly from data. In this
paper, our goal is to develop methods for training policies for socially
unobtrusive navigation, such that robots can navigate among humans in ways that
don&apos;t disturb human behavior. We introduce a definition for such behavior based
on the counterfactual perturbation of the human: if the robot had not intruded
into the space, would the human have acted in the same way? By minimizing this
counterfactual perturbation, we can induce robots to behave in ways that do not
alter the natural behavior of humans in the shared space. Instantiating this
principle requires training policies to minimize their effect on human
behavior, and this in turn requires data that allows us to model the behavior
of humans in the presence of robots. Therefore, our approach is based on two
key contributions. First, we collect a large dataset where an indoor mobile
robot interacts with human bystanders. Second, we utilize this dataset to train
policies that minimize counterfactual perturbation. We provide supplementary
videos and make publicly available the largest-of-its-kind visual navigation
dataset on our project page.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirose_N/0/1/0/all/0/1&quot;&gt;Noriaki Hirose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1&quot;&gt;Dhruv Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1&quot;&gt;Ajay Sridhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05965">
<title>Automating Model Comparison in Factor Graphs. (arXiv:2306.05965v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05965</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian state and parameter estimation have been automated effectively in a
variety of probabilistic programming languages. The process of model comparison
on the other hand, which still requires error-prone and time-consuming manual
derivations, is often overlooked despite its importance. This paper efficiently
automates Bayesian model averaging, selection, and combination by message
passing on a Forney-style factor graph with a custom mixture node. Parameter
and state inference, and model comparison can then be executed simultaneously
using message passing with scale factors. This approach shortens the model
design cycle and allows for the straightforward extension to hierarchical and
temporal model priors to accommodate for modeling complicated time-varying
processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erp_B/0/1/0/all/0/1&quot;&gt;Bart van Erp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nuijten_W/0/1/0/all/0/1&quot;&gt;Wouter W. L. Nuijten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laar_T/0/1/0/all/0/1&quot;&gt;Thijs van de Laar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vries_B/0/1/0/all/0/1&quot;&gt;Bert de Vries&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14680">
<title>A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy. (arXiv:2306.14680v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14680</link>
<description rdf:parseType="Literal">&lt;p&gt;The generation of virtual populations (VPs) of anatomy is essential for
conducting in silico trials of medical devices. Typically, the generated VP
should capture sufficient variability while remaining plausible and should
reflect the specific characteristics and demographics of the patients observed
in real populations. In several applications, it is desirable to synthesise
virtual populations in a \textit{controlled} manner, where relevant covariates
are used to conditionally synthesise virtual populations that fit a specific
target population/characteristics. We propose to equip a conditional
variational autoencoder (cVAE) with normalising flows to boost the flexibility
and complexity of the approximate posterior learnt, leading to enhanced
flexibility for controllable synthesis of VPs of anatomical structures. We
demonstrate the performance of our conditional flow VAE using a data set of
cardiac left ventricles acquired from 2360 patients, with associated
demographic information and clinical measurements (used as
covariates/conditional information). The results obtained indicate the
superiority of the proposed method for conditional synthesis of virtual
populations of cardiac left ventricles relative to a cVAE. Conditional
synthesis performance was evaluated in terms of generalisation and specificity
errors and in terms of the ability to preserve clinically relevant biomarkers
in synthesised VPs, that is, the left ventricular blood pool and myocardial
volume, relative to the real observed population.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dou_H/0/1/0/all/0/1&quot;&gt;Haoran Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ravikumar_N/0/1/0/all/0/1&quot;&gt;Nishant Ravikumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Frangi_A/0/1/0/all/0/1&quot;&gt;Alejandro F. Frangi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01995">
<title>Dynamic Feature-based Deep Reinforcement Learning for Flow Control of Circular Cylinder with Sparse Surface Pressure Sensing. (arXiv:2307.01995v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01995</link>
<description rdf:parseType="Literal">&lt;p&gt;This study proposes a self-learning algorithm for closed-loop cylinder wake
control targeting lower drag and lower lift fluctuations with the additional
challenge of sparse sensor information, taking deep reinforcement learning as
the starting point. DRL performance is significantly improved by lifting the
sensor signals to dynamic features (DF), which predict future flow states. The
resulting dynamic feature-based DRL (DF-DRL) automatically learns a feedback
control in the plant without a dynamic model. Results show that the drag
coefficient of the DF-DRL model is 25% less than the vanilla model based on
direct sensor feedback. More importantly, using only one surface pressure
sensor, DF-DRL can reduce the drag coefficient to a state-of-the-art
performance of about 8% at Re = 100 and significantly mitigate lift coefficient
fluctuations. Hence, DF-DRL allows the deployment of sparse sensing of the flow
without degrading the control performance. This method also shows good
robustness in controlling flow under higher Reynolds numbers, which reduces the
drag coefficient by 32.2% and 46.55% at Re = 500 and 1000, respectively,
indicating the broad applicability of the method. Since surface pressure
information is more straightforward to measure in realistic scenarios than flow
velocity information, this study provides a valuable reference for
experimentally designing the active flow control of a circular cylinder based
on wall pressure signals, which is an essential step toward further developing
intelligent control in realistic multi-input multi-output (MIMO) system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qiulei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1&quot;&gt;Lei Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1&quot;&gt;Gang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenli Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noack_B/0/1/0/all/0/1&quot;&gt;Bernd R. Noack&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07686">
<title>Creating a Dataset for High-Performance Computing Code Translation: A Bridge Between HPC Fortran and C++. (arXiv:2307.07686v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07686</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we present a novel dataset for training machine learning
models translating between OpenMP Fortran and C++ code. To ensure reliability
and applicability, the dataset is initially refined using a meticulous code
similarity test. The effectiveness of our dataset is assessed using both
quantitative (CodeBLEU) and qualitative (human evaluation) methods. We
demonstrate how this dataset can significantly improve the translation
capabilities of large-scale language models, with improvements of
$\mathbf{\times 5.1}$ for models with no prior coding knowledge and
$\mathbf{\times 9.9}$ for models with some coding familiarity. Our work
highlights the potential of this dataset to advance the field of code
translation for high-performance computing. The dataset is available at
https://github.com/bin123apple/Fortran-CPP-HPC-code-translation-dataset
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1&quot;&gt;Bin Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Caiwen Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Le Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1&quot;&gt;Pei-Hung Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1&quot;&gt;Chunhua Liao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11465">
<title>A Deep Learning Approach for Overall Survival Prediction in Lung Cancer with Missing Values. (arXiv:2307.11465v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.11465</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the most challenging fields where Artificial Intelligence (AI) can be
applied is lung cancer research, specifically non-small cell lung cancer
(NSCLC). In particular, overall survival (OS), the time between diagnosis and
death, is a vital indicator of patient status, enabling tailored treatment and
improved OS rates. In this analysis, there are two challenges to take into
account. First, few studies effectively exploit the information available from
each patient, leveraging both uncensored (i.e., dead) and censored (i.e.,
survivors) patients, considering also the events&apos; time. Second, the handling of
incomplete data is a common issue in the medical field. This problem is
typically tackled through the use of imputation methods. Our objective is to
present an AI model able to overcome these limits, effectively learning from
both censored and uncensored patients and their available features, for the
prediction of OS for NSCLC patients. We present a novel approach to survival
analysis with missing values in the context of NSCLC, which exploits the
strengths of the transformer architecture to account only for available
features without requiring any imputation strategy. By making use of ad-hoc
losses for OS, it is able to account for both censored and uncensored patients,
as well as changes in risks over time. We compared our method with
state-of-the-art models for survival analysis coupled with different imputation
strategies. We evaluated the results obtained over a period of 6 years using
different time granularities obtaining a Ct-index, a time-dependent variant of
the C-index, of 71.97, 77.58 and 80.72 for time units of 1 month, 1 year and 2
years, respectively, outperforming all state-of-the-art methods regardless of
the imputation method used.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caruso_C/0/1/0/all/0/1&quot;&gt;Camillo Maria Caruso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guarrasi_V/0/1/0/all/0/1&quot;&gt;Valerio Guarrasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramella_S/0/1/0/all/0/1&quot;&gt;Sara Ramella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soda_P/0/1/0/all/0/1&quot;&gt;Paolo Soda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13124">
<title>Conformal prediction for frequency-severity modeling. (arXiv:2307.13124v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13124</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a nonparametric model-agnostic framework for building prediction
intervals of insurance claims, with finite sample statistical guarantees,
extending the technique of split conformal prediction to the domain of
two-stage frequency-severity modeling. The effectiveness of the framework is
showcased with simulated and real datasets. When the underlying severity model
is a random forest, we extend the two-stage split conformal prediction
procedure, showing how the out-of-bag mechanism can be leveraged to eliminate
the need for a calibration set and to enable the production of prediction
intervals with adaptive width.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Graziadei_H/0/1/0/all/0/1&quot;&gt;Helton Graziadei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+F%2E_P/0/1/0/all/0/1&quot;&gt;Paulo C. Marques F.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Melo_E/0/1/0/all/0/1&quot;&gt;Eduardo F. L. de Melo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Targino_R/0/1/0/all/0/1&quot;&gt;Rodrigo S. Targino&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13494">
<title>Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v4 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13494</link>
<description rdf:parseType="Literal">&lt;p&gt;Learned cardinality estimation methods have achieved high precision compared
to traditional methods. Among learned methods, query-driven approaches face the
data and workload drift problem for a long time. Although both query-driven and
hybrid methods are proposed to avoid this problem, even the state-of-the-art of
them suffer from high training and estimation costs, limited scalability,
instability, and long-tailed distribution problem on high cardinality and
high-dimensional tables, which seriously affects the practical application of
learned cardinality estimators. In this paper, we prove that most of these
problems are directly caused by the widely used progressive sampling. We solve
this problem by introducing predicates information into the autoregressive
model and propose Duet, a stable, efficient, and scalable hybrid method to
estimate cardinality directly without sampling or any non-differentiable
process, which can not only reduces the inference complexity from O(n) to O(1)
compared to Naru and UAE but also achieve higher accuracy on high cardinality
and high-dimensional tables. Experimental results show that Duet can achieve
all the design goals above and be much more practical and even has a lower
inference cost on CPU than that of most learned methods on GPU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaixin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yabin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1&quot;&gt;Chang Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Donghua Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13692">
<title>ARB: Advanced Reasoning Benchmark for Large Language Models. (arXiv:2307.13692v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13692</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have demonstrated remarkable performance on
various quantitative reasoning and knowledge benchmarks. However, many of these
benchmarks are losing utility as LLMs get increasingly high scores, despite not
yet reaching expert performance in these domains. We introduce ARB, a novel
benchmark composed of advanced reasoning problems in multiple fields. ARB
presents a more challenging test than prior benchmarks, featuring problems in
mathematics, physics, biology, chemistry, and law. As a subset of ARB, we
introduce a challenging set of math and physics problems which require advanced
symbolic reasoning and domain knowledge. We evaluate recent models such as
GPT-4 and Claude on ARB and demonstrate that current models score well below
50% on more demanding tasks. In order to improve both automatic and assisted
evaluation capabilities, we introduce a rubric-based evaluation approach,
allowing GPT-4 to score its own intermediate reasoning steps. Further, we
conduct a human evaluation of the symbolic subset of ARB, finding promising
agreement between annotators and GPT-4 rubric evaluation scores.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawada_T/0/1/0/all/0/1&quot;&gt;Tomohiro Sawada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paleka_D/0/1/0/all/0/1&quot;&gt;Daniel Paleka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Havrilla_A/0/1/0/all/0/1&quot;&gt;Alexander Havrilla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tadepalli_P/0/1/0/all/0/1&quot;&gt;Pranav Tadepalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vidas_P/0/1/0/all/0/1&quot;&gt;Paula Vidas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kranias_A/0/1/0/all/0/1&quot;&gt;Alexander Kranias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nay_J/0/1/0/all/0/1&quot;&gt;John J. Nay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1&quot;&gt;Kshitij Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komatsuzaki_A/0/1/0/all/0/1&quot;&gt;Aran Komatsuzaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14935">
<title>Solving Data Quality Problems with Desbordante: a Demo. (arXiv:2307.14935v2 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14935</link>
<description rdf:parseType="Literal">&lt;p&gt;Data profiling is an essential process in modern data-driven industries. One
of its critical components is the discovery and validation of complex
statistics, including functional dependencies, data constraints, association
rules, and others.
&lt;/p&gt;
&lt;p&gt;However, most existing data profiling systems that focus on complex
statistics do not provide proper integration with the tools used by
contemporary data scientists. This creates a significant barrier to the
adoption of these tools in the industry. Moreover, existing systems were not
created with industrial-grade workloads in mind. Finally, they do not aim to
provide descriptive explanations, i.e. why a given pattern is not found. It is
a significant issue as it is essential to understand the underlying reasons for
a specific pattern&apos;s absence to make informed decisions based on the data.
&lt;/p&gt;
&lt;p&gt;Because of that, these patterns are effectively rest in thin air: their
application scope is rather limited, they are rarely used by the broader
public. At the same time, as we are going to demonstrate in this presentation,
complex statistics can be efficiently used to solve many classic data quality
problems.
&lt;/p&gt;
&lt;p&gt;Desbordante is an open-source data profiler that aims to close this gap. It
is built with emphasis on industrial application: it is efficient, scalable,
resilient to crashes, and provides explanations. Furthermore, it provides
seamless Python integration by offloading various costly operations to the C++
core, not only mining.
&lt;/p&gt;
&lt;p&gt;In this demonstration, we show several scenarios that allow end users to
solve different data quality problems. Namely, we showcase typo detection, data
deduplication, and data anomaly detection scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chernishev_G/0/1/0/all/0/1&quot;&gt;George Chernishev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polyntsov_M/0/1/0/all/0/1&quot;&gt;Michael Polyntsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chizhov_A/0/1/0/all/0/1&quot;&gt;Anton Chizhov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stupakov_K/0/1/0/all/0/1&quot;&gt;Kirill Stupakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shchuckin_I/0/1/0/all/0/1&quot;&gt;Ilya Shchuckin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smirnov_A/0/1/0/all/0/1&quot;&gt;Alexander Smirnov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strutovsky_M/0/1/0/all/0/1&quot;&gt;Maxim Strutovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shlyonskikh_A/0/1/0/all/0/1&quot;&gt;Alexey Shlyonskikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Firsov_M/0/1/0/all/0/1&quot;&gt;Mikhail Firsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manannikov_S/0/1/0/all/0/1&quot;&gt;Stepan Manannikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bobrov_N/0/1/0/all/0/1&quot;&gt;Nikita Bobrov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goncharov_D/0/1/0/all/0/1&quot;&gt;Daniil Goncharov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barutkin_I/0/1/0/all/0/1&quot;&gt;Ilia Barutkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shalnev_V/0/1/0/all/0/1&quot;&gt;Vladislav Shalnev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muraviev_K/0/1/0/all/0/1&quot;&gt;Kirill Muraviev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakhmukova_A/0/1/0/all/0/1&quot;&gt;Anna Rakhmukova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shcheka_D/0/1/0/all/0/1&quot;&gt;Dmitriy Shcheka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chernikov_A/0/1/0/all/0/1&quot;&gt;Anton Chernikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vyrodov_M/0/1/0/all/0/1&quot;&gt;Mikhail Vyrodov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurbatov_Y/0/1/0/all/0/1&quot;&gt;Yaroslav Kurbatov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fofanov_M/0/1/0/all/0/1&quot;&gt;Maxim Fofanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belokonnyi_S/0/1/0/all/0/1&quot;&gt;Sergei Belokonnyi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anosov_P/0/1/0/all/0/1&quot;&gt;Pavel Anosov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saliou_A/0/1/0/all/0/1&quot;&gt;Arthur Saliou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaisin_E/0/1/0/all/0/1&quot;&gt;Eduard Gaisin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smirnov_K/0/1/0/all/0/1&quot;&gt;Kirill Smirnov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14940">
<title>A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs. (arXiv:2307.14940v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14940</link>
<description rdf:parseType="Literal">&lt;p&gt;The continuous dynamics of natural systems has been effectively modelled
using Neural Ordinary Differential Equations (Neural ODEs). However, for
accurate and meaningful predictions, it is crucial that the models follow the
underlying rules or laws that govern these systems. In this work, we propose a
self-adaptive penalty algorithm for Neural ODEs to enable modelling of
constrained natural systems. The proposed self-adaptive penalty function can
dynamically adjust the penalty parameters. The explicit introduction of prior
knowledge helps to increase the interpretability of Neural ODE -based models.
We validate the proposed approach by modelling three natural systems with prior
knowledge constraints: population growth, chemical reaction evolution, and
damped harmonic oscillator motion. The numerical experiments and a comparison
with other penalty Neural ODE approaches and \emph{vanilla} Neural ODE,
demonstrate the effectiveness of the proposed self-adaptive penalty algorithm
for Neural ODEs in modelling constrained natural systems. Moreover, the
self-adaptive penalty approach provides more accurate and robust models with
reliable and meaningful predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coelho_C/0/1/0/all/0/1&quot;&gt;C. Coelho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_M/0/1/0/all/0/1&quot;&gt;M. Fernanda P. Costa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferras_L/0/1/0/all/0/1&quot;&gt;L. L. Ferr&amp;#xe1;s&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14442">
<title>Neural Schr\&quot;odinger Bridge with Sinkhorn Losses: Application to Data-driven Minimum Effort Control of Colloidal Self-assembly. (arXiv:2307.14442v1 [math.OC] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.14442</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that the minimum effort control of colloidal self-assembly can be
naturally formulated in the order-parameter space as a generalized
Schr\&quot;odinger bridge problem -- a class of fixed-horizon stochastic optimal
control problems that originated in the works of Erwin Schr\&quot;odinger in the
early 1930s. In recent years, this class of problems has seen a resurgence of
research activities in control and machine learning communities. Different from
the existing literature on the theory and computation for such problems, the
controlled drift and diffusion coefficients for colloidal self-assembly are
typically non-affine in control, and are difficult to obtain from physics-based
modeling. We deduce the conditions of optimality for such generalized problems,
and show that the resulting system of equations is structurally very different
from the existing results in a way that standard computational approaches no
longer apply. Thus motivated, we propose a data-driven learning and control
framework, named `neural Schr\&quot;odinger bridge&apos;, to solve such generalized
Schr\&quot;odinger bridge problems by innovating on recent advances in neural
networks. We illustrate the effectiveness of the proposed framework using a
numerical case study of colloidal self-assembly. We learn the controlled drift
and diffusion coefficients as two neural networks using molecular dynamics
simulation data, and then use these two to train a third network with Sinkhorn
losses designed for distributional endpoint constraints, specific for this
class of control problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nodozi_I/0/1/0/all/0/1&quot;&gt;Iman Nodozi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yan_C/0/1/0/all/0/1&quot;&gt;Charlie Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Khare_M/0/1/0/all/0/1&quot;&gt;Mira Khare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Halder_A/0/1/0/all/0/1&quot;&gt;Abhishek Halder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mesbah_A/0/1/0/all/0/1&quot;&gt;Ali Mesbah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14643">
<title>MVMR-FS : Non-parametric feature selection algorithm based on Maximum inter-class Variation and Minimum Redundancy. (arXiv:2307.14643v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.14643</link>
<description rdf:parseType="Literal">&lt;p&gt;How to accurately measure the relevance and redundancy of features is an
age-old challenge in the field of feature selection. However, existing
filter-based feature selection methods cannot directly measure redundancy for
continuous data. In addition, most methods rely on manually specifying the
number of features, which may introduce errors in the absence of expert
knowledge. In this paper, we propose a non-parametric feature selection
algorithm based on maximum inter-class variation and minimum redundancy,
abbreviated as MVMR-FS. We first introduce supervised and unsupervised kernel
density estimation on the features to capture their similarities and
differences in inter-class and overall distributions. Subsequently, we present
the criteria for maximum inter-class variation and minimum redundancy (MVMR),
wherein the inter-class probability distributions are employed to reflect
feature relevance and the distances between overall probability distributions
are used to quantify redundancy. Finally, we employ an AGA to search for the
feature subset that minimizes the MVMR. Compared with ten state-of-the-art
methods, MVMR-FS achieves the highest average accuracy and improves the
accuracy by 5% to 11%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_H/0/1/0/all/0/1&quot;&gt;Haitao Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shengbo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1&quot;&gt;Bin Xie&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>