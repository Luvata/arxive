<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-17T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07513" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07525" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07540" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07603" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07635" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07643" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07653" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07662" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07663" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07678" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07693" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07710" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07732" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07768" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07790" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07791" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07807" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07847" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07859" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07912" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07928" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07933" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07935" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07938" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07953" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07972" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07976" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07977" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07982" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08012" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08013" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08015" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08025" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08027" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08041" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1905.06683" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1911.02265" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.13389" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2107.10998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2107.12655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.06021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.13924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.06845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.14308" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.16329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.03923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.07433" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.05379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.12068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.10886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.07137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.14646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.16198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01450" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.05376" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.07158" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.08071" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09298" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.01947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.06719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.08125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.08739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.02521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.03665" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06294" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06441" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06608" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.09765" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.14267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.02867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.04654" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09181" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09706" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10840" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11165" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11228" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11324" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13005" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.01979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04628" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14301" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14758" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18022" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01359" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15445" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16175" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03903" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04525" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05201" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06533" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07246" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.07513">
<title>An empirical study of using radiology reports and images to improve ICU mortality prediction. (arXiv:2307.07513v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07513</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: The predictive Intensive Care Unit (ICU) scoring system plays an
important role in ICU management because it predicts important outcomes,
especially mortality. Many scoring systems have been developed and used in the
ICU. These scoring systems are primarily based on the structured clinical data
in the electronic health record (EHR), which may suffer the loss of important
clinical information in the narratives and images. Methods: In this work, we
build a deep learning based survival prediction model with multi-modality data
to predict ICU mortality. Four sets of features are investigated: (1)
physiological measurements of Simplified Acute Physiology Score (SAPS) II, (2)
common thorax diseases pre-defined by radiologists, (3) BERT-based text
representations, and (4) chest X-ray image features. We use the Medical
Information Mart for Intensive Care IV (MIMIC-IV) dataset to evaluate the
proposed model. Results: Our model achieves the average C-index of 0.7829 (95%
confidence interval, 0.7620-0.8038), which substantially exceeds that of the
baseline with SAPS-II features (0.7470 (0.7263-0.7676)). Ablation studies
further demonstrate the contributions of pre-defined labels (2.00%), text
features (2.44%), and image features (2.82%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Mingquan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Song Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Ying Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Lihui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yifan Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07516">
<title>Voting-based Multimodal Automatic Deception Detection. (arXiv:2307.07516v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07516</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic Deception Detection has been a hot research topic for a long time,
using machine learning and deep learning to automatically detect deception,
brings new light to this old field. In this paper, we proposed a voting-based
method for automatic deception detection from videos using audio, visual and
lexical features. Experiments were done on two datasets, the Real-life trial
dataset by Michigan University and the Miami University deception detection
dataset. Video samples were split into frames of images, audio, and
manuscripts. Our Voting-based Multimodal proposed solution consists of three
models. The first model is CNN for detecting deception from images, the second
model is Support Vector Machine (SVM) on Mel spectrograms for detecting
deception from audio and the third model is Word2Vec on Support Vector Machine
(SVM) for detecting deception from manuscripts. Our proposed solution
outperforms state of the art. Best results achieved on images, audio and text
were 97%, 96%, 92% respectively on Real-Life Trial Dataset, and 97%, 82%, 73%
on video, audio and text respectively on Miami University Deception Detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Touma_L/0/1/0/all/0/1&quot;&gt;Lana Touma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horani_M/0/1/0/all/0/1&quot;&gt;Mohammad Al Horani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tailouni_M/0/1/0/all/0/1&quot;&gt;Manar Tailouni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahabiah_A/0/1/0/all/0/1&quot;&gt;Anas Dahabiah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jallad_K/0/1/0/all/0/1&quot;&gt;Khloud Al Jallad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07518">
<title>CephGPT-4: An Interactive Multimodal Cephalometric Measurement and Diagnostic System with Visual Large Language Model. (arXiv:2307.07518v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07518</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale multimodal language models (LMMs) have achieved remarkable
success in general domains. However, the exploration of diagnostic language
models based on multimodal cephalometric medical data remains limited. In this
paper, we propose a novel multimodal cephalometric analysis and diagnostic
dialogue model. Firstly, a multimodal orthodontic medical dataset is
constructed, comprising cephalometric images and doctor-patient dialogue data,
with automatic analysis of cephalometric landmarks using U-net and generation
of diagnostic reports. Then, the cephalometric dataset and generated diagnostic
reports are separately fine-tuned on Minigpt-4 and VisualGLM. Results
demonstrate that the CephGPT-4 model exhibits excellent performance and has the
potential to revolutionize orthodontic measurement and diagnostic applications.
These innovations hold revolutionary application potential in the field of
orthodontics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jincong Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoxin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dian Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07525">
<title>HistoColAi: An Open-Source Web Platform for Collaborative Digital Histology Image Annotation with AI-Driven Predictive Integration. (arXiv:2307.07525v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.07525</link>
<description rdf:parseType="Literal">&lt;p&gt;Digital pathology has become a standard in the pathology workflow due to its
many benefits. These include the level of detail of the whole slide images
generated and the potential immediate sharing of cases between hospitals.
Recent advances in deep learning-based methods for image analysis make them of
potential aid in digital pathology. However, a major limitation in developing
computer-aided diagnostic systems for pathology is the lack of an intuitive and
open web application for data annotation. This paper proposes a web service
that efficiently provides a tool to visualize and annotate digitized
histological images. In addition, to show and validate the tool, in this paper
we include a use case centered on the diagnosis of spindle cell skin neoplasm
for multiple annotators. A usability study of the tool is also presented,
showing the feasibility of the developed tool.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pulgarin_Ospina_C/0/1/0/all/0/1&quot;&gt;Cristian Camilo Pulgar&amp;#xed;n-Ospina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amor_R/0/1/0/all/0/1&quot;&gt;Roc&amp;#xed;o del Amor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colomera_A/0/1/0/all/0/1&quot;&gt;Adri&amp;#xe1;n Colomera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_Rodriguez_J/0/1/0/all/0/1&quot;&gt;Julio Silva-Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naranjo_V/0/1/0/all/0/1&quot;&gt;Valery Naranjo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07528">
<title>PatchSorter: A High Throughput Deep Learning Digital Pathology Tool for Object Labeling. (arXiv:2307.07528v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2307.07528</link>
<description rdf:parseType="Literal">&lt;p&gt;The discovery of patterns associated with diagnosis, prognosis, and therapy
response in digital pathology images often requires intractable labeling of
large quantities of histological objects. Here we release an open-source
labeling tool, PatchSorter, which integrates deep learning with an intuitive
web interface. Using &amp;gt;100,000 objects, we demonstrate a &amp;gt;7x improvement in
labels per second over unaided labeling, with minimal impact on labeling
accuracy, thus enabling high-throughput labeling of large datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Walker_C/0/1/0/all/0/1&quot;&gt;Cedric Walker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Talawalla_T/0/1/0/all/0/1&quot;&gt;Tasneem Talawalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Toth_R/0/1/0/all/0/1&quot;&gt;Robert Toth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ambekar_A/0/1/0/all/0/1&quot;&gt;Akhil Ambekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rea_K/0/1/0/all/0/1&quot;&gt;Kien Rea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chamian_O/0/1/0/all/0/1&quot;&gt;Oswin Chamian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Fan_F/0/1/0/all/0/1&quot;&gt;Fan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Berezowska_S/0/1/0/all/0/1&quot;&gt;Sabina Berezowska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rottenberg_S/0/1/0/all/0/1&quot;&gt;Sven Rottenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Madabhushi_A/0/1/0/all/0/1&quot;&gt;Anant Madabhushi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Maillard_M/0/1/0/all/0/1&quot;&gt;Marie Maillard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Barisoni_L/0/1/0/all/0/1&quot;&gt;Laura Barisoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Horlings_H/0/1/0/all/0/1&quot;&gt;Hugo Mark Horlings&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Janowczyk_A/0/1/0/all/0/1&quot;&gt;Andrew Janowczyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07534">
<title>Masked Autoencoders for Unsupervised Anomaly Detection in Medical Images. (arXiv:2307.07534v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.07534</link>
<description rdf:parseType="Literal">&lt;p&gt;Pathological anomalies exhibit diverse appearances in medical imaging, making
it difficult to collect and annotate a representative amount of data required
to train deep learning models in a supervised setting. Therefore, in this work,
we tackle anomaly detection in medical images training our framework using only
healthy samples. We propose to use the Masked Autoencoder model to learn the
structure of the normal samples, then train an anomaly classifier on top of the
difference between the original image and the reconstruction provided by the
masked autoencoder. We train the anomaly classifier in a supervised manner
using as negative samples the reconstruction of the healthy scans, while as
positive samples, we use pseudo-abnormal scans obtained via our novel
pseudo-abnormal module. The pseudo-abnormal module alters the reconstruction of
the normal samples by changing the intensity of several regions. We conduct
experiments on two medical image data sets, namely BRATS2020 and LUNA16 and
compare our method with four state-of-the-art anomaly detection frameworks,
namely AST, RD4AD, AnoVAEGAN and f-AnoGAN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Georgescu_M/0/1/0/all/0/1&quot;&gt;Mariana-Iuliana Georgescu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07540">
<title>Flow-Guided Controllable Line Drawing Generation. (arXiv:2307.07540v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07540</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate the problem of automatically controllable
artistic character line drawing generation from photographs by proposing a
Vector Flow Aware and Line Controllable Image-to-Image Translation
architecture, which can be viewed as an appealing intersection between
Artificial Intelligence and Arts. Specifically, we first present an
Image-to-Flow network (I2FNet) to efficiently and robustly create the vector
flow field in a learning-based manner, which can provide a direction guide for
drawing lines. Then, we introduce our well-designed Double Flow Generator (DFG)
framework to fuse features from learned vector flow and input image flow
guaranteeing the spatial coherence of lines. Meanwhile, in order to allow for
controllable character line drawing generation, we integrate a Line Control
Matrix (LCM) into DFG and train a Line Control Regressor (LCR) to synthesize
drawings with different styles by elaborately controlling the level of details,
such as thickness, smoothness, and continuity, of lines. Finally, we design a
Fourier Transformation Loss to further constrain the character line generation
from the frequency domain view of the point. Quantitative and qualitative
experiments demonstrate that our approach can obtain superior performance in
producing high-resolution character line-drawing images with perceptually
realistic characteristics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1&quot;&gt;Chengyu Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xianfeng Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07541">
<title>ConTrack: Contextual Transformer for Device Tracking in X-ray. (arXiv:2307.07541v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07541</link>
<description rdf:parseType="Literal">&lt;p&gt;Device tracking is an important prerequisite for guidance during endovascular
procedures. Especially during cardiac interventions, detection and tracking of
guiding the catheter tip in 2D fluoroscopic images is important for
applications such as mapping vessels from angiography (high dose with contrast)
to fluoroscopy (low dose without contrast). Tracking the catheter tip poses
different challenges: the tip can be occluded by contrast during angiography or
interventional devices; and it is always in continuous movement due to the
cardiac and respiratory motions. To overcome these challenges, we propose
ConTrack, a transformer-based network that uses both spatial and temporal
contextual information for accurate device detection and tracking in both X-ray
fluoroscopy and angiography. The spatial information comes from the template
frames and the segmentation module: the template frames define the surroundings
of the device, whereas the segmentation module detects the entire device to
bring more context for the tip prediction. Using multiple templates makes the
model more robust to the change in appearance of the device when it is occluded
by the contrast agent. The flow information computed on the segmented catheter
mask between the current and the previous frame helps in further refining the
prediction by compensating for the respiratory and cardiac motions. The
experiments show that our method achieves 45% or higher accuracy in detection
and tracking when compared to state-of-the-art tracking models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demoustier_M/0/1/0/all/0/1&quot;&gt;Marc Demoustier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murthy_V/0/1/0/all/0/1&quot;&gt;Venkatesh Narasimha Murthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghesu_F/0/1/0/all/0/1&quot;&gt;Florin C. Ghesu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Comaniciu_D/0/1/0/all/0/1&quot;&gt;Dorin Comaniciu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07603">
<title>Gastrointestinal Disease Classification through Explainable and Cost-Sensitive Deep Neural Networks with Supervised Contrastive Learning. (arXiv:2307.07603v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07603</link>
<description rdf:parseType="Literal">&lt;p&gt;Gastrointestinal diseases pose significant healthcare chall-enges as they
manifest in diverse ways and can lead to potential complications. Ensuring
precise and timely classification of these diseases is pivotal in guiding
treatment choices and enhancing patient outcomes. This paper introduces a novel
approach on classifying gastrointestinal diseases by leveraging cost-sensitive
pre-trained deep convolutional neural network (CNN) architectures with
supervised contrastive learning. Our approach enables the network to learn
representations that capture vital disease-related features, while also
considering the relationships of similarity between samples. To tackle the
challenges posed by imbalanced datasets and the cost-sensitive nature of
misclassification errors in healthcare, we incorporate cost-sensitive learning.
By assigning distinct costs to misclassifications based on the disease class,
we prioritize accurate classification of critical conditions. Furthermore, we
enhance the interpretability of our model by integrating gradient-based
techniques from explainable artificial intelligence (AI). This inclusion
provides valuable insights into the decision-making process of the network,
aiding in understanding the features that contribute to disease classification.
To assess the effectiveness of our proposed approach, we perform extensive
experiments on a comprehensive gastrointestinal disease dataset, such as the
Hyper-Kvasir dataset. Through thorough comparisons with existing works, we
demonstrate the strong classification accuracy, robustness and interpretability
of our model. We have made the implementation of our proposed approach publicly
available at
https://github.com/dibya404/Gastrointestinal-Disease-Classification-through-Explainable-and-Cost-Sensitive-DNN-with-SCL
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nath_D/0/1/0/all/0/1&quot;&gt;Dibya Nath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahariar_G/0/1/0/all/0/1&quot;&gt;G. M. Shahariar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07620">
<title>Generalizable Embeddings with Cross-batch Metric Learning. (arXiv:2307.07620v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07620</link>
<description rdf:parseType="Literal">&lt;p&gt;Global average pooling (GAP) is a popular component in deep metric learning
(DML) for aggregating features. Its effectiveness is often attributed to
treating each feature vector as a distinct semantic entity and GAP as a
combination of them. Albeit substantiated, such an explanation&apos;s algorithmic
implications to learn generalizable entities to represent unseen classes, a
crucial DML goal, remain unclear. To address this, we formulate GAP as a convex
combination of learnable prototypes. We then show that the prototype learning
can be expressed as a recursive process fitting a linear predictor to a batch
of samples. Building on that perspective, we consider two batches of disjoint
classes at each iteration and regularize the learning by expressing the samples
of a batch with the prototypes that are fitted to the other batch. We validate
our approach on 4 popular DML benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurbuz_Y/0/1/0/all/0/1&quot;&gt;Yeti Z. Gurbuz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alatan_A/0/1/0/all/0/1&quot;&gt;A. Aydin Alatan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07635">
<title>CoTracker: It is Better to Track Together. (arXiv:2307.07635v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07635</link>
<description rdf:parseType="Literal">&lt;p&gt;Methods for video motion prediction either estimate jointly the instantaneous
motion of all points in a given video frame using optical flow or independently
track the motion of individual points throughout the video. The latter is true
even for powerful deep-learning methods that can track points through
occlusions. Tracking points individually ignores the strong correlation that
can exist between the points, for instance, because they belong to the same
physical object, potentially harming performance. In this paper, we thus
propose CoTracker, an architecture that jointly tracks multiple points
throughout an entire video. This architecture combines several ideas from the
optical flow and tracking literature in a new, flexible and powerful design. It
is based on a transformer network that models the correlation of different
points in time via specialised attention layers. The transformer iteratively
updates an estimate of several trajectories. It can be applied in a
sliding-window manner to very long videos, for which we engineer an unrolled
training loop. It can track from one to several points jointly and supports
adding new points to track at any time. The result is a flexible and powerful
tracking algorithm that outperforms state-of-the-art methods in almost all
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karaev_N/0/1/0/all/0/1&quot;&gt;Nikita Karaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rocco_I/0/1/0/all/0/1&quot;&gt;Ignacio Rocco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graham_B/0/1/0/all/0/1&quot;&gt;Benjamin Graham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neverova_N/0/1/0/all/0/1&quot;&gt;Natalia Neverova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1&quot;&gt;Andrea Vedaldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rupprecht_C/0/1/0/all/0/1&quot;&gt;Christian Rupprecht&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07643">
<title>ACF-Net: An Attention-enhanced Co-interactive Fusion Network for Automated Structural Condition Assessment in Visual Inspection. (arXiv:2307.07643v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07643</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiently monitoring the condition of civil infrastructures necessitates
automating the structural condition assessment in visual inspection. This paper
proposes an Attention-enhanced Co-interactive Fusion Network (ACF-Net) for
automatic structural condition assessment in visual bridge inspection. The
ACF-Net can simultaneously parse structural elements and segment surface
defects on the elements in inspection images. It integrates two task-specific
relearning subnets to extract task-specific features from an overall feature
embedding and a co-interactive feature fusion module to capture the spatial
correlation and facilitate information sharing between tasks. Experimental
results demonstrate that the proposed ACF-Net outperforms the current
state-of-the-art approaches, achieving promising performance with 92.11% mIoU
for element parsing and 87.16% mIoU for corrosion segmentation on the new
benchmark dataset Steel Bridge Condition Inspection Visual (SBCIV) testing set.
An ablation study reveals the strengths of ACF-Net, and a case study showcases
its capability to automate structural condition assessment. The code will be
open-source after acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhaozheng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1&quot;&gt;Ruwen Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07653">
<title>RFLA: A Stealthy Reflected Light Adversarial Attack in the Physical World. (arXiv:2307.07653v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07653</link>
<description rdf:parseType="Literal">&lt;p&gt;Physical adversarial attacks against deep neural networks (DNNs) have
recently gained increasing attention. The current mainstream physical attacks
use printed adversarial patches or camouflage to alter the appearance of the
target object. However, these approaches generate conspicuous adversarial
patterns that show poor stealthiness. Another physical deployable attack is the
optical attack, featuring stealthiness while exhibiting weakly in the daytime
with sunlight. In this paper, we propose a novel Reflected Light Attack (RFLA),
featuring effective and stealthy in both the digital and physical world, which
is implemented by placing the color transparent plastic sheet and a paper cut
of a specific shape in front of the mirror to create different colored
geometries on the target object. To achieve these goals, we devise a general
framework based on the circle to model the reflected light on the target
object. Specifically, we optimize a circle (composed of a coordinate and
radius) to carry various geometrical shapes determined by the optimized angle.
The fill color of the geometry shape and its corresponding transparency are
also optimized. We extensively evaluate the effectiveness of RFLA on different
datasets and models. Experiment results suggest that the proposed method
achieves over 99% success rate on different datasets and models in the digital
world. Additionally, we verify the effectiveness of the proposed method in
different physical environments by using sunlight or a flashlight.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Donghua Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wen Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1&quot;&gt;Tingsong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoqian Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07662">
<title>MPDIoU: A Loss for Efficient and Accurate Bounding Box Regression. (arXiv:2307.07662v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07662</link>
<description rdf:parseType="Literal">&lt;p&gt;Bounding box regression (BBR) has been widely used in object detection and
instance segmentation, which is an important step in object localization.
However, most of the existing loss functions for bounding box regression cannot
be optimized when the predicted box has the same aspect ratio as the
groundtruth box, but the width and height values are exactly different. In
order to tackle the issues mentioned above, we fully explore the geometric
features of horizontal rectangle and propose a novel bounding box similarity
comparison metric MPDIoU based on minimum point distance, which contains all of
the relevant factors considered in the existing loss functions, namely
overlapping or non-overlapping area, central points distance, and deviation of
width and height, while simplifying the calculation process. On this basis, we
propose a bounding box regression loss function based on MPDIoU, called LMPDIoU
. Experimental results show that the MPDIoU loss function is applied to
state-of-the-art instance segmentation (e.g., YOLACT) and object detection
(e.g., YOLOv7) model trained on PASCAL VOC, MS COCO, and IIIT5k outperforms
existing loss functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siliang_M/0/1/0/all/0/1&quot;&gt;Ma Siliang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yong_X/0/1/0/all/0/1&quot;&gt;Xu Yong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07663">
<title>INVE: Interactive Neural Video Editing. (arXiv:2307.07663v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07663</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Interactive Neural Video Editing (INVE), a real-time video editing
solution, which can assist the video editing process by consistently
propagating sparse frame edits to the entire video clip. Our method is inspired
by the recent work on Layered Neural Atlas (LNA). LNA, however, suffers from
two major drawbacks: (1) the method is too slow for interactive editing, and
(2) it offers insufficient support for some editing use cases, including direct
frame editing and rigid texture tracking. To address these challenges we
leverage and adopt highly efficient network architectures, powered by
hash-grids encoding, to substantially improve processing speed. In addition, we
learn bi-directional functions between image-atlas and introduce vectorized
editing, which collectively enables a much greater variety of edits in both the
atlas and the frames directly. Compared to LNA, our INVE reduces the learning
and inference time by a factor of 5, and supports various video editing
operations that LNA cannot. We showcase the superiority of INVE over LNA in
interactive video editing through a comprehensive quantitative and qualitative
analysis, highlighting its numerous advantages and improved performance. For
video results, please see https://gabriel-huang.github.io/inve/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiahui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1&quot;&gt;Leonid Sigal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1&quot;&gt;Kwang Moo Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_O/0/1/0/all/0/1&quot;&gt;Oliver Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Joon-Young Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07677">
<title>Learning from Pseudo-labeled Segmentation for Multi-Class Object Counting. (arXiv:2307.07677v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07677</link>
<description rdf:parseType="Literal">&lt;p&gt;Class-agnostic counting (CAC) has numerous potential applications across
various domains. The goal is to count objects of an arbitrary category during
testing, based on only a few annotated exemplars. In this paper, we point out
that the task of counting objects of interest when there are multiple object
classes in the image (namely, multi-class object counting) is particularly
challenging for current object counting models. They often greedily count every
object regardless of the exemplars. To address this issue, we propose
localizing the area containing the objects of interest via an exemplar-based
segmentation model before counting them. The key challenge here is the lack of
segmentation supervision to train this model. To this end, we propose a method
to obtain pseudo segmentation masks using only box exemplars and dot
annotations. We show that the segmentation model trained on these
pseudo-labeled masks can effectively localize objects of interest for an
arbitrary multi-class image based on the exemplars. To evaluate the performance
of different methods on multi-class counting, we introduce two new benchmarks,
a synthetic multi-class dataset and a new test set of real images in which
objects from multiple classes are present. Our proposed method shows a
significant advantage over the previous CAC methods on these two benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jingyi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1&quot;&gt;Hieu Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1&quot;&gt;Dimitris Samaras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07678">
<title>Both Spatial and Frequency Cues Contribute to High-Fidelity Image Inpainting. (arXiv:2307.07678v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07678</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative approaches have obtained great success in image inpainting
recently. However, most generative inpainting networks suffer from either
over-smooth results or aliasing artifacts. The former lacks high-frequency
details, while the latter lacks semantic structure. To address this issue, we
propose an effective Frequency-Spatial Complementary Network (FSCN) by
exploiting rich semantic information in both spatial and frequency domains.
Specifically, we introduce an extra Frequency Branch and Frequency Loss on the
spatial-based network to impose direct supervision on the frequency
information, and propose a Frequency-Spatial Cross-Attention Block (FSCAB) to
fuse multi-domain features and combine the corresponding characteristics. With
our FSCAB, the inpainting network is capable of capturing frequency information
and preserving visual consistency simultaneously. Extensive quantitative and
qualitative experiments demonstrate that our inpainting network can effectively
achieve superior results, outperforming previous state-of-the-art approaches
with significantly fewer parameters and less computation cost. The code will be
released soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Ze Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1&quot;&gt;Yalei Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_P/0/1/0/all/0/1&quot;&gt;Pengfei Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07680">
<title>Semantic Contrastive Bootstrapping for Single-positive Multi-label Recognition. (arXiv:2307.07680v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07680</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning multi-label image recognition with incomplete annotation is gaining
popularity due to its superior performance and significant labor savings when
compared to training with fully labeled datasets. Existing literature mainly
focuses on label completion and co-occurrence learning while facing
difficulties with the most common single-positive label manner. To tackle this
problem, we present a semantic contrastive bootstrapping (Scob) approach to
gradually recover the cross-object relationships by introducing class
activation as semantic guidance. With this learning guidance, we then propose a
recurrent semantic masked transformer to extract iconic object-level
representations and delve into the contrastive learning problems on multi-label
classification tasks. We further propose a bootstrapping framework in an
Expectation-Maximization fashion that iteratively optimizes the network
parameters and refines semantic guidance to alleviate possible disturbance
caused by wrong semantic guidance. Extensive experimental results demonstrate
that the proposed joint learning framework surpasses the state-of-the-art
models by a large margin on four public multi-label image recognition
benchmarks. Codes can be found at https://github.com/iCVTEAM/Scob.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07688">
<title>DRM-IR: Task-Adaptive Deep Unfolding Network for All-In-One Image Restoration. (arXiv:2307.07688v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07688</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing All-In-One image restoration (IR) methods usually lack flexible
modeling on various types of degradation, thus impeding the restoration
performance. To achieve All-In-One IR with higher task dexterity, this work
proposes an efficient Dynamic Reference Modeling paradigm (DRM-IR), which
consists of task-adaptive degradation modeling and model-based image restoring.
Specifically, these two subtasks are formalized as a pair of entangled
reference-based maximum a posteriori (MAP) inferences, which are optimized
synchronously in an unfolding-based manner. With the two cascaded subtasks,
DRM-IR first dynamically models the task-specific degradation based on a
reference image pair and further restores the image with the collected
degradation statistics. Besides, to bridge the semantic gap between the
reference and target degraded images, we further devise a Degradation Prior
Transmitter (DPT) that restrains the instance-specific feature differences.
DRM-IR explicitly provides superior flexibility for All-in-One IR while being
interpretable. Extensive experiments on multiple benchmark datasets show that
our DRM-IR achieves state-of-the-art in All-In-One IR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yuanshuo Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1&quot;&gt;Mingwen Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1&quot;&gt;Yecong Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wangmeng Zuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07691">
<title>A Survey on Change Detection Techniques in Document Images. (arXiv:2307.07691v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07691</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of change detection in images finds application in different
domains like diagnosis of diseases in the medical field, detecting growth
patterns of cities through remote sensing, and finding changes in legal
documents and contracts. However, this paper presents a survey on core
techniques and rules to detect changes in different versions of a document
image. Our discussions on change detection focus on two categories --
content-based and layout-based. The content-based techniques intelligently
extract and analyze the image contents (text or non-text) to show the possible
differences, whereas the layout-based techniques use structural information to
predict document changes. We also summarize the existing datasets and
evaluation metrics used in change detection experiments. The shortcomings and
challenges the existing methods face are reported, along with some pointers for
future research work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pun_A/0/1/0/all/0/1&quot;&gt;Abhinandan Kumar Pun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1&quot;&gt;Mohammed Javed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1&quot;&gt;David S. Doermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07693">
<title>Neural Deformable Models for 3D Bi-Ventricular Heart Shape Reconstruction and Modeling from 2D Sparse Cardiac Magnetic Resonance Imaging. (arXiv:2307.07693v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07693</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel neural deformable model (NDM) targeting at the
reconstruction and modeling of 3D bi-ventricular shape of the heart from 2D
sparse cardiac magnetic resonance (CMR) imaging data. We model the
bi-ventricular shape using blended deformable superquadrics, which are
parameterized by a set of geometric parameter functions and are capable of
deforming globally and locally. While global geometric parameter functions and
deformations capture gross shape features from visual data, local deformations,
parameterized as neural diffeomorphic point flows, can be learned to recover
the detailed heart shape.Different from iterative optimization methods used in
conventional deformable model formulations, NDMs can be trained to learn such
geometric parameter functions, global and local deformations from a shape
distribution manifold. Our NDM can learn to densify a sparse cardiac point
cloud with arbitrary scales and generate high-quality triangular meshes
automatically. It also enables the implicit learning of dense correspondences
among different heart shape instances for accurate cardiac shape registration.
Furthermore, the parameters of NDM are intuitive, and can be used by a
physician without sophisticated post-processing. Experimental results on a
large CMR dataset demonstrate the improved performance of NDM over conventional
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1&quot;&gt;Meng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanski_M/0/1/0/all/0/1&quot;&gt;Mikael Kanski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Axel_L/0/1/0/all/0/1&quot;&gt;Leon Axel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1&quot;&gt;Dimitris Metaxas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07708">
<title>PSGformer: Enhancing 3D Point Cloud Instance Segmentation via Precise Semantic Guidance. (arXiv:2307.07708v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07708</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing 3D instance segmentation methods are derived from 3D semantic
segmentation models. However, these indirect approaches suffer from certain
limitations. They fail to fully leverage global and local semantic information
for accurate prediction, which hampers the overall performance of the 3D
instance segmentation framework. To address these issues, this paper presents
PSGformer, a novel 3D instance segmentation network. PSGformer incorporates two
key advancements to enhance the performance of 3D instance segmentation.
Firstly, we propose a Multi-Level Semantic Aggregation Module, which
effectively captures scene features by employing foreground point filtering and
multi-radius aggregation. This module enables the acquisition of more detailed
semantic information from global and local perspectives. Secondly, PSGformer
introduces a Parallel Feature Fusion Transformer Module that independently
processes super-point features and aggregated features using transformers. The
model achieves a more comprehensive feature representation by the features
which connect global and local features. We conducted extensive experiments on
the ScanNetv2 dataset. Notably, PSGformer exceeds compared state-of-the-art
methods by 2.2% on ScanNetv2 hidden test set in terms of mAP. Our code and
models will be publicly released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Lei Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luan_W/0/1/0/all/0/1&quot;&gt;Wuyang Luan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yuan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1&quot;&gt;Qiang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junhui Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07710">
<title>ExposureDiffusion: Learning to Expose for Low-light Image Enhancement. (arXiv:2307.07710v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07710</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous raw image-based low-light image enhancement methods predominantly
relied on feed-forward neural networks to learn deterministic mappings from
low-light to normally-exposed images. However, they failed to capture critical
distribution information, leading to visually undesirable results. This work
addresses the issue by seamlessly integrating a diffusion model with a
physics-based exposure model. Different from a vanilla diffusion model that has
to perform Gaussian denoising, with the injected physics-based exposure model,
our restoration process can directly start from a noisy image instead of pure
noise. As such, our method obtains significantly improved performance and
reduced inference time compared with vanilla diffusion models. To make full use
of the advantages of different intermediate steps, we further propose an
adaptive residual layer that effectively screens out the side-effect in the
iterative refinement when the intermediate results have been already
well-exposed. The proposed framework can work with both real-paired datasets,
SOTA noise models, and different backbone networks. Note that, the proposed
framework is compatible with real-paired datasets, real/synthetic noise models,
and different backbone networks. We evaluate the proposed method on various
public benchmarks, achieving promising results with consistent improvements
using different exposure models and backbones. Besides, the proposed method
achieves better generalization capacity for unseen amplifying ratios and better
performance than a larger feedforward neural model when few parameters are
adopted.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yufei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenhan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1&quot;&gt;Lanqing Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_L/0/1/0/all/0/1&quot;&gt;Lap-Pui Chau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1&quot;&gt;Alex C. Kot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1&quot;&gt;Bihan Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07720">
<title>Spatial-Spectral Hyperspectral Classification based on Learnable 3D Group Convolution. (arXiv:2307.07720v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07720</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have faced many problems in hyperspectral image
classification, including the ineffective utilization of spectral-spatial joint
information and the problems of gradient vanishing and overfitting that arise
with increasing depth. In order to accelerate the deployment of models on edge
devices with strict latency requirements and limited computing power, this
paper proposes a learnable group convolution network (LGCNet) based on an
improved 3D-DenseNet model and a lightweight model design. The LGCNet module
improves the shortcomings of group convolution by introducing a dynamic
learning method for the input channels and convolution kernel grouping,
enabling flexible grouping structures and generating better representation
ability. Through the overall loss and gradient of the backpropagation network,
the 3D group convolution is dynamically determined and updated in an end-to-end
manner. The learnable number of channels and corresponding grouping can capture
different complementary visual features of input images, allowing the CNN to
learn richer feature representations. When extracting high-dimensional and
redundant hyperspectral data, the 3D convolution kernels also contain a large
amount of redundant information. The LGC module allows the 3D-DenseNet to
choose channel information with more semantic features, and is very efficient,
making it suitable for embedding in any deep neural network for acceleration
and efficiency improvements. LGC enables the 3D-CNN to achieve sufficient
feature extraction while also meeting speed and computing requirements.
Furthermore, LGCNet has achieved progress in inference speed and accuracy, and
outperforms mainstream hyperspectral image classification methods on the Indian
Pines, Pavia University, and KSC datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guandong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1&quot;&gt;Mengxia Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07725">
<title>Improving Translation Invariance in Convolutional Neural Networks with Peripheral Prediction Padding. (arXiv:2307.07725v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07725</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero padding is often used in convolutional neural networks to prevent the
feature map size from decreasing with each layer. However, recent studies have
shown that zero padding promotes encoding of absolute positional information,
which may adversely affect the performance of some tasks. In this work, a novel
padding method called Peripheral Prediction Padding (PP-Pad) method is
proposed, which enables end-to-end training of padding values suitable for each
task instead of zero padding. Moreover, novel metrics to quantitatively
evaluate the translation invariance of the model are presented. By evaluating
with these metrics, it was confirmed that the proposed method achieved higher
accuracy and translation invariance than the previous methods in a semantic
segmentation task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukai_K/0/1/0/all/0/1&quot;&gt;Kensuke Mukai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamanaka_T/0/1/0/all/0/1&quot;&gt;Takao Yamanaka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07729">
<title>Improving NeRF with Height Data for Utilization of GIS Data. (arXiv:2307.07729v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07729</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRF) has been applied to various tasks related to
representations of 3D scenes. Most studies based on NeRF have focused on a
small object, while a few studies have tried to reconstruct large-scale scenes
although these methods tend to require large computational cost. For the
application of NeRF to large-scale scenes, a method based on NeRF is proposed
in this paper to effectively use height data which can be obtained from GIS
(Geographic Information System). For this purpose, the scene space was divided
into multiple objects and a background using the height data to represent them
with separate neural networks. In addition, an adaptive sampling method is also
proposed by using the height data. As a result, the accuracy of image rendering
was improved with faster training speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aoki_H/0/1/0/all/0/1&quot;&gt;Hinata Aoki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamanaka_T/0/1/0/all/0/1&quot;&gt;Takao Yamanaka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07732">
<title>Prawn Morphometrics and Weight Estimation from Images using Deep Learning for Landmark Localization. (arXiv:2307.07732v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07732</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate weight estimation and morphometric analyses are useful in
aquaculture for optimizing feeding, predicting harvest yields, identifying
desirable traits for selective breeding, grading processes, and monitoring the
health status of production animals. However, the collection of phenotypic data
through traditional manual approaches at industrial scales and in real-time is
time-consuming, labour-intensive, and prone to errors. Digital imaging of
individuals and subsequent training of prediction models using Deep Learning
(DL) has the potential to rapidly and accurately acquire phenotypic data from
aquaculture species. In this study, we applied a novel DL approach to automate
weight estimation and morphometric analysis using the black tiger prawn
(Penaeus monodon) as a model crustacean. The DL approach comprises two main
components: a feature extraction module that efficiently combines low-level and
high-level features using the Kronecker product operation; followed by a
landmark localization module that then uses these features to predict the
coordinates of key morphological points (landmarks) on the prawn body. Once
these landmarks were extracted, weight was estimated using a weight regression
module based on the extracted landmarks using a fully connected network. For
morphometric analyses, we utilized the detected landmarks to derive five
important prawn traits. Principal Component Analysis (PCA) was also used to
identify landmark-derived distances, which were found to be highly correlated
with shape features such as body length, and width. We evaluated our approach
on a large dataset of 8164 images of the Black tiger prawn (Penaeus monodon)
collected from Australian farms. Our experimental results demonstrate that the
novel DL approach outperforms existing DL methods in terms of accuracy,
robustness, and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saleh_A/0/1/0/all/0/1&quot;&gt;Alzayat Saleh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1&quot;&gt;Md Mehedi Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raadsma_H/0/1/0/all/0/1&quot;&gt;Herman W Raadsma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khatkar_M/0/1/0/all/0/1&quot;&gt;Mehar S Khatkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jerry_D/0/1/0/all/0/1&quot;&gt;Dean R Jerry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azghadi_M/0/1/0/all/0/1&quot;&gt;Mostafa Rahimi Azghadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07742">
<title>SINC: Self-Supervised In-Context Learning for Vision-Language Tasks. (arXiv:2307.07742v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07742</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Pre-trained Transformers exhibit an intriguing capacity for in-context
learning. Without gradient updates, these models can rapidly construct new
predictors from demonstrations presented in the inputs. Recent works promote
this ability in the vision-language domain by incorporating visual information
into large language models that can already make in-context predictions.
However, these methods could inherit issues in the language domain, such as
template sensitivity and hallucination. Also, the scale of these language
models raises a significant demand for computations, making learning and
operating these models resource-intensive. To this end, we raise a question:
``How can we enable in-context learning for general models without being
constrained on large language models?&quot;. To answer it, we propose a succinct and
general framework, Self-supervised IN-Context learning (SINC), that introduces
a meta-model to learn on self-supervised prompts consisting of tailored
demonstrations. The learned models can be transferred to downstream tasks for
making in-context predictions on-the-fly. Extensive experiments show that SINC
outperforms gradient-based methods in various vision-language tasks under
few-shot settings. Furthermore, the designs of SINC help us investigate the
benefits of in-context learning across different tasks, and the analysis
further reveals the essential components for the emergence of in-context
learning in the vision-language domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yi-Syuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yun-Zhu Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeo_C/0/1/0/all/0/1&quot;&gt;Cheng Yu Yeo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jianlong Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shuai_H/0/1/0/all/0/1&quot;&gt;Hong-Han Shuai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07754">
<title>Bidirectionally Deformable Motion Modulation For Video-based Human Pose Transfer. (arXiv:2307.07754v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07754</link>
<description rdf:parseType="Literal">&lt;p&gt;Video-based human pose transfer is a video-to-video generation task that
animates a plain source human image based on a series of target human poses.
Considering the difficulties in transferring highly structural patterns on the
garments and discontinuous poses, existing methods often generate
unsatisfactory results such as distorted textures and flickering artifacts. To
address these issues, we propose a novel Deformable Motion Modulation (DMM)
that utilizes geometric kernel offset with adaptive weight modulation to
simultaneously perform feature alignment and style transfer. Different from
normal style modulation used in style transfer, the proposed modulation
mechanism adaptively reconstructs smoothed frames from style codes according to
the object shape through an irregular receptive field of view. To enhance the
spatio-temporal consistency, we leverage bidirectional propagation to extract
the hidden motion information from a warped image sequence generated by noisy
poses. The proposed feature propagation significantly enhances the motion
prediction ability by forward and backward propagation. Both quantitative and
qualitative experimental results demonstrate superiority over the
state-of-the-arts in terms of image fidelity and visual continuity. The source
code is publicly available at github.com/rocketappslab/bdmm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wing-Yin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Po_L/0/1/0/all/0/1&quot;&gt;Lai-Man Po&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_R/0/1/0/all/0/1&quot;&gt;Ray Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yuzhi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yu Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07757">
<title>Open Scene Understanding: Grounded Situation Recognition Meets Segment Anything for Helping People with Visual Impairments. (arXiv:2307.07757v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07757</link>
<description rdf:parseType="Literal">&lt;p&gt;Grounded Situation Recognition (GSR) is capable of recognizing and
interpreting visual scenes in a contextually intuitive way, yielding salient
activities (verbs) and the involved entities (roles) depicted in images. In
this work, we focus on the application of GSR in assisting people with visual
impairments (PVI). However, precise localization information of detected
objects is often required to navigate their surroundings confidently and make
informed decisions. For the first time, we propose an Open Scene Understanding
(OpenSU) system that aims to generate pixel-wise dense segmentation masks of
involved entities instead of bounding boxes. Specifically, we build our OpenSU
system on top of GSR by additionally adopting an efficient Segment Anything
Model (SAM). Furthermore, to enhance the feature extraction and interaction
between the encoder-decoder structure, we construct our OpenSU system using a
solid pure transformer backbone to improve the performance of GSR. In order to
accelerate the convergence, we replace all the activation functions within the
GSR decoders with GELU, thereby reducing the training duration. In quantitative
analysis, our model achieves state-of-the-art performance on the SWiG dataset.
Moreover, through field testing on dedicated assistive technology datasets and
application demonstrations, the proposed OpenSU system can be used to enhance
scene understanding and facilitate the independent mobility of people with
visual impairments. Our code will be available at
https://github.com/RuipingL/OpenSU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ruiping Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1&quot;&gt;Kunyu Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Junwei Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_K/0/1/0/all/0/1&quot;&gt;Ke Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yufan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kailun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1&quot;&gt;Rainer Stiefelhagen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07763">
<title>Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents. (arXiv:2307.07763v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.07763</link>
<description rdf:parseType="Literal">&lt;p&gt;The mobile robot relies on SLAM (Simultaneous Localization and Mapping) to
provide autonomous navigation and task execution in complex and unknown
environments. However, it is hard to develop a dedicated algorithm for mobile
robots due to dynamic and challenging situations, such as poor lighting
conditions and motion blur. To tackle this issue, we propose a tightly-coupled
LiDAR-visual SLAM based on geometric features, which includes two sub-systems
(LiDAR and monocular visual SLAM) and a fusion framework. The fusion framework
associates the depth and semantics of the multi-modal geometric features to
complement the visual line landmarks and to add direction optimization in
Bundle Adjustment (BA). This further constrains visual odometry. On the other
hand, the entire line segment detected by the visual subsystem overcomes the
limitation of the LiDAR subsystem, which can only perform the local calculation
for geometric features. It adjusts the direction of linear feature points and
filters out outliers, leading to a higher accurate odometry system. Finally, we
employ a module to detect the subsystem&apos;s operation, providing the LiDAR
subsystem&apos;s output as a complementary trajectory to our system while visual
subsystem tracking fails. The evaluation results on the public dataset M2DGR,
gathered from ground robots across various indoor and outdoor scenarios, show
that our system achieves more accurate and robust pose estimation compared to
current state-of-the-art multi-modal methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_K/0/1/0/all/0/1&quot;&gt;Ke Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ruiping Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1&quot;&gt;Kunyu Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Junwei Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kailun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1&quot;&gt;Rainer Stiefelhagen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07768">
<title>SoccerKDNet: A Knowledge Distillation Framework for Action Recognition in Soccer Videos. (arXiv:2307.07768v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07768</link>
<description rdf:parseType="Literal">&lt;p&gt;Classifying player actions from soccer videos is a challenging problem, which
has become increasingly important in sports analytics over the years. Most
state-of-the-art methods employ highly complex offline networks, which makes it
difficult to deploy such models in resource constrained scenarios. Here, in
this paper we propose a novel end-to-end knowledge distillation based transfer
learning network pre-trained on the Kinetics400 dataset and then perform
extensive analysis on the learned framework by introducing a unique loss
parameterization. We also introduce a new dataset named SoccerDB1 containing
448 videos and consisting of 4 diverse classes each of players playing soccer.
Furthermore, we introduce an unique loss parameter that help us linearly weigh
the extent to which the predictions of each network are utilized. Finally, we
also perform a thorough performance study using various changed
hyperparameters. We also benchmark the first classification results on the new
SoccerDB1 dataset obtaining 67.20% validation accuracy. Apart from
outperforming prior arts significantly, our model also generalizes to new
datasets easily. The dataset has been made publicly available at:
https://bit.ly/soccerdb1
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bose_S/0/1/0/all/0/1&quot;&gt;Sarosij Bose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Saikat Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1&quot;&gt;Amlan Chakrabarti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07790">
<title>Adaptive Nonlinear Latent Transformation for Conditional Face Editing. (arXiv:2307.07790v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07790</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent works for face editing usually manipulate the latent space of StyleGAN
via the linear semantic directions. However, they usually suffer from the
entanglement of facial attributes, need to tune the optimal editing strength,
and are limited to binary attributes with strong supervision signals. This
paper proposes a novel adaptive nonlinear latent transformation for
disentangled and conditional face editing, termed AdaTrans. Specifically, our
AdaTrans divides the manipulation process into several finer steps; i.e., the
direction and size at each step are conditioned on both the facial attributes
and the latent codes. In this way, AdaTrans describes an adaptive nonlinear
transformation trajectory to manipulate the faces into target attributes while
keeping other attributes unchanged. Then, AdaTrans leverages a predefined
density model to constrain the learned trajectory in the distribution of latent
codes by maximizing the likelihood of transformed latent code. Moreover, we
also propose a disentangled learning strategy under a mutual information
framework to eliminate the entanglement among attributes, which can further
relax the need for labeled data. Consequently, AdaTrans enables a controllable
face editing with the advantages of disentanglement, flexibility with
non-binary attributes, and high fidelity. Extensive experimental results on
various facial attributes demonstrate the qualitative and quantitative
effectiveness of the proposed AdaTrans over existing state-of-the-art methods,
especially in the most challenging scenarios with a large age gap and few
labeled examples. The source code is available at
https://github.com/Hzzone/AdaTrans.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhizhong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Siteng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1&quot;&gt;Hongming Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07791">
<title>Joint Adversarial and Collaborative Learning for Self-Supervised Action Recognition. (arXiv:2307.07791v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07791</link>
<description rdf:parseType="Literal">&lt;p&gt;Considering the instance-level discriminative ability, contrastive learning
methods, including MoCo and SimCLR, have been adapted from the original image
representation learning task to solve the self-supervised skeleton-based action
recognition task. These methods usually use multiple data streams (i.e., joint,
motion, and bone) for ensemble learning, meanwhile, how to construct a
discriminative feature space within a single stream and effectively aggregate
the information from multiple streams remains an open problem. To this end, we
first apply a new contrastive learning method called BYOL to learn from
skeleton data and formulate SkeletonBYOL as a simple yet effective baseline for
self-supervised skeleton-based action recognition. Inspired by SkeletonBYOL, we
further present a joint Adversarial and Collaborative Learning (ACL) framework,
which combines Cross-Model Adversarial Learning (CMAL) and Cross-Stream
Collaborative Learning (CSCL). Specifically, CMAL learns single-stream
representation by cross-model adversarial loss to obtain more discriminative
features. To aggregate and interact with multi-stream information, CSCL is
designed by generating similarity pseudo label of ensemble learning as
supervision and guiding feature generation for individual streams. Exhaustive
experiments on three datasets verify the complementary properties between CMAL
and CSCL and also verify that our method can perform favorably against
state-of-the-art methods using various evaluation protocols. Our code and
models are publicly available at \url{https://github.com/Levigty/ACL}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1&quot;&gt;Tianyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mengyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jingwen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yidi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07807">
<title>MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis. (arXiv:2307.07807v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.07807</link>
<description rdf:parseType="Literal">&lt;p&gt;Early diagnosis of renal cancer can greatly improve the survival rate of
patients. Contrast-enhanced ultrasound (CEUS) is a cost-effective and
non-invasive imaging technique and has become more and more frequently used for
renal tumor diagnosis. However, the classification of benign and malignant
renal tumors can still be very challenging due to the highly heterogeneous
appearance of cancer and imaging artifacts. Our aim is to detect and classify
renal tumors by integrating B-mode and CEUS-mode ultrasound videos. To this
end, we propose a novel multi-modal ultrasound video fusion network that can
effectively perform multi-modal feature fusion and video classification for
renal tumor diagnosis. The attention-based multi-modal fusion module uses
cross-attention and self-attention to extract modality-invariant features and
modality-specific features in parallel. In addition, we design an object-level
temporal aggregation (OTA) module that can automatically filter low-quality
features and efficiently integrate temporal information from multiple frames to
improve the accuracy of tumor diagnosis. Experimental results on a multicenter
dataset show that the proposed framework outperforms the single-modal models
and the competing methods. Furthermore, our OTA module achieves higher
classification accuracy than the frame-level predictions. Our code is available
at \url{https://github.com/JeunyuLi/MUAF}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Han Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ni_D/0/1/0/all/0/1&quot;&gt;Dong Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xue_W/0/1/0/all/0/1&quot;&gt;Wufeng Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Dongmei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jun Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07812">
<title>Multiscale Memory Comparator Transformer for Few-Shot Video Segmentation. (arXiv:2307.07812v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07812</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot video segmentation is the task of delineating a specific novel class
in a query video using few labelled support images. Typical approaches compare
support and query features while limiting comparisons to a single feature layer
and thereby ignore potentially valuable information. We present a meta-learned
Multiscale Memory Comparator (MMC) for few-shot video segmentation that
combines information across scales within a transformer decoder. Typical
multiscale transformer decoders for segmentation tasks learn a compressed
representation, their queries, through information exchange across scales.
Unlike previous work, we instead preserve the detailed feature maps during
across scale information exchange via a multiscale memory transformer decoding
to reduce confusion between the background and novel class. Integral to the
approach, we investigate multiple forms of information exchange across scales
in different tasks and provide insights with empirical evidence on which to use
in each task. The overall comparisons among query and support features benefit
from both rich semantics and precise localization. We demonstrate our approach
primarily on few-shot video object segmentation and an adapted version on the
fully supervised counterpart. In all cases, our approach outperforms the
baseline and yields state-of-the-art performance. Our code is publicly
available at https://github.com/MSiam/MMC-MultiscaleMemory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siam_M/0/1/0/all/0/1&quot;&gt;Mennatullah Siam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karim_R/0/1/0/all/0/1&quot;&gt;Rezaul Karim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;He Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1&quot;&gt;Richard Wildes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07813">
<title>TinyTracker: Ultra-Fast and Ultra-Low-Power Edge Vision for In-Sensor Gaze Estimation. (arXiv:2307.07813v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07813</link>
<description rdf:parseType="Literal">&lt;p&gt;Intelligent edge vision tasks face the challenge of power and latency
efficiency as the computation load is normally heavy for edge platforms. This
work leverages one of the first &quot;AI in sensor&quot; vision platforms, IMX500 by
Sony, to achieve ultra-fast and ultra-low-power end-to-end edge vision
applications. We evaluate the IMX500 and compare it to other edge platforms,
such as the Google Coral Dev Micro and Sony Spresense, by exploring gaze
estimation as a case study. We propose TinyTracker, a highly efficient, fully
quantized model for 2D gaze estimation designed to maximize the performance of
the edge vision systems considered in this study. TinyTracker achieves a 41x
size reduction (600Kb) compared to iTracker [1] without significant loss in
gaze estimation accuracy (maximum of 0.16 cm when fully quantized).
TinyTracker&apos;s deployment on the Sony IMX500 vision sensor results in end-to-end
latency of around 19ms. The camera takes around 17.9ms to read, process and
transmit the pixels to the accelerator. The inference time of the network is
0.86ms with an additional 0.24 ms for retrieving the results from the sensor.
and the overall energy consumption of the end-to-end system is 4.9 mJ,
including 0.06 mJ for inference. The end-to-end study shows that IMX500 is 1.7x
faster than CoralMicro (19ms vs 34.4ms) and 7x more power efficient (4.9mJ VS
34.2mJ)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonazzi_P/0/1/0/all/0/1&quot;&gt;Pietro Bonazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruegg_T/0/1/0/all/0/1&quot;&gt;Thomas Ruegg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1&quot;&gt;Sizhen Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magno_M/0/1/0/all/0/1&quot;&gt;Michele Magno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07829">
<title>HQG-Net: Unpaired Medical Image Enhancement with High-Quality Guidance. (arXiv:2307.07829v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.07829</link>
<description rdf:parseType="Literal">&lt;p&gt;Unpaired Medical Image Enhancement (UMIE) aims to transform a low-quality
(LQ) medical image into a high-quality (HQ) one without relying on paired
images for training. While most existing approaches are based on
Pix2Pix/CycleGAN and are effective to some extent, they fail to explicitly use
HQ information to guide the enhancement process, which can lead to undesired
artifacts and structural distortions. In this paper, we propose a novel UMIE
approach that avoids the above limitation of existing methods by directly
encoding HQ cues into the LQ enhancement process in a variational fashion and
thus model the UMIE task under the joint distribution between the LQ and HQ
domains. Specifically, we extract features from an HQ image and explicitly
insert the features, which are expected to encode HQ cues, into the enhancement
network to guide the LQ enhancement with the variational normalization module.
We train the enhancement network adversarially with a discriminator to ensure
the generated HQ image falls into the HQ domain. We further propose a
content-aware loss to guide the enhancement process with wavelet-based
pixel-level and multi-encoder-based feature-level constraints. Additionally, as
a key motivation for performing image enhancement is to make the enhanced
images serve better for downstream tasks, we propose a bi-level learning scheme
to optimize the UMIE task and downstream tasks cooperatively, helping generate
HQ images both visually appealing and favorable for downstream tasks.
Experiments on three medical datasets, including two newly collected datasets,
verify that the proposed method outperforms existing techniques in terms of
both enhancement quality and downstream task performance. We will make the code
and the newly collected datasets publicly available for community study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_C/0/1/0/all/0/1&quot;&gt;Chunming He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_G/0/1/0/all/0/1&quot;&gt;Guoxia Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Jiangpeng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Longxiang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yulun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaowei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07847">
<title>Neural Video Recovery for Cloud Gaming. (arXiv:2307.07847v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2307.07847</link>
<description rdf:parseType="Literal">&lt;p&gt;Cloud gaming is a multi-billion dollar industry. A client in cloud gaming
sends its movement to the game server on the Internet, which renders and
transmits the resulting video back. In order to provide a good gaming
experience, a latency below 80 ms is required. This means that video rendering,
encoding, transmission, decoding, and display have to finish within that time
frame, which is especially challenging to achieve due to server overload,
network congestion, and losses. In this paper, we propose a new method for
recovering lost or corrupted video frames in cloud gaming. Unlike traditional
video frame recovery, our approach uses game states to significantly enhance
recovery accuracy and utilizes partially decoded frames to recover lost
portions. We develop a holistic system that consists of (i) efficiently
extracting game states, (ii) modifying H.264 video decoder to generate a mask
to indicate which portions of video frames need recovery, and (iii) designing a
novel neural network to recover either complete or partial video frames. Our
approach is extensively evaluated using iPhone 12 and laptop implementations,
and we demonstrate the utility of game states in the game video recovery and
the effectiveness of our overall design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhaoyuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yifan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuozhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1&quot;&gt;Diyuan Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Lili Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07859">
<title>Unified Adversarial Patch for Cross-modal Attacks in the Physical World. (arXiv:2307.07859v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07859</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, physical adversarial attacks have been presented to evade
DNNs-based object detectors. To ensure the security, many scenarios are
simultaneously deployed with visible sensors and infrared sensors, leading to
the failures of these single-modal physical attacks. To show the potential
risks under such scenes, we propose a unified adversarial patch to perform
cross-modal physical attacks, i.e., fooling visible and infrared object
detectors at the same time via a single patch. Considering different imaging
mechanisms of visible and infrared sensors, our work focuses on modeling the
shapes of adversarial patches, which can be captured in different modalities
when they change. To this end, we design a novel boundary-limited shape
optimization to achieve the compact and smooth shapes, and thus they can be
easily implemented in the physical world. In addition, to balance the fooling
degree between visible detector and infrared detector during the optimization
process, we propose a score-aware iterative evaluation, which can guide the
adversarial patch to iteratively reduce the predicted scores of the multi-modal
sensors. We finally test our method against the one-stage detector: YOLOv3 and
the two-stage detector: Faster RCNN. Results show that our unified patch
achieves an Attack Success Rate (ASR) of 73.33% and 69.17%, respectively. More
importantly, we verify the effective attacks in the physical world when visible
and infrared sensors shoot the objects under various settings like different
angles, distances, postures, and scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xingxing Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yitong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jie Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07873">
<title>Towards Understanding Adversarial Transferability From Surrogate Training. (arXiv:2307.07873v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07873</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs
that successfully fool white-box surrogate models can also deceive other
black-box models with different architectures. Although a bunch of empirical
studies have provided guidance on generating highly transferable AEs, many of
these findings lack explanations and even lead to inconsistent advice. In this
paper, we take a further step towards understanding adversarial
transferability, with a particular focus on surrogate aspects. Starting from
the intriguing little robustness phenomenon, where models adversarially trained
with mildly perturbed adversarial samples can serve as better surrogates, we
attribute it to a trade-off between two predominant factors: model smoothness
and gradient similarity. Our investigations focus on their joint effects,
rather than their separate correlations with transferability. Through a series
of theoretical and empirical analyses, we conjecture that the data distribution
shift in adversarial training explains the degradation of gradient similarity.
Building on these insights, we explore the impacts of data augmentation and
gradient regularization on transferability and identify that the trade-off
generally exists in the various training mechanisms, thus building a
comprehensive blueprint for the regulation mechanism behind transferability.
Finally, we provide a general route for constructing better surrogates to boost
transferability which optimizes both model smoothness and gradient similarity
simultaneously, e.g., the combination of input gradient regularization and
sharpness-aware minimization (SAM), validated by extensive experiments. In
summary, we call for attention to the united impacts of these two factors for
launching effective transfer attacks, rather than optimizing one while ignoring
the other, and emphasize the crucial role of manipulating surrogate models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yechao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shengshan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Leo Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Junyu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Minghui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaogeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1&quot;&gt;Wei Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Hai Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07887">
<title>Handwritten and Printed Text Segmentation: A Signature Case Study. (arXiv:2307.07887v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07887</link>
<description rdf:parseType="Literal">&lt;p&gt;While analyzing scanned documents, handwritten text can overlay printed text.
This causes difficulties during the optical character recognition (OCR) and
digitization process of documents, and subsequently, hurts downstream NLP
tasks. Prior research either focuses only on the binary classification of
handwritten text, or performs a three-class segmentation of the document, i.e.,
recognition of handwritten, printed, and background pixels. This results in the
assignment of the handwritten and printed overlapping pixels to only one of the
classes, and thus, they are not accounted for in the other class. Thus, in this
research, we develop novel approaches for addressing the challenges of
handwritten and printed text segmentation with the goal of recovering text in
different classes in whole, especially improving the segmentation performance
on the overlapping parts. As such, to facilitate with this task, we introduce a
new dataset, SignaTR6K, collected from real legal documents, as well as a new
model architecture for handwritten and printed text segmentation task. Our best
configuration outperforms the prior work on two different datasets by 17.9% and
7.3% on IoU scores.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gholamian_S/0/1/0/all/0/1&quot;&gt;Sina Gholamian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vahdat_A/0/1/0/all/0/1&quot;&gt;Ali Vahdat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07892">
<title>Multitemporal SAR images change detection and visualization using RABASAR and simplified GLR. (arXiv:2307.07892v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07892</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the state of changed areas requires that precise information be
given about the changes. Thus, detecting different kinds of changes is
important for land surface monitoring. SAR sensors are ideal to fulfil this
task, because of their all-time and all-weather capabilities, with good
accuracy of the acquisition geometry and without effects of atmospheric
constituents for amplitude data. In this study, we propose a simplified
generalized likelihood ratio ($S_{GLR}$) method assuming that corresponding
temporal pixels have the same equivalent number of looks (ENL). Thanks to the
denoised data provided by a ratio-based multitemporal SAR image denoising
method (RABASAR), we successfully applied this similarity test approach to
compute the change areas. A new change magnitude index method and an improved
spectral clustering-based change classification method are also developed. In
addition, we apply the simplified generalized likelihood ratio to detect the
maximum change magnitude time, and the change starting and ending times. Then,
we propose to use an adaptation of the REACTIV method to visualize the
detection results vividly. The effectiveness of the proposed methods is
demonstrated through the processing of simulated and SAR images, and the
comparison with classical techniques. In particular, numerical experiments
proved that the developed method has good performances in detecting farmland
area changes, building area changes, harbour area changes and flooding area
changes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Weiying Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deledalle_C/0/1/0/all/0/1&quot;&gt;Charles-Alban Deledalle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denis_L/0/1/0/all/0/1&quot;&gt;Lo&amp;#xef;c Denis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maitre_H/0/1/0/all/0/1&quot;&gt;Henri Ma&amp;#xee;tre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolas_J/0/1/0/all/0/1&quot;&gt;Jean-Marie Nicolas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tupin_F/0/1/0/all/0/1&quot;&gt;Florence Tupin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07893">
<title>Anomaly Detection in Automated Fibre Placement: Learning with Data Limitations. (arXiv:2307.07893v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07893</link>
<description rdf:parseType="Literal">&lt;p&gt;Current defect detection systems for Automated Fibre Placement (AFP) are
mostly based on end-to-end supervised learning methods requiring abundant
labelled defective samples, which are not easily generated in sufficient
numbers. To address this data scarcity problem, we introduce an
autoencoder-based approach compatible with small datasets. Fortunately, the
problem from a foundational point of view can be simplified as a binary
classification between normal and abnormal samples. The proposed approach uses
a depth map of the fibre layup surface, split into small windows aligned to
each composite strip (tow). A subset of these windows that do not contain
anomalies is passed to an autoencoder to reconstruct the input. Because the
autoencoder is trained with normal samples, it produces more accurate
reconstructions for these samples than for abnormal ones. Therefore, the value
of reconstruction error is used as a quantitative metric for whether there are
potential anomalies. These values are combined to produce an anomaly map, which
can localize the manufacturing defects in the depth map. The results show that
although the autoencoder is trained with a very limited number of scans, the
proposed approach can produce sufficient binary classification accuracy and
specify the location of the defects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghamisi_A/0/1/0/all/0/1&quot;&gt;Assef Ghamisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charter_T/0/1/0/all/0/1&quot;&gt;Todd Charter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1&quot;&gt;Li Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivard_M/0/1/0/all/0/1&quot;&gt;Maxime Rivard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lund_G/0/1/0/all/0/1&quot;&gt;Gil Lund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najjaran_H/0/1/0/all/0/1&quot;&gt;Homayoun Najjaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07912">
<title>Predicting mechanical properties of Carbon Nanotube (CNT) images Using Multi-Layer Synthetic Finite Element Model Simulations. (arXiv:2307.07912v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07912</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a pipeline for predicting mechanical properties of
vertically-oriented carbon nanotube (CNT) forest images using a deep learning
model for artificial intelligence (AI)-based materials discovery. Our approach
incorporates an innovative data augmentation technique that involves the use of
multi-layer synthetic (MLS) or quasi-2.5D images which are generated by
blending 2D synthetic images. The MLS images more closely resemble 3D synthetic
and real scanning electron microscopy (SEM) images of CNTs but without the
computational cost of performing expensive 3D simulations or experiments.
Mechanical properties such as stiffness and buckling load for the MLS images
are estimated using a physics-based model. The proposed deep learning
architecture, CNTNeXt, builds upon our previous CNTNet neural network, using a
ResNeXt feature representation followed by random forest regression estimator.
Our machine learning approach for predicting CNT physical properties by
utilizing a blended set of synthetic images is expected to outperform single
synthetic image-based learning when it comes to predicting mechanical
properties of real scanning electron microscopy images. This has the potential
to accelerate understanding and control of CNT forest self-assembly for diverse
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Safavigerdini_K/0/1/0/all/0/1&quot;&gt;Kaveh Safavigerdini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nouduri_K/0/1/0/all/0/1&quot;&gt;Koundinya Nouduri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Surya_R/0/1/0/all/0/1&quot;&gt;Ramakrishna Surya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reinhard_A/0/1/0/all/0/1&quot;&gt;Andrew Reinhard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quinlan_Z/0/1/0/all/0/1&quot;&gt;Zach Quinlan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bunyak_F/0/1/0/all/0/1&quot;&gt;Filiz Bunyak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maschmann_M/0/1/0/all/0/1&quot;&gt;Matthew R. Maschmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palaniappan_K/0/1/0/all/0/1&quot;&gt;Kannappan Palaniappan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07916">
<title>On the Robustness of Split Learning against Adversarial Attacks. (arXiv:2307.07916v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07916</link>
<description rdf:parseType="Literal">&lt;p&gt;Split learning enables collaborative deep learning model training while
preserving data privacy and model security by avoiding direct sharing of raw
data and model details (i.e., sever and clients only hold partial sub-networks
and exchange intermediate computations). However, existing research has mainly
focused on examining its reliability for privacy protection, with little
investigation into model security. Specifically, by exploring full models,
attackers can launch adversarial attacks, and split learning can mitigate this
severe threat by only disclosing part of models to untrusted servers.This paper
aims to evaluate the robustness of split learning against adversarial attacks,
particularly in the most challenging setting where untrusted servers only have
access to the intermediate layers of the model.Existing adversarial attacks
mostly focus on the centralized setting instead of the collaborative setting,
thus, to better evaluate the robustness of split learning, we develop a
tailored attack called SPADV, which comprises two stages: 1) shadow model
training that addresses the issue of lacking part of the model and 2) local
adversarial attack that produces adversarial examples to evaluate.The first
stage only requires a few unlabeled non-IID data, and, in the second stage,
SPADV perturbs the intermediate output of natural samples to craft the
adversarial ones. The overall cost of the proposed attack process is relatively
low, yet the empirical attack effectiveness is significantly high,
demonstrating the surprising vulnerability of split learning to adversarial
attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1&quot;&gt;Mingyuan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wenmeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jun Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07928">
<title>Reinforced Disentanglement for Face Swapping without Skip Connection. (arXiv:2307.07928v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07928</link>
<description rdf:parseType="Literal">&lt;p&gt;The SOTA face swap models still suffer the problem of either target identity
(i.e., shape) being leaked or the target non-identity attributes (i.e.,
background, hair) failing to be fully preserved in the final results. We show
that this insufficient disentanglement is caused by two flawed designs that
were commonly adopted in prior models: (1) counting on only one compressed
encoder to represent both the semantic-level non-identity facial
attributes(i.e., pose) and the pixel-level non-facial region details, which is
contradictory to satisfy at the same time; (2) highly relying on long
skip-connections between the encoder and the final generator, leaking a certain
amount of target face identity into the result. To fix them, we introduce a new
face swap framework called &apos;WSC-swap&apos; that gets rid of skip connections and
uses two target encoders to respectively capture the pixel-level non-facial
region attributes and the semantic non-identity attributes in the face region.
To further reinforce the disentanglement learning for the target encoder, we
employ both identity removal loss via adversarial training (i.e., GAN) and the
non-identity preservation loss via prior 3DMM models like [11]. Extensive
experiments on both FaceForensics++ and CelebA-HQ show that our results
significantly outperform previous works on a rich set of metrics, including one
novel metric for measuring identity consistency that was completely neglected
before.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiaohang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xingyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_P/0/1/0/all/0/1&quot;&gt;Pengfei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1&quot;&gt;Heung-Yeung Shum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07929">
<title>DocTr: Document Transformer for Structured Information Extraction in Documents. (arXiv:2307.07929v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07929</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new formulation for structured information extraction (SIE) from
visually rich documents. It aims to address the limitations of existing IOB
tagging or graph-based formulations, which are either overly reliant on the
correct ordering of input text or struggle with decoding a complex graph.
Instead, motivated by anchor-based object detectors in vision, we represent an
entity as an anchor word and a bounding box, and represent entity linking as
the association between anchor words. This is more robust to text ordering, and
maintains a compact graph for entity linking. The formulation motivates us to
introduce 1) a DOCument TRansformer (DocTr) that aims at detecting and
associating entity bounding boxes in visually rich documents, and 2) a simple
pre-training strategy that helps learn entity detection in the context of
language. Evaluations on three SIE benchmarks show the effectiveness of the
proposed formulation, and the overall approach outperforms existing solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1&quot;&gt;Haofu Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+RoyChowdhury_A/0/1/0/all/0/1&quot;&gt;Aruni RoyChowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weijian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1&quot;&gt;Ankan Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuting Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhuowen Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Satzoda_R/0/1/0/all/0/1&quot;&gt;Ravi Kumar Satzoda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1&quot;&gt;R. Manmatha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahadevan_V/0/1/0/all/0/1&quot;&gt;Vijay Mahadevan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07933">
<title>Holistic Prototype Attention Network for Few-Shot VOS. (arXiv:2307.07933v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07933</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot video object segmentation (FSVOS) aims to segment dynamic objects of
unseen classes by resorting to a small set of support images that contain
pixel-level object annotations. Existing methods have demonstrated that the
domain agent-based attention mechanism is effective in FSVOS by learning the
correlation between support images and query frames. However, the agent frame
contains redundant pixel information and background noise, resulting in
inferior segmentation performance. Moreover, existing methods tend to ignore
inter-frame correlations in query videos. To alleviate the above dilemma, we
propose a holistic prototype attention network (HPAN) for advancing FSVOS.
Specifically, HPAN introduces a prototype graph attention module (PGAM) and a
bidirectional prototype attention module (BPAM), transferring informative
knowledge from seen to unseen classes. PGAM generates local prototypes from all
foreground features and then utilizes their internal correlations to enhance
the representation of the holistic prototypes. BPAM exploits the holistic
information from support images and video frames by fusing co-attention and
self-attention to achieve support-query semantic consistency and inner-frame
temporal consistency. Extensive experiments on YouTube-FSVOS have been provided
to demonstrate the effectiveness and superiority of our proposed HPAN method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yin Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xiruo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yazhou Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1&quot;&gt;Guo-Sen Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Heng-Tao Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07934">
<title>Contrastive Multi-Task Dense Prediction. (arXiv:2307.07934v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07934</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper targets the problem of multi-task dense prediction which aims to
achieve simultaneous learning and inference on a bunch of multiple dense
prediction tasks in a single framework. A core objective in design is how to
effectively model cross-task interactions to achieve a comprehensive
improvement on different tasks based on their inherent complementarity and
consistency. Existing works typically design extra expensive distillation
modules to perform explicit interaction computations among different
task-specific features in both training and inference, bringing difficulty in
adaptation for different task sets, and reducing efficiency due to clearly
increased size of multi-task models. In contrast, we introduce feature-wise
contrastive consistency into modeling the cross-task interactions for
multi-task dense prediction. We propose a novel multi-task contrastive
regularization method based on the consistency to effectively boost the
representation learning of the different sub-tasks, which can also be easily
generalized to different multi-task dense prediction frameworks, and costs no
additional computation in the inference. Extensive experiments on two
challenging datasets (i.e. NYUD-v2 and Pascal-Context) clearly demonstrate the
superiority of the proposed multi-task contrastive learning approach for dense
predictions, establishing new state-of-the-art performances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Siwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1&quot;&gt;Hanrong Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07935">
<title>S2R-ViT for Multi-Agent Cooperative Perception: Bridging the Gap from Simulation to Reality. (arXiv:2307.07935v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07935</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the lack of real multi-agent data and time-consuming of labeling,
existing multi-agent cooperative perception algorithms usually select the
simulated sensor data for training and validating. However, the perception
performance is degraded when these simulation-trained models are deployed to
the real world, due to the significant domain gap between the simulated and
real data. In this paper, we propose the first Simulation-to-Reality transfer
learning framework for multi-agent cooperative perception using a novel Vision
Transformer, named as S2R-ViT, which considers both the Implementation Gap and
Feature Gap between simulated and real data. We investigate the effects of
these two types of domain gaps and propose a novel uncertainty-aware vision
transformer to effectively relief the Implementation Gap and an agent-based
feature adaptation module with inter-agent and ego-agent discriminators to
reduce the Feature Gap. Our intensive experiments on the public multi-agent
cooperative perception datasets OPV2V and V2V4Real demonstrate that the
proposed S2R-ViT can effectively bridge the gap from simulation to reality and
outperform other methods significantly for point cloud-based 3D object
detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinlong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Runsheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Baolu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1&quot;&gt;Qin Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jiaqi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hongkai Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07938">
<title>CVSformer: Cross-View Synthesis Transformer for Semantic Scene Completion. (arXiv:2307.07938v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07938</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic scene completion (SSC) requires an accurate understanding of the
geometric and semantic relationships between the objects in the 3D scene for
reasoning the occluded objects. The popular SSC methods voxelize the 3D
objects, allowing the deep 3D convolutional network (3D CNN) to learn the
object relationships from the complex scenes. However, the current networks
lack the controllable kernels to model the object relationship across multiple
views, where appropriate views provide the relevant information for suggesting
the existence of the occluded objects. In this paper, we propose Cross-View
Synthesis Transformer (CVSformer), which consists of Multi-View Feature
Synthesis and Cross-View Transformer for learning cross-view object
relationships. In the multi-view feature synthesis, we use a set of 3D
convolutional kernels rotated differently to compute the multi-view features
for each voxel. In the cross-view transformer, we employ the cross-view fusion
to comprehensively learn the cross-view relationships, which form useful
information for enhancing the features of individual views. We use the enhanced
features to predict the geometric occupancies and semantic labels of all
voxels. We evaluate CVSformer on public datasets, where CVSformer yields
state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Haotian Dong&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_E/0/1/0/all/0/1&quot;&gt;Enhui Ma&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lubo Wang&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Miaohui Wang&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Wuyuan Xie&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qing Guo&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Ping Li&lt;/a&gt; (4), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Lingyu Liang&lt;/a&gt; (5), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kairui Yang&lt;/a&gt; (6), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Di Lin&lt;/a&gt; (1) ((1) Tianjin University, (2) Shenzhen University, (3) A*STAR, (4) The Hong Kong Polytechnic University, (5) South China University of Technology, (6) Alibaba Damo Academy)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07942">
<title>KECOR: Kernel Coding Rate Maximization for Active 3D Object Detection. (arXiv:2307.07942v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07942</link>
<description rdf:parseType="Literal">&lt;p&gt;Achieving a reliable LiDAR-based object detector in autonomous driving is
paramount, but its success hinges on obtaining large amounts of precise 3D
annotations. Active learning (AL) seeks to mitigate the annotation burden
through algorithms that use fewer labels and can attain performance comparable
to fully supervised learning. Although AL has shown promise, current approaches
prioritize the selection of unlabeled point clouds with high uncertainty and/or
diversity, leading to the selection of more instances for labeling and reduced
computational efficiency. In this paper, we resort to a novel kernel coding
rate maximization (KECOR) strategy which aims to identify the most informative
point clouds to acquire labels through the lens of information theory. Greedy
search is applied to seek desired point clouds that can maximize the minimal
number of bits required to encode the latent features. To determine the
uniqueness and informativeness of the selected samples from the model
perspective, we construct a proxy network of the 3D detector head and compute
the outer product of Jacobians from all proxy layers to form the empirical
neural tangent kernel (NTK) matrix. To accommodate both one-stage (i.e.,
SECOND) and two-stage detectors (i.e., PVRCNN), we further incorporate the
classification entropy maximization and well trade-off between detection
performance and the total number of bounding boxes selected for annotation.
Extensive experiments conducted on two 3D benchmarks and a 2D detection dataset
evidence the superiority and versatility of the proposed approach. Our results
show that approximately 44% box-level annotation costs and 26% computational
time are reduced compared to the state-of-the-art AL method, without
compromising detection performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yadan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuoxiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zhen Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1&quot;&gt;Mahsa Baktashmotlagh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07944">
<title>Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling. (arXiv:2307.07944v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07944</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain adaptation (DA) with the aid of pseudo labeling
techniques has emerged as a crucial approach for domain-adaptive 3D object
detection. While effective, existing DA methods suffer from a substantial drop
in performance when applied to a multi-class training setting, due to the
co-existence of low-quality pseudo labels and class imbalance issues. In this
paper, we address this challenge by proposing a novel ReDB framework tailored
for learning to detect all classes at once. Our approach produces Reliable,
Diverse, and class-Balanced pseudo 3D boxes to iteratively guide the
self-training on a distributionally different target domain. To alleviate
disruptions caused by the environmental discrepancy (e.g., beam numbers), the
proposed cross-domain examination (CDE) assesses the correctness of pseudo
labels by copy-pasting target instances into a source environment and measuring
the prediction consistency. To reduce computational overhead and mitigate the
object shift (e.g., scales and point densities), we design an overlapped boxes
counting (OBC) metric that allows to uniformly downsample pseudo-labeled
objects across different geometric characteristics. To confront the issue of
inter-class imbalance, we progressively augment the target point clouds with a
class-balanced set of pseudo-labeled target instances and source objects, which
boosts recognition accuracies on both frequently appearing and rare classes.
Experimental results on three benchmark datasets using both voxel-based (i.e.,
SECOND) and point-based 3D detectors (i.e., PointRCNN) demonstrate that our
proposed ReDB approach outperforms existing 3D domain adaptation methods by a
large margin, improving 23.15% mAP on the nuScenes $\rightarrow$ KITTI task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuoxiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yadan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1&quot;&gt;Mahsa Baktashmotlagh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07945">
<title>Surface Geometry Processing: An Efficient Normal-based Detail Representation. (arXiv:2307.07945v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07945</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of high-resolution 3D vision applications, the
traditional way of manipulating surface detail requires considerable memory and
computing time. To address these problems, we introduce an efficient surface
detail processing framework in 2D normal domain, which extracts new normal
feature representations as the carrier of micro geometry structures that are
illustrated both theoretically and empirically in this article. Compared with
the existing state of the arts, we verify and demonstrate that the proposed
normal-based representation has three important properties, including detail
separability, detail transferability and detail idempotence. Finally, three new
schemes are further designed for geometric surface detail processing
applications, including geometric texture synthesis, geometry detail transfer,
and 3D surface super-resolution. Theoretical analysis and experimental results
on the latest benchmark dataset verify the effectiveness and versatility of our
normal-based representation, which accepts 30 times of the input surface
vertices but at the same time only takes 6.5% memory cost and 14.0% running
time in comparison with existing competing algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Wuyuan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Miaohui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Di Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Boxin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jianmin Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07947">
<title>Language Conditioned Traffic Generation. (arXiv:2307.07947v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07947</link>
<description rdf:parseType="Literal">&lt;p&gt;Simulation forms the backbone of modern self-driving development. Simulators
help develop, test, and improve driving systems without putting humans,
vehicles, or their environment at risk. However, simulators face a major
challenge: They rely on realistic, scalable, yet interesting content. While
recent advances in rendering and scene reconstruction make great strides in
creating static scene assets, modeling their layout, dynamics, and behaviors
remains challenging. In this work, we turn to language as a source of
supervision for dynamic traffic scene generation. Our model, LCTGen, combines a
large language model with a transformer-based decoder architecture that selects
likely map locations from a dataset of maps, and produces an initial traffic
distribution, as well as the dynamics of each vehicle. LCTGen outperforms prior
work in both unconditional and conditional traffic scene generation in terms of
realism and fidelity. Code and video will be available at
https://ariostgx.github.io/lctgen.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1&quot;&gt;Shuhan Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ivanovic_B/0/1/0/all/0/1&quot;&gt;Boris Ivanovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1&quot;&gt;Xinshuo Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1&quot;&gt;Marco Pavone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kraehenbuehl_P/0/1/0/all/0/1&quot;&gt;Philipp Kraehenbuehl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07950">
<title>Accelerating Distributed ML Training via Selective Synchronization. (arXiv:2307.07950v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2307.07950</link>
<description rdf:parseType="Literal">&lt;p&gt;In distributed training, deep neural networks (DNNs) are launched over
multiple workers concurrently and aggregate their local updates on each step in
bulk-synchronous parallel (BSP) training. However, BSP does not linearly
scale-out due to high communication cost of aggregation. To mitigate this
overhead, alternatives like Federated Averaging (FedAvg) and Stale-Synchronous
Parallel (SSP) either reduce synchronization frequency or eliminate it
altogether, usually at the cost of lower final accuracy. In this paper, we
present \texttt{SelSync}, a practical, low-overhead method for DNN training
that dynamically chooses to incur or avoid communication at each step either by
calling the aggregation op or applying local updates based on their
significance. We propose various optimizations as part of \texttt{SelSync} to
improve convergence in the context of \textit{semi-synchronous} training. Our
system converges to the same or better accuracy than BSP while reducing
training time by up to 14$\times$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tyagi_S/0/1/0/all/0/1&quot;&gt;Sahil Tyagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swany_M/0/1/0/all/0/1&quot;&gt;Martin Swany&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07953">
<title>Accurate 3D Prediction of Missing Teeth in Diverse Patterns for Precise Dental Implant Planning. (arXiv:2307.07953v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07953</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the demand for dental implants has surged, driven by their
high success rates and esthetic advantages. However, accurate prediction of
missing teeth for precise digital implant planning remains a challenge due to
the intricate nature of dental structures and the variability in tooth loss
patterns. This study presents a novel framework for accurate prediction of
missing teeth in different patterns, facilitating digital implant planning. The
proposed framework begins by estimating point-to-point correspondence among a
dataset of dental mesh models reconstructed from CBCT images of healthy
subjects. Subsequently, tooth dictionaries are constructed for each tooth type,
encoding their position and shape information based on the established
point-to-point correspondence. To predict missing teeth in a given dental mesh
model, sparse coefficients are learned by sparsely representing adjacent teeth
of the missing teeth using the corresponding tooth dictionaries. These
coefficients are then applied to the dictionaries of the missing teeth to
generate accurate predictions of their positions and shapes. The evaluation
results on real subjects shows that our proposed framework achieves an average
prediction error of 1.04mm for predictions of single missing tooth and an
average prediction error of 1.33mm for the prediction of 14 missing teeth,
which demonstrates its capability of accurately predicting missing teeth in
various patterns. By accurately predicting missing teeth, dental professionals
can improve the planning and placement of dental implants, leading to better
esthetic and functional outcomes for patients undergoing dental implant
procedures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_P/0/1/0/all/0/1&quot;&gt;Peng Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yuning Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yue Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Min Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Zhongxiang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dinggang Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07961">
<title>EmoSet: A Large-scale Visual Emotion Dataset with Rich Attributes. (arXiv:2307.07961v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07961</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual Emotion Analysis (VEA) aims at predicting people&apos;s emotional responses
to visual stimuli. This is a promising, yet challenging, task in affective
computing, which has drawn increasing attention in recent years. Most of the
existing work in this area focuses on feature design, while little attention
has been paid to dataset construction. In this work, we introduce EmoSet, the
first large-scale visual emotion dataset annotated with rich attributes, which
is superior to existing datasets in four aspects: scale, annotation richness,
diversity, and data balance. EmoSet comprises 3.3 million images in total, with
118,102 of these images carefully labeled by human annotators, making it five
times larger than the largest existing dataset. EmoSet includes images from
social networks, as well as artistic images, and it is well balanced between
different emotion categories. Motivated by psychological studies, in addition
to emotion category, each image is also annotated with a set of describable
emotion attributes: brightness, colorfulness, scene type, object class, facial
expression, and human action, which can help understand visual emotions in a
precise and interpretable way. The relevance of these emotion attributes is
validated by analyzing the correlations between them and visual emotion, as
well as by designing an attribute module to help visual emotion recognition. We
believe EmoSet will bring some key insights and encourage further research in
visual emotion analysis and understanding. The data and code will be released
after the publication of this work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jingyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qirui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1&quot;&gt;Tingting Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lischinski_D/0/1/0/all/0/1&quot;&gt;Dani Lischinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1&quot;&gt;Daniel Cohen-Or&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hui Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07972">
<title>Dual-level Interaction for Domain Adaptive Semantic Segmentation. (arXiv:2307.07972v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07972</link>
<description rdf:parseType="Literal">&lt;p&gt;To circumvent the costly pixel-wise annotations of real-world images in the
semantic segmentation task, the Unsupervised Domain Adaptation (UDA) is
explored to firstly train a model with the labeled source data (synthetic
images) and then adapt it to the unlabeled target data (real images). Among all
the techniques being studied, the self-training approach recently secures its
position in domain adaptive semantic segmentation, where a model is trained
with target domain pseudo-labels. Current advances have mitigated noisy
pseudo-labels resulting from the domain gap. However, they still struggle with
erroneous pseudo-labels near the decision boundaries of the semantic
classifier. In this paper, we tackle this issue by proposing a dual-level
interaction for domain adaptation (DIDA) in semantic segmentation. Explicitly,
we encourage the different augmented views of the same pixel to have not only
similar class prediction (semantic-level) but also akin similarity relationship
respected to other pixels (instance-level). As it is impossible to keep
features of all pixel instances for a dataset, we novelly design and maintain a
labeled instance bank with dynamic updating strategies to selectively store the
informative features of instances. Further, DIDA performs cross-level
interaction with scattering and gathering techniques to regenerate more
reliable pseudolabels. Our method outperforms the state-of-the-art by a notable
margin, especially on confusing and long-tailed classes. Code is available at
https://github.com/RainJamesY/DIDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_D/0/1/0/all/0/1&quot;&gt;Dongyu Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Boheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Run Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lina Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07976">
<title>HRHD-HK: A benchmark dataset of high-rise and high-density urban scenes for 3D semantic segmentation of photogrammetric point clouds. (arXiv:2307.07976v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07976</link>
<description rdf:parseType="Literal">&lt;p&gt;Many existing 3D semantic segmentation methods, deep learning in computer
vision notably, claimed to achieve desired results on urban point clouds, in
which the city objects are too many and diverse for people to judge
qualitatively. Thus, it is significant to assess these methods quantitatively
in diversified real-world urban scenes, encompassing high-rise, low-rise,
high-density, and low-density urban areas. However, existing public benchmark
datasets primarily represent low-rise scenes from European cities and cannot
assess the methods comprehensively. This paper presents a benchmark dataset of
high-rise urban point clouds, namely High-Rise, High-Density urban scenes of
Hong Kong (HRHD-HK), which has been vacant for a long time. HRHD-HK arranged in
150 tiles contains 273 million colorful photogrammetric 3D points from diverse
urban settings. The semantic labels of HRHD-HK include building, vegetation,
road, waterbody, facility, terrain, and vehicle. To the best of our knowledge,
HRHD-HK is the first photogrammetric dataset that focuses on HRHD urban areas.
This paper also comprehensively evaluates eight popular semantic segmentation
methods on the HRHD-HK dataset. Experimental results confirmed plenty of room
for enhancing the current 3D semantic segmentation of point clouds, especially
for city objects with small volumes. Our dataset is publicly available at:
https://github.com/LuZaiJiaoXiaL/HRHD-HK.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Maosu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yijie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_A/0/1/0/all/0/1&quot;&gt;Anthony G.O. Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1&quot;&gt;Fan Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07977">
<title>Integrating Human Parsing and Pose Network for Human Action Recognition. (arXiv:2307.07977v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07977</link>
<description rdf:parseType="Literal">&lt;p&gt;Human skeletons and RGB sequences are both widely-adopted input modalities
for human action recognition. However, skeletons lack appearance features and
color data suffer large amount of irrelevant depiction. To address this, we
introduce human parsing feature map as a novel modality, since it can
selectively retain spatiotemporal features of the body parts, while filtering
out noises regarding outfits, backgrounds, etc. We propose an Integrating Human
Parsing and Pose Network (IPP-Net) for action recognition, which is the first
to leverage both skeletons and human parsing feature maps in dual-branch
approach. The human pose branch feeds compact skeletal representations of
different modalities in graph convolutional network to model pose features. In
human parsing branch, multi-frame body-part parsing features are extracted with
human detector and parser, which is later learnt using a convolutional
backbone. A late ensemble of two branches is adopted to get final predictions,
considering both robust keypoints and rich semantic body-part features.
Extensive experiments on NTU RGB+D and NTU RGB+D 120 benchmarks consistently
verify the effectiveness of the proposed IPP-Net, which outperforms the
existing action recognition methods. Our code is publicly available at
https://github.com/liujf69/IPP-Net-Parsing .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1&quot;&gt;Runwei Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yuhang Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinfu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_N/0/1/0/all/0/1&quot;&gt;Nan Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1&quot;&gt;Fanyang Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mengyuan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07982">
<title>A Survey of Techniques for Optimizing Transformer Inference. (arXiv:2307.07982v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07982</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have seen a phenomenal rise in performance and applications of
transformer neural networks. The family of transformer networks, including
Bidirectional Encoder Representations from Transformer (BERT), Generative
Pretrained Transformer (GPT) and Vision Transformer (ViT), have shown their
effectiveness across Natural Language Processing (NLP) and Computer Vision (CV)
domains. Transformer-based networks such as ChatGPT have impacted the lives of
common men. However, the quest for high predictive performance has led to an
exponential increase in transformers&apos; memory and compute footprint. Researchers
have proposed techniques to optimize transformer inference at all levels of
abstraction. This paper presents a comprehensive survey of techniques for
optimizing the inference phase of transformer networks. We survey techniques
such as knowledge distillation, pruning, quantization, neural architecture
search and lightweight network design at the algorithmic level. We further
review hardware-level optimization techniques and the design of novel hardware
accelerators for transformers. We summarize the quantitative results on the
number of parameters/FLOPs and accuracy of several models/techniques to
showcase the tradeoff exercised by them. We also outline future directions in
this rapidly evolving field of research. We believe that this survey will
educate both novice and seasoned researchers and also spark a plethora of
research efforts in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chitty_Venkata_K/0/1/0/all/0/1&quot;&gt;Krishna Teja Chitty-Venkata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1&quot;&gt;Sparsh Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emani_M/0/1/0/all/0/1&quot;&gt;Murali Emani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishwanath_V/0/1/0/all/0/1&quot;&gt;Venkatram Vishwanath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Somani_A/0/1/0/all/0/1&quot;&gt;Arun K. Somani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07998">
<title>LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network. (arXiv:2307.07998v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07998</link>
<description rdf:parseType="Literal">&lt;p&gt;The process of acquiring microscopic images in life sciences often results in
image degradation and corruption, characterised by the presence of noise and
blur, which poses significant challenges in accurately analysing and
interpreting the obtained data. This paper proposes LUCYD, a novel method for
the restoration of volumetric microscopy images that combines the
Richardson-Lucy deconvolution formula and the fusion of deep features obtained
by a fully convolutional network. By integrating the image formation process
into a feature-driven restoration model, the proposed approach aims to enhance
the quality of the restored images whilst reducing computational costs and
maintaining a high degree of interpretability. Our results demonstrate that
LUCYD outperforms the state-of-the-art methods in both synthetic and real
microscopy images, achieving superior performance in terms of image quality and
generalisability. We show that the model can handle various microscopy
modalities and different imaging conditions by evaluating it on two different
microscopy datasets, including volumetric widefield and light-sheet microscopy.
Our experiments indicate that LUCYD can significantly improve resolution,
contrast, and overall quality of microscopy images. Therefore, it can be a
valuable tool for microscopy image restoration and can facilitate further
research in various microscopy applications. We made the source code for the
model accessible under https://github.com/ctom2/lucyd-deconvolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chobola_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Chobola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_G/0/1/0/all/0/1&quot;&gt;Gesine M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dausmann_V/0/1/0/all/0/1&quot;&gt;Veit Dausmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theileis_A/0/1/0/all/0/1&quot;&gt;Anton Theileis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taucher_J/0/1/0/all/0/1&quot;&gt;Jan Taucher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huisken_J/0/1/0/all/0/1&quot;&gt;Jan Huisken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1&quot;&gt;Tingying Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08003">
<title>SHAMSUL: Simultaneous Heatmap-Analysis to investigate Medical Significance Utilizing Local interpretability methods. (arXiv:2307.08003v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.08003</link>
<description rdf:parseType="Literal">&lt;p&gt;The interpretability of deep neural networks has become a subject of great
interest within the medical and healthcare domain. This attention stems from
concerns regarding transparency, legal and ethical considerations, and the
medical significance of predictions generated by these deep neural networks in
clinical decision support systems. To address this matter, our study delves
into the application of four well-established interpretability methods: Local
Interpretable Model-agnostic Explanations (LIME), Shapley Additive exPlanations
(SHAP), Gradient-weighted Class Activation Mapping (Grad-CAM), and Layer-wise
Relevance Propagation (LRP). Leveraging the approach of transfer learning with
a multi-label-multi-class chest radiography dataset, we aim to interpret
predictions pertaining to specific pathology classes. Our analysis encompasses
both single-label and multi-label predictions, providing a comprehensive and
unbiased assessment through quantitative and qualitative investigations, which
are compared against human expert annotation. Notably, Grad-CAM demonstrates
the most favorable performance in quantitative evaluation, while the LIME
heatmap segmentation visualization exhibits the highest level of medical
significance. Our research highlights the strengths and limitations of these
interpretability methods and suggests that a multimodal-based approach,
incorporating diverse sources of information beyond chest radiography images,
could offer additional insights for enhancing interpretability in the medical
domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Mahbub Ul Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hollmen_J/0/1/0/all/0/1&quot;&gt;Jaakko Hollm&amp;#xe9;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baldvinsson_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf3;n R&amp;#xfa;nar Baldvinsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rahmani_R/0/1/0/all/0/1&quot;&gt;Rahim Rahmani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08012">
<title>Householder Projector for Unsupervised Latent Semantics Discovery. (arXiv:2307.08012v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08012</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs), especially the recent style-based
generators (StyleGANs), have versatile semantics in the structured latent
space. Latent semantics discovery methods emerge to move around the latent code
such that only one factor varies during the traversal. Recently, an
unsupervised method proposed a promising direction to directly use the
eigenvectors of the projection matrix that maps latent codes to features as the
interpretable directions. However, one overlooked fact is that the projection
matrix is non-orthogonal and the number of eigenvectors is too large. The
non-orthogonality would entangle semantic attributes in the top few
eigenvectors, and the large dimensionality might result in meaningless
variations among the directions even if the matrix is orthogonal. To avoid
these issues, we propose Householder Projector, a flexible and general low-rank
orthogonal matrix representation based on Householder transformations, to
parameterize the projection matrix. The orthogonality guarantees that the
eigenvectors correspond to disentangled interpretable semantics, while the
low-rank property encourages that each identified direction has meaningful
variations. We integrate our projector into pre-trained StyleGAN2/StyleGAN3 and
evaluate the models on several benchmarks. Within only $1\%$ of the original
training steps for fine-tuning, our projector helps StyleGANs to discover more
disentangled and precise semantic attributes without sacrificing image
fidelity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yue Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jichao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1&quot;&gt;Nicu Sebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08013">
<title>Revisiting Implicit Models: Sparsity Trade-offs Capability in Weight-tied Model for Vision Tasks. (arXiv:2307.08013v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08013</link>
<description rdf:parseType="Literal">&lt;p&gt;Implicit models such as Deep Equilibrium Models (DEQs) have garnered
significant attention in the community for their ability to train infinite
layer models with elegant solution-finding procedures and constant memory
footprint. However, despite several attempts, these methods are heavily
constrained by model inefficiency and optimization instability. Furthermore,
fair benchmarking across relevant methods for vision tasks is missing. In this
work, we revisit the line of implicit models and trace them back to the
original weight-tied models. Surprisingly, we observe that weight-tied models
are more effective, stable, as well as efficient on vision tasks, compared to
the DEQ variants. Through the lens of these simple-yet-clean weight-tied
models, we further study the fundamental limits in the model capacity of such
models and propose the use of distinct sparse masks to improve the model
capacity. Finally, for practitioners, we offer design guidelines regarding the
depth, width, and sparsity selection for weight-tied models, and demonstrate
the generalizability of our insights to other learning paradigms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Haobo Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumder_S/0/1/0/all/0/1&quot;&gt;Soumajit Majumder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tao Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08015">
<title>Boosting 3-DoF Ground-to-Satellite Camera Localization Accuracy via Geometry-Guided Cross-View Transformer. (arXiv:2307.08015v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08015</link>
<description rdf:parseType="Literal">&lt;p&gt;Image retrieval-based cross-view localization methods often lead to very
coarse camera pose estimation, due to the limited sampling density of the
database satellite images. In this paper, we propose a method to increase the
accuracy of a ground camera&apos;s location and orientation by estimating the
relative rotation and translation between the ground-level image and its
matched/retrieved satellite image. Our approach designs a geometry-guided
cross-view transformer that combines the benefits of conventional geometry and
learnable cross-view transformers to map the ground-view observations to an
overhead view. Given the synthesized overhead view and observed satellite
feature maps, we construct a neural pose optimizer with strong global
information embedding ability to estimate the relative rotation between them.
After aligning their rotations, we develop an uncertainty-guided spatial
correlation to generate a probability map of the vehicle locations, from which
the relative translation can be determined. Experimental results demonstrate
that our method significantly outperforms the state-of-the-art. Notably, the
likelihood of restricting the vehicle lateral pose to be within 1m of its
Ground Truth (GT) value on the cross-view KITTI dataset has been improved from
$35.54\%$ to $76.44\%$, and the likelihood of restricting the vehicle
orientation to be within $1^{\circ}$ of its GT value has been improved from
$19.64\%$ to $99.10\%$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yujiao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vora_A/0/1/0/all/0/1&quot;&gt;Ankit Vora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perincherry_A/0/1/0/all/0/1&quot;&gt;Akhil Perincherry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongdong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08016">
<title>Breaking Down the Task: A Unit-Grained Hybrid Training Framework for Vision and Language Decision Making. (arXiv:2307.08016v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08016</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision language decision making (VLDM) is a challenging multimodal task. The
agent have to understand complex human instructions and complete compositional
tasks involving environment navigation and object manipulation. However, the
long action sequences involved in VLDM make the task difficult to learn. From
an environment perspective, we find that task episodes can be divided into
fine-grained \textit{units}, each containing a navigation phase and an
interaction phase. Since the environment within a unit stays unchanged, we
propose a novel hybrid-training framework that enables active exploration in
the environment and reduces the exposure bias. Such framework leverages the
unit-grained configurations and is model-agnostic. Specifically, we design a
Unit-Transformer (UT) with an intrinsic recurrent state that maintains a
unit-scale cross-modal memory. Through extensive experiments on the TEACH
benchmark, we demonstrate that our proposed framework outperforms existing
state-of-the-art methods in terms of all evaluation metrics. Overall, our work
introduces a novel approach to tackling the VLDM task by breaking it down into
smaller, manageable units and utilizing a hybrid-training framework. By doing
so, we provide a more flexible and effective solution for multimodal decision
making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1&quot;&gt;Ruipu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiwen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhongyu Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08025">
<title>Analysing Gender Bias in Text-to-Image Models using Object Detection. (arXiv:2307.08025v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08025</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents a novel strategy to measure bias in text-to-image models.
Using paired prompts that specify gender and vaguely reference an object (e.g.
&quot;a man/woman holding an item&quot;) we can examine whether certain objects are
associated with a certain gender. In analysing results from Stable Diffusion,
we observed that male prompts generated objects such as ties, knives, trucks,
baseball bats, and bicycles more frequently. On the other hand, female prompts
were more likely to generate objects such as handbags, umbrellas, bowls,
bottles, and cups. We hope that the method outlined here will be a useful tool
for examining bias in text-to-image models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mannering_H/0/1/0/all/0/1&quot;&gt;Harvey Mannering&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08027">
<title>Multi-Object Discovery by Low-Dimensional Object Motion. (arXiv:2307.08027v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08027</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work in unsupervised multi-object segmentation shows impressive
results by predicting motion from a single image despite the inherent ambiguity
in predicting motion without the next image. On the other hand, the set of
possible motions for an image can be constrained to a low-dimensional space by
considering the scene structure and moving objects in it. We propose to model
pixel-wise geometry and object motion to remove ambiguity in reconstructing
flow from a single image. Specifically, we divide the image into coherently
moving regions and use depth to construct flow bases that best explain the
observed flow in each region. We achieve state-of-the-art results in
unsupervised multi-object segmentation on synthetic and real-world datasets by
modeling the scene structure and object motion. Our evaluation of the predicted
depth maps shows reliable performance in monocular depth estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Safadoust_S/0/1/0/all/0/1&quot;&gt;Sadra Safadoust&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guney_F/0/1/0/all/0/1&quot;&gt;Fatma G&amp;#xfc;ney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08041">
<title>Planting a SEED of Vision in Large Language Model. (arXiv:2307.08041v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08041</link>
<description rdf:parseType="Literal">&lt;p&gt;We present SEED, an elaborate image tokenizer that empowers Large Language
Models (LLMs) with the emergent ability to SEE and Draw at the same time.
Research on image tokenizers has previously reached an impasse, as frameworks
employing quantized visual tokens have lost prominence due to subpar
performance and convergence in multimodal comprehension (compared to BLIP-2,
etc.) or generation (compared to Stable Diffusion, etc.). Despite the
limitations, we remain confident in its natural capacity to unify visual and
textual representations, facilitating scalable multimodal training with LLM&apos;s
original recipe. In this study, we identify two crucial principles for the
architecture and training of SEED that effectively ease subsequent alignment
with LLMs. (1) Image tokens should be independent of 2D physical patch
positions and instead be produced with a 1D causal dependency, exhibiting
intrinsic interdependence that aligns with the left-to-right autoregressive
prediction mechanism in LLMs. (2) Image tokens should capture high-level
semantics consistent with the degree of semantic abstraction in words, and be
optimized for both discriminativeness and reconstruction during the tokenizer
training phase. As a result, the off-the-shelf LLM is able to perform both
image-to-text and text-to-image generation by incorporating our SEED through
efficient LoRA tuning. Comprehensive multimodal pretraining and instruction
tuning, which may yield improved results, are reserved for future
investigation. This version of SEED was trained in 5.7 days using only 64 V100
GPUs and 5M publicly available image-text pairs. Our preliminary study
emphasizes the great potential of discrete visual tokens in versatile
multimodal LLMs and the importance of proper image tokenizers in broader
research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yuying Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yixiao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Ziyun Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08048">
<title>A Novel SLCA-UNet Architecture for Automatic MRI Brain Tumor Segmentation. (arXiv:2307.08048v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.08048</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain tumor is deliberated as one of the severe health complications which
lead to decrease in life expectancy of the individuals and is also considered
as a prominent cause of mortality worldwide. Therefore, timely detection and
prediction of brain tumors can be helpful to prevent death rates due to brain
tumors. Biomedical image analysis is a widely known solution to diagnose brain
tumor. Although MRI is the current standard method for imaging tumors, its
clinical usefulness is constrained by the requirement of manual segmentation
which is time-consuming. Deep learning-based approaches have emerged as a
promising solution to develop automated biomedical image exploration tools and
the UNet architecture is commonly used for segmentation. However, the
traditional UNet has limitations in terms of complexity, training, accuracy,
and contextual information processing. As a result, the modified UNet
architecture, which incorporates residual dense blocks, layered attention, and
channel attention modules, in addition to stacked convolution, can effectively
capture both coarse and fine feature information. The proposed SLCA UNet
approach achieves good performance on the freely accessible Brain Tumor
Segmentation (BraTS) dataset, with an average performance of 0.845, 0.845,
0.999, and 8.1 in terms of Dice, Sensitivity, Specificity, and Hausdorff95 for
BraTS 2020 dataset, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+S_T/0/1/0/all/0/1&quot;&gt;Tejashwini P S&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+J_T/0/1/0/all/0/1&quot;&gt;Thriveni J&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+R_V/0/1/0/all/0/1&quot;&gt;Venugopal K R&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08051">
<title>TransNuSeg: A Lightweight Multi-Task Transformer for Nuclei Segmentation. (arXiv:2307.08051v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.08051</link>
<description rdf:parseType="Literal">&lt;p&gt;Nuclei appear small in size, yet, in real clinical practice, the global
spatial information and correlation of the color or brightness contrast between
nuclei and background, have been considered a crucial component for accurate
nuclei segmentation. However, the field of automatic nuclei segmentation is
dominated by Convolutional Neural Networks (CNNs), meanwhile, the potential of
the recently prevalent Transformers has not been fully explored, which is
powerful in capturing local-global correlations. To this end, we make the first
attempt at a pure Transformer framework for nuclei segmentation, called
TransNuSeg. Different from prior work, we decouple the challenging nuclei
segmentation task into an intrinsic multi-task learning task, where a
tri-decoder structure is employed for nuclei instance, nuclei edge, and
clustered edge segmentation respectively. To eliminate the divergent
predictions from different branches in previous work, a novel self distillation
loss is introduced to explicitly impose consistency regulation between
branches. Moreover, to formulate the high correlation between branches and also
reduce the number of parameters, an efficient attention sharing scheme is
proposed by partially sharing the self-attention heads amongst the
tri-decoders. Finally, a token MLP bottleneck replaces the over-parameterized
Transformer bottleneck for a further reduction in model complexity. Experiments
on two datasets of different modalities, including MoNuSeg have shown that our
methods can outperform state-of-the-art counterparts such as CA2.5-Net by 2-3%
Dice with 30% fewer parameters. In conclusion, TransNuSeg confirms the strength
of Transformer in the context of nuclei segmentation, which thus can serve as
an efficient solution for real clinical practice. Code is available at
https://github.com/zhenqi-he/transnuseg.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhenqi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Unberath_M/0/1/0/all/0/1&quot;&gt;Mathias Unberath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ke_J/0/1/0/all/0/1&quot;&gt;Jing Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yiqing Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08059">
<title>LafitE: Latent Diffusion Model with Feature Editing for Unsupervised Multi-class Anomaly Detection. (arXiv:2307.08059v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08059</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of flexible manufacturing systems that are required to produce
different types and quantities of products with minimal reconfiguration, this
paper addresses the problem of unsupervised multi-class anomaly detection:
develop a unified model to detect anomalies from objects belonging to multiple
classes when only normal data is accessible. We first explore the
generative-based approach and investigate latent diffusion models for
reconstruction to mitigate the notorious ``identity shortcut&apos;&apos; issue in
auto-encoder based methods. We then introduce a feature editing strategy that
modifies the input feature space of the diffusion model to further alleviate
``identity shortcuts&apos;&apos; and meanwhile improve the reconstruction quality of
normal regions, leading to fewer false positive predictions. Moreover, we are
the first who pose the problem of hyperparameter selection in unsupervised
anomaly detection, and propose a solution of synthesizing anomaly data for a
pseudo validation set to address this problem. Extensive experiments on
benchmark datasets MVTec-AD and MPDD show that the proposed LafitE, \ie, Latent
Diffusion Model with Feature Editing, outperforms state-of-art methods by a
significant margin in terms of average AUROC. The hyperparamters selected via
our pseudo validation set are well-matched to the real test set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Haonan Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_G/0/1/0/all/0/1&quot;&gt;Guanlong Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qianhui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karlsson_B/0/1/0/all/0/1&quot;&gt;Borje F. Karlsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Biqing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chin Yew Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08065">
<title>MaGNAS: A Mapping-Aware Graph Neural Architecture Search Framework for Heterogeneous MPSoC Deployment. (arXiv:2307.08065v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2307.08065</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) are becoming increasingly popular for
vision-based applications due to their intrinsic capacity in modeling
structural and contextual relations between various parts of an image frame. On
another front, the rising popularity of deep vision-based applications at the
edge has been facilitated by the recent advancements in heterogeneous
multi-processor Systems on Chips (MPSoCs) that enable inference under
real-time, stringent execution requirements. By extension, GNNs employed for
vision-based applications must adhere to the same execution requirements. Yet
contrary to typical deep neural networks, the irregular flow of graph learning
operations poses a challenge to running GNNs on such heterogeneous MPSoC
platforms. In this paper, we propose a novel unified design-mapping approach
for efficient processing of vision GNN workloads on heterogeneous MPSoC
platforms. Particularly, we develop MaGNAS, a mapping-aware Graph Neural
Architecture Search framework. MaGNAS proposes a GNN architectural design space
coupled with prospective mapping options on a heterogeneous SoC to identify
model architectures that maximize on-device resource efficiency. To achieve
this, MaGNAS employs a two-tier evolutionary search to identify optimal GNNs
and mapping pairings that yield the best performance trade-offs. Through
designing a supernet derived from the recent Vision GNN (ViG) architecture, we
conducted experiments on four (04) state-of-the-art vision datasets using both
(i) a real hardware SoC platform (NVIDIA Xavier AGX) and (ii) a
performance/cost model simulator for DNN accelerators. Our experimental results
demonstrate that MaGNAS is able to provide 1.57x latency speedup and is 3.38x
more energy-efficient for several vision datasets executed on the Xavier MPSoC
vs. the GPU-only deployment while sustaining an average 0.11% accuracy
reduction from the baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Odema_M/0/1/0/all/0/1&quot;&gt;Mohanad Odema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouzidi_H/0/1/0/all/0/1&quot;&gt;Halima Bouzidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouarnoughi_H/0/1/0/all/0/1&quot;&gt;Hamza Ouarnoughi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niar_S/0/1/0/all/0/1&quot;&gt;Smail Niar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1&quot;&gt;Mohammad Abdullah Al Faruque&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1905.06683">
<title>Uneven illumination surface defects inspection based on convolutional neural network. (arXiv:1905.06683v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1905.06683</link>
<description rdf:parseType="Literal">&lt;p&gt;Surface defect inspection based on machine vision is often affected by uneven
illumination. In order to improve the inspection rate of surface defects
inspection under uneven illumination condition, this paper proposes a method
for detecting surface image defects based on convolutional neural network,
which is based on the adjustment of convolutional neural networks, training
parameters, changing the structure of the network, to achieve the purpose of
accurately identifying various defects. Experimental on defect inspection of
copper strip and steel images shows that the convolutional neural network can
automatically learn features without preprocessing the image, and correct
identification of various types of image defects affected by uneven
illumination, thus overcoming the drawbacks of traditional machine vision
inspection methods under uneven illumination.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yulong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wenbin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiangrong Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1911.02265">
<title>Predictive modeling of brain tumor: A Deep learning approach. (arXiv:1911.02265v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1911.02265</link>
<description rdf:parseType="Literal">&lt;p&gt;Image processing concepts can visualize the different anatomy structure of
the human body. Recent advancements in the field of deep learning have made it
possible to detect the growth of cancerous tissue just by a patient&apos;s brain
Magnetic Resonance Imaging (MRI) scans. These methods require very high
accuracy and meager false negative rates to be of any practical use. This paper
presents a Convolutional Neural Network (CNN) based transfer learning approach
to classify the brain MRI scans into two classes using three pre-trained
models. The performances of these models are compared with each other.
Experimental results show that the Resnet-50 model achieves the highest
accuracy and least false negative rates as 95% and zero respectively. It is
followed by VGG-16 and Inception-V3 model with an accuracy of 90% and 55%
respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxena_P/0/1/0/all/0/1&quot;&gt;Priyansh Saxena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1&quot;&gt;Akshat Maheshwari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maheshwari_S/0/1/0/all/0/1&quot;&gt;Saumil Maheshwari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.13389">
<title>Generating Novel Scene Compositions from Single Images and Videos. (arXiv:2103.13389v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2103.13389</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a large dataset for training, generative adversarial networks (GANs)
can achieve remarkable performance for the image synthesis task. However,
training GANs in extremely low data regimes remains a challenge, as overfitting
often occurs, leading to memorization or training divergence. In this work, we
introduce SIV-GAN, an unconditional generative model that can generate new
scene compositions from a single training image or a single video clip. We
propose a two-branch discriminator architecture, with content and layout
branches designed to judge internal content and scene layout realism separately
from each other. This discriminator design enables synthesis of visually
plausible, novel compositions of a scene, with varying content and layout,
while preserving the context of the original sample. Compared to previous
single image GANs, our model generates more diverse, higher quality images,
while not being restricted to a single image setting. We further introduce a
new challenging task of learning from a few frames of a single video. In this
training setup the training images are highly similar to each other, which
makes it difficult for prior GAN models to achieve a synthesis of both high
quality and diversity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sushko_V/0/1/0/all/0/1&quot;&gt;Vadim Sushko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1&quot;&gt;Juergen Gall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khoreva_A/0/1/0/all/0/1&quot;&gt;Anna Khoreva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2107.10998">
<title>Pruning Ternary Quantization. (arXiv:2107.10998v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2107.10998</link>
<description rdf:parseType="Literal">&lt;p&gt;Inference time, model size, and accuracy are three key factors in deep model
compression. Most of the existing work addresses these three key factors
separately as it is difficult to optimize them all at the same time. For
example, low-bit quantization aims at obtaining a faster model; weight sharing
quantization aims at improving compression ratio and accuracy; and
mixed-precision quantization aims at balancing accuracy and inference time. To
simultaneously optimize bit-width, model size, and accuracy, we propose pruning
ternary quantization (PTQ): a simple, effective, symmetric ternary quantization
method. We integrate L2 normalization, pruning, and the weight decay term to
reduce the weight discrepancy in the gradient estimator during quantization,
thus producing highly compressed ternary weights. Our method brings the highest
test accuracy and the highest compression ratio. For example, it produces a
939kb (49$\times$) 2bit ternary ResNet-18 model with only 4\% accuracy drop on
the ImageNet dataset. It compresses 170MB Mask R-CNN to 5MB (34$\times$) with
only 2.8\% average precision drop. Our method is verified on image
classification, object detection/segmentation tasks with different network
structures such as ResNet-18, ResNet-50, and MobileNetV2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chen Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xue Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2107.12655">
<title>MKConv: Multidimensional Feature Representation for Point Cloud Analysis. (arXiv:2107.12655v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2107.12655</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the remarkable success of deep learning, an optimal convolution
operation on point clouds remains elusive owing to their irregular data
structure. Existing methods mainly focus on designing an effective continuous
kernel function that can handle an arbitrary point in continuous space. Various
approaches exhibiting high performance have been proposed, but we observe that
the standard pointwise feature is represented by 1D channels and can become
more informative when its representation involves additional spatial feature
dimensions. In this paper, we present Multidimensional Kernel Convolution
(MKConv), a novel convolution operator that learns to transform the point
feature representation from a vector to a multidimensional matrix. Unlike
standard point convolution, MKConv proceeds via two steps. (i) It first
activates the spatial dimensions of local feature representation by exploiting
multidimensional kernel weights. These spatially expanded features can
represent their embedded information through spatial correlation as well as
channel correlation in feature space, carrying more detailed local structure
information. (ii) Then, discrete convolutions are applied to the
multidimensional features which can be regarded as a grid-structured matrix. In
this way, we can utilize the discrete convolutions for point cloud data without
voxelization that suffers from information loss. Furthermore, we propose a
spatial attention module, Multidimensional Local Attention (MLA), to provide
comprehensive structure awareness within the local point set by reweighting the
spatial feature dimensions. We demonstrate that MKConv has excellent
applicability to point cloud processing tasks including object classification,
object part segmentation, and scene semantic segmentation with superior
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1&quot;&gt;Sungmin Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dogyoon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sangwon Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1&quot;&gt;Woojin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sangyoun Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.06021">
<title>Probabilistic Contrastive Learning for Domain Adaptation. (arXiv:2111.06021v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2111.06021</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning can largely enhance the feature discriminability in a
self-supervised manner and has achieved remarkable success for various visual
tasks. However, it is undesirably observed that the standard contrastive
paradigm (features+$\ell_{2}$ normalization) only brings little help for domain
adaptation. In this work, we delve into this phenomenon and find that the main
reason is due to the class weights (weights of the final fully connected layer)
which are vital for the recognition yet ignored in the optimization. To tackle
this issue, we propose a simple yet powerful Probabilistic Contrastive Learning
(PCL), which does not only assist in extracting discriminative features but
also enforces them to be clustered around the class weights. Specifically, we
break the standard contrastive paradigm by removing $\ell_{2}$ normalization
and replacing the features with probabilities. In this way, PCL can enforce the
probability to approximate the one-hot form, thereby reducing the deviation
between the features and class weights. Benefiting from the conciseness, PCL
can be well generalized to different settings. In this work, we conduct
extensive experiments on five tasks and observe consistent performance gains,
i.e., Unsupervised Domain Adaptation (UDA), Semi-Supervised Domain Adaptation
(SSDA), Semi-Supervised Learning (SSL), UDA Detection, and UDA Semantic
Segmentation. Notably, for UDA Semantic Segmentation on SYNTHIA, PCL surpasses
the sophisticated CPSL-D by $&amp;gt;\!2\%$ in terms of mean IoU with a much smaller
training cost (PCL: 1*3090, 5 days v.s. CPSL-D: 4*V100, 11 days). Code is
available at https://github.com/ljjcoder/Probabilistic-Contrastive-Learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yixin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zilei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1&quot;&gt;Keyu Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1&quot;&gt;Saihui Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.13924">
<title>A Practical Contrastive Learning Framework for Single-Image Super-Resolution. (arXiv:2111.13924v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2111.13924</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning has achieved remarkable success on various high-level
tasks, but there are fewer contrastive learning-based methods proposed for
low-level tasks. It is challenging to adopt vanilla contrastive learning
technologies proposed for high-level visual tasks to low-level image
restoration problems straightly. Because the acquired high-level global visual
representations are insufficient for low-level tasks requiring rich texture and
context information. In this paper, we investigate the contrastive
learning-based single image super-resolution from two perspectives: positive
and negative sample construction and feature embedding. The existing methods
take naive sample construction approaches (e.g., considering the low-quality
input as a negative sample and the ground truth as a positive sample) and adopt
a prior model (e.g., pre-trained VGG model) to obtain the feature embedding. To
this end, we propose a practical contrastive learning framework for SISR, named
PCL-SR. We involve the generation of many informative positive and hard
negative samples in frequency space. Instead of utilizing an additional
pre-trained network, we design a simple but effective embedding network
inherited from the discriminator network which is more task-friendly. Compared
with existing benchmark methods, we re-train them by our proposed PCL-SR
framework and achieve superior performance. Extensive experiments have been
conducted to show the effectiveness and technical contributions of our proposed
PCL-SR thorough ablation studies. The code and pre-trained models can be found
at https://github.com/Aitical/PCL-SISR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1&quot;&gt;Gang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Junjun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.06845">
<title>Taylor3DNet: Fast 3D Shape Inference With Landmark Points Based Taylor Series. (arXiv:2201.06845v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2201.06845</link>
<description rdf:parseType="Literal">&lt;p&gt;Benefiting from the continuous representation ability, deep implicit
functions can represent a shape at infinite resolution. However, extracting
high-resolution iso-surface from an implicit function requires
forward-propagating a network with a large number of parameters for numerous
query points, thus preventing the generation speed. Inspired by the Taylor
series, we propose Taylo3DNet to accelerate the inference of implicit shape
representations. Taylor3DNet exploits a set of discrete landmark points and
their corresponding Taylor series coefficients to represent the implicit field
of a 3D shape, and the number of landmark points is independent of the
resolution of the iso-surface extraction. Once the coefficients corresponding
to the landmark points are predicted, the network evaluation for each query
point can be simplified as a low-order Taylor series calculation with several
nearest landmark points. Based on this efficient representation, our
Taylor3DNet achieves a significantly faster inference speed than classical
network-based implicit functions. We evaluate our approach on reconstruction
tasks with various input types, and the results demonstrate that our approach
can improve the inference speed by a large margin without sacrificing the
performance compared with state-of-the-art baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yuting Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiale Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shenghua Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.14308">
<title>Temporal Transductive Inference for Few-Shot Video Object Segmentation. (arXiv:2203.14308v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.14308</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot video object segmentation (FS-VOS) aims at segmenting video frames
using a few labelled examples of classes not seen during initial training. In
this paper, we present a simple but effective temporal transductive inference
(TTI) approach that leverages temporal consistency in the unlabelled video
frames during few-shot inference. Key to our approach is the use of both global
and local temporal constraints. The objective of the global constraint is to
learn consistent linear classifiers for novel classes across the image
sequence, whereas the local constraint enforces the proportion of
foreground/background regions in each frame to be coherent across a local
temporal window. These constraints act as spatiotemporal regularizers during
the transductive inference to increase temporal coherence and reduce
overfitting on the few-shot support set. Empirically, our model outperforms
state-of-the-art meta-learning approaches in terms of mean intersection over
union on YouTube-VIS by 2.8%. In addition, we introduce improved benchmarks
that are exhaustively labelled (i.e. all object occurrences are labelled,
unlike the currently available), and present a more realistic evaluation
paradigm that targets data distribution shift between training and testing
sets. Our empirical results and in-depth analysis confirm the added benefits of
the proposed spatiotemporal regularizers to improve temporal coherence and
overcome certain overfitting scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siam_M/0/1/0/all/0/1&quot;&gt;Mennatullah Siam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1&quot;&gt;Konstantinos G. Derpanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1&quot;&gt;Richard P. Wildes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.16329">
<title>Parameter-efficient Model Adaptation for Vision Transformers. (arXiv:2203.16329v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.16329</link>
<description rdf:parseType="Literal">&lt;p&gt;In computer vision, it has achieved great transfer learning performance via
adapting large-scale pretrained vision models (e.g., vision transformers) to
downstream tasks. Common approaches for model adaptation either update all
model parameters or leverage linear probes. In this paper, we aim to study
parameter-efficient model adaptation strategies for vision transformers on the
image classification task. We formulate efficient model adaptation as a
subspace training problem and perform a comprehensive benchmarking over
different efficient adaptation methods. We conduct an empirical study on each
efficient model adaptation method focusing on its performance alongside
parameter cost. Furthermore, we propose a parameter-efficient model adaptation
framework, which first selects submodules by measuring local intrinsic
dimensions and then projects them into subspace for further decomposition via a
novel Kronecker Adaptation (KAdaptation) method. We analyze and compare our
method with a diverse set of baseline model adaptation methods (including
state-of-the-art methods for pretrained language models). Our method performs
the best in terms of the tradeoff between accuracy and parameter efficiency
across 20 image classification datasets under the few-shot setting and 7 image
classification datasets under the full-shot setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xuehai He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pengchuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Eric Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.03923">
<title>Unsupervised Discovery and Composition of Object Light Fields. (arXiv:2205.03923v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.03923</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural scene representations, both continuous and discrete, have recently
emerged as a powerful new paradigm for 3D scene understanding. Recent efforts
have tackled unsupervised discovery of object-centric neural scene
representations. However, the high cost of ray-marching, exacerbated by the
fact that each object representation has to be ray-marched separately, leads to
insufficiently sampled radiance fields and thus, noisy renderings, poor
framerates, and high memory and time complexity during training and rendering.
Here, we propose to represent objects in an object-centric, compositional scene
representation as light fields. We propose a novel light field compositor
module that enables reconstructing the global light field from a set of
object-centric light fields. Dubbed Compositional Object Light Fields (COLF),
our method enables unsupervised learning of object-centric neural scene
representations, state-of-the-art reconstruction and novel view synthesis
performance on standard datasets, and rendering and training speeds at orders
of magnitude faster than existing 3D approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_C/0/1/0/all/0/1&quot;&gt;Cameron Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hong-Xing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zakharov_S/0/1/0/all/0/1&quot;&gt;Sergey Zakharov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1&quot;&gt;Fredo Durand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1&quot;&gt;Vincent Sitzmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.07433">
<title>Binarizing by Classification: Is soft function really necessary?. (arXiv:2205.07433v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.07433</link>
<description rdf:parseType="Literal">&lt;p&gt;Binary neural networks leverage $\mathrm{Sign}$ function to binarize weights
and activations, which require gradient estimators to overcome its
non-differentiability and will inevitably bring gradient errors during
backpropagation. Although many hand-designed soft functions have been proposed
as gradient estimators to better approximate gradients, their mechanism is not
clear and there are still huge performance gaps between binary models and their
full-precision counterparts. To address these issues and reduce gradient error,
we propose to tackle network binarization as a binary classification problem
and use a multi-layer perceptron (MLP) as the classifier in the forward pass
and gradient estimator in the backward pass. Benefiting from the MLP&apos;s
theoretical capability to fit any continuous function, it can be adaptively
learned to binarize networks and backpropagate gradients without any prior
knowledge of soft functions. From this perspective, we further empirically
justify that even a simple linear function can outperform previous complex soft
functions. Extensive experiments demonstrate that the proposed method yields
surprising performance both in image classification and human pose estimation
tasks. Specifically, we achieve $65.7\%$ top-1 accuracy of ResNet-34 on
ImageNet dataset, with an absolute improvement of $2.6\%$. Moreover, we take
binarization as a lightweighting approach for pose estimation models and
propose well-designed binary pose estimation networks SBPN and BHRNet. When
evaluating on the challenging Microsoft COCO keypoint dataset, the proposed
method enables binary networks to achieve a mAP of up to $60.6$ for the first
time. Experiments conducted on real platforms demonstrate that BNN achieves a
better balance between performance and computational complexity, especially
when computational resources are extremely low.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yefei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Luoming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Weijia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hong Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.05379">
<title>Action-based Early Autism Diagnosis Using Contrastive Feature Learning. (arXiv:2209.05379v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.05379</link>
<description rdf:parseType="Literal">&lt;p&gt;Autism, also known as Autism Spectrum Disorder (or ASD), is a neurological
disorder. Its main symptoms include difficulty in (verbal and/or non-verbal)
communication, and rigid/repetitive behavior. These symptoms are often
indistinguishable from a normal (control) individual, due to which this
disorder remains undiagnosed in early childhood leading to delayed treatment.
Since the learning curve is steep during the initial age, an early diagnosis of
autism could allow to take adequate interventions at the right time, which
might positively affect the growth of an autistic child. Further, the
traditional methods of autism diagnosis require multiple visits to a
specialized psychiatrist, however this process can be time-consuming. In this
paper, we present a learning based approach to automate autism diagnosis using
simple and small action video clips of subjects. This task is particularly
challenging because the amount of annotated data available is small, and the
variations among samples from the two categories (ASD and control) are
generally indistinguishable. This is also evident from poor performance of a
binary classifier learned using the cross-entropy loss on top of a baseline
encoder. To address this, we adopt contrastive feature learning in both self
supervised and supervised learning frameworks, and show that these can lead to
a significant increase in the prediction accuracy of a binary classifier on
this task. We further validate this by conducting thorough experimental
analyses under different set-ups on two publicly available datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rani_A/0/1/0/all/0/1&quot;&gt;Asha Rani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1&quot;&gt;Pankaj Yadav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_Y/0/1/0/all/0/1&quot;&gt;Yashaswi Verma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.12068">
<title>NeRF-Loc: Transformer-Based Object Localization Within Neural Radiance Fields. (arXiv:2209.12068v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.12068</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRFs) have become a widely-applied scene
representation technique in recent years, showing advantages for robot
navigation and manipulation tasks. To further advance the utility of NeRFs for
robotics, we propose a transformer-based framework, NeRF-Loc, to extract 3D
bounding boxes of objects in NeRF scenes. NeRF-Loc takes a pre-trained NeRF
model and camera view as input and produces labeled, oriented 3D bounding boxes
of objects as output. Using current NeRF training tools, a robot can train a
NeRF environment model in real-time and, using our algorithm, identify 3D
bounding boxes of objects of interest within the NeRF for downstream navigation
or manipulation tasks. Concretely, we design a pair of paralleled transformer
encoder branches, namely the coarse stream and the fine stream, to encode both
the context and details of target objects. The encoded features are then fused
together with attention layers to alleviate ambiguities for accurate object
localization. We have compared our method with conventional RGB(-D) based
methods that take rendered RGB images and depths from NeRFs as inputs. Our
method is better than the baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiankai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Mingyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_H/0/1/0/all/0/1&quot;&gt;Hongwei Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingdong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liangjun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwager_M/0/1/0/all/0/1&quot;&gt;Mac Schwager&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.10886">
<title>Backdoor Attack and Defense in Federated Generative Adversarial Network-based Medical Image Synthesis. (arXiv:2210.10886v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.10886</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning-based image synthesis techniques have been applied in
healthcare research for generating medical images to support open research and
augment medical datasets. Training generative adversarial neural networks
(GANs) usually require large amounts of training data. Federated learning (FL)
provides a way of training a central model using distributed data while keeping
raw data locally. However, given that the FL server cannot access the raw data,
it is vulnerable to backdoor attacks, an adversarial by poisoning training
data. Most backdoor attack strategies focus on classification models and
centralized domains. It is still an open question if the existing backdoor
attacks can affect GAN training and, if so, how to defend against the attack in
the FL setting. In this work, we investigate the overlooked issue of backdoor
attacks in federated GANs (FedGANs). The success of this attack is subsequently
determined to be the result of some local discriminators overfitting the
poisoned data and corrupting the local GAN equilibrium, which then further
contaminates other clients when averaging the generator&apos;s parameters and yields
high generator loss. Therefore, we proposed FedDetect, an efficient and
effective way of defending against the backdoor attack in the FL setting, which
allows the server to detect the client&apos;s adversarial behavior based on their
losses and block the malicious clients. Our extensive experiments on two
medical datasets with different modalities demonstrate the backdoor attack on
FedGANs can result in synthetic images with low fidelity. After detecting and
suppressing the detected malicious clients using the proposed defense strategy,
we show that FedGANs can synthesize high-quality medical datasets (with labels)
for data augmentation to improve classification models&apos; performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1&quot;&gt;Ruinan Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.07137">
<title>DroneNet: Crowd Density Estimation using Self-ONNs for Drones. (arXiv:2211.07137v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.07137</link>
<description rdf:parseType="Literal">&lt;p&gt;Video surveillance using drones is both convenient and efficient due to the
ease of deployment and unobstructed movement of drones in many scenarios. An
interesting application of drone-based video surveillance is to estimate crowd
densities (both pedestrians and vehicles) in public places. Deep learning using
convolution neural networks (CNNs) is employed for automatic crowd counting and
density estimation using images and videos. However, the performance and
accuracy of such models typically depend upon the model architecture i.e.,
deeper CNN models improve accuracy at the cost of increased inference time. In
this paper, we propose a novel crowd density estimation model for drones
(DroneNet) using Self-organized Operational Neural Networks (Self-ONN).
Self-ONN provides efficient learning capabilities with lower computational
complexity as compared to CNN-based models. We tested our algorithm on two
drone-view public datasets. Our evaluation shows that the proposed DroneNet
shows superior performance on an equivalent CNN-based model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Muhammad Asif Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menouar_H/0/1/0/all/0/1&quot;&gt;Hamid Menouar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamila_R/0/1/0/all/0/1&quot;&gt;Ridha Hamila&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11629">
<title>PVT++: A Simple End-to-End Latency-Aware Visual Tracking Framework. (arXiv:2211.11629v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11629</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual object tracking is essential to intelligent robots. Most existing
approaches have ignored the online latency that can cause severe performance
degradation during real-world processing. Especially for unmanned aerial
vehicles (UAVs), where robust tracking is more challenging and onboard
computation is limited, the latency issue can be fatal. In this work, we
present a simple framework for end-to-end latency-aware tracking, i.e.,
end-to-end predictive visual tracking (PVT++). Unlike existing solutions that
naively append Kalman Filters after trackers, PVT++ can be jointly optimized,
so that it takes not only motion information but can also leverage the rich
visual knowledge in most pre-trained tracker models for robust prediction.
Besides, to bridge the training-evaluation domain gap, we propose a relative
motion factor, empowering PVT++ to generalize to the challenging and complex
UAV tracking scenes. These careful designs have made the small-capacity
lightweight PVT++ a widely effective solution. Additionally, this work presents
an extended latency-aware evaluation benchmark for assessing an any-speed
tracker in the online setting. Empirical results on a robotic platform from the
aerial perspective show that PVT++ can achieve significant performance gain on
various trackers and exhibit higher accuracy than prior solutions, largely
mitigating the degradation brought by latency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bowen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Ziyuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Junjie Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1&quot;&gt;Sebastian Scherer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1&quot;&gt;Changhong Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13435">
<title>A Benchmark of Long-tailed Instance Segmentation with Noisy Labels. (arXiv:2211.13435v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13435</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider the instance segmentation task on a long-tailed
dataset, which contains label noise, i.e., some of the annotations are
incorrect. There are two main reasons making this case realistic. First,
datasets collected from real world usually obey a long-tailed distribution.
Second, for instance segmentation datasets, as there are many instances in one
image and some of them are tiny, it is easier to introduce noise into the
annotations. Specifically, we propose a new dataset, which is a large
vocabulary long-tailed dataset containing label noise for instance
segmentation. Furthermore, we evaluate previous proposed instance segmentation
algorithms on this dataset. The results indicate that the noise in the training
dataset will hamper the model in learning rare categories and decrease the
overall performance, and inspire us to explore more effective approaches to
address this practical challenge. The code and dataset are available in
https://github.com/GuanlinLee/Noisy-LVIS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1&quot;&gt;Guowen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianwei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.14646">
<title>Towards Improved Input Masking for Convolutional Neural Networks. (arXiv:2211.14646v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.14646</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to remove features from the input of machine learning models is
very important to understand and interpret model predictions. However, this is
non-trivial for vision models since masking out parts of the input image
typically causes large distribution shifts. This is because the baseline color
used for masking (typically grey or black) is out of distribution. Furthermore,
the shape of the mask itself can contain unwanted signals which can be used by
the model for its predictions. Recently, there has been some progress in
mitigating this issue (called missingness bias) in image masking for vision
transformers. In this work, we propose a new masking method for CNNs we call
layer masking in which the missingness bias caused by masking is reduced to a
large extent. Intuitively, layer masking applies a mask to intermediate
activation maps so that the model only processes the unmasked input. We show
that our method (i) is able to eliminate or minimize the influence of the mask
shape or color on the output of the model, and (ii) is much better than
replacing the masked region by black or grey for input perturbation based
interpretability techniques like LIME. Thus, layer masking is much less
affected by missingness bias than other masking strategies. We also demonstrate
how the shape of the mask may leak information about the class, thus affecting
estimates of model reliance on class-relevant features derived from input
masking. Furthermore, we discuss the role of data augmentation techniques for
tackling this problem, and argue that they are not sufficient for preventing
model reliance on mask shape. The code for this project is publicly available
at https://github.com/SriramB-98/layer_masking
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasubramanian_S/0/1/0/all/0/1&quot;&gt;Sriram Balasubramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1&quot;&gt;Soheil Feizi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.16198">
<title>SuS-X: Training-Free Name-Only Transfer of Vision-Language Models. (arXiv:2211.16198v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.16198</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet
effective way to train large-scale vision-language models. CLIP demonstrates
impressive zero-shot classification and retrieval on diverse downstream tasks.
However, to leverage its full potential, fine-tuning still appears to be
necessary. Fine-tuning the entire CLIP model can be resource-intensive and
unstable. Moreover, recent methods that aim to circumvent this need for
fine-tuning still require access to images from the target distribution. In
this paper, we pursue a different approach and explore the regime of
training-free &quot;name-only transfer&quot; in which the only knowledge we possess about
the downstream task comprises the names of downstream target categories. We
propose a novel method, SuS-X, consisting of two key building blocks -- SuS and
TIP-X, that requires neither intensive fine-tuning nor costly labelled data.
SuS-X achieves state-of-the-art zero-shot classification results on 19
benchmark datasets. We further show the utility of TIP-X in the training-free
few-shot setting, where we again achieve state-of-the-art results over strong
training-free baselines. Code is available at
https://github.com/vishaal27/SuS-X.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Udandarao_V/0/1/0/all/0/1&quot;&gt;Vishaal Udandarao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Ankush Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1&quot;&gt;Samuel Albanie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01450">
<title>Crowd Density Estimation using Imperfect Labels. (arXiv:2212.01450v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01450</link>
<description rdf:parseType="Literal">&lt;p&gt;Density estimation is one of the most widely used methods for crowd counting
in which a deep learning model learns from head-annotated crowd images to
estimate crowd density in unseen images. Typically, the learning performance of
the model is highly impacted by the accuracy of the annotations and inaccurate
annotations may lead to localization and counting errors during prediction. A
significant amount of works exist on crowd counting using perfectly labelled
datasets but none of these explore the impact of annotation errors on the model
accuracy. In this paper, we investigate the impact of imperfect labels (both
noisy and missing labels) on crowd counting accuracy. We propose a system that
automatically generates imperfect labels using a deep learning model (called
annotator) which are then used to train a new crowd counting model (target
model). Our analysis on two crowd counting models and two benchmark datasets
shows that the proposed scheme achieves accuracy closer to that of the model
trained with perfect labels showing the robustness of crowd models to
annotation errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Muhammad Asif Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menouar_H/0/1/0/all/0/1&quot;&gt;Hamid Menouar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamila_R/0/1/0/all/0/1&quot;&gt;Ridha Hamila&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01452">
<title>CLIP: Train Faster with Less Data. (arXiv:2212.01452v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01452</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models require an enormous amount of data for training.
However, recently there is a shift in machine learning from model-centric to
data-centric approaches. In data-centric approaches, the focus is to refine and
improve the quality of the data to improve the learning performance of the
models rather than redesigning model architectures. In this paper, we propose
CLIP i.e., Curriculum Learning with Iterative data Pruning. CLIP combines two
data-centric approaches i.e., curriculum learning and dataset pruning to
improve the model learning accuracy and convergence speed. The proposed scheme
applies loss-aware dataset pruning to iteratively remove the least significant
samples and progressively reduces the size of the effective dataset in the
curriculum learning training. Extensive experiments performed on crowd density
estimation models validate the notion behind combining the two approaches by
reducing the convergence time and improving generalization. To our knowledge,
the idea of data pruning as an embedded process in curriculum learning is
novel.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Muhammad Asif Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamila_R/0/1/0/all/0/1&quot;&gt;Ridha Hamila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menouar_H/0/1/0/all/0/1&quot;&gt;Hamid Menouar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.05376">
<title>What&apos;s Wrong with the Absolute Trajectory Error?. (arXiv:2212.05376v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2212.05376</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the main limitations of the commonly used Absolute Trajectory Error
(ATE) is that it is highly sensitive to outliers. As a result, in the presence
of just a few outliers, it often fails to reflect the varying accuracy as the
inlier trajectory error or the number of outliers varies. In this work, we
propose an alternative error metric for evaluating the accuracy of the
reconstructed camera trajectory. Our metric, named Discernible Trajectory Error
(DTE), is computed in five steps: (1) Shift the ground-truth and estimated
trajectories such that both of their geometric medians are located at the
origin. (2) Rotate the estimated trajectory such that it minimizes the sum of
geodesic distances between the corresponding camera orientations. (3) Scale the
estimated trajectory such that the median distance of the cameras to their
geometric median is the same as that of the ground truth. (4) Compute and
winsorize the distances between the corresponding cameras. (5) Obtain the DTE
by taking the average of the mean and the root-mean-square (RMS) of the
winsorized distances. This metric is an attractive alternative to the ATE, in
that it is capable of discerning the varying trajectory accuracy as the inlier
trajectory error or the number of outliers varies. Using the similar idea, we
also propose a novel rotation error metric, named Discernible Rotation Error
(DRE), which has similar advantages to the DTE. Furthermore, we propose a
simple yet effective method for calibrating the camera-to-marker rotation,
which is needed for the computation of our metrics. Our methods are verified
through extensive simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seong Hun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1&quot;&gt;Javier Civera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.07158">
<title>Establishing a stronger baseline for lightweight contrastive models. (arXiv:2212.07158v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.07158</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research has reported a performance degradation in self-supervised
contrastive learning for specially designed efficient networks, such as
MobileNet and EfficientNet. A common practice to address this problem is to
introduce a pretrained contrastive teacher model and train the lightweight
networks with distillation signals generated by the teacher. However, it is
time and resource consuming to pretrain a teacher model when it is not
available. In this work, we aim to establish a stronger baseline for
lightweight contrastive models without using a pretrained teacher model.
Specifically, we show that the optimal recipe for efficient models is different
from that of larger models, and using the same training settings as ResNet50,
as previous research does, is inappropriate. Additionally, we observe a common
issu e in contrastive learning where either the positive or negative views can
be noisy, and propose a smoothed version of InfoNCE loss to alleviate this
problem. As a result, we successfully improve the linear evaluation results
from 36.3\% to 62.3\% for MobileNet-V3-Large and from 42.2\% to 65.8\% for
EfficientNet-B0 on ImageNet, closing the accuracy gap to ResNet50 with
$5\times$ fewer parameters. We hope our research will facilitate the usage of
lightweight contrastive models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wenye Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yifeng Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhixiong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hai-tao Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.08071">
<title>MAViL: Masked Audio-Video Learners. (arXiv:2212.08071v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.08071</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Masked Audio-Video Learners (MAViL) to train audio-visual
representations. Our approach learns with three complementary forms of
self-supervision: (1) reconstruction of masked audio and video input data, (2)
intra- and inter-modal contrastive learning with masking, and (3) self-training
by reconstructing joint audio-video contextualized features learned from the
first two objectives. Pre-training with MAViL not only enables the model to
perform well in audio-visual classification and retrieval tasks but also
improves representations of each modality in isolation, without using
information from the other modality for fine-tuning or inference. Empirically,
MAViL sets a new state-of-the-art on AudioSet (53.1 mAP) and VGGSound (67.1%
accuracy). For the first time, a self-supervised audio-visual model outperforms
ones that use external supervision on these benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Po-Yao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1&quot;&gt;Vasu Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryali_C/0/1/0/all/0/1&quot;&gt;Chaitanya Ryali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Haoqi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanghao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shang-Wen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1&quot;&gt;Gargi Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1&quot;&gt;Jitendra Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1&quot;&gt;Christoph Feichtenhofer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09298">
<title>From a Bird&apos;s Eye View to See: Joint Camera and Subject Registration without the Camera Calibration. (arXiv:2212.09298v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09298</link>
<description rdf:parseType="Literal">&lt;p&gt;We tackle a new problem of multi-view camera and subject registration in the
bird&apos;s eye view (BEV) without pre-given camera calibration. This is a very
challenging problem since its only input is several RGB images from different
first-person views (FPVs) for a multi-person scene, without the BEV image and
the calibration of the FPVs, while the output is a unified plane with the
localization and orientation of both the subjects and cameras in a BEV. We
propose an end-to-end framework solving this problem, whose main idea can be
divided into following parts: i) creating a view-transform subject detection
module to transform the FPV to a virtual BEV including localization and
orientation of each pedestrian, ii) deriving a geometric transformation based
method to estimate camera localization and view direction, i.e., the camera
registration in a unified BEV, iii) making use of spatial and appearance
information to aggregate the subjects into the unified BEV. We collect a new
large-scale synthetic dataset with rich annotations for evaluation. The
experimental results show the remarkable effectiveness of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1&quot;&gt;Zekun Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1&quot;&gt;Ruize Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1&quot;&gt;Wei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Feifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Song Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.01947">
<title>StitchNet: Composing Neural Networks from Pre-Trained Fragments. (arXiv:2301.01947v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.01947</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose StitchNet, a novel neural network creation paradigm that stitches
together fragments (one or more consecutive network layers) from multiple
pre-trained neural networks. StitchNet allows the creation of high-performing
neural networks without the large compute and data requirements needed under
traditional model creation processes via backpropagation training. We leverage
Centered Kernel Alignment (CKA) as a compatibility measure to efficiently guide
the selection of these fragments in composing a network for a given task
tailored to specific accuracy needs and computing resource constraints. We then
show that these fragments can be stitched together to create neural networks
with comparable accuracy to traditionally trained networks at a fraction of
computing resource and data requirements. Finally, we explore a novel
on-the-fly personalized model creation and inference application enabled by
this new paradigm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teerapittayanon_S/0/1/0/all/0/1&quot;&gt;Surat Teerapittayanon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Comiter_M/0/1/0/all/0/1&quot;&gt;Marcus Comiter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDanel_B/0/1/0/all/0/1&quot;&gt;Brad McDanel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kung_H/0/1/0/all/0/1&quot;&gt;H.T. Kung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.06719">
<title>FemtoDet: An Object Detection Baseline for Energy Versus Performance Tradeoffs. (arXiv:2301.06719v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.06719</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient detectors for edge devices are often optimized for parameters or
speed count metrics, which remain in weak correlation with the energy of
detectors.
&lt;/p&gt;
&lt;p&gt;However, some vision applications of convolutional neural networks, such as
always-on surveillance cameras, are critical for energy constraints.
&lt;/p&gt;
&lt;p&gt;This paper aims to serve as a baseline by designing detectors to reach
tradeoffs between energy and performance from two perspectives:
&lt;/p&gt;
&lt;p&gt;1) We extensively analyze various CNNs to identify low-energy architectures,
including selecting activation functions, convolutions operators, and feature
fusion structures on necks. These underappreciated details in past work
seriously affect the energy consumption of detectors;
&lt;/p&gt;
&lt;p&gt;2) To break through the dilemmatic energy-performance problem, we propose a
balanced detector driven by energy using discovered low-energy components named
\textit{FemtoDet}.
&lt;/p&gt;
&lt;p&gt;In addition to the novel construction, we improve FemtoDet by considering
convolutions and training strategy optimizations.
&lt;/p&gt;
&lt;p&gt;Specifically, we develop a new instance boundary enhancement (IBE) module for
convolution optimization to overcome the contradiction between the limited
capacity of CNNs and detection tasks in diverse spatial representations, and
propose a recursive warm-restart (RecWR) for optimizing training strategy to
escape the sub-optimization of light-weight detectors by considering the data
shift produced in popular augmentations.
&lt;/p&gt;
&lt;p&gt;As a result, FemtoDet with only 68.77k parameters achieves a competitive
score of 46.3 AP50 on PASCAL VOC and 1.11 W $\&amp;amp;$ 64.47 FPS on Qualcomm
Snapdragon 865 CPU platforms.
&lt;/p&gt;
&lt;p&gt;Extensive experiments on COCO and TJU-DHD datasets indicate that the proposed
method achieves competitive results in diverse scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_P/0/1/0/all/0/1&quot;&gt;Peng Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xu Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+AI_G/0/1/0/all/0/1&quot;&gt;Guo AI&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuexiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yawen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yefeng Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.08125">
<title>Diagnose Like a Pathologist: Transformer-Enabled Hierarchical Attention-Guided Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2301.08125v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.08125</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiple Instance Learning (MIL) and transformers are increasingly popular in
histopathology Whole Slide Image (WSI) classification. However, unlike human
pathologists who selectively observe specific regions of histopathology tissues
under different magnifications, most methods do not incorporate multiple
resolutions of the WSIs, hierarchically and attentively, thereby leading to a
loss of focus on the WSIs and information from other resolutions. To resolve
this issue, we propose a Hierarchical Attention-Guided Multiple Instance
Learning framework to fully exploit the WSIs. This framework can dynamically
and attentively discover the discriminative regions across multiple resolutions
of the WSIs. Within this framework, an Integrated Attention Transformer is
proposed to further enhance the performance of the transformer and obtain a
more holistic WSI (bag) representation. This transformer consists of multiple
Integrated Attention Modules, which is the combination of a transformer layer
and an aggregation module that produces a bag representation based on every
instance representation in that bag. The experimental results show that our
method achieved state-of-the-art performances on multiple datasets, including
Camelyon16, TCGA-RCC, TCGA-NSCLC, and an in-house IMGC dataset. The code is
available at https://github.com/BearCleverProud/HAG-MIL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Conghao Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sung_J/0/1/0/all/0/1&quot;&gt;Joseph J.Y. Sung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1&quot;&gt;Irwin King&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.08739">
<title>FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer. (arXiv:2301.08739v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.08739</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer, as an alternative to CNN, has been proven effective in many
modalities (e.g., texts and images). For 3D point cloud transformers, existing
efforts focus primarily on pushing their accuracy to the state-of-the-art
level. However, their latency lags behind sparse convolution-based models (3x
slower), hindering their usage in resource-constrained, latency-sensitive
applications (such as autonomous driving). This inefficiency comes from point
clouds&apos; sparse and irregular nature, whereas transformers are designed for
dense, regular workloads. This paper presents FlatFormer to close this latency
gap by trading spatial proximity for better computational regularity. We first
flatten the point cloud with window-based sorting and partition points into
groups of equal sizes rather than windows of equal shapes. This effectively
avoids expensive structuring and padding overheads. We then apply
self-attention within groups to extract local features, alternate sorting axis
to gather features from different directions, and shift windows to exchange
features across groups. FlatFormer delivers state-of-the-art accuracy on Waymo
Open Dataset with 4.6x speedup over (transformer-based) SST and 1.4x speedup
over (sparse convolutional) CenterPoint. This is the first point cloud
transformer that achieves real-time performance on edge GPUs and is faster than
sparse convolutional methods while achieving on-par or even superior accuracy
on large-scale benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhijian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xinyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Haotian Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Song Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.02521">
<title>Exploiting Partial Common Information Microstructure for Multi-Modal Brain Tumor Segmentation. (arXiv:2302.02521v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.02521</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning with multiple modalities is crucial for automated brain tumor
segmentation from magnetic resonance imaging data. Explicitly optimizing the
common information shared among all modalities (e.g., by maximizing the total
correlation) has been shown to achieve better feature representations and thus
enhance the segmentation performance. However, existing approaches are
oblivious to partial common information shared by subsets of the modalities. In
this paper, we show that identifying such partial common information can
significantly boost the discriminative power of image segmentation models. In
particular, we introduce a novel concept of partial common information mask
(PCI-mask) to provide a fine-grained characterization of what partial common
information is shared by which subsets of the modalities. By solving a masked
correlation maximization and simultaneously learning an optimal PCI-mask, we
identify the latent microstructure of partial common information and leverage
it in a self-attention module to selectively weight different feature
representations in multi-modal data. We implement our proposed framework on the
standard U-Net. Our experimental results on the Multi-modal Brain Tumor
Segmentation Challenge (BraTS) datasets outperform those of state-of-the-art
segmentation baselines, with validation Dice similarity coefficients of 0.920,
0.897, 0.837 for the whole tumor, tumor core, and enhancing tumor on
BraTS-2020.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_Y/0/1/0/all/0/1&quot;&gt;Yongsheng Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkataramani_G/0/1/0/all/0/1&quot;&gt;Guru Venkataramani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1&quot;&gt;Tian Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.03665">
<title>HumanMAC: Masked Motion Completion for Human Motion Prediction. (arXiv:2302.03665v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.03665</link>
<description rdf:parseType="Literal">&lt;p&gt;Human motion prediction is a classical problem in computer vision and
computer graphics, which has a wide range of practical applications. Previous
effects achieve great empirical performance based on an encoding-decoding
style. The methods of this style work by first encoding previous motions to
latent representations and then decoding the latent representations into
predicted motions. However, in practice, they are still unsatisfactory due to
several issues, including complicated loss constraints, cumbersome training
processes, and scarce switch of different categories of motions in prediction.
In this paper, to address the above issues, we jump out of the foregoing style
and propose a novel framework from a new perspective. Specifically, our
framework works in a masked completion fashion. In the training stage, we learn
a motion diffusion model that generates motions from random noise. In the
inference stage, with a denoising procedure, we make motion prediction
conditioning on observed motions to output more continuous and controllable
predictions. The proposed framework enjoys promising algorithmic properties,
which only needs one loss in optimization and is trained in an end-to-end
manner. Additionally, it accomplishes the switch of different categories of
motions effectively, which is significant in realistic tasks, e.g., the
animation task. Comprehensive experiments on benchmarks confirm the superiority
of the proposed framework. The project page is available at
https://lhchen.top/Human-MAC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Ling-Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiawei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yewen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1&quot;&gt;Yiren Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1&quot;&gt;Xiaobo Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tongliang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06235">
<title>A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models. (arXiv:2302.06235v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06235</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastively trained text-image models have the remarkable ability to
perform zero-shot classification, that is, classifying previously unseen images
into categories that the model has never been explicitly trained to identify.
However, these zero-shot classifiers need prompt engineering to achieve high
accuracy. Prompt engineering typically requires hand-crafting a set of prompts
for individual downstream tasks. In this work, we aim to automate this prompt
engineering and improve zero-shot accuracy through prompt ensembling. In
particular, we ask &quot;Given a large pool of prompts, can we automatically score
the prompts and ensemble those that are most suitable for a particular
downstream dataset, without needing access to labeled validation data?&quot;. We
demonstrate that this is possible. In doing so, we identify several pathologies
in a naive prompt scoring method where the score can be easily overconfident
due to biases in pre-training and test data, and we propose a novel prompt
scoring method that corrects for the biases. Using our proposed scoring method
to create a weighted average prompt ensemble, our method outperforms equal
average ensemble, as well as hand-crafted prompts, on ImageNet, 4 of its
variants, and 11 fine-grained classification benchmarks, all while being fully
automatic, optimization-free, and not requiring access to labeled validation
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allingham_J/0/1/0/all/0/1&quot;&gt;James Urquhart Allingham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jie Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dusenberry_M/0/1/0/all/0/1&quot;&gt;Michael W Dusenberry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xiuye Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yin Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1&quot;&gt;Dustin Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jeremiah Zhe Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1&quot;&gt;Balaji Lakshminarayanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06294">
<title>CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection. (arXiv:2302.06294v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06294</link>
<description rdf:parseType="Literal">&lt;p&gt;Formalizing surgical activities as triplets of the used instruments, actions
performed, and target anatomies is becoming a gold standard approach for
surgical activity modeling. The benefit is that this formalization helps to
obtain a more detailed understanding of tool-tissue interaction which can be
used to develop better Artificial Intelligence assistance for image-guided
surgery. Earlier efforts and the CholecTriplet challenge introduced in 2021
have put together techniques aimed at recognizing these triplets from surgical
footage. Estimating also the spatial locations of the triplets would offer a
more precise intraoperative context-aware decision support for
computer-assisted intervention. This paper presents the CholecTriplet2022
challenge, which extends surgical action triplet modeling from recognition to
detection. It includes weakly-supervised bounding box localization of every
visible surgical instrument (or tool), as the key actors, and the modeling of
each tool-activity in the form of &amp;lt;instrument, verb, target&amp;gt; triplet. The paper
describes a baseline method and 10 new deep learning algorithms presented at
the challenge to solve the task. It also provides thorough methodological
comparisons of the methods, an in-depth analysis of the obtained results across
multiple metrics, visual and procedural challenges; their significance, and
useful insights for future research directions and applications in surgery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nwoye_C/0/1/0/all/0/1&quot;&gt;Chinedu Innocent Nwoye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sharma_S/0/1/0/all/0/1&quot;&gt;Saurav Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Murali_A/0/1/0/all/0/1&quot;&gt;Aditya Murali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alapatt_D/0/1/0/all/0/1&quot;&gt;Deepak Alapatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vardazaryan_A/0/1/0/all/0/1&quot;&gt;Armine Vardazaryan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yuan_K/0/1/0/all/0/1&quot;&gt;Kun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hajek_J/0/1/0/all/0/1&quot;&gt;Jonas Hajek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Reiter_W/0/1/0/all/0/1&quot;&gt;Wolfgang Reiter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yamlahi_A/0/1/0/all/0/1&quot;&gt;Amine Yamlahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Smidt_F/0/1/0/all/0/1&quot;&gt;Finn-Henri Smidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zheng_G/0/1/0/all/0/1&quot;&gt;Guoyan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oliveira_B/0/1/0/all/0/1&quot;&gt;Bruno Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Torres_H/0/1/0/all/0/1&quot;&gt;Helena R. Torres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kondo_S/0/1/0/all/0/1&quot;&gt;Satoshi Kondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kasai_S/0/1/0/all/0/1&quot;&gt;Satoshi Kasai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Holm_F/0/1/0/all/0/1&quot;&gt;Felix Holm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ozsoy_E/0/1/0/all/0/1&quot;&gt;Ege &amp;#xd6;zsoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gui_S/0/1/0/all/0/1&quot;&gt;Shuangchun Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Han Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raviteja_S/0/1/0/all/0/1&quot;&gt;Sista Raviteja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sathish_R/0/1/0/all/0/1&quot;&gt;Rachana Sathish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Poudel_P/0/1/0/all/0/1&quot;&gt;Pranav Poudel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhattarai_B/0/1/0/all/0/1&quot;&gt;Binod Bhattarai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rui_G/0/1/0/all/0/1&quot;&gt;Guo Rui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schellenberg_M/0/1/0/all/0/1&quot;&gt;Melanie Schellenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vilaca_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o L. Vila&amp;#xe7;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Czempiel_T/0/1/0/all/0/1&quot;&gt;Tobias Czempiel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenkun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sheet_D/0/1/0/all/0/1&quot;&gt;Debdoot Sheet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thapa_S/0/1/0/all/0/1&quot;&gt;Shrawan Kumar Thapa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Berniker_M/0/1/0/all/0/1&quot;&gt;Max Berniker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Godau_P/0/1/0/all/0/1&quot;&gt;Patrick Godau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Morais_P/0/1/0/all/0/1&quot;&gt;Pedro Morais&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Regmi_S/0/1/0/all/0/1&quot;&gt;Sudarshan Regmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Thuy Nuong Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fonseca_J/0/1/0/all/0/1&quot;&gt;Jaime Fonseca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nolke_J/0/1/0/all/0/1&quot;&gt;Jan-Hinrich N&amp;#xf6;lke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lima_E/0/1/0/all/0/1&quot;&gt;Estev&amp;#xe3;o Lima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vazquez_E/0/1/0/all/0/1&quot;&gt;Eduard Vazquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maier_Hein_L/0/1/0/all/0/1&quot;&gt;Lena Maier-Hein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mascagni_P/0/1/0/all/0/1&quot;&gt;Pietro Mascagni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Seeliger_B/0/1/0/all/0/1&quot;&gt;Barbara Seeliger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1&quot;&gt;Cristians Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mutter_D/0/1/0/all/0/1&quot;&gt;Didier Mutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Padoy_N/0/1/0/all/0/1&quot;&gt;Nicolas Padoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06441">
<title>ContrasInver: Ultra-Sparse Label Semi-supervised Regression for Multi-dimensional Seismic Inversion. (arXiv:2302.06441v3 [physics.geo-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06441</link>
<description rdf:parseType="Literal">&lt;p&gt;The automated interpretation and inversion of seismic data have advanced
significantly with the development of Deep Learning (DL) methods. However,
these methods often require numerous costly well logs, limiting their
application only to mature or synthetic data. This paper presents ContrasInver,
a method that achieves seismic inversion using as few as two or three well
logs, significantly reducing current requirements. In ContrasInver, we propose
three key innovations to address the challenges of applying semi-supervised
learning to regression tasks with ultra-sparse labels. The Multi-dimensional
Sample Generation (MSG) technique pioneers a paradigm for sample generation in
multi-dimensional inversion. It produces a large number of diverse samples from
a single well, while establishing lateral continuity in seismic data. MSG
yields substantial improvements over current techniques, even without the use
of semi-supervised learning. The Region-Growing Training (RGT) strategy
leverages the inherent continuity of seismic data, effectively propagating
accuracy from closer to more distant regions based on the proximity of well
logs. The Impedance Vectorization Projection (IVP) vectorizes impedance values
and performs semi-supervised learning in a compressed space. We demonstrated
that the Jacobian matrix derived from this space can filter out some outlier
components in pseudo-label vectors, thereby solving the value confusion issue
in semi-supervised regression learning. In the experiments, ContrasInver
achieved state-of-the-art performance in the synthetic data SEAM I. In the
field data with two or three well logs, only the methods based on the
components proposed in this paper were able to achieve reasonable results. It&apos;s
the first data-driven approach yielding reliable results on the Netherlands F3
and Delft, using only three and two well logs respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Dou_Y/0/1/0/all/0/1&quot;&gt;Yimin Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kewen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lv_W/0/1/0/all/0/1&quot;&gt;Wenjun Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Timing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Duan_H/0/1/0/all/0/1&quot;&gt;Hongjie Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06494">
<title>Explicit3D: Graph Network with Spatial Inference for Single Image 3D Object Detection. (arXiv:2302.06494v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06494</link>
<description rdf:parseType="Literal">&lt;p&gt;Indoor 3D object detection is an essential task in single image scene
understanding, impacting spatial cognition fundamentally in visual reasoning.
Existing works on 3D object detection from a single image either pursue this
goal through independent predictions of each object or implicitly reason over
all possible objects, failing to harness relational geometric information
between objects. To address this problem, we propose a dynamic sparse graph
pipeline named Explicit3D based on object geometry and semantics features.
Taking the efficiency into consideration, we further define a relatedness score
and design a novel dynamic pruning algorithm followed by a cluster sampling
method for sparse scene graph generation and updating. Furthermore, our
Explicit3D introduces homogeneous matrices and defines new relative loss and
corner loss to model the spatial difference between target pairs explicitly.
Instead of using ground-truth labels as direct supervision, our relative and
corner loss are derived from the homogeneous transformation, which renders the
model to learn the geometric consistency between objects. The experimental
results on the SUN RGB-D dataset demonstrate that our Explicit3D achieves
better performance balance than the-state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yanjun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenming Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06608">
<title>3D-aware Blending with Generative NeRFs. (arXiv:2302.06608v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06608</link>
<description rdf:parseType="Literal">&lt;p&gt;Image blending aims to combine multiple images seamlessly. It remains
challenging for existing 2D-based methods, especially when input images are
misaligned due to differences in 3D camera poses and object shapes. To tackle
these issues, we propose a 3D-aware blending method using generative Neural
Radiance Fields (NeRF), including two key components: 3D-aware alignment and
3D-aware blending. For 3D-aware alignment, we first estimate the camera pose of
the reference image with respect to generative NeRFs and then perform 3D local
alignment for each part. To further leverage 3D information of the generative
NeRF, we propose 3D-aware blending that directly blends images on the NeRF&apos;s
latent representation space, rather than raw pixel space. Collectively, our
method outperforms existing 2D baselines, as validated by extensive
quantitative and qualitative evaluations with FFHQ and AFHQ-Cat.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyunsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gayoung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yunjey Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jin-Hwa Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun-Yan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.09765">
<title>ENInst: Enhancing Weakly-supervised Low-shot Instance Segmentation. (arXiv:2302.09765v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.09765</link>
<description rdf:parseType="Literal">&lt;p&gt;We address a weakly-supervised low-shot instance segmentation, an
annotation-efficient training method to deal with novel classes effectively.
Since it is an under-explored problem, we first investigate the difficulty of
the problem and identify the performance bottleneck by conducting systematic
analyses of model components and individual sub-tasks with a simple baseline
model. Based on the analyses, we propose ENInst with sub-task enhancement
methods: instance-wise mask refinement for enhancing pixel localization quality
and novel classifier composition for improving classification accuracy. Our
proposed method lifts the overall performance by enhancing the performance of
each sub-task. We demonstrate that our ENInst is 7.5 times more efficient in
achieving comparable performance to the existing fully-supervised few-shot
models and even outperforms them at times.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Bin_M/0/1/0/all/0/1&quot;&gt;Moon Ye-Bin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1&quot;&gt;Dongmin Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1&quot;&gt;Yongjin Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junsik Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1&quot;&gt;Tae-Hyun Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10893">
<title>Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness. (arXiv:2302.10893v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10893</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative AI models have recently achieved astonishing results in quality
and are consequently employed in a fast-growing number of applications.
However, since they are highly data-driven, relying on billion-sized datasets
randomly scraped from the internet, they also suffer from degenerated and
biased human behavior, as we demonstrate. In fact, they may even reinforce such
biases. To not only uncover but also combat these undesired effects, we present
a novel strategy, called Fair Diffusion, to attenuate biases after the
deployment of generative text-to-image models. Specifically, we demonstrate
shifting a bias, based on human instructions, in any direction yielding
arbitrarily new proportions for, e.g., identity groups. As our empirical
evaluation demonstrates, this introduced control enables instructing generative
image models on fairness, with no data filtering and additional training
required.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1&quot;&gt;Felix Friedrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brack_M/0/1/0/all/0/1&quot;&gt;Manuel Brack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1&quot;&gt;Lukas Struppek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1&quot;&gt;Dominik Hintersdorf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1&quot;&gt;Patrick Schramowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luccioni_S/0/1/0/all/0/1&quot;&gt;Sasha Luccioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.14267">
<title>Adversarial Attack with Raindrops. (arXiv:2302.14267v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.14267</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) are known to be vulnerable to adversarial
examples, which are usually designed artificially to fool DNNs, but rarely
exist in real-world scenarios. In this paper, we study the adversarial examples
caused by raindrops, to demonstrate that there exist plenty of natural
phenomena being able to work as adversarial attackers to DNNs. Moreover, we
present a new approach to generate adversarial raindrops, denoted as AdvRD,
using the generative adversarial network (GAN) technique to simulate natural
raindrops. The images crafted by our AdvRD look very similar to the real-world
raindrop images, statistically close to the distribution of true raindrop
images, and more importantly, can perform strong adversarial attack to the
state-of-the-art DNN models. On the other side, we show that the adversarial
training using our AdvRD images can significantly improve the robustness of
DNNs to the real-world raindrop attacks. Extensive experiments are carried out
to demonstrate that the images crafted by AdvRD are visually and statistically
close to the natural raindrop images, can work as strong attackers to DNN
models, and also help improve the robustness of DNNs to raindrop attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1&quot;&gt;Bingyi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_M/0/1/0/all/0/1&quot;&gt;Mingkang Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Huilin Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.02867">
<title>Dual Feedback Attention Framework via Boundary-Aware Auxiliary and Progressive Semantic Optimization for Salient Object Detection in Optical Remote Sensing Imagery. (arXiv:2303.02867v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.02867</link>
<description rdf:parseType="Literal">&lt;p&gt;Salient object detection in optical remote sensing image (ORSI-SOD) has
gradually attracted attention thanks to the development of deep learning (DL)
and salient object detection in natural scene image (NSI-SOD). However, NSI and
ORSI are different in many aspects, such as large coverage, complex background,
and large differences in target types and scales. Therefore, a new dedicated
method is needed for ORSI-SOD. In addition, existing methods do not pay
sufficient attention to the boundary of the object, and the completeness of the
final saliency map still needs improvement. To address these issues, we propose
a novel method called Dual Feedback Attention Framework via Boundary-Aware
Auxiliary and Progressive Semantic Optimization (DFA-BASO). First, Boundary
Protection Calibration (BPC) module is proposed to reduce the loss of edge
position information during forward propagation and suppress noise in low-level
features. Second, a Dual Feature Feedback Complementary (DFFC) module is
proposed based on BPC module. It aggregates boundary-semantic dual features and
provides effective feedback to coordinate features across different layers.
Finally, a Strong Semantic Feedback Refinement (SSFR) module is proposed to
obtain more complete saliency maps. This module further refines feature
representation and eliminates feature differences through a unique feedback
mechanism. Extensive experiments on two public datasets show that DFA-BASO
outperforms 15 state-of-the-art methods. Furthermore, this paper strongly
demonstrates the true contribution of DFA-BASO to ORSI-SOD by in-depth analysis
of the visualization figure. All codes can be found at
https://github.com/YUHsss/DFA-BASO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1&quot;&gt;Dejun Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hongyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Suning Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xingyu Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1&quot;&gt;Ziyang Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yakun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03323">
<title>CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning. (arXiv:2303.03323v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03323</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal contrastive pretraining has been used to train multimodal
representation models, such as CLIP, on large amounts of paired image-text
data. However, previous studies have revealed that such models are vulnerable
to backdoor attacks. Specifically, when trained on backdoored examples, CLIP
learns spurious correlations between the embedded backdoor trigger and the
target label, aligning their representations in the joint embedding space.
Injecting even a small number of poisoned examples, such as 75 examples in 3
million pretraining data, can significantly manipulate the model&apos;s behavior,
making it difficult to detect or unlearn such correlations. To address this
issue, we propose CleanCLIP, a finetuning framework that weakens the learned
spurious associations introduced by backdoor attacks by independently
re-aligning the representations for individual modalities. We demonstrate that
unsupervised finetuning using a combination of multimodal contrastive and
unimodal self-supervised objectives for individual modalities can significantly
reduce the impact of the backdoor attack. Additionally, we show that supervised
finetuning on task-specific labeled image data removes the backdoor trigger
from the CLIP vision encoder. We show empirically that CleanCLIP maintains
model performance on benign examples while erasing a range of backdoor attacks
on multimodal contrastive learning. The code and checkpoints are available at
https://github.com/nishadsinghi/CleanCLIP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1&quot;&gt;Hritik Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singhi_N/0/1/0/all/0/1&quot;&gt;Nishad Singhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1&quot;&gt;Fan Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1&quot;&gt;Aditya Grover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.04654">
<title>Aberration-Aware Depth-from-Focus. (arXiv:2303.04654v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.04654</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer vision methods for depth estimation usually use simple camera models
with idealized optics. For modern machine learning approaches, this creates an
issue when attempting to train deep networks with simulated data, especially
for focus-sensitive tasks like Depth-from-Focus. In this work, we investigate
the domain gap caused by off-axis aberrations that will affect the decision of
the best-focused frame in a focal stack. We then explore bridging this domain
gap through aberration-aware training (AAT). Our approach involves a
lightweight network that models lens aberrations at different positions and
focus distances, which is then integrated into the conventional network
training pipeline. We evaluate the generality of pretrained models on both
synthetic and real-world data. Our experimental results demonstrate that the
proposed AAT scheme can improve depth estimation accuracy without fine-tuning
the model or modifying the network architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xinge Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1&quot;&gt;Qiang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1&quot;&gt;Mohammed Elhoseiny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heidrich_W/0/1/0/all/0/1&quot;&gt;Wolfgang Heidrich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07679">
<title>Feature representations useful for predicting image memorability. (arXiv:2303.07679v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07679</link>
<description rdf:parseType="Literal">&lt;p&gt;Prediction of image memorability has attracted interest in various fields.
Consequently, the prediction accuracy of convolutional neural network (CNN)
models has been approaching the empirical upper bound estimated based on human
consistency. However, identifying which feature representations embedded in CNN
models are responsible for the high memorability prediction accuracy remains an
open question. To tackle this problem, we sought to identify
memorability-related feature representations in CNN models using brain
similarity. Specifically, memorability prediction accuracy and brain similarity
were examined across 16,860 layers in 64 CNN models pretrained for object
recognition. A clear tendency was observed in this comprehensive analysis that
layers with high memorability prediction accuracy had higher brain similarity
with the inferior temporal (IT) cortex, which is the highest stage in the
ventral visual pathway. Furthermore, fine-tuning of the 64 CNN models for
memorability prediction revealed that brain similarity with the IT cortex at
the penultimate layer positively correlated with the memorability prediction
accuracy of the models. This analysis also showed that the best fine-tuned
model provided accuracy comparable to state-of-the-art CNN models developed for
memorability prediction. Overall, the results of this study indicated that the
CNN models&apos; great success in predicting memorability relies on feature
representation acquisition, similar to the IT cortex. This study advances our
understanding of feature representations and their use in predicting image
memorability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1&quot;&gt;Takumi Harada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakai_H/0/1/0/all/0/1&quot;&gt;Hiroyuki Sakai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09181">
<title>Global Knowledge Calibration for Fast Open-Vocabulary Segmentation. (arXiv:2303.09181v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09181</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in pre-trained vision-language models, such as CLIP, have
enabled the segmentation of arbitrary concepts solely from textual inputs, a
process commonly referred to as open-vocabulary semantic segmentation (OVS).
However, existing OVS techniques confront a fundamental challenge: the trained
classifier tends to overfit on the base classes observed during training,
resulting in suboptimal generalization performance to unseen classes. To
mitigate this issue, recent studies have proposed the use of an additional
frozen pre-trained CLIP for classification. Nonetheless, this approach incurs
heavy computational overheads as the CLIP vision encoder must be repeatedly
forward-passed for each mask, rendering it impractical for real-world
applications. To address this challenge, our objective is to develop a fast OVS
model that can perform comparably or better without the extra computational
burden of the CLIP image encoder during inference. To this end, we propose a
core idea of preserving the generalizable representation when fine-tuning on
known classes. Specifically, we introduce a text diversification strategy that
generates a set of synonyms for each training category, which prevents the
learned representation from collapsing onto specific known category names.
Additionally, we employ a text-guided knowledge distillation method to preserve
the generalizable knowledge of CLIP. Extensive experiments demonstrate that our
proposed model achieves robust generalization performance across various
datasets. Furthermore, we perform a preliminary exploration of open-vocabulary
video segmentation and present a benchmark that can facilitate future
open-vocabulary research in the video domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1&quot;&gt;Kunyang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liew_J/0/1/0/all/0/1&quot;&gt;Jun Hao Liew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Henghui Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yunchao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiajun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yitong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yansong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yujiu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09508">
<title>LDMVFI: Video Frame Interpolation with Latent Diffusion Models. (arXiv:2303.09508v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09508</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing works on video frame interpolation (VFI) mostly employ deep neural
networks trained to minimize the L1 or L2 distance between their outputs and
ground-truth frames. Despite recent advances, existing VFI methods tend to
produce perceptually inferior results, particularly for challenging scenarios
including large motions and dynamic textures. Towards developing
perceptually-oriented VFI methods, we propose latent diffusion model-based VFI,
LDMVFI. This approaches the VFI problem from a generative perspective by
formulating it as a conditional generation problem. As the first effort to
address VFI using latent diffusion models, we rigorously benchmark our method
following the common evaluation protocol adopted in the existing VFI
literature. Our quantitative experiments and user study indicate that LDMVFI is
able to interpolate video content with superior perceptual quality compared to
the state of the art, even in the high-resolution regime. Our source code will
be made available here.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Danier_D/0/1/0/all/0/1&quot;&gt;Duolikun Danier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bull_D/0/1/0/all/0/1&quot;&gt;David Bull&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09522">
<title>P+: Extended Textual Conditioning in Text-to-Image Generation. (arXiv:2303.09522v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09522</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce an Extended Textual Conditioning space in text-to-image models,
referred to as $P+$. This space consists of multiple textual conditions,
derived from per-layer prompts, each corresponding to a layer of the denoising
U-net of the diffusion model.
&lt;/p&gt;
&lt;p&gt;We show that the extended space provides greater disentangling and control
over image synthesis. We further introduce Extended Textual Inversion (XTI),
where the images are inverted into $P+$, and represented by per-layer tokens.
&lt;/p&gt;
&lt;p&gt;We show that XTI is more expressive and precise, and converges faster than
the original Textual Inversion (TI) space. The extended inversion method does
not involve any noticeable trade-off between reconstruction and editability and
induces more regular inversions.
&lt;/p&gt;
&lt;p&gt;We conduct a series of extensive experiments to analyze and understand the
properties of the new space, and to showcase the effectiveness of our method
for personalizing text-to-image models. Furthermore, we utilize the unique
properties of this space to achieve previously unattainable results in
object-style mixing using text-to-image models. Project page:
https://prompt-plus.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1&quot;&gt;Andrey Voynov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1&quot;&gt;Qinghao Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1&quot;&gt;Daniel Cohen-Or&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1&quot;&gt;Kfir Aberman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09706">
<title>Unsupervised Self-Driving Attention Prediction via Uncertainty Mining and Knowledge Embedding. (arXiv:2303.09706v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09706</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting attention regions of interest is an important yet challenging task
for self-driving systems. Existing methodologies rely on large-scale labeled
traffic datasets that are labor-intensive to obtain. Besides, the huge domain
gap between natural scenes and traffic scenes in current datasets also limits
the potential for model training. To address these challenges, we are the first
to introduce an unsupervised way to predict self-driving attention by
uncertainty modeling and driving knowledge integration. Our approach&apos;s
Uncertainty Mining Branch (UMB) discovers commonalities and differences from
multiple generated pseudo-labels achieved from models pre-trained on natural
scenes by actively measuring the uncertainty. Meanwhile, our Knowledge
Embedding Block (KEB) bridges the domain gap by incorporating driving knowledge
to adaptively refine the generated pseudo-labels. Quantitative and qualitative
results with equivalent or even more impressive performance compared to
fully-supervised state-of-the-art approaches across all three public datasets
demonstrate the effectiveness of the proposed method and the potential of this
direction. The code will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1&quot;&gt;Pengfei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1&quot;&gt;Mengshi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weijian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Huadong Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10173">
<title>VideoSum: A Python Library for Surgical Video Summarization. (arXiv:2303.10173v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10173</link>
<description rdf:parseType="Literal">&lt;p&gt;The performance of deep learning (DL) algorithms is heavily influenced by the
quantity and the quality of the annotated data. However, in Surgical Data
Science, access to it is limited. It is thus unsurprising that substantial
research efforts are made to develop methods aiming at mitigating the scarcity
of annotated SDS data. In parallel, an increasing number of Computer Assisted
Interventions (CAI) datasets are being released, although the scale of these
remain limited. On these premises, data curation is becoming a key element of
many SDS research endeavors. Surgical video datasets are demanding to curate
and would benefit from dedicated support tools. In this work, we propose to
summarize surgical videos into storyboards or collages of representative frames
to ease visualization, annotation, and processing. Video summarization is
well-established for natural images. However, state-of-the-art methods
typically rely on models trained on human-made annotations, few methods have
been evaluated on surgical videos, and the availability of software packages
for the task is limited. We present videosum, an easy-to-use and open-source
Python library to generate storyboards from surgical videos that contains a
variety of unsupervised methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Garcia_Peraza_Herrera_L/0/1/0/all/0/1&quot;&gt;Luis C. Garcia-Peraza-Herrera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1&quot;&gt;Sebastien Ourselin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1&quot;&gt;Tom Vercauteren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10840">
<title>Ref-NeuS: Ambiguity-Reduced Neural Implicit Surface Learning for Multi-View Reconstruction with Reflection. (arXiv:2303.10840v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10840</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural implicit surface learning has shown significant progress in multi-view
3D reconstruction, where an object is represented by multilayer perceptrons
that provide continuous implicit surface representation and view-dependent
radiance. However, current methods often fail to accurately reconstruct
reflective surfaces, leading to severe ambiguity. To overcome this issue, we
propose Ref-NeuS, which aims to reduce ambiguity by attenuating the effect of
reflective surfaces. Specifically, we utilize an anomaly detector to estimate
an explicit reflection score with the guidance of multi-view context to
localize reflective surfaces. Afterward, we design a reflection-aware
photometric loss that adaptively reduces ambiguity by modeling rendered color
as a Gaussian distribution, with the reflection score representing the
variance. We show that together with a reflection direction-dependent radiance,
our model achieves high-quality surface reconstruction on reflective surfaces
and outperforms the state-of-the-arts by a large margin. Besides, our model is
also comparable on general surfaces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1&quot;&gt;Wenhang Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1&quot;&gt;Tao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haoyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Ying-Cong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11165">
<title>Computationally Budgeted Continual Learning: What Does Matter?. (arXiv:2303.11165v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11165</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual Learning (CL) aims to sequentially train models on streams of
incoming data that vary in distribution by preserving previous knowledge while
adapting to new data. Current CL literature focuses on restricted access to
previously seen data, while imposing no constraints on the computational budget
for training. This is unreasonable for applications in-the-wild, where systems
are primarily constrained by computational and time budgets, not storage. We
revisit this problem with a large-scale benchmark and analyze the performance
of traditional CL approaches in a compute-constrained setting, where effective
memory samples used in training can be implicitly restricted as a consequence
of limited computation. We conduct experiments evaluating various CL sampling
strategies, distillation losses, and partial fine-tuning on two large-scale
datasets, namely ImageNet2K and Continual Google Landmarks V2 in data
incremental, class incremental, and time incremental settings. Through
extensive experiments amounting to a total of over 1500 GPU-hours, we find
that, under compute-constrained setting, traditional CL approaches, with no
exception, fail to outperform a simple minimal baseline that samples uniformly
from memory. Our conclusions are consistent in a different number of stream
time steps, e.g., 20 to 200, and under several computational budgets. This
suggests that most existing CL methods are particularly too computationally
expensive for realistic budgeted deployment. Code for this project is available
at: https://github.com/drimpossible/BudgetCL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhu_A/0/1/0/all/0/1&quot;&gt;Ameya Prabhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hammoud_H/0/1/0/all/0/1&quot;&gt;Hasan Abed Al Kader Hammoud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1&quot;&gt;Puneet Dokania&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Ser-Nam Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1&quot;&gt;Bernard Ghanem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1&quot;&gt;Adel Bibi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11228">
<title>Bimodal SegNet: Instance Segmentation Fusing Events and RGB Frames for Robotic Grasping. (arXiv:2303.11228v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11228</link>
<description rdf:parseType="Literal">&lt;p&gt;Object segmentation for robotic grasping under dynamic conditions often faces
challenges such as occlusion, low light conditions, motion blur and object size
variance. To address these challenges, we propose a Deep Learning network that
fuses two types of visual signals, event-based data and RGB frame data. The
proposed Bimodal SegNet network has two distinct encoders, one for each signal
input and a spatial pyramidal pooling with atrous convolutions. Encoders
capture rich contextual information by pooling the concatenated features at
different resolutions while the decoder obtains sharp object boundaries. The
evaluation of the proposed method undertakes five unique image degradation
challenges including occlusion, blur, brightness, trajectory and scale variance
on the Event-based Segmentation (ESD) Dataset. The evaluation results show a
6-10\% segmentation accuracy improvement over state-of-the-art methods in terms
of mean intersection over the union and pixel accuracy. The model code is
available at https://github.com/sanket0707/Bimodal-SegNet.git
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kachole_S/0/1/0/all/0/1&quot;&gt;Sanket Kachole&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaoqian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naeini_F/0/1/0/all/0/1&quot;&gt;Fariborz Baghaei Naeini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muthusamy_R/0/1/0/all/0/1&quot;&gt;Rajkumar Muthusamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makris_D/0/1/0/all/0/1&quot;&gt;Dimitrios Makris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zweiri_Y/0/1/0/all/0/1&quot;&gt;Yahya Zweiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11324">
<title>Open-vocabulary Panoptic Segmentation with Embedding Modulation. (arXiv:2303.11324v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11324</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-vocabulary image segmentation is attracting increasing attention due to
its critical applications in the real world. Traditional closed-vocabulary
segmentation methods are not able to characterize novel objects, whereas
several recent open-vocabulary attempts obtain unsatisfactory results, i.e.,
notable performance reduction on the closed vocabulary and massive demand for
extra data. To this end, we propose OPSNet, an omnipotent and data-efficient
framework for Open-vocabulary Panoptic Segmentation. Specifically, the
exquisitely designed Embedding Modulation module, together with several
meticulous components, enables adequate embedding enhancement and information
exchange between the segmentation model and the visual-linguistic well-aligned
CLIP encoder, resulting in superior segmentation performance under both open-
and closed-vocabulary settings with much fewer need of additional data.
Extensive experimental evaluations are conducted across multiple datasets
(e.g., COCO, ADE20K, Cityscapes, and PascalContext) under various
circumstances, where the proposed OPSNet achieves state-of-the-art results,
which demonstrates the effectiveness and generality of the proposed approach.
The code and trained models will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Ser-Nam Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1&quot;&gt;Antonio Torralba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hengshuang Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13005">
<title>From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels. (arXiv:2303.13005v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13005</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge Distillation (KD) uses the teacher&apos;s prediction logits as soft
labels to guide the student, while self-KD does not need a real teacher to
require the soft labels. This work unifies the formulations of the two tasks by
decomposing and reorganizing the generic KD loss into a Normalized KD (NKD)
loss and customized soft labels for both target class (image&apos;s category) and
non-target classes named Universal Self-Knowledge Distillation (USKD). We
decompose the KD loss and find the non-target loss from it forces the student&apos;s
non-target logits to match the teacher&apos;s, but the sum of the two non-target
logits is different, preventing them from being identical. NKD normalizes the
non-target logits to equalize their sum. It can be generally used for KD and
self-KD to better use the soft labels for distillation loss. USKD generates
customized soft labels for both target and non-target classes without a
teacher. It smooths the target logit of the student as the soft target label
and uses the rank of the intermediate feature to generate the soft non-target
labels with Zipf&apos;s law. For KD with teachers, our NKD achieves state-of-the-art
performance on CIFAR-100 and ImageNet datasets, boosting the ImageNet Top-1
accuracy of ResNet18 from 69.90% to 71.96% with a ResNet-34 teacher. For
self-KD without teachers, USKD is the first self-KD method that can be
effectively applied to both CNN and ViT models with negligible additional time
and memory cost, resulting in new state-of-the-art results, such as 1.17% and
0.55% accuracy gains on ImageNet for MobileNet and DeiT-Tiny, respectively. Our
codes are available at https://github.com/yzd-v/cls_KD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhendong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Ailing Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianke Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1&quot;&gt;Chun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.01979">
<title>Glitch in the Matrix: A Large Scale Benchmark for Content Driven Audio-Visual Forgery Detection and Localization. (arXiv:2305.01979v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.01979</link>
<description rdf:parseType="Literal">&lt;p&gt;Most deepfake detection methods focus on detecting spatial and/or
spatio-temporal changes in facial attributes and are centered around the binary
classification task of detecting whether a video is real or fake. This is
because available benchmark datasets contain mostly visual-only modifications
present in the entirety of the video. However, a sophisticated deepfake may
include small segments of audio or audio-visual manipulations that can
completely change the meaning of the video content. To addresses this gap, we
propose and benchmark a new dataset, Localized Audio Visual DeepFake (LAV-DF),
consisting of strategic content-driven audio, visual and audio-visual
manipulations. The proposed baseline method, Boundary Aware Temporal Forgery
Detection (BA-TFD), is a 3D Convolutional Neural Network-based architecture
which effectively captures multimodal manipulations. We further improve (i.e.
BA-TFD+) the baseline method by replacing the backbone with a Multiscale Vision
Transformer and guide the training process with contrastive, frame
classification, boundary matching and multimodal boundary matching loss
functions. The quantitative analysis demonstrates the superiority of BA-TFD+ on
temporal forgery localization and deepfake detection tasks using several
benchmark datasets including our newly proposed dataset. The dataset, models
and code are available at https://github.com/ControlNet/LAV-DF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zhixi Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Shreya Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhall_A/0/1/0/all/0/1&quot;&gt;Abhinav Dhall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1&quot;&gt;Tom Gedeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stefanov_K/0/1/0/all/0/1&quot;&gt;Kalin Stefanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1&quot;&gt;Munawar Hayat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04195">
<title>Cross-Modal Retrieval for Motion and Text via MildTriple Loss. (arXiv:2305.04195v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04195</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-modal retrieval has become a prominent research topic in computer
vision and natural language processing with advances made in image-text and
video-text retrieval technologies. However, cross-modal retrieval between human
motion sequences and text has not garnered sufficient attention despite the
extensive application value it holds, such as aiding virtual reality
applications in better understanding users&apos; actions and language. This task
presents several challenges, including joint modeling of the two modalities,
demanding the understanding of person-centered information from text, and
learning behavior features from 3D human motion sequences. Previous work on
motion data modeling mainly relied on autoregressive feature extractors that
may forget previous information, while we propose an innovative model that
includes simple yet powerful transformer-based motion and text encoders, which
can learn representations from the two different modalities and capture
long-term dependencies. Furthermore, the overlap of the same atomic actions of
different human motions can cause semantic conflicts, leading us to explore a
new triplet loss function, MildTriple Loss. it leverages the similarity between
samples in intra-modal space to guide soft-hard negative sample mining in the
joint embedding space to train the triplet loss and reduce the violation caused
by false negative samples. We evaluated our model and method on the latest
HumanML3D and KIT Motion-Language datasets, achieving a 62.9\% recall for
motion retrieval and a 71.5\% recall for text retrieval (based on R@10) on the
HumanML3D dataset. Our code is available at
https://github.com/eanson023/rehamot.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Sheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xin Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mengyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04628">
<title>Target-driven One-Shot Unsupervised Domain Adaptation. (arXiv:2305.04628v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04628</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a novel framework for the challenging problem of
One-Shot Unsupervised Domain Adaptation (OSUDA), which aims to adapt to a
target domain with only a single unlabeled target sample. Unlike existing
approaches that rely on large labeled source and unlabeled target data, our
Target-driven One-Shot UDA (TOS-UDA) approach employs a learnable augmentation
strategy guided by the target sample&apos;s style to align the source distribution
with the target distribution. Our method consists of three modules: an
augmentation module, a style alignment module, and a classifier. Unlike
existing methods, our augmentation module allows for strong transformations of
the source samples, and the style of the single target sample available is
exploited to guide the augmentation by ensuring perceptual similarity.
Furthermore, our approach integrates augmentation with style alignment,
eliminating the need for separate pre-training on additional datasets. Our
method outperforms or performs comparably to existing OS-UDA methods on the
Digits and DomainNet benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carrazco_J/0/1/0/all/0/1&quot;&gt;Julio Ivan Davila Carrazco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadam_S/0/1/0/all/0/1&quot;&gt;Suvarna Kishorkumar Kadam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morerio_P/0/1/0/all/0/1&quot;&gt;Pietro Morerio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bue_A/0/1/0/all/0/1&quot;&gt;Alessio Del Bue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murino_V/0/1/0/all/0/1&quot;&gt;Vittorio Murino&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12032">
<title>The Waymo Open Sim Agents Challenge. (arXiv:2305.12032v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12032</link>
<description rdf:parseType="Literal">&lt;p&gt;Simulation with realistic, interactive agents represents a key task for
autonomous vehicle software development. In this work, we introduce the Waymo
Open Sim Agents Challenge (WOSAC). WOSAC is the first public challenge to
tackle this task and propose corresponding metrics. The goal of the challenge
is to stimulate the design of realistic simulators that can be used to evaluate
and train a behavior model for autonomous driving. We outline our evaluation
methodology, present results for a number of different baseline simulation
agent methods, and analyze several submissions to the 2023 competition which
ran from March 16, 2023 to May 23, 2023. The WOSAC evaluation server remains
open for submissions and we discuss open problems for the task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montali_N/0/1/0/all/0/1&quot;&gt;Nico Montali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lambert_J/0/1/0/all/0/1&quot;&gt;John Lambert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mougin_P/0/1/0/all/0/1&quot;&gt;Paul Mougin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuefler_A/0/1/0/all/0/1&quot;&gt;Alex Kuefler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rhinehart_N/0/1/0/all/0/1&quot;&gt;Nick Rhinehart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Michelle Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulino_C/0/1/0/all/0/1&quot;&gt;Cole Gulino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emrich_T/0/1/0/all/0/1&quot;&gt;Tristan Emrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zoey Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1&quot;&gt;Shimon Whiteson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+White_B/0/1/0/all/0/1&quot;&gt;Brandyn White&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1&quot;&gt;Dragomir Anguelov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14301">
<title>A Laplacian Pyramid Based Generative H&amp;E Stain Augmentation Network. (arXiv:2305.14301v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14301</link>
<description rdf:parseType="Literal">&lt;p&gt;Hematoxylin and Eosin (H&amp;amp;E) staining is a widely used sample preparation
procedure for enhancing the saturation of tissue sections and the contrast
between nuclei and cytoplasm in histology images for medical diagnostics.
However, various factors, such as the differences in the reagents used, result
in high variability in the colors of the stains actually recorded. This
variability poses a challenge in achieving generalization for machine-learning
based computer-aided diagnostic tools. To desensitize the learned models to
stain variations, we propose the Generative Stain Augmentation Network (G-SAN)
-- a GAN-based framework that augments a collection of cell images with
simulated yet realistic stain variations. At its core, G-SAN uses a novel and
highly computationally efficient Laplacian Pyramid (LP) based generator
architecture, that is capable of disentangling stain from cell morphology.
Through the task of patch classification and nucleus segmentation, we show that
using G-SAN-augmented training data provides on average 15.7% improvement in F1
score and 7.3% improvement in panoptic quality, respectively. Our code is
available at https://github.com/lifangda01/GSAN-Demo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fangda Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kak_A/0/1/0/all/0/1&quot;&gt;Avinash Kak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14758">
<title>MRN: Multiplexed Routing Network for Incremental Multilingual Text Recognition. (arXiv:2305.14758v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14758</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional Multilingual Text Recognition (MLTR) usually targets a fixed set
of languages and thus struggles to handle newly added languages or adapt to
ever-changing class distributions. In this paper, we introduce the Incremental
Multilingual Text Recognition (IMLTR) task in the incremental learning setting,
where new language data comes in batches. Compared to generic incremental
learning, IMLTR is even more challenging as it suffers from rehearsal-imbalance
(uneven distribution of sample characters in the rehearsal set). To address
this issue, we propose a Multiplexed Routing Network (MRN), where a series of
recognizers is trained for each language. Subsequently, a language predictor is
adopted to weigh the recognizers for voting. Since the recognizers are derived
from the original model, MRN effectively reduces the reliance on older data and
is better suited for rehearsal-imbalance. We extensively evaluate MRN on MLT17
and MLT19 datasets, outperforming existing state-of-the-art methods by a large
margin, i.e., accuracy improvement ranging from 10.3% to 27.4% under different
settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1&quot;&gt;Tianlun Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhineng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;BingChen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Gang Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18022">
<title>HGT: A Hierarchical GCN-Based Transformer for Multimodal Periprosthetic Joint Infection Diagnosis Using CT Images and Text. (arXiv:2305.18022v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18022</link>
<description rdf:parseType="Literal">&lt;p&gt;Prosthetic Joint Infection (PJI) is a prevalent and severe complication
characterized by high diagnostic challenges. Currently, a unified diagnostic
standard incorporating both computed tomography (CT) images and numerical text
data for PJI remains unestablished, owing to the substantial noise in CT images
and the disparity in data volume between CT images and text data. This study
introduces a diagnostic method, HGT, based on deep learning and multimodal
techniques. It effectively merges features from CT scan images and patients&apos;
numerical text data via a Unidirectional Selective Attention (USA) mechanism
and a graph convolutional network (GCN)-based feature fusion network. We
evaluated the proposed method on a custom-built multimodal PJI dataset,
assessing its performance through ablation experiments and interpretability
evaluations. Our method achieved an accuracy (ACC) of 91.4\% and an area under
the curve (AUC) of 95.9\%, outperforming recent multimodal approaches by 2.9\%
in ACC and 2.2\% in AUC, with a parameter count of only 68M. Notably, the
interpretability results highlighted our model&apos;s strong focus and localization
capabilities at lesion sites. This proposed method could provide clinicians
with additional diagnostic tools to enhance accuracy and efficiency in clinical
practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruiyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fujun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Hongwei Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18729">
<title>Real-World Image Variation by Aligning Diffusion Inversion Chain. (arXiv:2305.18729v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18729</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent diffusion model advancements have enabled high-fidelity images to be
generated using text prompts. However, a domain gap exists between generated
images and real-world images, which poses a challenge in generating
high-quality variations of real-world images. Our investigation uncovers that
this domain gap originates from a latents&apos; distribution gap in different
diffusion processes. To address this issue, we propose a novel inference
pipeline called Real-world Image Variation by ALignment (RIVAL) that utilizes
diffusion models to generate image variations from a single image exemplar. Our
pipeline enhances the generation quality of image variations by aligning the
image generation process to the source image&apos;s inversion chain. Specifically,
we demonstrate that step-wise latent distribution alignment is essential for
generating high-quality variations. To attain this, we design a cross-image
self-attention injection for feature interaction and a step-wise distribution
normalization to align the latent features. Incorporating these alignment
processes into a diffusion model allows RIVAL to generate high-quality image
variations without further parameter optimization. Our experimental results
demonstrate that our proposed approach outperforms existing methods with
respect to semantic-condition similarity and perceptual quality. Furthermore,
this generalized inference pipeline can be easily applied to other
diffusion-based generation tasks, such as image-conditioned text-to-image
generation and example-based image inpainting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuechen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Jinbo Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_E/0/1/0/all/0/1&quot;&gt;Eric Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jiaya Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01359">
<title>DWT-CompCNN: Deep Image Classification Network for High Throughput JPEG 2000 Compressed Documents. (arXiv:2306.01359v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01359</link>
<description rdf:parseType="Literal">&lt;p&gt;For any digital application with document images such as retrieval, the
classification of document images becomes an essential stage. Conventionally
for the purpose, the full versions of the documents, that is the uncompressed
document images make the input dataset, which poses a threat due to the big
volume required to accommodate the full versions of the documents. Therefore,
it would be novel, if the same classification task could be accomplished
directly (with some partial decompression) with the compressed representation
of documents in order to make the whole process computationally more efficient.
In this research work, a novel deep learning model, DWT CompCNN is proposed for
classification of documents that are compressed using High Throughput JPEG 2000
(HTJ2K) algorithm. The proposed DWT-CompCNN comprises of five convolutional
layers with filter sizes of 16, 32, 64, 128, and 256 consecutively for each
increasing layer to improve learning from the wavelet coefficients extracted
from the compressed images. Experiments are performed on two benchmark
datasets- Tobacco-3482 and RVL-CDIP, which demonstrate that the proposed model
is time and space efficient, and also achieves a better classification accuracy
in compressed domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisen_T/0/1/0/all/0/1&quot;&gt;Tejasvee Bisen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1&quot;&gt;Mohammed Javed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirtania_S/0/1/0/all/0/1&quot;&gt;Shashank Kirtania&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagabhushan_P/0/1/0/all/0/1&quot;&gt;P. Nagabhushan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06206">
<title>PotatoPestNet: A CTInceptionV3-RS-Based Neural Network for Accurate Identification of Potato Pests. (arXiv:2306.06206v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06206</link>
<description rdf:parseType="Literal">&lt;p&gt;Potatoes are the third-largest food crop globally, but their production
frequently encounters difficulties because of aggressive pest infestations. The
aim of this study is to investigate the various types and characteristics of
these pests and propose an efficient PotatoPestNet AI-based automatic potato
pest identification system. To accomplish this, we curated a reliable dataset
consisting of eight types of potato pests. We leveraged the power of transfer
learning by employing five customized, pre-trained transfer learning models:
CMobileNetV2, CNASLargeNet, CXception, CDenseNet201, and CInceptionV3, in
proposing a robust PotatoPestNet model to accurately classify potato pests. To
improve the models&apos; performance, we applied various augmentation techniques,
incorporated a global average pooling layer, and implemented proper
regularization methods. To further enhance the performance of the models, we
utilized random search (RS) optimization for hyperparameter tuning. This
optimization technique played a significant role in fine-tuning the models and
achieving improved performance. We evaluated the models both visually and
quantitatively, utilizing different evaluation metrics. The robustness of the
models in handling imbalanced datasets was assessed using the Receiver
Operating Characteristic (ROC) curve. Among the models, the Customized Tuned
Inception V3 (CTInceptionV3) model, optimized through random search,
demonstrated outstanding performance. It achieved the highest accuracy (91%),
precision (91%), recall (91%), and F1-score (91%), showcasing its superior
ability to accurately identify and classify potato pests.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talukder_M/0/1/0/all/0/1&quot;&gt;Md. Simul Hasan Talukder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sulaiman_R/0/1/0/all/0/1&quot;&gt;Rejwan Bin Sulaiman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1&quot;&gt;Mohammad Raziuddin Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nipun_M/0/1/0/all/0/1&quot;&gt;Musarrat Saberin Nipun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1&quot;&gt;Taminul Islam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07274">
<title>CryoChains: Heterogeneous Reconstruction of Molecular Assembly of Semi-flexible Chains from Cryo-EM Images. (arXiv:2306.07274v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07274</link>
<description rdf:parseType="Literal">&lt;p&gt;Cryogenic electron microscopy (cryo-EM) has transformed structural biology by
allowing to reconstruct 3D biomolecular structures up to near-atomic
resolution. However, the 3D reconstruction process remains challenging, as the
3D structures may exhibit substantial shape variations, while the 2D image
acquisition suffers from a low signal-to-noise ratio, requiring to acquire very
large datasets that are time-consuming to process. Current reconstruction
methods are precise but computationally expensive, or faster but lack a
physically-plausible model of large molecular shape variations. To fill this
gap, we propose CryoChains that encodes large deformations of biomolecules via
rigid body transformation of their chains, while representing their finer shape
variations with the normal mode analysis framework of biophysics. Our synthetic
data experiments on the human GABA\textsubscript{B} and heat shock protein show
that CryoChains gives a biophysically-grounded quantification of the
heterogeneous conformations of biomolecules, while reconstructing their 3D
molecular structures at an improved resolution compared to the current fastest,
interpretable deep learning method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koo_B/0/1/0/all/0/1&quot;&gt;Bongjin Koo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martel_J/0/1/0/all/0/1&quot;&gt;Julien Martel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peck_A/0/1/0/all/0/1&quot;&gt;Ariana Peck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_A/0/1/0/all/0/1&quot;&gt;Axel Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poitevin_F/0/1/0/all/0/1&quot;&gt;Fr&amp;#xe9;d&amp;#xe9;ric Poitevin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miolane_N/0/1/0/all/0/1&quot;&gt;Nina Miolane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15445">
<title>UniUD Submission to the EPIC-Kitchens-100 Multi-Instance Retrieval Challenge 2023. (arXiv:2306.15445v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15445</link>
<description rdf:parseType="Literal">&lt;p&gt;In this report, we present the technical details of our submission to the
EPIC-Kitchens-100 Multi-Instance Retrieval Challenge 2023. To participate in
the challenge, we ensembled two models trained with two different loss
functions on 25% of the training data. Our submission, visible on the public
leaderboard, obtains an average score of 56.81% nDCG and 42.63% mAP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falcon_A/0/1/0/all/0/1&quot;&gt;Alex Falcon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serra_G/0/1/0/all/0/1&quot;&gt;Giuseppe Serra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16175">
<title>$\mathbf{C}^2$Former: Calibrated and Complementary Transformer for RGB-Infrared Object Detection. (arXiv:2306.16175v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16175</link>
<description rdf:parseType="Literal">&lt;p&gt;Object detection on visible (RGB) and infrared (IR) images, as an emerging
solution to facilitate robust detection for around-the-clock applications, has
received extensive attention in recent years. With the help of IR images,
object detectors have been more reliable and robust in practical applications
by using RGB-IR combined information. However, existing methods still suffer
from modality miscalibration and fusion imprecision problems. Since transformer
has the powerful capability to model the pairwise correlations between
different features, in this paper, we propose a novel Calibrated and
Complementary Transformer called $\mathrm{C}^2$Former to address these two
problems simultaneously. In $\mathrm{C}^2$Former, we design an Inter-modality
Cross-Attention (ICA) module to obtain the calibrated and complementary
features by learning the cross-attention relationship between the RGB and IR
modality. To reduce the computational cost caused by computing the global
attention in ICA, an Adaptive Feature Sampling (AFS) module is introduced to
decrease the dimension of feature maps. Because $\mathrm{C}^2$Former performs
in the feature domain, it can be embedded into existed RGB-IR object detectors
via the backbone network. Thus, one single-stage and one two-stage object
detector both incorporating our $\mathrm{C}^2$Former are constructed to
evaluate its effectiveness and versatility. With extensive experiments on the
DroneVehicle and KAIST RGB-IR datasets, we verify that our method can fully
utilize the RGB-IR complementary information and achieve robust detection
results. The code is available at
https://github.com/yuanmaoxun/Calibrated-and-Complementary-Transformer-for-RGB-Infrared-Object-Detection.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Maoxun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xingxing Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16670">
<title>End-to-End Learnable Multi-Scale Feature Compression for VCM. (arXiv:2306.16670v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16670</link>
<description rdf:parseType="Literal">&lt;p&gt;The proliferation of deep learning-based machine vision applications has
given rise to a new type of compression, so called video coding for machine
(VCM). VCM differs from traditional video coding in that it is optimized for
machine vision performance instead of human visual quality. In the feature
compression track of MPEG-VCM, multi-scale features extracted from images are
subject to compression. Recent feature compression works have demonstrated that
the versatile video coding (VVC) standard-based approach can achieve a BD-rate
reduction of up to 96% against MPEG-VCM feature anchor. However, it is still
sub-optimal as VVC was not designed for extracted features but for natural
images. Moreover, the high encoding complexity of VVC makes it difficult to
design a lightweight encoder without sacrificing performance. To address these
challenges, we propose a novel multi-scale feature compression method that
enables both the end-to-end optimization on the extracted features and the
design of lightweight encoders. The proposed model combines a learnable
compressor with a multi-scale feature fusion network so that the redundancy in
the multi-scale features is effectively removed. Instead of simply cascading
the fusion network and the compression network, we integrate the fusion and
encoding processes in an interleaved way. Our model first encodes a
larger-scale feature to obtain a latent representation and then fuses the
latent with a smaller-scale feature. This process is successively performed
until the smallest-scale feature is fused and then the encoded latent at the
final stage is entropy-coded for transmission. The results show that our model
outperforms previous approaches by at least 52% BD-rate reduction and has
$\times5$ to $\times27$ times less encoding time for object detection...
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yeongwoong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1&quot;&gt;Hyewon Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Janghyun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Younhee Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jooyoung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1&quot;&gt;Se Yoon Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hui Yong Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16772">
<title>Learning from Synthetic Human Group Activities. (arXiv:2306.16772v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16772</link>
<description rdf:parseType="Literal">&lt;p&gt;The understanding of complex human interactions and group activities has
garnered attention in human-centric computer vision. However, the advancement
of the related tasks is hindered due to the difficulty of obtaining large-scale
labeled real-world datasets. To mitigate the issue, we propose M3Act, a
multi-view multi-group multi-person human atomic action and group activity data
generator. Powered by the Unity engine, M3Act contains simulation-ready 3D
scenes and human assets, configurable lighting and camera systems, highly
parameterized modular group activities, and a large degree of domain
randomization during the data generation process. Our data generator is capable
of generating large-scale datasets of human activities with multiple
viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality
annotations for individual persons and multi-person groups (2D bounding boxes,
instance segmentation masks, individual actions and group activity categories).
Using M3Act, we perform synthetic data pre-training for 2D skeleton-based group
activity recognition and RGB-based multi-person pose tracking. The results
indicate that learning from our synthetic datasets largely improves the model
performances on real-world datasets, with the highest gain of 5.59% and 7.32%
respectively in group and person recognition accuracy on CAD2, as well as an
improvement of 6.63 in MOTP on HiEve. Pre-training with our synthetic data also
leads to faster model convergence on downstream tasks (up to 6.8% faster).
Moreover, M3Act opens new research problems for 3D group activity generation.
We release M3Act3D, an 87.6-hour 3D motion dataset of human activities with
larger group sizes and higher complexity of inter-person interactions than
previous multi-person datasets. We define multiple metrics and propose a
competitive baseline for the novel task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Che-Jui Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Honglu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_P/0/1/0/all/0/1&quot;&gt;Parth Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhat_A/0/1/0/all/0/1&quot;&gt;Aditya Bhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1&quot;&gt;Seonghyeon Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1&quot;&gt;Samuel S. Sohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sejong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1&quot;&gt;Vladimir Pavlovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapadia_M/0/1/0/all/0/1&quot;&gt;Mubbasir Kapadia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17723">
<title>FlipNeRF: Flipped Reflection Rays for Few-shot Novel View Synthesis. (arXiv:2306.17723v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17723</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Field (NeRF) has been a mainstream in novel view synthesis
with its remarkable quality of rendered images and simple architecture.
Although NeRF has been developed in various directions improving continuously
its performance, the necessity of a dense set of multi-view images still exists
as a stumbling block to progress for practical application. In this work, we
propose FlipNeRF, a novel regularization method for few-shot novel view
synthesis by utilizing our proposed flipped reflection rays. The flipped
reflection rays are explicitly derived from the input ray directions and
estimated normal vectors, and play a role of effective additional training rays
while enabling to estimate more accurate surface normals and learn the 3D
geometry effectively. Since the surface normal and the scene depth are both
derived from the estimated densities along a ray, the accurate surface normal
leads to more exact depth estimation, which is a key factor for few-shot novel
view synthesis. Furthermore, with our proposed Uncertainty-aware Emptiness Loss
and Bottleneck Feature Consistency Loss, FlipNeRF is able to estimate more
reliable outputs with reducing floating artifacts effectively across the
different scene structures, and enhance the feature-level consistency between
the pair of the rays cast toward the photo-consistent pixels without any
additional feature extractor, respectively. Our FlipNeRF achieves the SOTA
performance on the multiple benchmarks across all the scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1&quot;&gt;Seunghyeon Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yeonjin Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1&quot;&gt;Nojun Kwak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00211">
<title>AIGCIQA2023: A Large-scale Image Quality Assessment Database for AI Generated Images: from the Perspectives of Quality, Authenticity and Correspondence. (arXiv:2307.00211v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00211</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, in order to get a better understanding of the human visual
preferences for AIGIs, a large-scale IQA database for AIGC is established,
which is named as AIGCIQA2023. We first generate over 2000 images based on 6
state-of-the-art text-to-image generation models using 100 prompts. Based on
these images, a well-organized subjective experiment is conducted to assess the
human visual preferences for each image from three perspectives including
quality, authenticity and correspondence. Finally, based on this large-scale
database, we conduct a benchmark experiment to evaluate the performance of
several state-of-the-art IQA metrics on our constructed database.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiarui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1&quot;&gt;Huiyu Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1&quot;&gt;Xiongkuo Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1&quot;&gt;Guangtao Zhai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01097">
<title>MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion. (arXiv:2307.01097v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01097</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces MVDiffusion, a simple yet effective multi-view image
generation method for scenarios where pixel-to-pixel correspondences are
available, such as perspective crops from panorama or multi-view images given
geometry (depth maps and poses). Unlike prior models that rely on iterative
image warping and inpainting, MVDiffusion concurrently generates all images
with a global awareness, encompassing high resolution and rich content,
effectively addressing the error accumulation prevalent in preceding models.
MVDiffusion specifically incorporates a correspondence-aware attention
mechanism, enabling effective cross-view interaction. This mechanism underpins
three pivotal modules: 1) a generation module that produces low-resolution
images while maintaining global correspondence, 2) an interpolation module that
densifies spatial coverage between images, and 3) a super-resolution module
that upscales into high-resolution outputs. In terms of panoramic imagery,
MVDiffusion can generate high-resolution photorealistic images up to
1024$\times$1024 pixels. For geometry-conditioned multi-view image generation,
MVDiffusion demonstrates the first method capable of generating a textured map
of a scene mesh. The project page is at https://mvdiffusion.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shitao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fuyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiacheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furukawa_Y/0/1/0/all/0/1&quot;&gt;Yasutaka Furukawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03903">
<title>Adversarial Self-Attack Defense and Spatial-Temporal Relation Mining for Visible-Infrared Video Person Re-Identification. (arXiv:2307.03903v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03903</link>
<description rdf:parseType="Literal">&lt;p&gt;In visible-infrared video person re-identification (re-ID), extracting
features not affected by complex scenes (such as modality, camera views,
pedestrian pose, background, etc.) changes, and mining and utilizing motion
information are the keys to solving cross-modal pedestrian identity matching.
To this end, the paper proposes a new visible-infrared video person re-ID
method from a novel perspective, i.e., adversarial self-attack defense and
spatial-temporal relation mining. In this work, the changes of views, posture,
background and modal discrepancy are considered as the main factors that cause
the perturbations of person identity features. Such interference information
contained in the training samples is used as an adversarial perturbation. It
performs adversarial attacks on the re-ID model during the training to make the
model more robust to these unfavorable factors. The attack from the adversarial
perturbation is introduced by activating the interference information contained
in the input samples without generating adversarial samples, and it can be thus
called adversarial self-attack. This design allows adversarial attack and
defense to be integrated into one framework. This paper further proposes a
spatial-temporal information-guided feature representation network to use the
information in video sequences. The network cannot only extract the information
contained in the video-frame sequences but also use the relation of the local
information in space to guide the network to extract more robust features. The
proposed method exhibits compelling performance on large-scale cross-modality
video datasets. The source code of the proposed method will be released at
https://github.com/lhf12278/xxx.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huafeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Le Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yafei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dapeng Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhengtao Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04525">
<title>Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans. (arXiv:2307.04525v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04525</link>
<description rdf:parseType="Literal">&lt;p&gt;Gastric cancer is the third leading cause of cancer-related mortality
worldwide, but no guideline-recommended screening test exists. Existing methods
can be invasive, expensive, and lack sensitivity to identify early-stage
gastric cancer. In this study, we explore the feasibility of using a deep
learning approach on non-contrast CT scans for gastric cancer detection. We
propose a novel cluster-induced Mask Transformer that jointly segments the
tumor and classifies abnormality in a multi-task manner. Our model incorporates
learnable clusters that encode the texture and shape prototypes of gastric
cancer, utilizing self- and cross-attention to interact with convolutional
features. In our experiments, the proposed method achieves a sensitivity of
85.0% and specificity of 92.6% for detecting gastric tumors on a hold-out test
set consisting of 100 patients with cancer and 148 normal. In comparison, two
radiologists have an average sensitivity of 73.5% and specificity of 84.3%. We
also obtain a specificity of 97.7% on an external test set with 903 normal
cases. Our approach performs comparably to established state-of-the-art gastric
cancer screening tools like blood testing and endoscopy, while also being more
sensitive in detecting early-stage cancer. This demonstrates the potential of
our approach as a novel, non-invasive, low-cost, and accurate method for
opportunistic gastric cancer screening.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Mingze Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yingda Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jiawen Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junli Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qiu_M/0/1/0/all/0/1&quot;&gt;Mingyan Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hexin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_B/0/1/0/all/0/1&quot;&gt;Bin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Le Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zaiyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Ling Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05201">
<title>The Staged Knowledge Distillation in Video Classification: Harmonizing Student Progress by a Complementary Weakly Supervised Framework. (arXiv:2307.05201v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05201</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of label-efficient learning on video data, the distillation
method and the structural design of the teacher-student architecture have a
significant impact on knowledge distillation. However, the relationship between
these factors has been overlooked in previous research. To address this gap, we
propose a new weakly supervised learning framework for knowledge distillation
in video classification that is designed to improve the efficiency and accuracy
of the student model. Our approach leverages the concept of substage-based
learning to distill knowledge based on the combination of student substages and
the correlation of corresponding substages. We also employ the progressive
cascade training method to address the accuracy loss caused by the large
capacity gap between the teacher and the student. Additionally, we propose a
pseudo-label optimization strategy to improve the initial data label. To
optimize the loss functions of different distillation substages during the
training process, we introduce a new loss method based on feature distribution.
We conduct extensive experiments on both real and simulated data sets,
demonstrating that our proposed approach outperforms existing distillation
methods in terms of knowledge distillation for video classification tasks. Our
proposed substage-based distillation approach has the potential to inform
future research on label-efficient learning for video data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zheng Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05508">
<title>Human in the AI loop via xAI and Active Learning for Visual Inspection. (arXiv:2307.05508v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05508</link>
<description rdf:parseType="Literal">&lt;p&gt;Industrial revolutions have historically disrupted manufacturing by
introducing automation into production. Increasing automation reshapes the role
of the human worker. Advances in robotics and artificial intelligence open new
frontiers of human-machine collaboration. Such collaboration can be realized
considering two sub-fields of artificial intelligence: active learning and
explainable artificial intelligence. Active learning aims to devise strategies
that help obtain data that allows machine learning algorithms to learn better.
On the other hand, explainable artificial intelligence aims to make the machine
learning models intelligible to the human person. The present work first
describes Industry 5.0, human-machine collaboration, and state-of-the-art
regarding quality inspection, emphasizing visual inspection. Then it outlines
how human-machine collaboration could be realized and enhanced in visual
inspection. Finally, some of the results obtained in the EU H2020 STAR project
regarding visual inspection are shared, considering artificial intelligence,
human digital twins, and cybersecurity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rozanec_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#x17e;e M. Ro&amp;#x17e;anec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montini_E/0/1/0/all/0/1&quot;&gt;Elias Montini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cutrona_V/0/1/0/all/0/1&quot;&gt;Vincenzo Cutrona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papamartzivanos_D/0/1/0/all/0/1&quot;&gt;Dimitrios Papamartzivanos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klemencic_T/0/1/0/all/0/1&quot;&gt;Timotej Klemen&amp;#x10d;i&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fortuna_B/0/1/0/all/0/1&quot;&gt;Bla&amp;#x17e; Fortuna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mladenic_D/0/1/0/all/0/1&quot;&gt;Dunja Mladeni&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veliou_E/0/1/0/all/0/1&quot;&gt;Entso Veliou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannetsos_T/0/1/0/all/0/1&quot;&gt;Thanassis Giannetsos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emmanouilidis_C/0/1/0/all/0/1&quot;&gt;Christos Emmanouilidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05766">
<title>Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting. (arXiv:2307.05766v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05766</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiology reporting is a crucial part of the communication between
radiologists and other medical professionals, but it can be time-consuming and
error-prone. One approach to alleviate this is structured reporting, which
saves time and enables a more accurate evaluation than free-text reports.
However, there is limited research on automating structured reporting, and no
public benchmark is available for evaluating and comparing different methods.
To close this gap, we introduce Rad-ReStruct, a new benchmark dataset that
provides fine-grained, hierarchically ordered annotations in the form of
structured reports for X-Ray images. We model the structured reporting task as
hierarchical visual question answering (VQA) and propose hi-VQA, a novel method
that considers prior context in the form of previously asked questions and
answers for populating a structured radiology report. Our experiments show that
hi-VQA achieves competitive performance to the state-of-the-art on the medical
VQA benchmark VQARad while performing best among methods without
domain-specific vision-language pretraining and provides a strong baseline on
Rad-ReStruct. Our work represents a significant step towards the automated
population of structured radiology reports and provides a valuable first
benchmark for future research in this area. We will make all annotations and
our code for annotation generation, model evaluation, and training publicly
available upon acceptance. Our dataset and code is available at
https://github.com/ChantalMP/Rad-ReStruct.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pellegrini_C/0/1/0/all/0/1&quot;&gt;Chantal Pellegrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keicher_M/0/1/0/all/0/1&quot;&gt;Matthias Keicher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozsoy_E/0/1/0/all/0/1&quot;&gt;Ege &amp;#xd6;zsoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06533">
<title>Domain-adaptive Person Re-identification without Cross-camera Paired Samples. (arXiv:2307.06533v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06533</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing person re-identification (re-ID) research mainly focuses on
pedestrian identity matching across cameras in adjacent areas. However, in
reality, it is inevitable to face the problem of pedestrian identity matching
across long-distance scenes. The cross-camera pedestrian samples collected from
long-distance scenes often have no positive samples. It is extremely
challenging to use cross-camera negative samples to achieve cross-region
pedestrian identity matching. Therefore, a novel domain-adaptive person re-ID
method that focuses on cross-camera consistent discriminative feature learning
under the supervision of unpaired samples is proposed. This method mainly
includes category synergy co-promotion module (CSCM) and cross-camera
consistent feature learning module (CCFLM). In CSCM, a task-specific feature
recombination (FRT) mechanism is proposed. This mechanism first groups features
according to their contributions to specific tasks. Then an interactive
promotion learning (IPL) scheme between feature groups is developed and
embedded in this mechanism to enhance feature discriminability. Since the
control parameters of the specific task model are reduced after division by
task, the generalization ability of the model is improved. In CCFLM,
instance-level feature distribution alignment and cross-camera identity
consistent learning methods are constructed. Therefore, the supervised model
training is achieved under the style supervision of the target domain by
exchanging styles between source-domain samples and target-domain samples, and
the challenges caused by the lack of cross-camera paired samples are solved by
utilizing cross-camera similar samples. In experiments, three challenging
datasets are used as target domains, and the effectiveness of the proposed
method is demonstrated through four experimental settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huafeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yanmei Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yafei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1&quot;&gt;Guanqiu Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhengtao Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06548">
<title>Multi-objective Evolutionary Search of Variable-length Composite Semantic Perturbations. (arXiv:2307.06548v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06548</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have proven to be vulnerable to adversarial attacks in
the form of adding specific perturbations on images to make wrong outputs.
Designing stronger adversarial attack methods can help more reliably evaluate
the robustness of DNN models. To release the harbor burden and improve the
attack performance, auto machine learning (AutoML) has recently emerged as one
successful technique to help automatically find the near-optimal adversarial
attack strategy. However, existing works about AutoML for adversarial attacks
only focus on $L_{\infty}$-norm-based perturbations. In fact, semantic
perturbations attract increasing attention due to their naturalnesses and
physical realizability. To bridge the gap between AutoML and semantic
adversarial attacks, we propose a novel method called multi-objective
evolutionary search of variable-length composite semantic perturbations
(MES-VCSP). Specifically, we construct the mathematical model of
variable-length composite semantic perturbations, which provides five
gradient-based semantic attack methods. The same type of perturbation in an
attack sequence is allowed to be performed multiple times. Besides, we
introduce the multi-objective evolutionary search consisting of NSGA-II and
neighborhood search to find near-optimal variable-length attack sequences.
Experimental results on CIFAR10 and ImageNet datasets show that compared with
existing methods, MES-VCSP can obtain adversarial examples with a higher attack
success rate, more naturalness, and less time cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jialiang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wen Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1&quot;&gt;Tingsong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoqian Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06947">
<title>Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition. (arXiv:2307.06947v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06947</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent video recognition models utilize Transformer models for long-range
spatio-temporal context modeling. Video transformer designs are based on
self-attention that can model global context at a high computational cost. In
comparison, convolutional designs for videos offer an efficient alternative but
lack long-range dependency modeling. Towards achieving the best of both
designs, this work proposes Video-FocalNet, an effective and efficient
architecture for video recognition that models both local and global contexts.
Video-FocalNet is based on a spatio-temporal focal modulation architecture that
reverses the interaction and aggregation steps of self-attention for better
efficiency. Further, the aggregation step and the interaction step are both
implemented using efficient convolution and element-wise multiplication
operations that are computationally less expensive than their self-attention
counterparts on video representations. We extensively explore the design space
of focal modulation-based spatio-temporal context modeling and demonstrate our
parallel spatial and temporal encoding design to be the optimal choice.
Video-FocalNets perform favorably well against the state-of-the-art
transformer-based models for video recognition on three large-scale datasets
(Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost. Our
code/models are released at https://github.com/TalalWasim/Video-FocalNets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasim_S/0/1/0/all/0/1&quot;&gt;Syed Talal Wasim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khattak_M/0/1/0/all/0/1&quot;&gt;Muhammad Uzair Khattak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1&quot;&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mubarak Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07125">
<title>CeRF: Convolutional Neural Radiance Fields for New View Synthesis with Derivatives of Ray Modeling. (arXiv:2307.07125v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07125</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, novel view synthesis has gained popularity in generating
high-fidelity images. While demonstrating superior performance in the task of
synthesizing novel views, the majority of these methods are still based on the
conventional multi-layer perceptron for scene embedding. Furthermore, light
field models suffer from geometric blurring during pixel rendering, while
radiance field-based volume rendering methods have multiple solutions for a
certain target of density distribution integration. To address these issues, we
introduce the Convolutional Neural Radiance Fields to model the derivatives of
radiance along rays. Based on 1D convolutional operations, our proposed method
effectively extracts potential ray representations through a structured neural
network architecture. Besides, with the proposed ray modeling, a proposed
recurrent module is employed to solve geometric ambiguity in the fully neural
rendering process. Extensive experiments demonstrate the promising results of
our proposed model compared with existing state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaoyan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1&quot;&gt;Dingbo Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenhui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changbo Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07246">
<title>Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training. (arXiv:2307.07246v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07246</link>
<description rdf:parseType="Literal">&lt;p&gt;The foundation models based on pre-training technology have significantly
advanced artificial intelligence from theoretical to practical applications.
These models have facilitated the feasibility of computer-aided diagnosis for
widespread use. Medical contrastive vision-language pre-training, which does
not require human annotations, is an effective approach for guiding
representation learning using description information in diagnostic reports.
However, the effectiveness of pre-training is limited by the large-scale
semantic overlap and shifting problems in medical field. To address these
issues, we propose the Knowledge-Boosting Contrastive Vision-Language
Pre-training framework (KoBo), which integrates clinical knowledge into the
learning of vision-language semantic consistency. The framework uses an
unbiased, open-set sample-wise knowledge representation to measure negative
sample noise and supplement the correspondence between vision-language mutual
information and clinical knowledge. Extensive experiments validate the effect
of our framework on eight tasks including classification, segmentation,
retrieval, and semantic relatedness, achieving comparable or better performance
with the zero-shot or few-shot settings. Our code is open on
https://github.com/ChenXiaoFei-CS/KoBo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaofei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuting He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1&quot;&gt;Cheng Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1&quot;&gt;Rongjun Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guanyu Yang&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>