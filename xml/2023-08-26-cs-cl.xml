<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CL updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-08-24T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computation and Language</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12372" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12419" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12490" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12519" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12531" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12635" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12643" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12674" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12734" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12833" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12888" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12896" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12966" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.05337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.12886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.08471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04504" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11764" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2308.12372">
<title>Vision Transformer Adapters for Generalizable Multitask Learning. (arXiv:2308.12372v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12372</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the first multitasking vision transformer adapters that learn
generalizable task affinities which can be applied to novel tasks and domains.
Integrated into an off-the-shelf vision transformer backbone, our adapters can
simultaneously solve multiple dense vision tasks in a parameter-efficient
manner, unlike existing multitasking transformers that are parametrically
expensive. In contrast to concurrent methods, we do not require retraining or
fine-tuning whenever a new task or domain is added. We introduce a task-adapted
attention mechanism within our adapter framework that combines gradient-based
task similarities with attention-based ones. The learned task affinities
generalize to the following settings: zero-shot task transfer, unsupervised
domain adaptation, and generalization without fine-tuning to novel domains. We
demonstrate that our approach outperforms not only the existing convolutional
neural network-based multitasking methods but also the vision transformer-based
ones. Our project page is at \url{https://ivrl.github.io/VTAGML}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharjee_D/0/1/0/all/0/1&quot;&gt;Deblina Bhattacharjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Susstrunk_S/0/1/0/all/0/1&quot;&gt;Sabine S&amp;#xfc;sstrunk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1&quot;&gt;Mathieu Salzmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12381">
<title>Inferring gender from name: a large scale performance evaluation study. (arXiv:2308.12381v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12381</link>
<description rdf:parseType="Literal">&lt;p&gt;A person&apos;s gender is a crucial piece of information when performing research
across a wide range of scientific disciplines, such as medicine, sociology,
political science, and economics, to name a few. However, in increasing
instances, especially given the proliferation of big data, gender information
is not readily available. In such cases researchers need to infer gender from
readily available information, primarily from persons&apos; names. While inferring
gender from name may raise some ethical questions, the lack of viable
alternatives means that researchers have to resort to such approaches when the
goal justifies the means - in the majority of such studies the goal is to
examine patterns and determinants of gender disparities. The necessity of
name-to-gender inference has generated an ever-growing domain of algorithmic
approaches and software products. These approaches have been used throughout
the world in academia, industry, governmental and non-governmental
organizations. Nevertheless, the existing approaches have yet to be
systematically evaluated and compared, making it challenging to determine the
optimal approach for future research. In this work, we conducted a large scale
performance evaluation of existing approaches for name-to-gender inference.
Analysis are performed using a variety of large annotated datasets of names. We
further propose two new hybrid approaches that achieve better performance than
any single existing approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krstovski_K/0/1/0/all/0/1&quot;&gt;Kriste Krstovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Ye Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12383">
<title>With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning. (arXiv:2308.12383v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12383</link>
<description rdf:parseType="Literal">&lt;p&gt;Image captioning, like many tasks involving vision and language, currently
relies on Transformer-based architectures for extracting the semantics in an
image and translating it into linguistically coherent descriptions. Although
successful, the attention operator only considers a weighted summation of
projections of the current input sample, therefore ignoring the relevant
semantic information which can come from the joint observation of other
samples. In this paper, we devise a network which can perform attention over
activations obtained while processing other training samples, through a
prototypical memory model. Our memory models the distribution of past keys and
values through the definition of prototype vectors which are both
discriminative and compact. Experimentally, we assess the performance of the
proposed model on the COCO dataset, in comparison with carefully designed
baselines and state-of-the-art approaches, and by investigating the role of
each of the proposed components. We demonstrate that our proposal can increase
the performance of an encoder-decoder Transformer by 3.7 CIDEr points both when
training in cross-entropy only and when fine-tuning with self-critical sequence
training. Source code and trained models are available at:
https://github.com/aimagelab/PMA-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barraco_M/0/1/0/all/0/1&quot;&gt;Manuele Barraco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarto_S/0/1/0/all/0/1&quot;&gt;Sara Sarto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1&quot;&gt;Marcella Cornia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1&quot;&gt;Rita Cucchiara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12419">
<title>Toward American Sign Language Processing in the Real World: Data, Tasks, and Methods. (arXiv:2308.12419v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12419</link>
<description rdf:parseType="Literal">&lt;p&gt;Sign language, which conveys meaning through gestures, is the chief means of
communication among deaf people. Recognizing sign language in natural settings
presents significant challenges due to factors such as lighting, background
clutter, and variations in signer characteristics. In this thesis, I study
automatic sign language processing in the wild, using signing videos collected
from the Internet. This thesis contributes new datasets, tasks, and methods.
Most chapters of this thesis address tasks related to fingerspelling, an
important component of sign language and yet has not been studied widely by
prior work. I present three new large-scale ASL datasets in the wild:
ChicagoFSWild, ChicagoFSWild+, and OpenASL. Using ChicagoFSWild and
ChicagoFSWild+, I address fingerspelling recognition, which consists of
transcribing fingerspelling sequences into text. I propose an end-to-end
approach based on iterative attention that allows recognition from a raw video
without explicit hand detection. I further show that using a Conformer-based
network jointly modeling handshape and mouthing can bring performance close to
that of humans. Next, I propose two tasks for building real-world
fingerspelling-based applications: fingerspelling detection and search. For
fingerspelling detection, I introduce a suite of evaluation metrics and a new
detection model via multi-task training. To address the problem of searching
for fingerspelled keywords in raw sign language videos, we propose a novel
method that jointly localizes and matches fingerspelling segments to text.
Finally, I will describe a benchmark for large-vocabulary open-domain sign
language translation based on OpenASL. To address the challenges of sign
language translation in realistic settings, we propose a set of techniques
including sign search as a pretext task for pre-training and fusion of mouthing
and handshape features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bowen Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12420">
<title>Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature. (arXiv:2308.12420v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2308.12420</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating
comprehensive insights into their diverse components. However, a systematic
literature review that emphasizes the Environmental, Sustainability, and
Governance (ESG) components of DLT remains lacking. To bridge this gap, we
selected 107 seed papers to build a citation network of 63,083 references and
refined it to a corpus of 24,539 publications for analysis. Then, we labeled
the named entities in 46 papers according to twelve top-level categories
derived from an established technology taxonomy and enhanced the taxonomy by
pinpointing DLT&apos;s ESG elements. Leveraging transformer-based language models,
we fine-tuned a pre-trained language model for a Named Entity Recognition (NER)
task using our labeled dataset. We used our fine-tuned language model to
distill the corpus to 505 key papers, facilitating a literature review via
named entities and temporal graph analysis on DLT evolution in the context of
ESG. Our contributions are a methodology to conduct a machine learning-driven
systematic literature review in the DLT field, placing a special emphasis on
ESG aspects. Furthermore, we present a first-of-its-kind NER dataset, composed
of 54,808 named entities, designed for DLT and ESG-related explorations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_W/0/1/0/all/0/1&quot;&gt;Walter Hernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tylinski_K/0/1/0/all/0/1&quot;&gt;Kamil Tylinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moore_A/0/1/0/all/0/1&quot;&gt;Alastair Moore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roche_N/0/1/0/all/0/1&quot;&gt;Niall Roche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vadgama_N/0/1/0/all/0/1&quot;&gt;Nikhil Vadgama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Treiblmaier_H/0/1/0/all/0/1&quot;&gt;Horst Treiblmaier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shangguan_J/0/1/0/all/0/1&quot;&gt;Jiangbo Shangguan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tasca_P/0/1/0/all/0/1&quot;&gt;Paolo Tasca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiahua Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12466">
<title>Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis. (arXiv:2308.12466v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12466</link>
<description rdf:parseType="Literal">&lt;p&gt;Since the introduction of ChatGPT and GPT-4, these models have been tested
across a large number of tasks. Their adeptness across domains is evident, but
their aptitude in playing games and specifically their aptitude in the realm of
poker has remained unexplored. Poker is a game that requires decision making
under uncertainty and incomplete information. In this paper, we put ChatGPT and
GPT-4 through the poker test and evaluate their poker skills. Our findings
reveal that while both models display an advanced understanding of poker,
encompassing concepts like the valuation of starting hands, playing positions
and other intricacies of game theory optimal (GTO) poker, both ChatGPT and
GPT-4 are NOT game theory optimal poker players.
&lt;/p&gt;
&lt;p&gt;Through a series of experiments, we first discover the characteristics of
optimal prompts and model parameters for playing poker with these models. Our
observations then unveil the distinct playing personas of the two models. We
first conclude that GPT-4 is a more advanced poker player than ChatGPT. This
exploration then sheds light on the divergent poker tactics of the two models:
ChatGPT&apos;s conservativeness juxtaposed against GPT-4&apos;s aggression. In poker
vernacular, when tasked to play GTO poker, ChatGPT plays like a Nit, which
means that it has a propensity to only engage with premium hands and folds a
majority of hands. When subjected to the same directive, GPT-4 plays like a
maniac, showcasing a loose and aggressive style of play. Both strategies,
although relatively advanced, are not game theory optimal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Akshat Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12477">
<title>American Stories: A Large-Scale Structured Text Dataset of Historical U.S. Newspapers. (arXiv:2308.12477v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12477</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing full text datasets of U.S. public domain newspapers do not recognize
the often complex layouts of newspaper scans, and as a result the digitized
content scrambles texts from articles, headlines, captions, advertisements, and
other layout regions. OCR quality can also be low. This study develops a novel,
deep learning pipeline for extracting full article texts from newspaper images
and applies it to the nearly 20 million scans in Library of Congress&apos;s public
domain Chronicling America collection. The pipeline includes layout detection,
legibility classification, custom OCR, and association of article texts
spanning multiple bounding boxes. To achieve high scalability, it is built with
efficient architectures designed for mobile phones. The resulting American
Stories dataset provides high quality data that could be used for pre-training
a large language model to achieve better understanding of historical English
and historical world knowledge. The dataset could also be added to the external
database of a retrieval-augmented language model to make historical information
- ranging from interpretations of political events to minutiae about the lives
of people&apos;s ancestors - more widely accessible. Furthermore, structured article
texts facilitate using transformer-based methods for popular social science
applications like topic classification, detection of reproduced content, and
news story clustering. Finally, American Stories provides a massive silver
quality dataset for innovating multimodal layout analysis models and other
multimodal applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dell_M/0/1/0/all/0/1&quot;&gt;Melissa Dell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carlson_J/0/1/0/all/0/1&quot;&gt;Jacob Carlson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bryan_T/0/1/0/all/0/1&quot;&gt;Tom Bryan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silcock_E/0/1/0/all/0/1&quot;&gt;Emily Silcock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1&quot;&gt;Abhishek Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zejiang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DAmico_Wong_L/0/1/0/all/0/1&quot;&gt;Luca D&amp;#x27;Amico-Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1&quot;&gt;Quan Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Querubin_P/0/1/0/all/0/1&quot;&gt;Pablo Querubin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heldring_L/0/1/0/all/0/1&quot;&gt;Leander Heldring&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12488">
<title>GPTEval: A Survey on Assessments of ChatGPT and GPT-4. (arXiv:2308.12488v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.12488</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of ChatGPT has generated much speculation in the press about
its potential to disrupt social and economic systems. Its astonishing language
ability has aroused strong curiosity among scholars about its performance in
different domains. There have been many studies evaluating the ability of
ChatGPT and GPT-4 in different tasks and disciplines. However, a comprehensive
review summarizing the collective assessment findings is lacking. The objective
of this survey is to thoroughly analyze prior assessments of ChatGPT and GPT-4,
focusing on its language and reasoning abilities, scientific knowledge, and
ethical considerations. Furthermore, an examination of the existing evaluation
methods is conducted, offering several recommendations for future research in
evaluating large language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1&quot;&gt;Rui Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guanyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xulang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1&quot;&gt;Frank Guerin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1&quot;&gt;Erik Cambria&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12490">
<title>MultiPA: a multi-task speech pronunciation assessment system for a closed and open response scenario. (arXiv:2308.12490v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12490</link>
<description rdf:parseType="Literal">&lt;p&gt;The design of automatic speech pronunciation assessment can be categorized
into closed and open response scenarios, each with strengths and limitations. A
system with the ability to function in both scenarios can cater to diverse
learning needs and provide a more precise and holistic assessment of
pronunciation skills. In this study, we propose a Multi-task Pronunciation
Assessment model called MultiPA. MultiPA provides an alternative to Kaldi-based
systems in that it has simpler format requirements and better compatibility
with other neural network models. Compared with previous open response systems,
MultiPA provides a wider range of evaluations, encompassing assessments at both
the sentence and word-level. Our experimental results show that MultiPA
achieves comparable performance when working in closed response scenarios and
maintains more robust performance when directly used for open responses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu-Wen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhou Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirschberg_J/0/1/0/all/0/1&quot;&gt;Julia Hirschberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12519">
<title>Large Language Model as Autonomous Decision Maker. (arXiv:2308.12519v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12519</link>
<description rdf:parseType="Literal">&lt;p&gt;While large language models (LLMs) exhibit impressive language understanding
and in-context learning abilities, their decision-making ability still heavily
relies on the guidance of task-specific expert knowledge when solving
real-world tasks. To unleash the potential of LLMs as autonomous decision
makers, this paper presents an approach JuDec to endow LLMs with the
self-judgment ability, enabling LLMs to achieve autonomous judgment and
exploration for decision making. Specifically, in JuDec, Elo-based
Self-Judgment Mechanism is designed to assign Elo scores to decision steps to
judge their values and utilities via pairwise comparisons between two solutions
and then guide the decision-searching process toward the optimal solution
accordingly. Experimental results on the ToolBench dataset demonstrate JuDec&apos;s
superiority over baselines, achieving over 10% improvement in Pass Rate on
diverse tasks. It offers higher-quality solutions and reduces costs (ChatGPT
API calls), highlighting its effectiveness and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yining Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1&quot;&gt;Xin Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yujia Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yankai Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Maosong Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12531">
<title>CARE: Co-Attention Network for Joint Entity and Relation Extraction. (arXiv:2308.12531v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12531</link>
<description rdf:parseType="Literal">&lt;p&gt;Joint entity and relation extraction is the fundamental task of information
extraction, consisting of two subtasks: named entity recognition and relation
extraction. Most existing joint extraction methods suffer from issues of
feature confusion or inadequate interaction between two subtasks. In this work,
we propose a Co-Attention network for joint entity and Relation Extraction
(CARE). Our approach involves learning separate representations for each
subtask, aiming to avoid feature overlap. At the core of our approach is the
co-attention module that captures two-way interaction between two subtasks,
allowing the model to leverage entity information for relation prediction and
vice versa, thus promoting mutual enhancement. Extensive experiments on three
joint entity-relation extraction benchmark datasets (NYT, WebNLG and SciERC)
show that our proposed model achieves superior performance, surpassing existing
baseline models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1&quot;&gt;Wenjun Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yamei Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12539">
<title>CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias. (arXiv:2308.12539v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12539</link>
<description rdf:parseType="Literal">&lt;p&gt;As language models (LMs) become increasingly powerful, it is important to
quantify and compare them for sociodemographic bias with potential for harm.
Prior bias measurement datasets are sensitive to perturbations in their
manually designed templates, therefore unreliable. To achieve reliability, we
introduce the Comprehensive Assessment of Language Model bias (CALM), a
benchmark dataset to quantify bias in LMs across three tasks. We integrate 16
existing datasets across different domains, such as Wikipedia and news
articles, to filter 224 templates from which we construct a dataset of 78,400
examples. We compare the diversity of CALM with prior datasets on metrics such
as average semantic similarity, and variation in template length, and test the
sensitivity to small perturbations. We show that our dataset is more diverse
and reliable than previous datasets, thus better capture the breadth of
linguistic variation required to reliably evaluate model bias. We evaluate 20
large language models including six prominent families of LMs such as Llama-2.
In two LM series, OPT and Bloom, we found that larger parameter models are more
biased than lower parameter models. We found the T0 series of models to be the
least biased. Furthermore, we noticed a tradeoff between gender and racial bias
with increasing model size in some model series. The code is available at
https://github.com/vipulgupta1011/CALM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1&quot;&gt;Vipul Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkit_P/0/1/0/all/0/1&quot;&gt;Pranav Narayanan Venkit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laurencon_H/0/1/0/all/0/1&quot;&gt;Hugo Lauren&amp;#xe7;on&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1&quot;&gt;Shomir Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1&quot;&gt;Rebecca J. Passonneau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12568">
<title>A Small and Fast BERT for Chinese Medical Punctuation Restoration. (arXiv:2308.12568v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12568</link>
<description rdf:parseType="Literal">&lt;p&gt;In clinical dictation, utterances after automatic speech recognition (ASR)
without explicit punctuation marks may lead to the misunderstanding of dictated
reports. To give a precise and understandable clinical report with ASR,
automatic punctuation restoration is required. Considering a practical
scenario, we propose a fast and light pre-trained model for Chinese medical
punctuation restoration based on &apos;pretraining and fine-tuning&apos; paradigm. In
this work, we distill pre-trained models by incorporating supervised
contrastive learning and a novel auxiliary pre-training task (Punctuation Mark
Prediction) to make it well-suited for punctuation restoration. Our experiments
on various distilled models reveal that our model can achieve 95% performance
while 10% model size relative to state-of-the-art Chinese RoBERTa.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_T/0/1/0/all/0/1&quot;&gt;Tongtao Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1&quot;&gt;Chen Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shilei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yi Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12578">
<title>Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models. (arXiv:2308.12578v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12578</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent researches indicate that Pre-trained Large Language Models (LLMs)
possess cognitive constructs similar to those observed in humans, prompting
researchers to investigate the cognitive aspects of LLMs. This paper focuses on
explicit and implicit social bias, a distinctive two-level cognitive construct
in psychology. It posits that individuals&apos; explicit social bias, which is their
conscious expression of bias in the statements, may differ from their implicit
social bias, which represents their unconscious bias. We propose a two-stage
approach and discover a parallel phenomenon in LLMs known as &quot;re-judge
inconsistency&quot; in social bias. In the initial stage, the LLM is tasked with
automatically completing statements, potentially incorporating implicit social
bias. However, in the subsequent stage, the same LLM re-judges the biased
statement generated by itself but contradicts it. We propose that this re-judge
inconsistency can be similar to the inconsistency between human&apos;s unaware
implicit social bias and their aware explicit social bias. Experimental
investigations on ChatGPT and GPT-4 concerning common gender biases examined in
psychology corroborate the highly stable nature of the re-judge inconsistency.
This finding may suggest that diverse cognitive constructs emerge as LLMs&apos;
capabilities strengthen. Consequently, leveraging psychological theories can
provide enhanced insights into the underlying mechanisms governing the
expressions of explicit and implicit constructs in LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yachao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Dongming Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ruifang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1&quot;&gt;Yuexian Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12604">
<title>PromptMRG: Diagnosis-Driven Prompts for Medical Report Generation. (arXiv:2308.12604v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12604</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic medical report generation (MRG) is of great research value as it
has the potential to relieve radiologists from the heavy burden of report
writing. Despite recent advancements, accurate MRG remains challenging due to
the need for precise clinical understanding and the identification of clinical
findings. Moreover, the imbalanced distribution of diseases makes the challenge
even more pronounced, as rare diseases are underrepresented in training data,
making their diagnostic performance unreliable. To address these challenges, we
propose diagnosis-driven prompts for medical report generation (PromptMRG), a
novel framework that aims to improve the diagnostic accuracy of MRG with the
guidance of diagnosis-aware prompts. Specifically, PromptMRG is based on
encoder-decoder architecture with an extra disease classification branch. When
generating reports, the diagnostic results from the classification branch are
converted into token prompts to explicitly guide the generation process. To
further improve the diagnostic accuracy, we design cross-modal feature
enhancement, which retrieves similar reports from the database to assist the
diagnosis of a query image by leveraging the knowledge from a pre-trained CLIP.
Moreover, the disease imbalanced issue is addressed by applying an adaptive
logit-adjusted loss to the classification branch based on the individual
learning status of each disease, which overcomes the barrier of text decoder&apos;s
inability to manipulate disease distributions. Experiments on two MRG
benchmarks show the effectiveness of the proposed method, where it obtains
state-of-the-art clinical efficacy performance on both datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Haibo Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_H/0/1/0/all/0/1&quot;&gt;Haoxuan Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12635">
<title>Advancing Hungarian Text Processing with HuSpaCy: Efficient and Accurate NLP Pipelines. (arXiv:2308.12635v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12635</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a set of industrial-grade text processing models for
Hungarian that achieve near state-of-the-art performance while balancing
resource efficiency and accuracy. Models have been implemented in the spaCy
framework, extending the HuSpaCy toolkit with several improvements to its
architecture. Compared to existing NLP tools for Hungarian, all of our
pipelines feature all basic text processing steps including tokenization,
sentence-boundary detection, part-of-speech tagging, morphological feature
tagging, lemmatization, dependency parsing and named entity recognition with
high accuracy and throughput. We thoroughly evaluated the proposed
enhancements, compared the pipelines with state-of-the-art tools and
demonstrated the competitive performance of the new models in all text
preprocessing steps. All experiments are reproducible and the pipelines are
freely available under a permissive license.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orosz_G/0/1/0/all/0/1&quot;&gt;Gy&amp;#xf6;rgy Orosz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szabo_G/0/1/0/all/0/1&quot;&gt;Gerg&amp;#x151; Szab&amp;#xf3;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berkecz_P/0/1/0/all/0/1&quot;&gt;P&amp;#xe9;ter Berkecz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szanto_Z/0/1/0/all/0/1&quot;&gt;Zsolt Sz&amp;#xe1;nt&amp;#xf3;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farkas_R/0/1/0/all/0/1&quot;&gt;Rich&amp;#xe1;rd Farkas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12643">
<title>Probabilistic Method of Measuring Linguistic Productivity. (arXiv:2308.12643v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12643</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper I propose a new way of measuring linguistic productivity that
objectively assesses the ability of an affix to be used to coin new complex
words and, unlike other popular measures, is not directly dependent upon token
frequency. Specifically, I suggest that linguistic productivity may be viewed
as the probability of an affix to combine with a random base. The advantages of
this approach include the following. First, token frequency does not dominate
the productivity measure but naturally influences the sampling of bases.
Second, we are not just counting attested word types with an affix but rather
simulating the construction of these types and then checking whether they are
attested in the corpus. Third, a corpus-based approach and randomised design
assure that true neologisms and words coined long ago have equal chances to be
selected. The proposed algorithm is evaluated both on English and Russian data.
The obtained results provide some valuable insights into the relation of
linguistic productivity to the number of types and tokens. It looks like
burgeoning linguistic productivity manifests itself in an increasing number of
types. However, this process unfolds in two stages: first comes the increase in
high-frequency items, and only then follows the increase in low-frequency
items.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monakhov_S/0/1/0/all/0/1&quot;&gt;Sergei Monakhov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12648">
<title>From Chatter to Matter: Addressing Critical Steps of Emotion Recognition Learning in Task-oriented Dialogue. (arXiv:2308.12648v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12648</link>
<description rdf:parseType="Literal">&lt;p&gt;Emotion recognition in conversations (ERC) is a crucial task for building
human-like conversational agents. While substantial efforts have been devoted
to ERC for chit-chat dialogues, the task-oriented counterpart is largely left
unattended. Directly applying chit-chat ERC models to task-oriented dialogues
(ToDs) results in suboptimal performance as these models overlook key features
such as the correlation between emotions and task completion in ToDs. In this
paper, we propose a framework that turns a chit-chat ERC model into a
task-oriented one, addressing three critical aspects: data, features and
objective. First, we devise two ways of augmenting rare emotions to improve ERC
performance. Second, we use dialogue states as auxiliary features to
incorporate key information from the goal of the user. Lastly, we leverage a
multi-aspect emotion definition in ToDs to devise a multi-task learning
objective and a novel emotion-distance weighted loss function. Our framework
yields significant improvements for a range of chit-chat ERC models on EmoWOZ,
a large-scale dataset for user emotion in ToDs. We further investigate the
generalisability of the best resulting model to predict user satisfaction in
different ToD datasets. A comparison with supervised baselines shows a strong
zero-shot capability, highlighting the potential usage of our framework in
wider scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shutong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lubis_N/0/1/0/all/0/1&quot;&gt;Nurul Lubis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruppik_B/0/1/0/all/0/1&quot;&gt;Benjamin Ruppik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geishauser_C/0/1/0/all/0/1&quot;&gt;Christian Geishauser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heck_M/0/1/0/all/0/1&quot;&gt;Michael Heck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hsien-chin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niekerk_C/0/1/0/all/0/1&quot;&gt;Carel van Niekerk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vukovic_R/0/1/0/all/0/1&quot;&gt;Renato Vukovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1&quot;&gt;Milica Ga&amp;#x161;i&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12674">
<title>Improving Translation Faithfulness of Large Language Models via Augmenting Instructions. (arXiv:2308.12674v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12674</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) present strong general capabilities, and a
current compelling challenge is stimulating their specialized capabilities,
such as machine translation, through low-cost instruction tuning. The standard
instruction-following data is sequentially organized as the concatenation of an
instruction, an input, and a response. As the attention mechanism of LLMs has
limitations on local focus, LLMs tend to focus more on the words or sentences
nearby at each position. This leads to a high risk of instruction forgetting
during decoding. To alleviate the above issues, We propose SWIE
(Segment-Weighted Instruction Embedding) and an instruction-following dataset
OVERMISS. SWIE improves the model instruction understanding by adding a global
instruction representation on the following input and response representations.
OVERMISS improves model faithfulness by comparing over-translation and
miss-translation results with the correct translation. We apply our methods to
two main-stream open-source LLMs, BLOOM and LLaMA. The experimental results
demonstrate significant improvements in translation performance with SWIE based
on BLOOMZ-3b, particularly in zero-shot and long text translations due to
reduced instruction forgetting risk. Additionally, OVERMISS outperforms the
baseline in translation performance (e.g. an increase in BLEU scores from 0.69
to 3.12 and an average improvement of 0.48 percentage comet scores for
LLaMA-7b) with further enhancements seen in models combining OVERMISS and SWIE
(e.g. the BLUE scores increase up to 0.56 from English to German across three
different backbones), and both exhibit improvements in the faithfulness metric
based on word alignment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yijie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yijin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1&quot;&gt;Fandong Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yufeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jinan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12711">
<title>Harnessing the Power of David against Goliath: Exploring Instruction Data Generation without Using Closed-Source Models. (arXiv:2308.12711v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12711</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction tuning is instrumental in enabling Large Language Models~(LLMs)
to follow user instructions to complete various open-domain tasks. The success
of instruction tuning depends on the availability of high-quality instruction
data. Owing to the exorbitant cost and substandard quality of human annotation,
recent works have been deeply engaged in the exploration of the utilization of
powerful closed-source models to generate instruction data automatically.
However, these methods carry potential risks arising from the usage
requirements of powerful closed-source models, which strictly forbid the
utilization of their outputs to develop machine learning models. To deal with
this problem, in this work, we explore alternative approaches to generate
high-quality instruction data that do not rely on closed-source models. Our
exploration includes an investigation of various existing instruction
generation methods, culminating in the integration of the most efficient
variant with two novel strategies to enhance the quality further. Evaluation
results from two benchmarks and the GPT-4 model demonstrate the effectiveness
of our generated instruction data, which can outperform Alpaca, a method
reliant on closed-source models. We hope that more progress can be achieved in
generating high-quality instruction data without using closed-source models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinrui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Juntao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1&quot;&gt;Jinxiong Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qishen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhongyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guannan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12734">
<title>Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion. (arXiv:2308.12734v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2308.12734</link>
<description rdf:parseType="Literal">&lt;p&gt;There are growing implications surrounding generative AI in the speech domain
that enable voice cloning and real-time voice conversion from one individual to
another. This technology poses a significant ethical threat and could lead to
breaches of privacy and misrepresentation, thus there is an urgent need for
real-time detection of AI-generated speech for DeepFake Voice Conversion. To
address the above emerging issues, the DEEP-VOICE dataset is generated in this
study, comprised of real human speech from eight well-known figures and their
speech converted to one another using Retrieval-based Voice Conversion.
Presenting as a binary classification problem of whether the speech is real or
AI-generated, statistical analysis of temporal audio features through t-testing
reveals that there are significantly different distributions. Hyperparameter
optimisation is implemented for machine learning models to identify the source
of speech. Following the training of 208 individual machine learning models
over 10-fold cross validation, it is found that the Extreme Gradient Boosting
model can achieve an average classification accuracy of 99.3% and can classify
speech in real-time, at around 0.004 milliseconds given one second of speech.
All data generated for this study is released publicly for future research on
AI speech detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bird_J/0/1/0/all/0/1&quot;&gt;Jordan J. Bird&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lotfi_A/0/1/0/all/0/1&quot;&gt;Ahmad Lotfi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12770">
<title>WavMark: Watermarking for Audio Generation. (arXiv:2308.12770v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2308.12770</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent breakthroughs in zero-shot voice synthesis have enabled imitating a
speaker&apos;s voice using just a few seconds of recording while maintaining a high
level of realism. Alongside its potential benefits, this powerful technology
introduces notable risks, including voice fraud and speaker impersonation.
Unlike the conventional approach of solely relying on passive methods for
detecting synthetic data, watermarking presents a proactive and robust defence
mechanism against these looming risks. This paper introduces an innovative
audio watermarking framework that encodes up to 32 bits of watermark within a
mere 1-second audio snippet. The watermark is imperceptible to human senses and
exhibits strong resilience against various attacks. It can serve as an
effective identifier for synthesized voices and holds potential for broader
applications in audio copyright protection. Moreover, this framework boasts
high flexibility, allowing for the combination of multiple watermark segments
to achieve heightened robustness and expanded capacity. Utilizing 10 to
20-second audio as the host, our approach demonstrates an average Bit Error
Rate (BER) of 0.48\% across ten common attacks, a remarkable reduction of over
2800\% in BER compared to the state-of-the-art watermarking tool. See
https://aka.ms/wavmark for demos of our work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guangyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shujie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xiaoyong Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1&quot;&gt;Furu Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12833">
<title>Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities. (arXiv:2308.12833v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12833</link>
<description rdf:parseType="Literal">&lt;p&gt;Spurred by the recent rapid increase in the development and distribution of
large language models (LLMs) across industry and academia, much recent work has
drawn attention to safety- and security-related threats and vulnerabilities of
LLMs, including in the context of potentially criminal activities.
Specifically, it has been shown that LLMs can be misused for fraud,
impersonation, and the generation of malware; while other authors have
considered the more general problem of AI alignment. It is important that
developers and practitioners alike are aware of security-related problems with
such models. In this paper, we provide an overview of existing - predominantly
scientific - efforts on identifying and mitigating threats and vulnerabilities
arising from LLMs. We present a taxonomy describing the relationship between
threats caused by the generative capabilities of LLMs, prevention measures
intended to address such threats, and vulnerabilities arising from imperfect
prevention measures. With our work, we hope to raise awareness of the
limitations of LLMs in light of such security concerns, among both experienced
developers and novel users of such technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mozes_M/0/1/0/all/0/1&quot;&gt;Maximilian Mozes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xuanli He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleinberg_B/0/1/0/all/0/1&quot;&gt;Bennett Kleinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffin_L/0/1/0/all/0/1&quot;&gt;Lewis D. Griffin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12842">
<title>Text Similarity from Image Contents using Statistical and Semantic Analysis Techniques. (arXiv:2308.12842v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12842</link>
<description rdf:parseType="Literal">&lt;p&gt;Plagiarism detection is one of the most researched areas among the Natural
Language Processing(NLP) community. A good plagiarism detection covers all the
NLP methods including semantics, named entities, paraphrases etc. and produces
detailed plagiarism reports. Detection of Cross Lingual Plagiarism requires
deep knowledge of various advanced methods and algorithms to perform effective
text similarity checking. Nowadays the plagiarists are also advancing
themselves from hiding the identity from being catch in such offense. The
plagiarists are bypassed from being detected with techniques like paraphrasing,
synonym replacement, mismatching citations, translating one language to
another. Image Content Plagiarism Detection (ICPD) has gained importance,
utilizing advanced image content processing to identify instances of plagiarism
to ensure the integrity of image content. The issue of plagiarism extends
beyond textual content, as images such as figures, graphs, and tables also have
the potential to be plagiarized. However, image content plagiarism detection
remains an unaddressed challenge. Therefore, there is a critical need to
develop methods and systems for detecting plagiarism in image content. In this
paper, the system has been implemented to detect plagiarism form contents of
Images such as Figures, Graphs, Tables etc. Along with statistical algorithms
such as Jaccard and Cosine, introducing semantic algorithms such as LSA, BERT,
WordNet outperformed in detecting efficient and accurate plagiarism.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_S/0/1/0/all/0/1&quot;&gt;Sagar Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Govilkar_S/0/1/0/all/0/1&quot;&gt;Sharvari Govilkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amin_D/0/1/0/all/0/1&quot;&gt;Dhiraj Amin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12877">
<title>DS4DH at #SMM4H 2023: Zero-Shot Adverse Drug Events Normalization using Sentence Transformers and Reciprocal-Rank Fusion. (arXiv:2308.12877v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12877</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper outlines the performance evaluation of a system for adverse drug
event normalization, developed by the Data Science for Digital Health group for
the Social Media Mining for Health Applications 2023 shared task 5. Shared task
5 targeted the normalization of adverse drug event mentions in Twitter to
standard concepts from the Medical Dictionary for Regulatory Activities
terminology. Our system hinges on a two-stage approach: BERT fine-tuning for
entity recognition, followed by zero-shot normalization using sentence
transformers and reciprocal-rank fusion. The approach yielded a precision of
44.9%, recall of 40.5%, and an F1-score of 42.6%. It outperformed the median
performance in shared task 5 by 10% and demonstrated the highest performance
among all participants. These results substantiate the effectiveness of our
approach and its potential application for adverse drug event normalization in
the realm of social media text mining.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yazdani_A/0/1/0/all/0/1&quot;&gt;Anthony Yazdani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rouhizadeh_H/0/1/0/all/0/1&quot;&gt;Hossein Rouhizadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_D/0/1/0/all/0/1&quot;&gt;David Vicente Alvarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teodoro_D/0/1/0/all/0/1&quot;&gt;Douglas Teodoro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12888">
<title>Inducing Causal Structure for Abstractive Text Summarization. (arXiv:2308.12888v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12888</link>
<description rdf:parseType="Literal">&lt;p&gt;The mainstream of data-driven abstractive summarization models tends to
explore the correlations rather than the causal relationships. Among such
correlations, there can be spurious ones which suffer from the language prior
learned from the training corpus and therefore undermine the overall
effectiveness of the learned model. To tackle this issue, we introduce a
Structural Causal Model (SCM) to induce the underlying causal structure of the
summarization data. We assume several latent causal factors and non-causal
factors, representing the content and style of the document and summary.
Theoretically, we prove that the latent factors in our SCM can be identified by
fitting the observed training data under certain conditions. On the basis of
this, we propose a Causality Inspired Sequence-to-Sequence model (CI-Seq2Seq)
to learn the causal representations that can mimic the causal factors, guiding
us to pursue causal information for summary generation. The key idea is to
reformulate the Variational Auto-encoder (VAE) to fit the joint distribution of
the document and summary variables from the training corpus. Experimental
results on two widely used text summarization datasets demonstrate the
advantages of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jiafeng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xueqi Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12890">
<title>Large Language Models Vote: Prompting for Rare Disease Identification. (arXiv:2308.12890v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12890</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of generative Large Language Models (LLMs) emphasizes the need
for accurate and efficient prompting approaches. LLMs are often applied in
Few-Shot Learning (FSL) contexts, where tasks are executed with minimal
training data. FSL has become popular in many Artificial Intelligence (AI)
subdomains, including AI for health. Rare diseases, affecting a small fraction
of the population, inherently require FSL techniques due to limited data
availability, though manual data collection and annotation is costly and
time-consuming. In this paper, we propose Models-Vote Prompting (MVP), a
flexible prompting approach for improving the performance of LLM queries in FSL
settings. MVP works by prompting numerous LLMs to perform the same tasks and
then conducting a majority vote on the resulting outputs. This method achieves
improved results to any one model in the ensemble on one-shot rare disease
identification and classification tasks. We also release a novel rare disease
dataset for FSL, available to those who agreed to the MIMIC-IV Data Use
Agreement (DUA). Furthermore, in using MVP, each model is prompted multiple
times, substantially increasing the time needed for manual annotation, and to
address this, we assess the feasibility of using JSON for automating generative
LLM evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oniani_D/0/1/0/all/0/1&quot;&gt;David Oniani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilsman_J/0/1/0/all/0/1&quot;&gt;Jordan Hilsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Fengyi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1&quot;&gt;Shiven Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanshan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12896">
<title>Beyond Document Page Classification: Design, Datasets, and Challenges. (arXiv:2308.12896v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12896</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper highlights the need to bring document classification benchmarking
closer to real-world applications, both in the nature of data tested ($X$:
multi-channel, multi-paged, multi-industry; $Y$: class distributions and label
set variety) and in classification tasks considered ($f$: multi-page document,
page stream, and document bundle classification, ...). We identify the lack of
public multi-page document classification datasets, formalize different
classification tasks arising in application scenarios, and motivate the value
of targeting efficient multi-page document representations. An experimental
study on proposed multi-page document classification datasets demonstrates that
current benchmarks have become irrelevant and need to be updated to evaluate
complete documents, as they naturally occur in practice. This reality check
also calls for more mature evaluation methodologies, covering calibration
evaluation, inference complexity (time-memory), and a range of realistic
distribution shifts (e.g., born-digital vs. scanning noise, shifting page
order). Our study ends on a hopeful note by recommending concrete avenues for
future improvements.}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Landeghem_J/0/1/0/all/0/1&quot;&gt;Jordy Van Landeghem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1&quot;&gt;Sanket Biswas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1&quot;&gt;Matthew B. Blaschko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1&quot;&gt;Marie-Francine Moens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12898">
<title>Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?. (arXiv:2308.12898v1 [cs.MM])</title>
<link>http://arxiv.org/abs/2308.12898</link>
<description rdf:parseType="Literal">&lt;p&gt;The multimedia community has shown a significant interest in perceiving and
representing the physical world with multimodal pretrained neural network
models, and among them, the visual-language pertaining (VLP) is, currently, the
most captivating topic. However, there have been few endeavors dedicated to the
exploration of 1) whether essential linguistic knowledge (e.g., semantics and
syntax) can be extracted during VLP, and 2) how such linguistic knowledge
impact or enhance the multimodal alignment. In response, here we aim to
elucidate the impact of comprehensive linguistic knowledge, including semantic
expression and syntactic structure, on multimodal alignment. Specifically, we
design and release the SNARE, the first large-scale multimodal alignment
probing benchmark, to detect the vital linguistic components, e.g., lexical,
semantic, and syntax knowledge, containing four tasks: Semantic structure,
Negation logic, Attribute ownership, and Relationship composition. Based on our
proposed probing benchmarks, our holistic analyses of five advanced VLP models
illustrate that the VLP model: i) shows insensitivity towards complex syntax
structures and relies on content words for sentence comprehension; ii)
demonstrates limited comprehension of combinations between sentences and
negations; iii) faces challenges in determining the presence of actions or
spatial relationships within visual information and struggles with verifying
the correctness of triple combinations. We make our benchmark and code
available at \url{https://github.com/WangFei-2019/SNARE/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Liang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1&quot;&gt;Jun Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Ye Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Changxing Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12923">
<title>Diagnosing Infeasible Optimization Problems Using Large Language Models. (arXiv:2308.12923v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2308.12923</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision-making problems can be represented as mathematical optimization
models, finding wide applications in fields such as economics, engineering and
manufacturing, transportation, and health care. Optimization models are
mathematical abstractions of the problem of making the best decision while
satisfying a set of requirements or constraints. One of the primary barriers to
deploying these models in practice is the challenge of helping practitioners
understand and interpret such models, particularly when they are infeasible,
meaning no decision satisfies all the constraints. Existing methods for
diagnosing infeasible optimization models often rely on expert systems,
necessitating significant background knowledge in optimization. In this paper,
we introduce OptiChat, a first-of-its-kind natural language-based system
equipped with a chatbot GUI for engaging in interactive conversations about
infeasible optimization models. OptiChat can provide natural language
descriptions of the optimization model itself, identify potential sources of
infeasibility, and offer suggestions to make the model feasible. The
implementation of OptiChat is built on GPT-4, which interfaces with an
optimization solver to identify the minimal subset of constraints that render
the entire optimization problem infeasible, also known as the Irreducible
Infeasible Subset (IIS). We utilize few-shot learning, expert chain-of-thought,
key-retrieve, and sentiment prompts to enhance OptiChat&apos;s reliability. Our
experiments demonstrate that OptiChat assists both expert and non-expert users
in improving their understanding of the optimization models, enabling them to
quickly identify the sources of infeasibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Constante_Flores_G/0/1/0/all/0/1&quot;&gt;Gonzalo E. Constante-Flores&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Can Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12950">
<title>Code Llama: Open Foundation Models for Code. (arXiv:2308.12950v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.12950</link>
<description rdf:parseType="Literal">&lt;p&gt;We release Code Llama, a family of large language models for code based on
Llama 2 providing state-of-the-art performance among open models, infilling
capabilities, support for large input contexts, and zero-shot instruction
following ability for programming tasks. We provide multiple flavors to cover a
wide range of applications: foundation models (Code Llama), Python
specializations (Code Llama - Python), and instruction-following models (Code
Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained
on sequences of 16k tokens and show improvements on inputs with up to 100k
tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support
infilling based on surrounding content. Code Llama reaches state-of-the-art
performance among open models on several code benchmarks, with scores of up to
53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python
7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform
every other publicly available model on MultiPL-E. We release Code Llama under
a permissive license that allows for both research and commercial use.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roziere_B/0/1/0/all/0/1&quot;&gt;Baptiste Rozi&amp;#xe8;re&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gehring_J/0/1/0/all/0/1&quot;&gt;Jonas Gehring&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gloeckle_F/0/1/0/all/0/1&quot;&gt;Fabian Gloeckle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sootla_S/0/1/0/all/0/1&quot;&gt;Sten Sootla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1&quot;&gt;Itai Gat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1&quot;&gt;Xiaoqing Ellen Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1&quot;&gt;Yossi Adi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1&quot;&gt;Tal Remez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rapin_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xe9;my Rapin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kozhevnikov_A/0/1/0/all/0/1&quot;&gt;Artyom Kozhevnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evtimov_I/0/1/0/all/0/1&quot;&gt;Ivan Evtimov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_J/0/1/0/all/0/1&quot;&gt;Joanna Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatt_M/0/1/0/all/0/1&quot;&gt;Manish Bhatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1&quot;&gt;Cristian Canton Ferrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grattafiori_A/0/1/0/all/0/1&quot;&gt;Aaron Grattafiori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1&quot;&gt;Wenhan Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Defossez_A/0/1/0/all/0/1&quot;&gt;Alexandre D&amp;#xe9;fossez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1&quot;&gt;Jade Copet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azhar_F/0/1/0/all/0/1&quot;&gt;Faisal Azhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Touvron_H/0/1/0/all/0/1&quot;&gt;Hugo Touvron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1&quot;&gt;Louis Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Usunier_N/0/1/0/all/0/1&quot;&gt;Nicolas Usunier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1&quot;&gt;Thomas Scialom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1&quot;&gt;Gabriel Synnaeve&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12966">
<title>Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities. (arXiv:2308.12966v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.12966</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the Qwen-VL series, a set of large-scale vision-language models
designed to perceive and understand both text and images. Comprising Qwen-VL
and Qwen-VL-Chat, these models exhibit remarkable performance in tasks like
image captioning, question answering, visual localization, and flexible
interaction. The evaluation covers a wide range of tasks including zero-shot
captioning, visual or document visual question answering, and grounding. We
demonstrate the Qwen-VL outperforms existing Large Vision Language Models
(LVLMs). We present their architecture, training, capabilities, and
performance, highlighting their contributions to advancing multimodal
artificial intelligence. Code, demo and models are available at
https://github.com/QwenLM/Qwen-VL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1&quot;&gt;Jinze Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1&quot;&gt;Shuai Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shusheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1&quot;&gt;Sinan Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Junyang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.05337">
<title>A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models. (arXiv:2201.05337v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2201.05337</link>
<description rdf:parseType="Literal">&lt;p&gt;Controllable Text Generation (CTG) is emerging area in the field of natural
language generation (NLG). It is regarded as crucial for the development of
advanced text generation technologies that better meet the specific constraints
in practical applications. In recent years, methods using large-scale
pre-trained language models (PLMs), in particular the widely used
transformer-based PLMs, have become a new paradigm of NLG, allowing generation
of more diverse and fluent text. However, due to the limited level of
interpretability of deep neural networks, the controllability of these methods
need to be guaranteed. To this end, controllable text generation using
transformer-based PLMs has become a rapidly growing yet challenging new
research hotspot. A diverse range of approaches have emerged in the recent 3-4
years, targeting different CTG tasks that require different types of controlled
constraints. In this paper, we present a systematic critical review on the
common tasks, main approaches, and evaluation methods in this area. Finally, we
discuss the challenges that the field is facing, and put forward various
promising future directions. To the best of our knowledge, this is the first
survey paper to summarize the state-of-the-art CTG techniques from the
perspective of Transformer-based PLMs. We hope it can help researchers and
practitioners in the related fields to quickly track the academic and
technological frontier, providing them with a landscape of the area and a
roadmap for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Haolin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shaoyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Ming Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Dawei Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.12886">
<title>Automatic Speech Recognition for Speech Assessment of Persian Preschool Children. (arXiv:2203.12886v10 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2203.12886</link>
<description rdf:parseType="Literal">&lt;p&gt;Preschool evaluation is crucial because it gives teachers and parents
influential knowledge about children&apos;s growth and development. The COVID-19
pandemic has highlighted the necessity of online assessment for preschool
children. One of the areas that should be tested is their ability to speak.
Employing an Automatic Speech Recognition (ASR) system would not help since
they are pre-trained on voices that differ from children&apos;s in terms of
frequency and amplitude. Because most of these are pre-trained with data in a
specific range of amplitude, their objectives do not make them ready for voices
in different amplitudes. To overcome this issue, we added a new objective to
the masking objective of the Wav2Vec 2.0 model called Random Frequency Pitch
(RFP). In addition, we used our newly introduced dataset to fine-tune our model
for Meaningless Words (MW) and Rapid Automatic Naming (RAN) tests. Using
masking in concatenation with RFP outperforms the masking objective of Wav2Vec
2.0 by reaching a Word Error Rate (WER) of 1.35. Our new approach reaches a WER
of 6.45 on the Persian section of the CommonVoice dataset. Furthermore, our
novel methodology produces positive outcomes in zero- and few-shot scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1&quot;&gt;Amirhossein Abaskohi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mortazavi_F/0/1/0/all/0/1&quot;&gt;Fatemeh Mortazavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1&quot;&gt;Hadi Moradi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.08471">
<title>Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion. (arXiv:2210.08471v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.08471</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based pre-trained models like BERT have achieved great progress
on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also
shown general benefits in multiple NLP tasks. However, how to efficiently
integrate dependency prior structure into pre-trained models to better model
complex semantic matching relations is still unsettled. In this paper, we
propose the \textbf{D}ependency-Enhanced \textbf{A}daptive \textbf{F}usion
\textbf{A}ttention (\textbf{DAFA}), which explicitly introduces dependency
structure into pre-trained models and adaptively fuses it with semantic
information. Specifically, \textbf{\emph{(i)}} DAFA first proposes a
structure-sensitive paradigm to construct a dependency matrix for calibrating
attention weights. It adopts an adaptive fusion module to integrate the
obtained dependency information and the original semantic signals. Moreover,
DAFA reconstructs the attention calculation flow and provides better
interpretability. By applying it on BERT, our method achieves state-of-the-art
or competitive performance on 10 public datasets, demonstrating the benefits of
adaptively fusing dependency structure in semantic matching task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jian Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1&quot;&gt;Di Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Rumei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuntao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sirui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1&quot;&gt;Minlong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yongxin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06152">
<title>Structure-CLIP: Towards Scene Graph Knowledge to Enhance Multi-modal Structured Representations. (arXiv:2305.06152v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06152</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale vision-language pre-training has achieved significant performance
in multi-modal understanding and generation tasks. However, existing methods
often perform poorly on image-text matching tasks that require structured
representations, i.e., representations of objects, attributes, and relations.
Previous models cannot make a distinction between ``An astronaut rides a horse&quot;
and ``A horse rides an astronaut&quot;. This is because they fail to fully leverage
structured knowledge when learning representations in multi-modal scenarios. In
this paper, we present an end-to-end framework Structure-CLIP, which integrates
Scene Graph Knowledge (SGK) to enhance multi-modal structured representations.
Firstly, we use scene graphs to guide the construction of semantic negative
examples, which results in an increased emphasis on learning structured
representations. Moreover, a Knowledge-Enhance Encoder (KEE) is proposed to
leverage SGK as input to further enhance structured representations. To verify
the effectiveness of the proposed framework, we pre-train our model with the
aforementioned approaches and conduct experiments on downstream tasks.
Experimental results demonstrate that Structure-CLIP achieves state-of-the-art
(SOTA) performance on VG-Attribution and VG-Relation datasets, with 12.5% and
4.1% ahead of the multi-modal SOTA model respectively. Meanwhile, the results
on MSCOCO indicate that Structure-CLIP significantly enhances the structured
representations while maintaining the ability of general representations. Our
code will be available soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yufeng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiji Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rongsheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weijie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhou Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1&quot;&gt;Tangjie Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wen Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04504">
<title>Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. (arXiv:2306.04504v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04504</link>
<description rdf:parseType="Literal">&lt;p&gt;ChatGPT is a large language model developed by OpenAI. Despite its impressive
performance across various tasks, no prior work has investigated its capability
in the biomedical domain yet. To this end, this paper aims to evaluate the
performance of ChatGPT on various benchmark biomedical tasks, such as relation
extraction, document classification, question answering, and summarization. To
the best of our knowledge, this is the first work that conducts an extensive
evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on
our evaluation that in biomedical datasets that have smaller training sets,
zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative
transformer models, such as BioGPT and BioBART. This suggests that ChatGPT&apos;s
pre-training on large text corpora makes it quite specialized even in the
biomedical domain. Our findings demonstrate that ChatGPT has the potential to
be a valuable tool for various tasks in the biomedical domain that lack large
annotated data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jahan_I/0/1/0/all/0/1&quot;&gt;Israt Jahan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1&quot;&gt;Md Tahmid Rahman Laskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1&quot;&gt;Chun Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jimmy Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04528">
<title>PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts. (arXiv:2306.04528v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04528</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing reliance on Large Language Models (LLMs) across academia and
industry necessitates a comprehensive understanding of their robustness to
prompts. In response to this vital need, we introduce PromptBench, a robustness
benchmark designed to measure LLMs&apos; resilience to adversarial prompts. This
study uses a plethora of adversarial textual attacks targeting prompts across
multiple levels: character, word, sentence, and semantic. These prompts are
then employed in diverse tasks, such as sentiment analysis, natural language
inference, reading comprehension, machine translation, and math
problem-solving. Our study generates 4,032 adversarial prompts, meticulously
evaluated over 8 tasks and 13 datasets, with 567,084 test samples in total. Our
findings demonstrate that contemporary LLMs are vulnerable to adversarial
prompts. Furthermore, we present comprehensive analysis to understand the
mystery behind prompt robustness and its transferability. We then offer
insightful robustness analysis and pragmatic recommendations for prompt
composition, beneficial to both researchers and everyday users. We make our
code, prompts, and methodologies to generate adversarial prompts publicly
accessible, thereby enabling and encouraging collaborative exploration in this
pivotal field: https://github.com/microsoft/promptbench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kaijie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiaheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zichen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yidong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Linyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wei Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1&quot;&gt;Neil Zhenqiang Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17810">
<title>A Massive Scale Semantic Similarity Dataset of Historical English. (arXiv:2306.17810v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17810</link>
<description rdf:parseType="Literal">&lt;p&gt;A diversity of tasks use language models trained on semantic similarity data.
While there are a variety of datasets that capture semantic similarity, they
are either constructed from modern web data or are relatively small datasets
created in the past decade by human annotators. This study utilizes a novel
source, newly digitized articles from off-copyright, local U.S. newspapers, to
assemble a massive-scale semantic similarity dataset spanning 70 years from
1920 to 1989 and containing nearly 400M positive semantic similarity pairs.
Historically, around half of articles in U.S. local newspapers came from
newswires like the Associated Press. While local papers reproduced articles
from the newswire, they wrote their own headlines, which form abstractive
summaries of the associated articles. We associate articles and their headlines
by exploiting document layouts and language understanding. We then use deep
neural methods to detect which articles are from the same underlying source, in
the presence of substantial noise and abridgement. The headlines of reproduced
articles form positive semantic similarity pairs. The resulting publicly
available HEADLINES dataset is significantly larger than most existing semantic
similarity datasets and covers a much longer span of time. It will facilitate
the application of contrastively trained semantic similarity models to a
variety of tasks, including the study of semantic change across space and time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silcock_E/0/1/0/all/0/1&quot;&gt;Emily Silcock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dell_M/0/1/0/all/0/1&quot;&gt;Melissa Dell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07134">
<title>Natural Language is All a Graph Needs. (arXiv:2308.07134v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07134</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of large-scale pre-trained language models, such as ChatGPT,
has revolutionized various research fields in artificial intelligence.
Transformers-based large language models (LLMs) have gradually replaced CNNs
and RNNs to unify fields of computer vision and natural language processing.
Compared with the data that exists relatively independently such as images,
videos or texts, graph is a type of data that contains rich structural and
relational information. Meanwhile, natural language, as one of the most
expressive mediums, excels in describing complex structures. However, existing
work on incorporating graph learning problems into the generative language
modeling framework remains very limited. As the importance of large language
models continues to grow, it becomes essential to explore whether LLMs can also
replace GNNs as the foundation model for graphs. In this paper, we propose
InstructGLM (Instruction-finetuned Graph Language Model), systematically design
highly scalable prompts based on natural language instructions, and use natural
language to describe the geometric structure and node features of the graph for
instruction tuning an LLM to perform learning and inference on graphs in a
generative manner. Our method exceeds all competitive GNN baselines on
ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of
our method and sheds light on generative large language models as the
foundation model for graph machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1&quot;&gt;Ruosong Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Caiqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Runhui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08043">
<title>DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue. (arXiv:2308.08043v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08043</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs), such as ChatGPT, are becoming increasingly
sophisticated, demonstrating capabilities that closely resemble those of
humans. These AI models are playing an essential role in assisting humans with
a wide array of tasks in daily life. A significant application of AI is its use
as a chat agent, responding to human inquiries across various domains. Current
LLMs have shown proficiency in answering general questions. However, basic
question-answering dialogue often falls short in complex diagnostic scenarios,
such as legal or medical consultations. These scenarios typically necessitate
Task-Oriented Dialogue (TOD), wherein an AI chat agent needs to proactively
pose questions and guide users towards specific task completion. Previous
fine-tuning models have underperformed in TOD, and current LLMs do not
inherently possess this capability. In this paper, we introduce DiagGPT
(Dialogue in Diagnosis GPT), an innovative method that extends LLMs to TOD
scenarios. Our experiments reveal that DiagGPT exhibits outstanding performance
in conducting TOD with users, demonstrating its potential for practical
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1&quot;&gt;Lang Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11596">
<title>SeamlessM4T-Massively Multilingual &amp; Multimodal Machine Translation. (arXiv:2308.11596v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11596</link>
<description rdf:parseType="Literal">&lt;p&gt;What does it take to create the Babel Fish, a tool that can help individuals
translate speech between any two languages? While recent breakthroughs in
text-based models have pushed machine translation coverage beyond 200
languages, unified speech-to-speech translation models have yet to achieve
similar strides. More specifically, conventional speech-to-speech translation
systems rely on cascaded systems that perform translation progressively,
putting high-performing unified systems out of reach. To address these gaps, we
introduce SeamlessM4T, a single model that supports speech-to-speech
translation, speech-to-text translation, text-to-speech translation,
text-to-text translation, and automatic speech recognition for up to 100
languages. To build this, we used 1 million hours of open speech audio data to
learn self-supervised speech representations with w2v-BERT 2.0. Subsequently,
we created a multimodal corpus of automatically aligned speech translations.
Filtered and combined with human-labeled and pseudo-labeled data, we developed
the first multilingual system capable of translating from and into English for
both speech and text. On FLEURS, SeamlessM4T sets a new standard for
translations into multiple target languages, achieving an improvement of 20%
BLEU over the previous SOTA in direct speech-to-text translation. Compared to
strong cascaded models, SeamlessM4T improves the quality of into-English
translation by 1.3 BLEU points in speech-to-text and by 2.6 ASR-BLEU points in
speech-to-speech. Tested for robustness, our system performs better against
background noises and speaker variations in speech-to-text tasks compared to
the current SOTA model. Critically, we evaluated SeamlessM4T on gender bias and
added toxicity to assess translation safety. Finally, all contributions in this
work are open-sourced and accessible at
https://github.com/facebookresearch/seamless_communication
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Communication_S/0/1/0/all/0/1&quot;&gt;Seamless Communication&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrault_L/0/1/0/all/0/1&quot;&gt;Lo&amp;#xef;c Barrault&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1&quot;&gt;Yu-An Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meglioli_M/0/1/0/all/0/1&quot;&gt;Mariano Cora Meglioli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dale_D/0/1/0/all/0/1&quot;&gt;David Dale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1&quot;&gt;Ning Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duquenne_P/0/1/0/all/0/1&quot;&gt;Paul-Ambroise Duquenne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elsahar_H/0/1/0/all/0/1&quot;&gt;Hady Elsahar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1&quot;&gt;Hongyu Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heffernan_K/0/1/0/all/0/1&quot;&gt;Kevin Heffernan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1&quot;&gt;John Hoffman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klaiber_C/0/1/0/all/0/1&quot;&gt;Christopher Klaiber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Pengwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Licht_D/0/1/0/all/0/1&quot;&gt;Daniel Licht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maillard_J/0/1/0/all/0/1&quot;&gt;Jean Maillard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakotoarison_A/0/1/0/all/0/1&quot;&gt;Alice Rakotoarison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadagopan_K/0/1/0/all/0/1&quot;&gt;Kaushik Ram Sadagopan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wenzek_G/0/1/0/all/0/1&quot;&gt;Guillaume Wenzek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_E/0/1/0/all/0/1&quot;&gt;Ethan Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akula_B/0/1/0/all/0/1&quot;&gt;Bapi Akula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Peng-Jen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hachem_N/0/1/0/all/0/1&quot;&gt;Naji El Hachem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellis_B/0/1/0/all/0/1&quot;&gt;Brian Ellis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_G/0/1/0/all/0/1&quot;&gt;Gabriel Mejia Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haaheim_J/0/1/0/all/0/1&quot;&gt;Justin Haaheim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hansanti_P/0/1/0/all/0/1&quot;&gt;Prangthip Hansanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Howes_R/0/1/0/all/0/1&quot;&gt;Russ Howes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Bernie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_M/0/1/0/all/0/1&quot;&gt;Min-Jae Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inaguma_H/0/1/0/all/0/1&quot;&gt;Hirofumi Inaguma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Somya Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalbassi_E/0/1/0/all/0/1&quot;&gt;Elahe Kalbassi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kallet_A/0/1/0/all/0/1&quot;&gt;Amanda Kallet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulikov_I/0/1/0/all/0/1&quot;&gt;Ilia Kulikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_J/0/1/0/all/0/1&quot;&gt;Janice Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Daniel Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xutai Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mavlyutov_R/0/1/0/all/0/1&quot;&gt;Ruslan Mavlyutov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peloquin_B/0/1/0/all/0/1&quot;&gt;Benjamin Peloquin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramadan_M/0/1/0/all/0/1&quot;&gt;Mohamed Ramadan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1&quot;&gt;Abinesh Ramakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1&quot;&gt;Anna Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1&quot;&gt;Kevin Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Tuan Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tufanov_I/0/1/0/all/0/1&quot;&gt;Igor Tufanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogeti_V/0/1/0/all/0/1&quot;&gt;Vish Vogeti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wood_C/0/1/0/all/0/1&quot;&gt;Carleigh Wood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yilin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bokai Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andrews_P/0/1/0/all/0/1&quot;&gt;Pierre Andrews&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balioglu_C/0/1/0/all/0/1&quot;&gt;Can Balioglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1&quot;&gt;Marta R. Costa-juss&amp;#xe0;&lt;/a&gt;, et al. (16 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11764">
<title>Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models. (arXiv:2308.11764v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11764</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP). Although convenient for research and practical applications, open-source
LLMs with fewer parameters often suffer from severe hallucinations compared to
their larger counterparts. This paper focuses on measuring and reducing
hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs
that are publicly available for research and commercial applications. We
introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed
to quantify the severity of hallucinations in LLMs. Additionally, we explore
techniques like knowledge injection and teacher-student approaches to alleviate
hallucinations in low-parameter LLMs. Our experiments effectively demonstrate
the reduction of hallucinations in challenging domains for these LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elaraby_M/0/1/0/all/0/1&quot;&gt;Mohamed Elaraby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1&quot;&gt;Mengyin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dunn_J/0/1/0/all/0/1&quot;&gt;Jacob Dunn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xueying Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shizhu Liu&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>