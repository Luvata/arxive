<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-20T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10182" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10194" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10203" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10205" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10237" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10275" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10278" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10284" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10296" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10440" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10455" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10487" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10499" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10504" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10554" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10575" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10584" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10593" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10601" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10603" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10616" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10624" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10625" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10632" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10636" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10638" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10642" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10664" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10667" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10685" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10696" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10697" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10745" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10768" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10784" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10787" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10790" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10816" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10822" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10843" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10853" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10864" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2009.03259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2105.11166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.04066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.04550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.05216" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.09753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.10552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.07902" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.00255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.05335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.06551" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.12361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.14085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.03434" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.04246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.13792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.01928" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.06262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08292" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12384" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09826" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07290" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08396" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07996" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09683" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13960" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14795" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01738" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02347" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05921" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08015" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09023" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09676" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10036" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10165" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.10182">
<title>Enhancing Super-Resolution Networks through Realistic Thick-Slice CT Simulation. (arXiv:2307.10182v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.10182</link>
<description rdf:parseType="Literal">&lt;p&gt;This study aims to develop and evaluate an innovative simulation algorithm
for generating thick-slice CT images that closely resemble actual images in the
AAPM-Mayo&apos;s 2016 Low Dose CT Grand Challenge dataset. The proposed method was
evaluated using Peak Signal-to-Noise Ratio (PSNR) and Root Mean Square Error
(RMSE) metrics, with the hypothesis that our simulation would produce images
more congruent with their real counterparts. Our proposed method demonstrated
substantial enhancements in terms of both PSNR and RMSE over other simulation
methods. The highest PSNR values were obtained with the proposed method,
yielding 49.7369 $\pm$ 2.5223 and 48.5801 $\pm$ 7.3271 for D45 and B30
reconstruction kernels, respectively. The proposed method also registered the
lowest RMSE with values of 0.0068 $\pm$ 0.0020 and 0.0108 $\pm$ 0.0099 for D45
and B30, respectively, indicating a distribution more closely aligned with the
authentic thick-slice image. Further validation of the proposed simulation
algorithm was conducted using the TCIA LDCT-and-Projection-data dataset. The
generated images were then leveraged to train four distinct super-resolution
(SR) models, which were subsequently evaluated using the real thick-slice
images from the 2016 Low Dose CT Grand Challenge dataset. When trained with
data produced by our novel algorithm, all four SR models exhibited enhanced
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xing_X/0/1/0/all/0/1&quot;&gt;Xiaodan Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10194">
<title>Important Clues that Facilitate Visual Emergence: Three Psychological Experiments. (arXiv:2307.10194v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2307.10194</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual emergence is the phenomenon in which the visual system obtains a
holistic perception after grouping and reorganizing local signals. The picture
Dalmatian dog is known for its use in explaining visual emergence. This type of
image, which consists of a set of discrete black speckles (speckles), is called
an emerging image. Not everyone can find the dog in Dalmatian dog, and among
those who can, the time spent varies greatly. Although Gestalt theory
summarizes perceptual organization into several principles, it remains
ambiguous how these principles affect the perception of emerging images. This
study, therefore, designed three psychological experiments to explore the
factors that influence the perception of emerging images. In the first, we
found that the density of speckles in the local area and the arrangements of
some key speckles played a key role in the perception of an emerging case. We
set parameters in the algorithm to characterize these two factors. We then
automatically generated diversified emerging-test images (ETIs) through the
algorithm and verified their effectiveness in two subsequent experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jingmeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hui Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10203">
<title>Combining Vision and EMG-Based Hand Tracking for Extended Reality Musical Instruments. (arXiv:2307.10203v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10203</link>
<description rdf:parseType="Literal">&lt;p&gt;Hand tracking is a critical component of natural user interactions in
extended reality (XR) environments, including extended reality musical
instruments (XRMIs). However, self-occlusion remains a significant challenge
for vision-based hand tracking systems, leading to inaccurate results and
degraded user experiences. In this paper, we propose a multimodal hand tracking
system that combines vision-based hand tracking with surface electromyography
(sEMG) data for finger joint angle estimation. We validate the effectiveness of
our system through a series of hand pose tasks designed to cover a wide range
of gestures, including those prone to self-occlusion. By comparing the
performance of our multimodal system to a baseline vision-based tracking
method, we demonstrate that our multimodal approach significantly improves
tracking accuracy for several finger joints prone to self-occlusion. These
findings suggest that our system has the potential to enhance XR experiences by
providing more accurate and robust hand tracking, even in the presence of
self-occlusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graf_M/0/1/0/all/0/1&quot;&gt;Max Graf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barthet_M/0/1/0/all/0/1&quot;&gt;Mathieu Barthet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10205">
<title>Adversarial Training Over Long-Tailed Distribution. (arXiv:2307.10205v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10205</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study adversarial training on datasets that obey the
long-tailed distribution, which is practical but rarely explored in previous
works. Compared with conventional adversarial training on balanced datasets,
this process falls into the dilemma of generating uneven adversarial examples
(AEs) and an unbalanced feature embedding space, causing the resulting model to
exhibit low robustness and accuracy on tail data. To combat that, we propose a
new adversarial training framework -- Re-balancing Adversarial Training (REAT).
This framework consists of two components: (1) a new training strategy inspired
by the term effective number to guide the model to generate more balanced and
informative AEs; (2) a carefully constructed penalty function to force a
satisfactory feature space. Evaluation results on different datasets and model
structures prove that REAT can effectively enhance the model&apos;s robustness and
preserve the model&apos;s clean accuracy. The code can be found in
https://github.com/GuanlinLee/REAT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1&quot;&gt;Guowen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianwei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10206">
<title>Volumetric Wireframe Parsing from Neural Attraction Fields. (arXiv:2307.10206v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10206</link>
<description rdf:parseType="Literal">&lt;p&gt;The primal sketch is a fundamental representation in Marr&apos;s vision theory,
which allows for parsimonious image-level processing from 2D to 2.5D
perception. This paper takes a further step by computing 3D primal sketch of
wireframes from a set of images with known camera poses, in which we take the
2D wireframes in multi-view images as the basis to compute 3D wireframes in a
volumetric rendering formulation. In our method, we first propose a NEural
Attraction (NEAT) Fields that parameterizes the 3D line segments with
coordinate Multi-Layer Perceptrons (MLPs), enabling us to learn the 3D line
segments from 2D observation without incurring any explicit feature
correspondences across views. We then present a novel Global Junction
Perceiving (GJP) module to perceive meaningful 3D junctions from the NEAT
Fields of 3D line segments by optimizing a randomly initialized
high-dimensional latent array and a lightweight decoding MLP. Benefitting from
our explicit modeling of 3D junctions, we finally compute the primal sketch of
3D wireframes by attracting the queried 3D line segments to the 3D junctions,
significantly simplifying the computation paradigm of 3D wireframe parsing. In
experiments, we evaluate our approach on the DTU and BlendedMVS datasets with
promising performance obtained. As far as we know, our method is the first
approach to achieve high-fidelity 3D wireframe parsing without requiring
explicit matching.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_N/0/1/0/all/0/1&quot;&gt;Nan Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1&quot;&gt;Bin Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yuxi Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1&quot;&gt;Liang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1&quot;&gt;Gui-Song Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tianfu Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10212">
<title>Capsule network with shortcut routing. (arXiv:2307.10212v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10212</link>
<description rdf:parseType="Literal">&lt;p&gt;This study introduces &quot;shortcut routing,&quot; a novel routing mechanism in
capsule networks that addresses computational inefficiencies by directly
activating global capsules from local capsules, eliminating intermediate
layers. An attention-based approach with fuzzy coefficients is also explored
for improved efficiency. Experimental results on Mnist, smallnorb, and affNist
datasets show comparable classification performance, achieving accuracies of
99.52%, 93.91%, and 89.02% respectively. The proposed fuzzy-based and
attention-based routing methods significantly reduce the number of calculations
by 1.42 and 2.5 times compared to EM routing, highlighting their computational
advantages in capsule networks. These findings contribute to the advancement of
efficient and accurate hierarchical pattern representation models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_D/0/1/0/all/0/1&quot;&gt;Dang Thanh Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trong_V/0/1/0/all/0/1&quot;&gt;Vo Hoang Trong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gwang_Hyun_Y/0/1/0/all/0/1&quot;&gt;Yu Gwang-Hyun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Young_K/0/1/0/all/0/1&quot;&gt;Kim Jin-Young&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10233">
<title>RayMVSNet++: Learning Ray-based 1D Implicit Fields for Accurate Multi-View Stereo. (arXiv:2307.10233v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10233</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning-based multi-view stereo (MVS) has by far centered around 3D
convolution on cost volumes. Due to the high computation and memory consumption
of 3D CNN, the resolution of output depth is often considerably limited.
Different from most existing works dedicated to adaptive refinement of cost
volumes, we opt to directly optimize the depth value along each camera ray,
mimicking the range finding of a laser scanner. This reduces the MVS problem to
ray-based depth optimization which is much more light-weight than full cost
volume optimization. In particular, we propose RayMVSNet which learns
sequential prediction of a 1D implicit field along each camera ray with the
zero-crossing point indicating scene depth. This sequential modeling, conducted
based on transformer features, essentially learns the epipolar line search in
traditional multi-view stereo. We devise a multi-task learning for better
optimization convergence and depth accuracy. We found the monotonicity property
of the SDFs along each ray greatly benefits the depth estimation. Our method
ranks top on both the DTU and the Tanks &amp;amp; Temples datasets over all previous
learning-based methods, achieving an overall reconstruction score of 0.33mm on
DTU and an F-score of 59.48% on Tanks &amp;amp; Temples. It is able to produce
high-quality depth estimation and point cloud reconstruction in challenging
scenarios such as objects/scenes with non-textured surface, severe occlusion,
and highly varying depth range. Further, we propose RayMVSNet++ to enhance
contextual feature aggregation for each ray through designing an attentional
gating unit to select semantically relevant neighboring rays within the local
frustum around that ray. RayMVSNet++ achieves state-of-the-art performance on
the ScanNet dataset. In particular, it attains an AbsRel of 0.058m and produces
accurate results on the two subsets of textureless regions and large depth
variation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yifei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_J/0/1/0/all/0/1&quot;&gt;Junhua Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dewen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zhiping Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kai Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10235">
<title>Towards Viewpoint-Invariant Visual Recognition via Adversarial Training. (arXiv:2307.10235v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10235</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual recognition models are not invariant to viewpoint changes in the 3D
world, as different viewing directions can dramatically affect the predictions
given the same object. Although many efforts have been devoted to making neural
networks invariant to 2D image translations and rotations, viewpoint invariance
is rarely investigated. As most models process images in the perspective view,
it is challenging to impose invariance to 3D viewpoint changes based only on 2D
inputs. Motivated by the success of adversarial training in promoting model
robustness, we propose Viewpoint-Invariant Adversarial Training (VIAT) to
improve viewpoint robustness of common image classifiers. By regarding
viewpoint transformation as an attack, VIAT is formulated as a minimax
optimization problem, where the inner maximization characterizes diverse
adversarial viewpoints by learning a Gaussian mixture distribution based on a
new attack GMVFool, while the outer minimization trains a viewpoint-invariant
classifier by minimizing the expected loss over the worst-case adversarial
viewpoint distributions. To further improve the generalization performance, a
distribution sharing strategy is introduced leveraging the transferability of
adversarial viewpoints across objects. Experiments validate the effectiveness
of VIAT in improving the viewpoint robustness of various image classifiers
based on the diversity of adversarial viewpoints generated by GMVFool.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_S/0/1/0/all/0/1&quot;&gt;Shouwei Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jianteng Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1&quot;&gt;Ning Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xingxing Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10237">
<title>CoNAN: Conditional Neural Aggregation Network For Unconstrained Face Feature Fusion. (arXiv:2307.10237v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10237</link>
<description rdf:parseType="Literal">&lt;p&gt;Face recognition from image sets acquired under unregulated and uncontrolled
settings, such as at large distances, low resolutions, varying viewpoints,
illumination, pose, and atmospheric conditions, is challenging. Face feature
aggregation, which involves aggregating a set of N feature representations
present in a template into a single global representation, plays a pivotal role
in such recognition systems. Existing works in traditional face feature
aggregation either utilize metadata or high-dimensional intermediate feature
representations to estimate feature quality for aggregation. However,
generating high-quality metadata or style information is not feasible for
extremely low-resolution faces captured in long-range and high altitude
settings. To overcome these limitations, we propose a feature distribution
conditioning approach called CoNAN for template aggregation. Specifically, our
method aims to learn a context vector conditioned over the distribution
information of the incoming feature set, which is utilized to weigh the
features based on their estimated informativeness. The proposed method produces
state-of-the-art results on long-range unconstrained face recognition datasets
such as BTS, and DroneSURF, validating the advantages of such an aggregation
strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jawade_B/0/1/0/all/0/1&quot;&gt;Bhavin Jawade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohan_D/0/1/0/all/0/1&quot;&gt;Deen Dayal Mohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fedorishin_D/0/1/0/all/0/1&quot;&gt;Dennis Fedorishin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Setlur_S/0/1/0/all/0/1&quot;&gt;Srirangaraj Setlur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Govindaraju_V/0/1/0/all/0/1&quot;&gt;Venu Govindaraju&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10246">
<title>Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey). (arXiv:2307.10246v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2307.10246</link>
<description rdf:parseType="Literal">&lt;p&gt;How does the brain represent different modes of information? Can we design a
system that automatically understands what the user is thinking? Such questions
can be answered by studying brain recordings like functional magnetic resonance
imaging (fMRI). As a first step, the neuroscience community has contributed
several large cognitive neuroscience datasets related to passive
reading/listening/viewing of concept words, narratives, pictures and movies.
Encoding and decoding models using these datasets have also been proposed in
the past two decades. These models serve as additional tools for basic research
in cognitive science and neuroscience. Encoding models aim at generating fMRI
brain representations given a stimulus automatically. They have several
practical applications in evaluating and diagnosing neurological conditions and
thus also help design therapies for brain damage. Decoding models solve the
inverse problem of reconstructing the stimuli given the fMRI. They are useful
for designing brain-machine or brain-computer interfaces. Inspired by the
effectiveness of deep learning models for natural language processing, computer
vision, and speech, recently several neural encoding and decoding models have
been proposed. In this survey, we will first discuss popular representations of
language, vision and speech stimuli, and present a summary of neuroscience
datasets. Further, we will review popular deep learning based encoding and
decoding architectures and note their benefits and limitations. Finally, we
will conclude with a brief summary and discussion about future trends. Given
the large amount of recently published work in the `computational cognitive
neuroscience&apos; community, we believe that this survey nicely organizes the
plethora of work and presents it as a coherent story.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Oota_S/0/1/0/all/0/1&quot;&gt;Subba Reddy Oota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gupta_M/0/1/0/all/0/1&quot;&gt;Manish Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bapi_R/0/1/0/all/0/1&quot;&gt;Raju S. Bapi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jobard_G/0/1/0/all/0/1&quot;&gt;Gael Jobard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Alexandre_F/0/1/0/all/0/1&quot;&gt;Frederic Alexandre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hinaut_X/0/1/0/all/0/1&quot;&gt;Xavier Hinaut&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10249">
<title>RCM-Fusion: Radar-Camera Multi-Level Fusion for 3D Object Detection. (arXiv:2307.10249v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10249</link>
<description rdf:parseType="Literal">&lt;p&gt;While LiDAR sensors have been succesfully applied to 3D object detection, the
affordability of radar and camera sensors has led to a growing interest in
fusiong radars and cameras for 3D object detection. However, previous
radar-camera fusion models have not been able to fully utilize radar
information in that initial 3D proposals were generated based on the camera
features only and the instance-level fusion is subsequently conducted. In this
paper, we propose radar-camera multi-level fusion (RCM-Fusion), which fuses
radar and camera modalities at both the feature-level and instance-level to
fully utilize radar information. At the feature-level, we propose a Radar
Guided BEV Encoder which utilizes radar Bird&apos;s-Eye-View (BEV) features to
transform image features into precise BEV representations and then adaptively
combines the radar and camera BEV features. At the instance-level, we propose a
Radar Grid Point Refinement module that reduces localization error by
considering the characteristics of the radar point clouds. The experiments
conducted on the public nuScenes dataset demonstrate that our proposed
RCM-Fusion offers 11.8% performance gain in nuScenes detection score (NDS) over
the camera-only baseline model and achieves state-of-the-art performaces among
radar-camera fusion methods in the nuScenes 3D object detection benchmark. Code
will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jisong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seong_M/0/1/0/all/0/1&quot;&gt;Minjae Seong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bang_G/0/1/0/all/0/1&quot;&gt;Geonho Bang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kum_D/0/1/0/all/0/1&quot;&gt;Dongsuk Kum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jun Won Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10267">
<title>On the Real-Time Semantic Segmentation of Aphid Clusters in the Wild. (arXiv:2307.10267v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10267</link>
<description rdf:parseType="Literal">&lt;p&gt;Aphid infestations can cause extensive damage to wheat and sorghum fields and
spread plant viruses, resulting in significant yield losses in agriculture. To
address this issue, farmers often rely on chemical pesticides, which are
inefficiently applied over large areas of fields. As a result, a considerable
amount of pesticide is wasted on areas without pests, while inadequate amounts
are applied to areas with severe infestations. The paper focuses on the urgent
need for an intelligent autonomous system that can locate and spray
infestations within complex crop canopies, reducing pesticide use and
environmental impact. We have collected and labeled a large aphid image dataset
in the field, and propose the use of real-time semantic segmentation models to
segment clusters of aphids. A multiscale dataset is generated to allow for
learning the clusters at different scales. We compare the segmentation speeds
and accuracy of four state-of-the-art real-time semantic segmentation models on
the aphid cluster dataset, benchmarking them against nonreal-time models. The
study results show the effectiveness of a real-time solution, which can reduce
inefficient pesticide use and increase crop yields, paving the way towards an
autonomous pest detection system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_R/0/1/0/all/0/1&quot;&gt;Raiyan Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Indris_C/0/1/0/all/0/1&quot;&gt;Christopher Indris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianxiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kaidong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCornack_B/0/1/0/all/0/1&quot;&gt;Brian McCornack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flippo_D/0/1/0/all/0/1&quot;&gt;Daniel Flippo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharda_A/0/1/0/all/0/1&quot;&gt;Ajay Sharda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guanghui Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10275">
<title>Survey on Controlable Image Synthesis with Deep Learning. (arXiv:2307.10275v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10275</link>
<description rdf:parseType="Literal">&lt;p&gt;Image synthesis has attracted emerging research interests in academic and
industry communities. Deep learning technologies especially the generative
models greatly inspired controllable image synthesis approaches and
applications, which aim to generate particular visual contents with latent
prompts. In order to further investigate low-level controllable image synthesis
problem which is crucial for fine image rendering and editing tasks, we present
a survey of some recent works on 3D controllable image synthesis using deep
learning. We first introduce the datasets and evaluation indicators for 3D
controllable image synthesis. Then, we review the state-of-the-art research for
geometrically controllable image synthesis in two aspects: 1)
Viewpoint/pose-controllable image synthesis; 2) Structure/shape-controllable
image synthesis. Furthermore, the photometrically controllable image synthesis
approaches are also reviewed for 3D re-lighting researches. While the emphasis
is on 3D controllable image synthesis algorithms, the related applications,
products and resources are also briefly summarized for practitioners.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shixiong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10278">
<title>Reclaiming the Horizon: Novel Visualization Designs for Time-Series Data with Large Value Ranges. (arXiv:2307.10278v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.10278</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce two novel visualization designs to support practitioners in
performing identification and discrimination tasks on large value ranges (i.e.,
several orders of magnitude) in time-series data: (1) The order of magnitude
horizon graph, which extends the classic horizon graph; and (2) the order of
magnitude line chart, which adapts the log-line chart. These new visualization
designs visualize large value ranges by explicitly splitting the mantissa m and
exponent e of a value v = m * 10e . We evaluate our novel designs against the
most relevant state-of-the-art visualizations in an empirical user study. It
focuses on four main tasks commonly employed in the analysis of time-series and
large value ranges visualization: identification, discrimination, estimation,
and trend detection. For each task we analyse error, confidence, and response
time. The new order of magnitude horizon graph performs better or equal to all
other designs in identification, discrimination, and estimation tasks. Only for
trend detection tasks, the more traditional horizon graphs reported better
performance. Our results are domain-independent, only requiring time-series
data with large value ranges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braun_D/0/1/0/all/0/1&quot;&gt;Daniel Braun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borgo_R/0/1/0/all/0/1&quot;&gt;Rita Borgo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sondag_M/0/1/0/all/0/1&quot;&gt;Max Sondag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Landesberger_T/0/1/0/all/0/1&quot;&gt;Tatiana von Landesberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10281">
<title>Semi-supervised Cycle-GAN for face photo-sketch translation in the wild. (arXiv:2307.10281v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10281</link>
<description rdf:parseType="Literal">&lt;p&gt;The performance of face photo-sketch translation has improved a lot thanks to
deep neural networks. GAN based methods trained on paired images can produce
high-quality results under laboratory settings. Such paired datasets are,
however, often very small and lack diversity. Meanwhile, Cycle-GANs trained
with unpaired photo-sketch datasets suffer from the \emph{steganography}
phenomenon, which makes them not effective to face photos in the wild. In this
paper, we introduce a semi-supervised approach with a noise-injection strategy,
named Semi-Cycle-GAN (SCG), to tackle these problems. For the first problem, we
propose a {\em pseudo sketch feature} representation for each input photo
composed from a small reference set of photo-sketch pairs, and use the
resulting {\em pseudo pairs} to supervise a photo-to-sketch generator
$G_{p2s}$. The outputs of $G_{p2s}$ can in turn help to train a sketch-to-photo
generator $G_{s2p}$ in a self-supervised manner. This allows us to train
$G_{p2s}$ and $G_{s2p}$ using a small reference set of photo-sketch pairs
together with a large face photo dataset (without ground-truth sketches). For
the second problem, we show that the simple noise-injection strategy works well
to alleviate the \emph{steganography} effect in SCG and helps to produce more
reasonable sketch-to-photo results with less overfitting than fully supervised
approaches. Experiments show that SCG achieves competitive performance on
public benchmarks and superior results on photos in the wild.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaofeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1&quot;&gt;Xiao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1&quot;&gt;Kwan-Yee K. Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10284">
<title>ECSIC: Epipolar Cross Attention for Stereo Image Compression. (arXiv:2307.10284v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.10284</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present ECSIC, a novel learned method for stereo image
compression. Our proposed method compresses the left and right images in a
joint manner by exploiting the mutual information between the images of the
stereo image pair using a novel stereo cross attention (SCA) module and two
stereo context modules. The SCA module performs cross-attention restricted to
the corresponding epipolar lines of the two images and processes them in
parallel. The stereo context modules improve the entropy estimation of the
second encoded image by using the first image as a context. We conduct an
extensive ablation study demonstrating the effectiveness of the proposed
modules and a comprehensive quantitative and qualitative comparison with
existing methods. ECSIC achieves state-of-the-art performance among stereo
image compression models on the two popular stereo image datasets Cityscapes
and InStereo2k while allowing for fast encoding and decoding, making it highly
practical for real-time applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wodlinger_M/0/1/0/all/0/1&quot;&gt;Matthias W&amp;#xf6;dlinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kotera_J/0/1/0/all/0/1&quot;&gt;Jan Kotera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Keglevic_M/0/1/0/all/0/1&quot;&gt;Manuel Keglevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sablatnig_R/0/1/0/all/0/1&quot;&gt;Robert Sablatnig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10296">
<title>Towards Automated Semantic Segmentation in Mammography Images. (arXiv:2307.10296v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.10296</link>
<description rdf:parseType="Literal">&lt;p&gt;Mammography images are widely used to detect non-palpable breast lesions or
nodules, preventing cancer and providing the opportunity to plan interventions
when necessary. The identification of some structures of interest is essential
to make a diagnosis and evaluate image adequacy. Thus, computer-aided detection
systems can be helpful in assisting medical interpretation by automatically
segmenting these landmark structures. In this paper, we propose a deep
learning-based framework for the segmentation of the nipple, the pectoral
muscle, the fibroglandular tissue, and the fatty tissue on standard-view
mammography images. We introduce a large private segmentation dataset and
extensive experiments considering different deep-learning model architectures.
Our experiments demonstrate accurate segmentation performance on variate and
challenging cases, showing that this framework can be integrated into clinical
practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sierra_Franco_C/0/1/0/all/0/1&quot;&gt;Cesar A. Sierra-Franco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hurtado_J/0/1/0/all/0/1&quot;&gt;Jan Hurtado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thomaz_V/0/1/0/all/0/1&quot;&gt;Victor de A. Thomaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cruz_L/0/1/0/all/0/1&quot;&gt;Leonardo C. da Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Silva_S/0/1/0/all/0/1&quot;&gt;Santiago V. Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raposo_A/0/1/0/all/0/1&quot;&gt;Alberto B. Raposo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10305">
<title>Tapestry of Time and Actions: Modeling Human Activity Sequences using Temporal Point Process Flows. (arXiv:2307.10305v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10305</link>
<description rdf:parseType="Literal">&lt;p&gt;Human beings always engage in a vast range of activities and tasks that
demonstrate their ability to adapt to different scenarios. Any human activity
can be represented as a temporal sequence of actions performed to achieve a
certain goal. Unlike the time series datasets extracted from electronics or
machines, these action sequences are highly disparate in their nature -- the
time to finish a sequence of actions can vary between different persons.
Therefore, understanding the dynamics of these sequences is essential for many
downstream tasks such as activity length prediction, goal prediction, next
action recommendation, etc. Existing neural network-based approaches that learn
a continuous-time activity sequence (or CTAS) are limited to the presence of
only visual data or are designed specifically for a particular task, i.e.,
limited to next action or goal prediction. In this paper, we present ProActive,
a neural marked temporal point process (MTPP) framework for modeling the
continuous-time distribution of actions in an activity sequence while
simultaneously addressing three high-impact problems -- next action prediction,
sequence-goal prediction, and end-to-end sequence generation. Specifically, we
utilize a self-attention module with temporal normalizing flows to model the
influence and the inter-arrival times between actions in a sequence. In
addition, we propose a novel addition over the ProActive model that can handle
variations in the order of actions, i.e., different methods of achieving a
given goal. We demonstrate that this variant can learn the order in which the
person or actor prefers to do their actions. Extensive experiments on sequences
derived from three activity recognition datasets show the significant accuracy
boost of ProActive over the state-of-the-art in terms of action and goal
prediction, and the first-ever application of end-to-end action sequence
generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1&quot;&gt;Vinayak Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bedathur_S/0/1/0/all/0/1&quot;&gt;Srikanta Bedathur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10316">
<title>CPCM: Contextual Point Cloud Modeling for Weakly-supervised Point Cloud Semantic Segmentation. (arXiv:2307.10316v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10316</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the task of weakly-supervised point cloud semantic segmentation with
sparse annotations (e.g., less than 0.1% points are labeled), aiming to reduce
the expensive cost of dense annotations. Unfortunately, with extremely sparse
annotated points, it is very difficult to extract both contextual and object
information for scene understanding such as semantic segmentation. Motivated by
masked modeling (e.g., MAE) in image and video representation learning, we seek
to endow the power of masked modeling to learn contextual information from
sparsely-annotated points. However, directly applying MAE to 3D point clouds
with sparse annotations may fail to work. First, it is nontrivial to
effectively mask out the informative visual context from 3D point clouds.
Second, how to fully exploit the sparse annotations for context modeling
remains an open question. In this paper, we propose a simple yet effective
Contextual Point Cloud Modeling (CPCM) method that consists of two parts: a
region-wise masking (RegionMask) strategy and a contextual masked training
(CMT) method. Specifically, RegionMask masks the point cloud continuously in
geometric space to construct a meaningful masked prediction task for subsequent
context learning. CMT disentangles the learning of supervised segmentation and
unsupervised masked context prediction for effectively learning the very
limited labeled points and mass unlabeled points, respectively. Extensive
experiments on the widely-tested ScanNet V2 and S3DIS benchmarks demonstrate
the superiority of CPCM over the state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lizhao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1&quot;&gt;Zhuangwei Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shangxin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xunlong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tianhang Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingdong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1&quot;&gt;Mingkui Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10334">
<title>Mitigating Viewer Impact from Disturbing Imagery using AI Filters: A User-Study. (arXiv:2307.10334v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10334</link>
<description rdf:parseType="Literal">&lt;p&gt;Exposure to disturbing imagery can significantly impact individuals,
especially professionals who encounter such content as part of their work. This
paper presents a user study, involving 107 participants, predominantly
journalists and human rights investigators, that explores the capability of
Artificial Intelligence (AI)-based image filters to potentially mitigate the
emotional impact of viewing such disturbing content. We tested five different
filter styles, both traditional (Blurring and Partial Blurring) and AI-based
(Drawing, Colored Drawing, and Painting), and measured their effectiveness in
terms of conveying image information while reducing emotional distress. Our
findings suggest that the AI-based Drawing style filter demonstrates the best
performance, offering a promising solution for reducing negative feelings
(-30.38%) while preserving the interpretability of the image (97.19%). Despite
the requirement for many professionals to eventually inspect the original
images, participants suggested potential strategies for integrating AI filters
into their workflow, such as using AI filters as an initial, preparatory step
before viewing the original image. Overall, this paper contributes to the
development of a more ethically considerate and effective visual environment
for professionals routinely engaging with potentially disturbing imagery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarridis_I/0/1/0/all/0/1&quot;&gt;Ioannis Sarridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spangenberg_J/0/1/0/all/0/1&quot;&gt;Jochen Spangenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadopoulou_O/0/1/0/all/0/1&quot;&gt;Olga Papadopoulou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1&quot;&gt;Symeon Papadopoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10350">
<title>Improving Multimodal Datasets with Image Captioning. (arXiv:2307.10350v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10350</link>
<description rdf:parseType="Literal">&lt;p&gt;Massive web datasets play a key role in the success of large vision-language
models like CLIP and Flamingo. However, the raw web data is noisy, and existing
filtering methods to reduce noise often come at the expense of data diversity.
Our work focuses on caption quality as one major source of noise, and studies
how generated captions can increase the utility of web-scraped datapoints with
nondescript text. Through exploring different mixing strategies for raw and
generated captions, we outperform the best filtering method proposed by the
DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a
candidate pool of 128M image-text pairs. Our best approach is also 2x better at
Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an
effective source of text supervision. In experimenting with different image
captioning models, we also demonstrate that the performance of a model on
standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliable
indicator of the utility of the captions it generates for multimodal training.
Finally, our experiments with using generated captions at DataComp&apos;s large
scale (1.28B image-text pairs) offer insights into the limitations of synthetic
text, as well as the importance of image curation with increasing training data
quantity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thao Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1&quot;&gt;Samir Yitzhak Gadre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1&quot;&gt;Gabriel Ilharco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Sewoong Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10373">
<title>TokenFlow: Consistent Diffusion Features for Consistent Video Editing. (arXiv:2307.10373v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10373</link>
<description rdf:parseType="Literal">&lt;p&gt;The generative AI revolution has recently expanded to videos. Nevertheless,
current state-of-the-art video models are still lagging behind image models in
terms of visual quality and user control over the generated content. In this
work, we present a framework that harnesses the power of a text-to-image
diffusion model for the task of text-driven video editing. Specifically, given
a source video and a target text-prompt, our method generates a high-quality
video that adheres to the target text, while preserving the spatial layout and
motion of the input video. Our method is based on a key observation that
consistency in the edited video can be obtained by enforcing consistency in the
diffusion feature space. We achieve this by explicitly propagating diffusion
features based on inter-frame correspondences, readily available in the model.
Thus, our framework does not require any training or fine-tuning, and can work
in conjunction with any off-the-shelf text-to-image editing method. We
demonstrate state-of-the-art editing results on a variety of real-world videos.
Webpage: https://diffusion-tokenflow.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geyer_M/0/1/0/all/0/1&quot;&gt;Michal Geyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bar_Tal_O/0/1/0/all/0/1&quot;&gt;Omer Bar-Tal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagon_S/0/1/0/all/0/1&quot;&gt;Shai Bagon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dekel_T/0/1/0/all/0/1&quot;&gt;Tali Dekel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10387">
<title>POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities. (arXiv:2307.10387v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10387</link>
<description rdf:parseType="Literal">&lt;p&gt;The surgical usage of Mixed Reality (MR) has received growing attention in
areas such as surgical navigation systems, skill assessment, and robot-assisted
surgeries. For such applications, pose estimation for hand and surgical
instruments from an egocentric perspective is a fundamental task and has been
studied extensively in the computer vision field in recent years. However, the
development of this field has been impeded by a lack of datasets, especially in
the surgical field, where bloody gloves and reflective metallic tools make it
hard to obtain 3D pose annotations for hands and objects using conventional
methods. To address this issue, we propose POV-Surgery, a large-scale,
synthetic, egocentric dataset focusing on pose estimation for hands with
different surgical gloves and three orthopedic surgical instruments, namely
scalpel, friem, and diskplacer. Our dataset consists of 53 sequences and 88,329
frames, featuring high-resolution RGB-D video streams with activity
annotations, accurate 3D and 2D annotations for hand-object pose, and 2D
hand-object segmentation masks. We fine-tune the current SOTA methods on
POV-Surgery and further show the generalizability when applying to real-life
cases with surgical gloves and tools by extensive evaluations. The code and the
dataset are publicly available at batfacewayne.github.io/POV_Surgery_io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ktistakis_S/0/1/0/all/0/1&quot;&gt;Sophokles Ktistakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Siwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meboldt_M/0/1/0/all/0/1&quot;&gt;Mirko Meboldt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lohmeyer_Q/0/1/0/all/0/1&quot;&gt;Quentin Lohmeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10404">
<title>Interpreting and Correcting Medical Image Classification with PIP-Net. (arXiv:2307.10404v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10404</link>
<description rdf:parseType="Literal">&lt;p&gt;Part-prototype models are explainable-by-design image classifiers, and a
promising alternative to black box AI. This paper explores the applicability
and potential of interpretable machine learning, in particular PIP-Net, for
automated diagnosis support on real-world medical imaging data. PIP-Net learns
human-understandable prototypical image parts and we evaluate its accuracy and
interpretability for fracture detection and skin cancer diagnosis. We find that
PIP-Net&apos;s decision making process is in line with medical classification
standards, while only provided with image-level class labels. Because of
PIP-Net&apos;s unsupervised pretraining of prototypes, data quality problems such as
undesired text in an X-ray or labelling errors can be easily identified.
Additionally, we are the first to show that humans can manually correct the
reasoning of PIP-Net by directly disabling undesired prototypes. We conclude
that part-prototype models are promising for medical applications due to their
interpretability and potential for advanced model debugging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nauta_M/0/1/0/all/0/1&quot;&gt;Meike Nauta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hegeman_J/0/1/0/all/0/1&quot;&gt;Johannes H. Hegeman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geerdink_J/0/1/0/all/0/1&quot;&gt;Jeroen Geerdink&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlotterer_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg Schl&amp;#xf6;tterer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keulen_M/0/1/0/all/0/1&quot;&gt;Maurice van Keulen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seifert_C/0/1/0/all/0/1&quot;&gt;Christin Seifert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10405">
<title>Generative Visual Question Answering. (arXiv:2307.10405v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10405</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal tasks involving vision and language in deep learning continue to
rise in popularity and are leading to the development of newer models that can
generalize beyond the extent of their training data. The current models lack
temporal generalization which enables models to adapt to changes in future
data. This paper discusses a viable approach to creating an advanced Visual
Question Answering (VQA) model which can produce successful results on temporal
generalization. We propose a new data set, GenVQA, utilizing images and
captions from the VQAv2 and MS-COCO dataset to generate new images through
stable diffusion. This augmented dataset is then used to test a combination of
seven baseline and cutting edge VQA models. Performance evaluation focuses on
questions mirroring the original VQAv2 dataset, with the answers having been
adjusted to the new images. This paper&apos;s purpose is to investigate the
robustness of several successful VQA models to assess their performance on
future data distributions. Model architectures are analyzed to identify common
stylistic choices that improve generalization under temporal distribution
shifts. This research highlights the importance of creating a large-scale
future shifted dataset. This data can enhance the robustness of VQA models,
allowing their future peers to have improved ability to adapt to temporal
distribution shifts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_E/0/1/0/all/0/1&quot;&gt;Ethan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Scotty Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_B/0/1/0/all/0/1&quot;&gt;Bhavesh Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10408">
<title>Explaining Autonomous Driving Actions with Visual Question Answering. (arXiv:2307.10408v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10408</link>
<description rdf:parseType="Literal">&lt;p&gt;The end-to-end learning ability of self-driving vehicles has achieved
significant milestones over the last decade owing to rapid advances in deep
learning and computer vision algorithms. However, as autonomous driving
technology is a safety-critical application of artificial intelligence (AI),
road accidents and established regulatory principles necessitate the need for
the explainability of intelligent action choices for self-driving vehicles. To
facilitate interpretability of decision-making in autonomous driving, we
present a Visual Question Answering (VQA) framework, which explains driving
actions with question-answering-based causal reasoning. To do so, we first
collect driving videos in a simulation environment using reinforcement learning
(RL) and extract consecutive frames from this log data uniformly for five
selected action categories. Further, we manually annotate the extracted frames
using question-answer pairs as justifications for the actions chosen in each
scenario. Finally, we evaluate the correctness of the VQA-predicted answers for
actions on unseen driving scenes. The empirical results suggest that the VQA
mechanism can provide support to interpret real-time decisions of autonomous
vehicles and help enhance overall driving safety.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atakishiyev_S/0/1/0/all/0/1&quot;&gt;Shahin Atakishiyev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1&quot;&gt;Mohammad Salameh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babiker_H/0/1/0/all/0/1&quot;&gt;Housam Babiker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1&quot;&gt;Randy Goebel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10422">
<title>PreDiff: Precipitation Nowcasting with Latent Diffusion Models. (arXiv:2307.10422v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10422</link>
<description rdf:parseType="Literal">&lt;p&gt;Earth system forecasting has traditionally relied on complex physical models
that are computationally expensive and require significant domain expertise. In
the past decade, the unprecedented increase in spatiotemporal Earth observation
data has enabled data-driven forecasting models using deep learning techniques.
These models have shown promise for diverse Earth system forecasting tasks but
either struggle with handling uncertainty or neglect domain-specific prior
knowledge, resulting in averaging possible futures to blurred forecasts or
generating physically implausible predictions. To address these limitations, we
propose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1)
We develop PreDiff, a conditional latent diffusion model capable of
probabilistic forecasts. 2) We incorporate an explicit knowledge control
mechanism to align forecasts with domain-specific physical constraints. This is
achieved by estimating the deviation from imposed constraints at each denoising
step and adjusting the transition distribution accordingly. We conduct
empirical studies on two datasets: N-body MNIST, a synthetic dataset with
chaotic behavior, and SEVIR, a real-world precipitation nowcasting dataset.
Specifically, we impose the law of conservation of energy in N-body MNIST and
anticipated precipitation intensity in SEVIR. Experiments demonstrate the
effectiveness of PreDiff in handling uncertainty, incorporating domain-specific
prior knowledge, and generating forecasts that exhibit high operational
utility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zhihan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xingjian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Boran Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xiaoyong Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maddix_D/0/1/0/all/0/1&quot;&gt;Danielle Maddix&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuyang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10440">
<title>Confidence Estimation Using Unlabeled Data. (arXiv:2307.10440v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10440</link>
<description rdf:parseType="Literal">&lt;p&gt;Overconfidence is a common issue for deep neural networks, limiting their
deployment in real-world applications. To better estimate confidence, existing
methods mostly focus on fully-supervised scenarios and rely on training labels.
In this paper, we propose the first confidence estimation method for a
semi-supervised setting, when most training labels are unavailable. We
stipulate that even with limited training labels, we can still reasonably
approximate the confidence of model on unlabeled samples by inspecting the
prediction consistency through the training process. We use training
consistency as a surrogate function and propose a consistency ranking loss for
confidence estimation. On both image classification and segmentation tasks, our
method achieves state-of-the-art performances in confidence estimation.
Furthermore, we show the benefit of the proposed method through a downstream
active learning task. The code is available at
https://github.com/TopoXLab/consistency-ranking-loss
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaoling Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10455">
<title>A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset. (arXiv:2307.10455v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10455</link>
<description rdf:parseType="Literal">&lt;p&gt;In an effort to catalog insect biodiversity, we propose a new large dataset
of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is
taxonomically classified by an expert, and also has associated genetic
information including raw nucleotide barcode sequences and assigned barcode
index numbers, which are genetically-based proxies for species classification.
This paper presents a curated million-image dataset, primarily to train
computer-vision models capable of providing image-based taxonomic assessment,
however, the dataset also presents compelling characteristics, the study of
which would be of interest to the broader machine learning community. Driven by
the biological nature inherent to the dataset, a characteristic long-tailed
class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is
a hierarchical classification scheme, presenting a highly fine-grained
classification problem at lower levels. Beyond spurring interest in
biodiversity research within the machine learning community, progress on
creating an image-based taxonomic classifier will also further the ultimate
goal of all BIOSCAN research: to lay the foundation for a comprehensive survey
of global biodiversity. This paper introduces the dataset and explores the
classification task through the implementation and analysis of a baseline
classifier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1&quot;&gt;Zahra Gharaee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1&quot;&gt;ZeMing Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pellegrino_N/0/1/0/all/0/1&quot;&gt;Nicholas Pellegrino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zarubiieva_I/0/1/0/all/0/1&quot;&gt;Iuliia Zarubiieva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haurum_J/0/1/0/all/0/1&quot;&gt;Joakim Bruslund Haurum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lowe_S/0/1/0/all/0/1&quot;&gt;Scott C. Lowe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McKeown_J/0/1/0/all/0/1&quot;&gt;Jaclyn T.A. McKeown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1&quot;&gt;Chris C.Y. Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McLeod_J/0/1/0/all/0/1&quot;&gt;Joschka McLeod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yi-Yun C Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agda_J/0/1/0/all/0/1&quot;&gt;Jireh Agda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratnasingham_S/0/1/0/all/0/1&quot;&gt;Sujeevan Ratnasingham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinke_D/0/1/0/all/0/1&quot;&gt;Dirk Steinke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1&quot;&gt;Angel X. Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1&quot;&gt;Graham W. Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fieguth_P/0/1/0/all/0/1&quot;&gt;Paul Fieguth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10471">
<title>Classification of Visualization Types and Perspectives in Patents. (arXiv:2307.10471v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10471</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the swift growth of patent applications each year, information and
multimedia retrieval approaches that facilitate patent exploration and
retrieval are of utmost importance. Different types of visualizations (e.g.,
graphs, technical drawings) and perspectives (e.g., side view, perspective) are
used to visualize details of innovations in patents. The classification of
these images enables a more efficient search and allows for further analysis.
So far, datasets for image type classification miss some important
visualization types for patents. Furthermore, related work does not make use of
recent deep learning approaches including transformers. In this paper, we adopt
state-of-the-art deep learning methods for the classification of visualization
types and perspectives in patent images. We extend the CLEF-IP dataset for
image type classification in patents to ten classes and provide manual ground
truth annotations. In addition, we derive a set of hierarchical classes from a
dataset that provides weakly-labeled data for image perspectives. Experimental
results have demonstrated the feasibility of the proposed approaches. Source
code, models, and dataset will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghauri_J/0/1/0/all/0/1&quot;&gt;Junaid Ahmed Ghauri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_Budack_E/0/1/0/all/0/1&quot;&gt;Eric M&amp;#xfc;ller-Budack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1&quot;&gt;Ralph Ewerth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10475">
<title>Findings of Factify 2: Multimodal Fake News Detection. (arXiv:2307.10475v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.10475</link>
<description rdf:parseType="Literal">&lt;p&gt;With social media usage growing exponentially in the past few years, fake
news has also become extremely prevalent. The detrimental impact of fake news
emphasizes the need for research focused on automating the detection of false
information and verifying its accuracy. In this work, we present the outcome of
the Factify 2 shared task, which provides a multi-modal fact verification and
satire news dataset, as part of the DeFactify 2 workshop at AAAI&apos;23. The data
calls for a comparison based approach to the task by pairing social media
claims with supporting documents, with both text and image, divided into 5
classes based on multi-modal relations. In the second iteration of this task we
had over 60 participants and 9 final test-set submissions. The best
performances came from the use of DeBERTa for text and Swinv2 and CLIP for
image. The highest F1 score averaged for all five classes was 81.82%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suryavardan_S/0/1/0/all/0/1&quot;&gt;S Suryavardan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Shreyash Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_M/0/1/0/all/0/1&quot;&gt;Megha Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1&quot;&gt;Parth Patwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rani_A/0/1/0/all/0/1&quot;&gt;Anku Rani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1&quot;&gt;Aman Chadha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reganti_A/0/1/0/all/0/1&quot;&gt;Aishwarya Reganti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Amitava Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1&quot;&gt;Amit Sheth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chinnakotla_M/0/1/0/all/0/1&quot;&gt;Manoj Chinnakotla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1&quot;&gt;Asif Ekbal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Srijan Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10487">
<title>Backdoor Attack against Object Detection with Clean Annotation. (arXiv:2307.10487v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10487</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have shown unprecedented success in object
detection tasks. However, it was also discovered that DNNs are vulnerable to
multiple kinds of attacks, including Backdoor Attacks. Through the attack, the
attacker manages to embed a hidden backdoor into the DNN such that the model
behaves normally on benign data samples, but makes attacker-specified judgments
given the occurrence of a predefined trigger. Although numerous backdoor
attacks have been experimented on image classification, backdoor attacks on
object detection tasks have not been properly investigated and explored. As
object detection has been adopted as an important module in multiple
security-sensitive applications such as autonomous driving, backdoor attacks on
object detection could pose even more severe threats. Inspired by the inherent
property of deep learning-based object detectors, we propose a simple yet
effective backdoor attack method against object detection without modifying the
ground truth annotations, specifically focusing on the object disappearance
attack and object generation attack. Extensive experiments and ablation studies
prove the effectiveness of our attack on two benchmark object detection
datasets, PASCAL VOC07+12 and MSCOCO, on which we achieve an attack success
rate of more than 92% with a poison rate of only 5%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yize Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenbin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Minhao Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10495">
<title>Novel Batch Active Learning Approach and Its Application to Synthetic Aperture Radar Datasets. (arXiv:2307.10495v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10495</link>
<description rdf:parseType="Literal">&lt;p&gt;Active learning improves the performance of machine learning methods by
judiciously selecting a limited number of unlabeled data points to query for
labels, with the aim of maximally improving the underlying classifier&apos;s
performance. Recent gains have been made using sequential active learning for
synthetic aperture radar (SAR) data &lt;a href=&quot;/abs/2204.00005&quot;&gt;arXiv:2204.00005&lt;/a&gt;. In each iteration,
sequential active learning selects a query set of size one while batch active
learning selects a query set of multiple datapoints. While batch active
learning methods exhibit greater efficiency, the challenge lies in maintaining
model accuracy relative to sequential active learning methods. We developed a
novel, two-part approach for batch active learning: Dijkstra&apos;s Annulus Core-Set
(DAC) for core-set generation and LocalMax for batch sampling. The batch active
learning process that combines DAC and LocalMax achieves nearly identical
accuracy as sequential active learning but is more efficient, proportional to
the batch size. As an application, a pipeline is built based on transfer
learning feature embedding, graph learning, DAC, and LocalMax to classify the
FUSAR-Ship and OpenSARShip datasets. Our pipeline outperforms the
state-of-the-art CNN-based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chapman_J/0/1/0/all/0/1&quot;&gt;James Chapman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bohan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zheng Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calder_J/0/1/0/all/0/1&quot;&gt;Jeff Calder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_K/0/1/0/all/0/1&quot;&gt;Kevin Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertozzi_A/0/1/0/all/0/1&quot;&gt;Andrea L. Bertozzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10499">
<title>Mining Conditional Part Semantics with Occluded Extrapolation for Human-Object Interaction Detection. (arXiv:2307.10499v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10499</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-Object Interaction Detection is a crucial aspect of human-centric scene
understanding, with important applications in various domains. Despite recent
progress in this field, recognizing subtle and detailed interactions remains
challenging. Existing methods try to use human-related clues to alleviate the
difficulty, but rely heavily on external annotations or knowledge, limiting
their practical applicability in real-world scenarios. In this work, we propose
a novel Part Semantic Network (PSN) to solve this problem. The core of PSN is a
Conditional Part Attention (CPA) mechanism, where human features are taken as
keys and values, and the object feature is used as query for the computation in
a cross-attention mechanism. In this way, our model learns to automatically
focus on the most informative human parts conditioned on the involved object,
generating more semantically meaningful features for interaction recognition.
Additionally, we propose an Occluded Part Extrapolation (OPE) strategy to
facilitate interaction recognition under occluded scenarios, which teaches the
model to extrapolate detailed features from partially occluded ones. Our method
consistently outperforms prior approaches on the V-COCO and HICO-DET datasets,
without external data or extra annotations. Additional ablation studies
validate the effectiveness of each component of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guangzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yangyang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1&quot;&gt;Mohan Kankanhalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10501">
<title>Eye Disease Classification Using Deep Learning Techniques. (arXiv:2307.10501v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10501</link>
<description rdf:parseType="Literal">&lt;p&gt;Eye is the essential sense organ for vision function. Due to the fact that
certain eye disorders might result in vision loss, it is essential to diagnose
and treat eye diseases early on. By identifying common eye illnesses and
performing an eye check, eye care providers can safeguard patients against
vision loss or blindness. Convolutional neural networks (CNN) and transfer
learning were employed in this study to discriminate between a normal eye and
one with diabetic retinopathy, cataract, or glaucoma disease. Using transfer
learning for multi-class classification, high accuracy was achieved at 94%
while the traditional CNN achieved 84% rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babaqi_T/0/1/0/all/0/1&quot;&gt;Tareq Babaqi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaradat_M/0/1/0/all/0/1&quot;&gt;Manar Jaradat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yildirim_A/0/1/0/all/0/1&quot;&gt;Ayse Erdem Yildirim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Nimer_S/0/1/0/all/0/1&quot;&gt;Saif H. Al-Nimer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Won_D/0/1/0/all/0/1&quot;&gt;Daehan Won&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10504">
<title>Identifying Interpretable Subspaces in Image Representations. (arXiv:2307.10504v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10504</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Automatic Feature Explanation using Contrasting Concepts (FALCON),
an interpretability framework to explain features of image representations. For
a target feature, FALCON captions its highly activating cropped images using a
large captioning dataset (like LAION-400m) and a pre-trained vision-language
model like CLIP. Each word among the captions is scored and ranked leading to a
small number of shared, human-understandable concepts that closely describe the
target feature. FALCON also applies contrastive interpretation using lowly
activating (counterfactual) images, to eliminate spurious concepts. Although
many existing approaches interpret features independently, we observe in
state-of-the-art self-supervised and supervised models, that less than 20% of
the representation space can be explained by individual features. We show that
features in larger spaces become more interpretable when studied in groups and
can be explained with high-order scoring concepts through FALCON. We discuss
how extracted concepts can be used to explain and debug failures in downstream
tasks. Finally, we present a technique to transfer concepts from one
(explainable) representation space to another unseen representation space by
learning a simple linear transformation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalibhat_N/0/1/0/all/0/1&quot;&gt;Neha Kalibhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhardwaj_S/0/1/0/all/0/1&quot;&gt;Shweta Bhardwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruss_B/0/1/0/all/0/1&quot;&gt;Bayan Bruss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1&quot;&gt;Hamed Firooz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1&quot;&gt;Maziar Sanjabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1&quot;&gt;Soheil Feizi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10506">
<title>Is Grad-CAM Explainable in Medical Images?. (arXiv:2307.10506v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.10506</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainable Deep Learning has gained significant attention in the field of
artificial intelligence (AI), particularly in domains such as medical imaging,
where accurate and interpretable machine learning models are crucial for
effective diagnosis and treatment planning. Grad-CAM is a baseline that
highlights the most critical regions of an image used in a deep learning
model&apos;s decision-making process, increasing interpretability and trust in the
results. It is applied in many computer vision (CV) tasks such as
classification and explanation. This study explores the principles of
Explainable Deep Learning and its relevance to medical imaging, discusses
various explainability techniques and their limitations, and examines medical
imaging applications of Grad-CAM. The findings highlight the potential of
Explainable Deep Learning and Grad-CAM in improving the accuracy and
interpretability of deep learning models in medical imaging. The code is
available in (will be available).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Suara_S/0/1/0/all/0/1&quot;&gt;Subhashis Suara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jha_A/0/1/0/all/0/1&quot;&gt;Aayush Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sinha_P/0/1/0/all/0/1&quot;&gt;Pratik Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sekh_A/0/1/0/all/0/1&quot;&gt;Arif Ahmed Sekh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10507">
<title>FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation. (arXiv:2307.10507v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10507</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-silo federated learning (FL) enables the development of machine
learning models on datasets distributed across data centers such as hospitals
and clinical research laboratories. However, recent research has found that
current FL algorithms face a trade-off between local and global performance
when confronted with distribution shifts. Specifically, personalized FL methods
have a tendency to overfit to local data, leading to a sharp valley in the
local model and inhibiting its ability to generalize to out-of-distribution
data. In this paper, we propose a novel federated model soup method (i.e.,
selective interpolation of model parameters) to optimize the trade-off between
local and global performance. Specifically, during the federated training
phase, each client maintains its own global model pool by monitoring the
performance of the interpolated model between the local and global models. This
allows us to alleviate overfitting and seek flat minima, which can
significantly improve the model&apos;s generalization performance. We evaluate our
method on retinal and pathological image classification tasks, and our proposed
method achieves significant improvements for out-of-distribution
generalization. Our code is available at https://github.com/ubc-tea/FedSoup.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Minghui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Meirui Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1&quot;&gt;Qi Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zehua Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10518">
<title>Interactive Segmentation for Diverse Gesture Types Without Context. (arXiv:2307.10518v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10518</link>
<description rdf:parseType="Literal">&lt;p&gt;Interactive segmentation entails a human marking an image to guide how a
model either creates or edits a segmentation. Our work addresses limitations of
existing methods: they either only support one gesture type for marking an
image (e.g., either clicks or scribbles) or require knowledge of the gesture
type being employed, and require specifying whether marked regions should be
included versus excluded in the final segmentation. We instead propose a
simplified interactive segmentation task where a user only must mark an image,
where the input can be of any gesture type without specifying the gesture type.
We support this new task by introducing the first interactive segmentation
dataset with multiple gesture types as well as a new evaluation metric capable
of holistically evaluating interactive segmentation algorithms. We then analyze
numerous interactive segmentation algorithms, including ones adapted for our
novel task. While we observe promising performance overall, we also highlight
areas for future improvement. To facilitate further extensions of this work, we
publicly share our new dataset at https://github.com/joshmyersdean/dig.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myers_Dean_J/0/1/0/all/0/1&quot;&gt;Josh Myers-Dean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yifei Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1&quot;&gt;Brian Price&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1&quot;&gt;Wilson Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurari_D/0/1/0/all/0/1&quot;&gt;Danna Gurari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10549">
<title>Dynamic Large Language Models on Blockchains. (arXiv:2307.10549v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10549</link>
<description rdf:parseType="Literal">&lt;p&gt;Training and deploying the large language models requires a large mount of
computational resource because the language models contain billions of
parameters and the text has thousands of tokens. Another problem is that the
large language models are static. They are fixed after the training process. To
tackle these issues, in this paper, we propose to train and deploy the dynamic
large language model on blockchains, which have high computation performance
and are distributed across a network of computers. A blockchain is a secure,
decentralized, and transparent system that allows for the creation of a
tamper-proof ledger for transactions without the need for intermediaries. The
dynamic large language models can continuously learn from the user input after
the training process. Our method provides a new way to develop the large
language models and also sheds a light on the next generation artificial
intelligence systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yuanhao Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10554">
<title>EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization. (arXiv:2307.10554v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10554</link>
<description rdf:parseType="Literal">&lt;p&gt;Mixed-Precision Quantization~(MQ) can achieve a competitive
accuracy-complexity trade-off for models. Conventional training-based search
methods require time-consuming candidate training to search optimized per-layer
bit-width configurations in MQ. Recently, some training-free approaches have
presented various MQ proxies and significantly improve search efficiency.
However, the correlation between these proxies and quantization accuracy is
poorly understood. To address the gap, we first build the MQ-Bench-101, which
involves different bit configurations and quantization results. Then, we
observe that the existing training-free proxies perform weak correlations on
the MQ-Bench-101. To efficiently seek superior proxies, we develop an automatic
search of proxies framework for MQ via evolving algorithms. In particular, we
devise an elaborate search space involving the existing proxies and perform an
evolution search to discover the best correlated MQ proxy. We proposed a
diversity-prompting selection strategy and compatibility screening protocol to
avoid premature convergence and improve search efficiency. In this way, our
Evolving proxies for Mixed-precision Quantization~(EMQ) framework allows the
auto-generation of proxies without heavy tuning and expert knowledge. Extensive
experiments on ImageNet with various ResNet and MobileNet families demonstrate
that our EMQ obtains superior performance than state-of-the-art mixed-precision
methods at a significantly reduced cost. The code will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1&quot;&gt;Peijie Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lujun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zimian Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1&quot;&gt;Xin Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1&quot;&gt;Zhiliang Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1&quot;&gt;Hengyue Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10567">
<title>No-frills Temporal Video Grounding: Multi-Scale Neighboring Attention and Zoom-in Boundary Detection. (arXiv:2307.10567v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10567</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal video grounding (TVG) aims to retrieve the time interval of a
language query from an untrimmed video. A significant challenge in TVG is the
low &quot;Semantic Noise Ratio (SNR)&quot;, which results in worse performance with lower
SNR. Prior works have addressed this challenge using sophisticated techniques.
In this paper, we propose a no-frills TVG model that consists of two core
modules, namely multi-scale neighboring attention and zoom-in boundary
detection. The multi-scale neighboring attention restricts each video token to
only aggregate visual contexts from its neighbor, enabling the extraction of
the most distinguishing information with multi-scale feature hierarchies from
high-ratio noises. The zoom-in boundary detection then focuses on local-wise
discrimination of the selected top candidates for fine-grained grounding
adjustment. With an end-to-end training strategy, our model achieves
competitive performance on different TVG benchmarks, while also having the
advantage of faster inference speed and lighter model parameters, thanks to its
lightweight architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Sipeng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1&quot;&gt;Qin Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10575">
<title>Boosting Federated Learning Convergence with Prototype Regularization. (arXiv:2307.10575v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10575</link>
<description rdf:parseType="Literal">&lt;p&gt;As a distributed machine learning technique, federated learning (FL) requires
clients to collaboratively train a shared model with an edge server without
leaking their local data. However, the heterogeneous data distribution among
clients often leads to a decrease in model performance. To tackle this issue,
this paper introduces a prototype-based regularization strategy to address the
heterogeneity in the data distribution. Specifically, the regularization
process involves the server aggregating local prototypes from distributed
clients to generate a global prototype, which is then sent back to the
individual clients to guide their local training. The experimental results on
MNIST and Fashion-MNIST show that our proposal achieves improvements of 3.3%
and 8.9% in average test accuracy, respectively, compared to the most popular
baseline FedAvg. Furthermore, our approach has a fast convergence rate in
heterogeneous settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1&quot;&gt;Huy Q. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1&quot;&gt;Choong Seon Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10577">
<title>Ethosight: A Joint-Embedding Based System for Nuanced Perception Using Contextual Label Affinity Metric and Reasoning Based Iterative Learning. (arXiv:2307.10577v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10577</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional computer vision models often require extensive manual effort for
data acquisition and validation, particularly when detecting subtle behavioral
nuances or events. The difficulty in distinguishing routine behaviors from
potential risks in real-world applications, like differentiating routine
shopping from potential shoplifting, further complicates the process.
&lt;/p&gt;
&lt;p&gt;We present Ethosight, a novel zero-shot computer vision algorithm. Ethosight
eradicates the need for pre-existing symbolic knowledge, initiating from a
clean slate based on user requirements and semantic knowledge of interest.
Using localized label affinity calculations and a reasoning-guided iterative
learning loop, Ethosight infers scene details and iteratively refines the label
set. Reasoning mechanisms can be derived from large language models like GPT4,
symbolic reasoners like OpenNARS, or hybrid systems.
&lt;/p&gt;
&lt;p&gt;Ethosight further capitalizes on the capabilities of a pre-trained
multi-modal model, ImageBind, generating accurate semantic knowledge of images
within a few cycles. It successfully captures both explicit and nuanced
elements efficiently. We also introduce the implementation of Korzybski&apos;s
&quot;time-binding&quot; concept in machines, which allows for generational learning and
knowledge sharing across deployments.
&lt;/p&gt;
&lt;p&gt;Our evaluations demonstrate Ethosight&apos;s efficacy across 40 complex use cases.
It has exhibited an exceptional ability to discern new areas of interest,
consistently generating high-affinity scores within the top five labels from a
set of a thousand. Tests conducted across diverse environments attest to
Ethosight&apos;s robust performance. Detailed results and case studies within the
main body of this paper and an appendix underscore a promising trajectory
towards enhancing the adaptability and resilience of computer vision models in
detecting and extracting subtle and nuanced behaviors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latapie_H/0/1/0/all/0/1&quot;&gt;Hugo Latapie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thorisson_K/0/1/0/all/0/1&quot;&gt;Kristinn R. Thorisson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrosyan_V/0/1/0/all/0/1&quot;&gt;Vahagn Petrosyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hammer_P/0/1/0/all/0/1&quot;&gt;Patrick Hammer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kynoch_B/0/1/0/all/0/1&quot;&gt;Brandon Kynoch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanning Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tangrui Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10584">
<title>Reference-based Painterly Inpainting via Diffusion: Crossing the Wild Reference Domain Gap. (arXiv:2307.10584v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10584</link>
<description rdf:parseType="Literal">&lt;p&gt;Have you ever imagined how it would look if we placed new objects into
paintings? For example, what would it look like if we placed a basketball into
Claude Monet&apos;s ``Water Lilies, Evening Effect&apos;&apos;? We propose Reference-based
Painterly Inpainting, a novel task that crosses the wild reference domain gap
and implants novel objects into artworks. Although previous works have examined
reference-based inpainting, they are not designed for large domain
discrepancies between the target and the reference, such as inpainting an
artistic image using a photorealistic reference. This paper proposes a novel
diffusion framework, dubbed RefPaint, to ``inpaint more wildly&apos;&apos; by taking such
references with large domain gaps. Built with an image-conditioned diffusion
model, we introduce a ladder-side branch and a masked fusion mechanism to work
with the inpainting mask. By decomposing the CLIP image embeddings at inference
time, one can manipulate the strength of semantic and style information with
ease. Experiments demonstrate that our proposed RefPaint framework produces
significantly better results than existing methods. Our method enables creative
painterly image inpainting with reference objects that would otherwise be
difficult to achieve. Project page: https://vita-group.github.io/RefPaint/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dejia Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xingqian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_W/0/1/0/all/0/1&quot;&gt;Wenyan Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Humphrey Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10593">
<title>Event Blob Tracking: An Asynchronous Real-Time Algorithm. (arXiv:2307.10593v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10593</link>
<description rdf:parseType="Literal">&lt;p&gt;Event-based cameras have become increasingly popular for tracking fast-moving
objects due to their high temporal resolution, low latency, and high dynamic
range. In this paper, we propose a novel algorithm for tracking event blobs
using raw events asynchronously in real time. We introduce the concept of an
event blob as a spatio-temporal likelihood of event occurrence where the
conditional spatial likelihood is blob-like. Many real-world objects generate
event blob data, for example, flickering LEDs such as car headlights or any
small foreground object moving against a static or slowly varying background.
The proposed algorithm uses a nearest neighbour classifier with a dynamic
threshold criteria for data association coupled with a Kalman filter to track
the event blob state. Our algorithm achieves highly accurate tracking and event
blob shape estimation even under challenging lighting conditions and high-speed
motions. The microsecond time resolution achieved means that the filter output
can be used to derive secondary information such as time-to-contact or range
estimation, that will enable applications to real-world problems such as
collision avoidance in autonomous driving.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molloy_T/0/1/0/all/0/1&quot;&gt;Timothy Molloy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goor_P/0/1/0/all/0/1&quot;&gt;Pieter van Goor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahony_R/0/1/0/all/0/1&quot;&gt;Robert Mahony&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10601">
<title>SCA-PVNet: Self-and-Cross Attention Based Aggregation of Point Cloud and Multi-View for 3D Object Retrieval. (arXiv:2307.10601v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10601</link>
<description rdf:parseType="Literal">&lt;p&gt;To address 3D object retrieval, substantial efforts have been made to
generate highly discriminative descriptors of 3D objects represented by a
single modality, e.g., voxels, point clouds or multi-view images. It is
promising to leverage the complementary information from multi-modality
representations of 3D objects to further improve retrieval performance.
However, multi-modality 3D object retrieval is rarely developed and analyzed on
large-scale datasets. In this paper, we propose self-and-cross attention based
aggregation of point cloud and multi-view images (SCA-PVNet) for 3D object
retrieval. With deep features extracted from point clouds and multi-view
images, we design two types of feature aggregation modules, namely the
In-Modality Aggregation Module (IMAM) and the Cross-Modality Aggregation Module
(CMAM), for effective feature fusion. IMAM leverages a self-attention mechanism
to aggregate multi-view features while CMAM exploits a cross-attention
mechanism to interact point cloud features with multi-view features. The final
descriptor of a 3D object for object retrieval can be obtained via
concatenating the aggregated features from both modules. Extensive experiments
and analysis are conducted on three datasets, ranging from small to large
scale, to show the superiority of the proposed SCA-PVNet over the
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dongyun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_A/0/1/0/all/0/1&quot;&gt;Aiyuan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1&quot;&gt;Shangbo Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiqun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10603">
<title>Physics-Driven Turbulence Image Restoration with Stochastic Refinement. (arXiv:2307.10603v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.10603</link>
<description rdf:parseType="Literal">&lt;p&gt;Image distortion by atmospheric turbulence is a stochastic degradation, which
is a critical problem in long-range optical imaging systems. A number of
research has been conducted during the past decades, including model-based and
emerging deep-learning solutions with the help of synthetic data. Although fast
and physics-grounded simulation tools have been introduced to help the
deep-learning models adapt to real-world turbulence conditions recently, the
training of such models only relies on the synthetic data and ground truth
pairs. This paper proposes the Physics-integrated Restoration Network (PiRN) to
bring the physics-based simulator directly into the training process to help
the network to disentangle the stochasticity from the degradation and the
underlying image. Furthermore, to overcome the ``average effect&quot; introduced by
deterministic models and the domain gap between the synthetic and real-world
degradation, we further introduce PiRN with Stochastic Refinement (PiRN-SR) to
boost its perceptual quality. Overall, our PiRN and PiRN-SR improve the
generalization to real-world unknown turbulence conditions and provide a
state-of-the-art restoration in both pixel-wise accuracy and perceptual
quality. Our codes are available at \url{https://github.com/VITA-Group/PiRN}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jaiswal_A/0/1/0/all/0/1&quot;&gt;Ajay Jaiswal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xingguang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chan_S/0/1/0/all/0/1&quot;&gt;Stanley H. Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10609">
<title>Hybrid Feature Embedding For Automatic Building Outline Extraction. (arXiv:2307.10609v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10609</link>
<description rdf:parseType="Literal">&lt;p&gt;Building outline extracted from high-resolution aerial images can be used in
various application fields such as change detection and disaster assessment.
However, traditional CNN model cannot recognize contours very precisely from
original images. In this paper, we proposed a CNN and Transformer based model
together with active contour model to deal with this problem. We also designed
a triple-branch decoder structure to handle different features generated by
encoder. Experiment results show that our model outperforms other baseline
model on two datasets, achieving 91.1% mIoU on Vaihingen and 83.8% on Bing
huts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ran_W/0/1/0/all/0/1&quot;&gt;Weihang Ran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Wei Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xiaodan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zipei Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shibasaki_R/0/1/0/all/0/1&quot;&gt;Ryosuke Shibasaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10616">
<title>Heterogeneous Federated Learning: State-of-the-art and Research Challenges. (arXiv:2307.10616v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10616</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) has drawn increasing attention owing to its potential
use in large-scale industrial applications. Existing federated learning works
mainly focus on model homogeneous settings. However, practical federated
learning typically faces the heterogeneity of data distributions, model
architectures, network environments, and hardware devices among participant
clients. Heterogeneous Federated Learning (HFL) is much more challenging, and
corresponding solutions are diverse and complex. Therefore, a systematic survey
on this topic about the research challenges and state-of-the-art is essential.
In this survey, we firstly summarize the various research challenges in HFL
from five aspects: statistical heterogeneity, model heterogeneity,
communication heterogeneity, device heterogeneity, and additional challenges.
In addition, recent advances in HFL are reviewed and a new taxonomy of existing
HFL methods is proposed with an in-depth analysis of their pros and cons. We
classify existing methods from three different levels according to the HFL
procedure: data-level, model-level, and server-level. Finally, several critical
and promising future research directions in HFL are discussed, which may
facilitate further developments in this field. A periodically updated
collection on HFL is available at https://github.com/marswhu/HFL_Survey.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1&quot;&gt;Mang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1&quot;&gt;Xiuwen Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bo Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuen_P/0/1/0/all/0/1&quot;&gt;Pong C. Yuen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10620">
<title>Quaternion tensor ring decomposition and application for color image inpainting. (arXiv:2307.10620v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10620</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, tensor networks have emerged as powerful tools for solving
large-scale optimization problems. One of the most promising tensor networks is
the tensor ring (TR) decomposition, which achieves circular dimensional
permutation invariance in the model through the utilization of the trace
operation and equitable treatment of the latent cores. On the other hand, more
recently, quaternions have gained significant attention and have been widely
utilized in color image processing tasks due to their effectiveness in encoding
color pixels. Therefore, in this paper, we propose the quaternion tensor ring
(QTR) decomposition, which inherits the powerful and generalized representation
abilities of the TR decomposition while leveraging the advantages of
quaternions for color pixel representation. In addition to providing the
definition of QTR decomposition and an algorithm for learning the QTR format,
this paper also proposes a low-rank quaternion tensor completion (LRQTC) model
and its algorithm for color image inpainting based on the QTR decomposition.
Finally, extensive experiments on color image inpainting demonstrate that the
proposed QTLRC method is highly competitive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1&quot;&gt;Jifei Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kou_K/0/1/0/all/0/1&quot;&gt;Kit Ian Kou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10624">
<title>Joint Skeletal and Semantic Embedding Loss for Micro-gesture Classification. (arXiv:2307.10624v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10624</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we briefly introduce the solution of our team HFUT-VUT for the
Micros-gesture Classification in the MiGA challenge at IJCAI 2023. The
micro-gesture classification task aims at recognizing the action category of a
given video based on the skeleton data. For this task, we propose a
3D-CNNs-based micro-gesture recognition network, which incorporates a skeletal
and semantic embedding loss to improve action classification performance.
Finally, we rank 1st in the Micro-gesture Classification Challenge, surpassing
the second-place team in terms of Top-1 accuracy by 1.10%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1&quot;&gt;Dan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guoliang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xinge Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10625">
<title>Learning Discriminative Visual-Text Representation for Polyp Re-Identification. (arXiv:2307.10625v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10625</link>
<description rdf:parseType="Literal">&lt;p&gt;Colonoscopic Polyp Re-Identification aims to match a specific polyp in a
large gallery with different cameras and views, which plays a key role for the
prevention and treatment of colorectal cancer in the computer-aided diagnosis.
However, traditional methods mainly focus on the visual representation
learning, while neglect to explore the potential of semantic features during
training, which may easily leads to poor generalization capability when adapted
the pretrained model into the new scenarios. To relieve this dilemma, we
propose a simple but effective training method named VT-ReID, which can
remarkably enrich the representation of polyp videos with the interchange of
high-level semantic information. Moreover, we elaborately design a novel
clustering mechanism to introduce prior knowledge from textual data, which
leverages contrastive learning to promote better separation from abundant
unlabeled text data. To the best of our knowledge, this is the first attempt to
employ the visual-text feature with clustering mechanism for the colonoscopic
polyp re-identification. Empirical results show that our method significantly
outperforms current state-of-the art methods with a clear margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1&quot;&gt;Suncheng Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Sijia Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_D/0/1/0/all/0/1&quot;&gt;Dahong Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10632">
<title>Parallelization of a new embedded application for automatic meteor detection. (arXiv:2307.10632v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10632</link>
<description rdf:parseType="Literal">&lt;p&gt;This article presents the methods used to parallelize a new computer vision
application. The system is able to automatically detect meteor from
non-stabilized cameras and noisy video sequences. The application is designed
to be embedded in weather balloons or for airborne observation campaigns. Thus,
the final target is a low power system-on-chip (&amp;lt; 10 Watts) while the software
needs to compute a stream of frames in real-time (&amp;gt; 25 frames per second). For
this, first the application is split in a tasks graph, then different
parallelization techniques are applied. Experiment results demonstrate the
efficiency of the parallelization methods. For instance, on the Raspberry Pi 4
and on a HD video sequence, the processing chain reaches 42 frames per second
while it only consumes 6 Watts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kandeepan_M/0/1/0/all/0/1&quot;&gt;Mathuran Kandeepan&lt;/a&gt; (ALSOC), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ciocan_C/0/1/0/all/0/1&quot;&gt;Clara Ciocan&lt;/a&gt; (ALSOC), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cassagne_A/0/1/0/all/0/1&quot;&gt;Adrien Cassagne&lt;/a&gt; (ALSOC), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacassagne_L/0/1/0/all/0/1&quot;&gt;Lionel Lacassagne&lt;/a&gt; (ALSOC)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10636">
<title>Learning and Evaluating Human Preferences for Conversational Head Generation. (arXiv:2307.10636v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10636</link>
<description rdf:parseType="Literal">&lt;p&gt;A reliable and comprehensive evaluation metric that aligns with manual
preference assessments is crucial for conversational head video synthesis
method development. Existing quantitative evaluations often fail to capture the
full complexity of human preference, as they only consider limited evaluation
dimensions. Qualitative evaluations and user studies offer a solution but are
time-consuming and labor-intensive. This limitation hinders the advancement of
conversational head generation algorithms and systems. In this paper, we
propose a novel learning-based evaluation metric named Preference Score (PS)
for fitting human preference according to the quantitative evaluations across
different dimensions. PS can serve as a quantitative evaluation without the
need for human annotation. Experimental results validate the superiority of
Preference Score in aligning with human perception, and also demonstrates
robustness and generalizability to unseen data, making it a valuable tool for
advancing conversation head generation. We expect this metric could facilitate
new advances in conversational head generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mohan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yalong Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1&quot;&gt;Ting Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tiejun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1&quot;&gt;Tao Mei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10638">
<title>Quantized Feature Distillation for Network Quantization. (arXiv:2307.10638v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10638</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural network quantization aims to accelerate and trim full-precision neural
network models by using low bit approximations. Methods adopting the
quantization aware training (QAT) paradigm have recently seen a rapid growth,
but are often conceptually complicated. This paper proposes a novel and highly
effective QAT method, quantized feature distillation (QFD). QFD first trains a
quantized (or binarized) representation as the teacher, then quantize the
network using knowledge distillation (KD). Quantitative results show that QFD
is more flexible and effective (i.e., quantization friendly) than previous
quantization methods. QFD surpasses existing methods by a noticeable margin on
not only image classification but also object detection, albeit being much
simpler. Furthermore, QFD quantizes ViT and Swin-Transformer on MS-COCO
detection and segmentation, which verifies its potential in real world
deployment. To the best of our knowledge, this is the first time that vision
transformers have been quantized in object detection and image segmentation
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Ke Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yin-Yin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jianxin Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10642">
<title>RetouchingFFHQ: A Large-scale Dataset for Fine-grained Face Retouching Detection. (arXiv:2307.10642v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10642</link>
<description rdf:parseType="Literal">&lt;p&gt;The widespread use of face retouching filters on short-video platforms has
raised concerns about the authenticity of digital appearances and the impact of
deceptive advertising. To address these issues, there is a pressing need to
develop advanced face retouching techniques. However, the lack of large-scale
and fine-grained face retouching datasets has been a major obstacle to progress
in this field. In this paper, we introduce RetouchingFFHQ, a large-scale and
fine-grained face retouching dataset that contains over half a million
conditionally-retouched images. RetouchingFFHQ stands out from previous
datasets due to its large scale, high quality, fine-grainedness, and
customization. By including four typical types of face retouching operations
and different retouching levels, we extend the binary face retouching detection
into a fine-grained, multi-retouching type, and multi-retouching level
estimation problem. Additionally, we propose a Multi-granularity Attention
Module (MAM) as a plugin for CNN backbones for enhanced cross-scale
representation learning. Extensive experiments using different baselines as
well as our proposed method on RetouchingFFHQ show decent performance on face
retouching detection. With the proposed new dataset, we believe there is great
potential for future work to tackle the challenging problem of real-world
fine-grained face retouching detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1&quot;&gt;Qichao Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaxin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haisheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1&quot;&gt;Zhenxing Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinpeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10664">
<title>Lighting up NeRF via Unsupervised Decomposition and Enhancement. (arXiv:2307.10664v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10664</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Field (NeRF) is a promising approach for synthesizing novel
views, given a set of images and the corresponding camera poses of a scene.
However, images photographed from a low-light scene can hardly be used to train
a NeRF model to produce high-quality results, due to their low pixel
intensities, heavy noise, and color distortion. Combining existing low-light
image enhancement methods with NeRF methods also does not work well due to the
view inconsistency caused by the individual 2D enhancement process. In this
paper, we propose a novel approach, called Low-Light NeRF (or LLNeRF), to
enhance the scene representation and synthesize normal-light novel views
directly from sRGB low-light images in an unsupervised manner. The core of our
approach is a decomposition of radiance field learning, which allows us to
enhance the illumination, reduce noise and correct the distorted colors jointly
with the NeRF optimization process. Our method is able to produce novel view
images with proper lighting and vivid colors and details, given a collection of
camera-finished low dynamic range (8-bits/channel) images from a low-light
scene. Experiments demonstrate that our method outperforms existing low-light
enhancement methods and NeRF methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaogang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Ke Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lau_R/0/1/0/all/0/1&quot;&gt;Rynson WH. Lau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10667">
<title>Efficient Unified Demosaicing for Bayer and Non-Bayer Patterned Image Sensors. (arXiv:2307.10667v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.10667</link>
<description rdf:parseType="Literal">&lt;p&gt;As the physical size of recent CMOS image sensors (CIS) gets smaller, the
latest mobile cameras are adopting unique non-Bayer color filter array (CFA)
patterns (e.g., Quad, Nona, QxQ), which consist of homogeneous color units with
adjacent pixels. These non-Bayer sensors are superior to conventional Bayer CFA
thanks to their changeable pixel-bin sizes for different light conditions but
may introduce visual artifacts during demosaicing due to their inherent pixel
pattern structures and sensor hardware characteristics. Previous demosaicing
methods have primarily focused on Bayer CFA, necessitating distinct
reconstruction methods for non-Bayer patterned CIS with various CFA modes under
different lighting conditions. In this work, we propose an efficient unified
demosaicing method that can be applied to both conventional Bayer RAW and
various non-Bayer CFAs&apos; RAW data in different operation modes. Our Knowledge
Learning-based demosaicing model for Adaptive Patterns, namely KLAP, utilizes
CFA-adaptive filters for only 1% key filters in the network for each CFA, but
still manages to effectively demosaic all the CFAs, yielding comparable
performance to the large-scale models. Furthermore, by employing meta-learning
during inference (KLAP-M), our model is able to eliminate unknown
sensor-generic artifacts in real RAW data, effectively bridging the gap between
synthetic images and real sensor RAW. Our KLAP and KLAP-M methods achieved
state-of-the-art demosaicing performance in both synthetic and real RAW data of
Bayer and non-Bayer CFAs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Haechang Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Park_D/0/1/0/all/0/1&quot;&gt;Dongwon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jeong_W/0/1/0/all/0/1&quot;&gt;Wongi Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kijeong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Je_H/0/1/0/all/0/1&quot;&gt;Hyunwoo Je&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ryu_D/0/1/0/all/0/1&quot;&gt;Dongil Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chun_S/0/1/0/all/0/1&quot;&gt;Se Young Chun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10677">
<title>Deep learning for classification of noisy QR codes. (arXiv:2307.10677v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10677</link>
<description rdf:parseType="Literal">&lt;p&gt;We wish to define the limits of a classical classification model based on
deep learning when applied to abstract images, which do not represent visually
identifiable objects.QR codes (Quick Response codes) fall into this category of
abstract images: one bit corresponding to one encoded character, QR codes were
not designed to be decoded manually. To understand the limitations of a deep
learning-based model for abstract image classification, we train an image
classification model on QR codes generated from information obtained when
reading a health pass. We compare a classification model with a classical
(deterministic) decoding method in the presence of noise. This study allows us
to conclude that a model based on deep learning can be relevant for the
understanding of abstract images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leygonie_R/0/1/0/all/0/1&quot;&gt;Rebecca Leygonie&lt;/a&gt; (LIPADE), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lobry_S/0/1/0/all/0/1&quot;&gt;Sylvain Lobry&lt;/a&gt; (LIPADE)), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+%28LIPADE%29_L/0/1/0/all/0/1&quot;&gt;Laurent Wendling (LIPADE)&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10685">
<title>Pre-train, Adapt and Detect: Multi-Task Adapter Tuning for Camouflaged Object Detection. (arXiv:2307.10685v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10685</link>
<description rdf:parseType="Literal">&lt;p&gt;Camouflaged object detection (COD), aiming to segment camouflaged objects
which exhibit similar patterns with the background, is a challenging task. Most
existing works are dedicated to establishing specialized modules to identify
camouflaged objects with complete and fine details, while the boundary can not
be well located for the lack of object-related semantics. In this paper, we
propose a novel ``pre-train, adapt and detect&quot; paradigm to detect camouflaged
objects. By introducing a large pre-trained model, abundant knowledge learned
from massive multi-modal data can be directly transferred to COD. A lightweight
parallel adapter is inserted to adjust the features suitable for the downstream
COD task. Extensive experiments on four challenging benchmark datasets
demonstrate that our method outperforms existing state-of-the-art COD models by
large margins. Moreover, we design a multi-task learning scheme for tuning the
adapter to exploit the shareable knowledge across different semantic classes.
Comprehensive experimental results showed that the generalization ability of
our model can be substantially improved with multi-task adapter initialization
on source tasks and multi-task adaptation on target tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1&quot;&gt;Yinghui Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1&quot;&gt;Dexuan Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shizhou Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Geng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ran_L/0/1/0/all/0/1&quot;&gt;Lingyan Ran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanning Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10695">
<title>Self2Self+: Single-Image Denoising with Self-Supervised Learning and Image Quality Assessment Loss. (arXiv:2307.10695v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10695</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, denoising methods based on supervised learning have exhibited
promising performance. However, their reliance on external datasets containing
noisy-clean image pairs restricts their applicability. To address this
limitation, researchers have focused on training denoising networks using
solely a set of noisy inputs. To improve the feasibility of denoising
procedures, in this study, we proposed a single-image self-supervised learning
method in which only the noisy input image is used for network training. Gated
convolution was used for feature extraction and no-reference image quality
assessment was used for guiding the training process. Moreover, the proposed
method sampled instances from the input image dataset using Bernoulli sampling
with a certain dropout rate for training. The corresponding result was produced
by averaging the generated predictions from various instances of the trained
network with dropouts. The experimental results indicated that the proposed
method achieved state-of-the-art denoising performance on both synthetic and
real-world datasets. This highlights the effectiveness and practicality of our
method as a potential solution for various noise removal tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1&quot;&gt;Jaekyun Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sanghwan Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10696">
<title>SLPD: Slide-level Prototypical Distillation for WSIs. (arXiv:2307.10696v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10696</link>
<description rdf:parseType="Literal">&lt;p&gt;Improving the feature representation ability is the foundation of many whole
slide pathological image (WSIs) tasks. Recent works have achieved great success
in pathological-specific self-supervised learning (SSL). However, most of them
only focus on learning patch-level representations, thus there is still a gap
between pretext and slide-level downstream tasks, e.g., subtyping, grading and
staging. Aiming towards slide-level representations, we propose Slide-Level
Prototypical Distillation (SLPD) to explore intra- and inter-slide semantic
structures for context modeling on WSIs. Specifically, we iteratively perform
intra-slide clustering for the regions (4096x4096 patches) within each WSI to
yield the prototypes and encourage the region representations to be closer to
the assigned prototypes. By representing each slide with its prototypes, we
further select similar slides by the set distance of prototypes and assign the
regions by cross-slide prototypes for distillation. SLPD achieves
state-of-the-art results on multiple slide-level benchmarks and demonstrates
that representation learning of semantic structures of slides can make a
suitable proxy task for WSI analysis. Code will be available at
https://github.com/Carboxy/SLPD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhimiao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tiancheng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yi Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10697">
<title>SqueezerFaceNet: Reducing a Small Face Recognition CNN Even More Via Filter Pruning. (arXiv:2307.10697v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10697</link>
<description rdf:parseType="Literal">&lt;p&gt;The widespread use of mobile devices for various digital services has created
a need for reliable and real-time person authentication. In this context,
facial recognition technologies have emerged as a dependable method for
verifying users due to the prevalence of cameras in mobile devices and their
integration into everyday applications. The rapid advancement of deep
Convolutional Neural Networks (CNNs) has led to numerous face verification
architectures. However, these models are often large and impractical for mobile
applications, reaching sizes of hundreds of megabytes with millions of
parameters. We address this issue by developing SqueezerFaceNet, a light face
recognition network which less than 1M parameters. This is achieved by applying
a network pruning method based on Taylor scores, where filters with small
importance scores are removed iteratively. Starting from an already small
network (of 1.24M) based on SqueezeNet, we show that it can be further reduced
(up to 40%) without an appreciable loss in performance. To the best of our
knowledge, we are the first to evaluate network pruning methods for the task of
face recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1&quot;&gt;Fernando Alonso-Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Diaz_K/0/1/0/all/0/1&quot;&gt;Kevin Hernandez-Diaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubio_J/0/1/0/all/0/1&quot;&gt;Jose Maria Buades Rubio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bigun_J/0/1/0/all/0/1&quot;&gt;Josef Bigun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10698">
<title>Reverse Knowledge Distillation: Training a Large Model using a Small One for Retinal Image Matching on Limited Data. (arXiv:2307.10698v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10698</link>
<description rdf:parseType="Literal">&lt;p&gt;Retinal image matching plays a crucial role in monitoring disease progression
and treatment response. However, datasets with matched keypoints between
temporally separated pairs of images are not available in abundance to train
transformer-based model. We propose a novel approach based on reverse knowledge
distillation to train large models with limited data while preventing
overfitting. Firstly, we propose architectural modifications to a CNN-based
semi-supervised method called SuperRetina that help us improve its results on a
publicly available dataset. Then, we train a computationally heavier model
based on a vision transformer encoder using the lighter CNN-based model, which
is counter-intuitive in the field knowledge-distillation research where
training lighter models based on heavier ones is the norm. Surprisingly, such
reverse knowledge distillation improves generalization even further. Our
experiments suggest that high-dimensional fitting in representation space may
prevent overfitting unlike training directly to match the final output. We also
provide a public dataset with annotations for retinal image keypoint detection
and matching to help the research community develop algorithms for retinal
image applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasser_S/0/1/0/all/0/1&quot;&gt;Sahar Almahfouz Nasser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupte_N/0/1/0/all/0/1&quot;&gt;Nihar Gupte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sethi_A/0/1/0/all/0/1&quot;&gt;Amit Sethi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10705">
<title>TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars. (arXiv:2307.10705v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10705</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation is a common task in autonomous driving to understand
the surrounding environment. Driveable Area Segmentation and Lane Detection are
particularly important for safe and efficient navigation on the road. However,
original semantic segmentation models are computationally expensive and require
high-end hardware, which is not feasible for embedded systems in autonomous
vehicles. This paper proposes a lightweight model for the driveable area and
lane line segmentation. TwinLiteNet is designed cheaply but achieves accurate
and efficient segmentation results. We evaluate TwinLiteNet on the BDD100K
dataset and compare it with modern models. Experimental results show that our
TwinLiteNet performs similarly to existing approaches, requiring significantly
fewer computational resources. Specifically, TwinLiteNet achieves a mIoU score
of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task
with only 0.4 million parameters and achieves 415 FPS on GPU RTX A5000.
Furthermore, TwinLiteNet can run in real-time on embedded devices with limited
computing power, especially since it achieves 60FPS on Jetson Xavier NX, making
it an ideal solution for self-driving vehicles. Code is available:
url{https://github.com/chequanghuy/TwinLiteNet}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_Q/0/1/0/all/0/1&quot;&gt;Quang Huy Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Dinh Phuc Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1&quot;&gt;Minh Quan Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_D/0/1/0/all/0/1&quot;&gt;Duc Khai Lam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10711">
<title>AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models. (arXiv:2307.10711v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10711</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing customization methods require access to multiple reference examples
to align pre-trained diffusion probabilistic models (DPMs) with user-provided
concepts. This paper aims to address the challenge of DPM customization when
the only available supervision is a differentiable metric defined on the
generated contents. Since the sampling procedure of DPMs involves recursive
calls to the denoising UNet, na\&quot;ive gradient backpropagation requires storing
the intermediate states of all iterations, resulting in extremely high memory
consumption. To overcome this issue, we propose a novel method AdjointDPM,
which first generates new samples from diffusion models by solving the
corresponding probability-flow ODEs. It then uses the adjoint sensitivity
method to backpropagate the gradients of the loss to the models&apos; parameters
(including conditioning signals, network weights, and initial noises) by
solving another augmented ODE. To reduce numerical errors in both the forward
generation and gradient backpropagation processes, we further reparameterize
the probability-flow ODE and augmented ODE as simple non-stiff ODEs using
exponential integration. Finally, we demonstrate the effectiveness of
AdjointDPM on three interesting tasks: converting visual effects into
identification text embeddings, finetuning DPMs for specific types of
stylization, and optimizing initial noise to generate adversarial samples for
security auditing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jiachun Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hanshu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liew_J/0/1/0/all/0/1&quot;&gt;Jun Hao Liew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1&quot;&gt;Vincent Y. F. Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10713">
<title>Kick Back &amp; Relax: Learning to Reconstruct the World by Watching SlowTV. (arXiv:2307.10713v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10713</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised monocular depth estimation (SS-MDE) has the potential to
scale to vast quantities of data. Unfortunately, existing approaches limit
themselves to the automotive domain, resulting in models incapable of
generalizing to complex environments such as natural or indoor settings.
&lt;/p&gt;
&lt;p&gt;To address this, we propose a large-scale SlowTV dataset curated from
YouTube, containing an order of magnitude more data than existing automotive
datasets. SlowTV contains 1.7M images from a rich diversity of environments,
such as worldwide seasonal hiking, scenic driving and scuba diving. Using this
dataset, we train an SS-MDE model that provides zero-shot generalization to a
large collection of indoor/outdoor datasets. The resulting model outperforms
all existing SSL approaches and closes the gap on supervised SoTA, despite
using a more efficient architecture.
&lt;/p&gt;
&lt;p&gt;We additionally introduce a collection of best-practices to further maximize
performance and zero-shot generalization. This includes 1) aspect ratio
augmentation, 2) camera intrinsic estimation, 3) support frame randomization
and 4) flexible motion estimation. Code is available at
https://github.com/jspenmar/slowtv_monodepth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spencer_J/0/1/0/all/0/1&quot;&gt;Jaime Spencer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1&quot;&gt;Chris Russell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadfield_S/0/1/0/all/0/1&quot;&gt;Simon Hadfield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1&quot;&gt;Richard Bowden&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10745">
<title>EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation. (arXiv:2307.10745v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10745</link>
<description rdf:parseType="Literal">&lt;p&gt;Active learning algorithms have become increasingly popular for training
models with limited data. However, selecting data for annotation remains a
challenging problem due to the limited information available on unseen data. To
address this issue, we propose EdgeAL, which utilizes the edge information of
unseen images as {\it a priori} information for measuring uncertainty. The
uncertainty is quantified by analyzing the divergence and entropy in model
predictions across edges. This measure is then used to select superpixels for
annotation. We demonstrate the effectiveness of EdgeAL on multi-class Optical
Coherence Tomography (OCT) segmentation tasks, where we achieved a 99% dice
score while reducing the annotation label cost to 12%, 2.3%, and 3%,
respectively, on three publicly available datasets (Duke, AROI, and UMN). The
source code is available at \url{https://github.com/Mak-Ta-Reque/EdgeAL}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadir_M/0/1/0/all/0/1&quot;&gt;Md Abdul Kadir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_H/0/1/0/all/0/1&quot;&gt;Hasan Md Tusfiqur Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1&quot;&gt;Daniel Sonntag&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10753">
<title>LBL: Logarithmic Barrier Loss Function for One-class Classification. (arXiv:2307.10753v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10753</link>
<description rdf:parseType="Literal">&lt;p&gt;One-class classification (OCC) aims to train a classifier only with the
target class data and attracts great attention for its strong applicability in
real-world application. Despite a lot of advances have been made in OCC, it
still lacks the effective OCC loss functions for deep learning. In this paper,
a novel logarithmic barrier function based OCC loss (LBL) that assigns large
gradients to the margin samples and thus derives more compact hypersphere, is
first proposed by approximating the OCC objective smoothly. But the
optimization of LBL may be instability especially when samples lie on the
boundary leading to the infinity loss. To address this issue, then, a
unilateral relaxation Sigmoid function is introduced into LBL and a novel OCC
loss named LBLSig is proposed. The LBLSig can be seen as the fusion of the mean
square error (MSE) and the cross entropy (CE) and the optimization of LBLSig is
smoother owing to the unilateral relaxation Sigmoid function. The effectiveness
of the proposed LBL and LBLSig is experimentally demonstrated in comparisons
with several state-of-the-art OCC algorithms on different network structures.
The source code can be found at https://github.com/ML-HDU/LBL_LBLSig.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianlei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dekang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wandong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jiuwen Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10763">
<title>MSQNet: Actor-agnostic Action Recognition with Multi-modal Query. (arXiv:2307.10763v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10763</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing action recognition methods are typically actor-specific due to the
intrinsic topological and apparent differences among the actors. This requires
actor-specific pose estimation (e.g., humans vs. animals), leading to
cumbersome model design complexity and high maintenance costs. Moreover, they
often focus on learning the visual modality alone and single-label
classification whilst neglecting other available information sources (e.g.,
class name text) and the concurrent occurrence of multiple actions. To overcome
these limitations, we propose a new approach called &apos;actor-agnostic multi-modal
multi-label action recognition,&apos; which offers a unified solution for various
types of actors, including humans and animals. We further formulate a novel
Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object
detection framework (e.g., DETR), characterized by leveraging visual and
textual modalities to represent the action classes better. The elimination of
actor-specific model designs is a key advantage, as it removes the need for
actor pose estimation altogether. Extensive experiments on five publicly
available benchmarks show that our MSQNet consistently outperforms the prior
arts of actor-specific alternatives on human and animal single- and multi-label
action recognition tasks by up to 50%. Code will be released at
https://github.com/mondalanindya/MSQNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1&quot;&gt;Anindya Mondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1&quot;&gt;Sauradip Nag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prada_J/0/1/0/all/0/1&quot;&gt;Joaquin M Prada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1&quot;&gt;Anjan Dutta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10768">
<title>Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory. (arXiv:2307.10768v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2307.10768</link>
<description rdf:parseType="Literal">&lt;p&gt;Working memory (WM), a fundamental cognitive process facilitating the
temporary storage, integration, manipulation, and retrieval of information,
plays a vital role in reasoning and decision-making tasks. Robust benchmark
datasets that capture the multifaceted nature of WM are crucial for the
effective development and evaluation of AI WM models. Here, we introduce a
comprehensive Working Memory (WorM) benchmark dataset for this purpose. WorM
comprises 10 tasks and a total of 1 million trials, assessing 4
functionalities, 3 domains, and 11 behavioral and neural characteristics of WM.
We jointly trained and tested state-of-the-art recurrent neural networks and
transformers on all these tasks. We also include human behavioral benchmarks as
an upper bound for comparison. Our results suggest that AI models replicate
some characteristics of WM in the brain, most notably primacy and recency
effects, and neural clusters and correlates specialized for different domains
and functionalities of WM. In the experiments, we also reveal some limitations
in existing models to approximate human behavior. This dataset serves as a
valuable resource for communities in cognitive psychology, neuroscience, and
AI, offering a standardized framework to compare and enhance WM models,
investigate WM&apos;s neural underpinnings, and develop WM models with human-like
capabilities. Our source code and data are available at
https://github.com/ZhangLab-DeepNeuroCogLab/WorM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sikarwar_A/0/1/0/all/0/1&quot;&gt;Ankur Sikarwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengmi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10776">
<title>Urban Radiance Field Representation with Deformable Neural Mesh Primitives. (arXiv:2307.10776v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10776</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRFs) have achieved great success in the past few
years. However, most current methods still require intensive resources due to
ray marching-based rendering. To construct urban-level radiance fields
efficiently, we design Deformable Neural Mesh Primitive~(DNMP), and propose to
parameterize the entire scene with such primitives. The DNMP is a flexible and
compact neural variant of classic mesh representation, which enjoys both the
efficiency of rasterization-based rendering and the powerful neural
representation capability for photo-realistic image synthesis. Specifically, a
DNMP consists of a set of connected deformable mesh vertices with paired vertex
features to parameterize the geometry and radiance information of a local area.
To constrain the degree of freedom for optimization and lower the storage
budgets, we enforce the shape of each primitive to be decoded from a relatively
low-dimensional latent space. The rendering colors are decoded from the vertex
features (interpolated with rasterization) by a view-dependent MLP. The DNMP
provides a new paradigm for urban-level scene representation with appealing
properties: $(1)$ High-quality rendering. Our method achieves leading
performance for novel view synthesis in urban scenarios. $(2)$ Low
computational costs. Our representation enables fast rendering (2.07ms/1k
pixels) and low peak memory usage (110MB/1k pixels). We also present a
lightweight version that can run 33$\times$ faster than vanilla NeRFs, and
comparable to the highly-optimized Instant-NGP (0.61 vs 0.71ms/1k pixels).
Project page: \href{https://dnmp.github.io/}{https://dnmp.github.io/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1&quot;&gt;Fan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kwan-Yee Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Changjun Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10780">
<title>Learned Thresholds Token Merging and Pruning for Vision Transformers. (arXiv:2307.10780v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10780</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers have demonstrated remarkable success in a wide range of
computer vision tasks over the last years. However, their high computational
costs remain a significant barrier to their practical deployment. In
particular, the complexity of transformer models is quadratic with respect to
the number of input tokens. Therefore techniques that reduce the number of
input tokens that need to be processed have been proposed. This paper
introduces Learned Thresholds token Merging and Pruning (LTMP), a novel
approach that leverages the strengths of both token merging and token pruning.
LTMP uses learned threshold masking modules that dynamically determine which
tokens to merge and which to prune. We demonstrate our approach with extensive
experiments on vision transformers on the ImageNet classification task. Our
results demonstrate that LTMP achieves state-of-the-art accuracy across
reduction rates while requiring only a single fine-tuning epoch, which is an
order of magnitude faster than previous methods. Code is available at
https://github.com/Mxbonn/ltmp .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonnaerens_M/0/1/0/all/0/1&quot;&gt;Maxim Bonnaerens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dambre_J/0/1/0/all/0/1&quot;&gt;Joni Dambre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10782">
<title>See More and Know More: Zero-shot Point Cloud Segmentation via Multi-modal Visual Data. (arXiv:2307.10782v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10782</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-shot point cloud segmentation aims to make deep models capable of
recognizing novel objects in point cloud that are unseen in the training phase.
Recent trends favor the pipeline which transfers knowledge from seen classes
with labels to unseen classes without labels. They typically align visual
features with semantic features obtained from word embedding by the supervision
of seen classes&apos; annotations. However, point cloud contains limited information
to fully match with semantic features. In fact, the rich appearance information
of images is a natural complement to the textureless point cloud, which is not
well explored in previous literature. Motivated by this, we propose a novel
multi-modal zero-shot learning method to better utilize the complementary
information of point clouds and images for more accurate visual-semantic
alignment. Extensive experiments are performed in two popular benchmarks, i.e.,
SemanticKITTI and nuScenes, and our method outperforms current SOTA methods
with 52% and 49% improvement on average for unseen class mIoU, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yuhang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1&quot;&gt;Qi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Runnan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1&quot;&gt;Yuenan Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xinge Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuexin Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10784">
<title>SMURF: Spatial Multi-Representation Fusion for 3D Object Detection with 4D Imaging Radar. (arXiv:2307.10784v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10784</link>
<description rdf:parseType="Literal">&lt;p&gt;The 4D Millimeter wave (mmWave) radar is a promising technology for vehicle
sensing due to its cost-effectiveness and operability in adverse weather
conditions. However, the adoption of this technology has been hindered by
sparsity and noise issues in radar point cloud data. This paper introduces
spatial multi-representation fusion (SMURF), a novel approach to 3D object
detection using a single 4D imaging radar. SMURF leverages multiple
representations of radar detection points, including pillarization and density
features of a multi-dimensional Gaussian mixture distribution through kernel
density estimation (KDE). KDE effectively mitigates measurement inaccuracy
caused by limited angular resolution and multi-path propagation of radar
signals. Additionally, KDE helps alleviate point cloud sparsity by capturing
density features. Experimental evaluations on View-of-Delft (VoD) and
TJ4DRadSet datasets demonstrate the effectiveness and generalization ability of
SMURF, outperforming recently proposed 4D imaging radar-based
single-representation models. Moreover, while using 4D imaging radar only,
SMURF still achieves comparable performance to the state-of-the-art 4D imaging
radar and camera fusion-based method, with an increase of 1.22% in the mean
average precision on bird&apos;s-eye view of TJ4DRadSet dataset and 1.32% in the 3D
mean average precision on the entire annotated area of VoD dataset. Our
proposed method demonstrates impressive inference time and addresses the
challenges of real-time detection, with the inference time no more than 0.05
seconds for most scans on both datasets. This research highlights the benefits
of 4D mmWave radar and is a strong benchmark for subsequent works regarding 3D
object detection with 4D imaging radar.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qiuchi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1&quot;&gt;Weiyi Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1&quot;&gt;Qing-Long Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Bing Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10787">
<title>Feed-Forward Source-Free Domain Adaptation via Class Prototypes. (arXiv:2307.10787v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10787</link>
<description rdf:parseType="Literal">&lt;p&gt;Source-free domain adaptation has become popular because of its practical
usefulness and no need to access source data. However, the adaptation process
still takes a considerable amount of time and is predominantly based on
optimization that relies on back-propagation. In this work we present a simple
feed-forward approach that challenges the need for back-propagation based
adaptation. Our approach is based on computing prototypes of classes under the
domain shift using a pre-trained model. It achieves strong improvements in
accuracy compared to the pre-trained model and requires only a small fraction
of time of existing domain adaptation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1&quot;&gt;Ondrej Bohdal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Da Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1&quot;&gt;Timothy Hospedales&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10790">
<title>Behavioral Analysis of Vision-and-Language Navigation Agents. (arXiv:2307.10790v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10790</link>
<description rdf:parseType="Literal">&lt;p&gt;To be successful, Vision-and-Language Navigation (VLN) agents must be able to
ground instructions to actions based on their surroundings. In this work, we
develop a methodology to study agent behavior on a skill-specific basis --
examining how well existing agents ground instructions about stopping, turning,
and moving towards specified objects or rooms. Our approach is based on
generating skill-specific interventions and measuring changes in agent
predictions. We present a detailed case study analyzing the behavior of a
recent agent and then compare multiple agents in terms of skill-specific
competency scores. This analysis suggests that biases from training have
lasting effects on agent behavior and that existing models are able to ground
simple referring expressions. Our comparisons between models show that
skill-specific scores correlate with improvements in overall VLN task
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zijiao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1&quot;&gt;Arjun Majumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Stefan Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10792">
<title>Optimizing PatchCore for Few/many-shot Anomaly Detection. (arXiv:2307.10792v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10792</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot anomaly detection (AD) is an emerging sub-field of general AD, and
tries to distinguish between normal and anomalous data using only few selected
samples. While newly proposed few-shot AD methods do compare against
pre-existing algorithms developed for the full-shot domain as baselines, they
do not dedicatedly optimize them for the few-shot setting. It thus remains
unclear if the performance of such pre-existing algorithms can be further
improved. We address said question in this work. Specifically, we present a
study on the AD/anomaly segmentation (AS) performance of PatchCore, the current
state-of-the-art full-shot AD/AS algorithm, in both the few-shot and the
many-shot settings. We hypothesize that further performance improvements can be
realized by (I) optimizing its various hyperparameters, and by (II)
transferring techniques known to improve few-shot supervised learning to the AD
domain. Exhaustive experiments on the public VisA and MVTec AD datasets reveal
that (I) significant performance improvements can be realized by optimizing
hyperparameters such as the underlying feature extractor, and that (II)
image-level augmentations can, but are not guaranteed, to improve performance.
Based on these findings, we achieve a new state of the art in few-shot AD on
VisA, further demonstrating the merit of adapting pre-existing AD/AS methods to
the few-shot setting. Last, we identify the investigation of feature extractors
with a strong inductive bias as a potential future research direction for
(few-shot) AD/AS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Triet Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rippel_O/0/1/0/all/0/1&quot;&gt;Oliver Rippel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10797">
<title>HyperReenact: One-Shot Reenactment via Jointly Learning to Refine and Retarget Faces. (arXiv:2307.10797v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10797</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present our method for neural face reenactment, called
HyperReenact, that aims to generate realistic talking head images of a source
identity, driven by a target facial pose. Existing state-of-the-art face
reenactment methods train controllable generative models that learn to
synthesize realistic facial images, yet producing reenacted faces that are
prone to significant visual artifacts, especially under the challenging
condition of extreme head pose changes, or requiring expensive few-shot
fine-tuning to better preserve the source identity characteristics. We propose
to address these limitations by leveraging the photorealistic generation
ability and the disentangled properties of a pretrained StyleGAN2 generator, by
first inverting the real images into its latent space and then using a
hypernetwork to perform: (i) refinement of the source identity characteristics
and (ii) facial pose re-targeting, eliminating this way the dependence on
external editing methods that typically produce artifacts. Our method operates
under the one-shot setting (i.e., using a single source frame) and allows for
cross-subject reenactment, without requiring any subject-specific fine-tuning.
We compare our method both quantitatively and qualitatively against several
state-of-the-art techniques on the standard benchmarks of VoxCeleb1 and
VoxCeleb2, demonstrating the superiority of our approach in producing
artifact-free images, exhibiting remarkable robustness even under extreme head
pose changes. We make the code and the pretrained models publicly available at:
https://github.com/StelaBou/HyperReenact .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bounareli_S/0/1/0/all/0/1&quot;&gt;Stella Bounareli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1&quot;&gt;Christos Tzelepis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Argyriou_V/0/1/0/all/0/1&quot;&gt;Vasileios Argyriou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1&quot;&gt;Ioannis Patras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1&quot;&gt;Georgios Tzimiropoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10802">
<title>Meta-Transformer: A Unified Framework for Multimodal Learning. (arXiv:2307.10802v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10802</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal learning aims to build models that can process and relate
information from multiple modalities. Despite years of development in this
field, it still remains challenging to design a unified network for processing
various modalities ($\textit{e.g.}$ natural language, 2D images, 3D point
clouds, audio, video, time series, tabular data) due to the inherent gaps among
them. In this work, we propose a framework, named Meta-Transformer, that
leverages a $\textbf{frozen}$ encoder to perform multimodal perception without
any paired multimodal training data. In Meta-Transformer, the raw input data
from various modalities are mapped into a shared token space, allowing a
subsequent encoder with frozen parameters to extract high-level semantic
features of the input data. Composed of three main components: a unified data
tokenizer, a modality-shared encoder, and task-specific heads for downstream
tasks, Meta-Transformer is the first framework to perform unified learning
across 12 modalities with unpaired data. Experiments on different benchmarks
reveal that Meta-Transformer can handle a wide range of tasks including
fundamental perception (text, image, point cloud, audio, video), practical
application (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph,
tabular, and time-series). Meta-Transformer indicates a promising future for
developing unified multimodal intelligence with transformers. Code will be
available at https://github.com/invictus717/MetaTransformer
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yiyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_K/0/1/0/all/0/1&quot;&gt;Kaixiong Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xiangyu Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10813">
<title>Perceptual Quality Assessment of Omnidirectional Audio-visual Signals. (arXiv:2307.10813v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10813</link>
<description rdf:parseType="Literal">&lt;p&gt;Omnidirectional videos (ODVs) play an increasingly important role in the
application fields of medical, education, advertising, tourism, etc. Assessing
the quality of ODVs is significant for service-providers to improve the user&apos;s
Quality of Experience (QoE). However, most existing quality assessment studies
for ODVs only focus on the visual distortions of videos, while ignoring that
the overall QoE also depends on the accompanying audio signals. In this paper,
we first establish a large-scale audio-visual quality assessment dataset for
omnidirectional videos, which includes 375 distorted omnidirectional
audio-visual (A/V) sequences generated from 15 high-quality pristine
omnidirectional A/V contents, and the corresponding perceptual audio-visual
quality scores. Then, we design three baseline methods for full-reference
omnidirectional audio-visual quality assessment (OAVQA), which combine existing
state-of-the-art single-mode audio and video QA models via multimodal fusion
strategies. We validate the effectiveness of the A/V multimodal fusion method
for OAVQA on our dataset, which provides a new benchmark for omnidirectional
QoE evaluation. Our dataset is available at https://github.com/iamazxl/OAVQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xilei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1&quot;&gt;Huiyu Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yuqin Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuxin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yucheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Li Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1&quot;&gt;Xiongkuo Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1&quot;&gt;Guangtao Zhai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10816">
<title>BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion. (arXiv:2307.10816v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10816</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent text-to-image diffusion models have demonstrated an astonishing
capacity to generate high-quality images. However, researchers mainly studied
the way of synthesizing images with only text prompts. While some works have
explored using other modalities as conditions, considerable paired data, e.g.,
box/mask-image pairs, and fine-tuning time are required for nurturing models.
As such paired data is time-consuming and labor-intensive to acquire and
restricted to a closed set, this potentially becomes the bottleneck for
applications in an open world. This paper focuses on the simplest form of
user-provided conditions, e.g., box or scribble. To mitigate the aforementioned
problem, we propose a training-free method to control objects and contexts in
the synthesized images adhering to the given spatial conditions. Specifically,
three spatial constraints, i.e., Inner-Box, Outer-Box, and Corner Constraints,
are designed and seamlessly integrated into the denoising step of diffusion
models, requiring no additional training and massive annotated layout data.
Extensive results show that the proposed constraints can control what and where
to present in the images while retaining the ability of the Stable Diffusion
model to synthesize with high fidelity and diverse concept coverage. The code
is publicly available at https://github.com/Sierkinhane/BoxDiff.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jinheng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuexiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yawen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haozhe Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wentian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yefeng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10822">
<title>Gradient-Semantic Compensation for Incremental Semantic Segmentation. (arXiv:2307.10822v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10822</link>
<description rdf:parseType="Literal">&lt;p&gt;Incremental semantic segmentation aims to continually learn the segmentation
of new coming classes without accessing the training data of previously learned
classes. However, most current methods fail to address catastrophic forgetting
and background shift since they 1) treat all previous classes equally without
considering different forgetting paces caused by imbalanced gradient
back-propagation; 2) lack strong semantic guidance between classes. To tackle
the above challenges, in this paper, we propose a Gradient-Semantic
Compensation (GSC) model, which surmounts incremental semantic segmentation
from both gradient and semantic perspectives. Specifically, to address
catastrophic forgetting from the gradient aspect, we develop a step-aware
gradient compensation that can balance forgetting paces of previously seen
classes via re-weighting gradient backpropagation. Meanwhile, we propose a
soft-sharp semantic relation distillation to distill consistent inter-class
semantic relations via soft labels for alleviating catastrophic forgetting from
the semantic aspect. In addition, we develop a prototypical pseudo re-labeling
that provides strong semantic guidance to mitigate background shift. It
produces high-quality pseudo labels for old classes in the background by
measuring distances between pixels and class-wise prototypes. Extensive
experiments on three public datasets, i.e., Pascal VOC 2012, ADE20K, and
Cityscapes, demonstrate the effectiveness of our proposed GSC model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_W/0/1/0/all/0/1&quot;&gt;Wei Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1&quot;&gt;Yang Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jiahua Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1&quot;&gt;Gan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Henghui Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10824">
<title>Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction like Radiologists. (arXiv:2307.10824v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.10824</link>
<description rdf:parseType="Literal">&lt;p&gt;Lung cancer is a leading cause of death worldwide and early screening is
critical for improving survival outcomes. In clinical practice, the contextual
structure of nodules and the accumulated experience of radiologists are the two
core elements related to the accuracy of identification of benign and malignant
nodules. Contextual information provides comprehensive information about
nodules such as location, shape, and peripheral vessels, and experienced
radiologists can search for clues from previous cases as a reference to enrich
the basis of decision-making. In this paper, we propose a radiologist-inspired
method to simulate the diagnostic process of radiologists, which is composed of
context parsing and prototype recalling modules. The context parsing module
first segments the context structure of nodules and then aggregates contextual
information for a more comprehensive understanding of the nodule. The prototype
recalling module utilizes prototype-based learning to condense previously
learned cases as prototypes for comparative analysis, which is updated online
in a momentum way during training. Building on the two modules, our method
leverages both the intrinsic characteristics of the nodules and the external
knowledge accumulated from other nodules to achieve a sound diagnosis. To meet
the needs of both low-dose and noncontrast screening, we collect a large-scale
dataset of 12,852 and 4,029 nodules from low-dose and noncontrast CTs
respectively, each with pathology- or follow-up-confirmed labels. Experiments
on several datasets demonstrate that our method achieves advanced screening
performance on both low-dose and noncontrast scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianpeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xianghua Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yuxing Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Minfeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jianfei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zaiyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Le Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Ling Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10829">
<title>Exact Diffusion Inversion via Bi-directional Integration Approximation. (arXiv:2307.10829v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10829</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, different methods have been proposed to address the inconsistency
issue of DDIM inversion to enable image editing, such as EDICT [36] and
Null-text inversion [22]. However, the above methods introduce considerable
computational overhead. In this paper, we propose a new technique, named
bi-directional integration approximation (BDIA), to perform exact diffusion
inversion with neglible computational overhead. Suppose we would like to
estimate the next diffusion state $\boldsymbol{z}_{i-1}$ at timestep $t_i$ with
the historical information $(i,\boldsymbol{z}_i)$ and
$(i+1,\boldsymbol{z}_{i+1})$. We first obtain the estimated Gaussian noise
$\hat{\boldsymbol{\epsilon}}(\boldsymbol{z}_i,i)$, and then apply the DDIM
update procedure twice for approximating the ODE integration over the next
time-slot $[t_i, t_{i-1}]$ in the forward manner and the previous time-slot
$[t_i, t_{t+1}]$ in the backward manner. The DDIM step for the previous
time-slot is used to refine the integration approximation made earlier when
computing $\boldsymbol{z}_i$. One nice property with BDIA-DDIM is that the
update expression for $\boldsymbol{z}_{i-1}$ is a linear combination of
$(\boldsymbol{z}_{i+1}, \boldsymbol{z}_i,
\hat{\boldsymbol{\epsilon}}(\boldsymbol{z}_i,i))$. This allows for exact
backward computation of $\boldsymbol{z}_{i+1}$ given $(\boldsymbol{z}_i,
\boldsymbol{z}_{i-1})$, thus leading to exact diffusion inversion. Experiments
on both image reconstruction and image editing were conducted, confirming our
statement.
&lt;/p&gt;
&lt;p&gt;BDIA can also be applied to improve the performance of other ODE solvers in
addition to DDIM. In our work, it is found that applying BDIA to the EDM
sampling procedure produces slightly better FID score over CIFAR10.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guoqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_J/0/1/0/all/0/1&quot;&gt;J. P. Lewis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleijn_W/0/1/0/all/0/1&quot;&gt;W. Bastiaan Kleijn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10842">
<title>Label Calibration for Semantic Segmentation Under Domain Shift. (arXiv:2307.10842v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10842</link>
<description rdf:parseType="Literal">&lt;p&gt;Performance of a pre-trained semantic segmentation model is likely to
substantially decrease on data from a new domain. We show a pre-trained model
can be adapted to unlabelled target domain data by calculating soft-label
prototypes under the domain shift and making predictions according to the
prototype closest to the vector with predicted class probabilities. The
proposed adaptation procedure is fast, comes almost for free in terms of
computational resources and leads to considerable performance improvements. We
demonstrate the benefits of such label calibration on the highly-practical
synthetic-to-real semantic segmentation problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1&quot;&gt;Ondrej Bohdal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Da Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1&quot;&gt;Timothy Hospedales&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10843">
<title>Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals for GPM: A U-Net Convolutional LSTM Architecture. (arXiv:2307.10843v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10843</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a deep learning architecture for nowcasting of
precipitation almost globally every 30 min with a 4-hour lead time. The
architecture fuses a U-Net and a convolutional long short-term memory (LSTM)
neural network and is trained using data from the Integrated MultisatellitE
Retrievals for GPM (IMERG) and a few key precipitation drivers from the Global
Forecast System (GFS). The impacts of different training loss functions,
including the mean-squared error (regression) and the focal-loss
(classification), on the quality of precipitation nowcasts are studied. The
results indicate that the regression network performs well in capturing light
precipitation (below 1.6 mm/hr), but the classification network can outperform
the regression network for nowcasting of precipitation extremes (&amp;gt;8 mm/hr), in
terms of the critical success index (CSI).. Using the Wasserstein distance, it
is shown that the predicted precipitation by the classification network has a
closer class probability distribution to the IMERG than the regression network.
It is uncovered that the inclusion of the physical variables can improve
precipitation nowcasting, especially at longer lead times in both networks.
Taking IMERG as a relative reference, a multi-scale analysis in terms of
fractions skill score (FSS), shows that the nowcasting machine remains skillful
(FSS &amp;gt; 0.5) at the resolution of 10 km compared to 50 km for GFS. For
precipitation rates greater than 4~mm/hr, only the classification network
remains FSS-skillful on scales greater than 50 km within a 2-hour lead time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahimi_R/0/1/0/all/0/1&quot;&gt;Reyhaneh Rahimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebtehaj_A/0/1/0/all/0/1&quot;&gt;Ardeshir Ebtehaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behrangi_A/0/1/0/all/0/1&quot;&gt;Ali Behrangi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1&quot;&gt;Jackson Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10845">
<title>Self-paced Weight Consolidation for Continual Learning. (arXiv:2307.10845v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10845</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning algorithms which keep the parameters of new tasks close to
that of previous tasks, are popular in preventing catastrophic forgetting in
sequential task learning settings. However, 1) the performance for the new
continual learner will be degraded without distinguishing the contributions of
previously learned tasks; 2) the computational cost will be greatly increased
with the number of tasks, since most existing algorithms need to regularize all
previous tasks when learning new tasks. To address the above challenges, we
propose a self-paced Weight Consolidation (spWC) framework to attain robust
continual learning via evaluating the discriminative contributions of previous
tasks. To be specific, we develop a self-paced regularization to reflect the
priorities of past tasks via measuring difficulty based on key performance
indicator (i.e., accuracy). When encountering a new task, all previous tasks
are sorted from &quot;difficult&quot; to &quot;easy&quot; based on the priorities. Then the
parameters of the new continual learner will be learned via selectively
maintaining the knowledge amongst more difficult past tasks, which could well
overcome catastrophic forgetting with less computational cost. We adopt an
alternative convex search to iteratively update the model parameters and
priority weights in the bi-convex formulation. The proposed spWC framework is
plug-and-play, which is applicable to most continual learning algorithms (e.g.,
EWC, MAS and RCIL) in different directions (e.g., classification and
segmentation). Experimental results on several public benchmark datasets
demonstrate that our proposed framework can effectively improve performance
when compared with other popular continual learning algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_W/0/1/0/all/0/1&quot;&gt;Wei Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1&quot;&gt;Yang Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1&quot;&gt;Gan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jiahua Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10853">
<title>Exploring Effective Priors and Efficient Models for Weakly-Supervised Change Detection. (arXiv:2307.10853v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10853</link>
<description rdf:parseType="Literal">&lt;p&gt;Weakly-supervised change detection (WSCD) aims to detect pixel-level changes
with only image-level annotations. Owing to its label efficiency, WSCD is
drawing increasing attention recently. However, current WSCD methods often
encounter the challenge of change missing and fabricating, i.e., the
inconsistency between image-level annotations and pixel-level predictions.
Specifically, change missing refer to the situation that the WSCD model fails
to predict any changed pixels, even though the image-level label indicates
changed, and vice versa for change fabricating. To address this challenge, in
this work, we leverage global-scale and local-scale priors in WSCD and propose
two components: a Dilated Prior (DP) decoder and a Label Gated (LG) constraint.
The DP decoder decodes samples with the changed image-level label, skips
samples with the unchanged label, and replaces them with an all-unchanged
pixel-level label. The LG constraint is derived from the correspondence between
changed representations and image-level labels, penalizing the model when it
mispredicts the change status. Additionally, we develop TransWCD, a simple yet
powerful transformer-based model, showcasing the potential of weakly-supervised
learning in change detection. By integrating the DP decoder and LG constraint
into TransWCD, we form TransWCD-DL. Our proposed TransWCD and TransWCD-DL
achieve significant +6.33% and +9.55% F1 score improvements over the
state-of-the-art methods on the WHU-CD dataset, respectively. Some performance
metrics even exceed several fully-supervised change detection (FSCD)
competitors. Code will be available at
https://github.com/zhenghuizhao/TransWCD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhenghui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ru_L/0/1/0/all/0/1&quot;&gt;Lixiang Ru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chen Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10854">
<title>BlendFace: Re-designing Identity Encoders for Face-Swapping. (arXiv:2307.10854v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10854</link>
<description rdf:parseType="Literal">&lt;p&gt;The great advancements of generative adversarial networks and face
recognition models in computer vision have made it possible to swap identities
on images from single sources. Although a lot of studies seems to have proposed
almost satisfactory solutions, we notice previous methods still suffer from an
identity-attribute entanglement that causes undesired attributes swapping
because widely used identity encoders, eg, ArcFace, have some crucial attribute
biases owing to their pretraining on face recognition tasks. To address this
issue, we design BlendFace, a novel identity encoder for face-swapping. The key
idea behind BlendFace is training face recognition models on blended images
whose attributes are replaced with those of another mitigates inter-personal
biases such as hairsyles. BlendFace feeds disentangled identity features into
generators and guides generators properly as an identity loss function.
Extensive experiments demonstrate that BlendFace improves the
identity-attribute disentanglement in face-swapping models, maintaining a
comparable quantitative performance to previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shiohara_K/0/1/0/all/0/1&quot;&gt;Kaede Shiohara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xingchao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taketomi_T/0/1/0/all/0/1&quot;&gt;Takafumi Taketomi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10864">
<title>Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing. (arXiv:2307.10864v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10864</link>
<description rdf:parseType="Literal">&lt;p&gt;Emerging large-scale text-to-image generative models, e.g., Stable Diffusion
(SD), have exhibited overwhelming results with high fidelity. Despite the
magnificent progress, current state-of-the-art models still struggle to
generate images fully adhering to the input prompt. Prior work, Attend &amp;amp;
Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming
to optimize cross-attention during inference time to better incorporate the
semantics. It demonstrates promising results in generating simple prompts,
e.g., ``a cat and a dog&apos;&apos;. However, its efficacy declines when dealing with
more complex prompts, and it does not explicitly address the problem of
improper attribute binding. To address the challenges posed by complex prompts
or scenarios involving multiple entities and to achieve improved attribute
binding, we propose Divide &amp;amp; Bind. We introduce two novel loss objectives for
GSN: a novel attendance loss and a binding loss. Our approach stands out in its
ability to faithfully synthesize desired objects with improved attribute
alignment from complex prompts and exhibits superior performance across
multiple evaluation benchmarks. More videos and updates can be found on the
project page \url{https://sites.google.com/view/divide-and-bind}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yumeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1&quot;&gt;Margret Keuper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khoreva_A/0/1/0/all/0/1&quot;&gt;Anna Khoreva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10867">
<title>FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback. (arXiv:2307.10867v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.10867</link>
<description rdf:parseType="Literal">&lt;p&gt;Captions are crucial for understanding scientific visualizations and
documents. Existing captioning methods for scientific figures rely on
figure-caption pairs extracted from documents for training, many of which fall
short with respect to metrics like helpfulness, explainability, and
visual-descriptiveness [15] leading to generated captions being misaligned with
reader preferences. To enable the generation of high-quality figure captions,
we introduce FigCaps-HF a new framework for figure-caption generation that can
incorporate domain expert feedback in generating captions optimized for reader
preferences. Our framework comprises of 1) an automatic method for evaluating
quality of figure-caption pairs, 2) a novel reinforcement learning with human
feedback (RLHF) method to optimize a generative figure-to-caption model for
reader preferences. We demonstrate the effectiveness of our simple learning
framework by improving performance over standard fine-tuning across different
types of models. In particular, when using BLIP as the base model, our RLHF
framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and
Meteor, respectively. Finally, we release a large-scale benchmark dataset with
human feedback on figure-caption pairs to enable further evaluation and
development of RLHF techniques for this problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Ashish Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_P/0/1/0/all/0/1&quot;&gt;Prateek Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zixuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Arpita Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sungchul Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bursztyn_V/0/1/0/all/0/1&quot;&gt;Victor Bursztyn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vlassis_N/0/1/0/all/0/1&quot;&gt;Nikos Vlassis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1&quot;&gt;Ryan A. Rossi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10873">
<title>Conservative Estimation of Perception Relevance of Dynamic Objects for Safe Trajectories in Automotive Scenarios. (arXiv:2307.10873v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10873</link>
<description rdf:parseType="Literal">&lt;p&gt;Having efficient testing strategies is a core challenge that needs to be
overcome for the release of automated driving. This necessitates clear
requirements as well as suitable methods for testing. In this work, the
requirements for perception modules are considered with respect to relevance.
The concept of relevance currently remains insufficiently defined and
specified. In this paper, we propose a novel methodology to overcome this
challenge by exemplary application to collision safety in the highway domain.
Using this general system and use case specification, a corresponding concept
for relevance is derived. Irrelevant objects are thus defined as objects which
do not limit the set of safe actions available to the ego vehicle under
consideration of all uncertainties. As an initial step, the use case is
decomposed into functional scenarios with respect to collision relevance. For
each functional scenario, possible actions of both the ego vehicle and any
other dynamic object are formalized as equations. This set of possible actions
is constrained by traffic rules, yielding relevance criteria. As a result, we
present a conservative estimation which dynamic objects are relevant for
perception and need to be considered for a complete evaluation. The estimation
provides requirements which are applicable for offline testing and validation
of perception components. A visualization is presented for examples from the
highD dataset, showing the plausibility of the results. Finally, a possibility
for a future validation of the presented relevance concept is outlined.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mori_K/0/1/0/all/0/1&quot;&gt;Ken Mori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Storms_K/0/1/0/all/0/1&quot;&gt;Kai Storms&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peters_S/0/1/0/all/0/1&quot;&gt;Steven Peters&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10875">
<title>Risk-optimized Outlier Removal for Robust Point Cloud Classification. (arXiv:2307.10875v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10875</link>
<description rdf:parseType="Literal">&lt;p&gt;The popularity of point cloud deep models for safety-critical purposes has
increased, but the reliability and security of these models can be compromised
by intentional or naturally occurring point cloud noise. To combat this issue,
we present a novel point cloud outlier removal method called PointCVaR, which
empowers standard-trained models to eliminate additional outliers and restore
the data. Our approach begins by conducting attribution analysis to determine
the influence of each point on the model output, which we refer to as point
risk. We then optimize the process of filtering high-risk points using
Conditional Value at Risk (CVaR) as the objective. The rationale for this
approach is based on the observation that noise points in point clouds tend to
cluster in the tail of the risk distribution, with a low frequency but a high
level of risk, resulting in significant interference with classification
results. Despite requiring no additional training effort, our method produces
exceptional results in various removal-and-classification experiments for noisy
point clouds, which are corrupted by random noise, adversarial noise, and
backdoor trigger noise. Impressively, it achieves 87% accuracy in defense
against the backdoor attack by removing triggers. Overall, the proposed
PointCVaR effectively eliminates noise points and enhances point cloud
classification, making it a promising plug-in module for various models in
different scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Junchi Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2009.03259">
<title>Implicit Multidimensional Projection of Local Subspaces. (arXiv:2009.03259v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2009.03259</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a visualization method to understand the effect of
multidimensional projection on local subspaces, using implicit function
differentiation. Here, we understand the local subspace as the multidimensional
local neighborhood of data points. Existing methods focus on the projection of
multidimensional data points, and the neighborhood information is ignored. Our
method is able to analyze the shape and directional information of the local
subspace to gain more insights into the global structure of the data through
the perception of local structures. Local subspaces are fitted by
multidimensional ellipses that are spanned by basis vectors. An accurate and
efficient vector transformation method is proposed based on analytical
differentiation of multidimensional projections formulated as implicit
functions. The results are visualized as glyphs and analyzed using a full set
of specifically-designed interactions supported in our efficient web-based
visualization tool. The usefulness of our method is demonstrated using various
multi- and high-dimensional benchmark datasets. Our implicit differentiation
vector transformation is evaluated through numerical comparisons; the overall
method is evaluated through exploration examples and use cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_R/0/1/0/all/0/1&quot;&gt;Rongzheng Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yumeng Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Liang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Baoquan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weiskopf_D/0/1/0/all/0/1&quot;&gt;Daniel Weiskopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhai Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2105.11166">
<title>AirNet: Neural Network Transmission over the Air. (arXiv:2105.11166v6 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/2105.11166</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art performance for many edge applications is achieved by deep
neural networks (DNNs). Often, these DNNs are location- and time-sensitive, and
must be delivered over a wireless channel rapidly and efficiently. In this
paper, we introduce AirNet, a family of novel training and transmission methods
that allow DNNs to be efficiently delivered over wireless channels under
stringent transmit power and latency constraints. This corresponds to a new
class of joint source-channel coding problems, aimed at delivering DNNs with
the goal of maximizing their accuracy at the receiver, rather than recovering
them with high fidelity. In AirNet, we propose the direct mapping of the DNN
parameters to transmitted channel symbols, while the network is trained to meet
the channel constraints, and exhibit robustness against channel noise. AirNet
achieves higher accuracy compared to separation-based alternatives. We further
improve the performance of AirNet by pruning the network below the available
bandwidth, and expanding it for improved robustness. We also benefit from
unequal error protection by selectively expanding important layers of the
network. Finally, we develop an approach, which simultaneously trains a
spectrum of DNNs, each targeting a different channel condition, resolving the
impractical memory requirements of training distinct networks for different
channel conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jankowski_M/0/1/0/all/0/1&quot;&gt;Mikolaj Jankowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1&quot;&gt;Deniz Gunduz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1&quot;&gt;Krystian Mikolajczyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.04066">
<title>Semantically Adversarial Scenario Generation with Explicit Knowledge Guidance. (arXiv:2106.04066v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2106.04066</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating adversarial scenarios, which have the potential to fail autonomous
driving systems, provides an effective way to improve robustness. Extending
purely data-driven generative models, recent specialized models satisfy
additional controllable requirements such as embedding a traffic sign in a
driving scene by manipulating patterns implicitly in the neuron level. In this
paper, we introduce a method to incorporate domain knowledge explicitly in the
generation process to achieve the Semantically Adversarial Generation (SAG). To
be consistent with the composition of driving scenes, we first categorize the
knowledge into two types, the property of objects and the relationship among
objects. We then propose a tree-structured variational auto-encoder (T-VAE) to
learn hierarchical scene representation. By imposing semantic rules on the
properties of nodes and edges in the tree structure, explicit knowledge
integration enables controllable generation. We construct a synthetic example
to illustrate the controllability and explainability of our method in a
succinct setting. We further extend to realistic environments for autonomous
vehicles: our method efficiently identifies adversarial driving scenes against
different state-of-the-art 3D point cloud segmentation models and satisfies the
traffic rules specified as the explicit knowledge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1&quot;&gt;Wenhao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Haohong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Ding Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.04550">
<title>DETReg: Unsupervised Pretraining with Region Priors for Object Detection. (arXiv:2106.04550v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2106.04550</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent self-supervised pretraining methods for object detection largely focus
on pretraining the backbone of the object detector, neglecting key parts of
detection architecture. Instead, we introduce DETReg, a new self-supervised
method that pretrains the entire object detection network, including the object
localization and embedding components. During pretraining, DETReg predicts
object localizations to match the localizations from an unsupervised region
proposal generator and simultaneously aligns the corresponding feature
embeddings with embeddings from a self-supervised image encoder. We implement
DETReg using the DETR family of detectors and show that it improves over
competitive baselines when finetuned on COCO, PASCAL VOC, and Airbus Ship
benchmarks. In low-data regimes DETReg achieves improved performance, e.g.,
when training with only 1% of the labels and in the few-shot learning settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bar_A/0/1/0/all/0/1&quot;&gt;Amir Bar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kantorov_V/0/1/0/all/0/1&quot;&gt;Vadim Kantorov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reed_C/0/1/0/all/0/1&quot;&gt;Colorado J Reed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1&quot;&gt;Roei Herzig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1&quot;&gt;Gal Chechik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1&quot;&gt;Anna Rohrbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1&quot;&gt;Amir Globerson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.05216">
<title>High-order Tensor Pooling with Attention for Action Recognition. (arXiv:2110.05216v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2110.05216</link>
<description rdf:parseType="Literal">&lt;p&gt;We aim at capturing high-order statistics of feature vectors formed by a
neural network, and propose end-to-end second- and higher-order pooling to form
a tensor descriptor. Tensor descriptors require a robust similarity measure due
to low numbers of aggregated vectors and the burstiness phenomenon, when a
given feature appears more/less frequently than statistically expected. The
Heat Diffusion Process (HDP) on a graph Laplacian is closely related to the
Eigenvalue Power Normalization (EPN) of the covariance/auto-correlation matrix,
whose inverse forms a loopy graph Laplacian. We show that the HDP and the EPN
play the same role, i.e., to boost or dampen the magnitude of the eigenspectrum
thus preventing the burstiness. We equip higher-order tensors with EPN which
acts as a spectral detector of higher-order occurrences to prevent burstiness.
We also prove that for a tensor of order r built from d dimensional feature
descriptors, such a detector gives the likelihood if at least one higher-order
occurrence is &apos;projected&apos; into one of binom(d,r) subspaces represented by the
tensor; thus forming a tensor power normalization metric endowed with
binom(d,r) such &apos;detectors&apos;. For experimental contributions, we apply several
second- and higher-order pooling variants to action recognition, provide
previously not presented comparisons of such pooling variants, and show
state-of-the-art results on HMDB-51, YUP++ and MPII Cooking Activities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1&quot;&gt;Piotr Koniusz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1&quot;&gt;Ke Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.09753">
<title>HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding. (arXiv:2205.09753v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2205.09753</link>
<description rdf:parseType="Literal">&lt;p&gt;Encoding a driving scene into vector representations has been an essential
task for autonomous driving that can benefit downstream tasks e.g. trajectory
prediction. The driving scene often involves heterogeneous elements such as the
different types of objects (agents, lanes, traffic signs) and the semantic
relations between objects are rich and diverse. Meanwhile, there also exist
relativity across elements, which means that the spatial relation is a relative
concept and need be encoded in a ego-centric manner instead of in a global
coordinate system. Based on these observations, we propose Heterogeneous
Driving Graph Transformer (HDGT), a backbone modelling the driving scene as a
heterogeneous graph with different types of nodes and edges. For heterogeneous
graph construction, we connect different types of nodes according to diverse
semantic relations. For spatial relation encoding, the coordinates of the node
as well as its in-edges are in the local node-centric coordinate system. For
the aggregation module in the graph neural network (GNN), we adopt the
transformer structure in a hierarchical way to fit the heterogeneous nature of
inputs. Experimental results show that HDGT achieves state-of-the-art
performance for the task of trajectory prediction, on INTERACTION Prediction
Challenge and Waymo Open Motion Challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xiaosong Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1&quot;&gt;Penghao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Li Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.10552">
<title>Vicinity Vision Transformer. (arXiv:2206.10552v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.10552</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers have shown great success on numerous computer vision
tasks. However, its central component, softmax attention, prohibits vision
transformers from scaling up to high-resolution images, due to both the
computational complexity and memory footprint being quadratic. Although linear
attention was introduced in natural language processing (NLP) tasks to mitigate
a similar issue, directly applying existing linear attention to vision
transformers may not lead to satisfactory results. We investigate this problem
and find that computer vision tasks focus more on local information compared
with NLP tasks. Based on this observation, we present a Vicinity Attention that
introduces a locality bias to vision transformers with linear complexity.
Specifically, for each image patch, we adjust its attention weight based on its
2D Manhattan distance measured by its neighbouring patches. In this case, the
neighbouring patches will receive stronger attention than far-away patches.
Moreover, since our Vicinity Attention requires the token length to be much
larger than the feature dimension to show its efficiency advantages, we further
propose a new Vicinity Vision Transformer (VVT) structure to reduce the feature
dimension without degenerating the accuracy. We perform extensive experiments
on the CIFAR100, ImageNet1K, and ADE20K datasets to validate the effectiveness
of our method. Our method has a slower growth rate of GFlops than previous
transformer-based and convolution-based networks when the input resolution
increases. In particular, our approach achieves state-of-the-art image
classification accuracy with 50% fewer parameters than previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Weixuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zhen Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;Hui Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaihao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1&quot;&gt;Nick Barnes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Birchfield_S/0/1/0/all/0/1&quot;&gt;Stan Birchfield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1&quot;&gt;Lingpeng Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yiran Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.07902">
<title>MetaMask: Revisiting Dimensional Confounder for Self-Supervised Learning. (arXiv:2209.07902v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.07902</link>
<description rdf:parseType="Literal">&lt;p&gt;As a successful approach to self-supervised learning, contrastive learning
aims to learn invariant information shared among distortions of the input
sample. While contrastive learning has yielded continuous advancements in
sampling strategy and architecture design, it still remains two persistent
defects: the interference of task-irrelevant information and sample
inefficiency, which are related to the recurring existence of trivial constant
solutions. From the perspective of dimensional analysis, we find out that the
dimensional redundancy and dimensional confounder are the intrinsic issues
behind the phenomena, and provide experimental evidence to support our
viewpoint. We further propose a simple yet effective approach MetaMask, short
for the dimensional Mask learned by Meta-learning, to learn representations
against dimensional redundancy and confounder. MetaMask adopts the
redundancy-reduction technique to tackle the dimensional redundancy issue and
innovatively introduces a dimensional mask to reduce the gradient effects of
specific dimensions containing the confounder, which is trained by employing a
meta-learning paradigm with the objective of improving the performance of
masked representations on a typical self-supervised task. We provide solid
theoretical analyses to prove MetaMask can obtain tighter risk bounds for
downstream classification compared to typical contrastive methods. Empirically,
our method achieves state-of-the-art performance on various benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiangmeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1&quot;&gt;Wenwen Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_W/0/1/0/all/0/1&quot;&gt;Wenyi Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Changwen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1&quot;&gt;Bing Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.00255">
<title>Cascaded Multi-Modal Mixing Transformers for Alzheimer&apos;s Disease Classification with Incomplete Data. (arXiv:2210.00255v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.00255</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate medical classification requires a large number of multi-modal data,
and in many cases, different feature types. Previous studies have shown
promising results when using multi-modal data, outperforming single-modality
models when classifying diseases such as Alzheimer&apos;s Disease (AD). However,
those models are usually not flexible enough to handle missing modalities.
Currently, the most common workaround is discarding samples with missing
modalities which leads to considerable data under-utilization. Adding to the
fact that labeled medical images are already scarce, the performance of
data-driven methods like deep learning can be severely hampered. Therefore, a
multi-modal method that can handle missing data in various clinical settings is
highly desirable. In this paper, we present Multi-Modal Mixing Transformer
(3MAT), a disease classification transformer that not only leverages
multi-modal data but also handles missing data scenarios. In this work, we test
3MT for AD and Cognitively normal (CN) classification and mild cognitive
impairment (MCI) conversion prediction to progressive MCI (pMCI) or stable MCI
(sMCI) using clinical and neuroimaging data. The model uses a novel Cascaded
Modality Transformer architecture with cross-attention to incorporate
multi-modal information for more informed predictions. We propose a novel
modality dropout mechanism to ensure an unprecedented level of modality
independence and robustness to handle missing data scenarios. The result is a
versatile network that enables the mixing of arbitrary numbers of modalities
with different feature types and also ensures full data utilization missing
data scenarios. The model is trained and evaluated on the ADNI dataset with the
SOTRA performance and further evaluated with the AIBL dataset with missing
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Linfeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Siyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+To_X/0/1/0/all/0/1&quot;&gt;Xuan Vinh To&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nasrallah_F/0/1/0/all/0/1&quot;&gt;Fatima Nasrallah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chandra_S/0/1/0/all/0/1&quot;&gt;Shekhar S. Chandra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.05335">
<title>MAP: Multimodal Uncertainty-Aware Vision-Language Pre-training Model. (arXiv:2210.05335v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.05335</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal semantic understanding often has to deal with uncertainty, which
means the obtained messages tend to refer to multiple targets. Such uncertainty
is problematic for our interpretation, including inter- and intra-modal
uncertainty. Little effort has studied the modeling of this uncertainty,
particularly in pre-training on unlabeled datasets and fine-tuning in
task-specific downstream datasets. In this paper, we project the
representations of all modalities as probabilistic distributions via a
Probability Distribution Encoder (PDE) by utilizing sequence-level
interactions. Compared to the existing deterministic methods, such uncertainty
modeling can convey richer multimodal semantic information and more complex
relationships. Furthermore, we integrate uncertainty modeling with popular
pre-training frameworks and propose suitable pre-training tasks:
Distribution-based Vision-Language Contrastive learning (D-VLC),
Distribution-based Masked Language Modeling (D-MLM), and Distribution-based
Image-Text Matching (D-ITM). The fine-tuned models are applied to challenging
downstream tasks, including image-text retrieval, visual question answering,
visual reasoning, and visual entailment, and achieve state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1&quot;&gt;Yatai Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yuan Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yanru Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongfa Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaxing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakai_T/0/1/0/all/0/1&quot;&gt;Tetsuya Sakai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yujiu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.06551">
<title>MotionBERT: A Unified Perspective on Learning Human Motion Representations. (arXiv:2210.06551v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.06551</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a unified perspective on tackling various human-centric video
tasks by learning human motion representations from large-scale and
heterogeneous data resources. Specifically, we propose a pretraining stage in
which a motion encoder is trained to recover the underlying 3D motion from
noisy partial 2D observations. The motion representations acquired in this way
incorporate geometric, kinematic, and physical knowledge about human motion,
which can be easily transferred to multiple downstream tasks. We implement the
motion encoder with a Dual-stream Spatio-temporal Transformer (DSTformer)
neural network. It could capture long-range spatio-temporal relationships among
the skeletal joints comprehensively and adaptively, exemplified by the lowest
3D pose estimation error so far when trained from scratch. Furthermore, our
proposed framework achieves state-of-the-art performance on all three
downstream tasks by simply finetuning the pretrained motion encoder with a
simple regression head (1-2 layers), which demonstrates the versatility of the
learned motion representations. Code and models are available at
https://motionbert.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wentao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaoxuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Libin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wayne Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhou Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.12361">
<title>MS-DCANet: A Novel Segmentation Network For Multi-Modality COVID-19 Medical Images. (arXiv:2210.12361v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.12361</link>
<description rdf:parseType="Literal">&lt;p&gt;The Coronavirus Disease 2019 (COVID-19) pandemic has increased the public
health burden and brought profound disaster to humans. For the particularity of
the COVID-19 medical images with blurred boundaries, low contrast and different
infection sites, some researchers have improved the accuracy by adding more
complexity. Also, they overlook the complexity of lesions, which hinder their
ability to capture the relationship between segmentation sites and the
background, as well as the edge contours and global context. However,
increasing the computational complexity, parameters and inference speed is
unfavorable for model transfer from laboratory to clinic. A perfect
segmentation network needs to balance the above three factors completely. To
solve the above issues, this paper propose a symmetric automatic segmentation
framework named MS-DCANet. We introduce Tokenized MLP block, a novel attention
scheme that use a shift-window mechanism to conditionally fuse local and global
features to get more continuous boundaries and spatial positioning
capabilities. It has greater understanding of irregular lesions contours.
MS-DCANet also uses several Dual Channel blocks and a Res-ASPP block to improve
the ability to recognize small targets. On multi-modality COVID-19 tasks,
MS-DCANet achieved state-of-the-art performance compared with other baselines.
It can well trade off the accuracy and complexity. To prove the strong
generalization ability of our proposed model, we apply it to other tasks (ISIC
2018 and BAA) and achieve satisfactory results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Huazheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;Jinglong Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_G/0/1/0/all/0/1&quot;&gt;Guangtao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Baoru Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Yuanyuan Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.14085">
<title>Positive unlabeled learning with tensor networks. (arXiv:2211.14085v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.14085</link>
<description rdf:parseType="Literal">&lt;p&gt;Positive unlabeled learning is a binary classification problem with positive
and unlabeled data. It is common in domains where negative labels are costly or
impossible to obtain, e.g., medicine and personalized advertising. Most
approaches to positive unlabeled learning apply to specific data types (e.g.,
images, categorical data) and can not generate new positive and negative
samples. This work introduces a feature-space distance-based tensor network
approach to the positive unlabeled learning problem. The presented method is
not domain specific and significantly improves the state-of-the-art results on
the MNIST image and 15 categorical/mixed datasets. The trained tensor network
model is also a generative model and enables the generation of new positive and
negative instances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zunkovic_B/0/1/0/all/0/1&quot;&gt;Bojan &amp;#x17d;unkovi&amp;#x10d;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.03434">
<title>Name Your Colour For the Task: Artificially Discover Colour Naming via Colour Quantisation Transformer. (arXiv:2212.03434v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.03434</link>
<description rdf:parseType="Literal">&lt;p&gt;The long-standing theory that a colour-naming system evolves under dual
pressure of efficient communication and perceptual mechanism is supported by
more and more linguistic studies, including analysing four decades of
diachronic data from the Nafaanra language. This inspires us to explore whether
machine learning could evolve and discover a similar colour-naming system via
optimising the communication efficiency represented by high-level recognition
performance. Here, we propose a novel colour quantisation transformer,
CQFormer, that quantises colour space while maintaining the accuracy of machine
recognition on the quantised images. Given an RGB image, Annotation Branch maps
it into an index map before generating the quantised image with a colour
palette; meanwhile the Palette Branch utilises a key-point detection way to
find proper colours in the palette among the whole colour space. By interacting
with colour annotation, CQFormer is able to balance both the machine vision
accuracy and colour perceptual structure such as distinct and stable colour
distribution for discovered colour system. Very interestingly, we even observe
the consistent evolution pattern between our artificial colour system and basic
colour terms across human languages. Besides, our colour quantisation method
also offers an efficient quantisation method that effectively compresses the
image storage while maintaining high performance in high-level recognition
tasks such as classification and detection. Extensive experiments demonstrate
the superior performance of our method with extremely low bit-rate colours,
showing potential to integrate into quantisation network to quantities from
image to network activation. The source code is available at
https://github.com/ryeocthiv/CQFormer
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Shenghan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1&quot;&gt;Lin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yue Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zenghui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1&quot;&gt;Tatsuya Harada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.04246">
<title>ViTPose++: Vision Transformer Foundation Model for Generic Body Pose Estimation. (arXiv:2212.04246v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.04246</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we show the surprisingly good properties of plain vision
transformers for body pose estimation from various aspects, namely simplicity
in model structure, scalability in model size, flexibility in training
paradigm, and transferability of knowledge between models, through a simple
baseline model dubbed ViTPose. Specifically, ViTPose employs the plain and
non-hierarchical vision transformer as an encoder to encode features and a
lightweight decoder to decode body keypoints in either a top-down or a
bottom-up manner. It can be scaled up from about 20M to 1B parameters by taking
advantage of the scalable model capacity and high parallelism of the vision
transformer, setting a new Pareto front for throughput and performance.
Besides, ViTPose is very flexible regarding the attention type, input
resolution, and pre-training and fine-tuning strategy. Based on the
flexibility, a novel ViTPose+ model is proposed to deal with heterogeneous body
keypoint categories in different types of body pose estimation tasks via
knowledge factorization, i.e., adopting task-agnostic and task-specific
feed-forward networks in the transformer. We also empirically demonstrate that
the knowledge of large ViTPose models can be easily transferred to small ones
via a simple knowledge token. Experimental results show that our ViTPose model
outperforms representative methods on the challenging MS COCO Human Keypoint
Detection benchmark at both top-down and bottom-up settings. Furthermore, our
ViTPose+ model achieves state-of-the-art performance simultaneously on a series
of body pose estimation tasks, including MS COCO, AI Challenger, OCHuman, MPII
for human keypoint detection, COCO-Wholebody for whole-body keypoint detection,
as well as AP-10K and APT-36K for animal keypoint detection, without
sacrificing inference speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yufei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.13792">
<title>Periocular Biometrics: A Modality for Unconstrained Scenarios. (arXiv:2212.13792v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.13792</link>
<description rdf:parseType="Literal">&lt;p&gt;Periocular refers to the externally visible region of the face that surrounds
the eye socket. This feature-rich area can provide accurate identification in
unconstrained or uncooperative scenarios, where the iris or face modalities may
not offer sufficient biometric cues due to factors such as partial occlusion or
high subject-to-camera distance. The COVID-19 pandemic has further highlighted
its importance, as the ocular region remained the only visible facial area even
in controlled settings due to the widespread use of masks. This paper discusses
the state of the art in periocular biometrics, presenting an overall framework
encompassing its most significant research aspects, which include: (a) ocular
definition, acquisition, and detection; (b) identity recognition, including
combination with other modalities and use of various spectra; and (c) ocular
soft-biometric analysis. Finally, we conclude by addressing current challenges
and proposing future directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1&quot;&gt;Fernando Alonso-Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bigun_J/0/1/0/all/0/1&quot;&gt;Josef Bigun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1&quot;&gt;Julian Fierrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1&quot;&gt;Naser Damer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Proenca_H/0/1/0/all/0/1&quot;&gt;Hugo Proen&amp;#xe7;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1&quot;&gt;Arun Ross&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.01928">
<title>Event Camera Data Pre-training. (arXiv:2301.01928v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.01928</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a pre-trained neural network for handling event camera
data. Our model is a self-supervised learning framework, and uses paired event
camera data and natural RGB images for training.
&lt;/p&gt;
&lt;p&gt;Our method contains three modules connected in a sequence: i) a family of
event data augmentations, generating meaningful event images for
self-supervised training; ii) a conditional masking strategy to sample
informative event patches from event images, encouraging our model to capture
the spatial layout of a scene and accelerating training; iii) a contrastive
learning approach, enforcing the similarity of embeddings between matching
event images, and between paired event and RGB images. An embedding projection
loss is proposed to avoid the model collapse when enforcing the event image
embedding similarities. A probability distribution alignment loss is proposed
to encourage the event image to be consistent with its paired RGB image in the
feature space.
&lt;/p&gt;
&lt;p&gt;Transfer learning performance on downstream tasks shows the superiority of
our method over state-of-the-art methods. For example, we achieve top-1
accuracy at 64.83% on the N-ImageNet dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Liyuan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.06262">
<title>Collaborative Perception in Autonomous Driving: Methods, Datasets and Challenges. (arXiv:2301.06262v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.06262</link>
<description rdf:parseType="Literal">&lt;p&gt;Collaborative perception is essential to address occlusion and sensor failure
issues in autonomous driving. In recent years, theoretical and experimental
investigations of novel works for collaborative perception have increased
tremendously. So far, however, few reviews have focused on systematical
collaboration modules and large-scale collaborative perception datasets. This
work reviews recent achievements in this field to bridge this gap and motivate
future research. We start with a brief overview of collaboration schemes. After
that, we systematically summarize the collaborative perception methods for
ideal scenarios and real-world issues. The former focus on collaboration
modules and efficiency, and the latter is devoted to addressing the problems in
actual application. Furthermore, we present large-scale public datasets and
summarize quantitative results on these benchmarks. Finally, we highlight gaps
and overlooked challenges between current academic research and real-world
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yushan Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huifang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1&quot;&gt;Congyan Lang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yidong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08292">
<title>Navya3DSeg -- Navya 3D Semantic Segmentation Dataset &amp; split generation for autonomous vehicles. (arXiv:2302.08292v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08292</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous driving (AD) perception today relies heavily on deep learning
based architectures requiring large scale annotated datasets with their
associated costs for curation and annotation. The 3D semantic data are useful
for core perception tasks such as obstacle detection and ego-vehicle
localization. We propose a new dataset, Navya 3D Segmentation (Navya3DSeg),
with a diverse label space corresponding to a large scale production grade
operational domain, including rural, urban, industrial sites and universities
from 13 countries. It contains 23 labeled sequences and 25 supplementary
sequences without labels, designed to explore self-supervised and
semi-supervised semantic segmentation benchmarks on point clouds. We also
propose a novel method for sequential dataset split generation based on
iterative multi-label stratification, and demonstrated to achieve a +1.2% mIoU
improvement over the original split proposed by SemanticKITTI dataset. A
complete benchmark for semantic segmentation task was performed, with state of
the art methods. Finally, we demonstrate an Active Learning (AL) based dataset
distillation framework. We introduce a novel heuristic-free sampling method
called ego-pose distance based sampling in the context of AL. A detailed
presentation on the dataset is available here
https://www.youtube.com/watch?v=5m6ALIs-s20.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almin_A/0/1/0/all/0/1&quot;&gt;Alexandre Almin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemarie_L/0/1/0/all/0/1&quot;&gt;L&amp;#xe9;o Lemari&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duong_A/0/1/0/all/0/1&quot;&gt;Anh Duong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiran_B/0/1/0/all/0/1&quot;&gt;B Ravi Kiran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12112">
<title>Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation. (arXiv:2303.12112v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12112</link>
<description rdf:parseType="Literal">&lt;p&gt;The CLIP model has been recently proven to be very effective for a variety of
cross-modal tasks, including the evaluation of captions generated from
vision-and-language architectures. In this paper, we propose a new recipe for a
contrastive-based evaluation metric for image captioning, namely
Positive-Augmented Contrastive learning Score (PAC-S), that in a novel way
unifies the learning of a contrastive visual-semantic space with the addition
of generated images and text on curated data. Experiments spanning several
datasets demonstrate that our new metric achieves the highest correlation with
human judgments on both images and videos, outperforming existing
reference-based metrics like CIDEr and SPICE and reference-free metrics like
CLIP-Score. Finally, we test the system-level correlation of the proposed
metric when considering popular image captioning approaches, and assess the
impact of employing different cross-modal features. Our source code and trained
models are publicly available at: https://github.com/aimagelab/pacscore.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarto_S/0/1/0/all/0/1&quot;&gt;Sara Sarto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barraco_M/0/1/0/all/0/1&quot;&gt;Manuele Barraco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1&quot;&gt;Marcella Cornia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1&quot;&gt;Rita Cucchiara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12384">
<title>RegFormer: An Efficient Projection-Aware Transformer Network for Large-Scale Point Cloud Registration. (arXiv:2303.12384v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12384</link>
<description rdf:parseType="Literal">&lt;p&gt;Although point cloud registration has achieved remarkable advances in
object-level and indoor scenes, large-scale registration methods are rarely
explored. Challenges mainly arise from the huge point number, complex
distribution, and outliers of outdoor LiDAR scans. In addition, most existing
registration works generally adopt a two-stage paradigm: They first find
correspondences by extracting discriminative local features, and then leverage
estimators (eg. RANSAC) to filter outliers, which are highly dependent on
well-designed descriptors and post-processing choices. To address these
problems, we propose an end-to-end transformer network (RegFormer) for
large-scale point cloud alignment without any further post-processing.
Specifically, a projection-aware hierarchical transformer is proposed to
capture long-range dependencies and filter outliers by extracting point
features globally. Our transformer has linear complexity, which guarantees high
efficiency even for large-scale scenes. Furthermore, to effectively reduce
mismatches, a bijective association transformer is designed for regressing the
initial transformation. Extensive experiments on KITTI and NuScenes datasets
demonstrate that our RegFormer achieves competitive performance in terms of
both accuracy and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiuming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guangming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhe Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Chaokang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1&quot;&gt;Marc Pollefeys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hesheng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13501">
<title>Chordal Averaging on Flag Manifolds and Its Applications. (arXiv:2303.13501v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13501</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new, provably-convergent algorithm for computing the
flag-mean and flag-median of a set of points on a flag manifold under the
chordal metric. The flag manifold is a mathematical space consisting of flags,
which are sequences of nested subspaces of a vector space that increase in
dimension. The flag manifold is a superset of a wide range of known matrix
spaces, including Stiefel and Grassmanians, making it a general object that is
useful in a wide variety computer vision problems.
&lt;/p&gt;
&lt;p&gt;To tackle the challenge of computing first order flag statistics, we first
transform the problem into one that involves auxiliary variables constrained to
the Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and
leveraging the numerical stability and efficiency of Stiefel-manifold
optimization enables us to compute the flag-mean effectively. Through a series
of experiments, we show the competence of our method in Grassmann and rotation
averaging, as well as principal component analysis. We release our source code
under https://github.com/nmank/FlagAveraging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mankovich_N/0/1/0/all/0/1&quot;&gt;Nathan Mankovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1&quot;&gt;Tolga Birdal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09826">
<title>Fairness in AI and Its Long-Term Implications on Society. (arXiv:2304.09826v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09826</link>
<description rdf:parseType="Literal">&lt;p&gt;Successful deployment of artificial intelligence (AI) in various settings has
led to numerous positive outcomes for individuals and society. However, AI
systems have also been shown to harm parts of the population due to biased
predictions. AI fairness focuses on mitigating such biases to ensure AI
decision making is not discriminatory towards certain groups. We take a closer
look at AI fairness and analyze how lack of AI fairness can lead to deepening
of biases over time and act as a social stressor. More specifically, we discuss
how biased models can lead to more negative real-world outcomes for certain
groups, which may then become more prevalent by deploying new AI models trained
on increasingly biased data, resulting in a feedback loop. If the issues
persist, they could be reinforced by interactions with other risks and have
severe implications on society in the form of social unrest. We examine current
strategies for improving AI fairness, assess their limitations in terms of
real-world deployment, and explore potential paths forward to ensure we reap
AI&apos;s benefits without causing society&apos;s collapse.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1&quot;&gt;Ondrej Bohdal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1&quot;&gt;Timothy Hospedales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1&quot;&gt;Fazl Barez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04247">
<title>Estimation of control area in badminton doubles with pose information from top and back view drone videos. (arXiv:2305.04247v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04247</link>
<description rdf:parseType="Literal">&lt;p&gt;The application of visual tracking to the performance analysis of sports
players in dynamic competitions is vital for effective coaching. In doubles
matches, coordinated positioning is crucial for maintaining control of the
court and minimizing opponents&apos; scoring opportunities. The analysis of such
teamwork plays a vital role in understanding the dynamics of the game. However,
previous studies have primarily focused on analyzing and assessing singles
players without considering occlusion in broadcast videos. These studies have
relied on discrete representations, which involve the analysis and
representation of specific actions (e.g., strokes) or events that occur during
the game while overlooking the meaningful spatial distribution. In this work,
we present the first annotated drone dataset from top and back views in
badminton doubles and propose a framework to estimate the control area
probability map, which can be used to evaluate teamwork performance. We present
an efficient framework of deep neural networks that enables the calculation of
full probability surfaces. This framework utilizes the embedding of a Gaussian
mixture map of players&apos; positions and employs graph convolution on their poses.
In the experiment, we verify our approach by comparing various baselines and
discovering the correlations between the score and control area. Additionally,
we propose a practical application for assessing optimal positioning to provide
instructions during a game. Our approach offers both visual and quantitative
evaluations of players&apos; movements, thereby providing valuable insights into
doubles teamwork. The dataset and related project code is available at
https://github.com/Ning-D/Drone_BD_ControlArea
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1&quot;&gt;Ning Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1&quot;&gt;Kazuya Takeda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1&quot;&gt;Wenhui Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bei_Y/0/1/0/all/0/1&quot;&gt;Yingjiu Bei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1&quot;&gt;Keisuke Fujii&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05610">
<title>Can point cloud networks learn statistical shape models of anatomies?. (arXiv:2305.05610v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05610</link>
<description rdf:parseType="Literal">&lt;p&gt;Statistical Shape Modeling (SSM) is a valuable tool for investigating and
quantifying anatomical variations within populations of anatomies. However,
traditional correspondence-based SSM generation methods have a prohibitive
inference process and require complete geometric proxies (e.g., high-resolution
binary volumes or surface meshes) as input shapes to construct the SSM.
Unordered 3D point cloud representations of shapes are more easily acquired
from various medical imaging practices (e.g., thresholded images and surface
scanning). Point cloud deep networks have recently achieved remarkable success
in learning permutation-invariant features for different point cloud tasks
(e.g., completion, semantic segmentation, classification). However, their
application to learning SSM from point clouds is to-date unexplored. In this
work, we demonstrate that existing point cloud encoder-decoder-based completion
networks can provide an untapped potential for SSM, capturing population-level
statistical representations of shapes while reducing the inference burden and
relaxing the input requirement. We discuss the limitations of these techniques
to the SSM application and suggest future improvements. Our work paves the way
for further exploration of point cloud deep learning for SSM, a promising
avenue for advancing shape analysis literature and broadening SSM to diverse
use cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adams_J/0/1/0/all/0/1&quot;&gt;Jadie Adams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhabian_S/0/1/0/all/0/1&quot;&gt;Shireen Elhabian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05797">
<title>Fully Bayesian VIB-DeepSSM. (arXiv:2305.05797v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05797</link>
<description rdf:parseType="Literal">&lt;p&gt;Statistical shape modeling (SSM) enables population-based quantitative
analysis of anatomical shapes, informing clinical diagnosis. Deep learning
approaches predict correspondence-based SSM directly from unsegmented 3D images
but require calibrated uncertainty quantification, motivating Bayesian
formulations. Variational information bottleneck DeepSSM (VIB-DeepSSM) is an
effective, principled framework for predicting probabilistic shapes of anatomy
from images with aleatoric uncertainty quantification. However, VIB is only
half-Bayesian and lacks epistemic uncertainty inference. We derive a fully
Bayesian VIB formulation and demonstrate the efficacy of two scalable
implementation approaches: concrete dropout and batch ensemble. Additionally,
we introduce a novel combination of the two that further enhances uncertainty
calibration via multimodal marginalization. Experiments on synthetic shapes and
left atrium data demonstrate that the fully Bayesian VIB network predicts SSM
from images with improved uncertainty reasoning without sacrificing accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adams_J/0/1/0/all/0/1&quot;&gt;Jadie Adams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhabian_S/0/1/0/all/0/1&quot;&gt;Shireen Elhabian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07290">
<title>The 3rd Anti-UAV Workshop &amp; Challenge: Methods and Results. (arXiv:2305.07290v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07290</link>
<description rdf:parseType="Literal">&lt;p&gt;The 3rd Anti-UAV Workshop &amp;amp; Challenge aims to encourage research in
developing novel and accurate methods for multi-scale object tracking. The
Anti-UAV dataset used for the Anti-UAV Challenge has been publicly released.
There are two main differences between this year&apos;s competition and the previous
two. First, we have expanded the existing dataset, and for the first time,
released a training set so that participants can focus on improving their
models. Second, we set up two tracks for the first time, i.e., Anti-UAV
Tracking and Anti-UAV Detection &amp;amp; Tracking. Around 76 participating teams from
the globe competed in the 3rd Anti-UAV Challenge. In this paper, we provide a
brief summary of the 3rd Anti-UAV Workshop &amp;amp; Challenge including brief
introductions to the top three methods in each track. The submission
leaderboard will be reopened for researchers that are interested in the
Anti-UAV challenge. The benchmark dataset and other information can be found
at: https://anti-uav.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jian Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1&quot;&gt;Lei Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_J/0/1/0/all/0/1&quot;&gt;Jiaming Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhihao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1&quot;&gt;Jiangqiang Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulshad_S/0/1/0/all/0/1&quot;&gt;Sadaf Gulshad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jiaojiao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tianyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xuefeng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shihan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1&quot;&gt;Guibo Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zechao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Baigui Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yandong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Satoh_S/0/1/0/all/0/1&quot;&gt;Shin ichi Satoh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Junliang Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shengmei_J/0/1/0/all/0/1&quot;&gt;Jane Shen Shengmei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08396">
<title>MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation. (arXiv:2305.08396v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08396</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNNs) have made significant strides in medical
image analysis in recent years. However, the local nature of the convolution
operator may pose a limitation for capturing global and long-range interactions
in CNNs. Recently, Transformers have gained popularity in the computer vision
community and also medical image segmentation due to their ability to process
global features effectively. The scalability issues of self-attention mechanism
and lack of the CNN-like inductive bias may have limited their adoption.
Therefore, hybrid Vision transformers (CNN-Transformer), exploiting advantages
of both Convolution and Self-attention Mechanisms, have gained importance. In
this work, we present MaxViT-UNet, an Encoder-Decoder based hybrid vision
transformer (CNN-Transformer) for medical image segmentation. The proposed
Hybrid Decoder, based on MaxViT-block, is designed to harness the power of both
the convolution and self-attention mechanisms at each decoding stage with
nominal computational burden. The inclusion of multi-axis self-attention,
within each decoder stage, significantly enhances the discriminating capacity
between the object and background regions, and thereby helps in improving the
segmentation efficiency. In the Hybrid Decoder block, the fusion process
commences by integrating the upsampled lower level decoder features, obtained
through transpose convolution, with the skip-connection features derived from
the hybrid encoder. Subsequently, the fused features undergo refinement through
the utilization of a multi-axis attention mechanism. The proposed decoder block
is repeated multiple times to progressively segment the nuclei regions.
Experimental results on MoNuSeg18 and MoNuSAC20 dataset demonstrates the
effectiveness of the proposed technique.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Abdul Rehman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Asifullah Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07996">
<title>Point spread function modelling for astronomical telescopes: a review focused on weak gravitational lensing studies. (arXiv:2306.07996v2 [astro-ph.IM] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07996</link>
<description rdf:parseType="Literal">&lt;p&gt;The accurate modelling of the Point Spread Function (PSF) is of paramount
importance in astronomical observations, as it allows for the correction of
distortions and blurring caused by the telescope and atmosphere. PSF modelling
is crucial for accurately measuring celestial objects&apos; properties. The last
decades brought us a steady increase in the power and complexity of
astronomical telescopes and instruments. Upcoming galaxy surveys like Euclid
and LSST will observe an unprecedented amount and quality of data. Modelling
the PSF for these new facilities and surveys requires novel modelling
techniques that can cope with the ever-tightening error requirements. The
purpose of this review is three-fold. First, we introduce the optical
background required for a more physically-motivated PSF modelling and propose
an observational model that can be reused for future developments. Second, we
provide an overview of the different physical contributors of the PSF,
including the optic- and detector-level contributors and the atmosphere. We
expect that the overview will help better understand the modelled effects.
Third, we discuss the different methods for PSF modelling from the parametric
and non-parametric families for ground- and space-based telescopes, with their
advantages and limitations. Validation methods for PSF models are then
addressed, with several metrics related to weak lensing studies discussed in
detail. Finally, we explore current challenges and future directions in PSF
modelling for astronomical telescopes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Liaudat_T/0/1/0/all/0/1&quot;&gt;Tobias Liaudat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Starck_J/0/1/0/all/0/1&quot;&gt;Jean-Luc Starck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Kilbinger_M/0/1/0/all/0/1&quot;&gt;Martin Kilbinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Frugier_P/0/1/0/all/0/1&quot;&gt;Pierre-Antoine Frugier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09683">
<title>Scaling Open-Vocabulary Object Detection. (arXiv:2306.09683v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09683</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-vocabulary object detection has benefited greatly from pretrained
vision-language models, but is still limited by the amount of available
detection training data. While detection training data can be expanded by using
Web image-text pairs as weak supervision, this has not been done at scales
comparable to image-level pretraining. Here, we scale up detection data with
self-training, which uses an existing detector to generate pseudo-box
annotations on image-text pairs. Major challenges in scaling self-training are
the choice of label space, pseudo-annotation filtering, and training
efficiency. We present the OWLv2 model and OWL-ST self-training recipe, which
address these challenges. OWLv2 surpasses the performance of previous
state-of-the-art open-vocabulary detectors already at comparable training
scales (~10M examples). However, with OWL-ST, we can scale to over 1B examples,
yielding further large improvement: With an L/14 architecture, OWL-ST improves
AP on LVIS rare classes, for which the model has seen no human box annotations,
from 31.2% to 44.6% (43% relative improvement). OWL-ST unlocks Web-scale
training for open-world localization, similar to what has been seen for image
classification and language modelling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1&quot;&gt;Matthias Minderer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1&quot;&gt;Alexey Gritsenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1&quot;&gt;Neil Houlsby&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13960">
<title>Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis. (arXiv:2306.13960v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13960</link>
<description rdf:parseType="Literal">&lt;p&gt;Regular group convolutional neural networks (G-CNNs) have been shown to
increase model performance and improve equivariance to different geometrical
symmetries. This work addresses the problem of SE(3), i.e., roto-translation
equivariance, on volumetric data. Volumetric image data is prevalent in many
medical settings. Motivated by the recent work on separable group convolutions,
we devise a SE(3) group convolution kernel separated into a continuous SO(3)
(rotation) kernel and a spatial kernel. We approximate equivariance to the
continuous setting by sampling uniform SO(3) grids. Our continuous SO(3) kernel
is parameterized via RBF interpolation on similarly uniform grids. We
demonstrate the advantages of our approach in volumetric medical image
analysis. Our SE(3) equivariant models consistently outperform CNNs and regular
discrete G-CNNs on challenging medical classification tasks and show
significantly improved generalization capabilities. Our approach achieves up to
a 16.5% gain in accuracy over regular CNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuipers_T/0/1/0/all/0/1&quot;&gt;Thijs P. Kuipers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1&quot;&gt;Erik J. Bekkers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14687">
<title>GSMorph: Gradient Surgery for cine-MRI Cardiac Deformable Registration. (arXiv:2306.14687v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14687</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning-based deformable registration methods have been widely
investigated in diverse medical applications. Learning-based deformable
registration relies on weighted objective functions trading off registration
accuracy and smoothness of the deformation field. Therefore, they inevitably
require tuning the hyperparameter for optimal registration performance. Tuning
the hyperparameters is highly computationally expensive and introduces
undesired dependencies on domain knowledge. In this study, we construct a
registration model based on the gradient surgery mechanism, named GSMorph, to
achieve a hyperparameter-free balance on multiple losses. In GSMorph, we
reformulate the optimization procedure by projecting the gradient of similarity
loss orthogonally to the plane associated with the smoothness constraint,
rather than additionally introducing a hyperparameter to balance these two
competing terms. Furthermore, our method is model-agnostic and can be merged
into any deep registration network without introducing extra parameters or
slowing down inference. In this study, We compared our method with
state-of-the-art (SOTA) deformable registration approaches over two publicly
available cardiac MRI datasets. GSMorph proves superior to five SOTA
learning-based registration models and two conventional registration
techniques, SyN and Demons, on both registration accuracy and smoothness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dou_H/0/1/0/all/0/1&quot;&gt;Haoran Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bi_N/0/1/0/all/0/1&quot;&gt;Ning Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Luyi Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mann_R/0/1/0/all/0/1&quot;&gt;Ritse Mann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ni_D/0/1/0/all/0/1&quot;&gt;Dong Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ravikumar_N/0/1/0/all/0/1&quot;&gt;Nishant Ravikumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Frangi_A/0/1/0/all/0/1&quot;&gt;Alejandro F. Frangi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yunzhi Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14795">
<title>MotionGPT: Human Motion as a Foreign Language. (arXiv:2306.14795v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14795</link>
<description rdf:parseType="Literal">&lt;p&gt;Though the advancement of pre-trained large language models unfolds, the
exploration of building a unified model for language and other multi-modal
data, such as motion, remains challenging and untouched so far. Fortunately,
human motion displays a semantic coupling akin to human language, often
perceived as a form of body language. By fusing language data with large-scale
motion models, motion-language pre-training that can enhance the performance of
motion-related tasks becomes feasible. Driven by this insight, we propose
MotionGPT, a unified, versatile, and user-friendly motion-language model to
handle multiple motion-relevant tasks. Specifically, we employ the discrete
vector quantization for human motion and transfer 3D motion into motion tokens,
similar to the generation process of word tokens. Building upon this &quot;motion
vocabulary&quot;, we perform language modeling on both motion and text in a unified
manner, treating human motion as a specific language. Moreover, inspired by
prompt learning, we pre-train MotionGPT with a mixture of motion-language data
and fine-tune it on prompt-based question-and-answer tasks. Extensive
experiments demonstrate that MotionGPT achieves state-of-the-art performances
on multiple motion tasks including text-driven motion generation, motion
captioning, motion prediction, and motion in-between.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Biao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jingyi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Gang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16997">
<title>Unsupervised 3D registration through optimization-guided cyclical self-training. (arXiv:2306.16997v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16997</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art deep learning-based registration methods employ three
different learning strategies: supervised learning, which requires costly
manual annotations, unsupervised learning, which heavily relies on hand-crafted
similarity metrics designed by domain experts, or learning from synthetic data,
which introduces a domain shift. To overcome the limitations of these
strategies, we propose a novel self-supervised learning paradigm for
unsupervised registration, relying on self-training. Our idea is based on two
key insights. Feature-based differentiable optimizers 1) perform reasonable
registration even from random features and 2) stabilize the training of the
preceding feature extraction network on noisy labels. Consequently, we propose
cyclical self-training, where pseudo labels are initialized as the displacement
fields inferred from random features and cyclically updated based on more and
more expressive features from the learning feature extractor, yielding a
self-reinforcement effect. We evaluate the method for abdomen and lung
registration, consistently surpassing metric-based supervision and
outperforming diverse state-of-the-art competitors. Source code is available at
https://github.com/multimodallearning/reg-cyclical-self-train.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bigalke_A/0/1/0/all/0/1&quot;&gt;Alexander Bigalke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1&quot;&gt;Lasse Hansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mok_T/0/1/0/all/0/1&quot;&gt;Tony C. W. Mok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinrich_M/0/1/0/all/0/1&quot;&gt;Mattias P. Heinrich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01091">
<title>UW-ProCCaps: UnderWater Progressive Colourisation with Capsules. (arXiv:2307.01091v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01091</link>
<description rdf:parseType="Literal">&lt;p&gt;Underwater images are fundamental for studying and understanding the status
of marine life. We focus on reducing the memory space required for image
storage while the memory space consumption in the collecting phase limits the
time lasting of this phase leading to the need for more image collection
campaigns. We present a novel machine-learning model that reconstructs the
colours of underwater images from their luminescence channel, thus saving 2/3
of the available storage space. Our model specialises in underwater colour
reconstruction and consists of an encoder-decoder architecture. The encoder is
composed of a convolutional encoder and a parallel specialised classifier
trained with webly-supervised data. The encoder and the decoder use layers of
capsules to capture the features of the entities in the image. The colour
reconstruction process recalls the progressive and the generative adversarial
training procedures. The progressive training gives the ground for a generative
adversarial routine focused on the refining of colours giving the image bright
and saturated colours which bring the image back to life. We validate the model
both qualitatively and quantitatively on four benchmark datasets. This is the
first attempt at colour reconstruction in greyscale underwater images.
Extensive results on four benchmark datasets demonstrate that our solution
outperforms state-of-the-art (SOTA) solutions. We also demonstrate that the
generated colourisation enhances the quality of images compared to enhancement
models at the SOTA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pucci_R/0/1/0/all/0/1&quot;&gt;Rita Pucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinel_N/0/1/0/all/0/1&quot;&gt;Niki Martinel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01738">
<title>Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis. (arXiv:2307.01738v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01738</link>
<description rdf:parseType="Literal">&lt;p&gt;Trustworthy deployment of deep learning medical imaging models into
real-world clinical practice requires that they be calibrated. However, models
that are well calibrated overall can still be poorly calibrated for a
sub-population, potentially resulting in a clinician unwittingly making poor
decisions for this group based on the recommendations of the model. Although
methods have been shown to successfully mitigate biases across subgroups in
terms of model accuracy, this work focuses on the open problem of mitigating
calibration biases in the context of medical image analysis. Our method does
not require subgroup attributes during training, permitting the flexibility to
mitigate biases for different choices of sensitive attributes without
re-training. To this end, we propose a novel two-stage method: Cluster-Focal to
first identify poorly calibrated samples, cluster them into groups, and then
introduce group-wise focal loss to improve calibration bias. We evaluate our
method on skin lesion classification with the public HAM10000 dataset, and on
predicting future lesional activity for multiple sclerosis (MS) patients. In
addition to considering traditional sensitive attributes (e.g. age, sex) with
demographic subgroups, we also consider biases among groups with different
image-derived attributes, such as lesion load, which are required in medical
image analysis. Our results demonstrate that our method effectively controls
calibration error in the worst-performing subgroups while preserving prediction
performance, and outperforming recent baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shui_C/0/1/0/all/0/1&quot;&gt;Changjian Shui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Szeto_J/0/1/0/all/0/1&quot;&gt;Justin Szeto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mehta_R/0/1/0/all/0/1&quot;&gt;Raghav Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1&quot;&gt;Douglas L. Arnold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1&quot;&gt;Tal Arbel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02347">
<title>Detecting Images Generated by Deep Diffusion Models using their Local Intrinsic Dimensionality. (arXiv:2307.02347v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02347</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models recently have been successfully applied for the visual
synthesis of strikingly realistic appearing images. This raises strong concerns
about their potential for malicious purposes. In this paper, we propose using
the lightweight multi Local Intrinsic Dimensionality (multiLID), which has been
originally developed in context of the detection of adversarial examples, for
the automatic detection of synthetic images and the identification of the
according generator networks. In contrast to many existing detection
approaches, which often only work for GAN-generated images, the proposed method
provides close to perfect detection results in many realistic use cases.
Extensive experiments on known and newly created datasets demonstrate that the
proposed multiLID approach exhibits superiority in diffusion detection and
model identification. Since the empirical evaluations of recent publications on
the detection of generated images are often mainly focused on the
&quot;LSUN-Bedroom&quot; dataset, we further establish a comprehensive benchmark for the
detection of diffusion-generated images, including samples from several
diffusion models with different image sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorenz_P/0/1/0/all/0/1&quot;&gt;Peter Lorenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durall_R/0/1/0/all/0/1&quot;&gt;Ricard Durall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1&quot;&gt;Janis Keuper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05921">
<title>Reading Radiology Imaging Like The Radiologist. (arXiv:2307.05921v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05921</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated radiology report generation aims to generate radiology reports that
contain rich, fine-grained descriptions of radiology imaging. Compared with
image captioning in the natural image domain, medical images are very similar
to each other, with only minor differences in the occurrence of diseases. Given
the importance of these minor differences in the radiology report, it is
crucial to encourage the model to focus more on the subtle regions of disease
occurrence. Secondly, the problem of visual and textual data biases is serious.
Not only do normal cases make up the majority of the dataset, but sentences
describing areas with pathological changes also constitute only a small part of
the paragraph. Lastly, generating medical image reports involves the challenge
of long text generation, which requires more expertise and empirical training
in medical knowledge. As a result, the difficulty of generating such reports is
increased. To address these challenges, we propose a disease-oriented retrieval
framework that utilizes similar reports as prior knowledge references. We
design a factual consistency captioning generator to generate more accurate and
factually consistent disease descriptions. Our framework can find most similar
reports for a given disease from the CXR database by retrieving a
disease-oriented mask consisting of the position and morphological
characteristics. By referencing the disease-oriented similar report and the
visual features, the factual consistency model can generate a more accurate
radiology report.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07269">
<title>Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation. (arXiv:2307.07269v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07269</link>
<description rdf:parseType="Literal">&lt;p&gt;It is imperative to ensure the robustness of deep learning models in critical
applications such as, healthcare. While recent advances in deep learning have
improved the performance of volumetric medical image segmentation models, these
models cannot be deployed for real-world applications immediately due to their
vulnerability to adversarial attacks. We present a 3D frequency domain
adversarial attack for volumetric medical image segmentation models and
demonstrate its advantages over conventional input or voxel domain attacks.
Using our proposed attack, we introduce a novel frequency domain adversarial
training approach for optimizing a robust model against voxel and frequency
domain attacks. Moreover, we propose frequency consistency loss to regulate our
frequency domain adversarial training that achieves a better tradeoff between
model&apos;s performance on clean and adversarial samples. Code is publicly
available at https://github.com/asif-hanif/vafa.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hanif_A/0/1/0/all/0/1&quot;&gt;Asif Hanif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Naseer_M/0/1/0/all/0/1&quot;&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mubarak Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08015">
<title>Boosting 3-DoF Ground-to-Satellite Camera Localization Accuracy via Geometry-Guided Cross-View Transformer. (arXiv:2307.08015v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08015</link>
<description rdf:parseType="Literal">&lt;p&gt;Image retrieval-based cross-view localization methods often lead to very
coarse camera pose estimation, due to the limited sampling density of the
database satellite images. In this paper, we propose a method to increase the
accuracy of a ground camera&apos;s location and orientation by estimating the
relative rotation and translation between the ground-level image and its
matched/retrieved satellite image. Our approach designs a geometry-guided
cross-view transformer that combines the benefits of conventional geometry and
learnable cross-view transformers to map the ground-view observations to an
overhead view. Given the synthesized overhead view and observed satellite
feature maps, we construct a neural pose optimizer with strong global
information embedding ability to estimate the relative rotation between them.
After aligning their rotations, we develop an uncertainty-guided spatial
correlation to generate a probability map of the vehicle locations, from which
the relative translation can be determined. Experimental results demonstrate
that our method significantly outperforms the state-of-the-art. Notably, the
likelihood of restricting the vehicle lateral pose to be within 1m of its
Ground Truth (GT) value on the cross-view KITTI dataset has been improved from
$35.54\%$ to $76.44\%$, and the likelihood of restricting the vehicle
orientation to be within $1^{\circ}$ of its GT value has been improved from
$19.64\%$ to $99.10\%$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yujiao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perincherry_A/0/1/0/all/0/1&quot;&gt;Akhil Perincherry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vora_A/0/1/0/all/0/1&quot;&gt;Ankit Vora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongdong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08930">
<title>Unsupervised Deep Graph Matching Based on Cycle Consistency. (arXiv:2307.08930v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08930</link>
<description rdf:parseType="Literal">&lt;p&gt;We contribute to the sparsely populated area of unsupervised deep graph
matching with application to keypoint matching in images. Contrary to the
standard \emph{supervised} approach, our method does not require ground truth
correspondences between keypoint pairs. Instead, it is self-supervised by
enforcing consistency of matchings between images of the same object category.
As the matching and the consistency loss are discrete, their derivatives cannot
be straightforwardly used for learning. We address this issue in a principled
way by building our method upon the recent results on black-box differentiation
of combinatorial solvers. This makes our method exceptionally flexible, as it
is compatible with arbitrary network architectures and combinatorial solvers.
Our experimental evaluation suggests that our technique sets a new
state-of-the-art for unsupervised graph matching.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tourani_S/0/1/0/all/0/1&quot;&gt;Siddharth Tourani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rother_C/0/1/0/all/0/1&quot;&gt;Carsten Rother&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Muhammad Haris Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savchynskyy_B/0/1/0/all/0/1&quot;&gt;Bogdan Savchynskyy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09023">
<title>LA-Net: Landmark-Aware Learning for Reliable Facial Expression Recognition under Label Noise. (arXiv:2307.09023v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09023</link>
<description rdf:parseType="Literal">&lt;p&gt;Facial expression recognition (FER) remains a challenging task due to the
ambiguity of expressions. The derived noisy labels significantly harm the
performance in real-world scenarios. To address this issue, we present a new
FER model named Landmark-Aware Net~(LA-Net), which leverages facial landmarks
to mitigate the impact of label noise from two perspectives. Firstly, LA-Net
uses landmark information to suppress the uncertainty in expression space and
constructs the label distribution of each sample by neighborhood aggregation,
which in turn improves the quality of training supervision. Secondly, the model
incorporates landmark information into expression representations using the
devised expression-landmark contrastive loss. The enhanced expression feature
extractor can be less susceptible to label noise. Our method can be integrated
with any deep neural network for better training supervision without
introducing extra inference costs. We conduct extensive experiments on both
in-the-wild datasets and synthetic noisy datasets and demonstrate that LA-Net
achieves state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jinshi Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09676">
<title>Domain Adaptation based Enhanced Detection for Autonomous Driving in Foggy and Rainy Weather. (arXiv:2307.09676v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09676</link>
<description rdf:parseType="Literal">&lt;p&gt;Typically, object detection methods for autonomous driving that rely on
supervised learning make the assumption of a consistent feature distribution
between the training and testing data, however such assumption may fail in
different weather conditions. Due to the domain gap, a detection model trained
under clear weather may not perform well in foggy and rainy conditions.
Overcoming detection bottlenecks in foggy and rainy weather is a real challenge
for autonomous vehicles deployed in the wild. To bridge the domain gap and
improve the performance of object detectionin foggy and rainy weather, this
paper presents a novel framework for domain-adaptive object detection. The
adaptations at both the image-level and object-level are intended to minimize
the differences in image style and object appearance between domains.
Furthermore, in order to improve the model&apos;s performance on challenging
examples, we introduce a novel adversarial gradient reversal layer that
conducts adversarial mining on difficult instances in addition to domain
adaptation. Additionally, we suggest generating an auxiliary domain through
data augmentation to enforce a new domain-level metric regularization.
Experimental findings on public V2V benchmark exhibit a substantial enhancement
in object detection specifically for foggy and rainy driving scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinlong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Runsheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1&quot;&gt;Qin Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jiaqi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hongkai Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09724">
<title>AesPA-Net: Aesthetic Pattern-Aware Style Transfer Networks. (arXiv:2307.09724v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09724</link>
<description rdf:parseType="Literal">&lt;p&gt;To deliver the artistic expression of the target style, recent studies
exploit the attention mechanism owing to its ability to map the local patches
of the style image to the corresponding patches of the content image. However,
because of the low semantic correspondence between arbitrary content and
artworks, the attention module repeatedly abuses specific local patches from
the style image, resulting in disharmonious and evident repetitive artifacts.
To overcome this limitation and accomplish impeccable artistic style transfer,
we focus on enhancing the attention mechanism and capturing the rhythm of
patterns that organize the style. In this paper, we introduce a novel metric,
namely pattern repeatability, that quantifies the repetition of patterns in the
style image. Based on the pattern repeatability, we propose Aesthetic
Pattern-Aware style transfer Networks (AesPA-Net) that discover the sweet spot
of local and global style expressions. In addition, we propose a novel
self-supervisory task to encourage the attention mechanism to learn precise and
meaningful semantic correspondence. Lastly, we introduce the patch-wise style
loss to transfer the elaborate rhythm of local patterns. Through qualitative
and quantitative evaluations, we verify the reliability of the proposed pattern
repeatability that aligns with human perception, and demonstrate the
superiority of the proposed framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1&quot;&gt;Kibeom Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1&quot;&gt;Seogkyu Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junsoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_N/0/1/0/all/0/1&quot;&gt;Namhyuk Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kunhee Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1&quot;&gt;Pilhyeon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Daesik Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1&quot;&gt;Youngjung Uh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1&quot;&gt;Hyeran Byun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09906">
<title>Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation. (arXiv:2307.09906v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09906</link>
<description rdf:parseType="Literal">&lt;p&gt;Talking head video generation aims to animate a human face in a still image
with dynamic poses and expressions using motion information derived from a
target-driving video, while maintaining the person&apos;s identity in the source
image. However, dramatic and complex motions in the driving video cause
ambiguous generation, because the still source image cannot provide sufficient
appearance information for occluded regions or delicate expression variations,
which produces severe artifacts and significantly degrades the generation
quality. To tackle this problem, we propose to learn a global facial
representation space, and design a novel implicit identity representation
conditioned memory compensation network, coined as MCNet, for high-fidelity
talking head generation.~Specifically, we devise a network module to learn a
unified spatial facial meta-memory bank from all training samples, which can
provide rich facial structure and appearance priors to compensate warped source
facial features for the generation. Furthermore, we propose an effective query
mechanism based on implicit identity representations learned from the discrete
keypoints of the source image. It can greatly facilitate the retrieval of more
correlated information from the memory bank for the compensation. Extensive
experiments demonstrate that MCNet can learn representative and complementary
facial memory, and can clearly outperform previous state-of-the-art talking
head generation methods on VoxCeleb1 and CelebV datasets. Please check our
\href{https://github.com/harlanhong/ICCV2023-MCNET}{Project}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_F/0/1/0/all/0/1&quot;&gt;Fa-Ting Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10036">
<title>Class Attention to Regions of Lesion for Imbalanced Medical Image Recognition. (arXiv:2307.10036v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10036</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated medical image classification is the key component in intelligent
diagnosis systems. However, most medical image datasets contain plenty of
samples of common diseases and just a handful of rare ones, leading to major
class imbalances. Currently, it is an open problem in intelligent diagnosis to
effectively learn from imbalanced training data. In this paper, we propose a
simple yet effective framework, named \textbf{C}lass \textbf{A}ttention to
\textbf{RE}gions of the lesion (CARE), to handle data imbalance issues by
embedding attention into the training process of \textbf{C}onvolutional
\textbf{N}eural \textbf{N}etworks (CNNs). The proposed attention module helps
CNNs attend to lesion regions of rare diseases, therefore helping CNNs to learn
their characteristics more effectively. In addition, this attention module
works only during the training phase and does not change the architecture of
the original network, so it can be directly combined with any existing CNN
architecture. The CARE framework needs bounding boxes to represent the lesion
regions of rare diseases. To alleviate the need for manual annotation, we
further developed variants of CARE by leveraging the traditional saliency
methods or a pretrained segmentation model for bounding box generation. Results
show that the CARE variants with automated bounding box generation are
comparable to the original CARE framework with \textit{manual} bounding box
annotations. A series of experiments on an imbalanced skin image dataset and a
pneumonia dataset indicates that our method can effectively help the network
focus on the lesion regions of rare diseases and remarkably improves the
classification performance of rare diseases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1&quot;&gt;Jia-Xin Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jiabin Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianguo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wei-shi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruixuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10165">
<title>Drone navigation and license place detection for vehicle location in indoor spaces. (arXiv:2307.10165v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10165</link>
<description rdf:parseType="Literal">&lt;p&gt;Millions of vehicles are transported every year, tightly parked in vessels or
boats. To reduce the risks of associated safety issues like fires, knowing the
location of vehicles is essential, since different vehicles may need different
mitigation measures, e.g. electric cars. This work is aimed at creating a
solution based on a nano-drone that navigates across rows of parked vehicles
and detects their license plates. We do so via a wall-following algorithm, and
a CNN trained to detect license plates. All computations are done in real-time
on the drone, which just sends position and detected images that allow the
creation of a 2D map with the position of the plates. Our solution is capable
of reading all plates across eight test cases (with several rows of plates,
different drone speeds, or low light) by aggregation of measurements across
several drone journeys.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arvidsson_M/0/1/0/all/0/1&quot;&gt;Moa Arvidsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawirot_S/0/1/0/all/0/1&quot;&gt;Sithichot Sawirot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Englund_C/0/1/0/all/0/1&quot;&gt;Cristofer Englund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1&quot;&gt;Fernando Alonso-Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torstensson_M/0/1/0/all/0/1&quot;&gt;Martin Torstensson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duran_B/0/1/0/all/0/1&quot;&gt;Boris Duran&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>