<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-12T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05669" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05697" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05707" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05784" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05844" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05861" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05878" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05889" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05927" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05958" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05970" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05992" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06031" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06038" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06054" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06056" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06070" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06079" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06095" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06145" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06169" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06185" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06214" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06222" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06242" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.05665" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.17255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.03553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.00282" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.11365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01738" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17896" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.05727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17328" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18292" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19454" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05179" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13302" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02509" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07827" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14960" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08036" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02416" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.14194" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15952" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04257" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04498" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05221" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.05669">
<title>Multi-Modal Gaze Following in Conversational Scenarios. (arXiv:2311.05669v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05669</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaze following estimates gaze targets of in-scene person by understanding
human behavior and scene information. Existing methods usually analyze scene
images for gaze following. However, compared with visual images, audio also
provides crucial cues for determining human behavior.This suggests that we can
further improve gaze following considering audio cues. In this paper, we
explore gaze following tasks in conversational scenarios. We propose a novel
multi-modal gaze following framework based on our observation ``audiences tend
to focus on the speaker&apos;&apos;. We first leverage the correlation between audio and
lips, and classify speakers and listeners in a scene. We then use the identity
information to enhance scene images and propose a gaze candidate estimation
network. The network estimates gaze candidates from enhanced scene images and
we use MLP to match subjects with candidates as classification tasks. Existing
gaze following datasets focus on visual images while ignore audios.To evaluate
our method, we collect a conversational dataset, VideoGazeSpeech (VGS), which
is the first gaze following dataset including images and audio. Our method
significantly outperforms existing methods in VGS datasets. The visualization
result also prove the advantage of audio cues in gaze following tasks. Our work
will inspire more researches in multi-modal gaze following estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1&quot;&gt;Yuqi Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhongqun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horanyi_N/0/1/0/all/0/1&quot;&gt;Nora Horanyi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1&quot;&gt;Jaewon Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yihua Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Hyung Jin Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05697">
<title>3DGAUnet: 3D generative adversarial networks with a 3D U-Net based generator to achieve the accurate and effective synthesis of clinical tumor image data for pancreatic cancer. (arXiv:2311.05697v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.05697</link>
<description rdf:parseType="Literal">&lt;p&gt;Pancreatic ductal adenocarcinoma (PDAC) presents a critical global health
challenge, and early detection is crucial for improving the 5-year survival
rate. Recent medical imaging and computational algorithm advances offer
potential solutions for early diagnosis. Deep learning, particularly in the
form of convolutional neural networks (CNNs), has demonstrated success in
medical image analysis tasks, including classification and segmentation.
However, the limited availability of clinical data for training purposes
continues to provide a significant obstacle. Data augmentation, generative
adversarial networks (GANs), and cross-validation are potential techniques to
address this limitation and improve model performance, but effective solutions
are still rare for 3D PDAC, where contrast is especially poor owing to the high
heterogeneity in both tumor and background tissues. In this study, we developed
a new GAN-based model, named 3DGAUnet, for generating realistic 3D CT images of
PDAC tumors and pancreatic tissue, which can generate the interslice connection
data that the existing 2D CT image synthesis models lack. Our innovation is to
develop a 3D U-Net architecture for the generator to improve shape and texture
learning for PDAC tumors and pancreatic tissue. Our approach offers a promising
path to tackle the urgent requirement for creative and synergistic methods to
combat PDAC. The development of this GAN-based model has the potential to
alleviate data scarcity issues, elevate the quality of synthesized data, and
thereby facilitate the progression of deep learning models to enhance the
accuracy and early detection of PDAC tumors, which could profoundly impact
patient outcomes. Furthermore, this model has the potential to be adapted to
other types of solid tumors, hence making significant contributions to the
field of medical imaging in terms of image processing models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hannah Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baine_M/0/1/0/all/0/1&quot;&gt;Michael Baine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hollingsworth_M/0/1/0/all/0/1&quot;&gt;Michael A. Hollingsworth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Du_H/0/1/0/all/0/1&quot;&gt;Huijing Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zheng_D/0/1/0/all/0/1&quot;&gt;Dandan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hongfeng Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05698">
<title>Mirasol3B: A Multimodal Autoregressive model for time-aligned and contextual modalities. (arXiv:2311.05698v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05698</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the main challenges of multimodal learning is the need to combine
heterogeneous modalities (e.g., video, audio, text). For example, video and
audio are obtained at much higher rates than text and are roughly aligned in
time. They are often not synchronized with text, which comes as a global
context, e.g., a title, or a description. Furthermore, video and audio inputs
are of much larger volumes, and grow as the video length increases, which
naturally requires more compute dedicated to these modalities and makes
modeling of long-range dependencies harder.
&lt;/p&gt;
&lt;p&gt;We here decouple the multimodal modeling, dividing it into separate, focused
autoregressive models, processing the inputs according to the characteristics
of the modalities. We propose a multimodal model, called Mirasol3B, consisting
of an autoregressive component for the time-synchronized modalities (audio and
video), and an autoregressive component for the context modalities which are
not necessarily aligned in time but are still sequential. To address the
long-sequences of the video-audio inputs, we propose to further partition the
video and audio sequences in consecutive snippets and autoregressively process
their representations. To that end, we propose a Combiner mechanism, which
models the audio-video information jointly within a timeframe. The Combiner
learns to extract audio and video features from raw spatio-temporal signals,
and then learns to fuse these features producing compact but expressive
representations per snippet.
&lt;/p&gt;
&lt;p&gt;Our approach achieves the state-of-the-art on well established multimodal
benchmarks, outperforming much larger models. It effectively addresses the high
computational demand of media inputs by both learning compact representations,
controlling the sequence length of the audio-video feature representations, and
modeling their dependencies in time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piergiovanni_A/0/1/0/all/0/1&quot;&gt;AJ Piergiovanni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nobel_I/0/1/0/all/0/1&quot;&gt;Isaac Nobel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dahun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1&quot;&gt;Michael S. Ryoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomes_V/0/1/0/all/0/1&quot;&gt;Victor Gomes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1&quot;&gt;Anelia Angelova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05707">
<title>FMViT: A multiple-frequency mixing Vision Transformer. (arXiv:2311.05707v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05707</link>
<description rdf:parseType="Literal">&lt;p&gt;The transformer model has gained widespread adoption in computer vision tasks
in recent times. However, due to the quadratic time and memory complexity of
self-attention, which is proportional to the number of input tokens, most
existing Vision Transformers (ViTs) encounter challenges in achieving efficient
performance in practical industrial deployment scenarios, such as TensorRT and
CoreML, where traditional CNNs excel. Although some recent attempts have been
made to design CNN-Transformer hybrid architectures to tackle this problem,
their overall performance has not met expectations. To tackle these challenges,
we propose an efficient hybrid ViT architecture named FMViT. This approach
enhances the model&apos;s expressive power by blending high-frequency features and
low-frequency features with varying frequencies, enabling it to capture both
local and global information effectively. Additionally, we introduce
deploy-friendly mechanisms such as Convolutional Multigroup Reparameterization
(gMLP), Lightweight Multi-head Self-Attention (RLMHSA), and Convolutional
Fusion Block (CFB) to further improve the model&apos;s performance and reduce
computational overhead. Our experiments demonstrate that FMViT surpasses
existing CNNs, ViTs, and CNNTransformer hybrid architectures in terms of
latency/accuracy trade-offs for various vision tasks. On the TensorRT platform,
FMViT outperforms Resnet101 by 2.5% (83.3% vs. 80.8%) in top-1 accuracy on the
ImageNet dataset while maintaining similar inference latency. Moreover, FMViT
achieves comparable performance with EfficientNet-B5, but with a 43%
improvement in inference speed. On CoreML, FMViT outperforms MobileOne by 2.6%
in top-1 accuracy on the ImageNet dataset, with inference latency comparable to
MobileOne (78.5% vs. 75.9%). Our code can be found at
https://github.com/tany0699/FMViT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1&quot;&gt;Wei Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1&quot;&gt;Yifeng Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xuansong Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05708">
<title>Intelligent Cervical Spine Fracture Detection Using Deep Learning Methods. (arXiv:2311.05708v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05708</link>
<description rdf:parseType="Literal">&lt;p&gt;Cervical spine fractures constitute a critical medical emergency, with the
potential for lifelong paralysis or even fatality if left untreated or
undetected. Over time, these fractures can deteriorate without intervention. To
address the lack of research on the practical application of deep learning
techniques for the detection of spine fractures, this study leverages a dataset
containing both cervical spine fractures and non-fractured computed tomography
images. This paper introduces a two-stage pipeline designed to identify the
presence of cervical vertebrae in each image slice and pinpoint the location of
fractures. In the first stage, a multi-input network, incorporating image and
image metadata, is trained. This network is based on the Global Context Vision
Transformer, and its performance is benchmarked against popular deep learning
image classification model. In the second stage, a YOLOv8 model is trained to
detect fractures within the images, and its effectiveness is compared to
YOLOv5. The obtained results indicate that the proposed algorithm significantly
reduces the workload of radiologists and enhances the accuracy of fracture
detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nejad_R/0/1/0/all/0/1&quot;&gt;Reza Behbahani Nejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komijani_A/0/1/0/all/0/1&quot;&gt;Amir Hossein Komijani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najafi_E/0/1/0/all/0/1&quot;&gt;Esmaeil Najafi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05709">
<title>OmniVec: Learning robust representations with cross modal sharing. (arXiv:2311.05709v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05709</link>
<description rdf:parseType="Literal">&lt;p&gt;Majority of research in learning based methods has been towards designing and
training networks for specific tasks. However, many of the learning based
tasks, across modalities, share commonalities and could be potentially tackled
in a joint framework. We present an approach in such direction, to learn
multiple tasks, in multiple modalities, with a unified architecture. The
proposed network is composed of task specific encoders, a common trunk in the
middle, followed by task specific prediction heads. We first pre-train it by
self-supervised masked training, followed by sequential training for the
different tasks. We train the network on all major modalities, e.g.\ visual,
audio, text and 3D, and report results on $22$ diverse and challenging public
benchmarks. We demonstrate empirically that, using a joint network to train
across modalities leads to meaningful information sharing and this allows us to
achieve state-of-the-art results on most of the benchmarks. We also show
generalization of the trained network on cross-modal tasks as well as unseen
datasets and tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1&quot;&gt;Siddharth Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1&quot;&gt;Gaurav Sharma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05725">
<title>Whole-body Detection, Recognition and Identification at Altitude and Range. (arXiv:2311.05725v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05725</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we address the challenging task of whole-body biometric
detection, recognition, and identification at distances of up to 500m and large
pitch angles of up to 50 degree. We propose an end-to-end system evaluated on
diverse datasets, including the challenging Biometric Recognition and
Identification at Range (BRIAR) dataset. Our approach involves pre-training the
detector on common image datasets and fine-tuning it on BRIAR&apos;s complex videos
and images. After detection, we extract body images and employ a feature
extractor for recognition. We conduct thorough evaluations under various
conditions, such as different ranges and angles in indoor, outdoor, and aerial
scenarios. Our method achieves an average F1 score of 98.29% at IoU = 0.7 and
demonstrates strong performance in recognition accuracy and true acceptance
rate at low false acceptance rates compared to existing models. On a test set
of 100 subjects with 444 distractors, our model achieves a rank-20 recognition
accuracy of 75.13% and a TAR@1%FAR of 54.09%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Siyuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kathirvel_R/0/1/0/all/0/1&quot;&gt;Ram Prabhakar Kathirvel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lau_C/0/1/0/all/0/1&quot;&gt;Chun Pong Lau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1&quot;&gt;Rama Chellappa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05729">
<title>GIPCOL: Graph-Injected Soft Prompting for Compositional Zero-Shot Learning. (arXiv:2311.05729v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05729</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained vision-language models (VLMs) have achieved promising success in
many fields, especially with prompt learning paradigm. In this work, we propose
GIP-COL (Graph-Injected Soft Prompting for COmpositional Learning) to better
explore the compositional zero-shot learning (CZSL) ability of VLMs within the
prompt-based learning framework. The soft prompt in GIPCOL is structured and
consists of the prefix learnable vectors, attribute label and object label. In
addition, the attribute and object labels in the soft prompt are designated as
nodes in a compositional graph. The compositional graph is constructed based on
the compositional structure of the objects and attributes extracted from the
training data and consequently feeds the updated concept representation into
the soft prompt to capture this compositional structure for a better prompting
for CZSL. With the new prompting strategy, GIPCOL achieves state-of-the-art AUC
results on all three CZSL benchmarks, including MIT-States, UT-Zappos, and
C-GQA datasets in both closed and open settings compared to previous non-CLIP
as well as CLIP-based methods. We analyze when and why GIPCOL operates well
given the CLIP backbone and its training data limitations, and our findings
shed light on designing more effective prompts for CZSL
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1&quot;&gt;Guangyue Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1&quot;&gt;Joyce Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1&quot;&gt;Parisa Kordjamshidi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05746">
<title>Bridging the Digital Divide: Performance Variation across Socio-Economic Factors in Vision-Language Models. (arXiv:2311.05746v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.05746</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the impressive performance of current AI models reported across
various tasks, performance reports often do not include evaluations of how
these models perform on the specific groups that will be impacted by these
technologies. Among the minority groups under-represented in AI, data from
low-income households are often overlooked in data collection and model
evaluation. We evaluate the performance of a state-of-the-art vision-language
model (CLIP) on a geo-diverse dataset containing household images associated
with different income values (Dollar Street) and show that performance
inequality exists among households of different income levels. Our results
indicate that performance for the poorer groups is consistently lower than the
wealthier groups across various topics and countries. We highlight insights
that can help mitigate these issues and propose actionable steps for
economic-level inclusive AI development. Code is available at
https://github.com/MichiganNLP/Bridging_the_Digital_Divide.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nwatu_J/0/1/0/all/0/1&quot;&gt;Joan Nwatu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1&quot;&gt;Oana Ignat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1&quot;&gt;Rada Mihalcea&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05770">
<title>PolyMaX: General Dense Prediction with Mask Transformer. (arXiv:2311.05770v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05770</link>
<description rdf:parseType="Literal">&lt;p&gt;Dense prediction tasks, such as semantic segmentation, depth estimation, and
surface normal prediction, can be easily formulated as per-pixel classification
(discrete outputs) or regression (continuous outputs). This per-pixel
prediction paradigm has remained popular due to the prevalence of fully
convolutional networks. However, on the recent frontier of segmentation task,
the community has been witnessing a shift of paradigm from per-pixel prediction
to cluster-prediction with the emergence of transformer architectures,
particularly the mask transformers, which directly predicts a label for a mask
instead of a pixel. Despite this shift, methods based on the per-pixel
prediction paradigm still dominate the benchmarks on the other dense prediction
tasks that require continuous outputs, such as depth estimation and surface
normal prediction. Motivated by the success of DORN and AdaBins in depth
estimation, achieved by discretizing the continuous output space, we propose to
generalize the cluster-prediction based method to general dense prediction
tasks. This allows us to unify dense prediction tasks with the mask transformer
framework. Remarkably, the resulting model PolyMaX demonstrates
state-of-the-art performance on three benchmarks of NYUD-v2 dataset. We hope
our simple yet effective design can inspire more research on exploiting mask
transformers for more dense prediction tasks. Code and model will be made
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Liangzhe Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilber_K/0/1/0/all/0/1&quot;&gt;Kimberly Wilber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Astuti Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xiuye Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1&quot;&gt;Siyuan Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Debats_S/0/1/0/all/0/1&quot;&gt;Stephanie Debats&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huisheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1&quot;&gt;Hartwig Adam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sirotenko_M/0/1/0/all/0/1&quot;&gt;Mikhail Sirotenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liang-Chieh Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05778">
<title>DONUT-hole: DONUT Sparsification by Harnessing Knowledge and Optimizing Learning Efficiency. (arXiv:2311.05778v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05778</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces DONUT-hole, a sparse OCR-free visual document
understanding (VDU) model that addresses the limitations of its predecessor
model, dubbed DONUT. The DONUT model, leveraging a transformer architecture,
overcoming the challenges of separate optical character recognition (OCR) and
visual semantic understanding (VSU) components. However, its deployment in
production environments and edge devices is hindered by high memory and
computational demands, particularly in large-scale request services. To
overcome these challenges, we propose an optimization strategy based on
knowledge distillation and model pruning. Our paradigm to produce DONUT-hole,
reduces the model denisty by 54\% while preserving performance. We also achieve
a global representational similarity index between DONUT and DONUT-hole based
on centered kernel alignment (CKA) metric of 0.79. Moreover, we evaluate the
effectiveness of DONUT-hole in the document image key information extraction
(KIE) task, highlighting its potential for developing more efficient VDU
systems for logistic companies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaikh_A/0/1/0/all/0/1&quot;&gt;Azhar Shaikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cochez_M/0/1/0/all/0/1&quot;&gt;Michael Cochez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diachkov_D/0/1/0/all/0/1&quot;&gt;Denis Diachkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rijcke_M/0/1/0/all/0/1&quot;&gt;Michiel de Rijcke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yousefi_S/0/1/0/all/0/1&quot;&gt;Sahar Yousefi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05779">
<title>Language-guided Robot Grasping: CLIP-based Referring Grasp Synthesis in Clutter. (arXiv:2311.05779v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.05779</link>
<description rdf:parseType="Literal">&lt;p&gt;Robots operating in human-centric environments require the integration of
visual grounding and grasping capabilities to effectively manipulate objects
based on user instructions. This work focuses on the task of referring grasp
synthesis, which predicts a grasp pose for an object referred through natural
language in cluttered scenes. Existing approaches often employ multi-stage
pipelines that first segment the referred object and then propose a suitable
grasp, and are evaluated in private datasets or simulators that do not capture
the complexity of natural indoor scenes. To address these limitations, we
develop a challenging benchmark based on cluttered indoor scenes from OCID
dataset, for which we generate referring expressions and connect them with
4-DoF grasp poses. Further, we propose a novel end-to-end model (CROG) that
leverages the visual grounding capabilities of CLIP to learn grasp synthesis
directly from image-text pairs. Our results show that vanilla integration of
CLIP with pretrained models transfers poorly in our challenging benchmark,
while CROG achieves significant improvements both in terms of grounding and
grasping. Extensive robot experiments in both simulation and hardware
demonstrate the effectiveness of our approach in challenging interactive object
grasping scenarios that include clutter.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tziafas_G/0/1/0/all/0/1&quot;&gt;Georgios Tziafas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yucheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1&quot;&gt;Arushi Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasaei_M/0/1/0/all/0/1&quot;&gt;Mohammadreza Kasaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhibin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasaei_H/0/1/0/all/0/1&quot;&gt;Hamidreza Kasaei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05784">
<title>Are &quot;Hierarchical&quot; Visual Representations Hierarchical?. (arXiv:2311.05784v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05784</link>
<description rdf:parseType="Literal">&lt;p&gt;Learned visual representations often capture large amounts of semantic
information for accurate downstream applications. Human understanding of the
world is fundamentally grounded in hierarchy. To mimic this and further improve
representation capabilities, the community has explored &quot;hierarchical&quot; visual
representations that aim at modeling the underlying hierarchy of the visual
world. In this work, we set out to investigate if hierarchical visual
representations truly capture the human perceived hierarchy better than
standard learned representations. To this end, we create HierNet, a suite of 12
datasets spanning 3 kinds of hierarchy from the BREEDs subset of ImageNet.
After extensive evaluation of Hyperbolic and Matryoshka Representations across
training setups, we conclude that they do not capture hierarchy any better than
the standard representations but can assist in other aspects like search
efficiency and interpretability. Our benchmark and the datasets are
open-sourced at https://github.com/ethanlshen/HierNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_E/0/1/0/all/0/1&quot;&gt;Ethan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1&quot;&gt;Ali Farhadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1&quot;&gt;Aditya Kusupati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05798">
<title>Synthesizing Bidirectional Temporal States of Knee Osteoarthritis Radiographs with Cycle-Consistent Generative Adversarial Neural Networks. (arXiv:2311.05798v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.05798</link>
<description rdf:parseType="Literal">&lt;p&gt;Knee Osteoarthritis (KOA), a leading cause of disability worldwide, is
challenging to detect early due to subtle radiographic indicators. Diverse,
extensive datasets are needed but are challenging to compile because of
privacy, data collection limitations, and the progressive nature of KOA.
However, a model capable of projecting genuine radiographs into different OA
stages could augment data pools, enhance algorithm training, and offer
pre-emptive prognostic insights. In this study, we trained a CycleGAN model to
synthesize past and future stages of KOA on any genuine radiograph. The model
was validated using a Convolutional Neural Network that was deceived into
misclassifying disease stages in transformed images, demonstrating the
CycleGAN&apos;s ability to effectively transform disease characteristics forward or
backward in time. The model was particularly effective in synthesizing future
disease states and showed an exceptional ability to retroactively transition
late-stage radiographs to earlier stages by eliminating osteophytes and
expanding knee joint space, signature characteristics of None or Doubtful KOA.
The model&apos;s results signify a promising potential for enhancing diagnostic
models, data augmentation, and educational and prognostic usage in healthcare.
Nevertheless, further refinement, validation, and a broader evaluation process
encompassing both CNN-based assessments and expert medical feedback are
emphasized for future research and development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prezja_F/0/1/0/all/0/1&quot;&gt;Fabi Prezja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Annala_L/0/1/0/all/0/1&quot;&gt;Leevi Annala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kiiskinen_S/0/1/0/all/0/1&quot;&gt;Sampsa Kiiskinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lahtinen_S/0/1/0/all/0/1&quot;&gt;Suvi Lahtinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ojala_T/0/1/0/all/0/1&quot;&gt;Timo Ojala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05799">
<title>Adaptive Variance Thresholding: A Novel Approach to Improve Existing Deep Transfer Vision Models and Advance Automatic Knee-Joint Osteoarthritis Classification. (arXiv:2311.05799v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.05799</link>
<description rdf:parseType="Literal">&lt;p&gt;Knee-Joint Osteoarthritis (KOA) is a prevalent cause of global disability and
is inherently complex to diagnose due to its subtle radiographic markers and
individualized progression. One promising classification avenue involves
applying deep learning methods; however, these techniques demand extensive,
diversified datasets, which pose substantial challenges due to medical data
collection restrictions. Existing practices typically resort to smaller
datasets and transfer learning. However, this approach often inherits
unnecessary pre-learned features that can clutter the classifier&apos;s vector
space, potentially hampering performance. This study proposes a novel paradigm
for improving post-training specialized classifiers by introducing adaptive
variance thresholding (AVT) followed by Neural Architecture Search (NAS). This
approach led to two key outcomes: an increase in the initial accuracy of the
pre-trained KOA models and a 60-fold reduction in the NAS input vector space,
thus facilitating faster inference speed and a more efficient hyperparameter
search. We also applied this approach to an external model trained for KOA
classification. Despite its initial performance, the application of our
methodology improved its average accuracy, making it one of the top three KOA
classification models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prezja_F/0/1/0/all/0/1&quot;&gt;Fabi Prezja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Annala_L/0/1/0/all/0/1&quot;&gt;Leevi Annala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kiiskinen_S/0/1/0/all/0/1&quot;&gt;Sampsa Kiiskinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lahtinen_S/0/1/0/all/0/1&quot;&gt;Suvi Lahtinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ojala_T/0/1/0/all/0/1&quot;&gt;Timo Ojala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05828">
<title>Diffusion Shape Prior for Wrinkle-Accurate Cloth Registration. (arXiv:2311.05828v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05828</link>
<description rdf:parseType="Literal">&lt;p&gt;Registering clothes from 4D scans with vertex-accurate correspondence is
challenging, yet important for dynamic appearance modeling and physics
parameter estimation from real-world data. However, previous methods either
rely on texture information, which is not always reliable, or achieve only
coarse-level alignment. In this work, we present a novel approach to enabling
accurate surface registration of texture-less clothes with large deformation.
Our key idea is to effectively leverage a shape prior learned from pre-captured
clothing using diffusion models. We also propose a multi-stage guidance scheme
based on learned functional maps, which stabilizes registration for large-scale
deformation even when they vary significantly from training data. Using
high-fidelity real captured clothes, our experiments show that the proposed
approach based on diffusion models generalizes better than surface registration
with VAE or PCA-based priors, outperforming both optimization-based and
learning-based non-rigid registration methods for both interpolation and
extrapolation tests.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jingfan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prada_F/0/1/0/all/0/1&quot;&gt;Fabian Prada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_D/0/1/0/all/0/1&quot;&gt;Donglai Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_J/0/1/0/all/0/1&quot;&gt;Javier Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chenglei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1&quot;&gt;Hyun Soo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shiratori_T/0/1/0/all/0/1&quot;&gt;Takaaki Shiratori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1&quot;&gt;Shunsuke Saito&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05836">
<title>Uncertainty-aware Single View Volumetric Rendering for Medical Neural Radiance Fields. (arXiv:2311.05836v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.05836</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of clinical medicine, computed tomography (CT) is an effective
medical imaging modality for the diagnosis of various pathologies. Compared
with X-ray images, CT images can provide more information, including
multi-planar slices and three-dimensional structures for clinical diagnosis.
However, CT imaging requires patients to be exposed to large doses of ionizing
radiation for a long time, which may cause irreversible physical harm. In this
paper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on
generated radiation fields. The network can learn a continuous representation
of CT projections from 2D X-ray images by obtaining the internal structure and
depth information and using adaptive loss weights to ensure the quality of the
generated images. Our model is trained on publicly available knee and chest
datasets, and we show the results of CT projection rendering with a single
X-ray and compare our method with other methods based on generated radiation
fields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jing Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fan_Q/0/1/0/all/0/1&quot;&gt;Qinrui Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Siwei Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05844">
<title>Face-StyleSpeech: Improved Face-to-Voice latent mapping for Natural Zero-shot Speech Synthesis from a Face Image. (arXiv:2311.05844v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05844</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating a voice from a face image is crucial for developing virtual humans
capable of interacting using their unique voices, without relying on
pre-recorded human speech. In this paper, we propose Face-StyleSpeech, a
zero-shot Text-To-Speech (TTS) synthesis model that generates natural speech
conditioned on a face image rather than reference speech. We hypothesize that
learning both speaker identity and prosody from a face image poses a
significant challenge. To address the issue, our TTS model incorporates both a
face encoder and a prosody encoder. The prosody encoder is specifically
designed to model prosodic features that are not captured only with a face
image, allowing the face encoder to focus solely on capturing the speaker
identity from the face image. Experimental results demonstrate that
Face-StyleSpeech effectively generates more natural speech from a face image
than baselines, even for the face images the model has not trained. Samples are
at our demo page https://face-stylespeech.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1&quot;&gt;Minki Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1&quot;&gt;Wooseok Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1&quot;&gt;Eunho Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05858">
<title>Layer-wise Auto-Weighting for Non-Stationary Test-Time Adaptation. (arXiv:2311.05858v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.05858</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the inevitability of domain shifts during inference in real-world
applications, test-time adaptation (TTA) is essential for model adaptation
after deployment. However, the real-world scenario of continuously changing
target distributions presents challenges including catastrophic forgetting and
error accumulation. Existing TTA methods for non-stationary domain shifts,
while effective, incur excessive computational load, making them impractical
for on-device settings. In this paper, we introduce a layer-wise auto-weighting
algorithm for continual and gradual TTA that autonomously identifies layers for
preservation or concentrated adaptation. By leveraging the Fisher Information
Matrix (FIM), we first design the learning weight to selectively focus on
layers associated with log-likelihood changes while preserving unrelated ones.
Then, we further propose an exponential min-max scaler to make certain layers
nearly frozen while mitigating outliers. This minimizes forgetting and error
accumulation, leading to efficient adaptation to non-stationary target
distribution. Experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C show our
method outperforms conventional continual and gradual TTA approaches while
significantly reducing computational load, highlighting the importance of
FIM-based learning weight in adapting to continuously or gradually shifting
target domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Junyoung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1&quot;&gt;Hyeongjun Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_I/0/1/0/all/0/1&quot;&gt;Ilhoon Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1&quot;&gt;Kwanghoon Sohn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05861">
<title>Domain Generalization by Learning from Privileged Medical Imaging Information. (arXiv:2311.05861v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05861</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning the ability to generalize knowledge between similar contexts is
particularly important in medical imaging as data distributions can shift
substantially from one hospital to another, or even from one machine to
another. To strengthen generalization, most state-of-the-art techniques inject
knowledge of the data distribution shifts by enforcing constraints on learned
features or regularizing parameters. We offer an alternative approach: Learning
from Privileged Medical Imaging Information (LPMII). We show that using some
privileged information such as tumor shape or location leads to stronger domain
generalization ability than current state-of-the-art techniques. This paper
demonstrates that by using privileged information to predict the severity of
intra-layer retinal fluid in optical coherence tomography scans, the
classification accuracy of a deep learning model operating on
out-of-distribution data improves from $0.911$ to $0.934$. This paper provides
a strong starting point for using privileged information in other medical
problems requiring generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korevaar_S/0/1/0/all/0/1&quot;&gt;Steven Korevaar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tennakoon_R/0/1/0/all/0/1&quot;&gt;Ruwan Tennakoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OBrien_R/0/1/0/all/0/1&quot;&gt;Ricky O&amp;#x27;Brien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahapatra_D/0/1/0/all/0/1&quot;&gt;Dwarikanath Mahapatra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bab_Hadiasha_A/0/1/0/all/0/1&quot;&gt;Alireza Bab-Hadiasha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05863">
<title>Watermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service. (arXiv:2311.05863v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2311.05863</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in vision-language pre-trained models (VLPs) have
significantly increased visual understanding and cross-modal analysis
capabilities. Companies have emerged to provide multi-modal Embedding as a
Service (EaaS) based on VLPs (e.g., CLIP-based VLPs), which cost a large amount
of training data and resources for high-performance service. However, existing
studies indicate that EaaS is vulnerable to model extraction attacks that
induce great loss for the owners of VLPs. Protecting the intellectual property
and commercial ownership of VLPs is increasingly crucial yet challenging. A
major solution of watermarking model for EaaS implants a backdoor in the model
by inserting verifiable trigger embeddings into texts, but it is only
applicable for large language models and is unrealistic due to data and model
privacy. In this paper, we propose a safe and robust backdoor-based embedding
watermarking method for VLPs called VLPMarker. VLPMarker utilizes embedding
orthogonal transformation to effectively inject triggers into the VLPs without
interfering with the model parameters, which achieves high-quality copyright
verification and minimal impact on model performance. To enhance the watermark
robustness, we further propose a collaborative copyright verification strategy
based on both backdoor trigger and embedding distribution, enhancing resilience
against various attacks. We increase the watermark practicality via an
out-of-distribution trigger selection approach, removing access to the model
training data and thus making it possible for many real-world scenarios. Our
extensive experiments on various datasets indicate that the proposed
watermarking approach is effective and safe for verifying the copyright of VLPs
for multi-modal EaaS and robust against model extraction attacks. Our code is
available at https://github.com/Pter61/vlpmarker.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yuanmin Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1&quot;&gt;Keke Gai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xiangyan Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yue Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_G/0/1/0/all/0/1&quot;&gt;Gang Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qi Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05870">
<title>Automated Heterogeneous Low-Bit Quantization of Multi-Model Deep Learning Inference Pipeline. (arXiv:2311.05870v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05870</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiple Deep Neural Networks (DNNs) integrated into single Deep Learning
(DL) inference pipelines e.g. Multi-Task Learning (MTL) or Ensemble Learning
(EL), etc., albeit very accurate, pose challenges for edge deployment. In these
systems, models vary in their quantization tolerance and resource demands,
requiring meticulous tuning for accuracy-latency balance. This paper introduces
an automated heterogeneous quantization approach for DL inference pipelines
with multiple DNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondal_J/0/1/0/all/0/1&quot;&gt;Jayeeta Mondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1&quot;&gt;Swarnava Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1&quot;&gt;Arijit Mukherjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05878">
<title>Central Angle Optimization for 360-degree Holographic 3D Content. (arXiv:2311.05878v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05878</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we propose a method to find an optimal central angle in deep
learning-based depth map estimation used to produce realistic holographic
content. The acquisition of RGB-depth map images as detailed as possible must
be performed to generate holograms of high quality, despite the high
computational cost. Therefore, we introduce a novel pipeline designed to
analyze various values of central angles between adjacent camera viewpoints
equidistant from the origin of an object-centered environment. Then we propose
the optimal central angle to generate high-quality holographic content. The
proposed pipeline comprises key steps such as comparing estimated depth maps
and comparing reconstructed CGHs (Computer-Generated Holograms) from RGB images
and estimated depth maps. We experimentally demonstrate and discuss the
relationship between the central angle and the quality of digital holographic
content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hakdong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_M/0/1/0/all/0/1&quot;&gt;Minsung Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1&quot;&gt;Cheongwon Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05889">
<title>Semantic Map Guided Synthesis of Wireless Capsule Endoscopy Images using Diffusion Models. (arXiv:2311.05889v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.05889</link>
<description rdf:parseType="Literal">&lt;p&gt;Wireless capsule endoscopy (WCE) is a non-invasive method for visualizing the
gastrointestinal (GI) tract, crucial for diagnosing GI tract diseases. However,
interpreting WCE results can be time-consuming and tiring. Existing studies
have employed deep neural networks (DNNs) for automatic GI tract lesion
detection, but acquiring sufficient training examples, particularly due to
privacy concerns, remains a challenge. Public WCE databases lack diversity and
quantity. To address this, we propose a novel approach leveraging generative
models, specifically the diffusion model (DM), for generating diverse WCE
images. Our model incorporates semantic map resulted from visualization scale
(VS) engine, enhancing the controllability and diversity of generated images.
We evaluate our approach using visual inspection and visual Turing tests,
demonstrating its effectiveness in generating realistic and diverse WCE images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Haejin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ju_J/0/1/0/all/0/1&quot;&gt;Jeongwoo Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jonghyuck Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yeoun Joo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jung_H/0/1/0/all/0/1&quot;&gt;Heechul Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05919">
<title>Inter-object Discriminative Graph Modeling for Indoor Scene Recognition. (arXiv:2311.05919v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05919</link>
<description rdf:parseType="Literal">&lt;p&gt;Variable scene layouts and coexisting objects across scenes make indoor scene
recognition still a challenging task. Leveraging object information within
scenes to enhance the distinguishability of feature representations has emerged
as a key approach in this domain. Currently, most object-assisted methods use a
separate branch to process object information, combining object and scene
features heuristically. However, few of them pay attention to interpretably
handle the hidden discriminative knowledge within object information. In this
paper, we propose to leverage discriminative object knowledge to enhance scene
feature representations. Initially, we capture the object-scene discriminative
relationships from a probabilistic perspective, which are transformed into an
Inter-Object Discriminative Prototype (IODP). Given the abundant prior
knowledge from IODP, we subsequently construct a Discriminative Graph Network
(DGN), in which pixel-level scene features are defined as nodes and the
discriminative relationships between node features are encoded as edges. DGN
aims to incorporate inter-object discriminative knowledge into the image
representation through graph convolution. With the proposed IODP and DGN, we
obtain state-of-the-art results on several widely used scene datasets,
demonstrating the effectiveness of the proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Chuanxin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hanbo Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yibin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05927">
<title>Automated Sperm Assessment Framework and Neural Network Specialized for Sperm Video Recognition. (arXiv:2311.05927v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05927</link>
<description rdf:parseType="Literal">&lt;p&gt;Infertility is a global health problem, and an increasing number of couples
are seeking medical assistance to achieve reproduction, at least half of which
are caused by men. The success rate of assisted reproductive technologies
depends on sperm assessment, in which experts determine whether sperm can be
used for reproduction based on morphology and motility of sperm. Previous sperm
assessment studies with deep learning have used datasets comprising images that
include only sperm heads, which cannot consider motility and other morphologies
of sperm. Furthermore, the labels of the dataset are one-hot, which provides
insufficient support for experts, because assessment results are inconsistent
between experts, and they have no absolute answer. Therefore, we constructed
the video dataset for sperm assessment whose videos include sperm head as well
as neck and tail, and its labels were annotated with soft-label. Furthermore,
we proposed the sperm assessment framework and the neural network, RoSTFine,
for sperm video recognition. Experimental results showed that RoSTFine could
improve the sperm assessment performances compared to existing video
recognition models and focus strongly on important sperm parts (i.e., head and
neck).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujii_T/0/1/0/all/0/1&quot;&gt;Takuro Fujii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakagawa_H/0/1/0/all/0/1&quot;&gt;Hayato Nakagawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takeshima_T/0/1/0/all/0/1&quot;&gt;Teppei Takeshima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yumura_Y/0/1/0/all/0/1&quot;&gt;Yasushi Yumura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamagami_T/0/1/0/all/0/1&quot;&gt;Tomoki Hamagami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05929">
<title>Efficient Segmentation with Texture in Ore Images Based on Box-supervised Approach. (arXiv:2311.05929v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05929</link>
<description rdf:parseType="Literal">&lt;p&gt;Image segmentation methods have been utilized to determine the particle size
distribution of crushed ores. Due to the complex working environment,
high-powered computing equipment is difficult to deploy. At the same time, the
ore distribution is stacked, and it is difficult to identify the complete
features. To address this issue, an effective box-supervised technique with
texture features is provided for ore image segmentation that can identify
complete and independent ores. Firstly, a ghost feature pyramid network
(Ghost-FPN) is proposed to process the features obtained from the backbone to
reduce redundant semantic information and computation generated by complex
networks. Then, an optimized detection head is proposed to obtain the feature
to maintain accuracy. Finally, Lab color space (Lab) and local binary patterns
(LBP) texture features are combined to form a fusion feature similarity-based
loss function to improve accuracy while incurring no loss. Experiments on MS
COCO have shown that the proposed fusion features are also worth studying on
other types of datasets. Extensive experimental results demonstrate the
effectiveness of the proposed method, which achieves over 50 frames per second
with a small model size of 21.6 MB. Meanwhile, the method maintains a high
level of accuracy compared with the state-of-the-art approaches on ore image
dataset. The source code is available at
\url{https://github.com/MVME-HBUT/OREINST}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1&quot;&gt;Guodong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Delong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yuting Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Le Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bo Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05958">
<title>A Neural Height-Map Approach for the Binocular Photometric Stereo Problem. (arXiv:2311.05958v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05958</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we propose a novel, highly practical, binocular photometric
stereo (PS) framework, which has same acquisition speed as single view PS,
however significantly improves the quality of the estimated geometry.
&lt;/p&gt;
&lt;p&gt;As in recent neural multi-view shape estimation frameworks such as NeRF,
SIREN and inverse graphics approaches to multi-view photometric stereo (e.g.
PS-NeRF) we formulate shape estimation task as learning of a differentiable
surface and texture representation by minimising surface normal discrepancy for
normals estimated from multiple varying light images for two views as well as
discrepancy between rendered surface intensity and observed images. Our method
differs from typical multi-view shape estimation approaches in two key ways.
First, our surface is represented not as a volume but as a neural heightmap
where heights of points on a surface are computed by a deep neural network.
Second, instead of predicting an average intensity as PS-NeRF or introducing
lambertian material assumptions as Guo et al., we use a learnt BRDF and perform
near-field per point intensity rendering.
&lt;/p&gt;
&lt;p&gt;Our method achieves the state-of-the-art performance on the DiLiGenT-MV
dataset adapted to binocular stereo setup as well as a new binocular
photometric stereo dataset - LUCES-ST.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Logothetis_F/0/1/0/all/0/1&quot;&gt;Fotios Logothetis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budvytis_I/0/1/0/all/0/1&quot;&gt;Ignas Budvytis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cipolla_R/0/1/0/all/0/1&quot;&gt;Roberto Cipolla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05970">
<title>Quantized Distillation: Optimizing Driver Activity Recognition Models for Resource-Constrained Environments. (arXiv:2311.05970v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05970</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning-based models are at the forefront of most driver observation
benchmarks due to their remarkable accuracies but are also associated with high
computational costs. This is challenging, as resources are often limited in
real-world driving scenarios. This paper introduces a lightweight framework for
resource-efficient driver activity recognition. The framework enhances 3D
MobileNet, a neural architecture optimized for speed in video classification,
by incorporating knowledge distillation and model quantization to balance model
accuracy and computational efficiency. Knowledge distillation helps maintain
accuracy while reducing the model size by leveraging soft labels from a larger
teacher model (I3D), instead of relying solely on original ground truth data.
Model quantization significantly lowers memory and computation demands by using
lower precision integers for model weights and activations. Extensive testing
on a public dataset for in-vehicle monitoring during autonomous driving
demonstrates that this new framework achieves a threefold reduction in model
size and a 1.4-fold improvement in inference time, compared to an already
optimized architecture. The code for this study is available at
https://github.com/calvintanama/qd-driver-activity-reco.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanama_C/0/1/0/all/0/1&quot;&gt;Calvin Tanama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1&quot;&gt;Kunyu Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marinov_Z/0/1/0/all/0/1&quot;&gt;Zdravko Marinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1&quot;&gt;Rainer Stiefelhagen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1&quot;&gt;Alina Roitberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05981">
<title>Comparing Male Nyala and Male Kudu Classification using Transfer Learning with ResNet-50 and VGG-16. (arXiv:2311.05981v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05981</link>
<description rdf:parseType="Literal">&lt;p&gt;Reliable and efficient monitoring of wild animals is crucial to inform
management and conservation decisions. The process of manually identifying
species of animals is time-consuming, monotonous, and expensive. Leveraging on
advances in deep learning and computer vision, we investigate in this paper the
efficiency of pre-trained models, specifically the VGG-16 and ResNet-50 model,
in identifying a male Kudu and a male Nyala in their natural habitats. These
pre-trained models have proven to be efficient in animal identification in
general. Still, there is little research on animals like the Kudu and Nyala,
who are usually well camouflaged and have similar features. The method of
transfer learning used in this paper is the fine-tuning method. The models are
evaluated before and after fine-tuning. The experimental results achieved an
accuracy of 93.2\% and 97.7\% for the VGG-16 and ResNet-50 models,
respectively, before fine-tuning and 97.7\% for both models after fine-tuning.
Although these results are impressive, it should be noted that they were taken
over a small sample size of 550 images split in half between the two classes;
therefore, this might not cater to enough scenarios to get a full conclusion of
the efficiency of the models. Therefore, there is room for more work in getting
a more extensive dataset and testing and extending to the female counterparts
of these species and the whole antelope species.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemani_T/0/1/0/all/0/1&quot;&gt;T.T Lemani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zyl_T/0/1/0/all/0/1&quot;&gt;T.L. van Zyl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05988">
<title>Vision Big Bird: Random Sparsification for Full Attention. (arXiv:2311.05988v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05988</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Transformers have shown promising performance in various vision
tasks. However, the high costs of global self-attention remain challenging for
Transformers, especially for high-resolution vision tasks. Inspired by one of
the most successful transformers-based models for NLP: Big Bird, we propose a
novel sparse attention mechanism for Vision Transformers (ViT). Specifically,
we separate the heads into three groups, the first group used convolutional
neural network (CNN) to extract local features and provide positional
information for the model, the second group used Random Sampling Windows
(RS-Win) for sparse self-attention calculation, and the third group reduces the
resolution of the keys and values by average pooling for global attention.
Based on these components, ViT maintains the sparsity of self-attention while
maintaining the merits of Big Bird (i.e., the model is a universal approximator
of sequence functions and is Turing complete). Moreover, our results show that
the positional encoding, a crucial component in ViTs, can be safely removed in
our model. Experiments show that Vision Big Bird demonstrates competitive
performance on common vision tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhemin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1&quot;&gt;Xun Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05992">
<title>Robust Adversarial Attacks Detection for Deep Learning based Relative Pose Estimation for Space Rendezvous. (arXiv:2311.05992v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05992</link>
<description rdf:parseType="Literal">&lt;p&gt;Research on developing deep learning techniques for autonomous spacecraft
relative navigation challenges is continuously growing in recent years.
Adopting those techniques offers enhanced performance. However, such approaches
also introduce heightened apprehensions regarding the trustability and security
of such deep learning methods through their susceptibility to adversarial
attacks. In this work, we propose a novel approach for adversarial attack
detection for deep neural network-based relative pose estimation schemes based
on the explainability concept. We develop for an orbital rendezvous scenario an
innovative relative pose estimation technique adopting our proposed
Convolutional Neural Network (CNN), which takes an image from the chaser&apos;s
onboard camera and outputs accurately the target&apos;s relative position and
rotation. We perturb seamlessly the input images using adversarial attacks that
are generated by the Fast Gradient Sign Method (FGSM). The adversarial attack
detector is then built based on a Long Short Term Memory (LSTM) network which
takes the explainability measure namely SHapley Value from the CNN-based pose
estimator and flags the detection of adversarial attacks when acting.
Simulation results show that the proposed adversarial attack detector achieves
a detection accuracy of 99.21%. Both the deep relative pose estimator and
adversarial attack detector are then tested on real data captured from our
laboratory-designed setup. The experimental results from our
laboratory-designed setup demonstrate that the proposed adversarial attack
detector achieves an average detection accuracy of 96.29%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aouf_N/0/1/0/all/0/1&quot;&gt;Nabil Aouf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pizarro_J/0/1/0/all/0/1&quot;&gt;Jose Pizarro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honvault_C/0/1/0/all/0/1&quot;&gt;Christophe Honvault&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06000">
<title>Keystroke Verification Challenge (KVC): Biometric and Fairness Benchmark Evaluation. (arXiv:2311.06000v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06000</link>
<description rdf:parseType="Literal">&lt;p&gt;Analyzing keystroke dynamics (KD) for biometric verification has several
advantages: it is among the most discriminative behavioral traits; keyboards
are among the most common human-computer interfaces, being the primary means
for users to enter textual data; its acquisition does not require additional
hardware, and its processing is relatively lightweight; and it allows for
transparently recognizing subjects. However, the heterogeneity of experimental
protocols and metrics, and the limited size of the databases adopted in the
literature impede direct comparisons between different systems, thus
representing an obstacle in the advancement of keystroke biometrics. To
alleviate this aspect, we present a new experimental framework to benchmark
KD-based biometric verification performance and fairness based on tweet-long
sequences of variable transcript text from over 185,000 subjects, acquired
through desktop and mobile keyboards, extracted from the Aalto Keystroke
Databases. The framework runs on CodaLab in the form of the Keystroke
Verification Challenge (KVC). Moreover, we also introduce a novel fairness
metric, the Skewed Impostor Ratio (SIR), to capture inter- and
intra-demographic group bias patterns in the verification scores. We
demonstrate the usefulness of the proposed framework by employing two
state-of-the-art keystroke verification systems, TypeNet and TypeFormer, to
compare different sets of input features, achieving a less privacy-invasive
system, by discarding the analysis of text content (ASCII codes of the keys
pressed) in favor of extended features in the time domain. Our experiments show
that this approach allows to maintain satisfactory performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stragapede_G/0/1/0/all/0/1&quot;&gt;Giuseppe Stragapede&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1&quot;&gt;Ruben Vera-Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1&quot;&gt;Ruben Tolosana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1&quot;&gt;Aythami Morales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1&quot;&gt;Naser Damer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1&quot;&gt;Julian Fierrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1&quot;&gt;Javier Ortega-Garcia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06009">
<title>Polar-Net: A Clinical-Friendly Model for Alzheimer&apos;s Disease Detection in OCTA Images. (arXiv:2311.06009v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.06009</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical Coherence Tomography Angiography (OCTA) is a promising tool for
detecting Alzheimer&apos;s disease (AD) by imaging the retinal microvasculature.
Ophthalmologists commonly use region-based analysis, such as the ETDRS grid, to
study OCTA image biomarkers and understand the correlation with AD. However,
existing studies have used general deep computer vision methods, which present
challenges in providing interpretable results and leveraging clinical prior
knowledge. To address these challenges, we propose a novel deep-learning
framework called Polar-Net. Our approach involves mapping OCTA images from
Cartesian coordinates to polar coordinates, which allows for the use of
approximate sector convolution and enables the implementation of the ETDRS
grid-based regional analysis method commonly used in clinical practice.
Furthermore, Polar-Net incorporates clinical prior information of each sector
region into the training process, which further enhances its performance.
Additionally, our framework adapts to acquire the importance of the
corresponding retinal region, which helps researchers and clinicians understand
the model&apos;s decision-making process in detecting AD and assess its conformity
to clinical observations. Through evaluations on private and public datasets,
we have demonstrated that Polar-Net outperforms existing state-of-the-art
methods and provides more valuable pathological evidence for the association
between retinal vascular changes and AD. In addition, we also show that the two
innovative modules introduced in our framework have a significant impact on
improving overall performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shouyue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jinkui Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yanwu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Huazhu Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xinyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yalin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yonghuai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yitian Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06018">
<title>U3DS$^3$: Unsupervised 3D Semantic Scene Segmentation. (arXiv:2311.06018v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06018</link>
<description rdf:parseType="Literal">&lt;p&gt;Contemporary point cloud segmentation approaches largely rely on richly
annotated 3D training data. However, it is both time-consuming and challenging
to obtain consistently accurate annotations for such 3D scene data. Moreover,
there is still a lack of investigation into fully unsupervised scene
segmentation for point clouds, especially for holistic 3D scenes. This paper
presents U3DS$^3$, as a step towards completely unsupervised point cloud
segmentation for any holistic 3D scenes. To achieve this, U3DS$^3$ leverages a
generalized unsupervised segmentation method for both object and background
across both indoor and outdoor static 3D point clouds with no requirement for
model pre-training, by leveraging only the inherent information of the point
cloud to achieve full 3D scene segmentation. The initial step of our proposed
approach involves generating superpoints based on the geometric characteristics
of each scene. Subsequently, it undergoes a learning process through a spatial
clustering-based methodology, followed by iterative training using
pseudo-labels generated in accordance with the cluster centroids. Moreover, by
leveraging the invariance and equivariance of the volumetric representations,
we apply the geometric transformation on voxelized features to provide two sets
of descriptors for robust representation learning. Finally, our evaluation
provides state-of-the-art results on the ScanNet and SemanticKITTI, and
competitive results on the S3DIS, benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaxu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhengdi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breckon_T/0/1/0/all/0/1&quot;&gt;Toby P. Breckon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1&quot;&gt;Hubert P.H. Shum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06031">
<title>Diagonal Hierarchical Consistency Learning for Semi-supervised Medical Image Segmentation. (arXiv:2311.06031v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06031</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image segmentation, which is essential for many clinical
applications, has achieved almost human-level performance via data-driven deep
learning techniques. Nevertheless, its performance is predicated on the costly
process of manually annotating a large amount of medical images. To this end,
we propose a novel framework for robust semi-supervised medical image
segmentation using diagonal hierarchical consistency (DiHC-Net). First, it is
composed of multiple sub-models with identical multi-scale architecture but
with distinct sub-layers, such as up-sampling and normalisation layers. Second,
a novel diagonal hierarchical consistency is enforced between one model&apos;s
intermediate and final prediction and other models&apos; soft pseudo labels in a
diagonal hierarchical fashion. Experimental results verify the efficacy of our
simple framework, outperforming all previous approaches on public Left Atrium
(LA) dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koo_H/0/1/0/all/0/1&quot;&gt;Heejoon Koo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06038">
<title>2D Image head pose estimation via latent space regression under occlusion settings. (arXiv:2311.06038v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06038</link>
<description rdf:parseType="Literal">&lt;p&gt;Head orientation is a challenging Computer Vision problem that has been
extensively researched having a wide variety of applications. However, current
state-of-the-art systems still underperform in the presence of occlusions and
are unreliable for many task applications in such scenarios. This work proposes
a novel deep learning approach for the problem of head pose estimation under
occlusions. The strategy is based on latent space regression as a fundamental
key to better structure the problem for occluded scenarios. Our model surpasses
several state-of-the-art methodologies for occluded HPE, and achieves similar
accuracy for non-occluded scenarios. We demonstrate the usefulness of the
proposed approach with: (i) two synthetically occluded versions of the BIWI and
AFLW2000 datasets, (ii) real-life occlusions of the Pandora dataset, and (iii)
a real-life application to human-robot interaction scenarios where face
occlusions often occur. Specifically, the autonomous feeding from a robotic
arm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celestino_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Celestino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marques_M/0/1/0/all/0/1&quot;&gt;Manuel Marques&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nascimento_J/0/1/0/all/0/1&quot;&gt;Jacinto C. Nascimento&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costeira_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Paulo Costeira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06043">
<title>Deep learning for 3D Object Detection and Tracking in Autonomous Driving: A Brief Survey. (arXiv:2311.06043v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06043</link>
<description rdf:parseType="Literal">&lt;p&gt;Object detection and tracking are vital and fundamental tasks for autonomous
driving, aiming at identifying and locating objects from those predefined
categories in a scene. 3D point cloud learning has been attracting more and
more attention among all other forms of self-driving data. Currently, there are
many deep learning methods for 3D object detection. However, the tasks of
object detection and tracking for point clouds still need intensive study due
to the unique characteristics of point cloud data. To help get a good grasp of
the present situation of this research, this paper shows recent advances in
deep learning methods for 3D object detection and tracking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yang Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06054">
<title>Refining the ONCE Benchmark with Hyperparameter Tuning. (arXiv:2311.06054v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06054</link>
<description rdf:parseType="Literal">&lt;p&gt;In response to the growing demand for 3D object detection in applications
such as autonomous driving, robotics, and augmented reality, this work focuses
on the evaluation of semi-supervised learning approaches for point cloud data.
The point cloud representation provides reliable and consistent observations
regardless of lighting conditions, thanks to advances in LiDAR sensors. Data
annotation is of paramount importance in the context of LiDAR applications, and
automating 3D data annotation with semi-supervised methods is a pivotal
challenge that promises to reduce the associated workload and facilitate the
emergence of cost-effective LiDAR solutions. Nevertheless, the task of
semi-supervised learning in the context of unordered point cloud data remains
formidable due to the inherent sparsity and incomplete shapes that hinder the
generation of accurate pseudo-labels. In this study, we consider these
challenges by posing the question: &quot;To what extent does unlabelled data
contribute to the enhancement of model performance?&quot; We show that improvements
from previous semi-supervised methods may not be as profound as previously
thought. Our results suggest that simple grid search hyperparameter tuning
applied to a supervised model can lead to state-of-the-art performance on the
ONCE dataset, while the contribution of unlabelled data appears to be
comparatively less exceptional.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golyadkin_M/0/1/0/all/0/1&quot;&gt;Maksim Golyadkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gambashidze_A/0/1/0/all/0/1&quot;&gt;Alexander Gambashidze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nurgaliev_I/0/1/0/all/0/1&quot;&gt;Ildar Nurgaliev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makarov_I/0/1/0/all/0/1&quot;&gt;Ilya Makarov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06056">
<title>Learning Contrastive Self-Distillation for Ultra-Fine-Grained Visual Categorization Targeting Limited Samples. (arXiv:2311.06056v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06056</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of intelligent multimedia analysis, ultra-fine-grained visual
categorization (Ultra-FGVC) plays a vital role in distinguishing intricate
subcategories within broader categories. However, this task is inherently
challenging due to the complex granularity of category subdivisions and the
limited availability of data for each category. To address these challenges,
this work proposes CSDNet, a pioneering framework that effectively explores
contrastive learning and self-distillation to learn discriminative
representations specifically designed for Ultra-FGVC tasks. CSDNet comprises
three main modules: Subcategory-Specific Discrepancy Parsing (SSDP), Dynamic
Discrepancy Learning (DDL), and Subcategory-Specific Discrepancy Transfer
(SSDT), which collectively enhance the generalization of deep models across
instance, feature, and logit prediction levels. To increase the diversity of
training samples, the SSDP module introduces augmented samples from different
viewpoints to spotlight subcategory-specific discrepancies. Simultaneously, the
proposed DDL module stores historical intermediate features by a dynamic memory
queue, which optimizes the feature learning space through iterative contrastive
learning. Furthermore, the SSDT module is developed by a novel
self-distillation paradigm at the logit prediction level of raw and augmented
samples, which effectively distills more subcategory-specific discrepancies
knowledge from the inherent structure of limited training data without
requiring additional annotations. Experimental results demonstrate that CSDNet
outperforms current state-of-the-art Ultra-FGVC methods, emphasizing its
powerful efficacy and adaptability in addressing Ultra-FGVC tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Ziye Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zechao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06057">
<title>Ulcerative Colitis Mayo Endoscopic Scoring Classification with Active Learning and Generative Data Augmentation. (arXiv:2311.06057v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.06057</link>
<description rdf:parseType="Literal">&lt;p&gt;Endoscopic imaging is commonly used to diagnose Ulcerative Colitis (UC) and
classify its severity. It has been shown that deep learning based methods are
effective in automated analysis of these images and can potentially be used to
aid medical doctors. Unleashing the full potential of these methods depends on
the availability of large amount of labeled images; however, obtaining and
labeling these images are quite challenging. In this paper, we propose a active
learning based generative augmentation method. The method involves generating a
large number of synthetic samples by training using a small dataset consisting
of real endoscopic images. The resulting data pool is narrowed down by using
active learning methods to select the most informative samples, which are then
used to train a classifier. We demonstrate the effectiveness of our method
through experiments on a publicly available endoscopic image dataset. The
results show that using synthesized samples in conjunction with active learning
leads to improved classification performance compared to using only the
original labeled examples and the baseline classification performance of 68.1%
increases to 74.5% in terms of Quadratic Weighted Kappa (QWK) Score. Another
observation is that, attaining equivalent performance using only real data
necessitated three times higher number of images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Caglar_U/0/1/0/all/0/1&quot;&gt;&amp;#xdc;mit Mert &amp;#xc7;a&amp;#x11f;lar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Inci_A/0/1/0/all/0/1&quot;&gt;Alperen &amp;#x130;nci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hanoglu_O/0/1/0/all/0/1&quot;&gt;O&amp;#x11f;uz Hano&amp;#x11f;lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Polat_G/0/1/0/all/0/1&quot;&gt;G&amp;#xf6;rkem Polat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Temizel_A/0/1/0/all/0/1&quot;&gt;Alptekin Temizel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06059">
<title>Improved Positional Encoding for Implicit Neural Representation based Compact Data Representation. (arXiv:2311.06059v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06059</link>
<description rdf:parseType="Literal">&lt;p&gt;Positional encodings are employed to capture the high frequency information
of the encoded signals in implicit neural representation (INR). In this paper,
we propose a novel positional encoding method which improves the reconstruction
quality of the INR. The proposed embedding method is more advantageous for the
compact data representation because it has a greater number of frequency basis
than the existing methods. Our experiments shows that the proposed method
achieves significant gain in the rate-distortion performance without
introducing any additional complexity in the compression task and higher
reconstruction quality in novel view synthesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damodaran_B/0/1/0/all/0/1&quot;&gt;Bharath Bhushan Damodaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schnitzler_F/0/1/0/all/0/1&quot;&gt;Francois Schnitzler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lambert_A/0/1/0/all/0/1&quot;&gt;Anne Lambert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hellier_P/0/1/0/all/0/1&quot;&gt;Pierre Hellier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06066">
<title>Lidar-based Norwegian tree species detection using deep learning. (arXiv:2311.06066v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06066</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: The mapping of tree species within Norwegian forests is a
time-consuming process, involving forest associations relying on manual
labeling by experts. The process can involve both aerial imagery, personal
familiarity, or on-scene references, and remote sensing data. The
state-of-the-art methods usually use high resolution aerial imagery with
semantic segmentation methods. Methods: We present a deep learning based tree
species classification model utilizing only lidar (Light Detection And Ranging)
data. The lidar images are segmented into four classes (Norway Spruce, Scots
Pine, Birch, background) with a U-Net based network. The model is trained with
focal loss over partial weak labels. A major benefit of the approach is that
both the lidar imagery and the base map for the labels have free and open
access. Results: Our tree species classification model achieves a
macro-averaged F1 score of 0.70 on an independent validation with National
Forest Inventory (NFI) in-situ sample plots. That is close to, but below the
performance of aerial, or aerial and lidar combined models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vermeer_M/0/1/0/all/0/1&quot;&gt;Martijn Vermeer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hay_J/0/1/0/all/0/1&quot;&gt;Jacob Alexander Hay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volgyes_D/0/1/0/all/0/1&quot;&gt;David V&amp;#xf6;lgyes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koma_Z/0/1/0/all/0/1&quot;&gt;Zs&amp;#xf3;fia Koma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breidenbach_J/0/1/0/all/0/1&quot;&gt;Johannes Breidenbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fantin_D/0/1/0/all/0/1&quot;&gt;Daniele Stefano Maria Fantin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06067">
<title>Attributes Grouping and Mining Hashing for Fine-Grained Image Retrieval. (arXiv:2311.06067v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2311.06067</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, hashing methods have been popular in the large-scale media
search for low storage and strong representation capabilities. To describe
objects with similar overall appearance but subtle differences, more and more
studies focus on hashing-based fine-grained image retrieval. Existing hashing
networks usually generate both local and global features through attention
guidance on the same deep activation tensor, which limits the diversity of
feature representations. To handle this limitation, we substitute convolutional
descriptors for attention-guided features and propose an Attributes Grouping
and Mining Hashing (AGMH), which groups and embeds the category-specific visual
attributes in multiple descriptors to generate a comprehensive feature
representation for efficient fine-grained image retrieval. Specifically, an
Attention Dispersion Loss (ADL) is designed to force the descriptors to attend
to various local regions and capture diverse subtle details. Moreover, we
propose a Stepwise Interactive External Attention (SIEA) to mine critical
attributes in each descriptor and construct correlations between fine-grained
attributes and objects. The attention mechanism is dedicated to learning
discrete attributes, which will not cost additional computations in hash codes
generation. Finally, the compact binary codes are learned by preserving
pairwise similarities. Experimental results demonstrate that AGMH consistently
yields the best performance against state-of-the-art methods on fine-grained
benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shikun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yichao Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiaobo Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06070">
<title>Learning-Based Biharmonic Augmentation for Point Cloud Classification. (arXiv:2311.06070v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06070</link>
<description rdf:parseType="Literal">&lt;p&gt;Point cloud datasets often suffer from inadequate sample sizes in comparison
to image datasets, making data augmentation challenging. While traditional
methods, like rigid transformations and scaling, have limited potential in
increasing dataset diversity due to their constraints on altering individual
sample shapes, we introduce the Biharmonic Augmentation (BA) method. BA is a
novel and efficient data augmentation technique that diversifies point cloud
data by imposing smooth non-rigid deformations on existing 3D structures. This
approach calculates biharmonic coordinates for the deformation function and
learns diverse deformation prototypes. Utilizing a CoefNet, our method predicts
coefficients to amalgamate these prototypes, ensuring comprehensive
deformation. Moreover, we present AdvTune, an advanced online augmentation
system that integrates adversarial training. This system synergistically
refines the CoefNet and the classification network, facilitating the automated
creation of adaptive shape deformations contingent on the learner status.
Comprehensive experimental analysis validates the superiority of Biharmonic
Augmentation, showcasing notable performance improvements over prevailing point
cloud augmentation techniques across varied network designs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jiacheng Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guosheng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Henghui Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jie Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yap_K/0/1/0/all/0/1&quot;&gt;Kim-Hui Yap&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06079">
<title>Enhancing Rock Image Segmentation in Digital Rock Physics: A Fusion of Generative AI and State-of-the-Art Neural Networks. (arXiv:2311.06079v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06079</link>
<description rdf:parseType="Literal">&lt;p&gt;In digital rock physics, analysing microstructures from CT and SEM scans is
crucial for estimating properties like porosity and pore connectivity.
Traditional segmentation methods like thresholding and CNNs often fall short in
accurately detailing rock microstructures and are prone to noise. U-Net
improved segmentation accuracy but required many expert-annotated samples, a
laborious and error-prone process due to complex pore shapes. Our study
employed an advanced generative AI model, the diffusion model, to overcome
these limitations. This model generated a vast dataset of CT/SEM and binary
segmentation pairs from a small initial dataset. We assessed the efficacy of
three neural networks: U-Net, Attention-U-net, and TransUNet, for segmenting
these enhanced images. The diffusion model proved to be an effective data
augmentation technique, improving the generalization and robustness of deep
learning models. TransU-Net, incorporating Transformer structures, demonstrated
superior segmentation accuracy and IoU metrics, outperforming both U-Net and
Attention-U-net. Our research advances rock image segmentation by combining the
diffusion model with cutting-edge neural networks, reducing dependency on
extensive expert data and boosting segmentation accuracy and robustness.
TransU-Net sets a new standard in digital rock physics, paving the way for
future geoscience and engineering breakthroughs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xupeng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1&quot;&gt;Hyung Kwak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jun Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shuyu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Bicheng Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06095">
<title>Dual input stream transformer for eye-tracking line assignment. (arXiv:2311.06095v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06095</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel Dual Input Stream Transformer (DIST) for the challenging
problem of assigning fixation points from eye-tracking data collected during
passage reading to the line of text that the reader was actually focused on.
This post-processing step is crucial for analysis of the reading data due to
the presence of noise in the form of vertical drift. We evaluate DIST against
nine classical approaches on a comprehensive suite of nine diverse datasets,
and demonstrate DIST&apos;s superiority. By combining multiple instances of the DIST
model in an ensemble we achieve an average accuracy of 98.5\% across all
datasets. Our approach presents a significant step towards addressing the
bottleneck of manual line assignment in reading research. Through extensive
model analysis and ablation studies, we identify key factors that contribute to
DIST&apos;s success, including the incorporation of line overlap features and the
use of a second input stream. Through evaluation on a set of diverse datasets
we demonstrate that DIST is robust to various experimental setups, making it a
safe first choice for practitioners in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mercier_T/0/1/0/all/0/1&quot;&gt;Thomas M. Mercier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budka_M/0/1/0/all/0/1&quot;&gt;Marcin Budka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasilev_M/0/1/0/all/0/1&quot;&gt;Martin R. Vasilev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirkby_J/0/1/0/all/0/1&quot;&gt;Julie A. Kirkby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Angele_B/0/1/0/all/0/1&quot;&gt;Bernhard Angele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slattery_T/0/1/0/all/0/1&quot;&gt;Timothy J. Slattery&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06118">
<title>Exploring the Efficacy of Base Data Augmentation Methods in Deep Learning-Based Radiograph Classification of Knee Joint Osteoarthritis. (arXiv:2311.06118v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.06118</link>
<description rdf:parseType="Literal">&lt;p&gt;Diagnosing knee joint osteoarthritis (KOA), a major cause of disability
worldwide, is challenging due to subtle radiographic indicators and the varied
progression of the disease. Using deep learning for KOA diagnosis requires
broad, comprehensive datasets. However, obtaining these datasets poses
significant challenges due to patient privacy concerns and data collection
restrictions. Additive data augmentation, which enhances data variability,
emerges as a promising solution. Yet, it&apos;s unclear which augmentation
techniques are most effective for KOA. This study explored various data
augmentation methods, including adversarial augmentations, and their impact on
KOA classification model performance. While some techniques improved
performance, others commonly used underperformed. We identified potential
confounding regions within the images using adversarial augmentation. This was
evidenced by our models&apos; ability to classify KL0 and KL4 grades accurately,
with the knee joint omitted. This observation suggested a model bias, which
might leverage unrelated features for classification currently present in
radiographs. Interestingly, removing the knee joint also led to an unexpected
improvement in KL1 classification accuracy. To better visualize these
paradoxical effects, we employed Grad-CAM, highlighting the associated regions.
Our study underscores the need for careful technique selection for improved
model performance and identifying and managing potential confounding regions in
radiographic KOA deep learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prezja_F/0/1/0/all/0/1&quot;&gt;Fabi Prezja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Annala_L/0/1/0/all/0/1&quot;&gt;Leevi Annala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kiiskinen_S/0/1/0/all/0/1&quot;&gt;Sampsa Kiiskinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ojala_T/0/1/0/all/0/1&quot;&gt;Timo Ojala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06122">
<title>Fight Fire with Fire: Combating Adversarial Patch Attacks using Pattern-randomized Defensive Patches. (arXiv:2311.06122v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06122</link>
<description rdf:parseType="Literal">&lt;p&gt;Object detection has found extensive applications in various tasks, but it is
also susceptible to adversarial patch attacks. Existing defense methods often
necessitate modifications to the target model or result in unacceptable time
overhead. In this paper, we adopt a counterattack approach, following the
principle of &quot;fight fire with fire,&quot; and propose a novel and general
methodology for defending adversarial attacks. We utilize an active defense
strategy by injecting two types of defensive patches, canary and woodpecker,
into the input to proactively probe or weaken potential adversarial patches
without altering the target model. Moreover, inspired by randomization
techniques employed in software security, we employ randomized canary and
woodpecker injection patterns to defend against defense-aware attacks. The
effectiveness and practicality of the proposed method are demonstrated through
comprehensive experiments. The results illustrate that canary and woodpecker
achieve high performance, even when confronted with unknown attack methods,
while incurring limited time overhead. Furthermore, our method also exhibits
sufficient robustness against defense-aware attacks, as evidenced by adaptive
attack experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jianan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiachun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1&quot;&gt;Changqing Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jianjun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_W/0/1/0/all/0/1&quot;&gt;Wei You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Wenchang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1&quot;&gt;Bin Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06137">
<title>MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty. (arXiv:2311.06137v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06137</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised monocular depth estimation methods aim to be used in critical
applications such as autonomous vehicles for environment analysis. To
circumvent the potential imperfections of these approaches, a quantification of
the prediction confidence is crucial to guide decision-making systems that rely
on depth estimation. In this paper, we propose MonoProb, a new unsupervised
monocular depth estimation method that returns an interpretable uncertainty,
which means that the uncertainty reflects the expected error of the network in
its depth predictions. We rethink the stereo or the structure-from-motion
paradigms used to train unsupervised monocular depth models as a probabilistic
problem. Within a single forward pass inference, this model provides a depth
prediction and a measure of its confidence, without increasing the inference
time. We then improve the performance on depth and uncertainty with a novel
self-distillation loss for which a student is supervised by a pseudo ground
truth that is a probability distribution on depth output by a teacher. To
quantify the performance of our models we design new metrics that, unlike
traditional ones, measure the absolute performance of uncertainty predictions.
Our experiments highlight enhancements achieved by our method on standard depth
and uncertainty metrics as well as on our tailored metrics.
https://github.com/CEA-LIST/MonoProb
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marsal_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Marsal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chabot_F/0/1/0/all/0/1&quot;&gt;Florian Chabot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loesch_A/0/1/0/all/0/1&quot;&gt;Angelique Loesch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grolleau_W/0/1/0/all/0/1&quot;&gt;William Grolleau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahbi_H/0/1/0/all/0/1&quot;&gt;Hichem Sahbi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06141">
<title>Federated Learning Across Decentralized and Unshared Archives for Remote Sensing Image Classification. (arXiv:2311.06141v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06141</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) enables the collaboration of multiple deep learning
models to learn from decentralized data archives (i.e., clients) without
accessing data on clients. Although FL offers ample opportunities in knowledge
discovery from distributed image archives, it is seldom considered in remote
sensing (RS). In this paper, as a first time in RS, we present a comparative
study of state-of-the-art FL algorithms. To this end, we initially provide a
systematic review of the FL algorithms presented in the computer vision
community for image classification problems, and select several
state-of-the-art FL algorithms based on their effectiveness with respect to
training data heterogeneity across clients (known as non-IID data). After
presenting an extensive overview of the selected algorithms, a theoretical
comparison of the algorithms is conducted based on their: 1) local training
complexity; 2) aggregation complexity; 3) learning efficiency; 4) communication
cost; and 5) scalability in terms of number of clients. As the classification
task, we consider multi-label classification (MLC) problem since RS images
typically consist of multiple classes, and thus can simultaneously be
associated with multi-labels. After the theoretical comparison, experimental
analyses are presented to compare them under different decentralization
scenarios in terms of MLC performance. Based on our comprehensive analyses, we
finally derive a guideline for selecting suitable FL algorithms in RS. The code
of this work will be publicly available at https://git.tu-berlin.de/rsim/FL-RS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buyuktas_B/0/1/0/all/0/1&quot;&gt;Bar&amp;#x131;&amp;#x15f; B&amp;#xfc;y&amp;#xfc;kta&amp;#x15f;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sumbul_G/0/1/0/all/0/1&quot;&gt;Gencer Sumbul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1&quot;&gt;Beg&amp;#xfc;m Demir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06145">
<title>An Evaluation of Forensic Facial Recognition. (arXiv:2311.06145v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06145</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in machine learning and computer vision have led to reported
facial recognition accuracies surpassing human performance. We question if
these systems will translate to real-world forensic scenarios in which a
potentially low-resolution, low-quality, partially-occluded image is compared
against a standard facial database. We describe the construction of a
large-scale synthetic facial dataset along with a controlled facial forensic
lineup, the combination of which allows for a controlled evaluation of facial
recognition under a range of real-world conditions. Using this synthetic
dataset, and a popular dataset of real faces, we evaluate the accuracy of two
popular neural-based recognition systems. We find that previously reported face
recognition accuracies of more than 95% drop to as low as 65% in this more
challenging forensic scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norman_J/0/1/0/all/0/1&quot;&gt;Justin Norman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1&quot;&gt;Shruti Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farid_H/0/1/0/all/0/1&quot;&gt;Hany Farid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06149">
<title>Dense Visual Odometry Using Genetic Algorithm. (arXiv:2311.06149v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.06149</link>
<description rdf:parseType="Literal">&lt;p&gt;Our work aims to estimate the camera motion mounted on the head of a mobile
robot or a moving object from RGB-D images in a static scene. The problem of
motion estimation is transformed into a nonlinear least squares function.
Methods for solving such problems are iterative. Various classic methods gave
an iterative solution by linearizing this function. We can also use the
metaheuristic optimization method to solve this problem and improve results. In
this paper, a new algorithm is developed for visual odometry using a sequence
of RGB-D images. This algorithm is based on a genetic algorithm. The proposed
iterative genetic algorithm searches using particles to estimate the optimal
motion and then compares it to the traditional methods. To evaluate our method,
we use the root mean square error to compare it with the based energy method
and another metaheuristic method. We prove the efficiency of our innovative
algorithm on a large set of images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Djema_S/0/1/0/all/0/1&quot;&gt;Slimane Djema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benselama_Z/0/1/0/all/0/1&quot;&gt;Zoubir Abdeslem Benselama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hedjar_R/0/1/0/all/0/1&quot;&gt;Ramdane Hedjar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdallah_K/0/1/0/all/0/1&quot;&gt;Krabi Abdallah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06169">
<title>Deep Fast Vision: A Python Library for Accelerated Deep Transfer Learning Vision Prototyping. (arXiv:2311.06169v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06169</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning-based vision is characterized by intricate frameworks that
often necessitate a profound understanding, presenting a barrier to newcomers
and limiting broad adoption. With many researchers grappling with the
constraints of smaller datasets, there&apos;s a pronounced reliance on pre-trained
neural networks, especially for tasks such as image classification. This
reliance is further intensified in niche imaging areas where obtaining vast
datasets is challenging. Despite the widespread use of transfer learning as a
remedy to the small dataset dilemma, a conspicuous absence of tailored auto-ML
solutions persists. Addressing these challenges is &quot;Deep Fast Vision&quot;, a python
library that streamlines the deep learning process. This tool offers a
user-friendly experience, enabling results through a simple nested dictionary
definition, helping to democratize deep learning for non-experts. Designed for
simplicity and scalability, Deep Fast Vision appears as a bridge, connecting
the complexities of existing deep learning frameworks with the needs of a
diverse user base.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prezja_F/0/1/0/all/0/1&quot;&gt;Fabi Prezja&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06176">
<title>Automatic Report Generation for Histopathology images using pre-trained Vision Transformers. (arXiv:2311.06176v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06176</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning for histopathology has been successfully used for disease
classification, image segmentation and more. However, combining image and text
modalities using current state-of-the-art methods has been a challenge due to
the high resolution of histopathology images. Automatic report generation for
histopathology images is one such challenge. In this work, we show that using
an existing pre-trained Vision Transformer in a two-step process of first using
it to encode 4096x4096 sized patches of the Whole Slide Image (WSI) and then
using it as the encoder and an LSTM decoder for report generation, we can build
a fairly performant and portable report generation mechanism that takes into
account the whole of the high resolution image, instead of just the patches. We
are also able to use representations from an existing powerful pre-trained
hierarchical vision transformer and show its usefulness in not just zero shot
classification but also for report generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1&quot;&gt;Saurav Sengupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1&quot;&gt;Donald E. Brown&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06185">
<title>An Automated Pipeline for Tumour-Infiltrating Lymphocyte Scoring in Breast Cancer. (arXiv:2311.06185v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.06185</link>
<description rdf:parseType="Literal">&lt;p&gt;Tumour-infiltrating lymphocytes (TILs) are considered as a valuable
prognostic markers in both triple-negative and human epidermal growth factor
receptor 2 (HER2) breast cancer. In this study, we introduce an innovative deep
learning pipeline based on the Efficient-UNet architecture to compute a TILs
score for breast cancer whole slide images. Our pipeline first segments
tumour-stroma regions and generates a tumour bulk mask. Subsequently, it
detects TILs within the tumour-associated stroma, generating a TILs score by
closely mirroring the pathologist&apos;s workflow. Our method exhibits
state-of-the-art performance in segmenting tumour/stroma areas and TILs
detection, as demonstrated by internal cross-validation on the TiGER Challenge
training dataset and evaluation on the final leaderboards. Additionally, our
TILs score proves competitive in predicting survival outcomes within the same
challenge, underscoring the clinical relevance and potential of our automated
TILs scoring system as a breast cancer prognostic tool.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shephard_A/0/1/0/all/0/1&quot;&gt;Adam J Shephard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jahanifar_M/0/1/0/all/0/1&quot;&gt;Mostafa Jahanifar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruoyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dawood_M/0/1/0/all/0/1&quot;&gt;Muhammad Dawood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Graham_S/0/1/0/all/0/1&quot;&gt;Simon Graham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sidlauskas_K/0/1/0/all/0/1&quot;&gt;Kastytis Sidlauskas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khurram_S/0/1/0/all/0/1&quot;&gt;Syed Ali Khurram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1&quot;&gt;Nasir M Rajpoot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raza_S/0/1/0/all/0/1&quot;&gt;Shan E Ahmed Raza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06211">
<title>ASSIST: Interactive Scene Nodes for Scalable and Realistic Indoor Simulation. (arXiv:2311.06211v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06211</link>
<description rdf:parseType="Literal">&lt;p&gt;We present ASSIST, an object-wise neural radiance field as a panoptic
representation for compositional and realistic simulation. Central to our
approach is a novel scene node data structure that stores the information of
each object in a unified fashion, allowing online interaction in both intra-
and cross-scene settings. By incorporating a differentiable neural network
along with the associated bounding box and semantic features, the proposed
structure guarantees user-friendly interaction on independent objects to scale
up novel view simulation. Objects in the scene can be queried, added,
duplicated, deleted, transformed, or swapped simply through mouse/keyboard
controls or language instructions. Experiments demonstrate the efficacy of the
proposed method, where scaled realistic simulation can be achieved through
interactive editing and compositional rendering, with color images, depth
images, and panoptic segmentation masks generated in a 3D consistent manner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1&quot;&gt;Zhide Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jiakai Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1&quot;&gt;Songen Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Sirui Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Weibo Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Liyi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zike Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Guyue Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06214">
<title>Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model. (arXiv:2311.06214v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06214</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-3D with diffusion models have achieved remarkable progress in recent
years. However, existing methods either rely on score distillation-based
optimization which suffer from slow inference, low diversity and Janus
problems, or are feed-forward methods that generate low quality results due to
the scarcity of 3D training data. In this paper, we propose Instant3D, a novel
method that generates high-quality and diverse 3D assets from text prompts in a
feed-forward manner. We adopt a two-stage paradigm, which first generates a
sparse set of four structured and consistent views from text in one shot with a
fine-tuned 2D text-to-image diffusion model, and then directly regresses the
NeRF from the generated images with a novel transformer-based sparse-view
reconstructor. Through extensive experiments, we demonstrate that our method
can generate high-quality, diverse and Janus-free 3D assets within 20 seconds,
which is two order of magnitude faster than previous optimization-based methods
that can take 1 to 10 hours. Our project webpage: https://jiahao.ai/instant3d/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiahao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1&quot;&gt;Hao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zexiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luan_F/0/1/0/all/0/1&quot;&gt;Fujun Luan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yinghao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yicong Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1&quot;&gt;Kalyan Sunkavalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shakhnarovich_G/0/1/0/all/0/1&quot;&gt;Greg Shakhnarovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1&quot;&gt;Sai Bi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06217">
<title>MultiIoT: Towards Large-scale Multisensory Learning for the Internet of Things. (arXiv:2311.06217v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.06217</link>
<description rdf:parseType="Literal">&lt;p&gt;The Internet of Things (IoT), the network integrating billions of smart
physical devices embedded with sensors, software, and communication
technologies for the purpose of connecting and exchanging data with other
devices and systems, is a critical and rapidly expanding component of our
modern world. The IoT ecosystem provides a rich source of real-world modalities
such as motion, thermal, geolocation, imaging, depth, sensors, video, and audio
for prediction tasks involving the pose, gaze, activities, and gestures of
humans as well as the touch, contact, pose, 3D of physical objects. Machine
learning presents a rich opportunity to automatically process IoT data at
scale, enabling efficient inference for impact in understanding human
wellbeing, controlling physical devices, and interconnecting smart cities. To
develop machine learning technologies for IoT, this paper proposes MultiIoT,
the most expansive IoT benchmark to date, encompassing over 1.15 million
samples from 12 modalities and 8 tasks. MultiIoT introduces unique challenges
involving (1) learning from many sensory modalities, (2) fine-grained
interactions across long temporal ranges, and (3) extreme heterogeneity due to
unique structure and noise topologies in real-world sensors. We also release a
set of strong modeling baselines, spanning modality and task-specific methods
to multisensory and multitask models to encourage future research in
multisensory representation learning for IoT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1&quot;&gt;Shentong Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Russ Salakhutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1&quot;&gt;Louis-Philippe Morency&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06218">
<title>Semantic-aware Video Representation for Few-shot Action Recognition. (arXiv:2311.06218v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06218</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work on action recognition leverages 3D features and textual
information to achieve state-of-the-art performance. However, most of the
current few-shot action recognition methods still rely on 2D frame-level
representations, often require additional components to model temporal
relations, and employ complex distance functions to achieve accurate alignment
of these representations. In addition, existing methods struggle to effectively
integrate textual semantics, some resorting to concatenation or addition of
textual and visual features, and some using text merely as an additional
supervision without truly achieving feature fusion and information transfer
from different modalities. In this work, we propose a simple yet effective
Semantic-Aware Few-Shot Action Recognition (SAFSAR) model to address these
issues. We show that directly leveraging a 3D feature extractor combined with
an effective feature-fusion scheme, and a simple cosine similarity for
classification can yield better performance without the need of extra
components for temporal modeling or complex distance functions. We introduce an
innovative scheme to encode the textual semantics into the video representation
which adaptively fuses features from text and video, and encourages the visual
encoder to extract more semantically consistent features. In this scheme,
SAFSAR achieves alignment and fusion in a compact way. Experiments on five
challenging few-shot action recognition benchmarks under various settings
demonstrate that the proposed SAFSAR model significantly improves the
state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yutao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bejar_B/0/1/0/all/0/1&quot;&gt;Benjamin Bejar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1&quot;&gt;Rene Vidal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06222">
<title>Diffusion Models for Earth Observation Use-cases: from cloud removal to urban change detection. (arXiv:2311.06222v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06222</link>
<description rdf:parseType="Literal">&lt;p&gt;The advancements in the state of the art of generative Artificial
Intelligence (AI) brought by diffusion models can be highly beneficial in novel
contexts involving Earth observation data. After introducing this new family of
generative models, this work proposes and analyses three use cases which
demonstrate the potential of diffusion-based approaches for satellite image
data. Namely, we tackle cloud removal and inpainting, dataset generation for
change-detection tasks, and urban replanning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanguigni_F/0/1/0/all/0/1&quot;&gt;Fulvio Sanguigni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Czerkawski_M/0/1/0/all/0/1&quot;&gt;Mikolaj Czerkawski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papa_L/0/1/0/all/0/1&quot;&gt;Lorenzo Papa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amerini_I/0/1/0/all/0/1&quot;&gt;Irene Amerini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saux_B/0/1/0/all/0/1&quot;&gt;Bertrand Le Saux&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06224">
<title>Harnessing Synthetic Datasets: The Role of Shape Bias in Deep Neural Network Generalization. (arXiv:2311.06224v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06224</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in deep learning have been primarily driven by the use of
large models trained on increasingly vast datasets. While neural scaling laws
have emerged to predict network performance given a specific level of
computational resources, the growing demand for expansive datasets raises
concerns. To address this, a new research direction has emerged, focusing on
the creation of synthetic data as a substitute. In this study, we investigate
how neural networks exhibit shape bias during training on synthetic datasets,
serving as an indicator of the synthetic data quality. Specifically, our
findings indicate three key points: (1) Shape bias varies across network
architectures and types of supervision, casting doubt on its reliability as a
predictor for generalization and its ability to explain differences in model
recognition compared to human capabilities. (2) Relying solely on shape bias to
estimate generalization is unreliable, as it is entangled with diversity and
naturalism. (3) We propose a novel interpretation of shape bias as a tool for
estimating the diversity of samples within a dataset. Our research aims to
clarify the implications of using synthetic data and its associated shape bias
in deep learning, addressing concerns regarding generalization and dataset
quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benarous_E/0/1/0/all/0/1&quot;&gt;Elior Benarous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anagnostidis_S/0/1/0/all/0/1&quot;&gt;Sotiris Anagnostidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biggio_L/0/1/0/all/0/1&quot;&gt;Luca Biggio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1&quot;&gt;Thomas Hofmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06231">
<title>Learning Human Action Recognition Representations Without Real Humans. (arXiv:2311.06231v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06231</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-training on massive video datasets has become essential to achieve high
action recognition performance on smaller downstream datasets. However, most
large-scale video datasets contain images of people and hence are accompanied
with issues related to privacy, ethics, and data protection, often preventing
them from being publicly shared for reproducible research. Existing work has
attempted to alleviate these problems by blurring faces, downsampling videos,
or training on synthetic data. On the other hand, analysis on the
transferability of privacy-preserving pre-trained models to downstream tasks
has been limited. In this work, we study this problem by first asking the
question: can we pre-train models for human action recognition with data that
does not include real humans? To this end, we present, for the first time, a
benchmark that leverages real-world videos with humans removed and synthetic
data containing virtual humans to pre-train a model. We then evaluate the
transferability of the representation learned on this data to a diverse set of
downstream action recognition benchmarks. Furthermore, we propose a novel
pre-training strategy, called Privacy-Preserving MAE-Align, to effectively
combine synthetic data and human-removed real data. Our approach outperforms
previous baselines by up to 5% and closes the performance gap between human and
no-human action recognition representations on downstream tasks, for both
linear probing and fine-tuning. Our benchmark, code, and models are available
at https://github.com/howardzh01/PPMA .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1&quot;&gt;Howard Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Samarth Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Donghyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1&quot;&gt;SouYoung Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1&quot;&gt;Rameswar Panda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1&quot;&gt;Hilde Kuehne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1&quot;&gt;Leonid Karlinsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saligrama_V/0/1/0/all/0/1&quot;&gt;Venkatesh Saligrama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliva_A/0/1/0/all/0/1&quot;&gt;Aude Oliva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1&quot;&gt;Rogerio Feris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06242">
<title>Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks. (arXiv:2311.06242v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.06242</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Florence-2, a novel vision foundation model with a unified,
prompt-based representation for a variety of computer vision and
vision-language tasks. While existing large vision models excel in transfer
learning, they struggle to perform a diversity of tasks with simple
instructions, a capability that implies handling the complexity of various
spatial hierarchy and semantic granularity. Florence-2 was designed to take
text-prompt as task instructions and generate desirable results in text forms,
whether it be captioning, object detection, grounding or segmentation. This
multi-task learning setup demands large-scale, high-quality annotated data. To
this end, we co-developed FLD-5B that consists of 5.4 billion comprehensive
visual annotations on 126 million images, using an iterative strategy of
automated image annotation and model refinement. We adopted a
sequence-to-sequence structure to train Florence-2 to perform versatile and
comprehensive vision tasks. Extensive evaluations on numerous tasks
demonstrated Florence-2 to be a strong vision foundation model contender with
unprecedented zero-shot and fine-tuning capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1&quot;&gt;Bin Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haiping Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weijian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xiyang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Houdong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yumao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1&quot;&gt;Michael Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Ce Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Lu Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06243">
<title>Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization. (arXiv:2311.06243v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.06243</link>
<description rdf:parseType="Literal">&lt;p&gt;Large foundation models are becoming ubiquitous, but training them from
scratch is prohibitively expensive. Thus, efficiently adapting these powerful
models to downstream tasks is increasingly important. In this paper, we study a
principled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstream
task adaptation. Despite demonstrating good generalizability, OFT still uses a
fairly large number of trainable parameters due to the high dimensionality of
orthogonal matrices. To address this, we start by examining OFT from an
information transmission perspective, and then identify a few key desiderata
that enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast
Fourier transform algorithm enables efficient information transmission, we
propose an efficient orthogonal parameterization using butterfly structures. We
apply this parameterization to OFT, creating a novel parameter-efficient
finetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a
special case, BOFT introduces a generalized orthogonal finetuning framework.
Finally, we conduct an extensive empirical study of adapting large vision
transformers, large language models, and text-to-image diffusion models to
various downstream tasks in vision and language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weiyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1&quot;&gt;Zeju Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiu_Y/0/1/0/all/0/1&quot;&gt;Yuliang Xiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Longhui Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Haiwen Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1&quot;&gt;Juyeon Heo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1&quot;&gt;Songyou Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yandong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1&quot;&gt;Michael J. Black&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1&quot;&gt;Adrian Weller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.05665">
<title>Chanakya: Learning Runtime Decisions for Adaptive Real-Time Perception. (arXiv:2106.05665v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2106.05665</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time perception requires planned resource utilization. Computational
planning in real-time perception is governed by two considerations -- accuracy
and latency. There exist run-time decisions (e.g. choice of input resolution)
that induce tradeoffs affecting performance on a given hardware, arising from
intrinsic (content, e.g. scene clutter) and extrinsic (system, e.g. resource
contention) characteristics.
&lt;/p&gt;
&lt;p&gt;Earlier runtime execution frameworks employed rule-based decision algorithms
and operated with a fixed algorithm latency budget to balance these concerns,
which is sub-optimal and inflexible. We propose Chanakya, a learned approximate
execution framework that naturally derives from the streaming perception
paradigm, to automatically learn decisions induced by these tradeoffs instead.
Chanakya is trained via novel rewards balancing accuracy and latency
implicitly, without approximating either objectives. Chanakya simultaneously
considers intrinsic and extrinsic context, and predicts decisions in a flexible
manner. Chanakya, designed with low overhead in mind, outperforms
state-of-the-art static and dynamic execution policies on public datasets on
both server GPUs and edge devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1&quot;&gt;Anurag Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balloli_V/0/1/0/all/0/1&quot;&gt;Vaibhav Balloli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nambi_A/0/1/0/all/0/1&quot;&gt;Akshay Nambi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Aditya Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganu_T/0/1/0/all/0/1&quot;&gt;Tanuja Ganu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.17255">
<title>A Cognitive Architecture for Machine Consciousness and Artificial Superintelligence: Thought Is Structured by the Iterative Updating of Working Memory. (arXiv:2203.17255v4 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2203.17255</link>
<description rdf:parseType="Literal">&lt;p&gt;This article provides an analytical framework for how to simulate human-like
thought processes within a computer. It describes how attention and memory
should be structured, updated, and used to search for associative additions to
the thought process. The working memory of mammals is made possible by two
forms of persistent activity: sustained firing (preserving information on the
order of seconds) and synaptic potentiation (preserving information on the
order of minutes to hours). The article uses a series of over 40 original
figures to systematically demonstrate how the iterative updating of these
working memory stores provides dynamic, functional structure to thought and
consciousness. In an AI implementation, these two stores should be updated
continuously and in an iterative fashion, meaning that, in the next state, some
proportion of the coactive representations should always be retained. Thus, the
set of concepts coactive in working memory will evolve gradually and
incrementally over time. This makes each state a revised iteration of the
preceding state and causes successive states to overlap and blend with respect
to the set of representations they contain. It is argued that without this
overlap, AI systems cannot achieve mental continuity or machine consciousness.
Persistent activity spreads activation energy throughout the hierarchical
network to search for the next associative update. This search of long-term
memory locates the most appropriate representation to be added to the global
workspace. The result is a chain of associatively linked intermediate states
capable of advancing toward a solution or goal. Iterative updating is
conceptualized here as an information processing strategy, a computational and
neurophysiological determinant of the stream of thought, and an algorithm for
designing and programming artificial general intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Reser_J/0/1/0/all/0/1&quot;&gt;Jared Edward Reser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.03553">
<title>From Heavy Rain Removal to Detail Restoration: A Faster and Better Network. (arXiv:2205.03553v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.03553</link>
<description rdf:parseType="Literal">&lt;p&gt;The dense rain accumulation in heavy rain can significantly wash out images
and thus destroy the background details of images. Although existing deep rain
removal models lead to improved performance for heavy rain removal, we find
that most of them ignore the detail reconstruction accuracy of rain-free
images. In this paper, we propose a dual-stage progressive enhancement network
(DPENet) to achieve effective deraining with structure-accurate rain-free
images. Two main modules are included in our framework, namely a rain streaks
removal network (R$^2$Net) and a detail reconstruction network (DRNet). The
former aims to achieve accurate rain removal, and the latter is designed to
recover the details of rain-free images. We introduce two main strategies
within our networks to achieve trade-off between the effectiveness of deraining
and the detail restoration of rain-free images. Firstly, a dilated dense
residual block (DDRB) within the rain streaks removal network is presented to
aggregate high/low level features of heavy rain. Secondly, an enhanced residual
pixel-wise attention block (ERPAB) within the detail reconstruction network is
designed for context information aggregation. We also propose a comprehensive
loss function to highlight the marginal and regional accuracy of rain-free
images. Extensive experiments on benchmark public datasets show both efficiency
and effectiveness of the proposed method in achieving structure-preserving
rain-free images for heavy rain removal. The source code and pre-trained models
can be found at \url{https://github.com/wybchd/DPENet}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1&quot;&gt;Tao Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yuanbo Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaihao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Ting Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.00282">
<title>(Un)likelihood Training for Interpretable Embedding. (arXiv:2207.00282v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.00282</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-modal representation learning has become a new normal for bridging the
semantic gap between text and visual data. Learning modality agnostic
representations in a continuous latent space, however, is often treated as a
black-box data-driven training process. It is well-known that the effectiveness
of representation learning depends heavily on the quality and scale of training
data. For video representation learning, having a complete set of labels that
annotate the full spectrum of video content for training is highly difficult if
not impossible. These issues, black-box training and dataset bias, make
representation learning practically challenging to be deployed for video
understanding due to unexplainable and unpredictable results. In this paper, we
propose two novel training objectives, likelihood and unlikelihood functions,
to unroll semantics behind embeddings while addressing the label sparsity
problem in training. The likelihood training aims to interpret semantics of
embeddings beyond training labels, while the unlikelihood training leverages
prior knowledge for regularization to ensure semantically coherent
interpretation. With both training objectives, a new encoder-decoder network,
which learns interpretable cross-modal representation, is proposed for ad-hoc
video search. Extensive experiments on TRECVid and MSR-VTT datasets show the
proposed network outperforms several state-of-the-art retrieval models with a
statistically significant performance margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiaxin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1&quot;&gt;Chong-Wah Ngo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1&quot;&gt;Wing-Kwong Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1&quot;&gt;Zhijian Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.11365">
<title>EgoEnv: Human-centric environment representations from egocentric video. (arXiv:2207.11365v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.11365</link>
<description rdf:parseType="Literal">&lt;p&gt;First-person video highlights a camera-wearer&apos;s activities in the context of
their persistent environment. However, current video understanding approaches
reason over visual features from short video clips that are detached from the
underlying physical space and capture only what is immediately visible. To
facilitate human-centric environment understanding, we present an approach that
links egocentric video and the environment by learning representations that are
predictive of the camera-wearer&apos;s (potentially unseen) local surroundings. We
train such models using videos from agents in simulated 3D environments where
the environment is fully observable, and test them on human-captured real-world
videos from unseen environments. On two human-centric video tasks, we show that
models equipped with our environment-aware features consistently outperform
their counterparts with traditional clip features. Moreover, despite being
trained exclusively on simulated videos, our approach successfully handles
real-world videos from HouseTours and Ego4D, and achieves state-of-the-art
results on the Ego4D NLQ challenge. Project page:
https://vision.cs.utexas.edu/projects/ego-env/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagarajan_T/0/1/0/all/0/1&quot;&gt;Tushar Nagarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramakrishnan_S/0/1/0/all/0/1&quot;&gt;Santhosh Kumar Ramakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desai_R/0/1/0/all/0/1&quot;&gt;Ruta Desai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hillis_J/0/1/0/all/0/1&quot;&gt;James Hillis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1&quot;&gt;Kristen Grauman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01738">
<title>ASIF: Coupled Data Turns Unimodal Models to Multimodal Without Training. (arXiv:2210.01738v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01738</link>
<description rdf:parseType="Literal">&lt;p&gt;CLIP proved that aligning visual and language spaces is key to solving many
vision tasks without explicit training, but required to train image and text
encoders from scratch on a huge dataset. LiT improved this by only training the
text encoder and using a pre-trained vision network. In this paper, we show
that a common space can be created without any training at all, using
single-domain encoders (trained with or without supervision) and a much smaller
amount of image-text pairs. Furthermore, our model has unique properties. Most
notably, deploying a new version with updated training samples can be done in a
matter of seconds. Additionally, the representations in the common space are
easily interpretable as every dimension corresponds to the similarity of the
input to a unique image-text pair in the multimodal dataset. Experiments on
standard zero-shot visual benchmarks demonstrate the typical transfer ability
of image-text models. Overall, our method represents a simple yet surprisingly
strong baseline for foundation multimodal models, raising important questions
on their data efficiency and on the role of retrieval in machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norelli_A/0/1/0/all/0/1&quot;&gt;Antonio Norelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fumero_M/0/1/0/all/0/1&quot;&gt;Marco Fumero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maiorca_V/0/1/0/all/0/1&quot;&gt;Valentino Maiorca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moschella_L/0/1/0/all/0/1&quot;&gt;Luca Moschella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1&quot;&gt;Emanuele Rodol&amp;#xe0;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1&quot;&gt;Francesco Locatello&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01596">
<title>Average degree of the essential variety. (arXiv:2212.01596v2 [math.AG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01596</link>
<description rdf:parseType="Literal">&lt;p&gt;The essential variety is an algebraic subvariety of dimension $5$ in real
projective space $\mathbb R\mathrm P^{8}$ which encodes the relative pose of
two calibrated pinhole cameras. The $5$-point algorithm in computer vision
computes the real points in the intersection of the essential variety with a
linear space of codimension $5$. The degree of the essential variety is $10$,
so this intersection consists of 10 complex points in general.
&lt;/p&gt;
&lt;p&gt;We compute the expected number of real intersection points when the linear
space is random. We focus on two probability distributions for linear spaces.
The first distribution is invariant under the action of the orthogonal group
$\mathrm{O}(9)$ acting on linear spaces in $\mathbb R\mathrm P^{8}$. In this
case, the expected number of real intersection points is equal to $4$. The
second distribution is motivated from computer vision and is defined by
choosing 5 point correspondences in the image planes $\mathbb R\mathrm
P^2\times \mathbb R\mathrm P^2$ uniformly at random. A Monte Carlo computation
suggests that with high probability the expected value lies in the interval
$(3.95 - 0.05,\ 3.95 + 0.05)$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Breiding_P/0/1/0/all/0/1&quot;&gt;Paul Breiding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Fairchild_S/0/1/0/all/0/1&quot;&gt;Samantha Fairchild&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Santarsiero_P/0/1/0/all/0/1&quot;&gt;Pierpaola Santarsiero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Shehu_E/0/1/0/all/0/1&quot;&gt;Elima Shehu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06755">
<title>Dataset Distillation with Convexified Implicit Gradients. (arXiv:2302.06755v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06755</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new dataset distillation algorithm using reparameterization and
convexification of implicit gradients (RCIG), that substantially improves the
state-of-the-art. To this end, we first formulate dataset distillation as a
bi-level optimization problem. Then, we show how implicit gradients can be
effectively used to compute meta-gradient updates. We further equip the
algorithm with a convexified approximation that corresponds to learning on top
of a frozen finite-width neural tangent kernel. Finally, we improve bias in
implicit gradients by parameterizing the neural network to enable analytical
computation of final-layer parameters given the body parameters. RCIG
establishes the new state-of-the-art on a diverse series of dataset
distillation tasks. Notably, with one image per class, on resized ImageNet,
RCIG sees on average a 108\% improvement over the previous state-of-the-art
distillation algorithm. Similarly, we observed a 66\% gain over SOTA on
Tiny-ImageNet and 37\% on CIFAR-100.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loo_N/0/1/0/all/0/1&quot;&gt;Noel Loo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1&quot;&gt;Ramin Hasani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1&quot;&gt;Mathias Lechner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1&quot;&gt;Daniela Rus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17896">
<title>Exploring the Limits of Deep Image Clustering using Pretrained Models. (arXiv:2303.17896v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17896</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a general methodology that learns to classify images without
labels by leveraging pretrained feature extractors. Our approach involves
self-distillation training of clustering heads based on the fact that nearest
neighbours in the pretrained feature space are likely to share the same label.
We propose a novel objective that learns associations between image features by
introducing a variant of pointwise mutual information together with instance
weighting. We demonstrate that the proposed objective is able to attenuate the
effect of false positive pairs while efficiently exploiting the structure in
the pretrained feature space. As a result, we improve the clustering accuracy
over $k$-means on $17$ different pretrained models by $6.1$\% and $12.2$\% on
ImageNet and CIFAR100, respectively. Finally, using self-supervised vision
transformers, we achieve a clustering accuracy of $61.6$\% on ImageNet. The
code is available at https://github.com/HHU-MMBS/TEMI-official-BMVC2023.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adaloglou_N/0/1/0/all/0/1&quot;&gt;Nikolas Adaloglou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michels_F/0/1/0/all/0/1&quot;&gt;Felix Michels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalisch_H/0/1/0/all/0/1&quot;&gt;Hamza Kalisch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kollmann_M/0/1/0/all/0/1&quot;&gt;Markus Kollmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.05727">
<title>Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks. (arXiv:2304.05727v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.05727</link>
<description rdf:parseType="Literal">&lt;p&gt;Robustness has become an important consideration in deep learning. With the
help of explainable AI, mismatches between an explained model&apos;s decision
strategy and the user&apos;s domain knowledge (e.g. Clever Hans effects) have been
identified as a starting point for improving faulty models. However, it is less
clear what to do when the user and the explanation agree. In this paper, we
demonstrate that acceptance of explanations by the user is not a guarantee for
a machine learning model to be robust against Clever Hans effects, which may
remain undetected. Such hidden flaws of the model can nevertheless be
mitigated, and we demonstrate this by contributing a new method,
Explanation-Guided Exposure Minimization (EGEM), that preemptively prunes
variations in the ML model that have not been the subject of positive
explanation feedback. Experiments demonstrate that our approach leads to models
that strongly reduce their reliance on hidden Clever Hans strategies, and
consequently achieve higher accuracy on new data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linhardt_L/0/1/0/all/0/1&quot;&gt;Lorenz Linhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1&quot;&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montavon_G/0/1/0/all/0/1&quot;&gt;Gr&amp;#xe9;goire Montavon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06244">
<title>Computationally-Efficient Neural Image Compression with Shallow Decoders. (arXiv:2304.06244v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06244</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural image compression methods have seen increasingly strong performance in
recent years. However, they suffer orders of magnitude higher computational
complexity compared to traditional codecs, which hinders their real-world
deployment. This paper takes a step forward towards closing this gap in
decoding complexity by using a shallow or even linear decoding transform
resembling that of JPEG. To compensate for the resulting drop in compression
performance, we exploit the often asymmetrical computation budget between
encoding and decoding, by adopting more powerful encoder networks and iterative
encoding. We theoretically formalize the intuition behind, and our experimental
results establish a new frontier in the trade-off between rate-distortion and
decoding complexity for neural image compression. Specifically, we achieve
rate-distortion performance competitive with the established mean-scale
hyperprior architecture of Minnen et al. (2018) at less than 50K decoding
FLOPs/pixel, reducing the baseline&apos;s overall decoding complexity by 80%, or
over 90% for the synthesis transform alone. Our code can be found at
https://github.com/mandt-lab/shallow-ntc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yibo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mandt_S/0/1/0/all/0/1&quot;&gt;Stephan Mandt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02317">
<title>Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings. (arXiv:2305.02317v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02317</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in large language models elicit reasoning in a chain of
thought that allows models to decompose problems in a human-like fashion.
Though this paradigm improves multi-step reasoning ability in language models,
it is limited by being unimodal and applied mainly to question-answering tasks.
We claim that incorporating visual augmentation into reasoning is essential,
especially for complex, imaginative tasks. Consequently, we introduce VCoT, a
novel method that leverages chain of thought prompting with vision-language
grounding to recursively bridge the logical gaps within sequential data. Our
method uses visual guidance to generate synthetic multimodal infillings that
add consistent and novel information to reduce the logical gaps for downstream
tasks that can benefit from temporal reasoning, as well as provide
interpretability into models&apos; multi-step reasoning. We apply VCoT to the Visual
Storytelling and WikiHow summarization datasets and demonstrate through human
evaluation that VCoT offers novel and consistent synthetic data augmentation
beating chain of thought baselines, which can be used to enhance downstream
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rose_D/0/1/0/all/0/1&quot;&gt;Daniel Rose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Himakunthala_V/0/1/0/all/0/1&quot;&gt;Vaishnavi Himakunthala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_A/0/1/0/all/0/1&quot;&gt;Andy Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ryan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_A/0/1/0/all/0/1&quot;&gt;Alex Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yujie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1&quot;&gt;Michael Saxon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonar_C/0/1/0/all/0/1&quot;&gt;Chinmay Sonar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirza_D/0/1/0/all/0/1&quot;&gt;Diba Mirza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12998">
<title>MFT: Long-Term Tracking of Every Pixel. (arXiv:2305.12998v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12998</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose MFT -- Multi-Flow dense Tracker -- a novel method for dense,
pixel-level, long-term tracking. The approach exploits optical flows estimated
not only between consecutive frames, but also for pairs of frames at
logarithmically spaced intervals. It selects the most reliable sequence of
flows on the basis of estimates of its geometric accuracy and the probability
of occlusion, both provided by a pre-trained CNN. We show that MFT achieves
competitive performance on the TAP-Vid benchmark, outperforming baselines by a
significant margin, and tracking densely orders of magnitude faster than the
state-of-the-art point-tracking methods. The method is insensitive to
medium-length occlusions and it is robustified by estimating flow with respect
to the reference frame, which reduces drift.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neoral_M/0/1/0/all/0/1&quot;&gt;Michal Neoral&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serych_J/0/1/0/all/0/1&quot;&gt;Jon&amp;#xe1;&amp;#x161; &amp;#x160;er&amp;#xfd;ch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1&quot;&gt;Ji&amp;#x159;&amp;#xed; Matas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17328">
<title>Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers. (arXiv:2305.17328v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17328</link>
<description rdf:parseType="Literal">&lt;p&gt;Deployment of Transformer models on edge devices is becoming increasingly
challenging due to the exponentially growing inference cost that scales
quadratically with the number of tokens in the input sequence. Token pruning is
an emerging solution to address this challenge due to its ease of deployment on
various Transformer backbones. However, most token pruning methods require
computationally expensive fine-tuning, which is undesirable in many edge
deployment cases. In this work, we propose Zero-TPrune, the first zero-shot
method that considers both the importance and similarity of tokens in
performing token pruning. It leverages the attention graph of pre-trained
Transformer models to produce an importance distribution for tokens via our
proposed Weighted Page Rank (WPR) algorithm. This distribution further guides
token partitioning for efficient similarity-based pruning. Due to the
elimination of the fine-tuning overhead, Zero-TPrune can prune large models at
negligible computational cost, switch between different pruning configurations
at no computational cost, and perform hyperparameter tuning efficiently. We
evaluate the performance of Zero-TPrune on vision tasks by applying it to
various vision Transformer backbones and testing them on ImageNet. Without any
fine-tuning, Zero-TPrune reduces the FLOPs cost of DeiT-S by 34.7\% and
improves its throughput by 45.3\% with only 0.4\% accuracy loss. Compared with
state-of-the-art pruning methods that require fine-tuning, Zero-TPrune not only
eliminates the need for fine-tuning after pruning but also does so with only
0.1\% accuracy loss. Compared with state-of-the-art fine-tuning-free pruning
methods, Zero-TPrune reduces accuracy loss by up to 49\% with the same or
higher throughput.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dedhia_B/0/1/0/all/0/1&quot;&gt;Bhishma Dedhia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_N/0/1/0/all/0/1&quot;&gt;Niraj K. Jha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18292">
<title>Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models. (arXiv:2305.18292v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18292</link>
<description rdf:parseType="Literal">&lt;p&gt;Public large-scale text-to-image diffusion models, such as Stable Diffusion,
have gained significant attention from the community. These models can be
easily customized for new concepts using low-rank adaptations (LoRAs). However,
the utilization of multiple concept LoRAs to jointly support multiple
customized concepts presents a challenge. We refer to this scenario as
decentralized multi-concept customization, which involves single-client concept
tuning and center-node concept fusion. In this paper, we propose a new
framework called Mix-of-Show that addresses the challenges of decentralized
multi-concept customization, including concept conflicts resulting from
existing single-client LoRA tuning and identity loss during model fusion.
Mix-of-Show adopts an embedding-decomposed LoRA (ED-LoRA) for single-client
tuning and gradient fusion for the center node to preserve the in-domain
essence of single concepts and support theoretically limitless concept fusion.
Additionally, we introduce regionally controllable sampling, which extends
spatially controllable sampling (e.g., ControlNet and T2I-Adaptor) to address
attribute binding and missing object problems in multi-concept sampling.
Extensive experiments demonstrate that Mix-of-Show is capable of composing
multiple customized concepts with high fidelity, including characters, objects,
and scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yuchao Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jay Zhangjie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yujun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yunpeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zihan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1&quot;&gt;Wuyou Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shuning Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Weijia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yixiao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19454">
<title>Dynamic Sparsity Is Channel-Level Sparsity Learner. (arXiv:2305.19454v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19454</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse training has received an upsurging interest in machine learning due to
its tantalizing saving potential for the entire training process as well as
inference. Dynamic sparse training (DST), as a leading sparse training
approach, can train deep neural networks at high sparsity from scratch to match
the performance of their dense counterparts. However, most if not all DST prior
arts demonstrate their effectiveness on unstructured sparsity with highly
irregular sparse patterns, which receives limited support in common hardware.
This limitation hinders the usage of DST in practice. In this paper, we propose
Channel-aware dynamic sparse (Chase), which for the first time seamlessly
translates the promise of unstructured dynamic sparsity to GPU-friendly
channel-level sparsity (not fine-grained N:M or group sparsity) during one
end-to-end training process, without any ad-hoc operations. The resulting small
sparse networks can be directly accelerated by commodity hardware, without
using any particularly sparsity-aware hardware accelerators. This appealing
outcome is partially motivated by a hidden phenomenon of dynamic sparsity:
off-the-shelf unstructured DST implicitly involves biased parameter
reallocation across channels, with a large fraction of channels (up to 60%)
being sparser than others. By progressively identifying and removing these
channels during training, our approach translates unstructured sparsity to
channel-wise sparsity. Our experimental results demonstrate that Chase achieves
1.7 X inference throughput speedup on common GPU devices without compromising
accuracy with ResNet-50 on ImageNet. We release our codes in
https://github.com/luuyin/chase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1&quot;&gt;Lu Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1&quot;&gt;Meng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tianjin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1&quot;&gt;Vlado Menkovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaolong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1&quot;&gt;Mykola Pechenizkiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shiwei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05179">
<title>M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models. (arXiv:2306.05179v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05179</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the existence of various benchmarks for evaluating natural language
processing models, we argue that human exams are a more suitable means of
evaluating general intelligence for large language models (LLMs), as they
inherently demand a much wider range of abilities such as language
understanding, domain knowledge, and problem-solving skills. To this end, we
introduce M3Exam, a novel benchmark sourced from real and official human exam
questions for evaluating LLMs in a multilingual, multimodal, and multilevel
context. M3Exam exhibits three unique characteristics: (1) multilingualism,
encompassing questions from multiple countries that require strong multilingual
proficiency and cultural knowledge; (2) multimodality, accounting for the
multimodal nature of many exam questions to test the model&apos;s multimodal
understanding capability; and (3) multilevel structure, featuring exams from
three critical educational periods to comprehensively assess a model&apos;s
proficiency at different levels. In total, M3Exam contains 12,317 questions in
9 diverse languages with three educational levels, where about 23\% of the
questions require processing images for successful solving. We assess the
performance of top-performing LLMs on M3Exam and find that current models,
including GPT-4, still struggle with multilingual text, particularly in
low-resource and non-Latin script languages. Multimodal LLMs also perform
poorly with complex multimodal questions. We believe that M3Exam can be a
valuable resource for comprehensively evaluating LLMs by examining their
multilingual and multimodal abilities and tracking their development. Data and
evaluation code is available at \url{https://github.com/DAMO-NLP-SG/M3Exam}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenxuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aljunied_S/0/1/0/all/0/1&quot;&gt;Sharifah Mahani Aljunied&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chia_Y/0/1/0/all/0/1&quot;&gt;Yew Ken Chia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1&quot;&gt;Lidong Bing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13302">
<title>An Overview about Emerging Technologies of Autonomous Driving. (arXiv:2306.13302v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13302</link>
<description rdf:parseType="Literal">&lt;p&gt;Since DARPA started Grand Challenges in 2004 and Urban Challenges in 2007,
autonomous driving has been the most active field of AI applications. This
paper gives an overview about technical aspects of autonomous driving
technologies and open problems. We investigate the major fields of self-driving
systems, such as perception, mapping and localization, prediction, planning and
control, simulation, V2X and safety etc. Especially we elaborate on all these
issues in a framework of data closed loop, a popular platform to solve the long
tailed autonomous driving problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yue Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zijiang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15410">
<title>AutoGraph: Predicting Lane Graphs from Traffic Observations. (arXiv:2306.15410v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15410</link>
<description rdf:parseType="Literal">&lt;p&gt;Lane graph estimation is a long-standing problem in the context of autonomous
driving. Previous works aimed at solving this problem by relying on
large-scale, hand-annotated lane graphs, introducing a data bottleneck for
training models to solve this task. To overcome this limitation, we propose to
use the motion patterns of traffic participants as lane graph annotations. In
our AutoGraph approach, we employ a pre-trained object tracker to collect the
tracklets of traffic participants such as vehicles and trucks. Based on the
location of these tracklets, we predict the successor lane graph from an
initial position using overhead RGB images only, not requiring any human
supervision. In a subsequent stage, we show how the individual successor
predictions can be aggregated into a consistent lane graph. We demonstrate the
efficacy of our approach on the UrbanLaneGraph dataset and perform extensive
quantitative and qualitative evaluations, indicating that AutoGraph is on par
with models trained on hand-annotated graph data. Model and dataset will be
made available at redacted-for-review.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zurn_J/0/1/0/all/0/1&quot;&gt;Jannik Z&amp;#xfc;rn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Posner_I/0/1/0/all/0/1&quot;&gt;Ingmar Posner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1&quot;&gt;Wolfram Burgard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02509">
<title>Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams). (arXiv:2307.02509v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02509</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a computational framework for the Wasserstein
auto-encoding of merge trees (MT-WAE), a novel extension of the classical
auto-encoder neural network architecture to the Wasserstein metric space of
merge trees. In contrast to traditional auto-encoders which operate on
vectorized data, our formulation explicitly manipulates merge trees on their
associated metric space at each layer of the network, resulting in superior
accuracy and interpretability. Our novel neural network approach can be
interpreted as a non-linear generalization of previous linear attempts [79] at
merge tree encoding. It also trivially extends to persistence diagrams.
Extensive experiments on public ensembles demonstrate the efficiency of our
algorithms, with MT-WAE computations in the orders of minutes on average. We
show the utility of our contributions in two applications adapted from previous
work on merge tree encoding [79]. First, we apply MT-WAE to merge tree
compression, by concisely representing them with their coordinates in the final
layer of our auto-encoder. Second, we document an application to dimensionality
reduction, by exploiting the latent space of our auto-encoder, for the visual
analysis of ensemble data. We illustrate the versatility of our framework by
introducing two penalty terms, to help preserve in the latent space both the
Wasserstein distances between merge trees, as well as their clusters. In both
applications, quantitative experiments assess the relevance of our framework.
Finally, we provide a C++ implementation that can be used for reproducibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pont_M/0/1/0/all/0/1&quot;&gt;Mahieu Pont&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tierny_J/0/1/0/all/0/1&quot;&gt;Julien Tierny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05016">
<title>TRansPose: Large-Scale Multispectral Dataset for Transparent Object. (arXiv:2307.05016v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05016</link>
<description rdf:parseType="Literal">&lt;p&gt;Transparent objects are encountered frequently in our daily lives, yet
recognizing them poses challenges for conventional vision sensors due to their
unique material properties, not being well perceived from RGB or depth cameras.
Overcoming this limitation, thermal infrared cameras have emerged as a
solution, offering improved visibility and shape information for transparent
objects. In this paper, we present TRansPose, the first large-scale
multispectral dataset that combines stereo RGB-D, thermal infrared (TIR)
images, and object poses to promote transparent object research. The dataset
includes 99 transparent objects, encompassing 43 household items, 27 recyclable
trashes, 29 chemical laboratory equivalents, and 12 non-transparent objects. It
comprises a vast collection of 333,819 images and 4,000,056 annotations,
providing instance-level segmentation masks, ground-truth poses, and completed
depth information. The data was acquired using a FLIR A65 thermal infrared
(TIR) camera, two Intel RealSense L515 RGB-D cameras, and a Franka Emika Panda
robot manipulator. Spanning 87 sequences, TRansPose covers various challenging
real-life scenarios, including objects filled with water, diverse lighting
conditions, heavy clutter, non-transparent or translucent containers, objects
in plastic bags, and multi-stacked objects. TRansPose dataset can be accessed
from the following link: https://sites.google.com/view/transpose-dataset
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jeongyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1&quot;&gt;Myung-Hwan Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1&quot;&gt;Sangwoo Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wooseong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_M/0/1/0/all/0/1&quot;&gt;Minwoo Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jaeho Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1&quot;&gt;Ayoung Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07827">
<title>Learning Better Keypoints for Multi-Object 6DoF Pose Estimation. (arXiv:2308.07827v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07827</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of keypoint selection, and find that the performance
of 6DoF pose estimation methods can be improved when pre-defined keypoint
locations are learned, rather than being heuristically selected as has been the
standard approach. We found that accuracy and efficiency can be improved by
training a graph network to select a set of disperse keypoints with similarly
distributed votes. These votes, learned by a regression network to accumulate
evidence for the keypoint locations, can be regressed more accurately compared
to previous heuristic keypoint algorithms. The proposed KeyGNet, supervised by
a combined loss measuring both Wasserstein distance and dispersion, learns the
color and geometry features of the target objects to estimate optimal keypoint
locations. Experiments demonstrate the keypoints selected by KeyGNet improved
the accuracy for all evaluation metrics of all seven datasets tested, for three
keypoint voting methods. The challenging Occlusion LINEMOD dataset notably
improved ADD(S) by +16.4% on PVN3D, and all core BOP datasets showed an AR
improvement for all objects, of between +1% and +21.5%. There was also a
notable increase in performance when transitioning from single object to
multiple object training using KeyGNet keypoints, essentially eliminating the
SISO-MIMO gap for Occlusion LINEMOD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yangzheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greenspan_M/0/1/0/all/0/1&quot;&gt;Michael Greenspan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14960">
<title>Read-only Prompt Optimization for Vision-Language Few-shot Learning. (arXiv:2308.14960v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14960</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, prompt tuning has proven effective in adapting pre-trained
vision-language models to downstream tasks. These methods aim to adapt the
pre-trained models by introducing learnable prompts while keeping pre-trained
weights frozen. However, learnable prompts can affect the internal
representation within the self-attention module, which may negatively impact
performance variance and generalization, especially in data-deficient settings.
To address these issues, we propose a novel approach, Read-only Prompt
Optimization (RPO). RPO leverages masked attention to prevent the internal
representation shift in the pre-trained model. Further, to facilitate the
optimization of RPO, the read-only prompts are initialized based on special
tokens of the pre-trained model. Our extensive experiments demonstrate that RPO
outperforms CLIP and CoCoOp in base-to-new generalization and domain
generalization while displaying better robustness. Also, the proposed method
achieves better generalization on extremely data-deficient settings, while
improving parameter efficiency and computational overhead. Code is available at
https://github.com/mlvlab/RPO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dongjun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Seokwon Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suh_J/0/1/0/all/0/1&quot;&gt;Jihee Suh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Joonmyung Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sanghyeok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyunwoo J.Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08036">
<title>BEA: Revisiting anchor-based object detection DNN using Budding Ensemble Architecture. (arXiv:2309.08036v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08036</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces the Budding Ensemble Architecture (BEA), a novel
reduced ensemble architecture for anchor-based object detection models. Object
detection models are crucial in vision-based tasks, particularly in autonomous
systems. They should provide precise bounding box detections while also
calibrating their predicted confidence scores, leading to higher-quality
uncertainty estimates. However, current models may make erroneous decisions due
to false positives receiving high scores or true positives being discarded due
to low scores. BEA aims to address these issues. The proposed loss functions in
BEA improve the confidence score calibration and lower the uncertainty error,
which results in a better distinction of true and false positives and,
eventually, higher accuracy of the object detection models. Both Base-YOLOv3
and SSD models were enhanced using the BEA method and its proposed loss
functions. The BEA on Base-YOLOv3 trained on the KITTI dataset results in a 6%
and 3.7% increase in mAP and AP50, respectively. Utilizing a well-balanced
uncertainty estimation threshold to discard samples in real-time even leads to
a 9.6% higher AP50 than its base model. This is attributed to a 40% increase in
the area under the AP50-based retention curve used to measure the quality of
calibration of confidence scores. Furthermore, BEA-YOLOV3 trained on KITTI
provides superior out-of-distribution detection on Citypersons, BDD100K, and
COCO datasets compared to the ensembles and vanilla models of YOLOv3 and
Gaussian-YOLOv3.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qutub_S/0/1/0/all/0/1&quot;&gt;Syed Sha Qutub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kose_N/0/1/0/all/0/1&quot;&gt;Neslihan Kose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosales_R/0/1/0/all/0/1&quot;&gt;Rafael Rosales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paulitsch_M/0/1/0/all/0/1&quot;&gt;Michael Paulitsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hagn_K/0/1/0/all/0/1&quot;&gt;Korbinian Hagn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geissler_F/0/1/0/all/0/1&quot;&gt;Florian Geissler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hinz_G/0/1/0/all/0/1&quot;&gt;Gereon Hinz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1&quot;&gt;Alois Knoll&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00723">
<title>HOH: Markerless Multimodal Human-Object-Human Handover Dataset with Large Object Count. (arXiv:2310.00723v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00723</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the HOH (Human-Object-Human) Handover Dataset, a large object
count dataset with 136 objects, to accelerate data-driven research on handover
studies, human-robot handover implementation, and artificial intelligence (AI)
on handover parameter estimation from 2D and 3D data of person interactions.
HOH contains multi-view RGB and depth data, skeletons, fused point clouds,
grasp type and handedness labels, object, giver hand, and receiver hand 2D and
3D segmentations, giver and receiver comfort ratings, and paired object
metadata and aligned 3D models for 2,720 handover interactions spanning 136
objects and 20 giver-receiver pairs-40 with role-reversal-organized from 40
participants. We also show experimental results of neural networks trained
using HOH to perform grasp, orientation, and trajectory prediction. As the only
fully markerless handover capture dataset, HOH represents natural human-human
handover interactions, overcoming challenges with markered datasets that
require specific suiting for body tracking, and lack high-resolution hand
tracking. To date, HOH is the largest handover dataset in number of objects,
participants, pairs with role reversal accounted for, and total interactions
captured.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiederhold_N/0/1/0/all/0/1&quot;&gt;Noah Wiederhold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Megyeri_A/0/1/0/all/0/1&quot;&gt;Ava Megyeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paris_D/0/1/0/all/0/1&quot;&gt;DiMaggio Paris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1&quot;&gt;Sean Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_N/0/1/0/all/0/1&quot;&gt;Natasha Kholgade Banerjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02416">
<title>Bag of Tricks for Fully Test-Time Adaptation. (arXiv:2310.02416v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02416</link>
<description rdf:parseType="Literal">&lt;p&gt;Fully Test-Time Adaptation (TTA), which aims at adapting models to data
drifts, has recently attracted wide interest. Numerous tricks and techniques
have been proposed to ensure robust learning on arbitrary streams of unlabeled
data. However, assessing the true impact of each individual technique and
obtaining a fair comparison still constitutes a significant challenge. To help
consolidate the community&apos;s knowledge, we present a categorization of selected
orthogonal TTA techniques, including small batch normalization, stream
rebalancing, reliable sample selection, and network confidence calibration. We
meticulously dissect the effect of each approach on different scenarios of
interest. Through our analysis, we shed light on trade-offs induced by those
techniques between accuracy, the computational power required, and model
complexity. We also uncover the synergy that arises when combining techniques
and are able to establish new state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mounsaveng_S/0/1/0/all/0/1&quot;&gt;Saypraseuth Mounsaveng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiaroni_F/0/1/0/all/0/1&quot;&gt;Florent Chiaroni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boudiaf_M/0/1/0/all/0/1&quot;&gt;Malik Boudiaf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1&quot;&gt;Marco Pedersoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1&quot;&gt;Ismail Ben Ayed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04010">
<title>Excision And Recovery: Visual Defect Obfuscation Based Self-Supervised Anomaly Detection Strategy. (arXiv:2310.04010v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04010</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to scarcity of anomaly situations in the early manufacturing stage, an
unsupervised anomaly detection (UAD) approach is widely adopted which only uses
normal samples for training. This approach is based on the assumption that the
trained UAD model will accurately reconstruct normal patterns but struggles
with unseen anomalous patterns. To enhance the UAD performance,
reconstruction-by-inpainting based methods have recently been investigated,
especially on the masking strategy of suspected defective regions. However,
there are still issues to overcome: 1) time-consuming inference due to multiple
masking, 2) output inconsistency by random masking strategy, and 3) inaccurate
reconstruction of normal patterns when the masked area is large. Motivated by
this, we propose a novel reconstruction-by-inpainting method, dubbed Excision
And Recovery (EAR), that features single deterministic masking based on the
ImageNet pre-trained DINO-ViT and visual obfuscation for hint-providing.
Experimental results on the MVTec AD dataset show that deterministic masking by
pre-trained attention effectively cuts out suspected defective regions and
resolve the aforementioned issues 1 and 2. Also, hint-providing by mosaicing
proves to enhance the UAD performance than emptying those regions by binary
masking, thereby overcomes issue 3. Our approach achieves a high UAD
performance without any change of the neural network structure. Thus, we
suggest that EAR be adopted in various manufacturing industries as a
practically deployable solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1&quot;&gt;YeongHyeon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1&quot;&gt;Sungho Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Myung Jin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yeonho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyeong Seok Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Juneho Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09998">
<title>SeUNet-Trans: A Simple yet Effective UNet-Transformer Model for Medical Image Segmentation. (arXiv:2310.09998v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09998</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated medical image segmentation is becoming increasingly crucial to
modern clinical practice, driven by the growing demand for precise diagnosis,
the push towards personalized treatment plans, and the advancements in machine
learning algorithms, especially the incorporation of deep learning methods.
While convolutional neural networks (CNN) have been prevalent among these
methods, the remarkable potential of Transformer-based models for computer
vision tasks is gaining more acknowledgment. To harness the advantages of both
CNN-based and Transformer-based models, we propose a simple yet effective
UNet-Transformer (seUNet-Trans) model for medical image segmentation. In our
approach, the UNet model is designed as a feature extractor to generate
multiple feature maps from the input images, then the maps are propagated into
a bridge layer, which is introduced to sequentially connect the UNet and the
Transformer. In this stage, we approach the pixel-level embedding technique
without position embedding vectors, aiming to make the model more efficient.
Moreover, we apply spatial-reduction attention in the Transformer to reduce the
computational/memory overhead. By leveraging the UNet architecture and the
self-attention mechanism, our model not only retains the preservation of both
local and global context information but also is capable of capturing
long-range dependencies between input elements. The proposed model is
extensively experimented on seven medical image segmentation datasets including
polyp segmentation to demonstrate its efficacy. Comparison with several
state-of-the-art segmentation models on these datasets shows the superior
performance of our proposed seUNet-Trans network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pham_T/0/1/0/all/0/1&quot;&gt;Tan-Hanh Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xianqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Kim-Doang Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.14194">
<title>Distractor-aware Event-based Tracking. (arXiv:2310.14194v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.14194</link>
<description rdf:parseType="Literal">&lt;p&gt;Event cameras, or dynamic vision sensors, have recently achieved success from
fundamental vision tasks to high-level vision researches. Due to its ability to
asynchronously capture light intensity changes, event camera has an inherent
advantage to capture moving objects in challenging scenarios including objects
under low light, high dynamic range, or fast moving objects. Thus event camera
are natural for visual object tracking. However, the current event-based
trackers derived from RGB trackers simply modify the input images to event
frames and still follow conventional tracking pipeline that mainly focus on
object texture for target distinction. As a result, the trackers may not be
robust dealing with challenging scenarios such as moving cameras and cluttered
foreground. In this paper, we propose a distractor-aware event-based tracker
that introduces transformer modules into Siamese network architecture (named
DANet). Specifically, our model is mainly composed of a motion-aware network
and a target-aware network, which simultaneously exploits both motion cues and
object contours from event data, so as to discover motion objects and identify
the target object by removing dynamic distractors. Our DANet can be trained in
an end-to-end manner without any post-processing and can run at over 80 FPS on
a single V100. We conduct comprehensive experiments on two large event tracking
datasets to validate the proposed model. We demonstrate that our tracker has
superior performance against the state-of-the-art trackers in terms of both
accuracy and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yingkai Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Meng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenxi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuanchen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1&quot;&gt;Baocai Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiaopeng Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15952">
<title>Improving Robustness and Reliability in Medical Image Classification with Latent-Guided Diffusion and Nested-Ensembles. (arXiv:2310.15952v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15952</link>
<description rdf:parseType="Literal">&lt;p&gt;While deep learning models have achieved remarkable success across a range of
medical image analysis tasks, deployment of these models in real clinical
contexts requires that they be robust to variability in the acquired images.
While many methods apply predefined transformations to augment the training
data to enhance test-time robustness, these transformations may not ensure the
model&apos;s robustness to the diverse variability seen in patient images. In this
paper, we introduce a novel three-stage approach based on transformers coupled
with conditional diffusion models, with the goal of improving model robustness
to the kinds of imaging variability commonly encountered in practice without
the need for pre-determined data augmentation strategies. To this end, multiple
image encoders first learn hierarchical feature representations to build
discriminative latent spaces. Next, a reverse diffusion process, guided by the
latent code, acts on an informative prior and proposes prediction candidates in
a generative manner. Finally, several prediction candidates are aggregated in a
bi-level aggregation protocol to produce the final output. Through extensive
experiments on medical imaging benchmark datasets, we show that our method
improves upon state-of-the-art methods in terms of robustness and confidence
calibration. Additionally, we introduce a strategy to quantify the prediction
uncertainty at the instance level, increasing their trustworthiness to
clinicians using them in clinical practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xing Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hengguan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nichyporuk_B/0/1/0/all/0/1&quot;&gt;Brennan Nichyporuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arbel_T/0/1/0/all/0/1&quot;&gt;Tal Arbel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19477">
<title>VDIP-TGV: Blind Image Deconvolution via Variational Deep Image Prior Empowered by Total Generalized Variation. (arXiv:2310.19477v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19477</link>
<description rdf:parseType="Literal">&lt;p&gt;Recovering clear images from blurry ones with an unknown blur kernel is a
challenging problem. Deep image prior (DIP) proposes to use the deep network as
a regularizer for a single image rather than as a supervised model, which
achieves encouraging results in the nonblind deblurring problem. However, since
the relationship between images and the network architectures is unclear, it is
hard to find a suitable architecture to provide sufficient constraints on the
estimated blur kernels and clean images. Also, DIP uses the sparse maximum a
posteriori (MAP), which is insufficient to enforce the selection of the
recovery image. Recently, variational deep image prior (VDIP) was proposed to
impose constraints on both blur kernels and recovery images and take the
standard deviation of the image into account during the optimization process by
the variational principle. However, we empirically find that VDIP struggles
with processing image details and tends to generate suboptimal results when the
blur kernel is large. Therefore, we combine total generalized variational (TGV)
regularization with VDIP in this paper to overcome these shortcomings of VDIP.
TGV is a flexible regularization that utilizes the characteristics of partial
derivatives of varying orders to regularize images at different scales,
reducing oil painting artifacts while maintaining sharp edges. The proposed
VDIP-TGV effectively recovers image edges and details by supplementing extra
gradient information through TGV. Additionally, this model is solved by the
alternating direction method of multipliers (ADMM), which effectively combines
traditional algorithms and deep learning methods. Experiments show that our
proposed VDIP-TGV surpasses various state-of-the-art models quantitatively and
qualitatively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tingting Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1&quot;&gt;Zhiyan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1&quot;&gt;Feng-Lei Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1&quot;&gt;Tieyong Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19981">
<title>&apos;Person&apos; == Light-skinned, Western Man, and Sexualization of Women of Color: Stereotypes in Stable Diffusion. (arXiv:2310.19981v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19981</link>
<description rdf:parseType="Literal">&lt;p&gt;We study stereotypes embedded within one of the most popular text-to-image
generators: Stable Diffusion. We examine what stereotypes of gender and
nationality/continental identity does Stable Diffusion display in the absence
of such information i.e. what gender and nationality/continental identity is
assigned to `a person&apos;, or to `a person from Asia&apos;. Using vision-language model
CLIP&apos;s cosine similarity to compare images generated by CLIP-based Stable
Diffusion v2.1 verified by manual examination, we chronicle results from 136
prompts (50 results/prompt) of front-facing images of persons from 6 different
continents, 27 nationalities and 3 genders. We observe how Stable Diffusion
outputs of `a person&apos; without any additional gender/nationality information
correspond closest to images of men and least with persons of nonbinary gender,
and to persons from Europe/North America over Africa/Asia, pointing towards
Stable Diffusion having a concerning representation of personhood to be a
European/North American man. We also show continental stereotypes and resultant
harms e.g. a person from Oceania is deemed to be Australian/New Zealander over
Papua New Guinean, pointing to the erasure of Indigenous Oceanic peoples, who
form a majority over descendants of colonizers both in Papua New Guinea and in
Oceania overall. Finally, we unexpectedly observe a pattern of
oversexualization of women, specifically Latin American, Mexican, Indian and
Egyptian women relative to other nationalities, measured through an NSFW
detector. This demonstrates how Stable Diffusion perpetuates Western
fetishization of women of color through objectification in media, which if left
unchecked will amplify this stereotypical representation. Image datasets are
made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Sourojit Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1&quot;&gt;Aylin Caliskan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00157">
<title>Score Normalization for a Faster Diffusion Exponential Integrator Sampler. (arXiv:2311.00157v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00157</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Zhang et al. have proposed the Diffusion Exponential Integrator
Sampler (DEIS) for fast generation of samples from Diffusion Models. It
leverages the semi-linear nature of the probability flow ordinary differential
equation (ODE) in order to greatly reduce integration error and improve
generation quality at low numbers of function evaluations (NFEs). Key to this
approach is the score function reparameterisation, which reduces the
integration error incurred from using a fixed score function estimate over each
integration step. The original authors use the default parameterisation used by
models trained for noise prediction -- multiply the score by the standard
deviation of the conditional forward noising distribution. We find that
although the mean absolute value of this score parameterisation is close to
constant for a large portion of the reverse sampling process, it changes
rapidly at the end of sampling. As a simple fix, we propose to instead
reparameterise the score (at inference) by dividing it by the average absolute
value of previous score estimates at that time step collected from offline high
NFE generations. We find that our score normalisation (DEIS-SN) consistently
improves FID compared to vanilla DEIS, showing an improvement at 10 NFEs from
6.44 to 5.57 on CIFAR-10 and from 5.9 to 4.95 on LSUN-Church 64x64. Our code is
available at https://github.com/mtkresearch/Diffusion-DEIS-SN
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1&quot;&gt;Guoxuan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danier_D/0/1/0/all/0/1&quot;&gt;Duolikun Danier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Ayan Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fotiadis_S/0/1/0/all/0/1&quot;&gt;Stathi Fotiadis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nabiei_F/0/1/0/all/0/1&quot;&gt;Farhang Nabiei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_U/0/1/0/all/0/1&quot;&gt;Ushnish Sengupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernacchia_A/0/1/0/all/0/1&quot;&gt;Alberto Bernacchia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02329">
<title>Complex Organ Mask Guided Radiology Report Generation. (arXiv:2311.02329v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02329</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of automatic report generation is to generate a clinically accurate
and coherent phrase from a single given X-ray image, which could alleviate the
workload of traditional radiology reporting. However, in a real-world scenario,
radiologists frequently face the challenge of producing extensive reports
derived from numerous medical images, thereby medical report generation from
multi-image perspective is needed. In this paper, we propose the Complex Organ
Mask Guided (termed as COMG) report generation model, which incorporates masks
from multiple organs (e.g., bones, lungs, heart, and mediastinum), to provide
more detailed information and guide the model&apos;s attention to these crucial body
regions. Specifically, we leverage prior knowledge of the disease corresponding
to each organ in the fusion process to enhance the disease identification phase
during the report generation process. Additionally, cosine similarity loss is
introduced as target function to ensure the convergence of cross-modal
consistency and facilitate model optimization.Experimental results on two
public datasets show that COMG achieves a 11.4% and 9.7% improvement in terms
of BLEU@4 scores over the SOTA model KiUT on IU-Xray and MIMIC, respectively.
The code is publicly available at https://github.com/GaryGuTC/COMG_model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_T/0/1/0/all/0/1&quot;&gt;Tiancheng Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dongnan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Weidong Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04257">
<title>mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration. (arXiv:2311.04257v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04257</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal Large Language Models (MLLMs) have demonstrated impressive
instruction abilities across various open-ended tasks. However, previous
methods primarily focus on enhancing multi-modal capabilities. In this work, we
introduce a versatile multi-modal large language model, mPLUG-Owl2, which
effectively leverages modality collaboration to improve performance in both
text and multi-modal tasks. mPLUG-Owl2 utilizes a modularized network design,
with the language decoder acting as a universal interface for managing
different modalities. Specifically, mPLUG-Owl2 incorporates shared functional
modules to facilitate modality collaboration and introduces a modality-adaptive
module that preserves modality-specific features. Extensive experiments reveal
that mPLUG-Owl2 is capable of generalizing both text tasks and multi-modal
tasks and achieving state-of-the-art performances with a single generic model.
Notably, mPLUG-Owl2 is the first MLLM model that demonstrates the modality
collaboration phenomenon in both pure-text and multi-modal scenarios, setting a
pioneering path in the development of future multi-modal foundation models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qinghao Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haiyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jiabo Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1&quot;&gt;Ming Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1&quot;&gt;Anwen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haowei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1&quot;&gt;Qi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Ji Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04498">
<title>NExT-Chat: An LMM for Chat, Detection and Segmentation. (arXiv:2311.04498v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04498</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of large language models (LLMs) has greatly advanced the
field of multimodal understanding, leading to the emergence of large multimodal
models (LMMs). In order to enhance the level of visual comprehension, recent
studies have equipped LMMs with region-level understanding capabilities by
representing object bounding box coordinates as a series of text sequences
(pixel2seq). In this paper, we introduce a novel paradigm for object location
modeling called pixel2emb method, where we ask the LMM to output the location
embeddings and then decoded by different decoders. This paradigm allows for
different location formats (such as bounding boxes and masks) to be used in
multimodal conversations Furthermore, this kind of embedding based location
modeling enables the utilization of existing practices in localization tasks,
such as detection and segmentation. In scenarios with limited resources, our
pixel2emb demonstrates superior performance compared to existing
state-of-the-art (SOTA) approaches in both the location input and output tasks
under fair comparison. Leveraging the proposed pixel2emb method, we train an
LMM named NExT-Chat and demonstrate its capability of handling multiple tasks
like visual grounding, region caption, and grounded reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Ao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Liming Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Chen-Wei Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yun Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1&quot;&gt;Wei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1&quot;&gt;Tat-Seng Chua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04698">
<title>Challenging Common Assumptions in Multi-task Learning. (arXiv:2311.04698v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04698</link>
<description rdf:parseType="Literal">&lt;p&gt;While multi-task learning (MTL) has gained significant attention in recent
years, its underlying mechanisms remain poorly understood. Recent methods did
not yield consistent performance improvements over single task learning (STL)
baselines, underscoring the importance of gaining more profound insights about
challenges specific to MTL. In our study, we challenge common assumptions in
MTL in the context of STL: First, the choice of optimizer has only been mildly
investigated in MTL. We show the pivotal role of common STL tools such as the
Adam optimizer in MTL. We deduce the effectiveness of Adam to its partial
loss-scale invariance. Second, the notion of gradient conflicts has often been
phrased as a specific problem in MTL. We delve into the role of gradient
conflicts in MTL and compare it to STL. For angular gradient alignment we find
no evidence that this is a unique problem in MTL. We emphasize differences in
gradient magnitude as the main distinguishing factor. Lastly, we compare the
transferability of features learned through MTL and STL on common image
corruptions, and find no conclusive evidence that MTL leads to superior
transferability. Overall, we find surprising similarities between STL and MTL
suggesting to consider methods from both fields in a broader context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elich_C/0/1/0/all/0/1&quot;&gt;Cathrin Elich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirchdorfer_L/0/1/0/all/0/1&quot;&gt;Lukas Kirchdorfer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1&quot;&gt;Jan M. K&amp;#xf6;hler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schott_L/0/1/0/all/0/1&quot;&gt;Lukas Schott&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05143">
<title>SCAAT: Improving Neural Network Interpretability via Saliency Constrained Adaptive Adversarial Training. (arXiv:2311.05143v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05143</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks (DNNs) are expected to provide explanation for users to
understand their black-box predictions. Saliency map is a common form of
explanation illustrating the heatmap of feature attributions, but it suffers
from noise in distinguishing important features. In this paper, we propose a
model-agnostic learning method called Saliency Constrained Adaptive Adversarial
Training (SCAAT) to improve the quality of such DNN interpretability. By
constructing adversarial samples under the guidance of saliency map, SCAAT
effectively eliminates most noise and makes saliency maps sparser and more
faithful without any modification to the model architecture. We apply SCAAT to
multiple DNNs and evaluate the quality of the generated saliency maps on
various natural and pathological image datasets. Evaluations on different
domains and metrics show that SCAAT significantly improves the interpretability
of DNNs by providing more faithful saliency maps without sacrificing their
predictive power.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Rui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_W/0/1/0/all/0/1&quot;&gt;Wenkang Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Peixiang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Lin Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05221">
<title>Let&apos;s Get the FACS Straight -- Reconstructing Obstructed Facial Features. (arXiv:2311.05221v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05221</link>
<description rdf:parseType="Literal">&lt;p&gt;The human face is one of the most crucial parts in interhuman communication.
Even when parts of the face are hidden or obstructed the underlying facial
movements can be understood. Machine learning approaches often fail in that
regard due to the complexity of the facial structures. To alleviate this
problem a common approach is to fine-tune a model for such a specific
application. However, this is computational intensive and might have to be
repeated for each desired analysis task. In this paper, we propose to
reconstruct obstructed facial parts to avoid the task of repeated fine-tuning.
As a result, existing facial analysis methods can be used without further
changes with respect to the data. In our approach, the restoration of facial
features is interpreted as a style transfer task between different recording
setups. By using the CycleGAN architecture the requirement of matched pairs,
which is often hard to fullfill, can be eliminated. To proof the viability of
our approach, we compare our reconstructions with real unobstructed recordings.
We created a novel data set in which 36 test subjects were recorded both with
and without 62 surface electromyography sensors attached to their faces. In our
evaluation, we feature typical facial analysis tasks, like the computation of
Facial Action Units and the detection of emotions. To further assess the
quality of the restoration, we also compare perceptional distances. We can
show, that scores similar to the videos without obstructing sensors can be
achieved.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buchner_T/0/1/0/all/0/1&quot;&gt;Tim B&amp;#xfc;chner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sickert_S/0/1/0/all/0/1&quot;&gt;Sven Sickert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volk_G/0/1/0/all/0/1&quot;&gt;Gerd Fabian Volk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anders_C/0/1/0/all/0/1&quot;&gt;Christoph Anders&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guntinas_Lichius_O/0/1/0/all/0/1&quot;&gt;Orlando Guntinas-Lichius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1&quot;&gt;Joachim Denzler&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>