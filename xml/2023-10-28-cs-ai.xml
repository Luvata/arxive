<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-10-26T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16853" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16990" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17011" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17022" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17025" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17041" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17042" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17049" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17064" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17120" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17140" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17146" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17158" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17167" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17178" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17183" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17207" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17228" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17238" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17261" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17330" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17353" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17372" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17389" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17415" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17416" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17427" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17429" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17490" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17513" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17537" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17551" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17555" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2104.02206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.14251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.01646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.12458" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.11699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.06589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.17218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.12593" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.02209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.04449" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03284" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11888" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13497" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15027" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11411" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.00633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02531" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13632" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14327" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15080" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16264" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16427" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.20081" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01665" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04220" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13004" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14685" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01951" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14284" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01270" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07510" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07593" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14660" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00100" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01551" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12914" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2010.01992" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2310.16842">
<title>Enhancing Energy-efficiency by Solving the Throughput Bottleneck of LSTM Cells for Embedded FPGAs. (arXiv:2310.16842v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2310.16842</link>
<description rdf:parseType="Literal">&lt;p&gt;To process sensor data in the Internet of Things(IoTs), embedded deep
learning for 1-dimensional data is an important technique. In the past, CNNs
were frequently used because they are simple to optimise for special embedded
hardware such as FPGAs. This work proposes a novel LSTM cell optimisation aimed
at energy-efficient inference on end devices. Using the traffic speed
prediction as a case study, a vanilla LSTM model with the optimised LSTM cell
achieves 17534 inferences per second while consuming only 3.8 $\mu$J per
inference on the FPGA \textit{XC7S15} from \textit{Spartan-7} family. It
achieves at least 5.4$\times$ faster throughput and 1.37$\times$ more energy
efficient than existing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chao Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_T/0/1/0/all/0/1&quot;&gt;Tianheng Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiele_G/0/1/0/all/0/1&quot;&gt;Gregor Schiele&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16853">
<title>CP-BCS: Binary Code Summarization Guided by Control Flow Graph and Pseudo Code. (arXiv:2310.16853v1 [cs.PL])</title>
<link>http://arxiv.org/abs/2310.16853</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatically generating function summaries for binaries is an extremely
valuable but challenging task, since it involves translating the execution
behavior and semantics of the low-level language (assembly code) into
human-readable natural language. However, most current works on understanding
assembly code are oriented towards generating function names, which involve
numerous abbreviations that make them still confusing. To bridge this gap, we
focus on generating complete summaries for binary functions, especially for
stripped binary (no symbol table and debug information in reality). To fully
exploit the semantics of assembly code, we present a control flow graph and
pseudo code guided binary code summarization framework called CP-BCS. CP-BCS
utilizes a bidirectional instruction-level control flow graph and pseudo code
that incorporates expert knowledge to learn the comprehensive binary function
execution behavior and logic semantics. We evaluate CP-BCS on 3 different
binary optimization levels (O1, O2, and O3) for 3 different computer
architectures (X86, X64, and ARM). The evaluation results demonstrate CP-BCS is
superior and significantly improves the efficiency of reverse engineering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1&quot;&gt;Tong Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lingfei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tengfei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yangkai Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Peiyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shouling Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16862">
<title>Balancing Augmentation with Edge-Utility Filter for Signed GNNs. (arXiv:2310.16862v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2310.16862</link>
<description rdf:parseType="Literal">&lt;p&gt;Signed graph neural networks (SGNNs) has recently drawn more attention as
many real-world networks are signed networks containing two types of edges:
positive and negative. The existence of negative edges affects the SGNN
robustness on two aspects. One is the semantic imbalance as the negative edges
are usually hard to obtain though they can provide potentially useful
information. The other is the structural unbalance, e.g. unbalanced triangles,
an indication of incompatible relationship among nodes. In this paper, we
propose a balancing augmentation method to address the above two aspects for
SGNNs. Firstly, the utility of each negative edge is measured by calculating
its occurrence in unbalanced structures. Secondly, the original signed graph is
selectively augmented with the use of (1) an edge perturbation regulator to
balance the number of positive and negative edges and to determine the ratio of
perturbed edges to original edges and (2) an edge utility filter to remove the
negative edges with low utility to make the graph structure more balanced.
Finally, a SGNN is trained on the augmented graph which effectively explores
the credible relationships. A detailed theoretical analysis is also conducted
to prove the effectiveness of each module. Experiments on five real-world
datasets in link prediction demonstrate that our method has the advantages of
effectiveness and generalization and can significantly improve the performance
of SGNN backbones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Ke-Jia Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1&quot;&gt;Yaming Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1&quot;&gt;Youran Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chuhan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16863">
<title>Graph-based multimodal multi-lesion DLBCL treatment response prediction from PET images. (arXiv:2310.16863v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2310.16863</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffuse Large B-cell Lymphoma (DLBCL) is a lymphatic cancer involving one or
more lymph nodes and extranodal sites. Its diagnostic and follow-up rely on
Positron Emission Tomography (PET) and Computed Tomography (CT). After
diagnosis, the number of nonresponding patients to standard front-line therapy
remains significant (30-40%). This work aims to develop a computer-aided
approach to identify high-risk patients requiring adapted treatment by
efficiently exploiting all the information available for each patient,
including both clinical and image data. We propose a method based on recent
graph neural networks that combine imaging information from multiple lesions,
and a cross-attention module to integrate different data modalities
efficiently. The model is trained and evaluated on a private prospective
multicentric dataset of 583 patients. Experimental results show that our
proposed method outperforms classical supervised methods based on either
clinical, imaging or both clinical and imaging data for the 2-year
progression-free survival (PFS) classification accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thiery_O/0/1/0/all/0/1&quot;&gt;Oriane Thiery&lt;/a&gt; (LS2N, LS2N - &amp;#xe9;quipe SIMS, CFE, Nantes Univ - ECN, Nantes Univ), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rizkallah_M/0/1/0/all/0/1&quot;&gt;Mira Rizkallah&lt;/a&gt; (LS2N, LS2N - &amp;#xe9;quipe SIMS, CFE, Nantes Univ - ECN, Nantes Univ), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bailly_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Bailly&lt;/a&gt; (CFE, IT, CRCI2NA, Nantes Univ), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bodet_Milin_C/0/1/0/all/0/1&quot;&gt;Caroline Bodet-Milin&lt;/a&gt; (CFE, IT, CRCI2NA, Nantes Univ), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Itti_E/0/1/0/all/0/1&quot;&gt;Emmanuel Itti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Casasnovas_R/0/1/0/all/0/1&quot;&gt;Ren&amp;#xe9;-Olivier Casasnovas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gouill_S/0/1/0/all/0/1&quot;&gt;Steven Le Gouill&lt;/a&gt; (CFE, IT, CRCI2NA, Nantes Univ), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Carlier_T/0/1/0/all/0/1&quot;&gt;Thomas Carlier&lt;/a&gt; (CFE, IT, CRCI2NA, Nantes Univ), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mateus_D/0/1/0/all/0/1&quot;&gt;Diana Mateus&lt;/a&gt; (LS2N - &amp;#xe9;quipe SIMS, LS2N, CFE, Nantes Univ - ECN, Nantes Univ)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16867">
<title>An Explainable Deep Learning-Based Method For Schizophrenia Diagnosis Using Generative Data-Augmentation. (arXiv:2310.16867v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.16867</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we leverage a deep learning-based method for the automatic
diagnosis of schizophrenia using EEG brain recordings. This approach utilizes
generative data augmentation, a powerful technique that enhances the accuracy
of the diagnosis. To enable the utilization of time-frequency features,
spectrograms were extracted from the raw signals. After exploring several
neural network architectural setups, a proper convolutional neural network
(CNN) was used for the initial diagnosis. Subsequently, using Wasserstein GAN
with Gradient Penalty (WGAN-GP) and Variational Autoencoder (VAE), two
different synthetic datasets were generated in order to augment the initial
dataset and address the over-fitting issue. The augmented dataset using VAE
achieved a 3.0\% improvement in accuracy reaching up to 99.0\% and yielded a
lower loss value as well as a faster convergence. Finally, we addressed the
lack of trust in black-box models using the Local Interpretable Model-agnostic
Explanations (LIME) algorithm to determine the most important superpixels
(frequencies) in the diagnosis process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saadatinia_M/0/1/0/all/0/1&quot;&gt;Mehrshad Saadatinia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salimi_Badr_A/0/1/0/all/0/1&quot;&gt;Armin Salimi-Badr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16919">
<title>Wide Flat Minimum Watermarking for Robust Ownership Verification of GANs. (arXiv:2310.16919v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.16919</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel multi-bit box-free watermarking method for the protection
of Intellectual Property Rights (IPR) of GANs with improved robustness against
white-box attacks like fine-tuning, pruning, quantization, and surrogate model
attacks. The watermark is embedded by adding an extra watermarking loss term
during GAN training, ensuring that the images generated by the GAN contain an
invisible watermark that can be retrieved by a pre-trained watermark decoder.
In order to improve the robustness against white-box model-level attacks, we
make sure that the model converges to a wide flat minimum of the watermarking
loss term, in such a way that any modification of the model parameters does not
erase the watermark. To do so, we add random noise vectors to the parameters of
the generator and require that the watermarking loss term is as invariant as
possible with respect to the presence of noise. This procedure forces the
generator to converge to a wide flat minimum of the watermarking loss. The
proposed method is architectureand dataset-agnostic, thus being applicable to
many different generation tasks and models, as well as to CNN-based image
processing architectures. We present the results of extensive experiments
showing that the presence of the watermark has a negligible impact on the
quality of the generated images, and proving the superior robustness of the
watermark against model modification and surrogate model attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_J/0/1/0/all/0/1&quot;&gt;Jianwei Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1&quot;&gt;Zhihua Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tondi_B/0/1/0/all/0/1&quot;&gt;Benedetta Tondi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barni_M/0/1/0/all/0/1&quot;&gt;Mauro Barni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16931">
<title>CL-MASR: A Continual Learning Benchmark for Multilingual ASR. (arXiv:2310.16931v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.16931</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern multilingual automatic speech recognition (ASR) systems like Whisper
have made it possible to transcribe audio in multiple languages with a single
model. However, current state-of-the-art ASR models are typically evaluated on
individual languages or in a multi-task setting, overlooking the challenge of
continually learning new languages. There is insufficient research on how to
add new languages without losing valuable information from previous data.
Furthermore, existing continual learning benchmarks focus mostly on vision and
language tasks, leaving continual learning for multilingual ASR largely
unexplored. To bridge this gap, we propose CL-MASR, a benchmark designed for
studying multilingual ASR in a continual learning setting. CL-MASR provides a
diverse set of continual learning methods implemented on top of large-scale
pretrained ASR models, along with common metrics to assess the effectiveness of
learning new languages while addressing the issue of catastrophic forgetting.
To the best of our knowledge, CL-MASR is the first continual learning benchmark
for the multilingual ASR task. The code is available at
https://github.com/speechbrain/benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Libera_L/0/1/0/all/0/1&quot;&gt;Luca Della Libera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousavi_P/0/1/0/all/0/1&quot;&gt;Pooneh Mousavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaiem_S/0/1/0/all/0/1&quot;&gt;Salah Zaiem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subakan_C/0/1/0/all/0/1&quot;&gt;Cem Subakan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravanelli_M/0/1/0/all/0/1&quot;&gt;Mirco Ravanelli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16978">
<title>The Significance of Machine Learning in Clinical Disease Diagnosis: A Review. (arXiv:2310.16978v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.16978</link>
<description rdf:parseType="Literal">&lt;p&gt;The global need for effective disease diagnosis remains substantial, given
the complexities of various disease mechanisms and diverse patient symptoms. To
tackle these challenges, researchers, physicians, and patients are turning to
machine learning (ML), an artificial intelligence (AI) discipline, to develop
solutions. By leveraging sophisticated ML and AI methods, healthcare
stakeholders gain enhanced diagnostic and treatment capabilities. However,
there is a scarcity of research focused on ML algorithms for enhancing the
accuracy and computational efficiency. This research investigates the capacity
of machine learning algorithms to improve the transmission of heart rate data
in time series healthcare metrics, concentrating particularly on optimizing
accuracy and efficiency. By exploring various ML algorithms used in healthcare
applications, the review presents the latest trends and approaches in ML-based
disease diagnosis (MLBDD). The factors under consideration include the
algorithm utilized, the types of diseases targeted, the data types employed,
the applications, and the evaluation metrics. This review aims to shed light on
the prospects of ML in healthcare, particularly in disease diagnosis. By
analyzing the current literature, the study provides insights into
state-of-the-art methodologies and their performance metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1&quot;&gt;S M Atikur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibtisum_S/0/1/0/all/0/1&quot;&gt;Sifat Ibtisum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bazgir_E/0/1/0/all/0/1&quot;&gt;Ehsan Bazgir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barai_T/0/1/0/all/0/1&quot;&gt;Tumpa Barai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16990">
<title>STEER: Semantic Turn Extension-Expansion Recognition for Voice Assistants. (arXiv:2310.16990v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.16990</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of a voice assistant system, steering refers to the phenomenon
in which a user issues a follow-up command attempting to direct or clarify a
previous turn. We propose STEER, a steering detection model that predicts
whether a follow-up turn is a user&apos;s attempt to steer the previous command.
Constructing a training dataset for steering use cases poses challenges due to
the cold-start problem. To overcome this, we developed heuristic rules to
sample opt-in usage data, approximating positive and negative samples without
any annotation. Our experimental results show promising performance in
identifying steering intent, with over 95% accuracy on our sampled data.
Moreover, STEER, in conjunction with our sampling strategy, aligns effectively
with real-world steering scenarios, as evidenced by its strong zero-shot
performance on a human-graded evaluation set. In addition to relying solely on
user transcripts as input, we introduce STEER+, an enhanced version of the
model. STEER+ utilizes a semantic parse tree to provide more context on
out-of-vocabulary words, such as named entities that often occur at the
sentence boundary. This further improves model performance, reducing error rate
in domains where entities frequently appear, such as messaging. Lastly, we
present a data analysis that highlights the improvement in user experience when
voice assistants support steering use cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Leon Liyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiarui Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1&quot;&gt;Joel Ruben Antony Moniz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1&quot;&gt;Aditya Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piraviperumal_D/0/1/0/all/0/1&quot;&gt;Dhivya Piraviperumal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Tien Dung Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzou_N/0/1/0/all/0/1&quot;&gt;Nicholas Tzou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17010">
<title>This Reads Like That: Deep Learning for Interpretable Natural Language Processing. (arXiv:2310.17010v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.17010</link>
<description rdf:parseType="Literal">&lt;p&gt;Prototype learning, a popular machine learning method designed for inherently
interpretable decisions, leverages similarities to learned prototypes for
classifying new data. While it is mainly applied in computer vision, in this
work, we build upon prior research and further explore the extension of
prototypical networks to natural language processing. We introduce a learned
weighted similarity measure that enhances the similarity computation by
focusing on informative dimensions of pre-trained sentence embeddings.
Additionally, we propose a post-hoc explainability mechanism that extracts
prediction-relevant words from both the prototype and input sentences. Finally,
we empirically demonstrate that our proposed method not only improves
predictive performance on the AG News and RT Polarity datasets over a previous
prototype-based approach, but also improves the faithfulness of explanations
compared to rationale-based recurrent convolutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fanconi_C/0/1/0/all/0/1&quot;&gt;Claudio Fanconi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vandenhirtz_M/0/1/0/all/0/1&quot;&gt;Moritz Vandenhirtz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Husmann_S/0/1/0/all/0/1&quot;&gt;Severin Husmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1&quot;&gt;Julia E. Vogt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17011">
<title>Personalized Speech-driven Expressive 3D Facial Animation Synthesis with Style Control. (arXiv:2310.17011v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.17011</link>
<description rdf:parseType="Literal">&lt;p&gt;Different people have different facial expressions while speaking
emotionally. A realistic facial animation system should consider such
identity-specific speaking styles and facial idiosyncrasies to achieve
high-degree of naturalness and plausibility. Existing approaches to
personalized speech-driven 3D facial animation either use one-hot identity
labels or rely-on person specific models which limit their scalability. We
present a personalized speech-driven expressive 3D facial animation synthesis
framework that models identity specific facial motion as latent representations
(called as styles), and synthesizes novel animations given a speech input with
the target style for various emotion categories. Our framework is trained in an
end-to-end fashion and has a non-autoregressive encoder-decoder architecture
with three main components: expression encoder, speech encoder and expression
decoder. Since, expressive facial motion includes both identity-specific style
and speech-related content information; expression encoder first disentangles
facial motion sequences into style and content representations, respectively.
Then, both of the speech encoder and the expression decoders input the
extracted style information to update transformer layer weights during training
phase. Our speech encoder also extracts speech phoneme label and duration
information to achieve better synchrony within the non-autoregressive synthesis
mechanism more effectively. Through detailed experiments, we demonstrate that
our approach produces temporally coherent facial expressions from input speech
while preserving the speaking styles of the target identities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bozkurt_E/0/1/0/all/0/1&quot;&gt;Elif Bozkurt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17017">
<title>An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives. (arXiv:2310.17017v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.17017</link>
<description rdf:parseType="Literal">&lt;p&gt;Mental health conversational agents (a.k.a. chatbots) are widely studied for
their potential to offer accessible support to those experiencing mental health
challenges. Previous surveys on the topic primarily consider papers published
in either computer science or medicine, leading to a divide in understanding
and hindering the sharing of beneficial knowledge between both domains. To
bridge this gap, we conduct a comprehensive literature review using the PRISMA
framework, reviewing 534 papers published in both computer science and
medicine. Our systematic review reveals 136 key papers on building mental
health-related conversational agents with diverse characteristics of modeling
and experimental design techniques. We find that computer science papers focus
on LLM techniques and evaluating response quality using automated metrics with
little attention to the application while medical papers use rule-based
conversational agents and outcome metrics to measure the health outcomes of
participants. Based on our findings on transparency, ethics, and cultural
heterogeneity in this review, we provide a few recommendations to help bridge
the disciplinary divide and enable the cross-disciplinary development of mental
health conversational agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1&quot;&gt;Young Min Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rai_S/0/1/0/all/0/1&quot;&gt;Sunny Rai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1&quot;&gt;Lyle Ungar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Sedoc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guntuku_S/0/1/0/all/0/1&quot;&gt;Sharath Chandra Guntuku&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17022">
<title>Controlled Decoding from Language Models. (arXiv:2310.17022v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17022</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose controlled decoding (CD), a novel off-policy reinforcement
learning method to control the autoregressive generation from language models
towards high reward outcomes. CD solves an off-policy reinforcement learning
problem through a value function for the reward, which we call a prefix scorer.
The prefix scorer is used at inference time to steer the generation towards
higher reward outcomes. We show that the prefix scorer may be trained on
(possibly) off-policy data to predict the expected reward when decoding is
continued from a partially decoded response. We empirically demonstrate that CD
is effective as a control mechanism on Reddit conversations corpus. We also
show that the modularity of the design of CD makes it possible to control for
multiple rewards, effectively solving a multi-objective reinforcement learning
problem with no additional complexity. Finally, we show that CD can be applied
in a novel blockwise fashion at inference-time, again without the need for any
training-time changes, essentially bridging the gap between the popular
best-of-$K$ strategy and token-level reinforcement learning. This makes CD a
promising approach for alignment of language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mudgal_S/0/1/0/all/0/1&quot;&gt;Sidharth Mudgal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganapathy_H/0/1/0/all/0/1&quot;&gt;Harish Ganapathy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;YaGuang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yanping Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Heng-Tze Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1&quot;&gt;Michael Collins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1&quot;&gt;Trevor Strohman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jilin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beutel_A/0/1/0/all/0/1&quot;&gt;Alex Beutel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1&quot;&gt;Ahmad Beirami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17025">
<title>netFound: Foundation Model for Network Security. (arXiv:2310.17025v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2310.17025</link>
<description rdf:parseType="Literal">&lt;p&gt;In ML for network security, traditional workflows rely on high-quality
labeled data and manual feature engineering, but limited datasets and human
expertise hinder feature selection, leading to models struggling to capture
crucial relationships and generalize effectively. Inspired by recent
advancements in ML application domains like GPT-4 and Vision Transformers, we
have developed netFound, a foundational model for network security. This model
undergoes pre-training using self-supervised algorithms applied to readily
available unlabeled network packet traces. netFound&apos;s design incorporates
hierarchical and multi-modal attributes of network traffic, effectively
capturing hidden networking contexts, including application logic,
communication protocols, and network conditions.
&lt;/p&gt;
&lt;p&gt;With this pre-trained foundation in place, we can fine-tune netFound for a
wide array of downstream tasks, even when dealing with low-quality, limited,
and noisy labeled data. Our experiments demonstrate netFound&apos;s superiority over
existing state-of-the-art ML-based solutions across three distinct network
downstream tasks: traffic classification, network intrusion detection, and APT
detection. Furthermore, we emphasize netFound&apos;s robustness against noisy and
missing labels, as well as its ability to generalize across temporal variations
and diverse network environments. Finally, through a series of ablation
studies, we provide comprehensive insights into how our design choices enable
netFound to more effectively capture hidden networking contexts, further
solidifying its performance and utility in network security applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guthula_S/0/1/0/all/0/1&quot;&gt;Satyandra Guthula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Battula_N/0/1/0/all/0/1&quot;&gt;Navya Battula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beltiukov_R/0/1/0/all/0/1&quot;&gt;Roman Beltiukov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1&quot;&gt;Wenbo Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Arpit Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17041">
<title>On Surgical Fine-tuning for Language Encoders. (arXiv:2310.17041v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.17041</link>
<description rdf:parseType="Literal">&lt;p&gt;Fine-tuning all the layers of a pre-trained neural language encoder (either
using all the parameters or using parameter-efficient methods) is often the
de-facto way of adapting it to a new task. We show evidence that for different
downstream language tasks, fine-tuning only a subset of layers is sufficient to
obtain performance that is close to and often better than fine-tuning all the
layers in the language encoder. We propose an efficient metric based on the
diagonal of the Fisher information matrix (FIM score), to select the candidate
layers for selective fine-tuning. We show, empirically on GLUE and SuperGLUE
tasks and across distinct language encoders, that this metric can effectively
select layers leading to a strong downstream performance. Our work highlights
that task-specific information corresponding to a given downstream task is
often localized within a few layers, and tuning only those is sufficient for
strong performance. Additionally, we demonstrate the robustness of the FIM
score to rank layers in a manner that remains constant during the optimization
process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lodha_A/0/1/0/all/0/1&quot;&gt;Abhilasha Lodha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belapurkar_G/0/1/0/all/0/1&quot;&gt;Gayatri Belapurkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chalkapurkar_S/0/1/0/all/0/1&quot;&gt;Saloni Chalkapurkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1&quot;&gt;Yuanming Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1&quot;&gt;Reshmi Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1&quot;&gt;Samyadeep Basu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrov_D/0/1/0/all/0/1&quot;&gt;Dmitrii Petrov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1&quot;&gt;Soundararajan Srinivasan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17042">
<title>StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling. (arXiv:2310.17042v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17042</link>
<description rdf:parseType="Literal">&lt;p&gt;In the rapidly advancing domain of deep learning optimization, this paper
unveils the StochGradAdam optimizer, a novel adaptation of the well-regarded
Adam algorithm. Central to StochGradAdam is its gradient sampling technique.
This method not only ensures stable convergence but also leverages the
advantages of selective gradient consideration, fostering robust training by
potentially mitigating the effects of noisy or outlier data and enhancing the
exploration of the loss landscape for more dependable convergence. In both
image classification and segmentation tasks, StochGradAdam has demonstrated
superior performance compared to the traditional Adam optimizer. By judiciously
sampling a subset of gradients at each iteration, the optimizer is optimized
for managing intricate models. The paper provides a comprehensive exploration
of StochGradAdam&apos;s methodology, from its mathematical foundations to bias
correction strategies, heralding a promising advancement in deep learning
training techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1&quot;&gt;Juyoung Yun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17049">
<title>Learning Repeatable Speech Embeddings Using An Intra-class Correlation Regularizer. (arXiv:2310.17049v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2310.17049</link>
<description rdf:parseType="Literal">&lt;p&gt;A good supervised embedding for a specific machine learning task is only
sensitive to changes in the label of interest and is invariant to other
confounding factors. We leverage the concept of repeatability from measurement
theory to describe this property and propose to use the intra-class correlation
coefficient (ICC) to evaluate the repeatability of embeddings. We then propose
a novel regularizer, the ICC regularizer, as a complementary component for
contrastive losses to guide deep neural networks to produce embeddings with
higher repeatability. We use simulated data to explain why the ICC regularizer
works better on minimizing the intra-class variance than the contrastive loss
alone. We implement the ICC regularizer and apply it to three speech tasks:
speaker verification, voice style conversion, and a clinical application for
detecting dysphonic voice. The experimental results demonstrate that adding an
ICC regularizer can improve the repeatability of learned embeddings compared to
only using the contrastive loss; further, these embeddings lead to improved
performance in these downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayasuriya_S/0/1/0/all/0/1&quot;&gt;Suren Jayasuriya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berisha_V/0/1/0/all/0/1&quot;&gt;Visar Berisha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17064">
<title>math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories. (arXiv:2310.17064v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.17064</link>
<description rdf:parseType="Literal">&lt;p&gt;As artificial intelligence (AI) gains greater adoption in a wide variety of
applications, it has immense potential to contribute to mathematical discovery,
by guiding conjecture generation, constructing counterexamples, assisting in
formalizing mathematics, and discovering connections between different
mathematical areas, to name a few.
&lt;/p&gt;
&lt;p&gt;While prior work has leveraged computers for exhaustive mathematical proof
search, recent efforts based on large language models (LLMs) aspire to position
computing platforms as co-contributors in the mathematical research process.
Despite their current limitations in logic and mathematical tasks, there is
growing interest in melding theorem proving systems with foundation models.
This work investigates the applicability of LLMs in formalizing advanced
mathematical concepts and proposes a framework that can critically review and
check mathematical reasoning in research papers. Given the noted reasoning
shortcomings of LLMs, our approach synergizes the capabilities of proof
assistants, specifically PVS, with LLMs, enabling a bridge between textual
descriptions in academic papers and formal specifications in PVS. By harnessing
the PVS environment, coupled with data ingestion and conversion mechanisms, we
envision an automated process, called \emph{math-PVS}, to extract and formalize
mathematical theorems from research papers, offering an innovative tool for
academic review and discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saidi_H/0/1/0/all/0/1&quot;&gt;Hassen Saidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1&quot;&gt;Susmit Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahai_T/0/1/0/all/0/1&quot;&gt;Tuhin Sahai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17072">
<title>Isometric Motion Manifold Primitives. (arXiv:2310.17072v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.17072</link>
<description rdf:parseType="Literal">&lt;p&gt;The Motion Manifold Primitive (MMP) produces, for a given task, a continuous
manifold of trajectories each of which can successfully complete the task. It
consists of the decoder function that parametrizes the manifold and the
probability density in the latent coordinate space. In this paper, we first
show that the MMP performance can significantly degrade due to the geometric
distortion in the latent space -- by distortion, we mean that similar motions
are not located nearby in the latent space. We then propose {\it Isometric
Motion Manifold Primitives (IMMP)} whose latent coordinate space preserves the
geometry of the manifold. For this purpose, we formulate and use a Riemannian
metric for the motion space (i.e., parametric curve space), which we call a
{\it CurveGeom Riemannian metric}. Experiments with planar obstacle-avoiding
motions and pushing manipulation tasks show that IMMP significantly outperforms
existing MMP methods. Code is available at
https://github.com/Gabe-YHLee/IMMP-public.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yonghyeon Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17086">
<title>Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models. (arXiv:2310.17086v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17086</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers are remarkably good at in-context learning (ICL) -- learning
from demonstrations without parameter updates -- but how they perform ICL
remains a mystery. Recent work suggests that Transformers may learn in-context
by internally running Gradient Descent, a first-order optimization method. In
this paper, we instead demonstrate that Transformers learn to implement
higher-order optimization methods to perform ICL. Focusing on in-context linear
regression, we show that Transformers learn to implement an algorithm very
similar to Iterative Newton&apos;s Method, a higher-order optimization method,
rather than Gradient Descent. Empirically, we show that predictions from
successive Transformer layers closely match different iterations of Newton&apos;s
Method linearly, with each middle layer roughly computing 3 iterations. In
contrast, exponentially more Gradient Descent steps are needed to match an
additional Transformers layer; this suggests that Transformers have an
comparable rate of convergence with high-order methods such as Iterative
Newton, which are exponentially faster than Gradient Descent. We also show that
Transformers can learn in-context on ill-conditioned data, a setting where
Gradient Descent struggles but Iterative Newton succeeds. Finally, we show
theoretical results which support our empirical findings and have a close
correspondence with them: we prove that Transformers can implement $k$
iterations of Newton&apos;s method with $\mathcal{O}(k)$ layers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1&quot;&gt;Deqing Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tian-Qi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1&quot;&gt;Robin Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharan_V/0/1/0/all/0/1&quot;&gt;Vatsal Sharan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17091">
<title>Detecting stealthy cyberattacks on adaptive cruise control vehicles: A machine learning approach. (arXiv:2310.17091v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2310.17091</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advent of vehicles equipped with advanced driver-assistance systems,
such as adaptive cruise control (ACC) and other automated driving features, the
potential for cyberattacks on these automated vehicles (AVs) has emerged. While
overt attacks that force vehicles to collide may be easily identified, more
insidious attacks, which only slightly alter driving behavior, can result in
network-wide increases in congestion, fuel consumption, and even crash risk
without being easily detected. To address the detection of such attacks, we
first present a traffic model framework for three types of potential
cyberattacks: malicious manipulation of vehicle control commands, false data
injection attacks on sensor measurements, and denial-of-service (DoS) attacks.
We then investigate the impacts of these attacks at both the individual vehicle
(micro) and traffic flow (macro) levels. A novel generative adversarial network
(GAN)-based anomaly detection model is proposed for real-time identification of
such attacks using vehicle trajectory data. We provide numerical evidence {to
demonstrate} the efficacy of our machine learning approach in detecting
cyberattacks on ACC-equipped vehicles. The proposed method is compared against
some recently proposed neural network models and observed to have higher
accuracy in identifying anomalous driving behaviors of ACC vehicles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_M/0/1/0/all/0/1&quot;&gt;Mingfeng Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stern_R/0/1/0/all/0/1&quot;&gt;Raphael Stern&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17120">
<title>Topic Segmentation of Semi-Structured and Unstructured Conversational Datasets using Language Models. (arXiv:2310.17120v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.17120</link>
<description rdf:parseType="Literal">&lt;p&gt;Breaking down a document or a conversation into multiple contiguous segments
based on its semantic structure is an important and challenging problem in NLP,
which can assist many downstream tasks. However, current works on topic
segmentation often focus on segmentation of structured texts. In this paper, we
comprehensively analyze the generalization capabilities of state-of-the-art
topic segmentation models on unstructured texts. We find that: (a) Current
strategies of pre-training on a large corpus of structured text such as
Wiki-727K do not help in transferability to unstructured conversational data.
(b) Training from scratch with only a relatively small-sized dataset of the
target unstructured domain improves the segmentation results by a significant
margin. We stress-test our proposed Topic Segmentation approach by
experimenting with multiple loss functions, in order to mitigate effects of
imbalance in unstructured conversational datasets. Our empirical evaluation
indicates that Focal Loss function is a robust alternative to Cross-Entropy and
re-weighted Cross-Entropy loss function when segmenting unstructured and
semi-structured chats.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1&quot;&gt;Reshmi Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kajal_H/0/1/0/all/0/1&quot;&gt;Harjeet Singh Kajal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamath_S/0/1/0/all/0/1&quot;&gt;Sharanya Kamath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrivastava_D/0/1/0/all/0/1&quot;&gt;Dhuri Shrivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1&quot;&gt;Samyadeep Basu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1&quot;&gt;Hansi Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1&quot;&gt;Soundararajan Srinivasan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17132">
<title>Unleashing the potential of GNNs via Bi-directional Knowledge Transfer. (arXiv:2310.17132v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17132</link>
<description rdf:parseType="Literal">&lt;p&gt;Based on the message-passing paradigm, there has been an amount of research
proposing diverse and impressive feature propagation mechanisms to improve the
performance of GNNs. However, less focus has been put on feature
transformation, another major operation of the message-passing framework. In
this paper, we first empirically investigate the performance of the feature
transformation operation in several typical GNNs. Unexpectedly, we notice that
GNNs do not completely free up the power of the inherent feature transformation
operation. By this observation, we propose the Bi-directional Knowledge
Transfer (BiKT), a plug-and-play approach to unleash the potential of the
feature transformation operations without modifying the original architecture.
Taking the feature transformation operation as a derived representation
learning model that shares parameters with the original GNN, the direct
prediction by this model provides a topological-agnostic knowledge feedback
that can further instruct the learning of GNN and the feature transformations
therein. On this basis, BiKT not only allows us to acquire knowledge from both
the GNN and its derived model but promotes each other by injecting the
knowledge into the other. In addition, a theoretical analysis is further
provided to demonstrate that BiKT improves the generalization bound of the GNNs
from the perspective of domain adaption. An extensive group of experiments on
up to 7 datasets with 5 typical GNNs demonstrates that BiKT brings up to 0.5% -
4% performance gain over the original GNN, which means a boosted GNN is
obtained. Meanwhile, the derived model also shows a powerful performance to
compete with or even surpass the original GNN, enabling us to flexibly apply it
independently to some other specific downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shuai Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhizhe Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhenfeng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xingxing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17133">
<title>Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs. (arXiv:2310.17133v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.17133</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an in-depth study of multimodal machine translation
(MMT), examining the prevailing understanding that MMT systems exhibit
decreased sensitivity to visual information when text inputs are complete.
Instead, we attribute this phenomenon to insufficient cross-modal interaction,
rather than image information redundancy. A novel approach is proposed to
generate parallel Visual Question-Answering (VQA) style pairs from the source
text, fostering more robust cross-modal interaction. Using Large Language
Models (LLMs), we explicitly model the probing signal in MMT to convert it into
VQA-style data to create the Multi30K-VQA dataset. An MMT-VQA multitask
learning framework is introduced to incorporate explicit probing signals from
the dataset into the MMT training process. Experimental results on two
widely-used benchmarks demonstrate the effectiveness of this novel approach.
Our code and data would be available at:
\url{https://github.com/libeineu/MMT-VQA}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_Y/0/1/0/all/0/1&quot;&gt;Yuxin Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1&quot;&gt;Chuanhao Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1&quot;&gt;Tong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Tong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jingbo Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17136">
<title>Core Challenge 2023: Solver and Graph Descriptions. (arXiv:2310.17136v1 [cs.PL])</title>
<link>http://arxiv.org/abs/2310.17136</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper collects all descriptions of solvers and ISR instances submitted
to CoRe Challenge 2023.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soh_T/0/1/0/all/0/1&quot;&gt;Takehide Soh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okamoto_Y/0/1/0/all/0/1&quot;&gt;Yoshio Okamoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ito_T/0/1/0/all/0/1&quot;&gt;Takehiro Ito&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17140">
<title>Symbolic Planning and Code Generation for Grounded Dialogue. (arXiv:2310.17140v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.17140</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) excel at processing and generating both text and
code. However, LLMs have had limited applicability in grounded task-oriented
dialogue as they are difficult to steer toward task objectives and fail to
handle novel grounding. We present a modular and interpretable grounded
dialogue system that addresses these shortcomings by composing LLMs with a
symbolic planner and grounded code execution. Our system consists of a reader
and planner: the reader leverages an LLM to convert partner utterances into
executable code, calling functions that perform grounding. The translated
code&apos;s output is stored to track dialogue state, while a symbolic planner
determines the next appropriate response. We evaluate our system&apos;s performance
on the demanding OneCommon dialogue task, involving collaborative reference
resolution on abstract images of scattered dots. Our system substantially
outperforms the previous state-of-the-art, including improving task success in
human evaluations from 56% to 69% in the most challenging setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiu_J/0/1/0/all/0/1&quot;&gt;Justin T. Chiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wenting Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Derek Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaduguru_S/0/1/0/all/0/1&quot;&gt;Saujas Vaduguru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1&quot;&gt;Alexander M. Rush&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1&quot;&gt;Daniel Fried&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17146">
<title>Counterfactual-Augmented Importance Sampling for Semi-Offline Policy Evaluation. (arXiv:2310.17146v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17146</link>
<description rdf:parseType="Literal">&lt;p&gt;In applying reinforcement learning (RL) to high-stakes domains, quantitative
and qualitative evaluation using observational data can help practitioners
understand the generalization performance of new policies. However, this type
of off-policy evaluation (OPE) is inherently limited since offline data may not
reflect the distribution shifts resulting from the application of new policies.
On the other hand, online evaluation by collecting rollouts according to the
new policy is often infeasible, as deploying new policies in these domains can
be unsafe. In this work, we propose a semi-offline evaluation framework as an
intermediate step between offline and online evaluation, where human users
provide annotations of unobserved counterfactual trajectories. While tempting
to simply augment existing data with such annotations, we show that this naive
approach can lead to biased results. Instead, we design a new family of OPE
estimators based on importance sampling (IS) and a novel weighting scheme that
incorporate counterfactual annotations without introducing additional bias. We
analyze the theoretical properties of our approach, showing its potential to
reduce both bias and variance compared to standard IS estimators. Our analyses
reveal important practical considerations for handling biased, noisy, or
missing annotations. In a series of proof-of-concept experiments involving
bandits and a healthcare-inspired simulator, we demonstrate that our approach
outperforms purely offline IS estimators and is robust to imperfect
annotations. Our framework, combined with principled human-centered design of
annotation solicitation, can enable the application of RL in high-stakes
domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shengpu Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1&quot;&gt;Jenna Wiens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17149">
<title>Explainable Spatio-Temporal Graph Neural Networks. (arXiv:2310.17149v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17149</link>
<description rdf:parseType="Literal">&lt;p&gt;Spatio-temporal graph neural networks (STGNNs) have gained popularity as a
powerful tool for effectively modeling spatio-temporal dependencies in diverse
real-world urban applications, including intelligent transportation and public
safety. However, the black-box nature of STGNNs limits their interpretability,
hindering their application in scenarios related to urban resource allocation
and policy formulation. To bridge this gap, we propose an Explainable
Spatio-Temporal Graph Neural Networks (STExplainer) framework that enhances
STGNNs with inherent explainability, enabling them to provide accurate
predictions and faithful explanations simultaneously. Our framework integrates
a unified spatio-temporal graph attention network with a positional information
fusion layer as the STG encoder and decoder, respectively. Furthermore, we
propose a structure distillation approach based on the Graph Information
Bottleneck (GIB) principle with an explainable objective, which is instantiated
by the STG encoder and decoder. Through extensive experiments, we demonstrate
that our STExplainer outperforms state-of-the-art baselines in terms of
predictive accuracy and explainability metrics (i.e., sparsity and fidelity) on
traffic and crime prediction tasks. Furthermore, our model exhibits superior
representation ability in alleviating data missing and sparsity issues. The
implementation code is available at: https://github.com/HKUDS/STExplainer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiabin Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1&quot;&gt;Lianghao Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17152">
<title>Technical Note: Feasibility of translating 3.0T-trained Deep-Learning Segmentation Models Out-of-the-Box on Low-Field MRI 0.55T Knee-MRI of Healthy Controls. (arXiv:2310.17152v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17152</link>
<description rdf:parseType="Literal">&lt;p&gt;In the current study, our purpose is to evaluate the feasibility of applying
deep learning (DL) enabled algorithms to quantify bilateral knee biomarkers in
healthy controls scanned at 0.55T and compared with 3.0T. The current study
assesses the performance of standard in-practice bone, and cartilage
segmentation algorithms at 0.55T, both qualitatively and quantitatively, in
terms of comparing segmentation performance, areas of improvement, and
compartment-wise cartilage thickness values between 0.55T vs. 3.0T. Initial
results demonstrate a usable to good technical feasibility of translating
existing quantitative deep-learning-based image segmentation techniques,
trained on 3.0T, out of 0.55T for knee MRI, in a multi-vendor acquisition
environment. Especially in terms of segmenting cartilage compartments, the
models perform almost equivalent to 3.0T in terms of Likert ranking. The 0.55T
low-field sustainable and easy-to-install MRI, as demonstrated, thus, can be
utilized for evaluating knee cartilage thickness and bone segmentations aided
by established DL algorithms trained at higher-field strengths out-of-the-box
initially. This could be utilized at the far-spread point-of-care locations
with a lack of radiologists available to manually segment low-field images, at
least till a decent base of low-field data pool is collated. With further
fine-tuning with manual labeling of low-field data or utilizing synthesized
higher SNR images from low-field images, OA biomarker quantification
performance is potentially guaranteed to be further improved.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharjee_R/0/1/0/all/0/1&quot;&gt;Rupsa Bhattacharjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akkaya_Z/0/1/0/all/0/1&quot;&gt;Zehra Akkaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luitjens_J/0/1/0/all/0/1&quot;&gt;Johanna Luitjens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_P/0/1/0/all/0/1&quot;&gt;Pan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedoia_V/0/1/0/all/0/1&quot;&gt;Valentina Pedoia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1&quot;&gt;Sharmila Majumdar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17158">
<title>CosmosDSR -- a methodology for automated detection and tracking of orbital debris using the Unscented Kalman Filter. (arXiv:2310.17158v1 [astro-ph.EP])</title>
<link>http://arxiv.org/abs/2310.17158</link>
<description rdf:parseType="Literal">&lt;p&gt;The Kessler syndrome refers to the escalating space debris from frequent
space activities, threatening future space exploration. Addressing this issue
is vital. Several AI models, including Convolutional Neural Networks (CNN),
Kernel Principal Component Analysis (KPCA), and Model-Agnostic Meta-Learning
(MAML), have been assessed with various data types. Earlier studies highlighted
the combination of the YOLO object detector and a linear Kalman filter for
object detection and tracking. Building on this, our project introduces
CosmosDSR, a novel methodology combining YOLOv3 with an Unscented Kalman Filter
for tracking satellites in sequential images, compared to a linear Kalman
filter. Using the SPARK dataset from the University of Luxembourg for training
and testing, the YOLOv3 precisely detected and classified all satellite
categories (mAP=97.18%, F1=0.95) with few errors (TP=4163, FP=209, FN=237).
Both CosmosDSR and the LKF tracked satellites accurately (UKF:
MSE=2.83/RMSE=1.66, LKF: MSE=2.84/RMSE=1.66). Despite concerns of class
imbalance and the absence of real images, the model shows promise. Future work
should address these limitations, increase tracking sample size, and improve
metrics. This research suggests the algorithm&apos;s potential in detecting and
tracking satellites, paving the way for solutions to the Kessler syndrome.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Roll_D/0/1/0/all/0/1&quot;&gt;Daniel S. Roll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Kurt_Z/0/1/0/all/0/1&quot;&gt;Zeyneb Kurt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Woo_W/0/1/0/all/0/1&quot;&gt;Wai Lok Woo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17162">
<title>Content-based Controls For Music Large Language Modeling. (arXiv:2310.17162v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.17162</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed a rapid growth of large-scale language models in
the domain of music audio. Such models enable end-to-end generation of
higher-quality music, and some allow conditioned generation using text
descriptions. However, the control power of text controls on music is
intrinsically limited, as they can only describe music indirectly through
meta-data (such as singers and instruments) or high-level representations (such
as genre and emotion). We aim to further equip the models with direct and
content-based controls on innate music languages such as pitch, chords and drum
track. To this end, we contribute Coco-Mulla, a content-based control method
for music large language modeling. It uses a parameter-efficient fine-tuning
(PEFT) method tailored for Transformer-based audio models. Experiments show
that our approach achieved high-quality music generation with low-resource
semi-supervised learning, tuning with less than 4% parameters compared to the
original model and training on a small dataset with fewer than 300 songs.
Moreover, our approach enables effective content-based controls, and we
illustrate the control power via chords and rhythms, two of the most salient
features of music audio. Furthermore, we show that by combining content-based
controls and text descriptions, our system achieves flexible music variation
generation and style transfer. Our source codes and demos are available online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liwei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1&quot;&gt;Gus Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Junyan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yixiao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17167">
<title>Improving Denoising Diffusion Models via Simultaneous Estimation of Image and Noise. (arXiv:2310.17167v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17167</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces two key contributions aimed at improving the speed and
quality of images generated through inverse diffusion processes. The first
contribution involves reparameterizing the diffusion process in terms of the
angle on a quarter-circular arc between the image and noise, specifically
setting the conventional $\displaystyle \sqrt{\bar{\alpha}}=\cos(\eta)$. This
reparameterization eliminates two singularities and allows for the expression
of diffusion evolution as a well-behaved ordinary differential equation (ODE).
In turn, this allows higher order ODE solvers such as Runge-Kutta methods to be
used effectively. The second contribution is to directly estimate both the
image ($\mathbf{x}_0$) and noise ($\mathbf{\epsilon}$) using our network, which
enables more stable calculations of the update step in the inverse diffusion
steps, as accurate estimation of both the image and noise are crucial at
different stages of the process. Together with these changes, our model
achieves faster generation, with the ability to converge on high-quality images
more quickly, and higher quality of the generated images, as measured by
metrics such as Frechet Inception Distance (FID), spatial Frechet Inception
Distance (sFID), precision, and recall.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenkai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehinger_K/0/1/0/all/0/1&quot;&gt;Krista A. Ehinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drummond_T/0/1/0/all/0/1&quot;&gt;Tom Drummond&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17176">
<title>A Deep Learning Approach to Teeth Segmentation and Orientation from Panoramic X-rays. (arXiv:2310.17176v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17176</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate teeth segmentation and orientation are fundamental in modern oral
healthcare, enabling precise diagnosis, treatment planning, and dental implant
design. In this study, we present a comprehensive approach to teeth
segmentation and orientation from panoramic X-ray images, leveraging deep
learning techniques. We build our model based on FUSegNet, a popular model
originally developed for wound segmentation, and introduce modifications by
incorporating grid-based attention gates into the skip connections. We
introduce oriented bounding box (OBB) generation through principal component
analysis (PCA) for precise tooth orientation estimation. Evaluating our
approach on the publicly available DNS dataset, comprising 543 panoramic X-ray
images, we achieve the highest Intersection-over-Union (IoU) score of 82.43%
and Dice Similarity Coefficient (DSC) score of 90.37% among compared models in
teeth instance segmentation. In OBB analysis, we obtain the Rotated IoU (RIoU)
score of 82.82%. We also conduct detailed analyses of individual tooth labels
and categorical performance, shedding light on strengths and weaknesses. The
proposed model&apos;s accuracy and versatility offer promising prospects for
improving dental diagnoses, treatment planning, and personalized healthcare in
the oral domain. Our generated OBB coordinates and codes are available at
https://github.com/mrinal054/Instance_teeth_segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhar_M/0/1/0/all/0/1&quot;&gt;Mrinal Kanti Dhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deb_M/0/1/0/all/0/1&quot;&gt;Mou Deb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madhab_D/0/1/0/all/0/1&quot;&gt;D. Madhab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zeyun Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17177">
<title>Bridging The Gaps Between Token Pruning and Full Pre-training via Masked Fine-tuning. (arXiv:2310.17177v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17177</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the success of transformers on various computer vision tasks, they
suffer from excessive memory and computational cost. Some works present dynamic
vision transformers to accelerate inference by pruning redundant tokens. A key
to improving token pruning is using well-trained models as initialization for
faster convergence and better performance. However, current base models usually
adopt full image training, i.e., using full images as inputs and keeping the
whole feature maps through the forward process, which causes inconsistencies
with dynamic models that gradually reduce tokens, including calculation
pattern, information amount and token selection strategy inconsistencies.
Inspired by MAE which performs masking and reconstruction self-supervised task,
we devise masked fine-tuning to bridge the gaps between pre-trained base models
used for initialization and token pruning based dynamic vision transformers, by
masking image patches and predicting the image class label based on left
unmasked patches. Extensive experiments on ImageNet demonstrate that base
models via masked fine-tuning gain strong occlusion robustness and ability
against information loss. With this better initialization, Dynamic ViT achieves
higher accuracies, especially under large token pruning ratios (e.g., 81.9% vs.
81.3%, and 62.3% vs. 58.9% for DeiT based Dynamic ViT/0.8 and Dynamic ViT/0.3).
Moreover, we apply our method into different token pruning based dynamic vision
transformers, different pre-trained models and randomly initialized models to
demonstrate the generalization ability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1&quot;&gt;Fengyuan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Limin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17178">
<title>Graphical Object-Centric Actor-Critic. (arXiv:2310.17178v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.17178</link>
<description rdf:parseType="Literal">&lt;p&gt;There have recently been significant advances in the problem of unsupervised
object-centric representation learning and its application to downstream tasks.
The latest works support the argument that employing disentangled object
representations in image-based object-centric reinforcement learning tasks
facilitates policy learning. We propose a novel object-centric reinforcement
learning algorithm combining actor-critic and model-based approaches to utilize
these representations effectively. In our approach, we use a transformer
encoder to extract object representations and graph neural networks to
approximate the dynamics of an environment. The proposed method fills a
research gap in developing efficient object-centric world models for
reinforcement learning settings that can be used for environments with discrete
or continuous action spaces. Our algorithm performs better in a visually
complex 3D robotic environment and a 2D environment with compositional
structure than the state-of-the-art model-free actor-critic algorithm built
upon transformer architecture and the state-of-the-art monolithic model-based
algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ugadiarov_L/0/1/0/all/0/1&quot;&gt;Leonid Ugadiarov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panov_A/0/1/0/all/0/1&quot;&gt;Aleksandr I. Panov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17183">
<title>Understanding the Effects of Projectors in Knowledge Distillation. (arXiv:2310.17183v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17183</link>
<description rdf:parseType="Literal">&lt;p&gt;Conventionally, during the knowledge distillation process (e.g. feature
distillation), an additional projector is often required to perform feature
transformation due to the dimension mismatch between the teacher and the
student networks. Interestingly, we discovered that even if the student and the
teacher have the same feature dimensions, adding a projector still helps to
improve the distillation performance. In addition, projectors even improve
logit distillation if we add them to the architecture too. Inspired by these
surprising findings and the general lack of understanding of the projectors in
the knowledge distillation process from existing literature, this paper
investigates the implicit role that projectors play but so far have been
overlooked. Our empirical study shows that the student with a projector (1)
obtains a better trade-off between the training accuracy and the testing
accuracy compared to the student without a projector when it has the same
feature dimensions as the teacher, (2) better preserves its similarity to the
teacher beyond shallow and numeric resemblance, from the view of Centered
Kernel Alignment (CKA), and (3) avoids being over-confident as the teacher does
at the testing phase. Motivated by the positive effects of projectors, we
propose a projector ensemble-based feature distillation method to further
improve distillation performance. Despite the simplicity of the proposed
strategy, empirical results from the evaluation of classification tasks on
benchmark datasets demonstrate the superior classification performance of our
method on a broad range of teacher-student pairs and verify from the aspects of
CKA and model calibration that the student&apos;s features are of improved quality
with the projector ensemble design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yudong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiajun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xuwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoog_F/0/1/0/all/0/1&quot;&gt;Frank de Hoog&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kusy_B/0/1/0/all/0/1&quot;&gt;Brano Kusy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zi Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17191">
<title>How do Language Models Bind Entities in Context?. (arXiv:2310.17191v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17191</link>
<description rdf:parseType="Literal">&lt;p&gt;To correctly use in-context information, language models (LMs) must bind
entities to their attributes. For example, given a context describing a &quot;green
square&quot; and a &quot;blue circle&quot;, LMs must bind the shapes to their respective
colors. We analyze LM representations and identify the binding ID mechanism: a
general mechanism for solving the binding problem, which we observe in every
sufficiently large model from the Pythia and LLaMA families. Using causal
interventions, we show that LMs&apos; internal activations represent binding
information by attaching binding ID vectors to corresponding entities and
attributes. We further show that binding ID vectors form a continuous subspace,
in which distances between binding ID vectors reflect their discernability.
Overall, our results uncover interpretable strategies in LMs for representing
symbolic knowledge in-context, providing a step towards understanding general
in-context reasoning in large-scale LMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiahai Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1&quot;&gt;Jacob Steinhardt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17200">
<title>Taming Gradient Variance in Federated Learning with Networked Control Variates. (arXiv:2310.17200v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17200</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning, a decentralized approach to machine learning, faces
significant challenges such as extensive communication overheads, slow
convergence, and unstable improvements. These challenges primarily stem from
the gradient variance due to heterogeneous client data distributions. To
address this, we introduce a novel Networked Control Variates (FedNCV)
framework for Federated Learning. We adopt the REINFORCE Leave-One-Out (RLOO)
as a fundamental control variate unit in the FedNCV framework, implemented at
both client and server levels. At the client level, the RLOO control variate is
employed to optimize local gradient updates, mitigating the variance introduced
by data samples. Once relayed to the server, the RLOO-based estimator further
provides an unbiased and low-variance aggregated gradient, leading to robust
global updates. This dual-side application is formalized as a linear
combination of composite control variates. We provide a mathematical expression
capturing this integration of double control variates within FedNCV and present
three theoretical results with corresponding proofs. This unique dual structure
equips FedNCV to address data heterogeneity and scalability issues, thus
potentially paving the way for large-scale applications. Moreover, we tested
FedNCV on six diverse datasets under a Dirichlet distribution with {\alpha} =
0.1, and benchmarked its performance against six SOTA methods, demonstrating
its superiority.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xingyan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yaling Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1&quot;&gt;Huaming Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yu Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17207">
<title>Efficient Data Fusion using the Tsetlin Machine. (arXiv:2310.17207v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.17207</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel way of assessing and fusing noisy dynamic data using a
Tsetlin Machine. Our approach consists in monitoring how explanations in form
of logical clauses that a TM learns changes with possible noise in dynamic
data. This way TM can recognize the noise by lowering weights of previously
learned clauses, or reflect it in the form of new clauses. We also perform a
comprehensive experimental study using notably different datasets that
demonstrated high performance of the proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_R/0/1/0/all/0/1&quot;&gt;Rupsa Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zadorozhny_V/0/1/0/all/0/1&quot;&gt;Vladimir I. Zadorozhny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1&quot;&gt;Ole-Christoffer Granmo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17212">
<title>Emotion Recognition by Video: A review. (arXiv:2310.17212v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17212</link>
<description rdf:parseType="Literal">&lt;p&gt;Video emotion recognition is an important branch of affective computing, and
its solutions can be applied in different fields such as human-computer
interaction (HCI) and intelligent medical treatment. Although the number of
papers published in the field of emotion recognition is increasing, there are
few comprehensive literature reviews covering related research on video emotion
recognition. Therefore, this paper selects articles published from 2015 to 2023
to systematize the existing trends in video emotion recognition in related
studies. In this paper, we first talk about two typical emotion models, then we
talk about databases that are frequently utilized for video emotion
recognition, including unimodal databases and multimodal databases. Next, we
look at and classify the specific structure and performance of modern unimodal
and multimodal video emotion recognition methods, talk about the benefits and
drawbacks of each, and then we compare them in detail in the tables. Further,
we sum up the primary difficulties right now looked by video emotion
recognition undertakings and point out probably the most encouraging future
headings, such as establishing an open benchmark database and better multimodal
fusion strategys. The essential objective of this paper is to assist scholarly
and modern scientists with keeping up to date with the most recent advances and
new improvements in this speedy, high-influence field of video emotion
recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1&quot;&gt;Junxiao Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xuecheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1&quot;&gt;Liangyu Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17217">
<title>Beyond MLE: Convex Learning for Text Generation. (arXiv:2310.17217v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.17217</link>
<description rdf:parseType="Literal">&lt;p&gt;Maximum likelihood estimation (MLE) is a statistical method used to estimate
the parameters of a probability distribution that best explain the observed
data. In the context of text generation, MLE is often used to train generative
language models, which can then be used to generate new text. However, we argue
that MLE is not always necessary and optimal, especially for closed-ended text
generation tasks like machine translation. In these tasks, the goal of model is
to generate the most appropriate response, which does not necessarily require
it to estimate the entire data distribution with MLE. To this end, we propose a
novel class of training objectives based on convex functions, which enables
text generation models to focus on highly probable outputs without having to
estimate the entire data distribution. We investigate the theoretical
properties of the optimal predicted distribution when applying convex functions
to the loss, demonstrating that convex functions can sharpen the optimal
distribution, thereby enabling the model to better capture outputs with high
probabilities. Experiments on various text generation tasks and models show the
effectiveness of our approach. It enables autoregressive models to bridge the
gap between greedy and beam search, and facilitates the learning of
non-autoregressive models with a maximum improvement of 9+ BLEU points.
Moreover, our approach also exhibits significant impact on large language
models (LLMs), substantially enhancing their generative capability on various
tasks. Source code is available at
\url{https://github.com/ictnlp/Convex-Learning}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1&quot;&gt;Chenze Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhengrui Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yang Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17228">
<title>TST$^\mathrm{R}$: Target Similarity Tuning Meets the Real World. (arXiv:2310.17228v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.17228</link>
<description rdf:parseType="Literal">&lt;p&gt;Target similarity tuning (TST) is a method of selecting relevant examples in
natural language (NL) to code generation through large language models (LLMs)
to improve performance. Its goal is to adapt a sentence embedding model to have
the similarity between two NL inputs match the similarity between their
associated code outputs. In this paper, we propose different methods to apply
and improve TST in the real world. First, we replace the sentence transformer
with embeddings from a larger model, which reduces sensitivity to the language
distribution and thus provides more flexibility in synthetic generation of
examples, and we train a tiny model that transforms these embeddings to a space
where embedding similarity matches code similarity, which allows the model to
remain a black box and only requires a few matrix multiplications at inference
time. Second, we how to efficiently select a smaller number of training
examples to train the TST model. Third, we introduce a ranking-based evaluation
for TST that does not require end-to-end code generation experiments, which can
be expensive to perform.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khatry_A/0/1/0/all/0/1&quot;&gt;Anirudh Khatry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulwani_S/0/1/0/all/0/1&quot;&gt;Sumit Gulwani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1&quot;&gt;Priyanshu Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1&quot;&gt;Vu Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singha_A/0/1/0/all/0/1&quot;&gt;Ananya Singha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Mukul Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbruggen_G/0/1/0/all/0/1&quot;&gt;Gust Verbruggen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17238">
<title>Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks. (arXiv:2310.17238v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.17238</link>
<description rdf:parseType="Literal">&lt;p&gt;Entity and Relation Extraction (ERE) is an important task in information
extraction. Recent marker-based pipeline models achieve state-of-the-art
performance, but still suffer from the error propagation issue. Also, most of
current ERE models do not take into account higher-order interactions between
multiple entities and relations, while higher-order modeling could be
beneficial.In this work, we propose HyperGraph neural network for ERE
($\hgnn{}$), which is built upon the PL-marker (a state-of-the-art marker-based
pipleline model). To alleviate error propagation,we use a high-recall pruner
mechanism to transfer the burden of entity identification and labeling from the
NER module to the joint module of our model. For higher-order modeling, we
build a hypergraph, where nodes are entities (provided by the span pruner) and
relations thereof, and hyperedges encode interactions between two different
relations or between a relation and its associated subject and object entities.
We then run a hypergraph neural network for higher-order inference by applying
message passing over the built hypergraph. Experiments on three widely used
benchmarks (\acef{}, \ace{} and \scierc{}) for ERE task show significant
improvements over the previous state-of-the-art PL-marker.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhaohui Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Songlin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1&quot;&gt;Kewei Tu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17245">
<title>CROP: Conservative Reward for Model-based Offline Policy Optimization. (arXiv:2310.17245v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17245</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline reinforcement learning (RL) aims to optimize policy using collected
data without online interactions. Model-based approaches are particularly
appealing for addressing offline RL challenges due to their capability to
mitigate the limitations of offline data through data generation using models.
Prior research has demonstrated that introducing conservatism into the model or
Q-function during policy optimization can effectively alleviate the prevalent
distribution drift problem in offline RL. However, the investigation into the
impacts of conservatism in reward estimation is still lacking. This paper
proposes a novel model-based offline RL algorithm, Conservative Reward for
model-based Offline Policy optimization (CROP), which conservatively estimates
the reward in model training. To achieve a conservative reward estimation, CROP
simultaneously minimizes the estimation error and the reward of random actions.
Theoretical analysis shows that this conservative reward mechanism leads to a
conservative policy evaluation and helps mitigate distribution drift.
Experiments on D4RL benchmarks showcase that the performance of CROP is
comparable to the state-of-the-art baselines. Notably, CROP establishes an
innovative connection between offline and online RL, highlighting that offline
RL problems can be tackled by adopting online RL techniques to the empirical
Markov decision process trained with a conservative reward. The source code is
available with https://github.com/G0K0URURI/CROP.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiao-Hu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiao-Liang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shi-Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zhen-Qiu Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiao-Yin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_M/0/1/0/all/0/1&quot;&gt;Mei-Jiang Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tian-Yu Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;De-Xing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1&quot;&gt;Bo-Xian Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1&quot;&gt;Zeng-Guang Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17250">
<title>IDENAS: Internal Dependency Exploration for Neural Architecture Search. (arXiv:2310.17250v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17250</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning is a powerful tool for extracting valuable information and
making various predictions from diverse datasets. Traditional algorithms rely
on well-defined input and output variables however, there are scenarios where
the distinction between the input and output variables and the underlying,
associated (input and output) layers of the model, are unknown. Neural
Architecture Search (NAS) and Feature Selection have emerged as promising
solutions in such scenarios. This research proposes IDENAS, an Internal
Dependency-based Exploration for Neural Architecture Search, integrating NAS
with feature selection. The methodology explores internal dependencies in the
complete parameter space for classification involving 1D sensor and 2D image
data as well. IDENAS employs a modified encoder-decoder model and the
Sequential Forward Search (SFS) algorithm, combining input-output configuration
search with embedded feature selection. Experimental results demonstrate
IDENASs superior performance in comparison to other algorithms, showcasing its
effectiveness in model development pipelines and automated machine learning. On
average, IDENAS achieved significant modelling improvements, underscoring its
significant contribution to advancing the state-of-the-art in neural
architecture search and feature selection integration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_A/0/1/0/all/0/1&quot;&gt;Anh T. Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viharos_Z/0/1/0/all/0/1&quot;&gt;Zsolt J. Viharos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17261">
<title>Attribute Based Interpretable Evaluation Metrics for Generative Models. (arXiv:2310.17261v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17261</link>
<description rdf:parseType="Literal">&lt;p&gt;When the training dataset comprises a 1:1 proportion of dogs to cats, a
generative model that produces 1:1 dogs and cats better resembles the training
species distribution than another model with 3:1 dogs and cats. Can we capture
this phenomenon using existing metrics? Unfortunately, we cannot, because these
metrics do not provide any interpretability beyond &quot;diversity&quot;. In this
context, we propose a new evaluation protocol that measures the divergence of a
set of generated images from the training set regarding the distribution of
attribute strengths as follows. Single-attribute Divergence (SaD) measures the
divergence regarding PDFs of a single attribute. Paired-attribute Divergence
(PaD) measures the divergence regarding joint PDFs of a pair of attributes.
They provide which attributes the models struggle. For measuring the attribute
strengths of an image, we propose Heterogeneous CLIPScore (HCS) which measures
the cosine similarity between image and text vectors with heterogeneous initial
points. With SaD and PaD, we reveal the following about existing generative
models. ProjectedGAN generates implausible attribute relationships such as a
baby with a beard even though it has competitive scores of existing metrics.
Diffusion models struggle to capture diverse colors in the datasets. The larger
sampling timesteps of latent diffusion model generate the more minor objects
including earrings and necklaces. Stable Diffusion v1.5 better captures the
attributes than v2.1. Our metrics lay a foundation for explainable evaluations
of generative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dongkyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_M/0/1/0/all/0/1&quot;&gt;Mingi Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1&quot;&gt;Youngjung Uh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17300">
<title>Comparing Photorealistic and Animated Embodied Conversational Agents in Serious Games: An Empirical Study on User Experience. (arXiv:2310.17300v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2310.17300</link>
<description rdf:parseType="Literal">&lt;p&gt;Embodied conversational agents (ECAs) are paradigms of conversational user
interfaces in the form of embodied characters. While ECAs offer various
manipulable features, this paper focuses on a study conducted to explore two
distinct levels of presentation realism. The two agent versions are
photorealistic and animated. The study aims to provide insights and design
suggestions for speech-enabled ECAs within serious game environments. A
within-subjects, two-by-two factorial design was employed for this research
with a cohort of 36 participants balanced for gender. The results showed that
both the photorealistic and the animated versions were perceived as highly
usable, with overall mean scores of 5.76 and 5.71, respectively. However, 69.4
per cent of the participants stated they preferred the photorealistic version,
25 per cent stated they preferred the animated version and 5.6 per cent had no
stated preference. The photorealistic agents were perceived as more realistic
and human-like, while the animated characters made the task feel more like a
game. Even though the agents&apos; realism had no significant effect on usability,
it positively influenced participants&apos; perceptions of the agent. This research
aims to lay the groundwork for future studies on ECA realism&apos;s impact in
serious games across diverse contexts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korre_D/0/1/0/all/0/1&quot;&gt;Danai Korre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17306">
<title>FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language. (arXiv:2310.17306v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.17306</link>
<description rdf:parseType="Literal">&lt;p&gt;Formatting is an important property in tables for visualization,
presentation, and analysis. Spreadsheet software allows users to automatically
format their tables by writing data-dependent conditional formatting (CF)
rules. Writing such rules is often challenging for users as it requires them to
understand and implement the underlying logic. We present FormaT5, a
transformer-based model that can generate a CF rule given the target table and
a natural language description of the desired formatting logic. We find that
user descriptions for these tasks are often under-specified or ambiguous,
making it harder for code generation systems to accurately learn the desired
rule in a single step. To tackle this problem of under-specification and
minimise argument errors, FormaT5 learns to predict placeholders though an
abstention objective. These placeholders can then be filled by a second model
or, when examples of rows that should be formatted are available, by a
programming-by-example system. To evaluate FormaT5 on diverse and real
scenarios, we create an extensive benchmark of 1053 CF tasks, containing
real-world descriptions collected from four different sources. We release our
benchmarks to encourage research in this area. Abstention and filling allow
FormaT5 to outperform 8 different neural approaches on our benchmarks, both
with and without examples. Our results illustrate the value of building
domain-specific learning systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Mukul Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cambronero_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Cambronero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulwani_S/0/1/0/all/0/1&quot;&gt;Sumit Gulwani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1&quot;&gt;Vu Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Negreanu_C/0/1/0/all/0/1&quot;&gt;Carina Negreanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nouri_E/0/1/0/all/0/1&quot;&gt;Elnaz Nouri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raza_M/0/1/0/all/0/1&quot;&gt;Mohammad Raza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbruggen_G/0/1/0/all/0/1&quot;&gt;Gust Verbruggen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17325">
<title>C-Disentanglement: Discovering Causally-Independent Generative Factors under an Inductive Bias of Confounder. (arXiv:2310.17325v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17325</link>
<description rdf:parseType="Literal">&lt;p&gt;Representation learning assumes that real-world data is generated by a few
semantically meaningful generative factors (i.e., sources of variation) and
aims to discover them in the latent space. These factors are expected to be
causally disentangled, meaning that distinct factors are encoded into separate
latent variables, and changes in one factor will not affect the values of the
others. Compared to statistical independence, causal disentanglement allows
more controllable data generation, improved robustness, and better
generalization. However, most existing work assumes unconfoundedness in the
discovery process, that there are no common causes to the generative factors
and thus obtain only statistical independence. In this paper, we recognize the
importance of modeling confounders in discovering causal generative factors.
Unfortunately, such factors are not identifiable without proper inductive bias.
We fill the gap by introducing a framework entitled Confounded-Disentanglement
(C-Disentanglement), the first framework that explicitly introduces the
inductive bias of confounder via labels from domain expertise. In addition, we
accordingly propose an approach to sufficiently identify the causally
disentangled factors under any inductive bias of the confounder. We conduct
extensive experiments on both synthetic and real-world datasets. Our method
demonstrates competitive results compared to various SOTA baselines in
obtaining causally disentangled features and downstream tasks under domain
shifts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jiaxin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1&quot;&gt;Bang An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuancheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yifan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Furong Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17330">
<title>CQM: Curriculum Reinforcement Learning with a Quantized World Model. (arXiv:2310.17330v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17330</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent curriculum Reinforcement Learning (RL) has shown notable progress in
solving complex tasks by proposing sequences of surrogate tasks. However, the
previous approaches often face challenges when they generate curriculum goals
in a high-dimensional space. Thus, they usually rely on manually specified goal
spaces. To alleviate this limitation and improve the scalability of the
curriculum, we propose a novel curriculum method that automatically defines the
semantic goal space which contains vital information for the curriculum
process, and suggests curriculum goals over it. To define the semantic goal
space, our method discretizes continuous observations via vector
quantized-variational autoencoders (VQ-VAE) and restores the temporal relations
between the discretized observations by a graph. Concurrently, ours suggests
uncertainty and temporal distance-aware curriculum goals that converges to the
final goals over the automatically composed goal space. We demonstrate that the
proposed method allows efficient explorations in an uninformed environment with
raw goal examples only. Also, ours outperforms the state-of-the-art curriculum
RL methods on data efficiency and performance, in various goal-reaching tasks
even with ego-centric visual inputs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seungjae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_D/0/1/0/all/0/1&quot;&gt;Daesol Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jonghae Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;H. Jin Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17353">
<title>Cultural Adaptation of Recipes. (arXiv:2310.17353v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.17353</link>
<description rdf:parseType="Literal">&lt;p&gt;Building upon the considerable advances in Large Language Models (LLMs), we
are now equipped to address more sophisticated tasks demanding a nuanced
understanding of cross-cultural contexts. A key example is recipe adaptation,
which goes beyond simple translation to include a grasp of ingredients,
culinary techniques, and dietary preferences specific to a given culture. We
introduce a new task involving the translation and cultural adaptation of
recipes between Chinese and English-speaking cuisines. To support this
investigation, we present CulturalRecipes, a unique dataset comprised of
automatically paired recipes written in Mandarin Chinese and English. This
dataset is further enriched with a human-written and curated test set. In this
intricate task of cross-cultural recipe adaptation, we evaluate the performance
of various methods, including GPT-4 and other LLMs, traditional machine
translation, and information retrieval techniques. Our comprehensive analysis
includes both automatic and human evaluation metrics. While GPT-4 exhibits
impressive abilities in adapting Chinese recipes into English, it still lags
behind human expertise when translating English recipes into Chinese. This
underscores the multifaceted nature of cultural adaptations. We anticipate that
these insights will significantly contribute to future research on
culturally-aware language models and their practical application in culturally
diverse contexts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kementchedjhieva_Y/0/1/0/all/0/1&quot;&gt;Yova Kementchedjhieva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1&quot;&gt;Ruixiang Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karamolegkou_A/0/1/0/all/0/1&quot;&gt;Antonia Karamolegkou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Li Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dare_M/0/1/0/all/0/1&quot;&gt;Megan Dare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donatelli_L/0/1/0/all/0/1&quot;&gt;Lucia Donatelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1&quot;&gt;Daniel Hershcovich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17370">
<title>Exploring the Potential of Generative AI for the World Wide Web. (arXiv:2310.17370v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.17370</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Artificial Intelligence (AI) is a cutting-edge technology capable
of producing text, images, and various media content leveraging generative
models and user prompts. Between 2022 and 2023, generative AI surged in
popularity with a plethora of applications spanning from AI-powered movies to
chatbots. In this paper, we delve into the potential of generative AI within
the realm of the World Wide Web, specifically focusing on image generation. Web
developers already harness generative AI to help crafting text and images,
while Web browsers might use it in the future to locally generate images for
tasks like repairing broken webpages, conserving bandwidth, and enhancing
privacy. To explore this research area, we have developed WebDiffusion, a tool
that allows to simulate a Web powered by stable diffusion, a popular
text-to-image model, from both a client and server perspective. WebDiffusion
further supports crowdsourcing of user opinions, which we use to evaluate the
quality and accuracy of 409 AI-generated images sourced from 60 webpages. Our
findings suggest that generative AI is already capable of producing pertinent
and high-quality Web images, even without requiring Web designers to manually
input prompts, just by leveraging contextual information available within the
webpages. However, we acknowledge that direct in-browser image generation
remains a challenge, as only highly powerful GPUs, such as the A40 and A100,
can (partially) compete with classic image downloads. Nevertheless, this
approach could be valuable for a subset of the images, for example when fixing
broken webpages or handling highly private content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+AlDahoul_N/0/1/0/all/0/1&quot;&gt;Nouar AlDahoul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Joseph Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varvello_M/0/1/0/all/0/1&quot;&gt;Matteo Varvello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaki_Y/0/1/0/all/0/1&quot;&gt;Yasir Zaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17372">
<title>Dialogue-based generation of self-driving simulation scenarios using Large Language Models. (arXiv:2310.17372v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.17372</link>
<description rdf:parseType="Literal">&lt;p&gt;Simulation is an invaluable tool for developing and evaluating controllers
for self-driving cars. Current simulation frameworks are driven by
highly-specialist domain specific languages, and so a natural language
interface would greatly enhance usability. But there is often a gap, consisting
of tacit assumptions the user is making, between a concise English utterance
and the executable code that captures the user&apos;s intent. In this paper we
describe a system that addresses this issue by supporting an extended
multimodal interaction: the user can follow up prior instructions with
refinements or revisions, in reaction to the simulations that have been
generated from their utterances so far. We use Large Language Models (LLMs) to
map the user&apos;s English utterances in this interaction into domain-specific
code, and so we explore the extent to which LLMs capture the context
sensitivity that&apos;s necessary for computing the speaker&apos;s intended message in
discourse.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miceli_Barone_A/0/1/0/all/0/1&quot;&gt;Antonio Valerio Miceli-Barone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lascarides_A/0/1/0/all/0/1&quot;&gt;Alex Lascarides&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Innes_C/0/1/0/all/0/1&quot;&gt;Craig Innes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17378">
<title>Optimization dependent generalization bound for ReLU networks based on sensitivity in the tangent bundle. (arXiv:2310.17378v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17378</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in deep learning have given us some very promising results on
the generalization ability of deep neural networks, however literature still
lacks a comprehensive theory explaining why heavily over-parametrized models
are able to generalize well while fitting the training data. In this paper we
propose a PAC type bound on the generalization error of feedforward ReLU
networks via estimating the Rademacher complexity of the set of networks
available from an initial parameter vector via gradient descent. The key idea
is to bound the sensitivity of the network&apos;s gradient to perturbation of the
input data along the optimization trajectory. The obtained bound does not
explicitly depend on the depth of the network. Our results are experimentally
verified on the MNIST and CIFAR-10 datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Racz_D/0/1/0/all/0/1&quot;&gt;D&amp;#xe1;niel R&amp;#xe1;cz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petreczky_M/0/1/0/all/0/1&quot;&gt;Mih&amp;#xe1;ly Petreczky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Csertan_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe1;s Csert&amp;#xe1;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daroczy_B/0/1/0/all/0/1&quot;&gt;B&amp;#xe1;lint Dar&amp;#xf3;czy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17379">
<title>YOLO-BEV: Generating Bird&apos;s-Eye View in the Same Way as 2D Object Detection. (arXiv:2310.17379v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17379</link>
<description rdf:parseType="Literal">&lt;p&gt;Vehicle perception systems strive to achieve comprehensive and rapid visual
interpretation of their surroundings for improved safety and navigation. We
introduce YOLO-BEV, an efficient framework that harnesses a unique surrounding
cameras setup to generate a 2D bird&apos;s-eye view of the vehicular environment. By
strategically positioning eight cameras, each at a 45-degree interval, our
system captures and integrates imagery into a coherent 3x3 grid format, leaving
the center blank, providing an enriched spatial representation that facilitates
efficient processing. In our approach, we employ YOLO&apos;s detection mechanism,
favoring its inherent advantages of swift response and compact model structure.
Instead of leveraging the conventional YOLO detection head, we augment it with
a custom-designed detection head, translating the panoramically captured data
into a unified bird&apos;s-eye view map of ego car. Preliminary results validate the
feasibility of YOLO-BEV in real-time vehicular perception tasks. With its
streamlined architecture and potential for rapid deployment due to minimized
parameters, YOLO-BEV poses as a promising tool that may reshape future
perspectives in autonomous driving systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Liguo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yanliang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1&quot;&gt;Alois Knoll&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17389">
<title>ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation. (arXiv:2310.17389v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.17389</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite remarkable advances that large language models have achieved in
chatbots, maintaining a non-toxic user-AI interactive environment has become
increasingly critical nowadays. However, previous efforts in toxicity detection
have been mostly based on benchmarks derived from social media content, leaving
the unique challenges inherent to real-world user-AI interactions
insufficiently explored. In this work, we introduce ToxicChat, a novel
benchmark based on real user queries from an open-source chatbot. This
benchmark contains the rich, nuanced phenomena that can be tricky for current
toxicity detection models to identify, revealing a significant domain
difference compared to social media content. Our systematic evaluation of
models trained on existing toxicity datasets has shown their shortcomings when
applied to this unique domain of ToxicChat. Our work illuminates the
potentially overlooked challenges of toxicity detection in real-world user-AI
conversations. In the future, ToxicChat can be a valuable resource to drive
further advancements toward building a safe and healthy environment for user-AI
interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1&quot;&gt;Yongqi Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yangkun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuxin Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yujia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1&quot;&gt;Jingbo Shang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17404">
<title>Invariance Measures for Neural Networks. (arXiv:2310.17404v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17404</link>
<description rdf:parseType="Literal">&lt;p&gt;Invariances in neural networks are useful and necessary for many tasks.
However, the representation of the invariance of most neural network models has
not been characterized. We propose measures to quantify the invariance of
neural networks in terms of their internal representation. The measures are
efficient and interpretable, and can be applied to any neural network model.
They are also more sensitive to invariance than previously defined measures. We
validate the measures and their properties in the domain of affine
transformations and the CIFAR10 and MNIST datasets, including their stability
and interpretability. Using the measures, we perform a first analysis of CNN
models and show that their internal invariance is remarkably stable to random
weight initializations, but not to changes in dataset or transformation. We
believe the measures will enable new avenues of research in invariance
representation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quiroga_F/0/1/0/all/0/1&quot;&gt;Facundo Manuel Quiroga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torrents_Barrena_J/0/1/0/all/0/1&quot;&gt;Jordina Torrents-Barrena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanzarini_L/0/1/0/all/0/1&quot;&gt;Laura Cristina Lanzarini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puig_Valls_D/0/1/0/all/0/1&quot;&gt;Domenec Puig-Valls&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17410">
<title>Synthesizing Efficiently Monitorable Formulas in Metric Temporal Logic. (arXiv:2310.17410v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.17410</link>
<description rdf:parseType="Literal">&lt;p&gt;In runtime verification, manually formalizing a specification for monitoring
system executions is a tedious and error-prone process. To address this issue,
we consider the problem of automatically synthesizing formal specifications
from system executions. To demonstrate our approach, we consider the popular
specification language Metric Temporal Logic (MTL), which is particularly
tailored towards specifying temporal properties for cyber-physical systems
(CPS). Most of the classical approaches for synthesizing temporal logic
formulas aim at minimizing the size of the formula. However, for efficiency in
monitoring, along with the size, the amount of &quot;lookahead&quot; required for the
specification becomes relevant, especially for safety-critical applications. We
formalize this notion and devise a learning algorithm that synthesizes concise
formulas having bounded lookahead. To do so, our algorithm reduces the
synthesis task to a series of satisfiability problems in Linear Real Arithmetic
(LRA) and generates MTL formulas from their satisfying assignments. The
reduction uses a novel encoding of a popular MTL monitoring procedure using
LRA. Finally, we implement our algorithm in a tool called TEAL and demonstrate
its ability to synthesize efficiently monitorable MTL formulas in a CPS
application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raha_R/0/1/0/all/0/1&quot;&gt;Ritam Raha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1&quot;&gt;Rajarshi Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fijalkow_N/0/1/0/all/0/1&quot;&gt;Nathanael Fijalkow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neider_D/0/1/0/all/0/1&quot;&gt;Daniel Neider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1&quot;&gt;Guillermo A. Perez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17415">
<title>PETA: Evaluating the Impact of Protein Transfer Learning with Sub-word Tokenization on Downstream Applications. (arXiv:2310.17415v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.17415</link>
<description rdf:parseType="Literal">&lt;p&gt;Large protein language models are adept at capturing the underlying
evolutionary information in primary structures, offering significant practical
value for protein engineering. Compared to natural language models, protein
amino acid sequences have a smaller data volume and a limited combinatorial
space. Choosing an appropriate vocabulary size to optimize the pre-trained
model is a pivotal issue. Moreover, despite the wealth of benchmarks and
studies in the natural language community, there remains a lack of a
comprehensive benchmark for systematically evaluating protein language model
quality. Given these challenges, PETA trained language models with 14 different
vocabulary sizes under three tokenization methods. It conducted thousands of
tests on 33 diverse downstream datasets to assess the models&apos; transfer learning
capabilities, incorporating two classification heads and three random seeds to
mitigate potential biases. Extensive experiments indicate that vocabulary sizes
between 50 and 200 optimize the model, whereas sizes exceeding 800
detrimentally affect the model&apos;s representational performance. Our code, model
weights and datasets are available at
https://github.com/ginnm/ProteinPretraining.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Yang Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mingchen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1&quot;&gt;Pan Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Ziyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Huiqun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_G/0/1/0/all/0/1&quot;&gt;Guisheng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1&quot;&gt;Liang Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17416">
<title>Goals are Enough: Inducing AdHoc cooperation among unseen Multi-Agent systems in IMFs. (arXiv:2310.17416v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.17416</link>
<description rdf:parseType="Literal">&lt;p&gt;Intent-based management will play a critical role in achieving customers&apos;
expectations in the next-generation mobile networks. Traditional methods cannot
perform efficient resource management since they tend to handle each
expectation independently. Existing approaches, e.g., based on multi-agent
reinforcement learning (MARL) allocate resources in an efficient fashion when
there are conflicting expectations on the network slice. However, in reality,
systems are often far more complex to be addressed by a standalone MARL
formulation. Often there exists a hierarchical structure of intent fulfilment
where multiple pre-trained, self-interested agents may need to be further
orchestrated by a supervisor or controller agent. Such agents may arrive in the
system adhoc, which then needs to be orchestrated along with other available
agents. Retraining the whole system every time is often infeasible given the
associated time and cost. Given the challenges, such adhoc coordination of
pre-trained systems could be achieved through an intelligent supervisor agent
which incentivizes pre-trained RL/MARL agents through sets of dynamic contracts
(goals or bonuses) and encourages them to act as a cohesive unit towards
fulfilling a global expectation. Some approaches use a rule-based supervisor
agent and deploy the hierarchical constituent agents sequentially, based on
human-coded rules.
&lt;/p&gt;
&lt;p&gt;In the current work, we propose a framework whereby pre-trained agents can be
orchestrated in parallel leveraging an AI-based supervisor agent. For this, we
propose to use Adhoc-Teaming approaches which assign optimal goals to the MARL
agents and incentivize them to exhibit certain desired behaviours. Results on
the network emulator show that the proposed approach results in faster and
improved fulfilment of expectations when compared to rule-based approaches and
even generalizes to changes in environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_K/0/1/0/all/0/1&quot;&gt;Kaushik Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perepu_S/0/1/0/all/0/1&quot;&gt;Satheesh K. Perepu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Abir Das&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17421">
<title>Distribution of Action Movements (DAM): A Descriptor for Human Action Recognition. (arXiv:2310.17421v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17421</link>
<description rdf:parseType="Literal">&lt;p&gt;Human action recognition from skeletal data is an important and active area
of research in which the state of the art has not yet achieved near-perfect
accuracy on many well-known datasets. In this paper, we introduce the
Distribution of Action Movements Descriptor, a novel action descriptor based on
the distribution of the directions of the motions of the joints between frames,
over the set of all possible motions in the dataset. The descriptor is computed
as a normalized histogram over a set of representative directions of the
joints, which are in turn obtained via clustering. While the descriptor is
global in the sense that it represents the overall distribution of movement
directions of an action, it is able to partially retain its temporal structure
by applying a windowing scheme.
&lt;/p&gt;
&lt;p&gt;The descriptor, together with a standard classifier, outperforms several
state-of-the-art techniques on many well-known datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quiroga_F/0/1/0/all/0/1&quot;&gt;Facundo Manuel Quiroga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ronchetti_F/0/1/0/all/0/1&quot;&gt;Franco Ronchetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanzarini_L/0/1/0/all/0/1&quot;&gt;Laura Lanzarini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eestrebou_C/0/1/0/all/0/1&quot;&gt;Cesar Eestrebou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17427">
<title>Handshape recognition for Argentinian Sign Language using ProbSom. (arXiv:2310.17427v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17427</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic sign language recognition is an important topic within the areas of
human-computer interaction and machine learning. On the one hand, it poses a
complex challenge that requires the intervention of various knowledge areas,
such as video processing, image processing, intelligent systems and
linguistics. On the other hand, robust recognition of sign language could
assist in the translation process and the integration of hearing-impaired
people.
&lt;/p&gt;
&lt;p&gt;This paper offers two main contributions: first, the creation of a database
of handshapes for the Argentinian Sign Language (LSA), which is a topic that
has barely been discussed so far. Secondly, a technique for image processing,
descriptor extraction and subsequent handshape classification using a
supervised adaptation of self-organizing maps that is called ProbSom. This
technique is compared to others in the state of the art, such as Support Vector
Machines (SVM), Random Forests, and Neural Networks.
&lt;/p&gt;
&lt;p&gt;The database that was built contains 800 images with 16 LSA handshapes, and
is a first step towards building a comprehensive database of Argentinian signs.
The ProbSom-based neural classifier, using the proposed descriptor, achieved an
accuracy rate above 90%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ronchetti_F/0/1/0/all/0/1&quot;&gt;Franco Ronchetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quiroga_F/0/1/0/all/0/1&quot;&gt;Facundo Manuel Quiroga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Estrebou_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;sar Estrebou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanzarini_L/0/1/0/all/0/1&quot;&gt;Laura Lanzarini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17429">
<title>LSA64: An Argentinian Sign Language Dataset. (arXiv:2310.17429v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17429</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic sign language recognition is a research area that encompasses
human-computer interaction, computer vision and machine learning. Robust
automatic recognition of sign language could assist in the translation process
and the integration of hearing-impaired people, as well as the teaching of sign
language to the hearing population. Sign languages differ significantly in
different countries and even regions, and their syntax and semantics are
different as well from those of written languages. While the techniques for
automatic sign language recognition are mostly the same for different
languages, training a recognition system for a new language requires having an
entire dataset for that language. This paper presents a dataset of 64 signs
from the Argentinian Sign Language (LSA). The dataset, called LSA64, contains
3200 videos of 64 different LSA signs recorded by 10 subjects, and is a first
step towards building a comprehensive research-level dataset of Argentinian
signs, specifically tailored to sign language recognition or other machine
learning tasks. The subjects that performed the signs wore colored gloves to
ease the hand tracking and segmentation steps, allowing experiments on the
dataset to focus specifically on the recognition of signs. We also present a
pre-processed version of the dataset, from which we computed statistics of
movement, position and handshape of the signs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ronchetti_F/0/1/0/all/0/1&quot;&gt;Franco Ronchetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quiroga_F/0/1/0/all/0/1&quot;&gt;Facundo Manuel Quiroga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Estrebou_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;sar Estrebou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanzarini_L/0/1/0/all/0/1&quot;&gt;Laura Lanzarini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosete_A/0/1/0/all/0/1&quot;&gt;Alejandro Rosete&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17451">
<title>Generating by Understanding: Neural Visual Generation with Logical Symbol Groundings. (arXiv:2310.17451v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.17451</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the great success of neural visual generative models in recent years,
integrating them with strong symbolic knowledge reasoning systems remains a
challenging task. The main challenges are two-fold: one is symbol assignment,
i.e. bonding latent factors of neural visual generators with meaningful symbols
from knowledge reasoning systems. Another is rule learning, i.e. learning new
rules, which govern the generative process of the data, to augment the
knowledge reasoning systems. To deal with these symbol grounding problems, we
propose a neural-symbolic learning approach, Abductive Visual Generation
(AbdGen), for integrating logic programming systems with neural visual
generative models based on the abductive learning framework. To achieve
reliable and efficient symbol assignment, the quantized abduction method is
introduced for generating abduction proposals by the nearest-neighbor lookups
within semantic codebooks. To achieve precise rule learning, the contrastive
meta-abduction method is proposed to eliminate wrong rules with positive cases
and avoid less-informative rules with negative cases simultaneously.
Experimental results on various benchmark datasets show that compared to the
baselines, AbdGen requires significantly fewer instance-level labeling
information for symbol assignment. Furthermore, our approach can effectively
learn underlying logical generative rules from data, which is out of the
capability of existing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yifei Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yu Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhexu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yao-Xiang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Wang-Zhou Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhong Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kun Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17462">
<title>Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion. (arXiv:2310.17462v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17462</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel method for precise 3D object localization in single images
from a single calibrated camera using only 2D labels. No expensive 3D labels
are needed. Thus, instead of using 3D labels, our model is trained with
easy-to-annotate 2D labels along with the physical knowledge of the object&apos;s
motion. Given this information, the model can infer the latent third dimension,
even though it has never seen this information during training. Our method is
evaluated on both synthetic and real-world datasets, and we are able to achieve
a mean distance error of just 6 cm in our experiments on real data. The results
indicate the method&apos;s potential as a step towards learning 3D object location
estimation, where collecting 3D data for training is not feasible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kienzle_D/0/1/0/all/0/1&quot;&gt;Daniel Kienzle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorenz_J/0/1/0/all/0/1&quot;&gt;Julian Lorenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ludwig_K/0/1/0/all/0/1&quot;&gt;Katja Ludwig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lienhart_R/0/1/0/all/0/1&quot;&gt;Rainer Lienhart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17489">
<title>Bias in Evaluation Processes: An Optimization-Based Model. (arXiv:2310.17489v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2310.17489</link>
<description rdf:parseType="Literal">&lt;p&gt;Biases with respect to socially-salient attributes of individuals have been
well documented in evaluation processes used in settings such as admissions and
hiring. We view such an evaluation process as a transformation of a
distribution of the true utility of an individual for a task to an observed
distribution and model it as a solution to a loss minimization problem subject
to an information constraint. Our model has two parameters that have been
identified as factors leading to biases: the resource-information trade-off
parameter in the information constraint and the risk-averseness parameter in
the loss function. We characterize the distributions that arise from our model
and study the effect of the parameters on the observed distribution. The
outputs of our model enrich the class of distributions that can be used to
capture variation across groups in the observed evaluations. We empirically
validate our model by fitting real-world datasets and use it to study the
effect of interventions in a downstream selection task. These results
contribute to an understanding of the emergence of bias in evaluation processes
and provide tools to guide the deployment of interventions to mitigate biases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celis_L/0/1/0/all/0/1&quot;&gt;L. Elisa Celis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Amit Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehrotra_A/0/1/0/all/0/1&quot;&gt;Anay Mehrotra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishnoi_N/0/1/0/all/0/1&quot;&gt;Nisheeth K. Vishnoi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17490">
<title>Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering. (arXiv:2310.17490v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.17490</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) enable zero-shot approaches in open-domain
question answering (ODQA), yet with limited advancements as the reader is
compared to the retriever. This study aims at the feasibility of a zero-shot
reader that addresses the challenges of computational cost and the need for
labeled data. We find that LLMs are distracted due to irrelevant documents in
the retrieved set and the overconfidence of the generated answers when they are
exploited as zero-shot readers. To tackle these problems, we mitigate the
impact of such documents via Distraction-aware Answer Selection (DAS) with a
negation-based instruction and score adjustment for proper answer selection.
Experimental results show that our approach successfully handles distraction
across diverse scenarios, enhancing the performance of zero-shot readers.
Furthermore, unlike supervised readers struggling with unseen data, zero-shot
readers demonstrate outstanding transferability without any training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1&quot;&gt;Sukmin Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Jeong yeon Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1&quot;&gt;Soyeong Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jong C. Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17492">
<title>Orchestration of Emulator Assisted Mobile Edge Tuning for AI Foundation Models: A Multi-Agent Deep Reinforcement Learning Approach. (arXiv:2310.17492v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.17492</link>
<description rdf:parseType="Literal">&lt;p&gt;The efficient deployment and fine-tuning of foundation models are pivotal in
contemporary artificial intelligence. In this study, we present a
groundbreaking paradigm integrating Mobile Edge Computing (MEC) with foundation
models, specifically designed to enhance local task performance on user
equipment (UE). Central to our approach is the innovative Emulator-Adapter
architecture, segmenting the foundation model into two cohesive modules. This
design not only conserves computational resources but also ensures adaptability
and fine-tuning efficiency for downstream tasks. Additionally, we introduce an
advanced resource allocation mechanism that is fine-tuned to the needs of the
Emulator-Adapter structure in decentralized settings. To address the challenges
presented by this system, we employ a hybrid multi-agent Deep Reinforcement
Learning (DRL) strategy, adept at handling mixed discrete-continuous action
spaces, ensuring dynamic and optimal resource allocations. Our comprehensive
simulations and validations underscore the practical viability of our approach,
demonstrating its robustness, efficiency, and scalability. Collectively, this
work offers a fresh perspective on deploying foundation models and balancing
computational efficiency with task proficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wenhan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1&quot;&gt;Terence Jie Chua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jun Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17512">
<title>CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents. (arXiv:2310.17512v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.17512</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have been widely used as agents to complete
different tasks, such as personal assistance or event planning. While most work
has focused on cooperation and collaboration between agents, little work
explores competition, another important mechanism that fosters the development
of society and economy. In this paper, we seek to examine the competition
behaviors in LLM-based agents. We first propose a general framework to study
the competition between agents. Then, we implement a practical competitive
environment using GPT-4 to simulate a virtual town with two types of agents,
including restaurant agents and customer agents. Specifically, restaurant
agents compete with each other to attract more customers, where the competition
fosters them to transform, such as cultivating new operating strategies. The
results of our experiments reveal several interesting findings ranging from
social learning to Matthew Effect, which aligns well with existing sociological
and economic theories. We believe that competition between agents deserves
further investigation to help us understand society better. The code will be
released soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qinlin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yixuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yiqiao Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kaijie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17513">
<title>The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17513</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that
leverages low-rank adaptation of weight matrices, has emerged as a prevalent
technique for fine-tuning pre-trained models such as large language models and
diffusion models. Despite its huge success in practice, the theoretical
underpinnings of LoRA have largely remained unexplored. This paper takes the
first step to bridge this gap by theoretically analyzing the expressive power
of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any
model $f$ to accurately represent any smaller target model $\overline{f}$ if
LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of
}\overline{f}}{\text{depth of }f}$. We also quantify the approximation error
when LoRA-rank is lower than the threshold. For Transformer networks, we show
any model can be adapted to a target model of the same size with
rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yuchen Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kangwook Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17526">
<title>Can large language models replace humans in the systematic review process? Evaluating GPT-4&apos;s efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. (arXiv:2310.17526v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.17526</link>
<description rdf:parseType="Literal">&lt;p&gt;Systematic reviews are vital for guiding practice, research, and policy, yet
they are often slow and labour-intensive. Large language models (LLMs) could
offer a way to speed up and automate systematic reviews, but their performance
in such tasks has not been comprehensively evaluated against humans, and no
study has tested GPT-4, the biggest LLM so far. This pre-registered study
evaluates GPT-4&apos;s capability in title/abstract screening, full-text review, and
data extraction across various literature types and languages using a
&apos;human-out-of-the-loop&apos; approach. Although GPT-4 had accuracy on par with human
performance in most tasks, results were skewed by chance agreement and dataset
imbalance. After adjusting for these, there was a moderate level of performance
for data extraction, and - barring studies that used highly reliable prompts -
screening performance levelled at none to moderate for different stages and
languages. When screening full-text literature using highly reliable prompts,
GPT-4&apos;s performance was &apos;almost perfect.&apos; Penalising GPT-4 for missing key
studies using highly reliable prompts improved its performance even more. Our
findings indicate that, currently, substantial caution should be used if LLMs
are being used to conduct systematic reviews, but suggest that, for certain
systematic review tasks delivered under reliable prompts, LLMs can rival human
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khraisha_Q/0/1/0/all/0/1&quot;&gt;Qusai Khraisha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Put_S/0/1/0/all/0/1&quot;&gt;Sophie Put&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kappenberg_J/0/1/0/all/0/1&quot;&gt;Johanna Kappenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warraitch_A/0/1/0/all/0/1&quot;&gt;Azza Warraitch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadfield_K/0/1/0/all/0/1&quot;&gt;Kristin Hadfield&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17534">
<title>SoK: Pitfalls in Evaluating Black-Box Attacks. (arXiv:2310.17534v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2310.17534</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerous works study black-box attacks on image classifiers. However, these
works make different assumptions on the adversary&apos;s knowledge and current
literature lacks a cohesive organization centered around the threat model. To
systematize knowledge in this area, we propose a taxonomy over the threat space
spanning the axes of feedback granularity, the access of interactive queries,
and the quality and quantity of the auxiliary data available to the attacker.
Our new taxonomy provides three key insights. 1) Despite extensive literature,
numerous under-explored threat spaces exist, which cannot be trivially solved
by adapting techniques from well-explored settings. We demonstrate this by
establishing a new state-of-the-art in the less-studied setting of access to
top-k confidence scores by adapting techniques from well-explored settings of
accessing the complete confidence vector, but show how it still falls short of
the more restrictive setting that only obtains the prediction label,
highlighting the need for more research. 2) Identification the threat model of
different attacks uncovers stronger baselines that challenge prior
state-of-the-art claims. We demonstrate this by enhancing an initially weaker
baseline (under interactive query access) via surrogate models, effectively
overturning claims in the respective paper. 3) Our taxonomy reveals
interactions between attacker knowledge that connect well to related areas,
such as model inversion and extraction attacks. We discuss how advances in
other areas can enable potentially stronger black-box attacks. Finally, we
emphasize the need for a more realistic assessment of attack success by
factoring in local attack runtime. This approach reveals the potential for
certain attacks to achieve notably higher success rates and the need to
evaluate attacks in diverse and harder settings, highlighting the need for
better selection criteria.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suya_F/0/1/0/all/0/1&quot;&gt;Fnu Suya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suri_A/0/1/0/all/0/1&quot;&gt;Anshuman Suri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tingwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Jingtao Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yuan Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_D/0/1/0/all/0/1&quot;&gt;David Evans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17537">
<title>Neuro-Inspired Fragmentation and Recall to Overcome Catastrophic Forgetting in Curiosity. (arXiv:2310.17537v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.17537</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning methods exhibit impressive performance on a range
of tasks but still struggle on hard exploration tasks in large environments
with sparse rewards. To address this, intrinsic rewards can be generated using
forward model prediction errors that decrease as the environment becomes known,
and incentivize an agent to explore novel states. While prediction-based
intrinsic rewards can help agents solve hard exploration tasks, they can suffer
from catastrophic forgetting and actually increase at visited states. We first
examine the conditions and causes of catastrophic forgetting in grid world
environments. We then propose a new method FARCuriosity, inspired by how humans
and animals learn. The method depends on fragmentation and recall: an agent
fragments an environment based on surprisal, and uses different local curiosity
modules (prediction-based intrinsic reward functions) for each fragment so that
modules are not trained on the entire environment. At each fragmentation event,
the agent stores the current module in long-term memory (LTM) and either
initializes a new module or recalls a previously stored module based on its
match with the current state. With fragmentation and recall, FARCuriosity
achieves less forgetting and better overall performance in games with varied
and heterogeneous environments in the Atari benchmark suite of tasks. Thus,
this work highlights the problem of catastrophic forgetting in prediction-based
curiosity methods and proposes a solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jaedong Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1&quot;&gt;Zhang-Wei Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1&quot;&gt;Eric Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boopathy_A/0/1/0/all/0/1&quot;&gt;Akhilan Boopathy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1&quot;&gt;Pulkit Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiete_I/0/1/0/all/0/1&quot;&gt;Ila Fiete&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17550">
<title>Human-Guided Complexity-Controlled Abstractions. (arXiv:2310.17550v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17550</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks often learn task-specific latent representations that fail to
generalize to novel settings or tasks. Conversely, humans learn discrete
representations (i.e., concepts or words) at a variety of abstraction levels
(e.g., ``bird&apos;&apos; vs. ``sparrow&apos;&apos;) and deploy the appropriate abstraction based
on task. Inspired by this, we train neural models to generate a spectrum of
discrete representations, and control the complexity of the representations
(roughly, how many bits are allocated for encoding inputs) by tuning the
entropy of the distribution over representations. In finetuning experiments,
using only a small number of labeled examples for a new task, we show that (1)
tuning the representation to a task-appropriate complexity level supports the
highest finetuning performance, and (2) in a human-participant study, users
were able to identify the appropriate complexity level for a downstream task
using visualizations of discrete representations. Our results indicate a
promising direction for rapid model finetuning by leveraging human insight.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1&quot;&gt;Andi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tucker_M/0/1/0/all/0/1&quot;&gt;Mycal Tucker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kenny_E/0/1/0/all/0/1&quot;&gt;Eoin Kenny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaslavsky_N/0/1/0/all/0/1&quot;&gt;Noga Zaslavsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1&quot;&gt;Pulkit Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1&quot;&gt;Julie Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17551">
<title>Unpacking the Ethical Value Alignment in Big Models. (arXiv:2310.17551v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2310.17551</link>
<description rdf:parseType="Literal">&lt;p&gt;Big models have greatly advanced AI&apos;s ability to understand, generate, and
manipulate information and content, enabling numerous applications. However, as
these models become increasingly integrated into everyday life, their inherent
ethical values and potential biases pose unforeseen risks to society. This
paper provides an overview of the risks and challenges associated with big
models, surveys existing AI ethics guidelines, and examines the ethical
implications arising from the limitations of these models. Taking a normative
ethics perspective, we propose a reassessment of recent normative guidelines,
highlighting the importance of collaborative efforts in academia to establish a
unified and universal AI ethics framework. Furthermore, we investigate the
moral inclinations of current mainstream LLMs using the Moral Foundation
theory, analyze existing alignment algorithms, and outline the unique
challenges encountered in aligning ethical values within them. To address these
challenges, we introduce a novel conceptual paradigm for aligning the ethical
values of big models and discuss promising research directions for alignment
criteria, evaluation, and method, representing an initial step towards the
interdisciplinary construction of the ethically aligned AI
&lt;/p&gt;
&lt;p&gt;This paper is a modified English version of our Chinese paper
https://crad.ict.ac.cn/cn/article/doi/10.7544/issn1000-1239.202330553, intended
to help non-Chinese native speakers better understand our work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1&quot;&gt;Xiaoyuan Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jing Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiting Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17552">
<title>Model-Based Runtime Monitoring with Interactive Imitation Learning. (arXiv:2310.17552v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2310.17552</link>
<description rdf:parseType="Literal">&lt;p&gt;Robot learning methods have recently made great strides, but generalization
and robustness challenges still hinder their widespread deployment. Failing to
detect and address potential failures renders state-of-the-art learning systems
not combat-ready for high-stakes tasks. Recent advances in interactive
imitation learning have presented a promising framework for human-robot
teaming, enabling the robots to operate safely and continually improve their
performances over long-term deployments. Nonetheless, existing methods
typically require constant human supervision and preemptive feedback, limiting
their practicality in realistic domains. This work aims to endow a robot with
the ability to monitor and detect errors during task execution. We introduce a
model-based runtime monitoring algorithm that learns from deployment data to
detect system anomalies and anticipate failures. Unlike prior work that cannot
foresee future failures or requires failure experiences for training, our
method learns a latent-space dynamics model and a failure classifier, enabling
our method to simulate future action outcomes and detect out-of-distribution
and high-risk states preemptively. We train our method within an interactive
imitation learning framework, where it continually updates the model from the
experiences of the human-robot team collected using trustworthy deployments.
Consequently, our method reduces the human workload needed over time while
ensuring reliable task execution. Our method outperforms the baselines across
system-level and unit-test metrics, with 23% and 40% higher success rates in
simulation and on physical hardware, respectively. More information at
https://ut-austin-rpl.github.io/sirius-runtime-monitor/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huihan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dass_S/0/1/0/all/0/1&quot;&gt;Shivin Dass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1&quot;&gt;Roberto Mart&amp;#xed;n-Mart&amp;#xed;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuke Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17555">
<title>Interactive Robot Learning from Verbal Correction. (arXiv:2310.17555v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2310.17555</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to learn and refine behavior after deployment has become ever
more important for robots as we design them to operate in unstructured
environments like households. In this work, we design a new learning system
based on large language model (LLM), OLAF, that allows everyday users to teach
a robot using verbal corrections when the robot makes mistakes, e.g., by saying
&quot;Stop what you&apos;re doing. You should move closer to the cup.&quot; A key feature of
OLAF is its ability to update the robot&apos;s visuomotor neural policy based on the
verbal feedback to avoid repeating mistakes in the future. This is in contrast
to existing LLM-based robotic systems, which only follow verbal commands or
corrections but not learn from them. We demonstrate the efficacy of our design
in experiments where a user teaches a robot to perform long-horizon
manipulation tasks both in simulation and on physical hardware, achieving on
average 20.0% improvement in policy success rate. Videos and more results are
at https://ut-austin-rpl.github.io/olaf/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huihan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Alice Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuke Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swaminathan_A/0/1/0/all/0/1&quot;&gt;Adith Swaminathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolobov_A/0/1/0/all/0/1&quot;&gt;Andrey Kolobov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Ching-An Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17559">
<title>Instability of computer vision models is a necessary result of the task itself. (arXiv:2310.17559v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17559</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples resulting from instability of current computer vision
models are an extremely important topic due to their potential to compromise
any application. In this paper we demonstrate that instability is inevitable
due to a) symmetries (translational invariance) of the data, b) the categorical
nature of the classification task, and c) the fundamental discrepancy of
classifying images as objects themselves. The issue is further exacerbated by
non-exhaustive labelling of the training data. Therefore we conclude that
instability is a necessary result of how the problem of computer vision is
currently formulated. While the problem cannot be eliminated, through the
analysis of the causes, we have arrived at ways how it can be partially
alleviated. These include i) increasing the resolution of images, ii) providing
contextual information for the image, iii) exhaustive labelling of training
data, and iv) preventing attackers from frequent access to the computer vision
system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turnbull_O/0/1/0/all/0/1&quot;&gt;Oliver Turnbull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cevora_G/0/1/0/all/0/1&quot;&gt;George Cevora&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17561">
<title>Bifurcations and loss jumps in RNN training. (arXiv:2310.17561v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17561</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent neural networks (RNNs) are popular machine learning tools for
modeling and forecasting sequential data and for inferring dynamical systems
(DS) from observed time series. Concepts from DS theory (DST) have variously
been used to further our understanding of both, how trained RNNs solve complex
tasks, and the training process itself. Bifurcations are particularly important
phenomena in DS, including RNNs, that refer to topological (qualitative)
changes in a system&apos;s dynamical behavior as one or more of its parameters are
varied. Knowing the bifurcation structure of an RNN will thus allow to deduce
many of its computational and dynamical properties, like its sensitivity to
parameter variations or its behavior during training. In particular,
bifurcations may account for sudden loss jumps observed in RNN training that
could severely impede the training process. Here we first mathematically prove
for a particular class of ReLU-based RNNs that certain bifurcations are indeed
associated with loss gradients tending toward infinity or zero. We then
introduce a novel heuristic algorithm for detecting all fixed points and
k-cycles in ReLU-based RNNs and their existence and stability regions, hence
bifurcation manifolds in parameter space. In contrast to previous numerical
algorithms for finding fixed points and common continuation methods, our
algorithm provides exact results and returns fixed points and cycles up to high
orders with surprisingly good scaling behavior. We exemplify the algorithm on
the analysis of the training process of RNNs, and find that the recently
introduced technique of generalized teacher forcing completely avoids certain
types of bifurcations in training. Thus, besides facilitating the DST analysis
of trained RNNs, our algorithm provides a powerful instrument for analyzing the
training process itself.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisenmann_L/0/1/0/all/0/1&quot;&gt;Lukas Eisenmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monfared_Z/0/1/0/all/0/1&quot;&gt;Zahra Monfared&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goring_N/0/1/0/all/0/1&quot;&gt;Niclas Alexander G&amp;#xf6;ring&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durstewitz_D/0/1/0/all/0/1&quot;&gt;Daniel Durstewitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2104.02206">
<title>Tuned Compositional Feature Replays for Efficient Stream Learning. (arXiv:2104.02206v7 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2104.02206</link>
<description rdf:parseType="Literal">&lt;p&gt;Our brains extract durable, generalizable knowledge from transient
experiences of the world. Artificial neural networks come nowhere close: when
tasked with learning to classify objects by training on non-repeating video
frames in temporal order (online stream learning), models that learn well from
shuffled datasets catastrophically forget old knowledge upon learning new
stimuli. We propose a new continual learning algorithm, Compositional Replay
Using Memory Blocks (CRUMB), which mitigates forgetting by replaying feature
maps reconstructed by recombining generic parts. CRUMB concatenates trainable
and re-usable &quot;memory block&quot; vectors to compositionally reconstruct feature map
tensors in convolutional neural networks, like crumbs forming a loaf of bread.
CRUMB stores the indices of memory blocks used to reconstruct new stimuli,
enabling replay of specific memories during later tasks. This reconstruction
mechanism also primes the neural network to minimize catastrophic forgetting by
forcing it to attend to information about object shapes more than information
about image textures, and stabilizes the network during stream learning by
providing a shared feature-level basis for all training examples. These
properties allow CRUMB to outperform an otherwise identical algorithm that
stores and replays raw images while occupying only 3.6% as much memory. We
stress-tested CRUMB alongside 13 competing methods on 7 challenging datasets.
To address the limited number of existing online stream learning datasets, we
introduce 2 new benchmarks by adapting existing datasets for stream learning.
With about 4% as much memory and 30% as much runtime, CRUMB mitigates
catastrophic forgetting more effectively than the prior state-of-the-art. Our
code is available on GitHub at https://github.com/MorganBDT/crumb.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talbot_M/0/1/0/all/0/1&quot;&gt;Morgan B. Talbot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zawar_R/0/1/0/all/0/1&quot;&gt;Rushikesh Zawar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badkundri_R/0/1/0/all/0/1&quot;&gt;Rohil Badkundri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengmi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1&quot;&gt;Gabriel Kreiman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.14251">
<title>Road Network Guided Fine-Grained Urban Traffic Flow Inference. (arXiv:2109.14251v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2109.14251</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate inference of fine-grained traffic flow from coarse-grained one is an
emerging yet crucial problem, which can help greatly reduce the number of the
required traffic monitoring sensors for cost savings. In this work, we notice
that traffic flow has a high correlation with road network, which was either
completely ignored or simply treated as an external factor in previous works.
To facilitate this problem, we propose a novel Road-Aware Traffic Flow
Magnifier (RATFM) that explicitly exploits the prior knowledge of road networks
to fully learn the road-aware spatial distribution of fine-grained traffic
flow. Specifically, a multi-directional 1D convolutional layer is first
introduced to extract the semantic feature of the road network. Subsequently,
we incorporate the road network feature and coarse-grained flow feature to
regularize the short-range spatial distribution modeling of road-relative
traffic flow. Furthermore, we take the road network feature as a query to
capture the long-range spatial distribution of traffic flow with a transformer
architecture. Benefiting from the road-aware inference mechanism, our method
can generate high-quality fine-grained traffic flow maps. Extensive experiments
on three real-world datasets show that the proposed RATFM outperforms
state-of-the-art models under various scenarios. Our code and datasets are
released at {\url{https://github.com/luimoli/RATFM}}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingbo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mengmeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Ziyi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Junfan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liang Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.01646">
<title>Investigating the usefulness of Quantum Blur. (arXiv:2112.01646v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2112.01646</link>
<description rdf:parseType="Literal">&lt;p&gt;Though some years remain before quantum computation can fully outperform
conventional computation, it already provides resources that can be used for
exploratory purposes in various fields. This includes certain tasks for
procedural generation in computer games, music and art. The so-called `Quantum
Blur&apos; method represents the first step on this journey, providing a simple
proof-of-principle example of how quantum software can be useful in these areas
today. Here we analyse the `Quantum Blur&apos; method and compare it to conventional
blur effects. This investigation was guided by discussions with the most
prominent user of the method, to determine which features were found most
useful. In particular we determine how these features depend on the quantum
phenomena of superposition and entanglement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wootton_J/0/1/0/all/0/1&quot;&gt;James R. Wootton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfaffhauser_M/0/1/0/all/0/1&quot;&gt;Marcel Pfaffhauser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.12458">
<title>Local Advantage Networks for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2112.12458v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2112.12458</link>
<description rdf:parseType="Literal">&lt;p&gt;Many recent successful off-policy multi-agent reinforcement learning (MARL)
algorithms for cooperative partially observable environments focus on finding
factorized value functions, leading to convoluted network structures. Building
on the structure of independent Q-learners, our LAN algorithm takes a radically
different approach, leveraging a dueling architecture to learn for each agent a
decentralized best-response policies via individual advantage functions. The
learning is stabilized by a centralized critic whose primary objective is to
reduce the moving target problem of the individual advantages. The critic,
whose network&apos;s size is independent of the number of agents, is cast aside
after learning. Evaluation on the StarCraft II multi-agent challenge benchmark
shows that LAN reaches state-of-the-art performance and is highly scalable with
respect to the number of agents, opening up a promising alternative direction
for MARL research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avalos_R/0/1/0/all/0/1&quot;&gt;Rapha&amp;#xeb;l Avalos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reymond_M/0/1/0/all/0/1&quot;&gt;Mathieu Reymond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowe_A/0/1/0/all/0/1&quot;&gt;Ann Now&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roijers_D/0/1/0/all/0/1&quot;&gt;Diederik M. Roijers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.11699">
<title>Semi-supervised Deep Multi-view Stereo. (arXiv:2207.11699v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.11699</link>
<description rdf:parseType="Literal">&lt;p&gt;Significant progress has been witnessed in learning-based Multi-view Stereo
(MVS) under supervised and unsupervised settings. To combine their respective
merits in accuracy and completeness, meantime reducing the demand for expensive
labeled data, this paper explores the problem of learning-based MVS in a
semi-supervised setting that only a tiny part of the MVS data is attached with
dense depth ground truth. However, due to huge variation of scenarios and
flexible settings in views, it may break the basic assumption in classic
semi-supervised learning, that unlabeled data and labeled data share the same
label space and data distribution, named as semi-supervised distribution-gap
ambiguity in the MVS problem. To handle these issues, we propose a novel
semi-supervised distribution-augmented MVS framework, namely SDA-MVS. For the
simple case that the basic assumption works in MVS data, consistency
regularization encourages the model predictions to be consistent between
original sample and randomly augmented sample. For further troublesome case
that the basic assumption is conflicted in MVS data, we propose a novel style
consistency loss to alleviate the negative effect caused by the distribution
gap. The visual style of unlabeled sample is transferred to labeled sample to
shrink the gap, and the model prediction of generated sample is further
supervised with the label in original labeled sample. The experimental results
in semi-supervised settings of multiple MVS datasets show the superior
performance of the proposed method. With the same settings in backbone network,
our proposed SDA-MVS outperforms its fully-supervised and unsupervised
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hongbin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weitao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1&quot;&gt;Haihong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Baigui Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xuansong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1&quot;&gt;Wenxiong Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.06589">
<title>Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks. (arXiv:2209.06589v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.06589</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) have become compelling models designed to
perform learning and inference on graph-structured data. However, little work
has been done to understand the fundamental limitations of GNNs for scaling to
larger graphs and generalizing to out-of-distribution (OOD) inputs. In this
paper, we use a random graph generator to systematically investigate how the
graph size and structural properties affect the predictive performance of GNNs.
We present specific evidence that the average node degree is a key feature in
determining whether GNNs can generalize to unseen graphs, and that the use of
multiple node update functions can improve the generalization performance of
GNNs when dealing with graphs of multimodal degree distributions. Accordingly,
we propose a multi-module GNN framework that allows the network to adapt
flexibly to new graphs by generalizing a single canonical nonlinear
transformation over aggregated inputs. Our results show that the multi-module
GNNs improve the OOD generalization on a variety of inference tasks in the
direction of diverse structural features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyungeun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1&quot;&gt;Kijung Yoon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.17218">
<title>Artificial intelligence in government: Concepts, standards, and a unified framework. (arXiv:2210.17218v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2210.17218</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in artificial intelligence (AI), especially in generative
language modelling, hold the promise of transforming government. Given the
advanced capabilities of new AI systems, it is critical that these are embedded
using standard operational procedures, clear epistemic criteria, and behave in
alignment with the normative expectations of society. Scholars in multiple
domains have subsequently begun to conceptualize the different forms that AI
applications may take, highlighting both their potential benefits and pitfalls.
However, the literature remains fragmented, with researchers in social science
disciplines like public administration and political science, and the
fast-moving fields of AI, ML, and robotics, all developing concepts in relative
isolation. Although there are calls to formalize the emerging study of AI in
government, a balanced account that captures the full depth of theoretical
perspectives needed to understand the consequences of embedding AI into a
public sector context is lacking. Here, we unify efforts across social and
technical disciplines by first conducting an integrative literature review to
identify and cluster 69 key terms that frequently co-occur in the
multidisciplinary study of AI. We then build on the results of this
bibliometric analysis to propose three new multifaceted concepts for
understanding and analysing AI-based systems for government (AI-GOV) in a more
unified way: (1) operational fitness, (2) epistemic alignment, and (3)
normative divergence. Finally, we put these concepts to work by using them as
dimensions in a conceptual typology of AI-GOV and connecting each with emerging
AI technical measurement standards to encourage operationalization, foster
cross-disciplinary dialogue, and stimulate debate among those aiming to rethink
government with AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Straub_V/0/1/0/all/0/1&quot;&gt;Vincent J. Straub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morgan_D/0/1/0/all/0/1&quot;&gt;Deborah Morgan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bright_J/0/1/0/all/0/1&quot;&gt;Jonathan Bright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Margetts_H/0/1/0/all/0/1&quot;&gt;Helen Margetts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01488">
<title>Event knowledge in large language models: the gap between the impossible and the unlikely. (arXiv:2212.01488v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01488</link>
<description rdf:parseType="Literal">&lt;p&gt;Word co-occurrence patterns in language corpora contain a surprising amount
of conceptual knowledge. Large language models (LLMs), trained to predict words
in context, leverage these patterns to achieve impressive performance on
diverse semantic tasks requiring world knowledge. An important but understudied
question about LLMs&apos; semantic abilities is whether they acquire generalized
knowledge of common events. Here, we test whether five pre-trained LLMs (from
2018&apos;s BERT to 2023&apos;s MPT) assign higher likelihood to plausible descriptions
of agent-patient interactions than to minimally different implausible versions
of the same event. Using three curated sets of minimal sentence pairs (total
n=1,215), we found that pre-trained LLMs possess substantial event knowledge,
outperforming other distributional language models. In particular, they almost
always assign higher likelihood to possible vs. impossible events (The teacher
bought the laptop vs. The laptop bought the teacher). However, LLMs show less
consistent preferences for likely vs. unlikely events (The nanny tutored the
boy vs. The boy tutored the nanny). In follow-up analyses, we show that (i) LLM
scores are driven by both plausibility and surface-level sentence features,
(ii) LLM scores generalize well across syntactic variants (active vs. passive
constructions) but less well across semantic variants (synonymous sentences),
(iii) some LLM errors mirror human judgment ambiguity, and (iv) sentence
plausibility serves as an organizing dimension in internal LLM representations.
Overall, our results show that important aspects of event knowledge naturally
emerge from distributional linguistic patterns, but also highlight a gap
between representations of possible/impossible and likely/unlikely events.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kauf_C/0/1/0/all/0/1&quot;&gt;Carina Kauf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ivanova_A/0/1/0/all/0/1&quot;&gt;Anna A. Ivanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rambelli_G/0/1/0/all/0/1&quot;&gt;Giulia Rambelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chersoni_E/0/1/0/all/0/1&quot;&gt;Emmanuele Chersoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+She_J/0/1/0/all/0/1&quot;&gt;Jingyuan Selena She&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_Z/0/1/0/all/0/1&quot;&gt;Zawad Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fedorenko_E/0/1/0/all/0/1&quot;&gt;Evelina Fedorenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lenci_A/0/1/0/all/0/1&quot;&gt;Alessandro Lenci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10813">
<title>Increasing Fairness via Combination with Learning Guarantees. (arXiv:2301.10813v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10813</link>
<description rdf:parseType="Literal">&lt;p&gt;The concern about underlying discrimination hidden in machine learning (ML)
models is increasing, as ML systems have been widely applied in more and more
real-world scenarios and any discrimination hidden in them will directly affect
human life. Many techniques have been developed to enhance fairness including
commonly-used group fairness measures and several fairness-aware methods
combining ensemble learning. However, existing fairness measures can only focus
on one aspect -- either group or individual fairness, and the hard
compatibility among them indicates a possibility of remaining biases even if
one of them is satisfied. Moreover, existing mechanisms to boost fairness
usually present empirical results to show validity, yet few of them discuss
whether fairness can be boosted with certain theoretical guarantees. To address
these issues, we propose a fairness quality measure named discriminative risk
to reflect both individual and group fairness aspects. Furthermore, we
investigate the properties of the proposed measure and propose first- and
second-order oracle bounds to show that fairness can be boosted via ensemble
combination with theoretical learning guarantees. The analysis is suitable for
both binary and multi-class classification. A pruning method is also proposed
to utilise our proposed measure and comprehensive experiments are conducted to
evaluate the effectiveness of the proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1&quot;&gt;Yijun Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_A/0/1/0/all/0/1&quot;&gt;Anqi Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1&quot;&gt;Nanguang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.12593">
<title>Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning. (arXiv:2301.12593v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.12593</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real-world domains require safe decision making in uncertain
environments. In this work, we introduce a deep reinforcement learning
framework for approaching this important problem. We consider a distribution
over transition models, and apply a risk-averse perspective towards model
uncertainty through the use of coherent distortion risk measures. We provide
robustness guarantees for this framework by showing it is equivalent to a
specific class of distributionally robust safe reinforcement learning problems.
Unlike existing approaches to robustness in deep reinforcement learning,
however, our formulation does not involve minimax optimization. This leads to
an efficient, model-free implementation of our approach that only requires
standard data collection from a single training environment. In experiments on
continuous control tasks with safety constraints, we demonstrate that our
framework produces robust performance and safety at deployment time across a
range of perturbed test environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Queeney_J/0/1/0/all/0/1&quot;&gt;James Queeney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benosman_M/0/1/0/all/0/1&quot;&gt;Mouhacine Benosman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.02209">
<title>A Theory of Link Prediction via Relational Weisfeiler-Leman on Knowledge Graphs. (arXiv:2302.02209v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.02209</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks are prominent models for representation learning over
graph-structured data. While the capabilities and limitations of these models
are well-understood for simple graphs, our understanding remains incomplete in
the context of knowledge graphs. Our goal is to provide a systematic
understanding of the landscape of graph neural networks for knowledge graphs
pertaining to the prominent task of link prediction. Our analysis entails a
unifying perspective on seemingly unrelated models and unlocks a series of
other models. The expressive power of various models is characterized via a
corresponding relational Weisfeiler-Leman algorithm. This analysis is extended
to provide a precise logical characterization of the class of functions
captured by a class of graph neural networks. The theoretical findings
presented in this paper explain the benefits of some widely employed practical
design choices, which are validated empirically.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xingyue Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orth_M/0/1/0/all/0/1&quot;&gt;Miguel Romero Orth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ceylan_I/0/1/0/all/0/1&quot;&gt;&amp;#x130;smail &amp;#x130;lkan Ceylan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barcelo_P/0/1/0/all/0/1&quot;&gt;Pablo Barcel&amp;#xf3;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.04449">
<title>Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals. (arXiv:2302.04449v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.04449</link>
<description rdf:parseType="Literal">&lt;p&gt;High sample complexity has long been a challenge for RL. On the other hand,
humans learn to perform tasks not only from interaction or demonstrations, but
also by reading unstructured text documents, e.g., instruction manuals.
Instruction manuals and wiki pages are among the most abundant data that could
inform agents of valuable features and policies or task-specific environmental
dynamics and reward structures. Therefore, we hypothesize that the ability to
utilize human-written instruction manuals to assist learning policies for
specific tasks should lead to a more efficient and better-performing agent. We
propose the Read and Reward framework. Read and Reward speeds up RL algorithms
on Atari games by reading manuals released by the Atari game developers. Our
framework consists of a QA Extraction module that extracts and summarizes
relevant information from the manual and a Reasoning module that evaluates
object-agent interactions based on information from the manual. An auxiliary
reward is then provided to a standard A2C RL agent, when interaction is
detected. Experimentally, various RL algorithms obtain significant improvement
in performance and training speed when assisted by our design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yewen Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azaria_A/0/1/0/all/0/1&quot;&gt;Amos Azaria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1&quot;&gt;Tom M. Mitchell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06961">
<title>DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical Awareness for Robust Fovea Localization. (arXiv:2302.06961v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06961</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate fovea localization is essential for analyzing retinal diseases to
prevent irreversible vision loss. While current deep learning-based methods
outperform traditional ones, they still face challenges such as the lack of
local anatomical landmarks around the fovea, the inability to robustly handle
diseased retinal images, and the variations in image conditions. In this paper,
we propose a novel transformer-based architecture called DualStreamFoveaNet
(DSFN) for multi-cue fusion. This architecture explicitly incorporates
long-range connections and global features using retina and vessel
distributions for robust fovea localization. We introduce a spatial attention
mechanism in the dual-stream encoder to extract and fuse self-learned
anatomical information, focusing more on features distributed along blood
vessels and significantly reducing computational costs by decreasing token
numbers. Our extensive experiments show that the proposed architecture achieves
state-of-the-art performance on two public datasets and one large-scale private
dataset. Furthermore, we demonstrate that the DSFN is more robust on both
normal and diseased retina images and has better generalization capacity in
cross-dataset experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Sifan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zilong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shaopeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jionglong Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xiaowei Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_K/0/1/0/all/0/1&quot;&gt;Kang Dang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00905">
<title>Open-World Object Manipulation using Pre-trained Vision-Language Models. (arXiv:2303.00905v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00905</link>
<description rdf:parseType="Literal">&lt;p&gt;For robots to follow instructions from people, they must be able to connect
the rich semantic information in human vocabulary, e.g. &quot;can you get me the
pink stuffed whale?&quot; to their sensory observations and actions. This brings up
a notably difficult challenge for robots: while robot learning approaches allow
robots to learn many different behaviors from first-hand experience, it is
impractical for robots to have first-hand experiences that span all of this
semantic information. We would like a robot&apos;s policy to be able to perceive and
pick up the pink stuffed whale, even if it has never seen any data interacting
with a stuffed whale before. Fortunately, static data on the internet has vast
semantic information, and this information is captured in pre-trained
vision-language models. In this paper, we study whether we can interface robot
policies with these pre-trained models, with the aim of allowing robots to
complete instructions involving object categories that the robot has never seen
first-hand. We develop a simple approach, which we call Manipulation of
Open-World Objects (MOO), which leverages a pre-trained vision-language model
to extract object-identifying information from the language command and image,
and conditions the robot policy on the current image, the instruction, and the
extracted object information. In a variety of experiments on a real mobile
manipulator, we find that MOO generalizes zero-shot to a wide range of novel
object categories and environments. In addition, we show how MOO generalizes to
other, non-language-based input modalities to specify the object of interest
such as finger pointing, and how it can be further extended to enable
open-world navigation and manipulation. The project&apos;s website and evaluation
videos can be found at https://robot-moo.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_A/0/1/0/all/0/1&quot;&gt;Austin Stone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Ted Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1&quot;&gt;Keerthana Gopalakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kuang-Huei Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuong_Q/0/1/0/all/0/1&quot;&gt;Quan Vuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wohlhart_P/0/1/0/all/0/1&quot;&gt;Paul Wohlhart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirmani_S/0/1/0/all/0/1&quot;&gt;Sean Kirmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zitkovich_B/0/1/0/all/0/1&quot;&gt;Brianna Zitkovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1&quot;&gt;Karol Hausman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03284">
<title>The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models. (arXiv:2303.03284v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03284</link>
<description rdf:parseType="Literal">&lt;p&gt;Partially Observable Markov Decision Processes (POMDPs) are used to model
environments where the full state cannot be perceived by an agent. As such the
agent needs to reason taking into account the past observations and actions.
However, simply remembering the full history is generally intractable due to
the exponential growth in the history space. Maintaining a probability
distribution that models the belief over what the true state is can be used as
a sufficient statistic of the history, but its computation requires access to
the model of the environment and is often intractable. While SOTA algorithms
use Recurrent Neural Networks to compress the observation-action history aiming
to learn a sufficient statistic, they lack guarantees of success and can lead
to sub-optimal policies. To overcome this, we propose the Wasserstein Belief
Updater, an RL algorithm that learns a latent model of the POMDP and an
approximation of the belief update. Our approach comes with theoretical
guarantees on the quality of our approximation ensuring that our outputted
beliefs allow for learning the optimal value function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avalos_R/0/1/0/all/0/1&quot;&gt;Raphael Avalos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delgrange_F/0/1/0/all/0/1&quot;&gt;Florent Delgrange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowe_A/0/1/0/all/0/1&quot;&gt;Ann Now&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1&quot;&gt;Guillermo A. P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roijers_D/0/1/0/all/0/1&quot;&gt;Diederik M. Roijers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11249">
<title>What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement. (arXiv:2303.11249v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11249</link>
<description rdf:parseType="Literal">&lt;p&gt;The question of what makes a data distribution suitable for deep learning is
a fundamental open problem. Focusing on locally connected neural networks (a
prevalent family of architectures that includes convolutional and recurrent
neural networks as well as local self-attention models), we address this
problem by adopting theoretical tools from quantum physics. Our main
theoretical result states that a certain locally connected neural network is
capable of accurate prediction over a data distribution if and only if the data
distribution admits low quantum entanglement under certain canonical partitions
of features. As a practical application of this result, we derive a
preprocessing method for enhancing the suitability of a data distribution to
locally connected neural networks. Experiments with widespread models over
various datasets demonstrate our findings. We hope that our use of quantum
entanglement will encourage further adoption of tools from physics for formally
reasoning about the relation between deep learning and real-world data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexander_Y/0/1/0/all/0/1&quot;&gt;Yotam Alexander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vega_N/0/1/0/all/0/1&quot;&gt;Nimrod De La Vega&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razin_N/0/1/0/all/0/1&quot;&gt;Noam Razin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1&quot;&gt;Nadav Cohen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11888">
<title>Penalty-Based Imitation Learning With Cross Semantics Generation Sensor Fusion for Autonomous Driving. (arXiv:2303.11888v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11888</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent times, there has been a growing focus on end-to-end autonomous
driving technologies. This technology involves the replacement of the entire
driving pipeline with a single neural network, which has a simpler structure
and faster inference time. However, while this approach reduces the number of
components in the driving pipeline, it also presents challenges related to
interpretability and safety. For instance, the trained policy may not always
comply with traffic rules, and it is difficult to determine the reason for such
misbehavior due to the lack of intermediate outputs. Additionally, the
successful implementation of autonomous driving technology heavily depends on
the reliable and expedient processing of sensory data to accurately perceive
the surrounding environment. In this paper, we provide penalty-based imitation
learning approach combined with cross semantics generation sensor fusion
technologies (P-CSG) to efficiently integrate multiple modalities of
information and enable the autonomous agent to effectively adhere to traffic
regulations. Our model undergoes evaluation within the Town 05 Long benchmark,
where we observe a remarkable increase in the driving score by more than 12%
when compared to the state-of-the-art (SOTA) model, InterFuser. Notably, our
model achieves this performance enhancement while achieving a 7-fold increase
in inference speed and reducing the model size by approximately 30%. For more
detailed information, including code-based resources, they can be found at
https://hk-zh.github.io/p-csg/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hongkuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_A/0/1/0/all/0/1&quot;&gt;Aifen Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1&quot;&gt;Letian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yinxian Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13497">
<title>TriPlaneNet: An Encoder for EG3D Inversion. (arXiv:2303.13497v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13497</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent progress in NeRF-based GANs has introduced a number of approaches for
high-resolution and high-fidelity generative modeling of human heads with a
possibility for novel view rendering. At the same time, one must solve an
inverse problem to be able to re-render or modify an existing image or video.
Despite the success of universal optimization-based methods for 2D GAN
inversion, those applied to 3D GANs may fail to extrapolate the result onto the
novel view, whereas optimization-based 3D GAN inversion methods are
time-consuming and can require at least several minutes per image. Fast
encoder-based techniques, such as those developed for StyleGAN, may also be
less appealing due to the lack of identity preservation. Our work introduces a
fast technique that bridges the gap between the two approaches by directly
utilizing the tri-plane representation presented for the EG3D generative model.
In particular, we build upon a feed-forward convolutional encoder for the
latent code and extend it with a fully-convolutional predictor of tri-plane
numerical offsets. The renderings are similar in quality to the ones produced
by optimization-based techniques and outperform the ones by encoder-based
methods. As we empirically prove, this is a consequence of directly operating
in the tri-plane space, not in the GAN parameter space, while making use of an
encoder-based trainable approach. Finally, we demonstrate significantly more
correct embedding of a face image in 3D than for all the baselines, further
strengthened by a probably symmetric prior enabled during training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattarai_A/0/1/0/all/0/1&quot;&gt;Ananta R. Bhattarai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1&quot;&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sevastopolsky_A/0/1/0/all/0/1&quot;&gt;Artem Sevastopolsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15027">
<title>A Survey on Causal Discovery Methods for I.I.D. and Time Series Data. (arXiv:2303.15027v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15027</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to understand causality from data is one of the major milestones
of human-level intelligence. Causal Discovery (CD) algorithms can identify the
cause-effect relationships among the variables of a system from related
observational data with certain assumptions. Over the years, several methods
have been developed primarily based on the statistical properties of data to
uncover the underlying causal mechanism. In this study, we present an extensive
discussion on the methods designed to perform causal discovery from both
independent and identically distributed (I.I.D.) data and time series data. For
this purpose, we first introduce the common terminologies used in causal
discovery literature and then provide a comprehensive discussion of the
algorithms designed to identify causal relations in different settings. We
further discuss some of the benchmark datasets available for evaluating the
algorithmic performance, off-the-shelf tools or software packages to perform
causal discovery readily, and the common metrics used to evaluate these
methods. We also evaluate some widely used causal discovery algorithms on
multiple benchmark datasets and compare their performances. Finally, we
conclude by discussing the research challenges and the applications of causal
discovery algorithms in multiple areas of interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_U/0/1/0/all/0/1&quot;&gt;Uzma Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1&quot;&gt;Emam Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gani_M/0/1/0/all/0/1&quot;&gt;Md Osman Gani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01091">
<title>Changes to Captions: An Attentive Network for Remote Sensing Change Captioning. (arXiv:2304.01091v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01091</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, advanced research has focused on the direct learning and
analysis of remote sensing images using natural language processing (NLP)
techniques. The ability to accurately describe changes occurring in
multi-temporal remote sensing images is becoming increasingly important for
geospatial understanding and land planning. Unlike natural image change
captioning tasks, remote sensing change captioning aims to capture the most
significant changes, irrespective of various influential factors such as
illumination, seasonal effects, and complex land covers. In this study, we
highlight the significance of accurately describing changes in remote sensing
images and present a comparison of the change captioning task for natural and
synthetic images and remote sensing images. To address the challenge of
generating accurate captions, we propose an attentive changes-to-captions
network, called Chg2Cap for short, for bi-temporal remote sensing images. The
network comprises three main components: 1) a Siamese CNN-based feature
extractor to collect high-level representations for each image pair; 2) an
attentive decoder that includes a hierarchical self-attention block to locate
change-related features and a residual block to generate the image embedding;
and 3) a transformer-based caption generator to decode the relationship between
the image embedding and the word embedding into a description. The proposed
Chg2Cap network is evaluated on two representative remote sensing datasets, and
a comprehensive experimental analysis is provided. The code and pre-trained
models will be available online at https://github.com/ShizhenChang/Chg2Cap.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shizhen Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghamisi_P/0/1/0/all/0/1&quot;&gt;Pedram Ghamisi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11411">
<title>Detecting Spoilers in Movie Reviews with External Movie Knowledge and User Networks. (arXiv:2304.11411v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2304.11411</link>
<description rdf:parseType="Literal">&lt;p&gt;Online movie review platforms are providing crowdsourced feedback for the
film industry and the general public, while spoiler reviews greatly compromise
user experience. Although preliminary research efforts were made to
automatically identify spoilers, they merely focus on the review content
itself, while robust spoiler detection requires putting the review into the
context of facts and knowledge regarding movies, user behavior on film review
platforms, and more. In light of these challenges, we first curate a
large-scale network-based spoiler detection dataset LCS and a comprehensive and
up-to-date movie knowledge base UKM. We then propose MVSD, a novel Multi-View
Spoiler Detection framework that takes into account the external knowledge
about movies and user activities on movie review platforms. Specifically, MVSD
constructs three interconnecting heterogeneous information networks to model
diverse data sources and their multi-view attributes, while we design and
employ a novel heterogeneous graph neural network architecture for spoiler
detection as node-level classification. Extensive experiments demonstrate that
MVSD advances the state-of-the-art on two spoiler detection datasets, while the
introduction of external knowledge and user interactions help ground robust
spoiler detection. Our data and code are available at
https://github.com/Arthur-Heng/Spoiler-Detection
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Heng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenqian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yuyang Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zhaoxuan Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shangbin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qinghua Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1&quot;&gt;Minnan Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.00633">
<title>Self-Evaluation Guided Beam Search for Reasoning. (arXiv:2305.00633v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.00633</link>
<description rdf:parseType="Literal">&lt;p&gt;Breaking down a problem into intermediate steps has demonstrated impressive
performance in Large Language Model (LLM) reasoning. However, the growth of the
reasoning chain introduces uncertainty and error accumulation, making it
challenging to elicit accurate final results. To tackle this challenge of
uncertainty in multi-step reasoning, we introduce a stepwise self-evaluation
mechanism to guide and calibrate the reasoning process of LLMs. We propose a
decoding algorithm integrating the self-evaluation guidance via stochastic beam
search. The self-evaluation guidance serves as a better-calibrated automatic
criterion, facilitating an efficient search in the reasoning space and
resulting in superior prediction quality. Stochastic beam search balances
exploitation and exploration of the search space with temperature-controlled
randomness. Our approach surpasses the corresponding Codex-backboned baselines
in few-shot accuracy by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQuA,
and StrategyQA benchmarks, respectively. Experiment results with Llama-2 on
arithmetic reasoning demonstrate the efficiency of our method in outperforming
the baseline methods with comparable computational budgets. Further analysis in
multi-step reasoning finds our self-evaluation guidance pinpoints logic
failures and leads to higher consistency and robustness. Our code is publicly
available at https://guideddecoding.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yuxi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1&quot;&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yiran Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1&quot;&gt;Min-Yen Kan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junxian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Qizhe Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02531">
<title>Can LLMs Capture Intertemporal Preferences?. (arXiv:2305.02531v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02531</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the viability of Large Language Models (LLMs), specifically
OpenAI&apos;s GPT-3.5 and GPT-4, in emulating human survey respondents and eliciting
preferences, with a focus on intertemporal choices. Leveraging the extensive
literature on intertemporal discounting for benchmarking, we examine responses
from LLMs across various languages and compare them to human responses,
exploring preferences between smaller, sooner, and larger, later rewards. Our
findings reveal that both GPT models demonstrate less patience than humans,
with GPT-3.5 exhibiting a lexicographic preference for earlier rewards, unlike
human decision-makers. Though GPT-4 does not display lexicographic preferences,
its measured discount rates are still considerably larger than those found in
humans. Interestingly, GPT models show greater patience in languages with weak
future tense references, such as German and Mandarin, aligning with existing
literature that suggests a correlation between language structure and
intertemporal preferences. We demonstrate how prompting GPT to explain its
decisions, a procedure we term ``chain-of-thought conjoint,&quot; can mitigate, but
does not eliminate, discrepancies between LLM and human responses. While
directly eliciting preferences using LLMs may yield misleading results,
combining chain-of-thought conjoint with topic modeling aids in hypothesis
generation, enabling researchers to explore the underpinnings of preferences.
Chain-of-thought conjoint provides a structured framework for marketers to use
LLMs to identify potential attributes or factors that can explain preference
heterogeneity across different customers and contexts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goli_A/0/1/0/all/0/1&quot;&gt;Ali Goli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Amandeep Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03598">
<title>NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03598</link>
<description rdf:parseType="Literal">&lt;p&gt;How can we interpret and retrieve medical evidence to support clinical
decisions? Clinical trial reports (CTR) amassed over the years contain
indispensable information for the development of personalized medicine.
However, it is practically infeasible to manually inspect over 400,000+
clinical trial reports in order to find the best evidence for experimental
treatments. Natural Language Inference (NLI) offers a potential solution to
this problem, by allowing the scalable computation of textual entailment.
However, existing NLI models perform poorly on biomedical corpora, and
previously published datasets fail to capture the full complexity of inference
over CTRs. In this work, we present a novel resource to advance research on NLI
for reasoning on CTRs. The resource includes two main tasks. Firstly, to
determine the inference relation between a natural language statement, and a
CTR. Secondly, to retrieve supporting facts to justify the predicted relation.
We provide NLI4CT, a corpus of 2400 statements and CTRs, annotated for these
tasks. Baselines on this corpus expose the limitations of existing NLI models,
with 6 state-of-the-art NLI models achieving a maximum F1 score of 0.627. To
the best of our knowledge, we are the first to design a task that covers the
interpretation of full CTRs. To encourage further work on this challenging
dataset, we make the corpus, competition leaderboard, website and code to
replicate the baseline experiments available at:
https://github.com/ai-systems/nli4ct
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jullien_M/0/1/0/all/0/1&quot;&gt;Ma&amp;#xeb;l Jullien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1&quot;&gt;Marco Valentino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frost_H/0/1/0/all/0/1&quot;&gt;Hannah Frost&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ORegan_P/0/1/0/all/0/1&quot;&gt;Paul O&amp;#x27;Regan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1&quot;&gt;Donal Landers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Freitas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04798">
<title>Multi-grained Hypergraph Interest Modeling for Conversational Recommendation. (arXiv:2305.04798v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04798</link>
<description rdf:parseType="Literal">&lt;p&gt;Conversational recommender system (CRS) interacts with users through
multi-turn dialogues in natural language, which aims to provide high-quality
recommendations for user&apos;s instant information need. Although great efforts
have been made to develop effective CRS, most of them still focus on the
contextual information from the current dialogue, usually suffering from the
data scarcity issue. Therefore, we consider leveraging historical dialogue data
to enrich the limited contexts of the current dialogue session.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a novel multi-grained hypergraph interest modeling
approach to capture user interest beneath intricate historical data from
different perspectives. As the core idea, we employ hypergraph to represent
complicated semantic relations underlying historical dialogues. In our
approach, we first employ the hypergraph structure to model users&apos; historical
dialogue sessions and form a session-based hypergraph, which captures
coarse-grained, session-level relations. Second, to alleviate the issue of data
scarcity, we use an external knowledge graph and construct a knowledge-based
hypergraph considering fine-grained, entity-level semantics. We further conduct
multi-grained hypergraph convolution on the two kinds of hypergraphs, and
utilize the enhanced representations to develop interest-aware CRS. Extensive
experiments on two benchmarks ReDial and TG-ReDial validate the effectiveness
of our approach on both recommendation and conversation tasks. Code is
available at: https://github.com/RUCAIBox/MHIM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_C/0/1/0/all/0/1&quot;&gt;Chenzhan Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1&quot;&gt;Yupeng Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yaliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13507">
<title>Multimodal Automated Fact-Checking: A Survey. (arXiv:2305.13507v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13507</link>
<description rdf:parseType="Literal">&lt;p&gt;Misinformation is often conveyed in multiple modalities, e.g. a miscaptioned
image. Multimodal misinformation is perceived as more credible by humans, and
spreads faster than its text-only counterparts. While an increasing body of
research investigates automated fact-checking (AFC), previous surveys mostly
focus on text. In this survey, we conceptualise a framework for AFC including
subtasks unique to multimodal misinformation. Furthermore, we discuss related
terms used in different communities and map them to our framework. We focus on
four modalities prevalent in real-world fact-checking: text, image, audio, and
video. We survey benchmarks and models, and discuss limitations and promising
directions for future research
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1&quot;&gt;Mubashara Akhtar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1&quot;&gt;Michael Schlichtkrull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhijiang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cocarascu_O/0/1/0/all/0/1&quot;&gt;Oana Cocarascu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simperl_E/0/1/0/all/0/1&quot;&gt;Elena Simperl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1&quot;&gt;Andreas Vlachos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13552">
<title>Squared Neural Families: A New Class of Tractable Density Models. (arXiv:2305.13552v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13552</link>
<description rdf:parseType="Literal">&lt;p&gt;Flexible models for probability distributions are an essential ingredient in
many machine learning tasks. We develop and investigate a new class of
probability distributions, which we call a Squared Neural Family (SNEFY),
formed by squaring the 2-norm of a neural network and normalising it with
respect to a base measure. Following the reasoning similar to the well
established connections between infinitely wide neural networks and Gaussian
processes, we show that SNEFYs admit closed form normalising constants in many
cases of interest, thereby resulting in flexible yet fully tractable density
models. SNEFYs strictly generalise classical exponential families, are closed
under conditioning, and have tractable marginal distributions. Their utility is
illustrated on a variety of density estimation, conditional density estimation,
and density estimation with missing data tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsuchida_R/0/1/0/all/0/1&quot;&gt;Russell Tsuchida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_C/0/1/0/all/0/1&quot;&gt;Cheng Soon Ong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sejdinovic_D/0/1/0/all/0/1&quot;&gt;Dino Sejdinovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13632">
<title>Detecting and Mitigating Hallucinations in Multilingual Summarisation. (arXiv:2305.13632v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13632</link>
<description rdf:parseType="Literal">&lt;p&gt;Hallucinations pose a significant challenge to the reliability of neural
models for abstractive summarisation. While automatically generated summaries
may be fluent, they often lack faithfulness to the original document. This
issue becomes even more pronounced in low-resource settings, such as
cross-lingual transfer. With the existing faithful metrics focusing on English,
even measuring the extent of this phenomenon in cross-lingual settings is hard.
To address this, we first develop a novel metric, mFACT, evaluating the
faithfulness of non-English summaries, leveraging translation-based transfer
from multiple English faithfulness metrics. We then propose a simple but
effective method to reduce hallucinations with a cross-lingual transfer, which
weighs the loss of each training example by its faithfulness score. Through
extensive experiments in multiple languages, we demonstrate that mFACT is the
metric that is most suited to detect hallucinations. Moreover, we find that our
proposed loss weighting method drastically increases both performance and
faithfulness according to both automatic and human evaluation when compared to
strong baselines for cross-lingual transfer such as MAD-X. Our code and dataset
are available at https://github.com/yfqiu-nlp/mfact-summ.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1&quot;&gt;Yifu Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziser_Y/0/1/0/all/0/1&quot;&gt;Yftah Ziser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1&quot;&gt;Anna Korhonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1&quot;&gt;Edoardo M. Ponti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1&quot;&gt;Shay B. Cohen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13850">
<title>Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document. (arXiv:2305.13850v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13850</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual Relation Extraction (VRE) is a powerful means of discovering
relationships between entities within visually-rich documents. Existing methods
often focus on manipulating entity features to find pairwise relations, yet
neglect the more fundamental structural information that links disparate entity
pairs together. The absence of global structure information may make the model
struggle to learn long-range relations and easily predict conflicted results.
To alleviate such limitations, we propose a \textbf{G}l\textbf{O}bal
\textbf{S}tructure knowledge-guided relation \textbf{E}xtraction
(\textbf{\model}) framework. {\model} initiates by generating preliminary
relation predictions on entity pairs extracted from a scanned image of the
document. Subsequently, global structural knowledge is captured from the
preceding iterative predictions, which are then incorporated into the
representations of the entities. This ``generate-capture-incorporate&apos;&apos; cycle is
repeated multiple times, allowing entity representations and global structure
knowledge to be mutually reinforced. Extensive experiments validate that
{\model} not only outperforms existing methods in the standard fine-tuning
setting but also reveals superior cross-lingual learning capabilities; indeed,
even yields stronger data-efficient performance in the low-resource setting.
The code for GOSE will be available at https://github.com/chenxn2020/GOSE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiangnan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Q/0/1/0/all/0/1&quot;&gt;Qian Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Juncheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1&quot;&gt;Duo Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaozhong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Siliang Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14327">
<title>Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation. (arXiv:2305.14327v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14327</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction tuning has emerged to enhance the capabilities of large language
models (LLMs) to comprehend instructions and generate appropriate responses.
Existing methods either manually annotate or employ LLM (e.g., GPT-series) to
generate data for instruction tuning. However, they often overlook associating
instructions with existing annotated datasets. In this paper, we propose
Dynosaur, a dynamic growth paradigm for the automatic curation of
instruction-tuning data. Based on the metadata of existing datasets, we use
LLMs to automatically construct instruction-tuning data by identifying relevant
data fields and generating appropriate instructions.
&lt;/p&gt;
&lt;p&gt;By leveraging the existing annotated datasets, Dynosaur offers several
advantages: 1) it reduces the API cost for generating instructions (e.g., it
costs less than $12 USD by calling GPT-3.5-turbo for generating 800K
instruction tuning samples; 2) it provides high-quality data for instruction
tuning (e.g., it performs better than Alpaca and Flan on Super-NI and Longform
with comparable data sizes); and 3) it supports the continuous improvement of
models by generating instruction-tuning data when a new annotated dataset
becomes available. We further investigate a continual learning scheme for
learning with the ever-growing instruction-tuning dataset, and demonstrate that
replaying tasks with diverse instruction embeddings not only helps mitigate
forgetting issues but generalizes to unseen tasks better.
&lt;/p&gt;
&lt;p&gt;Code and data are available at https://github.com/WadeYin9712/Dynosaur.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Da Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1&quot;&gt;Fan Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1&quot;&gt;Ming Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1&quot;&gt;Hritik Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiawei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14331">
<title>What Else Do I Need to Know? The Effect of Background Information on Users&apos; Reliance on QA Systems. (arXiv:2305.14331v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14331</link>
<description rdf:parseType="Literal">&lt;p&gt;NLP systems have shown impressive performance at answering questions by
retrieving relevant context. However, with the increasingly large models, it is
impossible and often undesirable to constrain models&apos; knowledge or reasoning to
only the retrieved context. This leads to a mismatch between the information
that the models access to derive the answer and the information that is
available to the user to assess the model predicted answer. In this work, we
study how users interact with QA systems in the absence of sufficient
information to assess their predictions. Further, we ask whether adding the
requisite background helps mitigate users&apos; over-reliance on predictions. Our
study reveals that users rely on model predictions even in the absence of
sufficient information needed to assess the model&apos;s correctness. Providing the
relevant background, however, helps users better catch model errors, reducing
over-reliance on incorrect predictions. On the flip side, background
information also increases users&apos; confidence in their accurate as well as
inaccurate judgments. Our work highlights that supporting users&apos; verification
of QA predictions is an important, yet challenging, problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1&quot;&gt;Navita Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Briakou_E/0/1/0/all/0/1&quot;&gt;Eleftheria Briakou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Amanda Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baumler_C/0/1/0/all/0/1&quot;&gt;Connor Baumler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonial_C/0/1/0/all/0/1&quot;&gt;Claire Bonial&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Micher_J/0/1/0/all/0/1&quot;&gt;Jeffrey Micher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voss_C/0/1/0/all/0/1&quot;&gt;Clare R. Voss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carpuat_M/0/1/0/all/0/1&quot;&gt;Marine Carpuat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daume_H/0/1/0/all/0/1&quot;&gt;Hal Daum&amp;#xe9; III&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14858">
<title>Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers. (arXiv:2305.14858v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14858</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have achieved great success in machine learning applications.
Normalization techniques, such as Layer Normalization (LayerNorm, LN) and Root
Mean Square Normalization (RMSNorm), play a critical role in accelerating and
stabilizing the training of Transformers. While LayerNorm recenters and
rescales input vectors, RMSNorm only rescales the vectors by their RMS value.
Despite being more computationally efficient, RMSNorm may compromise the
representation ability of Transformers. There is currently no consensus
regarding the preferred normalization technique, as some models employ
LayerNorm while others utilize RMSNorm, especially in recent large language
models. It is challenging to convert Transformers with one normalization to the
other type. While there is an ongoing disagreement between the two
normalization types, we propose a solution to unify two mainstream Transformer
architectures, Pre-LN and Pre-RMSNorm Transformers. By removing the inherent
redundant mean information in the main branch of Pre-LN Transformers, we can
reduce LayerNorm to RMSNorm, achieving higher efficiency. We further propose
the Compressed RMSNorm (CRMSNorm) and Pre-CRMSNorm Transformer based on a
lossless compression of the zero-mean vectors. We formally establish the
equivalence of Pre-LN, Pre-RMSNorm, and Pre-CRMSNorm Transformer variants in
both training and inference. It implies that Pre-LN Transformers can be
substituted with Pre-(C)RMSNorm counterparts at almost no cost, offering the
same arithmetic functionality along with free efficiency improvement.
Experiments demonstrate that we can reduce the training and inference time of
Pre-LN Transformers by 1% - 10%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zixuan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hanqing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_D/0/1/0/all/0/1&quot;&gt;David Z. Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15080">
<title>Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models. (arXiv:2305.15080v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15080</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in Large Language Models (LLMs) have stimulated a surge of
research aimed at extending their applications to the visual domain. While
these models exhibit promise in generating abstract image captions and
facilitating natural conversations, their performance on text-rich images still
requires improvement. In this paper, we introduce Contrastive Reading Model
(Cream), a novel neural architecture designed to enhance the language-image
understanding capability of LLMs by capturing intricate details that are often
overlooked in existing methods. Cream combines vision and auxiliary encoders,
fortified by a contrastive feature alignment technique, to achieve a more
effective comprehension of language information in visually situated contexts
within the images. Our approach bridges the gap between vision and language
understanding, paving the way for the development of more sophisticated
Document Intelligence Assistants. Through rigorous evaluations across diverse
visually-situated language understanding tasks that demand reasoning
capabilities, we demonstrate the compelling performance of Cream, positioning
it as a prominent model in the field of visual document understanding. We
provide our codebase and newly-generated datasets at
https://github.com/naver-ai/cream .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1&quot;&gt;Geewook Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hodong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Daehee Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1&quot;&gt;Haeji Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sanghee Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yoonsik Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1&quot;&gt;Sangdoo Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kil_T/0/1/0/all/0/1&quot;&gt;Taeho Kil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1&quot;&gt;Bado Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Seunghyun Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16264">
<title>Scaling Data-Constrained Language Models. (arXiv:2305.16264v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16264</link>
<description rdf:parseType="Literal">&lt;p&gt;The current trend of scaling language models involves increasing both
parameter count and training dataset size. Extrapolating this trend suggests
that training dataset size may soon be limited by the amount of text data
available on the internet. Motivated by this limit, we investigate scaling
language models in data-constrained regimes. Specifically, we run a large set
of experiments varying the extent of data repetition and compute budget,
ranging up to 900 billion training tokens and 9 billion parameter models. We
find that with constrained data for a fixed compute budget, training with up to
4 epochs of repeated data yields negligible changes to loss compared to having
unique data. However, with more repetition, the value of adding compute
eventually decays to zero. We propose and empirically validate a scaling law
for compute optimality that accounts for the decreasing value of repeated
tokens and excess parameters. Finally, we experiment with approaches mitigating
data scarcity, including augmenting the training dataset with code data or
removing commonly used filters. Models and datasets from our 400 training runs
are freely available at https://github.com/huggingface/datablations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1&quot;&gt;Niklas Muennighoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1&quot;&gt;Alexander M. Rush&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barak_B/0/1/0/all/0/1&quot;&gt;Boaz Barak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scao_T/0/1/0/all/0/1&quot;&gt;Teven Le Scao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piktus_A/0/1/0/all/0/1&quot;&gt;Aleksandra Piktus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tazi_N/0/1/0/all/0/1&quot;&gt;Nouamane Tazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pyysalo_S/0/1/0/all/0/1&quot;&gt;Sampo Pyysalo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1&quot;&gt;Thomas Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1&quot;&gt;Colin Raffel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16427">
<title>Neural (Tangent Kernel) Collapse. (arXiv:2305.16427v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16427</link>
<description rdf:parseType="Literal">&lt;p&gt;This work bridges two important concepts: the Neural Tangent Kernel (NTK),
which captures the evolution of deep neural networks (DNNs) during training,
and the Neural Collapse (NC) phenomenon, which refers to the emergence of
symmetry and structure in the last-layer features of well-trained
classification DNNs. We adopt the natural assumption that the empirical NTK
develops a block structure aligned with the class labels, i.e., samples within
the same class have stronger correlations than samples from different classes.
Under this assumption, we derive the dynamics of DNNs trained with mean squared
(MSE) loss and break them into interpretable phases. Moreover, we identify an
invariant that captures the essence of the dynamics, and use it to prove the
emergence of NC in DNNs with block-structured NTK. We provide large-scale
numerical experiments on three common DNN architectures and three benchmark
datasets to support our theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seleznova_M/0/1/0/all/0/1&quot;&gt;Mariia Seleznova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weitzner_D/0/1/0/all/0/1&quot;&gt;Dana Weitzner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1&quot;&gt;Raja Giryes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1&quot;&gt;Gitta Kutyniok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_H/0/1/0/all/0/1&quot;&gt;Hung-Hsu Chou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.20081">
<title>Efficient Diffusion Policies for Offline Reinforcement Learning. (arXiv:2305.20081v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.20081</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline reinforcement learning (RL) aims to learn optimal policies from
offline datasets, where the parameterization of policies is crucial but often
overlooked. Recently, Diffsuion-QL significantly boosts the performance of
offline RL by representing a policy with a diffusion model, whose success
relies on a parametrized Markov Chain with hundreds of steps for sampling.
However, Diffusion-QL suffers from two critical limitations. 1) It is
computationally inefficient to forward and backward through the whole Markov
chain during training. 2) It is incompatible with maximum likelihood-based RL
algorithms (e.g., policy gradient methods) as the likelihood of diffusion
models is intractable. Therefore, we propose efficient diffusion policy (EDP)
to overcome these two challenges. EDP approximately constructs actions from
corrupted ones at training to avoid running the sampling chain. We conduct
extensive experiments on the D4RL benchmark. The results show that EDP can
reduce the diffusion policy training time from 5 days to 5 hours on
gym-locomotion tasks. Moreover, we show that EDP is compatible with various
offline RL algorithms (TD3, CRR, and IQL) and achieves new state-of-the-art on
D4RL by large margins over previous methods. Our code is available at
https://github.com/sail-sg/edp.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1&quot;&gt;Bingyi Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1&quot;&gt;Chao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1&quot;&gt;Tianyu Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shuicheng Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01665">
<title>SourceP: Detecting Ponzi Schemes on Ethereum with Source Code. (arXiv:2306.01665v3 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01665</link>
<description rdf:parseType="Literal">&lt;p&gt;As blockchain technology becomes more and more popular, a typical financial
scam, the Ponzi scheme, has also emerged in the blockchain platform Ethereum.
This Ponzi scheme deployed through smart contracts, also known as the smart
Ponzi scheme, has caused a lot of economic losses and negative impacts.
Existing methods for detecting smart Ponzi schemes on Ethereum mainly rely on
bytecode features, opcode features, account features, and transaction behavior
features of smart contracts, and the performance of identifying schemes is
insufficient. In this paper, we propose SourceP, a method to detect smart Ponzi
schemes on the Ethereum platform using pre-trained models and data flow, which
only requires using the source code of smart contracts as features to explore
the possibility of detecting smart Ponzi schemes from another direction.
SourceP reduces the difficulty of data acquisition and feature extraction of
existing detection methods while increasing the interpretability of the model.
Specifically, we first convert the source code of a smart contract into a data
flow graph and then introduce a pre-trained model based on learning code
representations to build a classification model to identify Ponzi schemes in
smart contracts. The experimental results show that SourceP achieves 87.2\%
recall and 90.7\% F-score for detecting smart Ponzi schemes within Ethereum&apos;s
smart contract dataset, outperforming state-of-the-art methods in terms of
performance and sustainability. We also demonstrate through additional
experiments that pre-trained models and data flow play an important
contribution to SourceP, as well as proving that SourceP has a good
generalization ability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Pengcheng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1&quot;&gt;Liang Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1&quot;&gt;Keting Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04220">
<title>Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04220</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline reinforcement learning (RL) offers an appealing approach to
real-world tasks by learning policies from pre-collected datasets without
interacting with the environment. However, the performance of existing offline
RL algorithms heavily depends on the scale and state-action space coverage of
datasets. Real-world data collection is often expensive and uncontrollable,
leading to small and narrowly covered datasets and posing significant
challenges for practical deployments of offline RL. In this paper, we provide a
new insight that leveraging the fundamental symmetry of system dynamics can
substantially enhance offline RL performance under small datasets.
Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced
Dynamics Model (TDM), which establishes consistency between a pair of forward
and reverse latent dynamics. TDM provides both well-behaved representations for
small datasets and a new reliability measure for OOD samples based on
compliance with the T-symmetry. These can be readily used to construct a new
offline RL algorithm (TSRL) with less conservative policy constraints and a
reliable latent space data augmentation procedure. Based on extensive
experiments, we find TSRL achieves great performance on small benchmark
datasets with as few as 1% of the original samples, which significantly
outperforms the recent offline RL algorithms in terms of data efficiency and
generalizability.Code is available at: https://github.com/pcheng2/TSRL
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1&quot;&gt;Peng Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1&quot;&gt;Xianyuan Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhihao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenjia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shoucheng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Han Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Youfang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Li Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07092">
<title>Tuning Legged Locomotion Controllers via Safe Bayesian Optimization. (arXiv:2306.07092v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07092</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a data-driven strategy to streamline the deployment of
model-based controllers in legged robotic hardware platforms. Our approach
leverages a model-free safe learning algorithm to automate the tuning of
control gains, addressing the mismatch between the simplified model used in the
control formulation and the real system. This method substantially mitigates
the risk of hazardous interactions with the robot by sample-efficiently
optimizing parameters within a probably safe region. Additionally, we extend
the applicability of our approach to incorporate the different gait parameters
as contexts, leading to a safe, sample-efficient exploration algorithm capable
of tuning a motion controller for diverse gait patterns. We validate our method
through simulation and hardware experiments, where we demonstrate that the
algorithm obtains superior performance on tuning a model-based motion
controller for multiple gaits safely.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Widmer_D/0/1/0/all/0/1&quot;&gt;Daniel Widmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1&quot;&gt;Dongho Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sukhija_B/0/1/0/all/0/1&quot;&gt;Bhavya Sukhija&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hubotter_J/0/1/0/all/0/1&quot;&gt;Jonas H&amp;#xfc;botter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1&quot;&gt;Andreas Krause&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coros_S/0/1/0/all/0/1&quot;&gt;Stelian Coros&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13004">
<title>Can Differentiable Decision Trees Learn Interpretable Reward Functions?. (arXiv:2306.13004v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13004</link>
<description rdf:parseType="Literal">&lt;p&gt;There is an increasing interest in learning reward functions that model human
preferences. However, many frameworks use blackbox learning methods that, while
expressive, are difficult to interpret. We propose and evaluate a novel
approach for learning expressive and interpretable reward functions from
preferences using Differentiable Decision Trees (DDTs). Our experiments across
several domains, including CartPole, Visual Gridworld environments and Atari
games, provide evidence that that the tree structure of our learned reward
function is useful in determining the extent to which the reward function is
aligned with human preferences. We provide experimental evidence that reward
DDTs can achieve competitive performance when compared with larger capacity
deep neural network reward functions. We also observe that the choice between
soft and hard (argmax) output of reward DDT reveals a tension between wanting
highly shaped rewards to ensure good RL performance, while also wanting
simpler, more interpretable rewards.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalra_A/0/1/0/all/0/1&quot;&gt;Akansha Kalra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1&quot;&gt;Daniel S. Brown&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14685">
<title>DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models. (arXiv:2306.14685v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14685</link>
<description rdf:parseType="Literal">&lt;p&gt;Even though trained mainly on images, we discover that pretrained diffusion
models show impressive power in guiding sketch synthesis. In this paper, we
present DiffSketcher, an innovative algorithm that creates \textit{vectorized}
free-hand sketches using natural language input. DiffSketcher is developed
based on a pre-trained text-to-image diffusion model. It performs the task by
directly optimizing a set of B\&apos;ezier curves with an extended version of the
score distillation sampling (SDS) loss, which allows us to use a raster-level
diffusion model as a prior for optimizing a parametric vectorized sketch
generator. Furthermore, we explore attention maps embedded in the diffusion
model for effective stroke initialization to speed up the generation process.
The generated sketches demonstrate multiple levels of abstraction while
maintaining recognizability, underlying structure, and essential visual details
of the subject drawn. Our experiments show that DiffSketcher achieves greater
quality than prior work. The code and demo of DiffSketcher can be found at
https://ximinng.github.io/DiffSketcher-project/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1&quot;&gt;Ximing Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chuang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Haitao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dong Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01951">
<title>A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks. (arXiv:2307.01951v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01951</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) have become increasingly popular for
classification tasks on graph-structured data. Yet, the interplay between graph
topology and feature evolution in GNNs is not well understood. In this paper,
we focus on node-wise classification, illustrated with community detection on
stochastic block model graphs, and explore the feature evolution through the
lens of the &quot;Neural Collapse&quot; (NC) phenomenon. When training instance-wise deep
classifiers (e.g. for image classification) beyond the zero training error
point, NC demonstrates a reduction in the deepest features&apos; within-class
variability and an increased alignment of their class means to certain
symmetric structures. We start with an empirical study that shows that a
decrease in within-class variability is also prevalent in the node-wise
classification setting, however, not to the extent observed in the
instance-wise case. Then, we theoretically study this distinction.
Specifically, we show that even an &quot;optimistic&quot; mathematical model requires
that the graphs obey a strict structural condition in order to possess a
minimizer with exact collapse. Interestingly, this condition is viable also for
heterophilic graphs and relates to recent empirical studies on settings with
improved GNNs&apos; generalization. Furthermore, by studying the gradient dynamics
of the theoretical model, we provide reasoning for the partial collapse
observed empirically. Finally, we present a study on the evolution of within-
and between-class feature variability across layers of a well-trained GNN and
contrast the behavior with spectral methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kothapalli_V/0/1/0/all/0/1&quot;&gt;Vignesh Kothapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tirer_T/0/1/0/all/0/1&quot;&gt;Tom Tirer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1&quot;&gt;Joan Bruna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04721">
<title>Large Language Models as General Pattern Machines. (arXiv:2307.04721v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04721</link>
<description rdf:parseType="Literal">&lt;p&gt;We observe that pre-trained large language models (LLMs) are capable of
autoregressively completing complex token sequences -- from arbitrary ones
procedurally generated by probabilistic context-free grammars (PCFG), to more
rich spatial patterns found in the Abstraction and Reasoning Corpus (ARC), a
general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern
completion proficiency can be partially retained even when the sequences are
expressed using tokens randomly sampled from the vocabulary. These results
suggest that without any additional training, LLMs can serve as general
sequence modelers, driven by in-context learning. In this work, we investigate
how these zero-shot capabilities may be applied to problems in robotics -- from
extrapolating sequences of numbers that represent states over time to complete
simple motions, to least-to-most prompting of reward-conditioned trajectories
that can discover and represent closed-loop policies (e.g., a stabilizing
controller for CartPole). While difficult to deploy today for real systems due
to latency, context size limitations, and compute costs, the approach of using
LLMs to drive low-level control may provide an exciting glimpse into how the
patterns among words could be transferred to actions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirchandani_S/0/1/0/all/0/1&quot;&gt;Suvir Mirchandani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1&quot;&gt;Pete Florence&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1&quot;&gt;Brian Ichter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Driess_D/0/1/0/all/0/1&quot;&gt;Danny Driess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arenas_M/0/1/0/all/0/1&quot;&gt;Montserrat Gonzalez Arenas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1&quot;&gt;Kanishka Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1&quot;&gt;Dorsa Sadigh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Andy Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05891">
<title>PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks. (arXiv:2307.05891v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05891</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning (RL) has shown immense potential for learning to
control systems through data alone. However, one challenge deep RL faces is
that the full state of the system is often not observable. When this is the
case, the policy needs to leverage the history of observations to infer the
current state. At the same time, differences between the training and testing
environments makes it critical for the policy not to overfit to the sequence of
observations it sees at training time. As such, there is an important balancing
act between having the history encoder be flexible enough to extract relevant
information, yet be robust to changes in the environment. To strike this
balance, we look to the PID controller for inspiration. We assert the PID
controller&apos;s success shows that only summing and differencing are needed to
accumulate information over time for many control tasks. Following this
principle, we propose two architectures for encoding history: one that directly
uses PID features and another that extends these core ideas and can be used in
arbitrary control tasks. When compared with prior approaches, our encoders
produce policies that are often more robust and achieve better performance on a
variety of tracking tasks. Going beyond tracking tasks, our policies achieve
1.7x better performance on average over previous state-of-the-art methods on a
suite of locomotion control tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Char_I/0/1/0/all/0/1&quot;&gt;Ian Char&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jeff Schneider&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13633">
<title>Adaptive whitening with fast gain modulation and slow synaptic plasticity. (arXiv:2308.13633v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13633</link>
<description rdf:parseType="Literal">&lt;p&gt;Neurons in early sensory areas rapidly adapt to changing sensory statistics,
both by normalizing the variance of their individual responses and by reducing
correlations between their responses. Together, these transformations may be
viewed as an adaptive form of statistical whitening. Existing mechanistic
models of adaptive whitening exclusively use either synaptic plasticity or gain
modulation as the biological substrate for adaptation; however, on their own,
each of these models has significant limitations. In this work, we unify these
approaches in a normative multi-timescale mechanistic model that adaptively
whitens its responses with complementary computational roles for synaptic
plasticity and gain modulation. Gains are modified on a fast timescale to adapt
to the current statistical context, whereas synapses are modified on a slow
timescale to match structural properties of the input statistics that are
invariant across contexts. Our model is derived from a novel multi-timescale
whitening objective that factorizes the inverse whitening matrix into basis
vectors, which correspond to synaptic weights, and a diagonal matrix, which
corresponds to neuronal gains. We test our model on synthetic and natural
datasets and find that the synapses learn optimal configurations over long
timescales that enable adaptive whitening on short timescales using gain
modulation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Duong_L/0/1/0/all/0/1&quot;&gt;Lyndon R. Duong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Simoncelli_E/0/1/0/all/0/1&quot;&gt;Eero P. Simoncelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chklovskii_D/0/1/0/all/0/1&quot;&gt;Dmitri B. Chklovskii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lipshutz_D/0/1/0/all/0/1&quot;&gt;David Lipshutz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14284">
<title>LLM Powered Sim-to-real Transfer for Traffic Signal Control. (arXiv:2308.14284v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14284</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerous solutions are proposed for the Traffic Signal Control (TSC) tasks
aiming to provide efficient transportation and mitigate congestion waste. In
recent, promising results have been attained by Reinforcement Learning (RL)
methods through trial and error in simulators, bringing confidence in solving
cities&apos; congestion headaches. However, there still exist performance gaps when
simulator-trained policies are deployed to the real world. This issue is mainly
introduced by the system dynamic difference between the training simulator and
the real-world environments. The Large Language Models (LLMs) are trained on
mass knowledge and proved to be equipped with astonishing inference abilities.
In this work, we leverage LLMs to understand and profile the system dynamics by
a prompt-based grounded action transformation. Accepting the cloze prompt
template, and then filling in the answer based on accessible context, the
pre-trained LLM&apos;s inference ability is exploited and applied to understand how
weather conditions, traffic states, and road types influence traffic dynamics,
being aware of this, the policies&apos; action is taken and grounded based on
realistic dynamics, thus help the agent learn a more realistic policy. We
conduct experiments using DQN to show the effectiveness of the proposed
PromptGAT&apos;s ability in mitigating the performance gap from simulation to
reality (sim-to-real).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Da_L/0/1/0/all/0/1&quot;&gt;Longchao Da&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1&quot;&gt;Minchiuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1&quot;&gt;Hao Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hua Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01270">
<title>COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action Spotting using Transformers. (arXiv:2309.01270v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01270</link>
<description rdf:parseType="Literal">&lt;p&gt;We present COMEDIAN, a novel pipeline to initialize spatiotemporal
transformers for action spotting, which involves self-supervised learning and
knowledge distillation. Action spotting is a timestamp-level temporal action
detection task. Our pipeline consists of three steps, with two initialization
stages. First, we perform self-supervised initialization of a spatial
transformer using short videos as input. Additionally, we initialize a temporal
transformer that enhances the spatial transformer&apos;s outputs with global context
through knowledge distillation from a pre-computed feature bank aligned with
each short video segment. In the final step, we fine-tune the transformers to
the action spotting task. The experiments, conducted on the SoccerNet-v2
dataset, demonstrate state-of-the-art performance and validate the
effectiveness of COMEDIAN&apos;s pretraining paradigm. Our results highlight several
advantages of our pretraining pipeline, including improved performance and
faster convergence compared to non-pretrained models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denize_J/0/1/0/all/0/1&quot;&gt;Julien Denize&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liashuha_M/0/1/0/all/0/1&quot;&gt;Mykola Liashuha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabarisoa_J/0/1/0/all/0/1&quot;&gt;Jaonary Rabarisoa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orcesi_A/0/1/0/all/0/1&quot;&gt;Astrid Orcesi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herault_R/0/1/0/all/0/1&quot;&gt;Romain H&amp;#xe9;rault&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07510">
<title>Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions. (arXiv:2309.07510v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07510</link>
<description rdf:parseType="Literal">&lt;p&gt;Perceiving and manipulating 3D articulated objects in diverse environments is
essential for home-assistant robots. Recent studies have shown that point-level
affordance provides actionable priors for downstream manipulation tasks.
However, existing works primarily focus on single-object scenarios with
homogeneous agents, overlooking the realistic constraints imposed by the
environment and the agent&apos;s morphology, e.g., occlusions and physical
limitations. In this paper, we propose an environment-aware affordance
framework that incorporates both object-level actionable priors and environment
constraints. Unlike object-centric affordance approaches, learning
environment-aware affordance faces the challenge of combinatorial explosion due
to the complexity of various occlusions, characterized by their quantities,
geometries, positions and poses. To address this and enhance data efficiency,
we introduce a novel contrastive affordance learning framework capable of
training on scenes containing a single occluder and generalizing to scenes with
complex occluder combinations. Experiments demonstrate the effectiveness of our
proposed approach in learning affordance considering environment constraints.
Project page at https://chengkaiacademycity.github.io/EnvAwareAfford/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruihai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1&quot;&gt;Kai Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_C/0/1/0/all/0/1&quot;&gt;Chuanruo Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_G/0/1/0/all/0/1&quot;&gt;Guanqi Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07593">
<title>Statistically Valid Variable Importance Assessment through Conditional Permutations. (arXiv:2309.07593v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07593</link>
<description rdf:parseType="Literal">&lt;p&gt;Variable importance assessment has become a crucial step in machine-learning
applications when using complex learners, such as deep neural networks, on
large-scale data. Removal-based importance assessment is currently the
reference approach, particularly when statistical guarantees are sought to
justify variable inclusion. It is often implemented with variable permutation
schemes. On the flip side, these approaches risk misidentifying unimportant
variables as important in the presence of correlations among covariates. Here
we develop a systematic approach for studying Conditional Permutation
Importance (CPI) that is model agnostic and computationally lean, as well as
reusable benchmarks of state-of-the-art variable importance estimators. We show
theoretically and empirically that $\textit{CPI}$ overcomes the limitations of
standard permutation importance by providing accurate type-I error control.
When used with a deep neural network, $\textit{CPI}$ consistently showed top
accuracy across benchmarks. An experiment on real-world data analysis in a
large-scale medical dataset showed that $\textit{CPI}$ provides a more
parsimonious selection of statistically significant variables. Our results
suggest that $\textit{CPI}$ can be readily used as drop-in replacement for
permutation-based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chamma_A/0/1/0/all/0/1&quot;&gt;Ahmad Chamma&lt;/a&gt; (1 and 2 and 3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engemann_D/0/1/0/all/0/1&quot;&gt;Denis A. Engemann&lt;/a&gt; (4), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thirion_B/0/1/0/all/0/1&quot;&gt;Bertrand Thirion&lt;/a&gt; (1 and 2 and 3) ((1) Inria, (2) Universite Paris Saclay, (3) CEA, (4) Roche Pharma Research and Early Development, Neuroscience and Rare Diseases, Roche Innovation Center Basel, F. Hoffmann-La Roche Ltd., Basel, Switzerland)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14660">
<title>CoFiI2P: Coarse-to-Fine Correspondences for Image-to-Point Cloud Registration. (arXiv:2309.14660v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14660</link>
<description rdf:parseType="Literal">&lt;p&gt;Image-to-point cloud (I2P) registration is a fundamental task in the field of
autonomous vehicles and transportation systems for cross-modality data fusion
and localization. Existing I2P registration methods estimate correspondences at
the point/pixel level, often overlooking global alignment. However, I2P
matching can easily converge to a local optimum when performed without
high-level guidance from global constraints. To address this issue, this paper
introduces CoFiI2P, a novel I2P registration network that extracts
correspondences in a coarse-to-fine manner to achieve the globally optimal
solution. First, the image and point cloud data are processed through a Siamese
encoder-decoder network for hierarchical feature extraction. Second, a
coarse-to-fine matching module is designed to leverage these features and
establish robust feature correspondences. Specifically, In the coarse matching
phase, a novel I2P transformer module is employed to capture both homogeneous
and heterogeneous global information from the image and point cloud data. This
enables the estimation of coarse super-point/super-pixel matching pairs with
discriminative descriptors. In the fine matching module, point/pixel pairs are
established with the guidance of super-point/super-pixel correspondences.
Finally, based on matching pairs, the transform matrix is estimated with the
EPnP-RANSAC algorithm. Extensive experiments conducted on the KITTI dataset
demonstrate that CoFiI2P achieves impressive results, with a relative rotation
error (RRE) of 1.14 degrees and a relative translation error (RTE) of 0.29
meters. These results represent a significant improvement of 84\% in RRE and
89\% in RTE compared to the current state-of-the-art (SOTA) method. Qualitative
results are available at https://youtu.be/ovbedasXuZE. The source code will be
publicly released at https://github.com/kang-1-2-3/CoFiI2P.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1&quot;&gt;Shuhao Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1&quot;&gt;Youqi Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianping Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_F/0/1/0/all/0/1&quot;&gt;Fuxun Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fangning Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bisheng Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00100">
<title>Multilingual Natural Language Processing Model for Radiology Reports -- The Summary is all you need!. (arXiv:2310.00100v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00100</link>
<description rdf:parseType="Literal">&lt;p&gt;The impression section of a radiology report summarizes important radiology
findings and plays a critical role in communicating these findings to
physicians. However, the preparation of these summaries is time-consuming and
error-prone for radiologists. Recently, numerous models for radiology report
summarization have been developed. Nevertheless, there is currently no model
that can summarize these reports in multiple languages. Such a model could
greatly improve future research and the development of Deep Learning models
that incorporate data from patients with different ethnic backgrounds. In this
study, the generation of radiology impressions in different languages was
automated by fine-tuning a model, publicly available, based on a multilingual
text-to-text Transformer to summarize findings available in English,
Portuguese, and German radiology reports. In a blind test, two board-certified
radiologists indicated that for at least 70% of the system-generated summaries,
the quality matched or exceeded the corresponding human-written summaries,
suggesting substantial clinical reliability. Furthermore, this study showed
that the multilingual model outperformed other models that specialized in
summarizing radiology reports in only one language, as well as models that were
not specifically designed for summarizing radiology reports, such as ChatGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindo_M/0/1/0/all/0/1&quot;&gt;Mariana Lindo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_A/0/1/0/all/0/1&quot;&gt;Ana Sofia Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianning Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luijten_G/0/1/0/all/0/1&quot;&gt;Gijs Luijten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Correia_G/0/1/0/all/0/1&quot;&gt;Gustavo Correia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Moon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1&quot;&gt;Jens Kleesiek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1&quot;&gt;Jan Egger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alves_V/0/1/0/all/0/1&quot;&gt;Victor Alves&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01551">
<title>Harnessing the Power of Choices in Decision Tree Learning. (arXiv:2310.01551v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01551</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a simple generalization of standard and empirically successful
decision tree learning algorithms such as ID3, C4.5, and CART. These
algorithms, which have been central to machine learning for decades, are greedy
in nature: they grow a decision tree by iteratively splitting on the best
attribute. Our algorithm, Top-$k$, considers the $k$ best attributes as
possible splits instead of just the single best attribute. We demonstrate,
theoretically and empirically, the power of this simple generalization. We
first prove a {\sl greediness hierarchy theorem} showing that for every $k \in
\mathbb{N}$, Top-$(k+1)$ can be dramatically more powerful than Top-$k$: there
are data distributions for which the former achieves accuracy $1-\varepsilon$,
whereas the latter only achieves accuracy $\frac1{2}+\varepsilon$. We then
show, through extensive experiments, that Top-$k$ outperforms the two main
approaches to decision tree learning: classic greedy algorithms and more recent
&quot;optimal decision tree&quot; algorithms. On one hand, Top-$k$ consistently enjoys
significant accuracy gains over greedy algorithms across a wide range of
benchmarks. On the other hand, Top-$k$ is markedly more scalable than optimal
decision tree algorithms and is able to handle dataset and feature set sizes
that remain far beyond the reach of these algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blanc_G/0/1/0/all/0/1&quot;&gt;Guy Blanc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lange_J/0/1/0/all/0/1&quot;&gt;Jane Lange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pabbaraju_C/0/1/0/all/0/1&quot;&gt;Chirag Pabbaraju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sullivan_C/0/1/0/all/0/1&quot;&gt;Colin Sullivan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1&quot;&gt;Li-Yang Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiwari_M/0/1/0/all/0/1&quot;&gt;Mo Tiwari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02255">
<title>MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models. (arXiv:2310.02255v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02255</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit
impressive problem-solving skills in many tasks and domains, but their ability
in mathematical reasoning in visual contexts has not been systematically
studied. To bridge this gap, we present MathVista, a benchmark designed to
combine challenges from diverse mathematical and visual tasks. It consists of
6,141 examples, derived from 28 existing multimodal datasets involving
mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and
PaperQA). Completing these tasks requires fine-grained, deep visual
understanding and compositional reasoning, which all state-of-the-art
foundation models find challenging. With MathVista, we have conducted a
comprehensive, quantitative evaluation of 12 prominent foundation models. The
best-performing GPT-4V model achieves an overall accuracy of 49.9%,
substantially outperforming Bard, the second-best performer, by 15.1%. Our
in-depth analysis reveals that the superiority of GPT-4V is mainly attributed
to its enhanced visual perception and mathematical reasoning. However, GPT-4V
still falls short of human performance by 10.4%, as it often struggles to
understand complex figures and perform rigorous reasoning. This significant gap
underscores the critical role that MathVista will play in the development of
general-purpose AI agents capable of tackling mathematically intensive and
visually rich real-world tasks. We further explore the new ability of
self-verification, the application of self-consistency, and the interactive
chatbot capabilities of GPT-4V, highlighting its promising potential for future
research. The project is available at https://mathvista.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Pan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1&quot;&gt;Hritik Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1&quot;&gt;Tony Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiacheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1&quot;&gt;Hannaneh Hajishirzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1&quot;&gt;Michel Galley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11191">
<title>Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding. (arXiv:2310.11191v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11191</link>
<description rdf:parseType="Literal">&lt;p&gt;Text simplification has emerged as an increasingly useful application of AI
for bridging the communication gap in specialized fields such as medicine,
where the lexicon is often dominated by technical jargon and complex
constructs. Despite notable progress, methods in medical simplification
sometimes result in the generated text having lower quality and diversity. In
this work, we explore ways to further improve the readability of text
simplification in the medical domain. We propose (1) a new unlikelihood loss
that encourages generation of simpler terms and (2) a reranked beam search
decoding method that optimizes for simplicity, which achieve better performance
on readability metrics on three datasets. This study&apos;s findings offer promising
avenues for improving text simplification in the medical field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flores_L/0/1/0/all/0/1&quot;&gt;Lorenzo Jaime Yu Flores&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Heyuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1&quot;&gt;Kejian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chheang_S/0/1/0/all/0/1&quot;&gt;Sophie Chheang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1&quot;&gt;Arman Cohan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12880">
<title>TwinPot: Digital Twin-assisted Honeypot for Cyber-Secure Smart Seaports. (arXiv:2310.12880v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12880</link>
<description rdf:parseType="Literal">&lt;p&gt;The idea of next-generation ports has become more apparent in the last ten
years in response to the challenge posed by the rising demand for efficiency
and the ever-increasing volume of goods. In this new era of intelligent
infrastructure and facilities, it is evident that cyber-security has recently
received the most significant attention from the seaport and maritime
authorities, and it is a primary concern on the agenda of most ports.
Traditional security solutions can be applied to safeguard IoT and
Cyber-Physical Systems (CPS) from harmful entities. Nevertheless, security
researchers can only watch, examine, and learn about the behaviors of attackers
if these solutions operate more transparently. Herein, honeypots are potential
solutions since they offer valuable information about the attackers. It can be
virtual or physical. Virtual honeypots must be more realistic to entice
attackers, necessitating better high-fidelity. To this end, Digital Twin (DT)
technology can be employed to increase the complexity and simulation fidelity
of the honeypots. Seaports can be attacked from both their existing devices and
external devices at the same time. Existing mechanisms are insufficient to
detect external attacks; therefore, the current systems cannot handle attacks
at the desired level. DT and honeypot technologies can be used together to
tackle them. Consequently, we suggest a DT-assisted honeypot, called TwinPot,
for external attacks in smart seaports. Moreover, we propose an intelligent
attack detection mechanism to handle different attack types using DT for
internal attacks. Finally, we build an extensive smart seaport dataset for
internal and external attacks using the MANSIM tool and two existing datasets
to test the performance of our system. We show that under simultaneous internal
and external attacks on the system, our solution successfully detects internal
and external attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yigit_Y/0/1/0/all/0/1&quot;&gt;Yagmur Yigit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kinaci_O/0/1/0/all/0/1&quot;&gt;Omer Kemal Kinaci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duong_T/0/1/0/all/0/1&quot;&gt;Trung Q. Duong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canberk_B/0/1/0/all/0/1&quot;&gt;Berk Canberk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12914">
<title>Network-Aware AutoML Framework for Software-Defined Sensor Networks. (arXiv:2310.12914v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12914</link>
<description rdf:parseType="Literal">&lt;p&gt;As the current detection solutions of distributed denial of service attacks
(DDoS) need additional infrastructures to handle high aggregate data rates,
they are not suitable for sensor networks or the Internet of Things. Besides,
the security architecture of software-defined sensor networks needs to pay
attention to the vulnerabilities of both software-defined networks and sensor
networks. In this paper, we propose a network-aware automated machine learning
(AutoML) framework which detects DDoS attacks in software-defined sensor
networks. Our framework selects an ideal machine learning algorithm to detect
DDoS attacks in network-constrained environments, using metrics such as
variable traffic load, heterogeneous traffic rate, and detection time while
preventing over-fitting. Our contributions are two-fold: (i) we first
investigate the trade-off between the efficiency of ML algorithms and
network/traffic state in the scope of DDoS detection. (ii) we design and
implement a software architecture containing open-source network tools, with
the deployment of multiple ML algorithms. Lastly, we show that under the denial
of service attacks, our framework ensures the traffic packets are still
delivered within the network with additional delays.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horsanali_E/0/1/0/all/0/1&quot;&gt;Emre Horsanali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yigit_Y/0/1/0/all/0/1&quot;&gt;Yagmur Yigit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Secinti_G/0/1/0/all/0/1&quot;&gt;Gokhan Secinti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karameseoglu_A/0/1/0/all/0/1&quot;&gt;Aytac Karameseoglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canberk_B/0/1/0/all/0/1&quot;&gt;Berk Canberk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12924">
<title>Digital Twin-Enabled Intelligent DDoS Detection Mechanism for Autonomous Core Networks. (arXiv:2310.12924v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12924</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing distributed denial of service attack (DDoS) solutions cannot handle
highly aggregated data rates; thus, they are unsuitable for Internet service
provider (ISP) core networks. This article proposes a digital twin-enabled
intelligent DDoS detection mechanism using an online learning method for
autonomous systems. Our contributions are three-fold: we first design a DDoS
detection architecture based on the digital twin for ISP core networks. We
implemented a Yet Another Next Generation (YANG) model and an automated feature
selection (AutoFS) module to handle core network data. We used an online
learning approach to update the model instantly and efficiently, improve the
learning model quickly, and ensure accurate predictions. Finally, we reveal
that our proposed solution successfully detects DDoS attacks and updates the
feature selection method and learning model with a true classification rate of
ninety-seven percent. Our proposed solution can estimate the attack within
approximately fifteen minutes after the DDoS attack starts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yigit_Y/0/1/0/all/0/1&quot;&gt;Yagmur Yigit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bal_B/0/1/0/all/0/1&quot;&gt;Bahadir Bal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karameseoglu_A/0/1/0/all/0/1&quot;&gt;Aytac Karameseoglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duong_T/0/1/0/all/0/1&quot;&gt;Trung Q. Duong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canberk_B/0/1/0/all/0/1&quot;&gt;Berk Canberk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16218">
<title>Knowledge Editing for Large Language Models: A Survey. (arXiv:2310.16218v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16218</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have recently transformed both the academic and
industrial landscapes due to their remarkable capacity to understand, analyze,
and generate texts based on their vast knowledge and reasoning ability.
Nevertheless, one major drawback of LLMs is their substantial computational
cost for pre-training due to their unprecedented amounts of parameters. The
disadvantage is exacerbated when new knowledge frequently needs to be
introduced into the pre-trained model. Therefore, it is imperative to develop
effective and efficient techniques to update pre-trained LLMs. Traditional
methods encode new knowledge in pre-trained LLMs through direct fine-tuning.
However, naively re-training LLMs can be computationally intensive and risks
degenerating valuable pre-trained knowledge irrelevant to the update in the
model. Recently, Knowledge-based Model Editing (KME) has attracted increasing
attention, which aims to precisely modify the LLMs to incorporate specific
knowledge, without negatively influencing other irrelevant knowledge. In this
survey, we aim to provide a comprehensive and in-depth overview of recent
advances in the field of KME. We first introduce a general formulation of KME
to encompass different KME strategies. Afterward, we provide an innovative
taxonomy of KME techniques based on how the new knowledge is introduced into
pre-trained LLMs, and investigate existing KME strategies while analyzing key
insights, advantages, and limitations of methods from each category. Moreover,
representative metrics, datasets, and applications of KME are introduced
accordingly. Finally, we provide an in-depth analysis regarding the
practicality and remaining challenges of KME and suggest promising research
directions for further advancement in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Song Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yaochen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haochen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zaiyi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jundong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16713">
<title>SkyMath: Technical Report. (arXiv:2310.16713v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16713</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have shown great potential to solve varieties of
natural language processing (NLP) tasks, including mathematical reasoning. In
this work, we present SkyMath, a large language model for mathematics with 13
billion parameters. By applying self-compare fine-tuning, we have enhanced
mathematical reasoning abilities of Skywork-13B-Base remarkably. On GSM8K,
SkyMath outperforms all known open-source models of similar size and has
established a new SOTA performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Liu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Haihua Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1&quot;&gt;Wenjun Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Lei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenxia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lunan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jianfei Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1&quot;&gt;Tianwen Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Biye Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Liang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Bo Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guoliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xuejie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xilin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Rui Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16776">
<title>DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection. (arXiv:2310.16776v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16776</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances have led to the availability of many pre-trained language
models (PLMs); however, a question that remains is how much data is truly
needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT,
a data-efficient fine-tuning framework that leverages unsupervised core-set
selection to minimize the amount of data needed to fine-tune PLMs for
downstream tasks. We demonstrate the efficacy of our DEFT framework in the
context of text-editing LMs, and compare to the state-of-the art text-editing
model, CoEDIT. Our quantitative and qualitative results demonstrate that DEFT
models are just as accurate as CoEDIT while being finetuned on ~70% less data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1&quot;&gt;Devleena Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khetan_V/0/1/0/all/0/1&quot;&gt;Vivek Khetan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16779">
<title>Multi-scale Diffusion Denoised Smoothing. (arXiv:2310.16779v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16779</link>
<description rdf:parseType="Literal">&lt;p&gt;Along with recent diffusion models, randomized smoothing has become one of a
few tangible approaches that offers adversarial robustness to models at scale,
e.g., those of large pre-trained models. Specifically, one can perform
randomized smoothing on any classifier via a simple &quot;denoise-and-classify&quot;
pipeline, so-called denoised smoothing, given that an accurate denoiser is
available - such as diffusion model. In this paper, we present scalable methods
to address the current trade-off between certified robustness and accuracy in
denoised smoothing. Our key idea is to &quot;selectively&quot; apply smoothing among
multiple noise scales, coined multi-scale smoothing, which can be efficiently
implemented with a single diffusion model. This approach also suggests a new
objective to compare the collective robustness of multi-scale smoothed
classifiers, and questions which representation of diffusion model would
maximize the objective. To address this, we propose to further fine-tune
diffusion model (a) to perform consistent denoising whenever the original image
is recoverable, but (b) to generate rather diverse outputs otherwise. Our
experiments show that the proposed multi-scale smoothing scheme combined with
diffusion fine-tuning enables strong certified robustness available with high
noise level while maintaining its accuracy closer to non-smoothed classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1&quot;&gt;Jongheon Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jinwoo Shin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2010.01992">
<title>Improving Few-Shot Learning through Multi-task Representation Learning Theory. (arXiv:2010.01992v3 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2010.01992</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider the framework of multi-task representation (MTR)
learning where the goal is to use source tasks to learn a representation that
reduces the sample complexity of solving a target task. We start by reviewing
recent advances in MTR theory and show that they can provide novel insights for
popular meta-learning algorithms when analyzed within this framework. In
particular, we highlight a fundamental difference between gradient-based and
metric-based algorithms in practice and put forward a theoretical analysis to
explain it. Finally, we use the derived insights to improve the performance of
meta-learning methods via a new spectral-based regularization term and confirm
its efficiency through experimental studies on few-shot classification
benchmarks. To the best of our knowledge, this is the first contribution that
puts the most recent learning bounds of MTR theory into practice for the task
of few-shot classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouniot_Q/0/1/0/all/0/1&quot;&gt;Quentin Bouniot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redko_I/0/1/0/all/0/1&quot;&gt;Ievgen Redko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Audigier_R/0/1/0/all/0/1&quot;&gt;Romaric Audigier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loesch_A/0/1/0/all/0/1&quot;&gt;Ang&amp;#xe9;lique Loesch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habrard_A/0/1/0/all/0/1&quot;&gt;Amaury Habrard&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>