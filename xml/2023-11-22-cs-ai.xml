<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-20T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10735" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10737" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10749" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10774" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10775" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10784" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10796" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10801" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10804" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10807" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10809" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10840" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10844" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10921" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10922" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10928" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10933" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10953" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10983" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11029" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1904.10552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2009.07448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.00388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.11482" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.11239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.02249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.11349" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.09124" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.08966" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.00111" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.05507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06476" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.13335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15445" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11203" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06146" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14669" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01098" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10974" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07053" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07510" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12253" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17113" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01827" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02230" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03358" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08278" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09590" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10348" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15127" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20049" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20327" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00530" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04666" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05659" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07127" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07989" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08427" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09115" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09574" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19845" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.10723">
<title>Large Language Models in Finance: A Survey. (arXiv:2311.10723v1 [q-fin.GN])</title>
<link>http://arxiv.org/abs/2311.10723</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in large language models (LLMs) have opened new possibilities
for artificial intelligence applications in finance. In this paper, we provide
a practical survey focused on two key aspects of utilizing LLMs for financial
tasks: existing solutions and guidance for adoption.
&lt;/p&gt;
&lt;p&gt;First, we review current approaches employing LLMs in finance, including
leveraging pretrained models via zero-shot or few-shot learning, fine-tuning on
domain-specific data, and training custom LLMs from scratch. We summarize key
models and evaluate their performance improvements on financial natural
language processing tasks.
&lt;/p&gt;
&lt;p&gt;Second, we propose a decision framework to guide financial professionals in
selecting the appropriate LLM solution based on their use case constraints
around data, compute, and performance needs. The framework provides a pathway
from lightweight experimentation to heavy investment in customized LLMs.
&lt;/p&gt;
&lt;p&gt;Lastly, we discuss limitations and challenges around leveraging LLMs in
financial applications. Overall, this survey aims to synthesize the
state-of-the-art and provide a roadmap for responsibly applying LLMs to advance
financial AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yinheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shaofei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Han Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10725">
<title>Should they? Mobile Biometrics and Technopolicy meet Queer Community Considerations. (arXiv:2311.10725v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.10725</link>
<description rdf:parseType="Literal">&lt;p&gt;Smartphones are integral to our daily lives and activities, providing us with
basic functions like texting and phone calls to more complex motion-based
functionalities like navigation, mobile gaming, and fitness-tracking. To
facilitate these functionalities, smartphones rely on integrated sensors like
accelerometers and gyroscopes. These sensors provide personalized measurements
that, in turn, contribute to tasks such as analyzing biometric data for mobile
health purposes. In addition to benefiting smartphone users, biometric data
holds significant value for researchers engaged in biometric identification
research. Nonetheless, utilizing this user data for biometric identification
tasks, such as gait and gender recognition, raises serious privacy, normative,
and ethical concerns, particularly within the queer community. Concerns of
algorithmic bias and algorithmically-driven dysphoria surface from a historical
backdrop of marginalization, surveillance, harassment, discrimination, and
violence against the queer community. In this position paper, we contribute to
the timely discourse on safeguarding human rights within AI-driven systems by
providing a sense of challenges, tensions, and opportunities for new data
protections and biometric collection practices in a way that grapples with the
sociotechnical realities of the queer community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1&quot;&gt;Anaelia Ovalle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1&quot;&gt;Davi Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boyd_A/0/1/0/all/0/1&quot;&gt;Alicia Boyd&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10735">
<title>Safe Navigation: Training Autonomous Vehicles using Deep Reinforcement Learning in CARLA. (arXiv:2311.10735v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10735</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous vehicles have the potential to revolutionize transportation, but
they must be able to navigate safely in traffic before they can be deployed on
public roads. The goal of this project is to train autonomous vehicles to make
decisions to navigate in uncertain environments using deep reinforcement
learning techniques using the CARLA simulator. The simulator provides a
realistic and urban environment for training and testing self-driving models.
Deep Q-Networks (DQN) are used to predict driving actions. The study involves
the integration of collision sensors, segmentation, and depth camera for better
object detection and distance estimation. The model is tested on 4 different
trajectories in presence of different types of 4-wheeled vehicles and
pedestrians. The segmentation and depth cameras were utilized to ensure
accurate localization of objects and distance measurement. Our proposed method
successfully navigated the self-driving vehicle to its final destination with a
high success rate without colliding with other vehicles, pedestrians, or going
on the sidewalk. To ensure the optimal performance of our reinforcement
learning (RL) models in navigating complex traffic scenarios, we implemented a
pre-processing step to reduce the state space. This involved processing the
images and sensor output before feeding them into the model. Despite
significantly decreasing the state space, our approach yielded robust models
that successfully navigated through traffic with high levels of safety and
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nehme_G/0/1/0/all/0/1&quot;&gt;Ghadi Nehme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deo_T/0/1/0/all/0/1&quot;&gt;Tejas Y. Deo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10737">
<title>AI-enhanced Auto-correction of Programming Exercises: How Effective is GPT-3.5?. (arXiv:2311.10737v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.10737</link>
<description rdf:parseType="Literal">&lt;p&gt;Timely formative feedback is considered as one of the most important drivers
for effective learning. Delivering timely and individualized feedback is
particularly challenging in large classes in higher education. Recently Large
Language Models such as GPT-3 became available to the public that showed
promising results on various tasks such as code generation and code
explanation. This paper investigates the potential of AI in providing
personalized code correction and generating feedback. Based on existing student
submissions of two different real-world assignments, the correctness of the
AI-aided e-assessment as well as the characteristics such as fault
localization, correctness of hints, and code style suggestions of the generated
feedback are investigated. The results show that 73 % of the submissions were
correctly identified as either correct or incorrect. In 59 % of these cases,
GPT-3.5 also successfully generated effective and high-quality feedback.
Additionally, GPT-3.5 exhibited weaknesses in its evaluation, including
localization of errors that were not the actual errors, or even hallucinated
errors. Implications and potential new usage scenarios are discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azaiz_I/0/1/0/all/0/1&quot;&gt;Imen Azaiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deckarm_O/0/1/0/all/0/1&quot;&gt;Oliver Deckarm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strickroth_S/0/1/0/all/0/1&quot;&gt;Sven Strickroth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10747">
<title>Safety-aware Causal Representation for Trustworthy Reinforcement Learning in Autonomous Driving. (arXiv:2311.10747v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.10747</link>
<description rdf:parseType="Literal">&lt;p&gt;In the domain of autonomous driving, the Learning from Demonstration (LfD)
paradigm has exhibited notable efficacy in addressing sequential
decision-making problems. However, consistently achieving safety in varying
traffic contexts, especially in safety-critical scenarios, poses a significant
challenge due to the long-tailed and unforeseen scenarios absent from offline
datasets. In this paper, we introduce the saFety-aware strUctured Scenario
representatION (FUSION), a pioneering methodology conceived to facilitate the
learning of an adaptive end-to-end driving policy by leveraging structured
scenario information. FUSION capitalizes on the causal relationships between
decomposed reward, cost, state, and action space, constructing a framework for
structured sequential reasoning under dynamic traffic environments. We conduct
rigorous evaluations in two typical real-world settings of distribution shift
in autonomous vehicles, demonstrating the good balance between safety cost and
utility reward of FUSION compared to contemporary state-of-the-art safety-aware
LfD baselines. Empirical evidence under diverse driving scenarios attests that
FUSION significantly enhances the safety and generalizability of autonomous
driving agents, even in the face of challenging and unseen environments.
Furthermore, our ablation studies reveal noticeable improvements in the
integration of causal representation into the safe offline RL problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Haohong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1&quot;&gt;Wenhao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zuxin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1&quot;&gt;Yaru Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jiacheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1&quot;&gt;Yuming Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Ding Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10749">
<title>Measuring Five Accountable Talk Moves to Improve Instruction at Scale. (arXiv:2311.10749v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.10749</link>
<description rdf:parseType="Literal">&lt;p&gt;Providing consistent, individualized feedback to teachers on their
instruction can improve student learning outcomes. Such feedback can especially
benefit novice instructors who teach on online platforms and have limited
access to instructional training. To build scalable measures of instruction, we
fine-tune RoBERTa and GPT models to identify five instructional talk moves
inspired by accountable talk theory: adding on, connecting, eliciting, probing
and revoicing students&apos; ideas. We fine-tune these models on a newly annotated
dataset of 2500 instructor utterances derived from transcripts of small group
instruction in an online computer science course, Code in Place. Although we
find that GPT-3 consistently outperforms RoBERTa in terms of precision, its
recall varies significantly. We correlate the instructors&apos; use of each talk
move with indicators of student engagement and satisfaction, including
students&apos; section attendance, section ratings, and assignment completion rates.
We find that using talk moves generally correlates positively with student
outcomes, and connecting student ideas has the largest positive impact. These
results corroborate previous research on the effectiveness of accountable talk
moves and provide exciting avenues for using these models to provide
instructors with useful, scalable feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kupor_A/0/1/0/all/0/1&quot;&gt;Ashlee Kupor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morgan_C/0/1/0/all/0/1&quot;&gt;Candice Morgan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demszky_D/0/1/0/all/0/1&quot;&gt;Dorottya Demszky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10751">
<title>ProAgent: From Robotic Process Automation to Agentic Process Automation. (arXiv:2311.10751v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.10751</link>
<description rdf:parseType="Literal">&lt;p&gt;From ancient water wheels to robotic process automation (RPA), automation
technology has evolved throughout history to liberate human beings from arduous
tasks. Yet, RPA struggles with tasks needing human-like intelligence,
especially in elaborate design of workflow construction and dynamic
decision-making in workflow execution. As Large Language Models (LLMs) have
emerged human-like intelligence, this paper introduces Agentic Process
Automation (APA), a groundbreaking automation paradigm using LLM-based agents
for advanced automation by offloading the human labor to agents associated with
construction and execution. We then instantiate ProAgent, an LLM-based agent
designed to craft workflows from human instructions and make intricate
decisions by coordinating specialized agents. Empirical experiments are
conducted to detail its construction and execution procedure of workflow,
showcasing the feasibility of APA, unveiling the possibility of a new paradigm
of automation driven by agents. Our code is public at
https://github.com/OpenBMB/ProAgent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yining Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1&quot;&gt;Xin Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1&quot;&gt;Shizuo Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jiannan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yujia Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yaxi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Heyang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huadong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yankai Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Maosong Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10757">
<title>How Contentious Terms About People and Cultures are Used in Linked Open Data. (arXiv:2311.10757v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10757</link>
<description rdf:parseType="Literal">&lt;p&gt;Web resources in linked open data (LOD) are comprehensible to humans through
literal textual values attached to them, such as labels, notes, or comments.
Word choices in literals may not always be neutral. When outdated and
culturally stereotyping terminology is used in literals, they may appear as
offensive to users in interfaces and propagate stereotypes to algorithms
trained on them. We study how frequently and in which literals contentious
terms about people and cultures occur in LOD and whether there are attempts to
mark the usage of such terms. For our analysis, we reuse English and Dutch
terms from a knowledge graph that provides opinions of experts from the
cultural heritage domain about terms&apos; contentiousness. We inspect occurrences
of these terms in four widely used datasets: Wikidata, The Getty Art &amp;amp;
Architecture Thesaurus, Princeton WordNet, and Open Dutch WordNet. Some terms
are ambiguous and contentious only in particular senses. Applying word sense
disambiguation, we generate a set of literals relevant to our analysis. We
found that outdated, derogatory, stereotyping terms frequently appear in
descriptive and labelling literals, such as preferred labels that are usually
displayed in interfaces and used for indexing. In some cases, LOD contributors
mark contentious terms with words and phrases in literals (implicit markers) or
properties linked to resources (explicit markers). However, such marking is
rare and non-consistent in all datasets. Our quantitative and qualitative
insights could be helpful in developing more systematic approaches to address
the propagation of stereotypes via LOD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nesterov_A/0/1/0/all/0/1&quot;&gt;Andrei Nesterov&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hollink_L/0/1/0/all/0/1&quot;&gt;Laura Hollink&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ossenbruggen_J/0/1/0/all/0/1&quot;&gt;Jacco van Ossenbruggen&lt;/a&gt; (2) ((1) Centrum Wiskunde &amp;amp; Informatica, (2) VU University Amsterdam)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10764">
<title>Deep Group Interest Modeling of Full Lifelong User Behaviors for CTR Prediction. (arXiv:2311.10764v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2311.10764</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracting users&apos; interests from their lifelong behavior sequence is crucial
for predicting Click-Through Rate (CTR). Most current methods employ a
two-stage process for efficiency: they first select historical behaviors
related to the candidate item and then deduce the user&apos;s interest from this
narrowed-down behavior sub-sequence. This two-stage paradigm, though effective,
leads to information loss. Solely using users&apos; lifelong click behaviors doesn&apos;t
provide a complete picture of their interests, leading to suboptimal
performance. In our research, we introduce the Deep Group Interest Network
(DGIN), an end-to-end method to model the user&apos;s entire behavior history. This
includes all post-registration actions, such as clicks, cart additions,
purchases, and more, providing a nuanced user understanding. We start by
grouping the full range of behaviors using a relevant key (like item_id) to
enhance efficiency. This process reduces the behavior length significantly,
from O(10^4) to O(10^2). To mitigate the potential loss of information due to
grouping, we incorporate two categories of group attributes. Within each group,
we calculate statistical information on various heterogeneous behaviors (like
behavior counts) and employ self-attention mechanisms to highlight unique
behavior characteristics (like behavior type). Based on this reorganized
behavior data, the user&apos;s interests are derived using the Transformer
technique. Additionally, we identify a subset of behaviors that share the same
item_id with the candidate item from the lifelong behavior sequence. The
insights from this subset reveal the user&apos;s decision-making process related to
the candidate item, improving prediction accuracy. Our comprehensive
evaluation, both on industrial and public datasets, validates DGIN&apos;s efficacy
and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1&quot;&gt;Xuyang Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Haoran Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_j/0/1/0/all/0/1&quot;&gt;jin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1&quot;&gt;Defu Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_T/0/1/0/all/0/1&quot;&gt;Tan Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jia Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1&quot;&gt;Jun Lei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10766">
<title>Value FULCRA: Mapping Large Language Models to the Multidimensional Spectrum of Basic Human Values. (arXiv:2311.10766v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10766</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid advancement of Large Language Models (LLMs) has attracted much
attention to value alignment for their responsible development. However, how to
define values in this context remains a largely unexplored question. Existing
work mainly follows the Helpful, Honest, Harmless principle and specifies
values as risk criteria formulated in the AI community, e.g., fairness and
privacy protection, suffering from poor clarity, adaptability and transparency.
Inspired by basic values in humanity and social science across cultures, this
work proposes a novel basic value alignment paradigm and introduces a value
space spanned by basic value dimensions. All LLMs&apos; behaviors can be mapped into
the space by identifying the underlying values, possessing the potential to
address the three challenges. To foster future research, we apply the
representative Schwartz&apos;s Theory of Basic Values as an initialized example and
construct FULCRA, a dataset consisting of 5k (LLM output, value vector) pairs.
Our extensive analysis of FULCRA reveals the underlying relation between basic
values and LLMs&apos; behaviors, demonstrating that our approach not only covers
existing mainstream risks but also anticipates possibly unidentified ones.
Additionally, we present an initial implementation of the basic value
evaluation and alignment, paving the way for future research in this line.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jing Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1&quot;&gt;Xiaoyuan Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiting Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yifan Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10767">
<title>Optimizing IaC Configurations: a Case Study Using Nature-inspired Computing. (arXiv:2311.10767v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2311.10767</link>
<description rdf:parseType="Literal">&lt;p&gt;In the last years, one of the fields of artificial intelligence that has been
investigated the most is nature-inspired computing. The research done on this
specific topic showcases the interest that sparks in researchers and
practitioners, who put their focus on this paradigm because of the adaptability
and ability of nature-inspired algorithms to reach high-quality outcomes on a
wide range of problems. In fact, this kind of methods has been successfully
applied to solve real-world problems in heterogeneous fields such as medicine,
transportation, industry, or software engineering. Our main objective with this
paper is to describe a tool based on nature-inspired computing for solving a
specific software engineering problem. The problem faced consists of optimizing
Infrastructure as Code deployment configurations. For this reason, the name of
the system is IaC Optimizer Platform. A prototypical version of the IOP was
described in previous works, in which the functionality of this platform was
introduced. With this paper, we take a step forward by describing the final
release of the IOP, highlighting its main contribution regarding the current
state-of-the-art, and justifying the decisions made on its implementation.
Also, we contextualize the IOP within the complete platform in which it is
embedded, describing how a user can benefit from its use. To do that, we also
present and solve a real-world use case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osaba_E/0/1/0/all/0/1&quot;&gt;Eneko Osaba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benguria_G/0/1/0/all/0/1&quot;&gt;Gorka Benguria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lobo_J/0/1/0/all/0/1&quot;&gt;Jesus L. Lobo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diaz_de_Arcaya_J/0/1/0/all/0/1&quot;&gt;Josu Diaz-de-Arcaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_J/0/1/0/all/0/1&quot;&gt;Juncal Alonso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Etxaniz_I/0/1/0/all/0/1&quot;&gt;I&amp;#xf1;aki Etxaniz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10770">
<title>Exponentially Faster Language Modelling. (arXiv:2311.10770v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10770</link>
<description rdf:parseType="Literal">&lt;p&gt;Language models only really need to use an exponential fraction of their
neurons for individual inferences. As proof, we present FastBERT, a BERT
variant that uses 0.3\% of its neurons during inference while performing on par
with similar BERT models. FastBERT selectively engages just 12 out of 4095
neurons for each layer inference. This is achieved by replacing feedforward
networks with fast feedforward networks (FFFs). While no truly efficient
implementation currently exists to unlock the full acceleration potential of
conditional neural execution, we provide high-level CPU code achieving 78x
speedup over the optimized baseline feedforward implementation, and a PyTorch
implementation delivering 40x speedup over the equivalent batched feedforward
inference. We publish our training code, benchmarking setup, and model weights.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belcak_P/0/1/0/all/0/1&quot;&gt;Peter Belcak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1&quot;&gt;Roger Wattenhofer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10773">
<title>User Persona Identification and New Service Adaptation Recommendation. (arXiv:2311.10773v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2311.10773</link>
<description rdf:parseType="Literal">&lt;p&gt;Providing a personalized user experience on information dense webpages helps
users in reaching their end-goals sooner. We explore an automated approach to
identifying user personas by leveraging high dimensional trajectory information
from user sessions on webpages. While neural collaborative filtering (NCF)
approaches pay little attention to token semantics, our method introduces
SessionBERT, a Transformer-backed language model trained from scratch on the
masked language modeling (mlm) objective for user trajectories (pages,
metadata, billing in a session) aiming to capture semantics within them. Our
results show that representations learned through SessionBERT are able to
consistently outperform a BERT-base model providing a 3% and 1% relative
improvement in F1-score for predicting page links and next services. We
leverage SessionBERT and extend it to provide recommendations (top-5) for the
next most-relevant services that a user would be likely to use. We achieve a
HIT@5 of 58% from our recommendation model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabari_N/0/1/0/all/0/1&quot;&gt;Narges Tabari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swamy_S/0/1/0/all/0/1&quot;&gt;Sandesh Swamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gangadharaiah_R/0/1/0/all/0/1&quot;&gt;Rashmi Gangadharaiah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10774">
<title>MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning. (arXiv:2311.10774v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10774</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of large language models (LLMs) and their
integration into large multimodal models (LMMs), there has been impressive
progress in zero-shot completion of user-oriented vision-language tasks.
However, a gap remains in the domain of chart image understanding due to the
distinct abstract components in charts. To address this, we introduce a
large-scale MultiModal Chart Instruction (MMC-Instruction) dataset comprising
600k instances supporting diverse tasks and chart types. Leveraging this data,
we develop MultiModal Chart Assistant (MMCA), an LMM that achieves
state-of-the-art performance on existing chart QA benchmarks. Recognizing the
need for a comprehensive evaluation of LMM chart understanding, we also propose
a MultiModal Chart Benchmark (MMC-Benchmark), a comprehensive human-annotated
benchmark with 9 distinct tasks evaluating reasoning capabilities over charts.
Extensive experiments on MMC-Benchmark reveal the limitations of existing LMMs
on correctly interpreting charts, even for the most recent GPT-4V model. Our
work provides an instruction-tuning methodology and benchmark to advance
multimodal understanding of charts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fuxiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wenlin Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianshu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1&quot;&gt;Kaiqiang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1&quot;&gt;Sangwoo Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yacoob_Y/0/1/0/all/0/1&quot;&gt;Yaser Yacoob&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10775">
<title>ToolTalk: Evaluating Tool-Usage in a Conversational Setting. (arXiv:2311.10775v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10775</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have displayed massive improvements in reasoning
and decision-making skills and can hold natural conversations with users. Many
recent works seek to augment LLM-based assistants with external tools so they
can access private or up-to-date information and carry out actions on behalf of
users. To better measure the performance of these assistants, this paper
introduces ToolTalk, a benchmark consisting of complex user intents requiring
multi-step tool usage specified through dialogue. ToolTalk contains 28 tools
grouped into 7 plugins, and includes a complete simulated implementation of
each tool, allowing for fully automated evaluation of assistants that rely on
execution feedback. ToolTalk also emphasizes tools that externally affect the
world rather than only tools for referencing or searching information. We
evaluate GPT-3.5 and GPT-4 on ToolTalk resulting in success rates of 26% and
50% respectively. Our analysis of the errors reveals three major categories and
suggests some future directions for improvement. We release ToolTalk at
https://github.com/microsoft/ToolTalk.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farn_N/0/1/0/all/0/1&quot;&gt;Nicholas Farn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_R/0/1/0/all/0/1&quot;&gt;Richard Shin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10776">
<title>Towards an Automatic AI Agent for Reaction Condition Recommendation in Chemical Synthesis. (arXiv:2311.10776v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2311.10776</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) for reaction condition optimization has become
an important topic in the pharmaceutical industry, given that a data-driven AI
model can assist drug discovery and accelerate reaction design. However,
existing AI models lack the chemical insights and real-time knowledge
acquisition abilities of experienced human chemists. This paper proposes a
Large Language Model (LLM) empowered AI agent to bridge this gap. We put forth
a novel three-phase paradigm and applied advanced intelligence-enhancement
methods like in-context learning and multi-LLM debate so that the AI agent can
borrow human insight and update its knowledge by searching the latest chemical
literature. Additionally, we introduce a novel Coarse-label Contrastive
Learning (CCL) based chemical fingerprint that greatly enhances the agent&apos;s
performance in optimizing the reaction condition. With the above efforts, the
proposed AI agent can autonomously generate the optimal reaction condition
recommendation without any human interaction. Further, the agent is highly
professional in terms of chemical reactions. It demonstrates close-to-human
performance and strong generalization capability in both dry-lab and wet-lab
experiments. As the first attempt in the chemical AI agent, this work goes a
step further in the field of &quot;AI for chemistry&quot; and opens up new possibilities
for computer-aided synthesis planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kexin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junyou Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kunyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yuyang Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jiahui Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiamin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guangyong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lanqing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jiezhong Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1&quot;&gt;Qun Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1&quot;&gt;Pheng Ann Heng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10779">
<title>Knowledge Plugins: Enhancing Large Language Models for Domain-Specific Recommendations. (arXiv:2311.10779v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2311.10779</link>
<description rdf:parseType="Literal">&lt;p&gt;The significant progress of large language models (LLMs) provides a promising
opportunity to build human-like systems for various practical applications.
However, when applied to specific task domains, an LLM pre-trained on a
general-purpose corpus may exhibit a deficit or inadequacy in two types of
domain-specific knowledge. One is a comprehensive set of domain data that is
typically large-scale and continuously evolving. The other is specific working
patterns of this domain reflected in the data. The absence or inadequacy of
such knowledge impacts the performance of the LLM. In this paper, we propose a
general paradigm that augments LLMs with DOmain-specific KnowledgE to enhance
their performance on practical applications, namely DOKE. This paradigm relies
on a domain knowledge extractor, working in three steps: 1) preparing effective
knowledge for the task; 2) selecting the knowledge for each specific sample;
and 3) expressing the knowledge in an LLM-understandable way. Then, the
extracted knowledge is incorporated through prompts, without any computational
cost of model fine-tuning. We instantiate the general paradigm on a widespread
application, i.e. recommender systems, where critical item attributes and
collaborative filtering signals are incorporated. Experimental results
demonstrate that DOKE can substantially improve the performance of LLMs in
specific domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jing Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1&quot;&gt;Jianxun Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiting Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1&quot;&gt;Xiaoyuan Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10780">
<title>Extending Neural Network Verification to a Larger Family of Piece-wise Linear Activation Functions. (arXiv:2311.10780v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10780</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we extend an available neural network verification technique
to support a wider class of piece-wise linear activation functions.
Furthermore, we extend the algorithms, which provide in their original form
exact respectively over-approximative results for bounded input sets
represented as start sets, to allow also unbounded input set. We implemented
our algorithms and demonstrated their effectiveness in some case studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antal_L/0/1/0/all/0/1&quot;&gt;L&amp;#xe1;szl&amp;#xf3; Antal&lt;/a&gt; (RWTH Aachen University), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masara_H/0/1/0/all/0/1&quot;&gt;Hana Masara&lt;/a&gt; (RWTH Aachen University), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abraham_E/0/1/0/all/0/1&quot;&gt;Erika &amp;#xc1;brah&amp;#xe1;m&lt;/a&gt; (RWTH Aachen University)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10781">
<title>Can Language Model Moderators Improve the Health of Online Discourse?. (arXiv:2311.10781v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10781</link>
<description rdf:parseType="Literal">&lt;p&gt;Human moderation of online conversation is essential to maintaining civility
and focus in a dialogue, but is challenging to scale and harmful to moderators.
The inclusion of sophisticated natural language generation modules as a force
multiplier aid moderators is a tantalizing prospect, but adequate evaluation
approaches have so far been elusive. In this paper, we establish a systematic
definition of conversational moderation effectiveness through a
multidisciplinary lens that incorporates insights from social science. We then
propose a comprehensive evaluation framework that uses this definition to asses
models&apos; moderation capabilities independently of human intervention. With our
framework, we conduct the first known study of conversational dialogue models
as moderators, finding that appropriately prompted models can provide specific
and fair feedback on toxic behavior but struggle to influence users to increase
their levels of respect and cooperation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1&quot;&gt;Hyundong Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shuai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1&quot;&gt;Taiwei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1&quot;&gt;Darpan Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rizk_B/0/1/0/all/0/1&quot;&gt;Basem Rizk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuyang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zixun Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_N/0/1/0/all/0/1&quot;&gt;Nuan Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gratch_J/0/1/0/all/0/1&quot;&gt;Jonathan Gratch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrara_E/0/1/0/all/0/1&quot;&gt;Emilio Ferrara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1&quot;&gt;Jonathan May&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10784">
<title>ExFake: Towards an Explainable Fake News Detection Based on Content and Social Context Information. (arXiv:2311.10784v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10784</link>
<description rdf:parseType="Literal">&lt;p&gt;ExFake is an explainable fake news detection system based on content and
context-level information. It is concerned with the veracity analysis of online
posts based on their content, social context (i.e., online users&apos; credibility
and historical behaviour), and data coming from trusted entities such as
fact-checking websites and named entities. Unlike state-of-the-art systems, an
Explainable AI (XAI) assistant is also adopted to help online social networks
(OSN) users develop good reflexes when faced with any doubted information that
spreads on social networks. The trustworthiness of OSN users is also addressed
by assigning a credibility score to OSN users, as OSN users are one of the main
culprits for spreading fake news. Experimental analysis on a real-world dataset
demonstrates that ExFake significantly outperforms other baseline methods for
fake news detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amri_S/0/1/0/all/0/1&quot;&gt;Sabrine Amri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boleilanga_H/0/1/0/all/0/1&quot;&gt;Henri-Cedric Mputu Boleilanga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aimeur_E/0/1/0/all/0/1&quot;&gt;Esma A&amp;#xef;meur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10786">
<title>A Systems-Theoretical Formalization of Closed Systems. (arXiv:2311.10786v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10786</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a lack of formalism for some key foundational concepts in systems
engineering. One of the most recently acknowledged deficits is the inadequacy
of systems engineering practices for engineering intelligent systems. In our
previous works, we proposed that closed systems precepts could be used to
accomplish a required paradigm shift for the systems engineering of intelligent
systems. However, to enable such a shift, formal foundations for closed systems
precepts that expand the theory of systems engineering are needed. The concept
of closure is a critical concept in the formalism underlying closed systems
precepts. In this paper, we provide formal, systems- and information-theoretic
definitions of closure to identify and distinguish different types of closed
systems. Then, we assert a mathematical framework to evaluate the subjective
formation of the boundaries and constraints of such systems. Finally, we argue
that engineering an intelligent system can benefit from appropriate closed and
open systems paradigms on multiple levels of abstraction of the system. In the
main, this framework will provide the necessary fundamentals to aid in systems
engineering of intelligent systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shadab_N/0/1/0/all/0/1&quot;&gt;Niloofar Shadab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cody_T/0/1/0/all/0/1&quot;&gt;Tyler Cody&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salado_A/0/1/0/all/0/1&quot;&gt;Alejandro Salado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beling_P/0/1/0/all/0/1&quot;&gt;Peter Beling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10788">
<title>Efficient Temporally-Aware DeepFake Detection using H.264 Motion Vectors. (arXiv:2311.10788v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10788</link>
<description rdf:parseType="Literal">&lt;p&gt;Video DeepFakes are fake media created with Deep Learning (DL) that
manipulate a person&apos;s expression or identity. Most current DeepFake detection
methods analyze each frame independently, ignoring inconsistencies and
unnatural movements between frames. Some newer methods employ optical flow
models to capture this temporal aspect, but they are computationally expensive.
In contrast, we propose using the related but often ignored Motion Vectors
(MVs) and Information Masks (IMs) from the H.264 video codec, to detect
temporal inconsistencies in DeepFakes. Our experiments show that this approach
is effective and has minimal computational costs, compared with per-frame
RGB-only methods. This could lead to new, real-time temporally-aware DeepFake
detection methods for video calls and streaming.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gronquist_P/0/1/0/all/0/1&quot;&gt;Peter Gr&amp;#xf6;nquist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yufan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qingyi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verardo_A/0/1/0/all/0/1&quot;&gt;Alessio Verardo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Susstrunk_S/0/1/0/all/0/1&quot;&gt;Sabine S&amp;#xfc;sstrunk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10792">
<title>Attention Mechanism for Lithium-Ion Battery Lifespan Prediction: Temporal and Cyclic Attention. (arXiv:2311.10792v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10792</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately predicting the lifespan of lithium-ion batteries (LIBs) is pivotal
for optimizing usage and preventing accidents. Previous studies in constructing
prediction models often relied on inputs challenging to measure in real-time
operations and failed to capture intra-cycle and inter-cycle data patterns,
essential features for accurate predictions, comprehensively. In this study, we
employ attention mechanisms (AM) to develop data-driven models for predicting
LIB lifespan using easily measurable inputs such as voltage, current,
temperature, and capacity data. The developed model integrates recurrent neural
network (RNN) and convolutional neural network (CNN) components, featuring two
types of attention mechanisms: temporal attention (TA) and cyclic attention
(CA). The inclusion of TA aims to identify important time steps within each
cycle by scoring the hidden states of the RNN, whereas CA strives to capture
key features of inter-cycle correlations through self-attention (SA). This
enhances model accuracy and elucidates critical features in the input data. To
validate our method, we apply it to publicly available cycling data consisting
of three batches of cycling modes. The calculated TA scores highlight the rest
phase as a key characteristic distinguishing LIB data among different batches.
Additionally, CA scores reveal variations in the importance of cycles across
batches. By leveraging CA scores, we explore the potential to reduce the number
of cycles in the input data. The single-head and multi-head attentions enable
us to decrease the input dimension from 100 to 50 and 30 cycles, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaewook Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heo_S/0/1/0/all/0/1&quot;&gt;Seongmin Heo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jay H. Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10793">
<title>Traffic Sign Interpretation in Real Road Scene. (arXiv:2311.10793v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10793</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing traffic sign-related works are dedicated to detecting and
recognizing part of traffic signs individually, which fails to analyze the
global semantic logic among signs and may convey inaccurate traffic
instruction. Following the above issues, we propose a traffic sign
interpretation (TSI) task, which aims to interpret global semantic interrelated
traffic signs (e.g.,~driving instruction-related texts, symbols, and guide
panels) into a natural language for providing accurate instruction support to
autonomous or assistant driving. Meanwhile, we design a multi-task learning
architecture for TSI, which is responsible for detecting and recognizing
various traffic signs and interpreting them into a natural language like a
human. Furthermore, the absence of a public TSI available dataset prompts us to
build a traffic sign interpretation dataset, namely TSI-CN. The dataset
consists of real road scene images, which are captured from the highway and the
urban way in China from a driver&apos;s perspective. It contains rich location
labels of texts, symbols, and guide panels, and the corresponding natural
language description labels. Experiments on TSI-CN demonstrate that the TSI
task is achievable and the TSI architecture can interpret traffic signs from
scenes successfully even if there is a complex semantic logic among signs. The
TSI-CN dataset and the source code of the TSI architecture will be publicly
available after the revision process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chuang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_K/0/1/0/all/0/1&quot;&gt;Kai Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mulin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Haozhao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1&quot;&gt;Tao Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Changxing Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1&quot;&gt;Han Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bingxuan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10796">
<title>Emotion-Aware Music Recommendation System: Enhancing User Experience Through Real-Time Emotional Context. (arXiv:2311.10796v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2311.10796</link>
<description rdf:parseType="Literal">&lt;p&gt;This study addresses the deficiency in conventional music recommendation
systems by focusing on the vital role of emotions in shaping users music
choices. These systems often disregard the emotional context, relying
predominantly on past listening behavior and failing to consider the dynamic
and evolving nature of users emotional preferences. This gap leads to several
limitations. Users may receive recommendations that do not match their current
mood, which diminishes the quality of their music experience. Furthermore,
without accounting for emotions, the systems might overlook undiscovered or
lesser-known songs that have a profound emotional impact on users. To combat
these limitations, this research introduces an AI model that incorporates
emotional context into the song recommendation process. By accurately detecting
users real-time emotions, the model can generate personalized song
recommendations that align with the users emotional state. This approach aims
to enhance the user experience by offering music that resonates with their
current mood, elicits the desired emotions, and creates a more immersive and
meaningful listening experience. By considering emotional context in the song
recommendation process, the proposed model offers an opportunity for a more
personalized and emotionally resonant musical journey.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babu_T/0/1/0/all/0/1&quot;&gt;Tina Babu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_R/0/1/0/all/0/1&quot;&gt;Rekha R Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+A_G/0/1/0/all/0/1&quot;&gt;Geetha A&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10797">
<title>TaCo: Enhancing Cross-Lingual Transfer for Low-Resource Languages in LLMs through Translation-Assisted Chain-of-Thought Processes. (arXiv:2311.10797v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10797</link>
<description rdf:parseType="Literal">&lt;p&gt;LLMs such as ChatGPT and PaLM can be utilized to train on a new language and
revitalize low-resource languages. However, it is evidently very costly to
pretrain pr fine-tune LLMs to adopt new languages. Another challenge is the
limitation of benchmark datasets and the metrics used to measure the
performance of models in multilingual settings. This paper proposes
cost-effective solutions to both of the aforementioned challenges. We introduce
the Multilingual Instruction-Tuning Dataset (MITS), which is comprised of the
translation of Alpaca-52K, Dolly-15K, and Vicuna Benchmark in 132 languages.
Also, we propose a new method called \emph{TaCo: Translation-Assisted
Cross-Linguality}, which make uses of translation in a chain-of-thought process
to instruction-tune LLMs on a new languages through a curriculum learning
process. As a proof of concept, we experimented with the instruction-tuned
Guanaco-33B model and performed further instruction tuning using the TaCo
method in three low-resource languages and one high-resource language. Our
results show that the TaCo method impresses the GPT-4 with 82% for a
low-resource language in the Vicuna Benchmark dataset, and boosts performance
by double in contrast to the performance of instruction tuning only. Our
results show that TaCo is a promising method for creating multilingual LLMs,
even for low-resource languages. We have released our datasets and the model
adapters, and encourage the research community to make use of these resources
towards advancing work on multilingual LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Upadhayay_B/0/1/0/all/0/1&quot;&gt;Bibek Upadhayay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behzadan_V/0/1/0/all/0/1&quot;&gt;Vahid Behzadan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10798">
<title>INSPECT: A Multimodal Dataset for Pulmonary Embolism Diagnosis and Prognosis. (arXiv:2311.10798v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10798</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthesizing information from multiple data sources plays a crucial role in
the practice of modern medicine. Current applications of artificial
intelligence in medicine often focus on single-modality data due to a lack of
publicly available, multimodal medical datasets. To address this limitation, we
introduce INSPECT, which contains de-identified longitudinal records from a
large cohort of patients at risk for pulmonary embolism (PE), along with ground
truth labels for multiple outcomes. INSPECT contains data from 19,402 patients,
including CT images, radiology report impression sections, and structured
electronic health record (EHR) data (i.e. demographics, diagnoses, procedures,
vitals, and medications). Using INSPECT, we develop and release a benchmark for
evaluating several baseline modeling approaches on a variety of important PE
related tasks. We evaluate image-only, EHR-only, and multimodal fusion models.
Trained models and the de-identified dataset are made available for
non-commercial use under a data use agreement. To the best of our knowledge,
INSPECT is the largest multimodal dataset integrating 3D medical imaging and
EHR for reproducible methods evaluation and research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shih-Cheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1&quot;&gt;Zepeng Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinberg_E/0/1/0/all/0/1&quot;&gt;Ethan Steinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_C/0/1/0/all/0/1&quot;&gt;Chia-Chun Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1&quot;&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1&quot;&gt;Curtis P. Langlotz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1&quot;&gt;Serena Yeung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Nigam H. Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fries_J/0/1/0/all/0/1&quot;&gt;Jason A. Fries&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10801">
<title>Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools. (arXiv:2311.10801v1 [q-fin.PM])</title>
<link>http://arxiv.org/abs/2311.10801</link>
<description rdf:parseType="Literal">&lt;p&gt;Portfolio management (PM) is a fundamental financial trading task, which
explores the optimal periodical reallocation of capitals into different stocks
to pursue long-term profits. Reinforcement learning (RL) has recently shown its
potential to train profitable agents for PM through interacting with financial
markets. However, existing work mostly focuses on fixed stock pools, which is
inconsistent with investors&apos; practical demand. Specifically, the target stock
pool of different investors varies dramatically due to their discrepancy on
market states and individual investors may temporally adjust stocks they desire
to trade (e.g., adding one popular stocks), which lead to customizable stock
pools (CSPs). Existing RL methods require to retrain RL agents even with a tiny
change of the stock pool, which leads to high computational cost and unstable
performance. To tackle this challenge, we propose EarnMore, a rEinforcement
leARNing framework with Maskable stOck REpresentation to handle PM with CSPs
through one-shot training in a global stock pool (GSP). Specifically, we first
introduce a mechanism to mask out the representation of the stocks outside the
target pool. Second, we learn meaningful stock representations through a
self-supervised masking and reconstruction process. Third, a re-weighting
mechanism is designed to make the portfolio concentrate on favorable stocks and
neglect the stocks outside the target pool. Through extensive experiments on 8
subset stock pools of the US stock market, we demonstrate that EarnMore
significantly outperforms 14 state-of-the-art baselines in terms of 6 popular
financial metrics with over 40% improvement on profit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wentao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10804">
<title>A Study on Altering the Latent Space of Pretrained Text to Speech Models for Improved Expressiveness. (arXiv:2311.10804v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10804</link>
<description rdf:parseType="Literal">&lt;p&gt;This report explores the challenge of enhancing expressiveness control in
Text-to-Speech (TTS) models by augmenting a frozen pretrained model with a
Diffusion Model that is conditioned on joint semantic audio/text embeddings.
The paper identifies the challenges encountered when working with a VAE-based
TTS model and evaluates different image-to-image methods for altering latent
speech features. Our results offer valuable insights into the complexities of
adding expressiveness control to TTS systems and open avenues for future
research in this direction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogel_M/0/1/0/all/0/1&quot;&gt;Mathias Vogel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10805">
<title>Towards a Standardized Reinforcement Learning Framework for AAM Contingency Management. (arXiv:2311.10805v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10805</link>
<description rdf:parseType="Literal">&lt;p&gt;Advanced Air Mobility (AAM) is the next generation of air transportation that
includes new entrants such as electric vertical takeoff and landing (eVTOL)
aircraft, increasingly autonomous flight operations, and small UAS package
delivery. With these new vehicles and operational concepts comes a desire to
increase densities far beyond what occurs today in and around urban areas, to
utilize new battery technology, and to move toward more autonomously-piloted
aircraft. To achieve these goals, it becomes essential to introduce new safety
management system capabilities that can rapidly assess risk as it evolves
across a span of complex hazards and, if necessary, mitigate risk by executing
appropriate contingencies via supervised or automated decision-making during
flights. Recently, reinforcement learning has shown promise for real-time
decision making across a wide variety of applications including contingency
management. In this work, we formulate the contingency management problem as a
Markov Decision Process (MDP) and integrate the contingency management MDP into
the AAM-Gym simulation framework. This enables rapid prototyping of
reinforcement learning algorithms and evaluation of existing systems, thus
providing a community benchmark for future algorithm development. We report
baseline statistical information for the environment and provide example
performance metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_L/0/1/0/all/0/1&quot;&gt;Luis E. Alvarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brittain_M/0/1/0/all/0/1&quot;&gt;Marc W. Brittain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breeden_K/0/1/0/all/0/1&quot;&gt;Kara Breeden&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10806">
<title>SEA++: Multi-Graph-based High-Order Sensor Alignment for Multivariate Time-Series Unsupervised Domain Adaptation. (arXiv:2311.10806v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10806</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised Domain Adaptation (UDA) methods have been successful in reducing
label dependency by minimizing the domain discrepancy between a labeled source
domain and an unlabeled target domain. However, these methods face challenges
when dealing with Multivariate Time-Series (MTS) data. MTS data typically
consist of multiple sensors, each with its own unique distribution. This
characteristic makes it hard to adapt existing UDA methods, which mainly focus
on aligning global features while overlooking the distribution discrepancies at
the sensor level, to reduce domain discrepancies for MTS data. To address this
issue, a practical domain adaptation scenario is formulated as Multivariate
Time-Series Unsupervised Domain Adaptation (MTS-UDA). In this paper, we propose
SEnsor Alignment (SEA) for MTS-UDA, aiming to reduce domain discrepancy at both
the local and global sensor levels. At the local sensor level, we design
endo-feature alignment, which aligns sensor features and their correlations
across domains. To reduce domain discrepancy at the global sensor level, we
design exo-feature alignment that enforces restrictions on global sensor
features. We further extend SEA to SEA++ by enhancing the endo-feature
alignment. Particularly, we incorporate multi-graph-based high-order alignment
for both sensor features and their correlations. Extensive empirical results
have demonstrated the state-of-the-art performance of our SEA and SEA++ on
public MTS datasets for MTS-UDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yucheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuecong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianfei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Min Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoli Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Lihua Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhenghua Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10807">
<title>SENetV2: Aggregated dense layer for channelwise and global representations. (arXiv:2311.10807v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10807</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNNs) have revolutionized image classification
by extracting spatial features and enabling state-of-the-art accuracy in
vision-based tasks. The squeeze and excitation network proposed module gathers
channelwise representations of the input. Multilayer perceptrons (MLP) learn
global representation from the data and in most image classification models
used to learn extracted features of the image. In this paper, we introduce a
novel aggregated multilayer perceptron, a multi-branch dense layer, within the
Squeeze excitation residual module designed to surpass the performance of
existing architectures. Our approach leverages a combination of squeeze
excitation network module with dense layers. This fusion enhances the network&apos;s
ability to capture channel-wise patterns and have global knowledge, leading to
a better feature representation. This proposed model has a negligible increase
in parameters when compared to SENet. We conduct extensive experiments on
benchmark datasets to validate the model and compare them with established
architectures. Experimental results demonstrate a remarkable increase in the
classification accuracy of the proposed model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanan_M/0/1/0/all/0/1&quot;&gt;Mahendran Narayanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10809">
<title>Extracting periodontitis diagnosis in clinical notes with RoBERTa and regular expression. (arXiv:2311.10809v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10809</link>
<description rdf:parseType="Literal">&lt;p&gt;This study aimed to utilize text processing and natural language processing
(NLP) models to mine clinical notes for the diagnosis of periodontitis and to
evaluate the performance of a named entity recognition (NER) model on different
regular expression (RE) methods. Two complexity levels of RE methods were used
to extract and generate the training data. The SpaCy package and RoBERTa
transformer models were used to build the NER model and evaluate its
performance with the manual-labeled gold standards. The comparison of the RE
methods with the gold standard showed that as the complexity increased in the
RE algorithms, the F1 score increased from 0.3-0.4 to around 0.9. The NER
models demonstrated excellent predictions, with the simple RE method showing
0.84-0.92 in the evaluation metrics, and the advanced and combined RE method
demonstrating 0.95-0.99 in the evaluation. This study provided an example of
the benefit of combining NER methods and NLP models in extracting target
information from free-text to structured data and fulfilling the need for
missing diagnoses from unstructured notes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1&quot;&gt;Yao-Shun Chuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chun-Teh Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brandon_R/0/1/0/all/0/1&quot;&gt;Ryan Brandon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Trung Duong Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tokede_O/0/1/0/all/0/1&quot;&gt;Oluwabunmi Tokede&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walji_M/0/1/0/all/0/1&quot;&gt;Muhammad F. Walji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xiaoqian Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10810">
<title>Use GPT-J Prompt Generation with RoBERTa for NER Models on Diagnosis Extraction of Periodontal Diagnosis from Electronic Dental Records. (arXiv:2311.10810v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10810</link>
<description rdf:parseType="Literal">&lt;p&gt;This study explored the usability of prompt generation on named entity
recognition (NER) tasks and the performance in different settings of the
prompt. The prompt generation by GPT-J models was utilized to directly test the
gold standard as well as to generate the seed and further fed to the RoBERTa
model with the spaCy package. In the direct test, a lower ratio of negative
examples with higher numbers of examples in prompt achieved the best results
with a F1 score of 0.72. The performance revealed consistency, 0.92-0.97 in the
F1 score, in all settings after training with the RoBERTa model. The study
highlighted the importance of seed quality rather than quantity in feeding NER
models. This research reports on an efficient and accurate way to mine clinical
notes for periodontal diagnoses, allowing researchers to easily and quickly
build a NER model with the prompt generation approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1&quot;&gt;Yao-Shun Chuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xiaoqian Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chun-Teh Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brandon_R/0/1/0/all/0/1&quot;&gt;Ryan Brandon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1&quot;&gt;Duong Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tokede_O/0/1/0/all/0/1&quot;&gt;Oluwabunmi Tokede&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walji_M/0/1/0/all/0/1&quot;&gt;Muhammad F. Walji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10811">
<title>A novel post-hoc explanation comparison metric and applications. (arXiv:2311.10811v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10811</link>
<description rdf:parseType="Literal">&lt;p&gt;Explanatory systems make the behavior of machine learning models more
transparent, but are often inconsistent. To quantify the differences between
explanatory systems, this paper presents the Shreyan Distance, a novel metric
based on the weighted difference between ranked feature importance lists
produced by such systems. This paper uses the Shreyan Distance to compare two
explanatory systems, SHAP and LIME, for both regression and classification
learning tasks. Because we find that the average Shreyan Distance varies
significantly between these two tasks, we conclude that consistency between
explainers not only depends on inherent properties of the explainers
themselves, but also the type of learning task. This paper further contributes
the XAISuite library, which integrates the Shreyan distance algorithm into
machine learning pipelines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1&quot;&gt;Shreyan Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilpin_L/0/1/0/all/0/1&quot;&gt;Leilani Gilpin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10813">
<title>A Language Agent for Autonomous Driving. (arXiv:2311.10813v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10813</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-level driving is an ultimate goal of autonomous driving. Conventional
approaches formulate autonomous driving as a perception-prediction-planning
framework, yet their systems do not capitalize on the inherent reasoning
ability and experiential knowledge of humans. In this paper, we propose a
fundamental paradigm shift from current pipelines, exploiting Large Language
Models (LLMs) as a cognitive agent to integrate human-like intelligence into
autonomous driving systems. Our approach, termed Agent-Driver, transforms the
traditional autonomous driving pipeline by introducing a versatile tool library
accessible via function calls, a cognitive memory of common sense and
experiential knowledge for decision-making, and a reasoning engine capable of
chain-of-thought reasoning, task planning, motion planning, and
self-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive
common sense and robust reasoning capabilities, thus enabling a more nuanced,
human-like approach to autonomous driving. We evaluate our approach on the
large-scale nuScenes benchmark, and extensive experiments substantiate that our
Agent-Driver significantly outperforms the state-of-the-art driving methods by
a large margin. Our approach also demonstrates superior interpretability and
few-shot learning ability to these methods. Project page:
\href{https://github.com/USC-GVL/Agent-Driver/blob/main/index.html}{here}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1&quot;&gt;Jiageng Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Junjie Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yuxi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1&quot;&gt;Marco Pavone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10832">
<title>Exploring Machine Learning Models for Federated Learning: A Review of Approaches, Performance, and Limitations. (arXiv:2311.10832v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10832</link>
<description rdf:parseType="Literal">&lt;p&gt;In the growing world of artificial intelligence, federated learning is a
distributed learning framework enhanced to preserve the privacy of individuals&apos;
data. Federated learning lays the groundwork for collaborative research in
areas where the data is sensitive. Federated learning has several implications
for real-world problems. In times of crisis, when real-time decision-making is
critical, federated learning allows multiple entities to work collectively
without sharing sensitive data. This distributed approach enables us to
leverage information from multiple sources and gain more diverse insights. This
paper is a systematic review of the literature on privacy-preserving machine
learning in the last few years based on the Preferred Reporting Items for
Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Specifically, we have
presented an extensive review of supervised/unsupervised machine learning
algorithms, ensemble methods, meta-heuristic approaches, blockchain technology,
and reinforcement learning used in the framework of federated learning, in
addition to an overview of federated learning applications. This paper reviews
the literature on the components of federated learning and its applications in
the last few years. The main purpose of this work is to provide researchers and
practitioners with a comprehensive overview of federated learning from the
machine learning point of view. A discussion of some open problems and future
research directions in federated learning is also provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jafarigol_E/0/1/0/all/0/1&quot;&gt;Elaheh Jafarigol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trafalis_T/0/1/0/all/0/1&quot;&gt;Theodore Trafalis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razzaghi_T/0/1/0/all/0/1&quot;&gt;Talayeh Razzaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zamankhani_M/0/1/0/all/0/1&quot;&gt;Mona Zamankhani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10840">
<title>Integration and Implementation Strategies for AI Algorithm Deployment with Smart Routing Rules and Workflow Management. (arXiv:2311.10840v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10840</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper reviews the challenges hindering the widespread adoption of
artificial intelligence (AI) solutions in the healthcare industry, focusing on
computer vision applications for medical imaging, and how interoperability and
enterprise-grade scalability can be used to address these challenges. The
complex nature of healthcare workflows, intricacies in managing large and
secure medical imaging data, and the absence of standardized frameworks for AI
development pose significant barriers and require a new paradigm to address
them. The role of interoperability is examined in this paper as a crucial
factor in connecting disparate applications within healthcare workflows.
Standards such as DICOM, Health Level 7 HL7, and Integrating the Healthcare
Enterprise (IHE) are highlighted as foundational for common imaging workflows.
A specific focus is placed on the role of DICOM gateways, with Laurel Bridge
leading transformational efforts in this area. To drive enterprise scalability,
new tools are needed. Project MONAI, established in 2019, is introduced as an
initiative aiming to redefine the development of medical AI applications. The
MONAI Deploy App SDK, a component of Project MONAI, is identified as a key tool
in simplifying the packaging and deployment process, enabling repeatable,
scalable, and standardized deployment patterns for AI applications. The
abstract underscores the potential impact of successful AI adoption in
healthcare, offering physicians both life-saving and time-saving insights and
driving efficiencies in radiology department workflows. The collaborative
efforts between academia and industry, exemplified by collaborations with
organizations like NVIDIA and Laurel Bridge, are emphasized as essential for
advancing the adoption of healthcare AI solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erdal_B/0/1/0/all/0/1&quot;&gt;Barbaros Selnur Erdal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1&quot;&gt;Vikash Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demirer_M/0/1/0/all/0/1&quot;&gt;Mutlu Demirer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fair_K/0/1/0/all/0/1&quot;&gt;Kim H. Fair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+White_R/0/1/0/all/0/1&quot;&gt;Richard D. White&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blair_J/0/1/0/all/0/1&quot;&gt;Jeff Blair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deichert_B/0/1/0/all/0/1&quot;&gt;Barbara Deichert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lafleur_L/0/1/0/all/0/1&quot;&gt;Laurie Lafleur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1&quot;&gt;Ming Melvin Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bericat_D/0/1/0/all/0/1&quot;&gt;David Bericat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Genereaux_B/0/1/0/all/0/1&quot;&gt;Brad Genereaux&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10844">
<title>Artificial Intelligence in Fetal Resting-State Functional MRI Brain Segmentation: A Comparative Analysis of 3D UNet, VNet, and HighRes-Net Models. (arXiv:2311.10844v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2311.10844</link>
<description rdf:parseType="Literal">&lt;p&gt;Introduction: Fetal resting-state functional magnetic resonance imaging
(rs-fMRI) is a rapidly evolving field that provides valuable insight into brain
development before birth. Accurate segmentation of the fetal brain from the
surrounding tissue in nonstationary 3D brain volumes poses a significant
challenge in this domain. Current available tools have 0.15 accuracy. Aim: This
study introduced a novel application of artificial intelligence (AI) for
automated brain segmentation in fetal brain fMRI, magnetic resonance imaging
(fMRI). Open datasets were employed to train AI models, assess their
performance, and analyze their capabilities and limitations in addressing the
specific challenges associated with fetal brain fMRI segmentation. Method: We
utilized an open-source fetal functional MRI (fMRI) dataset consisting of 160
cases (reference: fetal-fMRI - OpenNeuro). An AI model for fMRI segmentation
was developed using a 5-fold cross-validation methodology. Three AI models were
employed: 3D UNet, VNet, and HighResNet. Optuna, an automated
hyperparameter-tuning tool, was used to optimize these models. Results and
Discussion: The Dice scores of the three AI models (VNet, UNet, and
HighRes-net) were compared, including a comparison between manually tuned and
automatically tuned models using Optuna. Our findings shed light on the
performance of different AI models for fetal resting-state fMRI brain
segmentation. Although the VNet model showed promise in this application,
further investigation is required to fully explore the potential and
limitations of each model, including the HighRes-net model. This study serves
as a foundation for further extensive research into the applications of AI in
fetal brain fMRI segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Vahedifard_F/0/1/0/all/0/1&quot;&gt;Farzan Vahedifard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuchu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kocak_M/0/1/0/all/0/1&quot;&gt;Mehmet Kocak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ai_H/0/1/0/all/0/1&quot;&gt;H. Asher Ai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Supanich_M/0/1/0/all/0/1&quot;&gt;Mark Supanich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sica%2E_C/0/1/0/all/0/1&quot;&gt;Christopher Sica.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Marathu_K/0/1/0/all/0/1&quot;&gt;Kranthi K Marathu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Adler_S/0/1/0/all/0/1&quot;&gt;Seth Adler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Orouskhani_M/0/1/0/all/0/1&quot;&gt;Maysam Orouskhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Byrd_S/0/1/0/all/0/1&quot;&gt;Sharon Byrd&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10856">
<title>Exploring the Consistency, Quality and Challenges in Manual and Automated Coding of Free-text Diagnoses from Hospital Outpatient Letters. (arXiv:2311.10856v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10856</link>
<description rdf:parseType="Literal">&lt;p&gt;Coding of unstructured clinical free-text to produce interoperable structured
data is essential to improve direct care, support clinical communication and to
enable clinical research.However, manual clinical coding is difficult and time
consuming, which motivates the development and use of natural language
processing for automated coding. This work evaluates the quality and
consistency of both manual and automated clinical coding of diagnoses from
hospital outpatient letters. Using 100 randomly selected letters, two human
clinicians performed coding of diagnosis lists to SNOMED CT. Automated coding
was also performed using IMO&apos;s Concept Tagger. A gold standard was constructed
by a panel of clinicians from a subset of the annotated diagnoses. This was
used to evaluate the quality and consistency of both manual and automated
coding via (1) a distance-based metric, treating SNOMED CT as a graph, and (2)
a qualitative metric agreed upon by the panel of clinicians. Correlation
between the two metrics was also evaluated. Comparing human and
computer-generated codes to the gold standard, the results indicate that humans
slightly out-performed automated coding, while both performed notably better
when there was only a single diagnosis contained in the free-text description.
Automated coding was considered acceptable by the panel of clinicians in
approximately 90% of cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Del_Pinto_W/0/1/0/all/0/1&quot;&gt;Warren Del-Pinto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demetriou_G/0/1/0/all/0/1&quot;&gt;George Demetriou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jani_M/0/1/0/all/0/1&quot;&gt;Meghna Jani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1&quot;&gt;Rikesh Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gray_L/0/1/0/all/0/1&quot;&gt;Leanne Gray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bulcock_A/0/1/0/all/0/1&quot;&gt;Alex Bulcock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peek_N/0/1/0/all/0/1&quot;&gt;Niels Peek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanter_A/0/1/0/all/0/1&quot;&gt;Andrew S. Kanter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dixon_W/0/1/0/all/0/1&quot;&gt;William G Dixon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1&quot;&gt;Goran Nenadic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10862">
<title>Formal concept analysis for evaluating intrinsic dimension of a natural language. (arXiv:2311.10862v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10862</link>
<description rdf:parseType="Literal">&lt;p&gt;Some results of a computational experiment for determining the intrinsic
dimension of linguistic varieties for the Bengali and Russian languages are
presented. At the same time, both sets of words and sets of bigrams in these
languages were considered separately. The method used to solve this problem was
based on formal concept analysis algorithms. It was found that the intrinsic
dimensions of these languages are significantly less than the dimensions used
in popular neural network models in natural language processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuznetsov_S/0/1/0/all/0/1&quot;&gt;Sergei O. Kuznetsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gromov_V/0/1/0/all/0/1&quot;&gt;Vasilii A. Gromov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borodin_N/0/1/0/all/0/1&quot;&gt;Nikita S. Borodin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Divavin_A/0/1/0/all/0/1&quot;&gt;Andrei M. Divavin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10863">
<title>Verified Compositional Neuro-Symbolic Control for Stochastic Systems with Temporal Logic Tasks. (arXiv:2311.10863v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.10863</link>
<description rdf:parseType="Literal">&lt;p&gt;Several methods have been proposed recently to learn neural network (NN)
controllers for autonomous agents, with unknown and stochastic dynamics, tasked
with complex missions captured by Linear Temporal Logic (LTL). Due to the
sample-inefficiency of the majority of these works, compositional learning
methods have been proposed decomposing the LTL specification into smaller
sub-tasks. Then, separate controllers are learned and composed to satisfy the
original task. A key challenge within these approaches is that they often lack
safety guarantees or the provided guarantees are impractical. This paper aims
to address this challenge. Particularly, we consider autonomous systems with
unknown and stochastic dynamics and LTL-encoded tasks. We assume that the
system is equipped with a finite set of base skills modeled by trained NN
feedback controllers. Our goal is to check if there exists a temporal
composition of the trained NN controllers - and if so, to compute it - that
will yield a composite system behavior that satisfies the assigned LTL task
with probability one. We propose a new approach that relies on a novel
integration of automata theory and data-driven reachability analysis tools for
NN-controlled stochastic systems. The resulting neuro-symbolic controller
allows the agent to generate safe behaviors for unseen complex temporal logic
tasks in a zero-shot fashion by leveraging its base skills. We show correctness
of the proposed method and we provide conditions under which it is complete. To
the best of our knowledge, this is the first work that designs verified
temporal compositions of NN controllers for unknown and stochastic systems.
Finally, we provide extensive numerical simulations and hardware experiments on
robot navigation tasks to demonstrate the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1&quot;&gt;Kaiyuan Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zihe Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kantaros_Y/0/1/0/all/0/1&quot;&gt;Yiannis Kantaros&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10892">
<title>The Hidden Linear Structure in Score-Based Models and its Application. (arXiv:2311.10892v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10892</link>
<description rdf:parseType="Literal">&lt;p&gt;Score-based models have achieved remarkable results in the generative
modeling of many domains. By learning the gradient of smoothed data
distribution, they can iteratively generate samples from complex distribution
e.g. natural images.
&lt;/p&gt;
&lt;p&gt;However, is there any universal structure in the gradient field that will
eventually be learned by any neural network? Here, we aim to find such
structures through a normative analysis of the score function.
&lt;/p&gt;
&lt;p&gt;First, we derived the closed-form solution to the scored-based model with a
Gaussian score. We claimed that for well-trained diffusion models, the learned
score at a high noise scale is well approximated by the linear score of
Gaussian. We demonstrated this through empirical validation of pre-trained
images diffusion model and theoretical analysis of the score function. This
finding enabled us to precisely predict the initial diffusion trajectory using
the analytical solution and to accelerate image sampling by 15-30\% by skipping
the initial phase without sacrificing image quality. Our finding of the linear
structure in the score-based model has implications for better model design and
data pre-processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Binxu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vastola_J/0/1/0/all/0/1&quot;&gt;John J. Vastola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10898">
<title>On Functional Activations in Deep Neural Networks. (arXiv:2311.10898v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10898</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: Deep neural networks have proven to be powerful computational
tools for modeling, prediction, and generation. However, the workings of these
models have generally been opaque. Recent work has shown that the performance
of some models are modulated by overlapping functional networks of connections
within the models. Here the techniques of functional neuroimaging are applied
to an exemplary large language model to probe its functional structure.
Methods: A series of block-designed task-based prompt sequences were generated
to probe the Facebook Galactica-125M model. Tasks included prompts relating to
political science, medical imaging, paleontology, archeology, pathology, and
random strings presented in an off/on/off pattern with prompts about other
random topics. For the generation of each output token, all layer output values
were saved to create an effective time series. General linear models were fit
to the data to identify layer output values which were active with the tasks.
Results: Distinct, overlapping networks were identified with each task. Most
overlap was observed between medical imaging and pathology networks. These
networks were repeatable across repeated performance of related tasks, and
correspondence of identified functional networks and activation in tasks not
used to define the functional networks was shown to accurately identify the
presented task. Conclusion: The techniques of functional neuroimaging can be
applied to deep neural networks as a means to probe their workings. Identified
functional networks hold the potential for use in model alignment, modulation
of model output, and identifying weights to target in fine-tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nencka_A/0/1/0/all/0/1&quot;&gt;Andrew S. Nencka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muftuler_L/0/1/0/all/0/1&quot;&gt;L. Tugan Muftuler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+LaViolette_P/0/1/0/all/0/1&quot;&gt;Peter LaViolette&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koch_K/0/1/0/all/0/1&quot;&gt;Kevin M. Koch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10905">
<title>Flexible Model Interpretability through Natural Language Model Editing. (arXiv:2311.10905v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10905</link>
<description rdf:parseType="Literal">&lt;p&gt;Model interpretability and model editing are crucial goals in the age of
large language models. Interestingly, there exists a link between these two
goals: if a method is able to systematically edit model behavior with regard to
a human concept of interest, this editor method can help make internal
representations more interpretable by pointing towards relevant representations
and systematically manipulating them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DOosterlinck_K/0/1/0/all/0/1&quot;&gt;Karel D&amp;#x27;Oosterlinck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1&quot;&gt;Thomas Demeester&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1&quot;&gt;Chris Develder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1&quot;&gt;Christopher Potts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10920">
<title>Understanding and Mitigating Classification Errors Through Interpretable Token Patterns. (arXiv:2311.10920v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10920</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art NLP methods achieve human-like performance on many tasks,
but make errors nevertheless. Characterizing these errors in easily
interpretable terms gives insight into whether a classifier is prone to making
systematic errors, but also gives a way to act and improve the classifier. We
propose to discover those patterns of tokens that distinguish correct and
erroneous predictions as to obtain global and interpretable descriptions for
arbitrary NLP classifiers. We formulate the problem of finding a succinct and
non-redundant set of such patterns in terms of the Minimum Description Length
principle. Through an extensive set of experiments, we show that our method,
Premise, performs well in practice. Unlike existing solutions, it recovers
ground truth, even on highly imbalanced data over large vocabularies. In VQA
and NER case studies, we confirm that it gives clear and actionable insight
into the systematic errors made by NLP classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hedderich_M/0/1/0/all/0/1&quot;&gt;Michael A. Hedderich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_J/0/1/0/all/0/1&quot;&gt;Jonas Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1&quot;&gt;Dietrich Klakow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vreeken_J/0/1/0/all/0/1&quot;&gt;Jilles Vreeken&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10921">
<title>Compact and Intuitive Airfoil Parameterization Method through Physics-aware Variational Autoencoder. (arXiv:2311.10921v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10921</link>
<description rdf:parseType="Literal">&lt;p&gt;Airfoil shape optimization plays a critical role in the design of
high-performance aircraft. However, the high-dimensional nature of airfoil
representation causes the challenging problem known as the &quot;curse of
dimensionality&quot;. To overcome this problem, numerous airfoil parameterization
methods have been developed, which can be broadly classified as
polynomial-based and data-driven approaches. Each of these methods has
desirable characteristics such as flexibility, parsimony, feasibility, and
intuitiveness, but a single approach that encompasses all of these attributes
has yet to be found. For example, polynomial-based methods struggle to balance
parsimony and flexibility, while data-driven methods lack in feasibility and
intuitiveness. In recent years, generative models, such as generative
adversarial networks and variational autoencoders, have shown promising
potential in airfoil parameterization. However, these models still face
challenges related to intuitiveness due to their black-box nature. To address
this issue, we developed a novel airfoil parameterization method using
physics-aware variational autoencoder. The proposed method not only explicitly
separates the generation of thickness and camber distributions to produce
smooth and non-intersecting airfoils, thereby improving feasibility, but it
also directly aligns its latent dimensions with geometric features of the
airfoil, significantly enhancing intuitiveness. Finally, extensive comparative
studies were performed to demonstrate the effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yu-Eop Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dawoon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yee_K/0/1/0/all/0/1&quot;&gt;Kwanjung Yee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10922">
<title>Explainable Product Classification for Customs. (arXiv:2311.10922v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10922</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of assigning internationally accepted commodity codes (aka HS codes)
to traded goods is a critical function of customs offices. Like court decisions
made by judges, this task follows the doctrine of precedent and can be
nontrivial even for experienced officers. Together with the Korea Customs
Service (KCS), we propose a first-ever explainable decision supporting model
that suggests the most likely subheadings (i.e., the first six digits) of the
HS code. The model also provides reasoning for its suggestion in the form of a
document that is interpretable by customs officers. We evaluated the model
using 5,000 cases that recently received a classification request. The results
showed that the top-3 suggestions made by our model had an accuracy of 93.9\%
when classifying 925 challenging subheadings. A user study with 32 customs
experts further confirmed that our algorithmic suggestions accompanied by
explainable reasonings, can substantially reduce the time and effort taken by
customs officers for classification reviews.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1&quot;&gt;Eunji Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sihyeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sundong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1&quot;&gt;Soyeon Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Heeja Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1&quot;&gt;Meeyoung Cha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10928">
<title>CAMRA: Copilot for AMR Annotation. (arXiv:2311.10928v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10928</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce CAMRA (Copilot for AMR Annotatations), a
cutting-edge web-based tool designed for constructing Abstract Meaning
Representation (AMR) from natural language text. CAMRA offers a novel approach
to deep lexical semantics annotation such as AMR, treating AMR annotation akin
to coding in programming languages. Leveraging the familiarity of programming
paradigms, CAMRA encompasses all essential features of existing AMR editors,
including example lookup, while going a step further by integrating Propbank
roleset lookup as an autocomplete feature within the tool. Notably, CAMRA
incorporates AMR parser models as coding co-pilots, greatly enhancing the
efficiency and accuracy of AMR annotators. To demonstrate the tool&apos;s
capabilities, we provide a live demo accessible at: https://camra.colorado.edu
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jon Z. Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1&quot;&gt;Shafiuddin Rehan Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonn_J/0/1/0/all/0/1&quot;&gt;Julia Bonn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wright_Bettner_K/0/1/0/all/0/1&quot;&gt;Kristin Wright-Bettner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palmer_M/0/1/0/all/0/1&quot;&gt;Martha Palmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1&quot;&gt;James H. Martin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10931">
<title>FLORIDA: Fake-looking Real Images Dataset. (arXiv:2311.10931v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10931</link>
<description rdf:parseType="Literal">&lt;p&gt;Although extensive research has been carried out to evaluate the
effectiveness of AI tools and models in detecting deep fakes, the question
remains unanswered regarding whether these models can accurately identify
genuine images that appear artificial. In this study, as an initial step
towards addressing this issue, we have curated a dataset of 510 genuine images
that exhibit a fake appearance and conducted an assessment using two AI models.
We show that two models exhibited subpar performance when applied to our
dataset. Additionally, our dataset can serve as a valuable tool for assessing
the ability of deep learning models to comprehend complex visual stimuli. We
anticipate that this research will stimulate further discussions and
investigations in this area. Our dataset is accessible at
https://github.com/aliborji/FLORIDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1&quot;&gt;Ali Borji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10932">
<title>Cognitive bias in large language models: Cautious optimism meets anti-Panglossian meliorism. (arXiv:2311.10932v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10932</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional discussions of bias in large language models focus on a
conception of bias closely tied to unfairness, especially as affecting
marginalized groups. Recent work raises the novel possibility of assessing the
outputs of large language models for a range of cognitive biases familiar from
research in judgment and decisionmaking. My aim in this paper is to draw two
lessons from recent discussions of cognitive bias in large language models:
cautious optimism about the prevalence of bias in current models coupled with
an anti-Panglossian willingness to concede the existence of some genuine biases
and work to reduce them. I draw out philosophical implications of this
discussion for the rationality of human cognitive biases as well as the role of
unrepresentative data in driving model biases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thorstad_D/0/1/0/all/0/1&quot;&gt;David Thorstad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10933">
<title>Representing visual classification as a linear combination of words. (arXiv:2311.10933v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10933</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainability is a longstanding challenge in deep learning, especially in
high-stakes domains like healthcare. Common explainability methods highlight
image regions that drive an AI model&apos;s decision. Humans, however, heavily rely
on language to convey explanations of not only &quot;where&quot; but &quot;what&quot;.
Additionally, most explainability approaches focus on explaining individual AI
predictions, rather than describing the features used by an AI model in
general. The latter would be especially useful for model and dataset auditing,
and potentially even knowledge generation as AI is increasingly being used in
novel tasks. Here, we present an explainability strategy that uses a
vision-language model to identify language-based descriptors of a visual
classification task. By leveraging a pre-trained joint embedding space between
images and text, our approach estimates a new classification task as a linear
combination of words, resulting in a weight for each word that indicates its
alignment with the vision-based classifier. We assess our approach using two
medical imaging classification tasks, where we find that the resulting
descriptors largely align with clinical knowledge despite a lack of
domain-specific language training. However, our approach also identifies the
potential for &apos;shortcut connections&apos; in the public datasets used. Towards a
functional measure of explainability, we perform a pilot reader study where we
find that the AI-identified words can enable non-expert humans to perform a
specialized medical task at a non-trivial level. Altogether, our results
emphasize the potential of using multimodal foundational models to deliver
intuitive, language-based explanations of visual tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1&quot;&gt;Shobhit Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Semenov_Y/0/1/0/all/0/1&quot;&gt;Yevgeniy R. Semenov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lotter_W/0/1/0/all/0/1&quot;&gt;William Lotter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10934">
<title>Case Repositories: Towards Case-Based Reasoning for AI Alignment. (arXiv:2311.10934v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10934</link>
<description rdf:parseType="Literal">&lt;p&gt;Case studies commonly form the pedagogical backbone in law, ethics, and many
other domains that face complex and ambiguous societal questions informed by
human values. Similar complexities and ambiguities arise when we consider how
AI should be aligned in practice: when faced with vast quantities of diverse
(and sometimes conflicting) values from different individuals and communities,
with whose values is AI to align, and how should AI do so? We propose a
complementary approach to constitutional AI alignment, grounded in ideas from
case-based reasoning (CBR), that focuses on the construction of policies
through judgments on a set of cases. We present a process to assemble such a
case repository by: 1) gathering a set of ``seed&apos;&apos; cases -- questions one may
ask an AI system -- in a particular domain from discussions in online
communities, 2) eliciting domain-specific key dimensions for cases through
workshops with domain experts, 3) using LLMs to generate variations of cases
not seen in the wild, and 4) engaging with the public to judge and improve
cases. We then discuss how such a case repository could assist in AI alignment,
both through directly acting as precedents to ground acceptable behaviors, and
as a medium for individuals and communities to engage in moral reasoning around
AI
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1&quot;&gt;K. J. Kevin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ze_Q/0/1/0/all/0/1&quot;&gt;Quan Ze&lt;/a&gt; (Jim) &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen/0/1/0/all/0/1&quot;&gt;Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheong_I/0/1/0/all/0/1&quot;&gt;Inyoung Cheong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_K/0/1/0/all/0/1&quot;&gt;King Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Amy X. Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10940">
<title>Practical Estimation of Ensemble Accuracy. (arXiv:2311.10940v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10940</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensemble learning combines several individual models to obtain better
generalization performance. In this work we present a practical method for
estimating the joint power of several classifiers which differs from existing
approaches by {\em not relying on labels}, hence enabling the work in
unsupervised setting of huge datasets. It differs from existing methods which
define a &quot;diversity measure&quot;.
&lt;/p&gt;
&lt;p&gt;The heart of the method is a combinatorial bound on the number of mistakes
the ensemble is likely to make. The bound can be efficiently approximated in
time linear in the number of samples. Thus allowing an efficient search for a
combination of classifiers that are likely to produce higher joint accuracy.
Moreover, having the bound applicable to unlabeled data makes it both accurate
and practical in modern setting of unsupervised learning. We demonstrate the
method on popular large-scale face recognition datasets which provide a useful
playground for fine-grain classification tasks using noisy data over many
classes.
&lt;/p&gt;
&lt;p&gt;The proposed framework fits neatly in trending practices of unsupervised
learning. It is a measure of the inherent independence of a set of classifiers
not relying on extra information such as another classifier or labeled data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haber_S/0/1/0/all/0/1&quot;&gt;Simi Haber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wexler_Y/0/1/0/all/0/1&quot;&gt;Yonatan Wexler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10945">
<title>An Empirical Bayes Framework for Open-Domain Dialogue Generation. (arXiv:2311.10945v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10945</link>
<description rdf:parseType="Literal">&lt;p&gt;To engage human users in meaningful conversation, open-domain dialogue agents
are required to generate diverse and contextually coherent dialogue. Despite
recent advancements, which can be attributed to the usage of pretrained
language models, the generation of diverse and coherent dialogue remains an
open research problem. A popular approach to address this issue involves the
adaptation of variational frameworks. However, while these approaches
successfully improve diversity, they tend to compromise on contextual
coherence. Hence, we propose the Bayesian Open-domain Dialogue with Empirical
Bayes (BODEB) framework, an empirical bayes framework for constructing an
Bayesian open-domain dialogue agent by leveraging pretrained parameters to
inform the prior and posterior parameter distributions. Empirical results show
that BODEB achieves better results in terms of both diversity and coherence
compared to variational frameworks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jing Yang Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kong Aik Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1&quot;&gt;Woon-Seng Gan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10947">
<title>RecExplainer: Aligning Large Language Models for Recommendation Model Interpretability. (arXiv:2311.10947v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2311.10947</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender systems are widely used in various online services, with
embedding-based models being particularly popular due to their expressiveness
in representing complex signals. However, these models often lack
interpretability, making them less reliable and transparent for both users and
developers. With the emergence of large language models (LLMs), we find that
their capabilities in language expression, knowledge-aware reasoning, and
instruction following are exceptionally powerful. Based on this, we propose a
new model interpretation approach for recommender systems, by using LLMs as
surrogate models and learn to mimic and comprehend target recommender models.
Specifically, we introduce three alignment methods: behavior alignment,
intention alignment, and hybrid alignment. Behavior alignment operates in the
language space, representing user preferences and item information as text to
learn the recommendation model&apos;s behavior; intention alignment works in the
latent space of the recommendation model, using user and item representations
to understand the model&apos;s behavior; hybrid alignment combines both language and
latent spaces for alignment training. To demonstrate the effectiveness of our
methods, we conduct evaluation from two perspectives: alignment effect, and
explanation generation ability on three public datasets. Experimental results
indicate that our approach effectively enables LLMs to comprehend the patterns
of recommendation models and generate highly credible recommendation
explanations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1&quot;&gt;Jianxun Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jing Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1&quot;&gt;Defu Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10953">
<title>HungerGist: An Interpretable Predictive Model for Food Insecurity. (arXiv:2311.10953v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10953</link>
<description rdf:parseType="Literal">&lt;p&gt;The escalating food insecurity in Africa, caused by factors such as war,
climate change, and poverty, demonstrates the critical need for advanced early
warning systems. Traditional methodologies, relying on expert-curated data
encompassing climate, geography, and social disturbances, often fall short due
to data limitations, hindering comprehensive analysis and potential discovery
of new predictive factors. To address this, this paper introduces &quot;HungerGist&quot;,
a multi-task deep learning model utilizing news texts and NLP techniques. Using
a corpus of over 53,000 news articles from nine African countries over four
years, we demonstrate that our model, trained solely on news data, outperforms
the baseline method trained on both traditional risk factors and human-curated
keywords. In addition, our method has the ability to detect critical texts that
contain interpretable signals known as &quot;gists.&quot; Moreover, our examination of
these gists indicates that this approach has the potential to reveal latent
factors that would otherwise remain concealed in unstructured texts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_Y/0/1/0/all/0/1&quot;&gt;Yongsu Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1&quot;&gt;Muheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yu-Ru Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zian Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10983">
<title>Multiple View Geometry Transformers for 3D Human Pose Estimation. (arXiv:2311.10983v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10983</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we aim to improve the 3D reasoning ability of Transformers in
multi-view 3D human pose estimation. Recent works have focused on end-to-end
learning-based transformer designs, which struggle to resolve geometric
information accurately, particularly during occlusion. Instead, we propose a
novel hybrid model, MVGFormer, which has a series of geometric and appearance
modules organized in an iterative manner. The geometry modules are
learning-free and handle all viewpoint-dependent 3D tasks geometrically which
notably improves the model&apos;s generalization ability. The appearance modules are
learnable and are dedicated to estimating 2D poses from image signals
end-to-end which enables them to achieve accurate estimates even when occlusion
occurs, leading to a model that is both accurate and generalizable to new
cameras and geometries. We evaluate our approach for both in-domain and
out-of-domain settings, where our model consistently outperforms
state-of-the-art methods, and especially does so by a significant margin in the
out-of-domain setting. We will release the code and models:
https://github.com/XunshanMan/MVGFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jialiang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chunyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Han Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1&quot;&gt;Steven L. Waslander&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11014">
<title>Lesion Search with Self-supervised Learning. (arXiv:2311.11014v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.11014</link>
<description rdf:parseType="Literal">&lt;p&gt;Content-based image retrieval (CBIR) with self-supervised learning (SSL)
accelerates clinicians&apos; interpretation of similar images without manual
annotations. We develop a CBIR from the contrastive learning SimCLR and
incorporate a generalized-mean (GeM) pooling followed by L2 normalization to
classify lesion types and retrieve similar images before clinicians&apos; analysis.
Results have shown improved performance. We additionally build an open-source
application for image analysis and retrieval. The application is easy to
integrate, relieving manual efforts and suggesting the potential to support
clinicians&apos; everyday activities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_K/0/1/0/all/0/1&quot;&gt;Kristin Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jiali Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haehn_D/0/1/0/all/0/1&quot;&gt;Daniel Haehn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11029">
<title>Geometric Data Augmentations to Mitigate Distribution Shifts in Pollen Classification from Microscopic Images. (arXiv:2311.11029v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.11029</link>
<description rdf:parseType="Literal">&lt;p&gt;Distribution shifts are characterized by differences between the training and
test data distributions. They can significantly reduce the accuracy of machine
learning models deployed in real-world scenarios. This paper explores the
distribution shift problem when classifying pollen grains from microscopic
images collected in the wild with a low-cost camera sensor. We leverage the
domain knowledge that geometric features are highly important for accurate
pollen identification and introduce two novel geometric image augmentation
techniques to significantly narrow the accuracy gap between the model
performance on the train and test datasets. In particular, we show that
Tenengrad and ImageToSketch filters are highly effective to balance the shape
and texture information while leaving out unimportant details that may confuse
the model. Extensive evaluations on various model architectures demonstrate a
consistent improvement of the model generalization to field data of up to 14%
achieved by the geometric augmentation techniques when compared to a wide range
of standard image augmentations. The approach is validated through an ablation
study using pollen hydration tests to recover the shape of dry pollen grains.
The proposed geometric augmentations also receive the highest scores according
to the affinity and diversity measures from the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1&quot;&gt;Nam Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saukh_O/0/1/0/all/0/1&quot;&gt;Olga Saukh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11030">
<title>Data Center Audio/Video Intelligence on Device (DAVID) -- An Edge-AI Platform for Smart-Toys. (arXiv:2311.11030v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.11030</link>
<description rdf:parseType="Literal">&lt;p&gt;An overview is given of the DAVID Smart-Toy platform, one of the first Edge
AI platform designs to incorporate advanced low-power data processing by neural
inference models co-located with the relevant image or audio sensors. There is
also on-board capability for in-device text-to-speech generation. Two
alternative embodiments are presented: a smart Teddy-bear, and a roving
dog-like robot. The platform offers a speech-driven user interface and can
observe and interpret user actions and facial expressions via its computer
vision sensor node. A particular benefit of this design is that no personally
identifiable information passes beyond the neural inference nodes thus
providing inbuilt compliance with data protection regulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cosache_G/0/1/0/all/0/1&quot;&gt;Gabriel Cosache&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salgado_F/0/1/0/all/0/1&quot;&gt;Francisco Salgado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rotariu_C/0/1/0/all/0/1&quot;&gt;Cosmin Rotariu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sterpu_G/0/1/0/all/0/1&quot;&gt;George Sterpu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1&quot;&gt;Rishabh Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corcoran_P/0/1/0/all/0/1&quot;&gt;Peter Corcoran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11039">
<title>Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment. (arXiv:2311.11039v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.11039</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthetic data is being used lately for training deep neural networks in
computer vision applications such as object detection, object segmentation and
6D object pose estimation. Domain randomization hereby plays an important role
in reducing the simulation to reality gap. However, this generalization might
not be effective in specialized domains like a production environment involving
complex assemblies. Either the individual parts, trained with synthetic images,
are integrated in much larger assemblies making them indistinguishable from
their counterparts and result in false positives or are partially occluded just
enough to give rise to false negatives. Domain knowledge is vital in these
cases and if conceived effectively while generating synthetic data, can show a
considerable improvement in bridging the simulation to reality gap. This paper
focuses on synthetic data generation procedures for parts and assemblies used
in a production environment. The basic procedures for synthetic data generation
and their various combinations are evaluated and compared on images captured in
a production environment, where results show up to 15% improvement using
combinations of basic procedures. Reducing the simulation to reality gap in
this way can aid to utilize the true potential of robot assisted production
using artificial intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawal_P/0/1/0/all/0/1&quot;&gt;Parth Rawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sompura_M/0/1/0/all/0/1&quot;&gt;Mrunal Sompura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hintze_W/0/1/0/all/0/1&quot;&gt;Wolfgang Hintze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11045">
<title>Orca 2: Teaching Small Language Models How to Reason. (arXiv:2311.11045v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.11045</link>
<description rdf:parseType="Literal">&lt;p&gt;Orca 1 learns from rich signals, such as explanation traces, allowing it to
outperform conventional instruction-tuned models on benchmarks like BigBench
Hard and AGIEval. In Orca 2, we continue exploring how improved training
signals can enhance smaller LMs&apos; reasoning abilities. Research on training
small LMs has often relied on imitation learning to replicate the output of
more capable models. We contend that excessive emphasis on imitation may
restrict the potential of smaller models. We seek to teach small LMs to employ
different solution strategies for different tasks, potentially different from
the one used by the larger model. For example, while larger models might
provide a direct answer to a complex task, smaller models may not have the same
capacity. In Orca 2, we teach the model various reasoning techniques
(step-by-step, recall then generate, recall-reason-generate, direct answer,
etc.). More crucially, we aim to help the model learn to determine the most
effective solution strategy for each task. We evaluate Orca 2 using a
comprehensive set of 15 diverse benchmarks (corresponding to approximately 100
tasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of
similar size and attains performance levels similar or better to those of
models 5-10x larger, as assessed on complex tasks that test advanced reasoning
abilities in zero-shot settings. We open-source Orca 2 to encourage further
research on the development, evaluation, and alignment of smaller LMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1&quot;&gt;Arindam Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corro_L/0/1/0/all/0/1&quot;&gt;Luciano Del Corro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahajan_S/0/1/0/all/0/1&quot;&gt;Shweti Mahajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Codas_A/0/1/0/all/0/1&quot;&gt;Andres Codas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simoes_C/0/1/0/all/0/1&quot;&gt;Clarisse Simoes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1&quot;&gt;Sahaj Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xuxi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razdaibiedina_A/0/1/0/all/0/1&quot;&gt;Anastasia Razdaibiedina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_E/0/1/0/all/0/1&quot;&gt;Erik Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_K/0/1/0/all/0/1&quot;&gt;Kriti Aggarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1&quot;&gt;Hamid Palangi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1&quot;&gt;Guoqing Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosset_C/0/1/0/all/0/1&quot;&gt;Corby Rosset&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khanpour_H/0/1/0/all/0/1&quot;&gt;Hamed Khanpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1&quot;&gt;Ahmed Awadallah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11055">
<title>Designing Interpretable ML System to Enhance Trustworthy AI in Healthcare: A Systematic Review of the Last Decade to A Proposed Robust Framework. (arXiv:2311.11055v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.11055</link>
<description rdf:parseType="Literal">&lt;p&gt;AI-based medical technologies, including wearables, telemedicine, LLMs, and
digital care twins, significantly impact healthcare. Ensuring AI results are
accurate and interpretable is crucial, especially for clinicians. This paper
reviews processes and challenges of interpretable ML (IML) and explainable AI
(XAI) in healthcare. Objectives include reviewing XAI processes, methods,
applications, and challenges, with a focus on quality control. The IML process
is classified into data pre-processing interpretability, interpretable
modeling, and post-processing interpretability. The paper aims to establish the
importance of robust interpretability in healthcare through experimental
results, providing insights for creating communicable clinician-AI tools.
Research questions, eligibility criteria, and goals were identified following
PRISMA and PICO methods. PubMed, Scopus, and Web of Science were systematically
searched using specific strings. The survey introduces a step-by-step roadmap
for implementing XAI in clinical applications, addressing existing gaps and
acknowledging XAI model limitations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasarian_E/0/1/0/all/0/1&quot;&gt;Elham Nasarian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1&quot;&gt;Roohallah Alizadehsani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acharyac_U/0/1/0/all/0/1&quot;&gt;U. Rajendra Acharyac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsui_d/0/1/0/all/0/1&quot;&gt;d Kwok-Leung Tsui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1904.10552">
<title>ML-KFHE: Multi-label ensemble classification algorithm exploiting sensor fusion properties of the Kalman filter. (arXiv:1904.10552v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1904.10552</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the success of ensemble classification methods in multi-class
classification problems, ensemble methods based on approaches other than
bagging have not been widely explored for multi-label classification problems.
The Kalman Filter-based Heuristic Ensemble (KFHE) is an ensemble method that
exploits the sensor fusion properties of the Kalman filter to combine several
classifier models, and that has been shown to be very effective. This work
proposes a multi-label version of KFHE, ML-KFHE, demonstrating the
effectiveness of the KFHE method on multi-label datasets. Two variants are
introduced based on the underlying component classifier algorithm,
ML-KFHE-HOMER, and ML-KFHE-CC which uses HOMER and Classifier Chain (CC) as the
underlying multi-label algorithms respectively. ML-KFHE-HOMER and ML-KFHE-CC
sequentially train multiple HOMER and CC multi-label classifiers and aggregate
their outputs using the sensor fusion properties of the Kalman filter.
Extensive experiments and detailed analysis were performed on thirteen
multi-label datasets and eight other algorithms, which included
state-of-the-art ensemble methods. The results show, for both versions, the
ML-KFHE framework improves the predictive performance significantly with
respect to bagged combinations of HOMER (named E-HOMER), also introduced in
this paper, and bagged combination of CC, Ensemble Classifier Chains (ECC),
thus demonstrating the effectiveness of ML-KFHE. Also, the ML-KFHE-HOMER
variant was found to perform consistently and significantly better than the
compared multi-label methods including existing approaches based on ensembles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pakrashi_A/0/1/0/all/0/1&quot;&gt;Arjun Pakrashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1&quot;&gt;Brian Mac Namee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2009.07448">
<title>Question Directed Graph Attention Network for Numerical Reasoning over Text. (arXiv:2009.07448v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2009.07448</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerical reasoning over texts, such as addition, subtraction, sorting and
counting, is a challenging machine reading comprehension task, since it
requires both natural language understanding and arithmetic computation. To
address this challenge, we propose a heterogeneous graph representation for the
context of the passage and question needed for such reasoning, and design a
question directed graph attention network to drive multi-step numerical
reasoning over this context graph. The code link is at:
https://github.com/emnlp2020qdgat/QDGAT
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kunlong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weidi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xingyi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiaochuan_Z/0/1/0/all/0/1&quot;&gt;Zou Xiaochuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Le Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Taifeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yuan Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1&quot;&gt;Wei Chu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.00388">
<title>Boolean proportions. (arXiv:2109.00388v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2109.00388</link>
<description rdf:parseType="Literal">&lt;p&gt;The author has recently introduced an abstract algebraic framework of
analogical proportions within the general setting of universal algebra. This
paper studies analogical proportions in the boolean domain consisting of two
elements 0 and 1 within his framework. It turns out that our notion of boolean
proportions coincides with two prominent models from the literature in
different settings. This means that we can capture two separate modellings of
boolean proportions within a single framework which is mathematically appealing
and provides further evidence for its robustness and applicability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antic_C/0/1/0/all/0/1&quot;&gt;Christian Anti&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.11482">
<title>Representations of epistemic uncertainty and awareness in data-driven strategies. (arXiv:2110.11482v7 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2110.11482</link>
<description rdf:parseType="Literal">&lt;p&gt;The diffusion of AI and big data is reshaping decision-making processes by
increasing the amount of information that supports decisions while reducing
direct interaction with data and empirical evidence. This paradigm shift
introduces new sources of uncertainty, as limited data observability results in
ambiguity and a lack of interpretability. The need for the proper analysis of
data-driven strategies motivates the search for new models that can describe
this type of bounded access to knowledge. This contribution presents a novel
theoretical model for uncertainty in knowledge representation and its transfer
mediated by agents. We provide a dynamical description of knowledge states by
endowing our model with a structure to compare and combine them. Specifically,
an update is represented through combinations, and its explainability is based
on its consistency in different dimensional representations. We look at
inequivalent knowledge representations in terms of multiplicity of inferences,
preference relations, and information measures. Furthermore, we define a formal
analogy with two scenarios that illustrate non-classical uncertainty in terms
of ambiguity (Ellsberg&apos;s model) and reasoning about knowledge mediated by other
agents observing data (Wigner&apos;s friend). Finally, we discuss some implications
of the proposed model for data-driven strategies, with special attention to
reasoning under uncertainty about business value dimensions and the design of
measurement tools for their assessment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Angelelli_M/0/1/0/all/0/1&quot;&gt;Mario Angelelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gervasi_M/0/1/0/all/0/1&quot;&gt;Massimiliano Gervasi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.11239">
<title>Diagnosing AI Explanation Methods with Folk Concepts of Behavior. (arXiv:2201.11239v6 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2201.11239</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate a formalism for the conditions of a successful explanation of
AI. We consider &quot;success&quot; to depend not only on what information the
explanation contains, but also on what information the human explainee
understands from it. Theory of mind literature discusses the folk concepts that
humans use to understand and generalize behavior. We posit that folk concepts
of behavior provide us with a &quot;language&quot; that humans understand behavior with.
We use these folk concepts as a framework of social attribution by the human
explainee - the information constructs that humans are likely to comprehend
from explanations - by introducing a blueprint for an explanatory narrative
(Figure 1) that explains AI behavior with these constructs. We then demonstrate
that many XAI methods today can be mapped to folk concepts of behavior in a
qualitative evaluation. This allows us to uncover their failure modes that
prevent current methods from explaining successfully - i.e., the information
constructs that are missing for any given XAI method, and whose inclusion can
decrease the likelihood of misunderstanding AI behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacovi_A/0/1/0/all/0/1&quot;&gt;Alon Jacovi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bastings_J/0/1/0/all/0/1&quot;&gt;Jasmijn Bastings&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1&quot;&gt;Sebastian Gehrmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1&quot;&gt;Yoav Goldberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filippova_K/0/1/0/all/0/1&quot;&gt;Katja Filippova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.02249">
<title>Learning Task Embeddings for Teamwork Adaptation in Multi-Agent Reinforcement Learning. (arXiv:2207.02249v2 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/2207.02249</link>
<description rdf:parseType="Literal">&lt;p&gt;Successful deployment of multi-agent reinforcement learning often requires
agents to adapt their behaviour. In this work, we discuss the problem of
teamwork adaptation in which a team of agents needs to adapt their policies to
solve novel tasks with limited fine-tuning. Motivated by the intuition that
agents need to be able to identify and distinguish tasks in order to adapt
their behaviour to the current task, we propose to learn multi-agent task
embeddings (MATE). These task embeddings are trained using an encoder-decoder
architecture optimised for reconstruction of the transition and reward
functions which uniquely identify tasks. We show that a team of agents is able
to adapt to novel tasks when provided with task embeddings. We propose three
MATE training paradigms: independent MATE, centralised MATE, and mixed MATE
which vary in the information used for the task encoding. We show that the
embeddings learned by MATE identify tasks and provide useful information which
agents leverage during adaptation to novel tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schafer_L/0/1/0/all/0/1&quot;&gt;Lukas Sch&amp;#xe4;fer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christianos_F/0/1/0/all/0/1&quot;&gt;Filippos Christianos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1&quot;&gt;Amos Storkey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1&quot;&gt;Stefano V. Albrecht&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.11349">
<title>Dynamic Memory-based Curiosity: A Bootstrap Approach for Exploration. (arXiv:2208.11349v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2208.11349</link>
<description rdf:parseType="Literal">&lt;p&gt;The sparsity of extrinsic rewards poses a serious challenge for reinforcement
learning (RL). Currently, many efforts have been made on curiosity which can
provide a representative intrinsic reward for effective exploration. However,
the challenge is still far from being solved. In this paper, we present a novel
curiosity for RL, named DyMeCu, which stands for Dynamic Memory-based
Curiosity. Inspired by human curiosity and information theory, DyMeCu consists
of a dynamic memory and dual online learners. The curiosity arouses if
memorized information can not deal with the current state, and the information
gap between dual learners can be formulated as the intrinsic reward for agents,
and then such state information can be consolidated into the dynamic memory.
Compared with previous curiosity methods, DyMeCu can better mimic human
curiosity with dynamic memory, and the memory module can be dynamically grown
based on a bootstrap paradigm with dual learners. On multiple benchmarks
including DeepMind Control Suite and Atari Suite, large-scale empirical
experiments are conducted and the results demonstrate that DyMeCu outperforms
competitive curiosity-based methods with or without extrinsic rewards. We will
release the code to enhance reproducibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zijian Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;YiYing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kele Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1&quot;&gt;Yuanzhao Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1&quot;&gt;Dawei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1&quot;&gt;Bo Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1&quot;&gt;XinJun Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huaimin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.09124">
<title>A Review of Intelligent Music Generation Systems. (arXiv:2211.09124v3 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2211.09124</link>
<description rdf:parseType="Literal">&lt;p&gt;With the introduction of ChatGPT, the public&apos;s perception of AI-generated
content (AIGC) has begun to reshape. Artificial intelligence has significantly
reduced the barrier to entry for non-professionals in creative endeavors,
enhancing the efficiency of content creation. Recent advancements have seen
significant improvements in the quality of symbolic music generation, which is
enabled by the use of modern generative algorithms to extract patterns implicit
in a piece of music based on rule constraints or a musical corpus.
Nevertheless, existing literature reviews tend to present a conventional and
conservative perspective on future development trajectories, with a notable
absence of thorough benchmarking of generative models. This paper provides a
survey and analysis of recent intelligent music generation techniques,
outlining their respective characteristics and discussing existing methods for
evaluation. Additionally, the paper compares the different characteristics of
music generation techniques in the East and West as well as analysing the
field&apos;s development prospects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Ziyi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hanwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1&quot;&gt;Junwei Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yi Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qidi Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.08966">
<title>Graph Learning and Its Advancements on Large Language Models: A Holistic Survey. (arXiv:2212.08966v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2212.08966</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph learning is a prevalent domain that endeavors to learn the intricate
relationships among nodes and the topological structure of graphs. Over the
years, graph learning has transcended from graph theory to graph data mining.
With the advent of representation learning, it has attained remarkable
performance in diverse scenarios. Owing to its extensive application prospects,
graph learning attracts copious attention. While some researchers have
accomplished impressive surveys on graph learning, they failed to connect
related objectives, methods, and applications in a more coherent way. As a
result, they did not encompass current ample scenarios and challenging problems
due to the rapid expansion of graph learning. Particularly, large language
models have recently had a disruptive effect on human life, but they also show
relative weakness in structured scenarios. The question of how to make these
models more powerful with graph learning remains open. Our survey focuses on
the most recent advancements in integrating graph learning with pre-trained
language models, specifically emphasizing their application within the domain
of large language models. Different from previous surveys on graph learning, we
provide a holistic review that analyzes current works from the perspective of
graph structure, and discusses the latest applications, trends, and challenges
in graph learning. Specifically, we commence by proposing a taxonomy and then
summarize the methods employed in graph learning. We then provide a detailed
elucidation of mainstream applications. Finally, we propose future directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1&quot;&gt;Shaopeng Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xingyan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1&quot;&gt;Fuzhen Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Ji Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1&quot;&gt;Fuji Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kou_G/0/1/0/all/0/1&quot;&gt;Gang Kou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.00111">
<title>Learning Universal Policies via Text-Guided Video Generation. (arXiv:2302.00111v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2302.00111</link>
<description rdf:parseType="Literal">&lt;p&gt;A goal of artificial intelligence is to construct an agent that can solve a
wide variety of tasks. Recent progress in text-guided image synthesis has
yielded models with an impressive ability to generate complex novel images,
exhibiting combinatorial generalization across domains. Motivated by this
success, we investigate whether such tools can be used to construct more
general-purpose agents. Specifically, we cast the sequential decision making
problem as a text-conditioned video generation problem, where, given a
text-encoded specification of a desired goal, a planner synthesizes a set of
future frames depicting its planned actions in the future, after which control
actions are extracted from the generated video. By leveraging text as the
underlying goal specification, we are able to naturally and combinatorially
generalize to novel goals. The proposed policy-as-video formulation can further
represent environments with different state and action spaces in a unified
space of images, which, for example, enables learning and generalization across
a variety of robot manipulation tasks. Finally, by leveraging pretrained
language embeddings and widely available videos from the internet, the approach
enables knowledge transfer through predicting highly realistic video plans for
real robots.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yilun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mengjiao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Hanjun Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nachum_O/0/1/0/all/0/1&quot;&gt;Ofir Nachum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1&quot;&gt;Dale Schuurmans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.05507">
<title>Language Decision Transformers with Exponential Tilt for Interactive Text Environments. (arXiv:2302.05507v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2302.05507</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-based game environments are challenging because agents must deal with
long sequences of text, execute compositional actions using text and learn from
sparse rewards. We address these challenges by proposing Language Decision
Transformers (LDTs), a framework that is based on transformer language models
and decision transformers (DTs). Our LDTs extend DTs with 3 components: (1)
exponential tilt to guide the agent towards high obtainable goals, (2) novel
goal conditioning methods yielding better results than the traditional
return-to-go (sum of all future rewards), and (3) a model of future
observations that improves agent performance. LDTs are the first to address
offline RL with DTs on these challenging games. Our experiments show that LDTs
achieve the highest scores among many different types of agents on some of the
most challenging Jericho games, such as Enchanter.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gontier_N/0/1/0/all/0/1&quot;&gt;Nicolas Gontier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1&quot;&gt;Pau Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1&quot;&gt;Issam Laradji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1&quot;&gt;David Vazquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1&quot;&gt;Christopher Pal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06476">
<title>Is ChatGPT a General-Purpose Natural Language Processing Task Solver?. (arXiv:2302.06476v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06476</link>
<description rdf:parseType="Literal">&lt;p&gt;Spurred by advancements in scale, large language models (LLMs) have
demonstrated the ability to perform a variety of natural language processing
(NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently,
the debut of ChatGPT has drawn a great deal of attention from the natural
language processing (NLP) community due to the fact that it can generate
high-quality responses to human input and self-correct previous mistakes based
on subsequent conversations. However, it is not yet known whether ChatGPT can
serve as a generalist model that can perform many NLP tasks zero-shot. In this
work, we empirically analyze the zero-shot learning ability of ChatGPT by
evaluating it on 20 popular NLP datasets covering 7 representative task
categories. With extensive empirical studies, we demonstrate both the
effectiveness and limitations of the current version of ChatGPT. We find that
ChatGPT performs well on many tasks favoring reasoning capabilities (e.g.,
arithmetic reasoning) while it still faces challenges when solving specific
tasks such as sequence tagging. We additionally provide in-depth analysis
through qualitative case studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1&quot;&gt;Chengwei Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Aston Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhuosheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1&quot;&gt;Michihiro Yasunaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Diyi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08143">
<title>Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?. (arXiv:2302.08143v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08143</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt tuning (PT) which only tunes the embeddings of an additional sequence
of tokens per task, keeping the pre-trained language model (PLM) frozen, has
shown remarkable performance in few-shot learning. Despite this, PT has been
shown to rely heavily on good initialization of the prompt embeddings. In this
work, we study meta prompt tuning (MPT) to systematically explore how
meta-learning can help improve (if it can) cross-task generalization in PT
through learning to initialize the prompt embeddings from other relevant tasks.
We empirically analyze a representative set of meta learning algorithms in a
wide range of adaptation settings with different source/target task
configurations on a large set of few-shot tasks. With extensive experiments and
analysis, we demonstrate the effectiveness of MPT. We find the improvement to
be significant particularly on classification tasks. For other kinds of tasks
such as question answering, we observe that while MPT can outperform PT in most
cases, it does not always outperform multi-task learning. We further provide an
in-depth analysis from the perspective of task similarity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1&quot;&gt;Chengwei Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Ruochen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1&quot;&gt;Shafiq Joty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11552">
<title>Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC. (arXiv:2302.11552v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11552</link>
<description rdf:parseType="Literal">&lt;p&gt;Since their introduction, diffusion models have quickly become the prevailing
approach to generative modeling in many domains. They can be interpreted as
learning the gradients of a time-varying sequence of log-probability density
functions. This interpretation has motivated classifier-based and
classifier-free guidance as methods for post-hoc control of diffusion models.
In this work, we build upon these ideas using the score-based interpretation of
diffusion models, and explore alternative ways to condition, modify, and reuse
diffusion models for tasks involving compositional generation and guidance. In
particular, we investigate why certain types of composition fail using current
techniques and present a number of solutions. We conclude that the sampler (not
the model) is responsible for this failure and propose new samplers, inspired
by MCMC, which enable successful compositional generation. Further, we propose
an energy-based parameterization of diffusion models which enables the use of
new compositional operators and more sophisticated, Metropolis-corrected
samplers. Intriguingly we find these samplers lead to notable improvements in
compositional generation across a wide set of problems such as
classifier-guided ImageNet modeling and compositional text-to-image generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yilun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durkan_C/0/1/0/all/0/1&quot;&gt;Conor Durkan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strudel_R/0/1/0/all/0/1&quot;&gt;Robin Strudel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dieleman_S/0/1/0/all/0/1&quot;&gt;Sander Dieleman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1&quot;&gt;Rob Fergus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1&quot;&gt;Jascha Sohl-Dickstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1&quot;&gt;Arnaud Doucet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grathwohl_W/0/1/0/all/0/1&quot;&gt;Will Grathwohl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.13335">
<title>Diffusion Model-Augmented Behavioral Cloning. (arXiv:2302.13335v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.13335</link>
<description rdf:parseType="Literal">&lt;p&gt;Imitation learning addresses the challenge of learning by observing an
expert&apos;s demonstrations without access to reward signals from environments.
Most existing imitation learning methods that do not require interacting with
environments either model the expert distribution as the conditional
probability p(a|s) (e.g., behavioral cloning, BC) or the joint probability p(s,
a). Despite its simplicity, modeling the conditional probability with BC
usually struggles with generalization. While modeling the joint probability can
lead to improved generalization performance, the inference procedure is often
time-consuming and the model can suffer from manifold overfitting. This work
proposes an imitation learning framework that benefits from modeling both the
conditional and joint probability of the expert distribution. Our proposed
diffusion model-augmented behavioral cloning (DBC) employs a diffusion model
trained to model expert behaviors and learns a policy to optimize both the BC
loss (conditional) and our proposed diffusion model loss (joint). DBC
outperforms baselines in various continuous control tasks in navigation, robot
arm manipulation, dexterous manipulation, and locomotion. We design additional
experiments to verify the limitations of modeling either the conditional
probability or the joint probability of the expert distribution as well as
compare different generative models. Ablation studies justify the effectiveness
of our design choices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hsiang-Chun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shang-Fu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_M/0/1/0/all/0/1&quot;&gt;Ming-Hao Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1&quot;&gt;Chun-Mao Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shao-Hua Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06597">
<title>Non-Orthogonal Multiple Access Enhanced Multi-User Semantic Communication. (arXiv:2303.06597v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06597</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic communication serves as a novel paradigm and attracts the broad
interest of researchers. One critical aspect of it is the multi-user semantic
communication theory, which can further promote its application to the
practical network environment. While most existing works focused on the design
of end-to-end single-user semantic transmission, a novel non-orthogonal
multiple access (NOMA)-based multi-user semantic communication system named
NOMASC is proposed in this paper. The proposed system can support semantic
tranmission of multiple users with diverse modalities of source information. To
avoid high demand for hardware, an asymmetric quantizer is employed at the end
of the semantic encoder for discretizing the continuous full-resolution
semantic feature. In addition, a neural network model is proposed for mapping
the discrete feature into self-learned symbols and accomplishing intelligent
multi-user detection (MUD) at the receiver. Simulation results demonstrate that
the proposed system holds good performance in non-orthogonal transmission of
multiple user signals and outperforms the other methods, especially at
low-to-medium SNRs. Moreover, it has high robustness under various simulation
settings and mismatched test scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weizhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Haotai Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1&quot;&gt;Chen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaodong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Ping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kaijun Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15445">
<title>IRFL: Image Recognition of Figurative Language. (arXiv:2303.15445v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15445</link>
<description rdf:parseType="Literal">&lt;p&gt;Figures of speech such as metaphors, similes, and idioms are integral parts
of human communication. They are ubiquitous in many forms of discourse,
allowing people to convey complex, abstract ideas and evoke emotion. As
figurative forms are often conveyed through multiple modalities (e.g., both
text and images), understanding multimodal figurative language is an important
AI challenge, weaving together profound vision, language, commonsense and
cultural knowledge.
&lt;/p&gt;
&lt;p&gt;In this work, we develop the Image Recognition of Figurative Language (IRFL)
dataset. We leverage human annotation and an automatic pipeline we created to
generate a multimodal dataset, and introduce two novel tasks as a benchmark for
multimodal figurative language understanding. We experimented with
state-of-the-art vision and language models and found that the best (22%)
performed substantially worse than humans (97%). We release our dataset,
benchmark, and code, in hopes of driving the development of models that can
better understand figurative language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yosef_R/0/1/0/all/0/1&quot;&gt;Ron Yosef&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1&quot;&gt;Dafna Shahaf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06657">
<title>On Practical Robust Reinforcement Learning: Practical Uncertainty Set and Double-Agent Algorithm. (arXiv:2305.06657v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06657</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust reinforcement learning (RRL) aims at seeking a robust policy to
optimize the worst case performance over an uncertainty set of Markov decision
processes (MDPs). This set contains some perturbed MDPs from a nominal MDP
(N-MDP) that generate samples for training, which reflects some potential
mismatches between training (i.e., N-MDP) and true environments. In this paper
we present an elaborated uncertainty set by excluding some implausible MDPs
from the existing sets. Under this uncertainty set, we develop a sample-based
RRL algorithm (named ARQ-Learning) for tabular setting and characterize its
finite-time error bound. Also, it is proved that ARQ-Learning converges as fast
as the standard Q-Learning and robust Q-Learning while ensuring better
robustness. We introduce an additional pessimistic agent which can tackle the
major bottleneck for the extension of ARQ-Learning into the cases with larger
or continuous state spaces. Incorporating this idea into RL algorithms, we
propose double-agent algorithms for model-free RRL. Via experiments, we
demonstrate the effectiveness of the proposed algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_U/0/1/0/all/0/1&quot;&gt;Ukjo Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Songnam Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11203">
<title>PDP: Parameter-free Differentiable Pruning is All You Need. (arXiv:2305.11203v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11203</link>
<description rdf:parseType="Literal">&lt;p&gt;DNN pruning is a popular way to reduce the size of a model, improve the
inference latency, and minimize the power consumption on DNN accelerators.
However, existing approaches might be too complex, expensive or ineffective to
apply to a variety of vision/language tasks, DNN architectures and to honor
structured pruning constraints. In this paper, we propose an efficient yet
effective train-time pruning scheme, Parameter-free Differentiable Pruning
(PDP), which offers state-of-the-art qualities in model size, accuracy, and
training cost. PDP uses a dynamic function of weights during training to
generate soft pruning masks for the weights in a parameter-free manner for a
given pruning target. While differentiable, the simplicity and efficiency of
PDP make it universal enough to deliver state-of-the-art
random/structured/channel pruning results on various vision and natural
language tasks. For example, for MobileNet-v1, PDP can achieve 68.2% top-1
ImageNet1k accuracy at 86.6% sparsity, which is 1.7% higher accuracy than those
from the state-of-the-art algorithms. Also, PDP yields over 83.1% accuracy on
Multi-Genre Natural Language Inference with 90% sparsity for BERT, while the
next best from the existing techniques shows 81.5% accuracy. In addition, PDP
can be applied to structured pruning, such as N:M pruning and channel pruning.
For 1:4 structured pruning of ResNet18, PDP improved the top-1 ImageNet1k
accuracy by over 3.6% over the state-of-the-art. For channel pruning of
ResNet50, PDP reduced the top-1 ImageNet1k accuracy by 0.6% from the
state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1&quot;&gt;Minsik Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adya_S/0/1/0/all/0/1&quot;&gt;Saurabh Adya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naik_D/0/1/0/all/0/1&quot;&gt;Devang Naik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17010">
<title>Let the Flows Tell: Solving Graph Combinatorial Optimization Problems with GFlowNets. (arXiv:2305.17010v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17010</link>
<description rdf:parseType="Literal">&lt;p&gt;Combinatorial optimization (CO) problems are often NP-hard and thus out of
reach for exact algorithms, making them a tempting domain to apply machine
learning methods. The highly structured constraints in these problems can
hinder either optimization or sampling directly in the solution space. On the
other hand, GFlowNets have recently emerged as a powerful machinery to
efficiently sample from composite unnormalized densities sequentially and have
the potential to amortize such solution-searching processes in CO, as well as
generate diverse solution candidates. In this paper, we design Markov decision
processes (MDPs) for different combinatorial problems and propose to train
conditional GFlowNets to sample from the solution space. Efficient training
techniques are also developed to benefit long-range credit assignment. Through
extensive experiments on a variety of different CO tasks with synthetic and
realistic data, we demonstrate that GFlowNet policies can efficiently find
high-quality solutions. Our implementation is open-sourced at
https://github.com/zdhNarsil/GFlowNet-CombOpt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dinghuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Hanjun Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malkin_N/0/1/0/all/0/1&quot;&gt;Nikolay Malkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1&quot;&gt;Aaron Courville&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Ling Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18399">
<title>On the impact of activation and normalization in obtaining isometric embeddings at initialization. (arXiv:2305.18399v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18399</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we explore the structure of the penultimate Gram matrix in
deep neural networks, which contains the pairwise inner products of outputs
corresponding to a batch of inputs. In several architectures it has been
observed that this Gram matrix becomes degenerate with depth at initialization,
which dramatically slows training. Normalization layers, such as batch or layer
normalization, play a pivotal role in preventing the rank collapse issue.
Despite promising advances, the existing theoretical results do not extend to
layer normalization, which is widely used in transformers, and can not
quantitatively characterize the role of non-linear activations. To bridge this
gap, we prove that layer normalization, in conjunction with activation layers,
biases the Gram matrix of a multilayer perceptron towards the identity matrix
at an exponential rate with depth at initialization. We quantify this rate
using the Hermite expansion of the activation function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joudaki_A/0/1/0/all/0/1&quot;&gt;Amir Joudaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daneshmand_H/0/1/0/all/0/1&quot;&gt;Hadi Daneshmand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1&quot;&gt;Francis Bach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06146">
<title>Hidden Classification Layers: Enhancing linear separability between classes in neural networks layers. (arXiv:2306.06146v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06146</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of classification problems, Deep Learning (DL) approaches
represent state of art. Many DL approaches are based on variations of standard
multi-layer feed-forward neural networks. These are also referred to as deep
networks. The basic idea is that each hidden neural layer accomplishes a data
transformation which is expected to make the data representation &quot;somewhat more
linearly separable&quot; than the previous one to obtain a final data representation
which is as linearly separable as possible. However, determining the
appropriate neural network parameters that can perform these transformations is
a critical problem. In this paper, we investigate the impact on deep network
classifier performances of a training approach favouring solutions where data
representations at the hidden layers have a higher degree of linear
separability between the classes with respect to standard methods. To this aim,
we propose a neural network architecture which induces an error function
involving the outputs of all the network layers. Although similar approaches
have already been partially discussed in the past literature, here we propose a
new architecture with a novel error function and an extensive experimental
analysis. This experimental analysis was made in the context of image
classification tasks considering four widely used datasets. The results show
that our approach improves the accuracy on the test set in all the considered
cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apicella_A/0/1/0/all/0/1&quot;&gt;Andrea Apicella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isgro_F/0/1/0/all/0/1&quot;&gt;Francesco Isgr&amp;#xf2;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prevete_R/0/1/0/all/0/1&quot;&gt;Roberto Prevete&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07691">
<title>StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models. (arXiv:2306.07691v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07691</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that
leverages style diffusion and adversarial training with large speech language
models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its
predecessor by modeling styles as a latent random variable through diffusion
models to generate the most suitable style for the text without requiring
reference speech, achieving efficient latent diffusion while benefiting from
the diverse speech synthesis offered by diffusion models. Furthermore, we
employ large pre-trained SLMs, such as WavLM, as discriminators with our novel
differentiable duration modeling for end-to-end training, resulting in improved
speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker
LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by
native English speakers. Moreover, when trained on the LibriTTS dataset, our
model outperforms previous publicly available models for zero-shot speaker
adaptation. This work achieves the first human-level TTS on both single and
multispeaker datasets, showcasing the potential of style diffusion and
adversarial training with large SLMs. The audio demos and source code are
available at https://styletts2.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yinghao Aaron Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Cong Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raghavan_V/0/1/0/all/0/1&quot;&gt;Vinay S. Raghavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mischler_G/0/1/0/all/0/1&quot;&gt;Gavin Mischler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mesgarani_N/0/1/0/all/0/1&quot;&gt;Nima Mesgarani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04962">
<title>Intrinsically motivated graph exploration using network theories of human curiosity. (arXiv:2307.04962v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04962</link>
<description rdf:parseType="Literal">&lt;p&gt;Intrinsically motivated exploration has proven useful for reinforcement
learning, even without additional extrinsic rewards. When the environment is
naturally represented as a graph, how to guide exploration best remains an open
question. In this work, we propose a novel approach for exploring
graph-structured data motivated by two theories of human curiosity: the
information gap theory and the compression progress theory. The theories view
curiosity as an intrinsic motivation to optimize for topological features of
subgraphs induced by nodes visited in the environment. We use these proposed
features as rewards for graph neural-network-based reinforcement learning. On
multiple classes of synthetically generated graphs, we find that trained agents
generalize to longer exploratory walks and larger environments than are seen
during training. Our method computes more efficiently than the greedy
evaluation of the relevant topological properties. The proposed intrinsic
motivations bear particular relevance for recommender systems. We demonstrate
that next-node recommendations considering curiosity are more predictive of
human choices than PageRank centrality in several real-world graph
environments, including MovieLens, Amazon Books, and Wikipedia.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patankar_S/0/1/0/all/0/1&quot;&gt;Shubhankar P. Patankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouellet_M/0/1/0/all/0/1&quot;&gt;Mathieu Ouellet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cervino_J/0/1/0/all/0/1&quot;&gt;Juan Cervino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1&quot;&gt;Alejandro Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1&quot;&gt;Kieran A. Murphy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bassett_D/0/1/0/all/0/1&quot;&gt;Dani S. Bassett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05832">
<title>Bag of Views: An Appearance-based Approach to Next-Best-View Planning for 3D Reconstruction. (arXiv:2307.05832v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05832</link>
<description rdf:parseType="Literal">&lt;p&gt;UAV-based intelligent data acquisition for 3D reconstruction and monitoring
of infrastructure has experienced an increasing surge of interest due to recent
advancements in image processing and deep learning-based techniques. View
planning is an essential part of this task that dictates the information
capture strategy and heavily impacts the quality of the 3D model generated from
the captured data. Recent methods have used prior knowledge or partial
reconstruction of the target to accomplish view planning for active
reconstruction; the former approach poses a challenge for complex or newly
identified targets while the latter is computationally expensive. In this work,
we present Bag-of-Views (BoV), a fully appearance-based model used to assign
utility to the captured views for both offline dataset refinement and online
next-best-view (NBV) planning applications targeting the task of 3D
reconstruction. With this contribution, we also developed the View Planning
Toolbox (VPT), a lightweight package for training and testing machine
learning-based view planning frameworks, custom view dataset generation of
arbitrary 3D scenes, and 3D reconstruction. Through experiments which pair a
BoV-based reinforcement learning model with VPT, we demonstrate the efficacy of
our model in reducing the number of required views for high-quality
reconstructions in dataset refinement and NBV planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gazani_S/0/1/0/all/0/1&quot;&gt;Sara Hatami Gazani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tucsok_M/0/1/0/all/0/1&quot;&gt;Matthew Tucsok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mantegh_I/0/1/0/all/0/1&quot;&gt;Iraj Mantegh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najjaran_H/0/1/0/all/0/1&quot;&gt;Homayoun Najjaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14669">
<title>Fuzzy order-sorted feature logic. (arXiv:2307.14669v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14669</link>
<description rdf:parseType="Literal">&lt;p&gt;Order-Sorted Feature (OSF) logic is a knowledge representation and reasoning
language based on function-denoting feature symbols and set-denoting sort
symbols ordered in a subsumption lattice. OSF logic allows the construction of
record-like terms that represent classes of entities and that are themselves
ordered in a subsumption relation. The unification algorithm for such
structures provides an efficient calculus of type subsumption, which has been
applied in computational linguistics and implemented in constraint logic
programming languages such as LOGIN and LIFE and automated reasoners such as
CEDAR. This work generalizes OSF logic to a fuzzy setting. We give a flexible
definition of a fuzzy subsumption relation which generalizes Zadeh&apos;s inclusion
between fuzzy sets. Based on this definition we define a fuzzy semantics of OSF
logic where sort symbols and OSF terms denote fuzzy sets. We extend the
subsumption relation to OSF terms and prove that it constitutes a fuzzy partial
order with the property that two OSF terms are subsumed by one another in the
crisp sense if and only if their subsumption degree is greater than 0. We show
how to find the greatest lower bound of two OSF terms by unifying them and how
to compute the subsumption degree between two OSF terms, and we provide the
complexity of these operations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milanese_G/0/1/0/all/0/1&quot;&gt;Gian Carlo Milanese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pasi_G/0/1/0/all/0/1&quot;&gt;Gabriella Pasi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15244">
<title>BOURNE: Bootstrapped Self-supervised Learning Framework for Unified Graph Anomaly Detection. (arXiv:2307.15244v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15244</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph anomaly detection (GAD) has gained increasing attention in recent years
due to its critical application in a wide range of domains, such as social
networks, financial risk management, and traffic analysis. Existing GAD methods
can be categorized into node and edge anomaly detection models based on the
type of graph objects being detected. However, these methods typically treat
node and edge anomalies as separate tasks, overlooking their associations and
frequent co-occurrences in real-world graphs. As a result, they fail to
leverage the complementary information provided by node and edge anomalies for
mutual detection. Additionally, state-of-the-art GAD methods, such as CoLA and
SL-GAD, heavily rely on negative pair sampling in contrastive learning, which
incurs high computational costs, hindering their scalability to large graphs.
To address these limitations, we propose a novel unified graph anomaly
detection framework based on bootstrapped self-supervised learning (named
BOURNE). We extract a subgraph (graph view) centered on each target node as
node context and transform it into a dual hypergraph (hypergraph view) as edge
context. These views are encoded using graph and hypergraph neural networks to
capture the representations of nodes, edges, and their associated contexts. By
swapping the context embeddings between nodes and edges and measuring the
agreement in the embedding space, we enable the mutual detection of node and
edge anomalies. Furthermore, BOURNE can eliminate the need for negative
sampling, thereby enhancing its efficiency in handling large graphs. Extensive
experiments conducted on six benchmark datasets demonstrate the superior
effectiveness and efficiency of BOURNE in detecting both node and edge
anomalies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1&quot;&gt;Mengting He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_X/0/1/0/all/0/1&quot;&gt;Xuequn Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jieming Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1&quot;&gt;Bin Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Hongzhi Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16387">
<title>Relation-Oriented: Toward Causal Knowledge-Aligned AGI. (arXiv:2307.16387v10 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16387</link>
<description rdf:parseType="Literal">&lt;p&gt;The current relationship modeling paradigm, grounded in the observational
i.i.d assumption, fundamentally misaligns with our causal knowledge
understanding due to two key oversights: 1) the unobservable relations, which
lead to undetectable hierarchical levels of knowledge, driving the need for
model generalizability; 2) the cognitive relative timings, which crucially
support our structural knowledge comprehension, resulting in inherent biases
within the present Observation-Oriented paradigm. Adopting a novel
Relation-Oriented perspective, this paper proposes a new framework to unify the
various confusions surrounding causality learning, ranging from traditional
causal inference to modern language models.
&lt;/p&gt;
&lt;p&gt;Also, relation-indexed representation learning (RIRL) is raised as a baseline
implementation method of the proposed new paradigm, alongside comprehensive
experiments demonstrating its efficacy in autonomously identifying dynamical
effects in relationship learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01098">
<title>Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search. (arXiv:2308.01098v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01098</link>
<description rdf:parseType="Literal">&lt;p&gt;Search query classification, as an effective way to understand user intents,
is of great importance in real-world online ads systems. To ensure a lower
latency, a shallow model (e.g. FastText) is widely used for efficient online
inference. However, the representation ability of the FastText model is
insufficient, resulting in poor classification performance, especially on some
low-frequency queries and tailed categories. Using a deeper and more complex
model (e.g. BERT) is an effective solution, but it will cause a higher online
inference latency and more expensive computing costs. Thus, how to juggle both
inference efficiency and classification performance is obviously of great
practical importance. To overcome this challenge, in this paper, we propose
knowledge condensation (KC), a simple yet effective knowledge distillation
framework to boost the classification performance of the online FastText model
under strict low latency constraints. Specifically, we propose to train an
offline BERT model to retrieve more potentially relevant data. Benefiting from
its powerful semantic representation, more relevant labels not exposed in the
historical data will be added into the training set for better FastText model
training. Moreover, a novel distribution-diverse multi-expert learning strategy
is proposed to further improve the mining ability of relevant data. By training
multiple BERT models from different data distributions, it can respectively
perform better at high, middle, and low-frequency search queries. The model
ensemble from multi-distribution makes its retrieval ability more powerful. We
have deployed two versions of this framework in JD search, and both offline
experiments and online A/B testing from multiple datasets have validated the
effectiveness of the proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_K/0/1/0/all/0/1&quot;&gt;Kun-Peng Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_M/0/1/0/all/0/1&quot;&gt;Ming Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zheng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xue Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xi-Wei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1&quot;&gt;Chang-Ping Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhan-Gang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jing-He Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jing-Ping Shao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10974">
<title>&quot;Guinea Pig Trials&quot; Utilizing GPT: A Novel Smart Agent-Based Modeling Approach for Studying Firm Competition and Collusion. (arXiv:2308.10974v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10974</link>
<description rdf:parseType="Literal">&lt;p&gt;Firm competition and collusion involve complex dynamics, particularly when
considering communication among firms. Such issues can be modeled as problems
of complex systems, traditionally approached through experiments involving
human subjects or agent-based modeling methods. We propose an innovative
framework called Smart Agent-Based Modeling (SABM), wherein smart agents,
supported by GPT-4 technologies, represent firms, and interact with one
another. We conducted a controlled experiment to study firm price competition
and collusion behaviors under various conditions. SABM is more cost-effective
and flexible compared to conducting experiments with human subjects. Smart
agents possess an extensive knowledge base for decision-making and exhibit
human-like strategic abilities, surpassing traditional ABM agents. Furthermore,
smart agents can simulate human conversation and be personalized, making them
ideal for studying complex situations involving communication. Our results
demonstrate that, in the absence of communication, smart agents consistently
reach tacit collusion, leading to prices converging at levels higher than the
Bertrand equilibrium price but lower than monopoly or cartel prices. When
communication is allowed, smart agents achieve a higher-level collusion with
prices close to cartel prices. Collusion forms more quickly with communication,
while price convergence is smoother without it. These results indicate that
communication enhances trust between firms, encouraging frequent small price
deviations to explore opportunities for a higher-level win-win situation and
reducing the likelihood of triggering a price war. We also assigned different
personas to firms to analyze behavioral differences and tested variant models
under diverse market structures. The findings showcase the effectiveness and
robustness of SABM and provide intriguing insights into competition and
collusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zengqing Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chuan Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12634">
<title>Towards Hierarchical Regional Transformer-based Multiple Instance Learning. (arXiv:2308.12634v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12634</link>
<description rdf:parseType="Literal">&lt;p&gt;The classification of gigapixel histopathology images with deep multiple
instance learning models has become a critical task in digital pathology and
precision medicine. In this work, we propose a Transformer-based multiple
instance learning approach that replaces the traditional learned attention
mechanism with a regional, Vision Transformer inspired self-attention
mechanism. We present a method that fuses regional patch information to derive
slide-level predictions and show how this regional aggregation can be stacked
to hierarchically process features on different distance levels. To increase
predictive accuracy, especially for datasets with small, local morphological
features, we introduce a method to focus the image processing on high attention
regions during inference. Our approach is able to significantly improve
performance over the baseline on two histopathology datasets and points towards
promising directions for further research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cersovsky_J/0/1/0/all/0/1&quot;&gt;Josef Cersovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1&quot;&gt;Sadegh Mohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kainmueller_D/0/1/0/all/0/1&quot;&gt;Dagmar Kainmueller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoehne_J/0/1/0/all/0/1&quot;&gt;Johannes Hoehne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07053">
<title>Pearl&apos;s and Jeffrey&apos;s Update as Modes of Learning in Probabilistic Programming. (arXiv:2309.07053v2 [cs.LO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07053</link>
<description rdf:parseType="Literal">&lt;p&gt;The concept of updating a probability distribution in the light of new
evidence lies at the heart of statistics and machine learning. Pearl&apos;s and
Jeffrey&apos;s rule are two natural update mechanisms which lead to different
outcomes, yet the similarities and differences remain mysterious. This paper
clarifies their relationship in several ways: via separate descriptions of the
two update mechanisms in terms of probabilistic programs and sampling
semantics, and via different notions of likelihood (for Pearl and for Jeffrey).
Moreover, it is shown that Jeffrey&apos;s update rule arises via variational
inference. In terms of categorical probability theory, this amounts to an
analysis of the situation in terms of the behaviour of the multiset functor,
extended to the Kleisli category of the distribution monad.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobs_B/0/1/0/all/0/1&quot;&gt;Bart Jacobs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stein_D/0/1/0/all/0/1&quot;&gt;Dario Stein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07510">
<title>Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions. (arXiv:2309.07510v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07510</link>
<description rdf:parseType="Literal">&lt;p&gt;Perceiving and manipulating 3D articulated objects in diverse environments is
essential for home-assistant robots. Recent studies have shown that point-level
affordance provides actionable priors for downstream manipulation tasks.
However, existing works primarily focus on single-object scenarios with
homogeneous agents, overlooking the realistic constraints imposed by the
environment and the agent&apos;s morphology, e.g., occlusions and physical
limitations. In this paper, we propose an environment-aware affordance
framework that incorporates both object-level actionable priors and environment
constraints. Unlike object-centric affordance approaches, learning
environment-aware affordance faces the challenge of combinatorial explosion due
to the complexity of various occlusions, characterized by their quantities,
geometries, positions and poses. To address this and enhance data efficiency,
we introduce a novel contrastive affordance learning framework capable of
training on scenes containing a single occluder and generalizing to scenes with
complex occluder combinations. Experiments demonstrate the effectiveness of our
proposed approach in learning affordance considering environment constraints.
Project page at https://chengkaiacademycity.github.io/EnvAwareAfford/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1&quot;&gt;Kai Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruihai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_C/0/1/0/all/0/1&quot;&gt;Chuanruo Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_G/0/1/0/all/0/1&quot;&gt;Guanqi Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12253">
<title>SALSA-CLRS: A Sparse and Scalable Benchmark for Algorithmic Reasoning. (arXiv:2309.12253v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12253</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce an extension to the CLRS algorithmic learning benchmark,
prioritizing scalability and the utilization of sparse representations. Many
algorithms in CLRS require global memory or information exchange, mirrored in
its execution model, which constructs fully connected (not sparse) graphs based
on the underlying problem. Despite CLRS&apos;s aim of assessing how effectively
learned algorithms can generalize to larger instances, the existing execution
model becomes a significant constraint due to its demanding memory requirements
and runtime (hard to scale). However, many important algorithms do not demand a
fully connected graph; these algorithms, primarily distributed in nature, align
closely with the message-passing paradigm employed by Graph Neural Networks.
Hence, we propose SALSA-CLRS, an extension of the current CLRS benchmark
specifically with scalability and sparseness in mind. Our approach includes
adapted algorithms from the original CLRS benchmark and introduces new problems
from distributed and randomized algorithms. Moreover, we perform a thorough
empirical evaluation of our benchmark. Code is publicly available at
https://github.com/jkminder/SALSA-CLRS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minder_J/0/1/0/all/0/1&quot;&gt;Julian Minder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grotschla_F/0/1/0/all/0/1&quot;&gt;Florian Gr&amp;#xf6;tschla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathys_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xeb;l Mathys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1&quot;&gt;Roger Wattenhofer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13409">
<title>Time-Series Forecasting: Unleashing Long-Term Dependencies with Fractionally Differenced Data. (arXiv:2309.13409v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13409</link>
<description rdf:parseType="Literal">&lt;p&gt;This study introduces a novel forecasting strategy that leverages the power
of fractional differencing (FD) to capture both short- and long-term
dependencies in time series data. Unlike traditional integer differencing
methods, FD preserves memory in series while stabilizing it for modeling
purposes. By applying FD to financial data from the SPY index and incorporating
sentiment analysis from news reports, this empirical analysis explores the
effectiveness of FD in conjunction with binary classification of target
variables. Supervised classification algorithms were employed to validate the
performance of FD series. The results demonstrate the superiority of FD over
integer differencing, as confirmed by Receiver Operating Characteristic/Area
Under the Curve (ROCAUC) and Mathews Correlation Coefficient (MCC) evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maitra_S/0/1/0/all/0/1&quot;&gt;Sarit Maitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_V/0/1/0/all/0/1&quot;&gt;Vivek Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dwivedi_S/0/1/0/all/0/1&quot;&gt;Srashti Dwivedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1&quot;&gt;Sukanya Kundu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kundu_G/0/1/0/all/0/1&quot;&gt;Goutam Kumar Kundu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16661">
<title>SA2-Net: Scale-aware Attention Network for Microscopic Image Segmentation. (arXiv:2309.16661v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16661</link>
<description rdf:parseType="Literal">&lt;p&gt;Microscopic image segmentation is a challenging task, wherein the objective
is to assign semantic labels to each pixel in a given microscopic image. While
convolutional neural networks (CNNs) form the foundation of many existing
frameworks, they often struggle to explicitly capture long-range dependencies.
Although transformers were initially devised to address this issue using
self-attention, it has been proven that both local and global features are
crucial for addressing diverse challenges in microscopic images, including
variations in shape, size, appearance, and target region density. In this
paper, we introduce SA2-Net, an attention-guided method that leverages
multi-scale feature learning to effectively handle diverse structures within
microscopic images. Specifically, we propose scale-aware attention (SA2) module
designed to capture inherent variations in scales and shapes of microscopic
regions, such as cells, for accurate segmentation. This module incorporates
local attention at each level of multi-stage features, as well as global
attention across multiple resolutions. Furthermore, we address the issue of
blurred region boundaries (e.g., cell boundaries) by introducing a novel
upsampling strategy called the Adaptive Up-Attention (AuA) module. This module
enhances the discriminative ability for improved localization of microscopic
regions using an explicit attention mechanism. Extensive experiments on five
challenging datasets demonstrate the benefits of our SA2-Net model. Our source
code is publicly available at \url{https://github.com/mustansarfiaz/SA2-Net}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiaz_M/0/1/0/all/0/1&quot;&gt;Mustansar Fiaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heidari_M/0/1/0/all/0/1&quot;&gt;Moein Heidari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1&quot;&gt;Rao Muhammad Anwer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cholakkal_H/0/1/0/all/0/1&quot;&gt;Hisham Cholakkal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17113">
<title>Meta-Path Learning for Multi-relational Graph Neural Networks. (arXiv:2309.17113v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17113</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing multi-relational graph neural networks use one of two strategies for
identifying informative relations: either they reduce this problem to low-level
weight learning, or they rely on handcrafted chains of relational dependencies,
called meta-paths. However, the former approach faces challenges in the
presence of many relations (e.g., knowledge graphs), while the latter requires
substantial domain expertise to identify relevant meta-paths. In this work we
propose a novel approach to learn meta-paths and meta-path GNNs that are highly
accurate based on a small number of informative meta-paths. Key element of our
approach is a scoring function for measuring the potential informativeness of a
relation in the incremental construction of the meta-path. Our experimental
evaluation shows that the approach manages to correctly identify relevant
meta-paths even with a large number of relations, and substantially outperforms
existing multi-relational GNNs on synthetic and real-world experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrini_F/0/1/0/all/0/1&quot;&gt;Francesco Ferrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Longa_A/0/1/0/all/0/1&quot;&gt;Antonio Longa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1&quot;&gt;Andrea Passerini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaeger_M/0/1/0/all/0/1&quot;&gt;Manfred Jaeger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01827">
<title>Learning and reusing primitive behaviours to improve Hindsight Experience Replay sample efficiency. (arXiv:2310.01827v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01827</link>
<description rdf:parseType="Literal">&lt;p&gt;Hindsight Experience Replay (HER) is a technique used in reinforcement
learning (RL) that has proven to be very efficient for training off-policy
RL-based agents to solve goal-based robotic manipulation tasks using sparse
rewards. Even though HER improves the sample efficiency of RL-based agents by
learning from mistakes made in past experiences, it does not provide any
guidance while exploring the environment. This leads to very large training
times due to the volume of experience required to train an agent using this
replay strategy. In this paper, we propose a method that uses primitive
behaviours that have been previously learned to solve simple tasks in order to
guide the agent toward more rewarding actions during exploration while learning
other more complex tasks. This guidance, however, is not executed by a manually
designed curriculum, but rather using a critic network to decide at each
timestep whether or not to use the actions proposed by the previously-learned
primitive policies. We evaluate our method by comparing its performance against
HER and other more efficient variations of this algorithm in several block
manipulation tasks. We demonstrate the agents can learn a successful policy
faster when using our proposed method, both in terms of sample efficiency and
computation time. Code is available at https://github.com/franroldans/qmp-her.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_F/0/1/0/all/0/1&quot;&gt;Francisco Roldan Sanchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bulens_D/0/1/0/all/0/1&quot;&gt;David Cordova Bulens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1&quot;&gt;Kevin McGuinness&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redmond_S/0/1/0/all/0/1&quot;&gt;Stephen Redmond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1&quot;&gt;Noel O&amp;#x27;Connor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02230">
<title>Leveraging Diffusion Disentangled Representations to Mitigate Shortcuts in Underspecified Visual Tasks. (arXiv:2310.02230v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02230</link>
<description rdf:parseType="Literal">&lt;p&gt;Spurious correlations in the data, where multiple cues are predictive of the
target labels, often lead to shortcut learning phenomena, where a model may
rely on erroneous, easy-to-learn, cues while ignoring reliable ones. In this
work, we propose an ensemble diversification framework exploiting the
generation of synthetic counterfactuals using Diffusion Probabilistic Models
(DPMs). We discover that DPMs have the inherent capability to represent
multiple visual cues independently, even when they are largely correlated in
the training data. We leverage this characteristic to encourage model diversity
and empirically show the efficacy of the approach with respect to several
diversification objectives. We show that diffusion-guided diversification can
lead models to avert attention from shortcut cues, achieving ensemble diversity
performance comparable to previous methods requiring additional data
collection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scimeca_L/0/1/0/all/0/1&quot;&gt;Luca Scimeca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubinstein_A/0/1/0/all/0/1&quot;&gt;Alexander Rubinstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolicioiu_A/0/1/0/all/0/1&quot;&gt;Armand Mihai Nicolicioiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1&quot;&gt;Damien Teney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03358">
<title>Enhancing Robust Representation in Adversarial Training: Alignment and Exclusion Criteria. (arXiv:2310.03358v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03358</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are vulnerable to adversarial noise. Adversarial
Training (AT) has been demonstrated to be the most effective defense strategy
to protect neural networks from being fooled. However, we find AT omits to
learning robust features, resulting in poor performance of adversarial
robustness. To address this issue, we highlight two criteria of robust
representation: (1) Exclusion: \emph{the feature of examples keeps away from
that of other classes}; (2) Alignment: \emph{the feature of natural and
corresponding adversarial examples is close to each other}. These motivate us
to propose a generic framework of AT to gain robust representation, by the
asymmetric negative contrast and reverse attention. Specifically, we design an
asymmetric negative contrast based on predicted probabilities, to push away
examples of different classes in the feature space. Moreover, we propose to
weight feature by parameters of the linear classifier as the reverse attention,
to obtain class-aware feature and pull close the feature of the same class.
Empirical evaluations on three benchmark datasets show our methods greatly
advance the robustness of AT and achieve state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1&quot;&gt;Nuoyan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nannan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Decheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Dawei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04918">
<title>Robust Network Pruning With Sparse Entropic Wasserstein Regression. (arXiv:2310.04918v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04918</link>
<description rdf:parseType="Literal">&lt;p&gt;This study tackles the issue of neural network pruning that inaccurate
gradients exist when computing the empirical Fisher Information Matrix (FIM).
We introduce an entropic Wasserstein regression (EWR) formulation, capitalizing
on the geometric attributes of the optimal transport (OT) problem. This is
analytically showcased to excel in noise mitigation by adopting neighborhood
interpolation across data points. The unique strength of the Wasserstein
distance is its intrinsic ability to strike a balance between noise reduction
and covariance information preservation. Extensive experiments performed on
various networks show comparable performance of the proposed method with
state-of-the-art (SoTA) network pruning algorithms. Our proposed method
outperforms the SoTA when the network size or the target sparsity is large, the
gain is even larger with the existence of noisy gradients, possibly from noisy
data, analog memory, or adversarial attacks. Notably, our proposed method
achieves a gain of 6% improvement in accuracy and 8% improvement in testing
loss for MobileNetV1 with less than one-fourth of the network parameters
remaining.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_L/0/1/0/all/0/1&quot;&gt;Lei You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hei Victor Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08278">
<title>Lag-Llama: Towards Foundation Models for Time Series Forecasting. (arXiv:2310.08278v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08278</link>
<description rdf:parseType="Literal">&lt;p&gt;Aiming to build foundation models for time-series forecasting and study their
scaling behavior, we present here our work-in-progress on Lag-Llama, a
general-purpose univariate probabilistic time-series forecasting model trained
on a large collection of time-series data. The model shows good zero-shot
prediction capabilities on unseen &quot;out-of-distribution&quot; time-series datasets,
outperforming supervised baselines. We use smoothly broken power-laws to fit
and predict model scaling behavior. The open source code is made available at
https://github.com/kashif/pytorch-transformer-ts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasul_K/0/1/0/all/0/1&quot;&gt;Kashif Rasul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashok_A/0/1/0/all/0/1&quot;&gt;Arjun Ashok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1&quot;&gt;Andrew Robert Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khorasani_A/0/1/0/all/0/1&quot;&gt;Arian Khorasani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adamopoulos_G/0/1/0/all/0/1&quot;&gt;George Adamopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhagwatkar_R/0/1/0/all/0/1&quot;&gt;Rishika Bhagwatkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilos_M/0/1/0/all/0/1&quot;&gt;Marin Bilo&amp;#x161;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghonia_H/0/1/0/all/0/1&quot;&gt;Hena Ghonia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassen_N/0/1/0/all/0/1&quot;&gt;Nadhir Vincent Hassen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_A/0/1/0/all/0/1&quot;&gt;Anderson Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Sahil Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1&quot;&gt;Alexandre Drouin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chapados_N/0/1/0/all/0/1&quot;&gt;Nicolas Chapados&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nevmyvaka_Y/0/1/0/all/0/1&quot;&gt;Yuriy Nevmyvaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1&quot;&gt;Irina Rish&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08782">
<title>Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning. (arXiv:2310.08782v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08782</link>
<description rdf:parseType="Literal">&lt;p&gt;Massive data is often considered essential for deep learning applications,
but it also incurs significant computational and infrastructural costs.
Therefore, dataset pruning (DP) has emerged as an effective way to improve data
efficiency by identifying and removing redundant training samples without
sacrificing performance. In this work, we aim to address the problem of DP for
transfer learning, i.e., how to prune a source dataset for improved pretraining
efficiency and lossless finetuning accuracy on downstream target tasks. To our
best knowledge, the problem of DP for transfer learning remains open, as
previous studies have primarily addressed DP and transfer learning as separate
problems. By contrast, we establish a unified viewpoint to integrate DP with
transfer learning and find that existing DP methods are not suitable for the
transfer learning paradigm. We then propose two new DP methods, label mapping
and feature mapping, for supervised and self-supervised pretraining settings
respectively, by revisiting the DP problem through the lens of source-target
domain mapping. Furthermore, we demonstrate the effectiveness of our approach
on numerous transfer learning tasks. We show that source data classes can be
pruned by up to 40% ~ 80% without sacrificing downstream performance, resulting
in a significant 2 ~ 5 times speed-up during the pretraining stage. Besides,
our proposal exhibits broad applicability and can improve other computationally
intensive transfer learning techniques, such as adversarial pretraining. Codes
are available at https://github.com/OPTML-Group/DP4TL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yihua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yimeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Aochuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jinghan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiancheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Gaowen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1&quot;&gt;Mingyi Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shiyu Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sijia Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09590">
<title>Solving Math Word Problems with Reexamination. (arXiv:2310.09590v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09590</link>
<description rdf:parseType="Literal">&lt;p&gt;Math word problem (MWP) solving aims to understand the descriptive math
problem and calculate the result, for which previous efforts are mostly devoted
to upgrade different technical modules. This paper brings a different
perspective of \textit{reexamination process} during training by introducing a
pseudo-dual task to enhance the MWP solving. We propose a pseudo-dual (PseDual)
learning scheme to model such process, which is model-agnostic thus can be
adapted to any existing MWP solvers. The pseudo-dual task is specifically
defined as filling the numbers in the expression back into the original word
problem with numbers masked. To facilitate the effective joint learning of the
two tasks, we further design a scheduled fusion strategy for the number
infilling task, which smoothly switches the input from the ground-truth math
expressions to the predicted ones. Our pseudo-dual learning scheme has been
tested and proven effective when being equipped in several representative MWP
solvers through empirical studies. \textit{The codes and trained models are
available at:} \url{https://github.com/steven640pixel/PsedualMWP}.
\end{abstract}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bin_Y/0/1/0/all/0/1&quot;&gt;Yi Bin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Wenhao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yujuan Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1&quot;&gt;See-Kiong Ng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09886">
<title>Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation. (arXiv:2310.09886v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09886</link>
<description rdf:parseType="Literal">&lt;p&gt;Lifelong sequence generation (LSG), a problem in continual learning, aims to
continually train a model on a sequence of generation tasks to learn constantly
emerging new generation patterns while avoiding the forgetting of previous
knowledge. Existing LSG methods mainly focus on maintaining old knowledge while
paying little attention to knowledge transfer across tasks. In contrast, humans
can better learn new tasks by leveraging previously acquired knowledge from
similar tasks. Inspired by the learning paradigm of humans, we propose Dynamic
Module Expansion and Adaptation (DMEA), which enables the model to dynamically
determine the architecture for acquiring new knowledge based on task
correlation and select the most similar previous tasks to facilitate adaptation
to new tasks. In addition, as the learning process can easily be biased towards
the current task which might cause more severe forgetting of previously learned
knowledge, we propose dynamic gradient scaling to balance the learning of the
current task and replayed tasks. With extensive experiments, we demonstrate
that DMEA can consistently outperform existing methods in different LSG
settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1&quot;&gt;Chengwei Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1&quot;&gt;Shafiq Joty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10348">
<title>Attribution Patching Outperforms Automated Circuit Discovery. (arXiv:2310.10348v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10348</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated interpretability research has recently attracted attention as a
potential research direction that could scale explanations of neural network
behavior to large models. Existing automated circuit discovery work applies
activation patching to identify subnetworks responsible for solving specific
tasks (circuits). In this work, we show that a simple method based on
attribution patching outperforms all existing methods while requiring just two
forward passes and a backward pass. We apply a linear approximation to
activation patching to estimate the importance of each edge in the
computational subgraph. Using this approximation, we prune the least important
edges of the network. We survey the performance and limitations of this method,
finding that averaged over all tasks our method has greater AUC from circuit
recovery than other methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syed_A/0/1/0/all/0/1&quot;&gt;Aaquib Syed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rager_C/0/1/0/all/0/1&quot;&gt;Can Rager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conmy_A/0/1/0/all/0/1&quot;&gt;Arthur Conmy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15127">
<title>Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models. (arXiv:2310.15127v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15127</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained and frozen large language models (LLMs) can effectively map
simple scene rearrangement instructions to programs over a robot&apos;s visuomotor
functions through appropriate few-shot example prompting. To parse open-domain
natural language and adapt to a user&apos;s idiosyncratic procedures, not known
during prompt engineering time, fixed prompts fall short. In this paper, we
introduce HELPER, an embodied agent equipped with an external memory of
language-program pairs that parses free-form human-robot dialogue into action
programs through retrieval-augmented LLM prompting: relevant memories are
retrieved based on the current dialogue, instruction, correction, or VLM
description, and used as in-context prompt examples for LLM querying. The
memory is expanded during deployment to include pairs of user&apos;s language and
action plans, to assist future inferences and personalize them to the user&apos;s
language and routines. HELPER sets a new state-of-the-art in the TEACh
benchmark in both Execution from Dialog History (EDH) and Trajectory from
Dialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for
TfD. Our models, code, and video results can be found in our project&apos;s website:
https://helper-agent-llm.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarch_G/0/1/0/all/0/1&quot;&gt;Gabriel Sarch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarr_M/0/1/0/all/0/1&quot;&gt;Michael J. Tarr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1&quot;&gt;Katerina Fragkiadaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15778">
<title>Preserving Patient Privacy in MRI Scans: A Comprehensive Approach with 3D Masked Autoencoders. (arXiv:2310.15778v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15778</link>
<description rdf:parseType="Literal">&lt;p&gt;MRI scans provide valuable medical information, however they also contain
sensitive and personally identifiable information (PII) that needs to be
protected. Whereas MRI metadata is easily sanitized, MRI image data is a
privacy risk because it contains information to render highly-realistic 3D
visualizations of a patient&apos;s head, enabling malicious actors to possibly
identify the subject by cross-referencing a database. Data anonymization and
de-identification is concerned with ensuring the privacy and confidentiality of
individuals&apos; personal information. Traditional MRI de-identification methods
remove privacy-sensitive parts (e.g. eyes, nose etc.) from a given scan. This
comes at the expense of introducing a domain shift that can throw off
downstream analyses. Recently, a GAN-based approach was proposed to de-identify
a patient&apos;s scan by remodeling it (\eg changing the face) rather than by
removing parts. In this work, we propose CP-MAE, a model that de-identifies the
face using masked autoencoders and that outperforms all previous approaches in
terms of downstream task performance as well as de-identification. With our
method we are able to synthesize scans of resolution up to $256^3$ (previously
$128^3$) which constitutes an eight-fold increase in the number of voxels.
Using our construction we were able to design a system that exhibits a highly
robust training stage, making it easy to fit the network on novel data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goten_L/0/1/0/all/0/1&quot;&gt;Lennart Alexander Van der Goten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1&quot;&gt;Kevin Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18365">
<title>Using GPT-4 to Augment Unbalanced Data for Automatic Scoring. (arXiv:2310.18365v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18365</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning-based automatic scoring can be challenging if students&apos;
responses are unbalanced across scoring categories, as it introduces
uncertainty in the machine training process. To meet this challenge, we
introduce a novel text data augmentation framework using GPT-4, a generative
large language model, specifically tailored for unbalanced datasets in
automatic scoring. Our experimental dataset comprised student-written responses
to two science items. We crafted prompts for GPT-4 to generate responses
resembling student-written answers, particularly for the minority scoring
classes, to augment the data. We then finetuned DistillBERT for automatic
scoring based on the augmented and original datasets. Model performance was
assessed using accuracy, precision, recall, and F1 score. We incorporate varied
amounts of augmented data to examine scoring performance, and our findings
revealed remarkedly improved model performance. The average maximum increase
observed across two items is: 3.5% for accuracy, 30.6% for precision, 21.1% for
recall, and 24.2% for F1 score. Notably, using just 5% of the augmented data
led to substantial improvements: 2.6%, 29.2%, 15.1%, and 19.6%. Interestingly,
the extent of improvement varied depending on specific datasets. Moreover, we
found that a varying amount of augmented data (5%-40%) was needed to obtain a
stable improvement. We also compare models trained with GPT-4 augmented data
and those trained with additional student-written responses. The findings
indicate that former ones match or even exceed the performance of the latter.
Specifically, there is an average difference of 1.7%, 1.9%, 11.0%, and 7.8% for
four metrics separately. This research underscores the potential and
effectiveness of data augmentation techniques utilizing GPT-4 in addressing
unbalanced datasets within automated assessment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1&quot;&gt;Luyang Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gyeong-Geon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1&quot;&gt;Xiaoming Zhai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18534">
<title>Multi Time Scale World Models. (arXiv:2310.18534v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18534</link>
<description rdf:parseType="Literal">&lt;p&gt;Intelligent agents use internal world models to reason and make predictions
about different courses of their actions at many scales. Devising learning
paradigms and architectures that allow machines to learn world models that
operate at multiple levels of temporal abstractions while dealing with complex
uncertainty predictions is a major technical hurdle. In this work, we propose a
probabilistic formalism to learn multi-time scale world models which we call
the Multi Time Scale State Space (MTS3) model. Our model uses a computationally
efficient inference scheme on multiple time scales for highly accurate
long-horizon predictions and uncertainty estimates over several seconds into
the future. Our experiments, which focus on action conditional long horizon
future predictions, show that MTS3 outperforms recent methods on several system
identification benchmarks including complex simulated and real-world dynamical
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaj_V/0/1/0/all/0/1&quot;&gt;Vaisakh Shaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zadeh_S/0/1/0/all/0/1&quot;&gt;Saleh Gholam Zadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demir_O/0/1/0/all/0/1&quot;&gt;Ozan Demir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Douat_L/0/1/0/all/0/1&quot;&gt;Luiz Ricardo Douat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1&quot;&gt;Gerhard Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18948">
<title>Building a Safer Maritime Environment Through Multi-Path Long-Term Vessel Trajectory Forecasting. (arXiv:2310.18948v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18948</link>
<description rdf:parseType="Literal">&lt;p&gt;Maritime transportation is paramount in achieving global economic growth,
entailing concurrent ecological obligations in sustainability and safeguarding
endangered marine species, most notably preserving large whale populations. In
this regard, the Automatic Identification System (AIS) data plays a significant
role by offering real-time streaming data on vessel movement, allowing enhanced
traffic monitoring. This study explores using AIS data to prevent
vessel-to-whale collisions by forecasting long-term vessel trajectories from
engineered AIS data sequences. For such a task, we have developed an
encoder-decoder model architecture using Bidirectional Long Short-Term Memory
Networks (Bi-LSTM) to predict the next 12 hours of vessel trajectories using 1
to 3 hours of AIS data as input. We feed the model with probabilistic features
engineered from historical AIS data that refer to each trajectory&apos;s potential
route and destination. The model then predicts the vessel&apos;s trajectory,
considering these additional features by leveraging convolutional layers for
spatial feature learning and a position-aware attention mechanism that
increases the importance of recent timesteps of a sequence during temporal
feature learning. The probabilistic features have an F1 Score of approximately
85% and 75% for each feature type, respectively, demonstrating their
effectiveness in augmenting information to the neural network. We test our
model on the Gulf of St. Lawrence, a region known to be the habitat of North
Atlantic Right Whales (NARW). Our model achieved a high R2 score of over 98%
using various techniques and features. It stands out among other approaches as
it can make complex decisions during turnings and path selection. Our study
highlights the potential of data engineering and trajectory forecasting models
for marine life species preservation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spadon_G/0/1/0/all/0/1&quot;&gt;Gabriel Spadon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_J/0/1/0/all/0/1&quot;&gt;Jay Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1&quot;&gt;Matthew Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vela_S/0/1/0/all/0/1&quot;&gt;Sarah Vela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gehrmann_R/0/1/0/all/0/1&quot;&gt;Romina Gehrmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eden_D/0/1/0/all/0/1&quot;&gt;Derek Eden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berkel_J/0/1/0/all/0/1&quot;&gt;Joshua van Berkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soares_A/0/1/0/all/0/1&quot;&gt;Amilcar Soares&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fablet_R/0/1/0/all/0/1&quot;&gt;Ronan Fablet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelot_R/0/1/0/all/0/1&quot;&gt;Ronald Pelot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matwin_S/0/1/0/all/0/1&quot;&gt;Stan Matwin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19727">
<title>Generating Medical Prescriptions with Conditional Transformer. (arXiv:2310.19727v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19727</link>
<description rdf:parseType="Literal">&lt;p&gt;Access to real-world medication prescriptions is essential for medical
research and healthcare quality improvement. However, access to real medication
prescriptions is often limited due to the sensitive nature of the information
expressed. Additionally, manually labelling these instructions for training and
fine-tuning Natural Language Processing (NLP) models can be tedious and
expensive. We introduce a novel task-specific model architecture,
Label-To-Text-Transformer (\textbf{LT3}), tailored to generate synthetic
medication prescriptions based on provided labels, such as a vocabulary list of
medications and their attributes. LT3 is trained on a set of around 2K lines of
medication prescriptions extracted from the MIMIC-III database, allowing the
model to produce valuable synthetic medication prescriptions. We evaluate LT3&apos;s
performance by contrasting it with a state-of-the-art Pre-trained Language
Model (PLM), T5, analysing the quality and diversity of generated texts. We
deploy the generated synthetic data to train the SpacyNER model for the Named
Entity Recognition (NER) task over the n2c2-2018 dataset. The experiments show
that the model trained on synthetic data can achieve a 96-98\% F1 score at
Label Recognition on Drug, Frequency, Route, Strength, and Form. LT3 codes and
data will be shared at
\url{https://github.com/HECTA-UoM/Label-To-Text-Transformer}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belkadi_S/0/1/0/all/0/1&quot;&gt;Samuel Belkadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Micheletti_N/0/1/0/all/0/1&quot;&gt;Nicolo Micheletti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Lifeng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Del_Pinto_W/0/1/0/all/0/1&quot;&gt;Warren Del-Pinto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1&quot;&gt;Goran Nenadic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20049">
<title>SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics. (arXiv:2310.20049v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20049</link>
<description rdf:parseType="Literal">&lt;p&gt;Simulating fluid dynamics is crucial for the design and development process,
ranging from simple valves to complex turbomachinery. Accurately solving the
underlying physical equations is computationally expensive. Therefore,
learning-based solvers that model interactions on meshes have gained interest
due to their promising speed-ups. However, it is unknown to what extent these
models truly understand the underlying physical principles and can generalize
rather than interpolate. Generalization is a key requirement for a
general-purpose fluid simulator, which should adapt to different topologies,
resolutions, or thermodynamic ranges. We propose SURF, a benchmark designed to
test the $\textit{generalization}$ of learned graph-based fluid simulators.
SURF comprises individual datasets and provides specific performance and
generalization metrics for evaluating and comparing different models. We
empirically demonstrate the applicability of SURF by thoroughly investigating
the two state-of-the-art graph-based models, yielding new insights into their
generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kunzli_S/0/1/0/all/0/1&quot;&gt;Stefan K&amp;#xfc;nzli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grotschla_F/0/1/0/all/0/1&quot;&gt;Florian Gr&amp;#xf6;tschla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathys_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xeb;l Mathys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1&quot;&gt;Roger Wattenhofer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20327">
<title>Improving Entropy-Based Test-Time Adaptation from a Clustering View. (arXiv:2310.20327v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20327</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain shift is a common problem in the realistic world, where training data
and test data follow different data distributions. To deal with this problem,
fully test-time adaptation (TTA) leverages the unlabeled data encountered
during test time to adapt the model. In particular, Entropy-Based TTA (EBTTA)
methods, which minimize the prediction&apos;s entropy on test samples, have shown
great success. In this paper, we introduce a new perspective on the EBTTA,
which interprets these methods from a view of clustering. It is an iterative
algorithm: 1) in the assignment step, the forward process of the EBTTA models
is the assignment of labels for these test samples, and 2) in the updating
step, the backward process is the update of the model via the assigned samples.
Based on the interpretation, we can gain a deeper understanding of EBTTA, where
we show that the entropy loss would further increase the largest probability.
Accordingly, we offer an alternative explanation for why existing EBTTA methods
are sensitive to initial assignments, outliers, and batch size. This
observation can guide us to put forward the improvement of EBTTA. We propose
robust label assignment, weight adjustment, and gradient accumulation to
alleviate the above problems. Experimental results demonstrate that our method
can achieve consistent improvements on various datasets. Code is provided in
the supplementary material.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guoliang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1&quot;&gt;Hanjiang Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jian Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00530">
<title>The Development of LLMs for Embodied Navigation. (arXiv:2311.00530v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00530</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the rapid advancement of Large Language Models (LLMs) such
as the Generative Pre-trained Transformer (GPT) has attracted increasing
attention due to their potential in a variety of practical applications. The
application of LLMs with Embodied Intelligence has emerged as a significant
area of focus. Among the myriad applications of LLMs, navigation tasks are
particularly noteworthy because they demand a deep understanding of the
environment and quick, accurate decision-making. LLMs can augment embodied
intelligence systems with sophisticated environmental perception and
decision-making support, leveraging their robust language and image-processing
capabilities. This article offers an exhaustive summary of the symbiosis
between LLMs and embodied intelligence with a focus on navigation. It reviews
state-of-the-art models, research methodologies, and assesses the advantages
and disadvantages of existing embodied navigation models and datasets. Finally,
the article elucidates the role of LLMs in embodied intelligence, based on
current research, and forecasts future directions in the field. A comprehensive
list of studies in this survey is available at
https://github.com/Rongtao-Xu/Awesome-LLM-EN
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jinzhou Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1&quot;&gt;Han Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1&quot;&gt;Xuxiang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Rongtao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Man Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1&quot;&gt;Li Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shibiao Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01310">
<title>Scattering Vision Transformer: Spectral Mixing Matters. (arXiv:2311.01310v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01310</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers have gained significant attention and achieved
state-of-the-art performance in various computer vision tasks, including image
classification, instance segmentation, and object detection. However,
challenges remain in addressing attention complexity and effectively capturing
fine-grained information within images. Existing solutions often resort to
down-sampling operations, such as pooling, to reduce computational cost.
Unfortunately, such operations are non-invertible and can result in information
loss. In this paper, we present a novel approach called Scattering Vision
Transformer (SVT) to tackle these challenges. SVT incorporates a spectrally
scattering network that enables the capture of intricate image details. SVT
overcomes the invertibility issue associated with down-sampling operations by
separating low-frequency and high-frequency components. Furthermore, SVT
introduces a unique spectral gating network utilizing Einstein multiplication
for token and channel mixing, effectively reducing complexity. We show that SVT
achieves state-of-the-art performance on the ImageNet dataset with a
significant reduction in a number of parameters and FLOPS. SVT shows 2\%
improvement over LiTv2 and iFormer. SVT-H-S reaches 84.2\% top-1 accuracy,
while SVT-H-B reaches 85.2\% (state-of-art for base versions) and SVT-H-L
reaches 85.7\% (again state-of-art for large versions). SVT also shows
comparable results in other vision tasks such as instance segmentation. SVT
also outperforms other transformers in transfer learning on standard datasets
such as CIFAR10, CIFAR100, Oxford Flower, and Stanford Car datasets. The
project page is available on this
webpage.\url{https://badripatro.github.io/svt/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patro_B/0/1/0/all/0/1&quot;&gt;Badri N. Patro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agneeswaran_V/0/1/0/all/0/1&quot;&gt;Vijay Srinivas Agneeswaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02651">
<title>Compute at Scale -- A Broad Investigation into the Data Center Industry. (arXiv:2311.02651v3 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02651</link>
<description rdf:parseType="Literal">&lt;p&gt;This report characterizes the data center industry and its importance for AI
development. Data centers are industrial facilities that efficiently provide
compute at scale and thus constitute the engine rooms of today&apos;s digital
economy. As large-scale AI training and inference become increasingly
computationally expensive, they are dominantly executed from this designated
infrastructure. Key features of data centers include large-scale compute
clusters that require extensive cooling and consume large amounts of power, the
need for fast connectivity both within the data center and to the internet, and
an emphasis on security and reliability. The global industry is valued at
approximately $250B and is expected to double over the next seven years. There
are likely about 500 large (above 10 MW) data centers globally, with the US,
Europe, and China constituting the most important markets. The report further
covers important actors, business models, main inputs, and typical locations of
data centers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pilz_K/0/1/0/all/0/1&quot;&gt;Konstantin Pilz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heim_L/0/1/0/all/0/1&quot;&gt;Lennart Heim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04666">
<title>Pre-training LLMs using human-like development data corpus. (arXiv:2311.04666v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04666</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained Large Language Models (LLMs) have shown success in a diverse set
of language inference and understanding tasks. The pre-training stage of LLMs
looks at a large corpus of raw textual data. The BabyLM shared task compares
LLM pre-training to human language acquisition, where the number of tokens seen
by 13-year-old kids is magnitudes smaller than the number of tokens seen by
LLMs. In this work, we pre-train and evaluate LLMs on their ability to learn
contextual word representations using roughly the same number of tokens as seen
by children. We provide a strong set of baselines; with different
architectures, evaluation of changes in performance across epochs, and reported
pre-training metrics for the strict small and strict tracks of the task. We
also try to loosely replicate the RoBERTa baseline given by the task organizers
to observe the training robustness to hyperparameter selection and
replicability. We provide the submission details to the strict and strict-small
tracks in this report.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhardwaj_K/0/1/0/all/0/1&quot;&gt;Khushi Bhardwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Raj Sanjay Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varma_S/0/1/0/all/0/1&quot;&gt;Sashank Varma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05659">
<title>Enhancing Instance-Level Image Classification with Set-Level Labels. (arXiv:2311.05659v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05659</link>
<description rdf:parseType="Literal">&lt;p&gt;Instance-level image classification tasks have traditionally relied on
single-instance labels to train models, e.g., few-shot learning and transfer
learning. However, set-level coarse-grained labels that capture relationships
among instances can provide richer information in real-world scenarios. In this
paper, we present a novel approach to enhance instance-level image
classification by leveraging set-level labels. We provide a theoretical
analysis of the proposed method, including recognition conditions for fast
excess risk rate, shedding light on the theoretical foundations of our
approach. We conducted experiments on two distinct categories of datasets:
natural image datasets and histopathology image datasets. Our experimental
results demonstrate the effectiveness of our approach, showcasing improved
classification performance compared to traditional single-instance label-based
methods. Notably, our algorithm achieves 13% improvement in classification
accuracy compared to the strongest baseline on the histopathology image
classification benchmarks. Importantly, our experimental findings align with
the theoretical analysis, reinforcing the robustness and reliability of our
proposed method. This work bridges the gap between instance-level and set-level
image classification, offering a promising avenue for advancing the
capabilities of image classification models with set-level coarse-grained
labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Renyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Aly A. Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grossman_R/0/1/0/all/0/1&quot;&gt;Robert L. Grossman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06310">
<title>$\textit{Labor Space}$: A Unifying Representation of the Labor Market via Large Language Models. (arXiv:2311.06310v2 [physics.soc-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06310</link>
<description rdf:parseType="Literal">&lt;p&gt;The labor market is a complex ecosystem comprising diverse, interconnected
entities, such as industries, occupations, skills, and firms. Due to the lack
of a systematic method to map these heterogeneous entities together, each
entity has been analyzed in isolation or only through pairwise relationships,
inhibiting comprehensive understanding of the whole ecosystem. Here, we
introduce $\textit{Labor Space}$, a vector-space embedding of heterogeneous
labor market entities, derived through applying a large language model with
fine-tuning. Labor Space exposes the complex relational fabric of various labor
market constituents, facilitating coherent integrative analysis of industries,
occupations, skills, and firms, while retaining type-specific clustering. We
demonstrate its unprecedented analytical capacities, including positioning
heterogeneous entities on an economic axes, such as
`Manufacturing--Healthcare&apos;. Furthermore, by allowing vector arithmetic of
these entities, Labor Space enables the exploration of complex inter-unit
relations, and subsequently the estimation of the ramifications of economic
shocks on individual units and their ripple effect across the labor market. We
posit that Labor Space provides policymakers and business leaders with a
comprehensive unifying framework for labor market analysis and simulation,
fostering more nuanced and effective strategic decision-making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seongwoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ahn_Y/0/1/0/all/0/1&quot;&gt;Yong-Yeol Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jaehyuk Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07127">
<title>Untargeted Black-box Attacks for Social Recommendations. (arXiv:2311.07127v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07127</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise of online social networks has facilitated the evolution of social
recommender systems, which incorporate social relations to enhance users&apos;
decision-making process. With the great success of Graph Neural Networks in
learning node representations, GNN-based social recommendations have been
widely studied to model user-item interactions and user-user social relations
simultaneously. Despite their great successes, recent studies have shown that
these advanced recommender systems are highly vulnerable to adversarial
attacks, in which attackers can inject well-designed fake user profiles to
disrupt recommendation performances. While most existing studies mainly focus
on targeted attacks to promote target items on vanilla recommender systems,
untargeted attacks to degrade the overall prediction performance are less
explored on social recommendations under a black-box scenario. To perform
untargeted attacks on social recommender systems, attackers can construct
malicious social relationships for fake users to enhance the attack
performance. However, the coordination of social relations and item profiles is
challenging for attacking black-box social recommendations. To address this
limitation, we first conduct several preliminary studies to demonstrate the
effectiveness of cross-community connections and cold-start items in degrading
recommendations performance. Specifically, we propose a novel framework
Multiattack based on multi-agent reinforcement learning to coordinate the
generation of cold-start item profiles and cross-community social relations for
conducting untargeted attacks on black-box social recommendations.
Comprehensive experiments on various real-world datasets demonstrate the
effectiveness of our proposed attacking framework under the black-box setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1&quot;&gt;Wenqi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiao-yong Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_X/0/1/0/all/0/1&quot;&gt;Xiaowei Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qing Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07633">
<title>Rethinking and Benchmarking Predict-then-Optimize Paradigm for Combinatorial Optimization Problems. (arXiv:2311.07633v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07633</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerous web applications rely on solving combinatorial optimization
problems, such as energy cost-aware scheduling, budget allocation on web
advertising, and graph matching on social networks. However, many optimization
problems involve unknown coefficients, and improper predictions of these
factors may lead to inferior decisions which may cause energy wastage,
inefficient resource allocation, inappropriate matching in social networks,
etc. Such a research topic is referred to as &quot;Predict-Then-Optimize (PTO)&quot;
which considers the performance of prediction and decision-making in a unified
system. A noteworthy recent development is the end-to-end methods by directly
optimizing the ultimate decision quality which claims to yield better results
in contrast to the traditional two-stage approach. However, the evaluation
benchmarks in this field are fragmented and the effectiveness of various models
in different scenarios remains unclear, hindering the comprehensive assessment
and fast deployment of these methods. To address these issues, we provide a
comprehensive categorization of current approaches and integrate existing
experimental scenarios to establish a unified benchmark, elucidating the
circumstances under which end-to-end training yields improvements, as well as
the contexts in which it performs ineffectively. We also introduce a new
dataset for the industrial combinatorial advertising problem for inclusive
finance to open-source. We hope the rethinking and benchmarking of PTO could
facilitate more convenient evaluation and deployment, and inspire further
improvements both in the academy and industry within this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_H/0/1/0/all/0/1&quot;&gt;Haoyu Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_H/0/1/0/all/0/1&quot;&gt;Hang Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Runzhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07723">
<title>Generalization Analogies: A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains. (arXiv:2311.07723v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07723</link>
<description rdf:parseType="Literal">&lt;p&gt;As AI systems become more intelligent and their behavior becomes more
challenging to assess, they may learn to game the flaws of human feedback
instead of genuinely striving to follow instructions; however, this risk can be
mitigated by controlling how LLMs generalize human feedback to situations where
it is unreliable. To better understand how reward models generalize, we craft
69 distribution shifts spanning 8 categories. We find that reward models do not
learn to evaluate `instruction-following&apos; by default and instead favor personas
that resemble internet text. Techniques for interpreting reward models&apos;
internal representations achieve better generalization than standard
fine-tuning, but still frequently fail to distinguish instruction-following
from conflated behaviors. We consolidate the 15 most challenging distribution
shifts into the GENeralization analogIES (GENIES) benchmark, which we hope will
enable progress toward controlling reward model generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clymer_J/0/1/0/all/0/1&quot;&gt;Joshua Clymer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baker_G/0/1/0/all/0/1&quot;&gt;Garrett Baker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramani_R/0/1/0/all/0/1&quot;&gt;Rohan Subramani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sam Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07750">
<title>SynthEnsemble: A Fusion of CNN, Vision Transformer, and Hybrid Models for Multi-Label Chest X-Ray Classification. (arXiv:2311.07750v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07750</link>
<description rdf:parseType="Literal">&lt;p&gt;Chest X-rays are widely used to diagnose thoracic diseases, but the lack of
detailed information about these abnormalities makes it challenging to develop
accurate automated diagnosis systems, which is crucial for early detection and
effective treatment. To address this challenge, we employed deep learning
techniques to identify patterns in chest X-rays that correspond to different
diseases. We conducted experiments on the &quot;ChestX-ray14&quot; dataset using various
pre-trained CNNs, transformers, hybrid(CNN+Transformer) models and classical
models. The best individual model was the CoAtNet, which achieved an area under
the receiver operating characteristic curve (AUROC) of 84.2%. By combining the
predictions of all trained models using a weighted average ensemble where the
weight of each model was determined using differential evolution, we further
improved the AUROC to 85.4%, outperforming other state-of-the-art methods in
this field. Our findings demonstrate the potential of deep learning techniques,
particularly ensemble deep learning, for improving the accuracy of automatic
diagnosis of thoracic diseases from chest X-rays.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashraf_S/0/1/0/all/0/1&quot;&gt;S.M. Nabil Ashraf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mamun_M/0/1/0/all/0/1&quot;&gt;Md. Adyelullahil Mamun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdullah_H/0/1/0/all/0/1&quot;&gt;Hasnat Md. Abdullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Md. Golam Rabiul Alam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07780">
<title>Parrot-Trained Adversarial Examples: Pushing the Practicality of Black-Box Audio Attacks against Speaker Recognition Models. (arXiv:2311.07780v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07780</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio adversarial examples (AEs) have posed significant security challenges
to real-world speaker recognition systems. Most black-box attacks still require
certain information from the speaker recognition model to be effective (e.g.,
keeping probing and requiring the knowledge of similarity scores). This work
aims to push the practicality of the black-box attacks by minimizing the
attacker&apos;s knowledge about a target speaker recognition model. Although it is
not feasible for an attacker to succeed with completely zero knowledge, we
assume that the attacker only knows a short (or a few seconds) speech sample of
a target speaker. Without any probing to gain further knowledge about the
target model, we propose a new mechanism, called parrot training, to generate
AEs against the target model. Motivated by recent advancements in voice
conversion (VC), we propose to use the one short sentence knowledge to generate
more synthetic speech samples that sound like the target speaker, called parrot
speech. Then, we use these parrot speech samples to train a parrot-trained(PT)
surrogate model for the attacker. Under a joint transferability and perception
framework, we investigate different ways to generate AEs on the PT model
(called PT-AEs) to ensure the PT-AEs can be generated with high transferability
to a black-box target model with good human perceptual quality. Real-world
experiments show that the resultant PT-AEs achieve the attack success rates of
45.8% - 80.8% against the open-source models in the digital-line scenario and
47.9% - 58.3% against smart devices, including Apple HomePod (Siri), Amazon
Echo, and Google Home, in the over-the-air scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_R/0/1/0/all/0/1&quot;&gt;Rui Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_Z/0/1/0/all/0/1&quot;&gt;Zhe Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Leah Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhuo Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07989">
<title>A Survey on Language Models for Code. (arXiv:2311.07989v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07989</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we systematically review the recent advancements in code
processing with language models, covering 50+ models, 30+ evaluation tasks,
150+ datasets, and 550 related works. We break down code processing models into
general language models represented by the GPT family and specialized models
that are specifically pretrained on code, often with tailored objectives. We
discuss the relations and differences between these models, and highlight the
historical transition of code modeling from statistical models and RNNs to
pretrained Transformers and LLMs, which is exactly the same course that had
been taken by NLP. We also discuss code-specific features such as AST, CFG, and
unit tests, along with their application in training code language models, and
identify key challenges and potential future directions in this domain. We keep
the survey open and updated on GitHub repository at
https://github.com/codefuse-ai/Awesome-Code-LLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Ziyin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaoyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bingchang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1&quot;&gt;Cong Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1&quot;&gt;Zi Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08393">
<title>MVSA-Net: Multi-View State-Action Recognition for Robust and Deployable Trajectory Generation. (arXiv:2311.08393v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08393</link>
<description rdf:parseType="Literal">&lt;p&gt;The learn-from-observation (LfO) paradigm is a human-inspired mode for a
robot to learn to perform a task simply by watching it being performed. LfO can
facilitate robot integration on factory floors by minimizing disruption and
reducing tedious programming. A key component of the LfO pipeline is a
transformation of the depth camera frames to the corresponding task state and
action pairs, which are then relayed to learning techniques such as imitation
or inverse reinforcement learning for understanding the task parameters. While
several existing computer vision models analyze videos for activity
recognition, SA-Net specifically targets robotic LfO from RGB-D data. However,
SA-Net and many other models analyze frame data captured from a single
viewpoint. Their analysis is therefore highly sensitive to occlusions of the
observed task, which are frequent in deployments. An obvious way of reducing
occlusions is to simultaneously observe the task from multiple viewpoints and
synchronously fuse the multiple streams in the model. Toward this, we present
multi-view SA-Net, which generalizes the SA-Net model to allow the perception
of multiple viewpoints of the task activity, integrate them, and better
recognize the state and action in each frame. Performance evaluations on two
distinct domains establish that MVSA-Net recognizes the state-action pairs
under occlusion more accurately compared to single-view MVSA-Net and other
baselines. Our ablation studies further evaluate its performance under
different ambient conditions and establish the contribution of the architecture
components. As such, MVSA-Net offers a significantly more robust and deployable
state-action trajectory generation compared to previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asali_E/0/1/0/all/0/1&quot;&gt;Ehsan Asali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_P/0/1/0/all/0/1&quot;&gt;Prashant Doshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jin Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08427">
<title>Towards a Transportable Causal Network Model Based on Observational Healthcare Data. (arXiv:2311.08427v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08427</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the last decades, many prognostic models based on artificial
intelligence techniques have been used to provide detailed predictions in
healthcare. Unfortunately, the real-world observational data used to train and
validate these models are almost always affected by biases that can strongly
impact the outcomes validity: two examples are values missing not-at-random and
selection bias. Addressing them is a key element in achieving transportability
and in studying the causal relationships that are critical in clinical decision
making, going beyond simpler statistical approaches based on probabilistic
association.
&lt;/p&gt;
&lt;p&gt;In this context, we propose a novel approach that combines selection
diagrams, missingness graphs, causal discovery and prior knowledge into a
single graphical model to estimate the cardiovascular risk of adolescent and
young females who survived breast cancer. We learn this model from data
comprising two different cohorts of patients. The resulting causal network
model is validated by expert clinicians in terms of risk assessment, accuracy
and explainability, and provides a prognostic model that outperforms competing
machine learning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernasconi_A/0/1/0/all/0/1&quot;&gt;Alice Bernasconi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zanga_A/0/1/0/all/0/1&quot;&gt;Alessio Zanga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucas_P/0/1/0/all/0/1&quot;&gt;Peter J.F. Lucas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scutari_M/0/1/0/all/0/1&quot;&gt;Marco Scutari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stella_F/0/1/0/all/0/1&quot;&gt;Fabio Stella&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09115">
<title>HEALNet -- Hybrid Multi-Modal Fusion for Heterogeneous Biomedical Data. (arXiv:2311.09115v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09115</link>
<description rdf:parseType="Literal">&lt;p&gt;Technological advances in medical data collection such as high-resolution
histopathology and high-throughput genomic sequencing have contributed to the
rising requirement for multi-modal biomedical modelling, specifically for
image, tabular, and graph data. Most multi-modal deep learning approaches use
modality-specific architectures that are trained separately and cannot capture
the crucial cross-modal information that motivates the integration of different
data sources. This paper presents the Hybrid Early-fusion Attention Learning
Network (HEALNet): a flexible multi-modal fusion architecture, which a)
preserves modality-specific structural information, b) captures the cross-modal
interactions and structural information in a shared latent space, c) can
effectively handle missing modalities during training and inference, and d)
enables intuitive model inspection by learning on the raw data input instead of
opaque embeddings. We conduct multi-modal survival analysis on Whole Slide
Images and Multi-omic data on four cancer cohorts of The Cancer Genome Atlas
(TCGA). HEALNet achieves state-of-the-art performance, substantially improving
over both uni-modal and recent multi-modal baselines, whilst being robust in
scenarios with missing modalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemker_K/0/1/0/all/0/1&quot;&gt;Konstantin Hemker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simidjievski_N/0/1/0/all/0/1&quot;&gt;Nikola Simidjievski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamnik_M/0/1/0/all/0/1&quot;&gt;Mateja Jamnik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09574">
<title>LymphoML: An interpretable artificial intelligence-based method identifies morphologic features that correlate with lymphoma subtype. (arXiv:2311.09574v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09574</link>
<description rdf:parseType="Literal">&lt;p&gt;The accurate classification of lymphoma subtypes using hematoxylin and eosin
(H&amp;amp;E)-stained tissue is complicated by the wide range of morphological features
these cancers can exhibit. We present LymphoML - an interpretable machine
learning method that identifies morphologic features that correlate with
lymphoma subtypes. Our method applies steps to process H&amp;amp;E-stained tissue
microarray cores, segment nuclei and cells, compute features encompassing
morphology, texture, and architecture, and train gradient-boosted models to
make diagnostic predictions. LymphoML&apos;s interpretable models, developed on a
limited volume of H&amp;amp;E-stained tissue, achieve non-inferior diagnostic accuracy
to pathologists using whole-slide images and outperform black box deep-learning
on a dataset of 670 cases from Guatemala spanning 8 lymphoma subtypes. Using
SHapley Additive exPlanation (SHAP) analysis, we assess the impact of each
feature on model prediction and find that nuclear shape features are most
discriminative for DLBCL (F1-score: 78.7%) and classical Hodgkin lymphoma
(F1-score: 74.5%). Finally, we provide the first demonstration that a model
combining features from H&amp;amp;E-stained tissue with features from a standardized
panel of 6 immunostains results in a similar diagnostic accuracy (85.3%) to a
46-stain panel (86.1%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1&quot;&gt;Vivek Shankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaoli Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_V/0/1/0/all/0/1&quot;&gt;Vrishab Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1&quot;&gt;Brent Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_O/0/1/0/all/0/1&quot;&gt;Oscar Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rojansky_R/0/1/0/all/0/1&quot;&gt;Rebecca Rojansky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1&quot;&gt;Andrew Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valvert_F/0/1/0/all/0/1&quot;&gt;Fabiola Valvert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Briercheck_E/0/1/0/all/0/1&quot;&gt;Edward Briercheck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinstock_D/0/1/0/all/0/1&quot;&gt;David Weinstock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natkunam_Y/0/1/0/all/0/1&quot;&gt;Yasodha Natkunam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_Pol_S/0/1/0/all/0/1&quot;&gt;Sebastian Fernandez-Pol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1&quot;&gt;Pranav Rajpurkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09680">
<title>Trustworthy Large Models in Vision: A Survey. (arXiv:2311.09680v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09680</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid progress of Large Models (LMs) has recently revolutionized various
fields of deep learning with remarkable grades, ranging from Natural Language
Processing (NLP) to Computer Vision (CV). However, LMs are increasingly
challenged and criticized by academia and industry due to their powerful
performance but untrustworthy behavior, which urgently needs to be alleviated
by reliable methods. Despite the abundance of literature on trustworthy LMs in
NLP, a systematic survey specifically delving into the trustworthiness of LMs
in CV remains absent. In order to mitigate this gap, we summarize four relevant
concerns that obstruct the trustworthy usage in vision of LMs in this survey,
including 1) human misuse, 2) vulnerability, 3) inherent issue and 4)
interpretability. By highlighting corresponding challenge, countermeasures, and
discussion in each topic, we hope this survey will facilitate readers&apos;
understanding of this field, promote alignment of LMs with human expectations
and enable trustworthy LMs to serve as welfare rather than disaster for human
society.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Ziyan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10057">
<title>The Song Describer Dataset: a Corpus of Audio Captions for Music-and-Language Evaluation. (arXiv:2311.10057v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10057</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the Song Describer dataset (SDD), a new crowdsourced corpus of
high-quality audio-caption pairs, designed for the evaluation of
music-and-language models. The dataset consists of 1.1k human-written natural
language descriptions of 706 music recordings, all publicly accessible and
released under Creative Common licenses. To showcase the use of our dataset, we
benchmark popular models on three key music-and-language tasks (music
captioning, text-to-music generation and music-language retrieval). Our
experiments highlight the importance of cross-dataset evaluation and offer
insights into how researchers can use SDD to gain a broader understanding of
model performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manco_I/0/1/0/all/0/1&quot;&gt;Ilaria Manco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weck_B/0/1/0/all/0/1&quot;&gt;Benno Weck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doh_S/0/1/0/all/0/1&quot;&gt;SeungHeon Doh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Won_M/0/1/0/all/0/1&quot;&gt;Minz Won&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yixiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bodganov_D/0/1/0/all/0/1&quot;&gt;Dmitry Bodganov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yusong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Ke Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tovstogan_P/0/1/0/all/0/1&quot;&gt;Philip Tovstogan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1&quot;&gt;Emmanouil Benetos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quinton_E/0/1/0/all/0/1&quot;&gt;Elio Quinton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1&quot;&gt;Gy&amp;#xf6;rgy Fazekas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_J/0/1/0/all/0/1&quot;&gt;Juhan Nam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10090">
<title>JaxMARL: Multi-Agent RL Environments in JAX. (arXiv:2311.10090v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10090</link>
<description rdf:parseType="Literal">&lt;p&gt;Benchmarks play an important role in the development of machine learning
algorithms. For example, research in reinforcement learning (RL) has been
heavily influenced by available environments and benchmarks. However, RL
environments are traditionally run on the CPU, limiting their scalability with
typical academic compute. Recent advancements in JAX have enabled the wider use
of hardware acceleration to overcome these computational hurdles, enabling
massively parallel RL training pipelines and environments. This is particularly
useful for multi-agent reinforcement learning (MARL) research. First of all,
multiple agents must be considered at each environment step, adding
computational burden, and secondly, the sample complexity is increased due to
non-stationarity, decentralised partial observability, or other MARL
challenges. In this paper, we present JaxMARL, the first open-source code base
that combines ease-of-use with GPU enabled efficiency, and supports a large
number of commonly used MARL environments as well as popular baseline
algorithms. When considering wall clock time, our experiments show that per-run
our JAX-based training pipeline is up to 12500x faster than existing
approaches. This enables efficient and thorough evaluations, with the potential
to alleviate the evaluation crisis of the field. We also introduce and
benchmark SMAX, a vectorised, simplified version of the popular StarCraft
Multi-Agent Challenge, which removes the need to run the StarCraft II game
engine. This not only enables GPU acceleration, but also provides a more
flexible MARL environment, unlocking the potential for self-play,
meta-learning, and other future applications in MARL. We provide code at
https://github.com/flairox/jaxmarl.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rutherford_A/0/1/0/all/0/1&quot;&gt;Alexander Rutherford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellis_B/0/1/0/all/0/1&quot;&gt;Benjamin Ellis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallici_M/0/1/0/all/0/1&quot;&gt;Matteo Gallici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cook_J/0/1/0/all/0/1&quot;&gt;Jonathan Cook&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lupu_A/0/1/0/all/0/1&quot;&gt;Andrei Lupu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ingvarsson_G/0/1/0/all/0/1&quot;&gt;Gardar Ingvarsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willi_T/0/1/0/all/0/1&quot;&gt;Timon Willi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Akbir Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witt_C/0/1/0/all/0/1&quot;&gt;Christian Schroeder de Witt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souly_A/0/1/0/all/0/1&quot;&gt;Alexandra Souly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bandyopadhyay_S/0/1/0/all/0/1&quot;&gt;Saptarashmi Bandyopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samvelyan_M/0/1/0/all/0/1&quot;&gt;Mikayel Samvelyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Minqi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lange_R/0/1/0/all/0/1&quot;&gt;Robert Tjarko Lange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1&quot;&gt;Shimon Whiteson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacerda_B/0/1/0/all/0/1&quot;&gt;Bruno Lacerda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hawes_N/0/1/0/all/0/1&quot;&gt;Nick Hawes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1&quot;&gt;Tim Rocktaschel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chris Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1&quot;&gt;Jakob Nicolaus Foerster&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10217">
<title>A Language and Its Dimensions: Intrinsic Dimensions of Language Fractal Structures. (arXiv:2311.10217v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10217</link>
<description rdf:parseType="Literal">&lt;p&gt;The present paper introduces a novel object of study - a language fractal
structure. We hypothesize that a set of embeddings of all $n$-grams of a
natural language constitutes a representative sample of this fractal set. (We
use the term Hailonakea to refer to the sum total of all language fractal
structures, over all $n$). The paper estimates intrinsic (genuine) dimensions
of language fractal structures for the Russian and English languages. To this
end, we employ methods based on (1) topological data analysis and (2) a minimum
spanning tree of a data graph for a cloud of points considered (Steele
theorem). For both languages, for all $n$, the intrinsic dimensions appear to
be non-integer values (typical for fractal sets), close to 9 for both of the
Russian and English language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gromov_V/0/1/0/all/0/1&quot;&gt;Vasilii A. Gromov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borodin_N/0/1/0/all/0/1&quot;&gt;Nikita S. Borodin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yerbolova_A/0/1/0/all/0/1&quot;&gt;Asel S. Yerbolova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19845">
<title>Modified Genetic Algorithm for Feature Selection and Hyper Parameter Optimization: Case of XGBoost in Spam Prediction. (arXiv:2310.19845v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2310.19845</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, spam on online social networks has attracted attention in the
research and business world. Twitter has become the preferred medium to spread
spam content. Many research efforts attempted to encounter social networks
spam. Twitter brought extra challenges represented by the feature space size,
and imbalanced data distributions. Usually, the related research works focus on
part of these main challenges or produce black-box models. In this paper, we
propose a modified genetic algorithm for simultaneous dimensionality reduction
and hyper parameter optimization over imbalanced datasets. The algorithm
initialized an eXtreme Gradient Boosting classifier and reduced the features
space of tweets dataset; to generate a spam prediction model. The model is
validated using a 50 times repeated 10-fold stratified cross-validation, and
analyzed using nonparametric statistical tests. The resulted prediction model
attains on average 82.32\% and 92.67\% in terms of geometric mean and accuracy
respectively, utilizing less than 10\% of the total feature space. The
empirical results show that the modified genetic algorithm outperforms $Chi^2$
and $PCA$ feature selection methods. In addition, eXtreme Gradient Boosting
outperforms many machine learning algorithms, including BERT-based deep
learning model, in spam prediction. Furthermore, the proposed approach is
applied to SMS spam modeling and compared to related works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghatasheh_N/0/1/0/all/0/1&quot;&gt;Nazeeh Ghatasheh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altaharwa_I/0/1/0/all/0/1&quot;&gt;Ismail Altaharwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aldebei_K/0/1/0/all/0/1&quot;&gt;Khaled Aldebei&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>