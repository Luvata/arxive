<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-08-03T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01368" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01369" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01375" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01390" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01414" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01415" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01423" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01438" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01519" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01538" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01543" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01546" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01551" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01556" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01557" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01622" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01626" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01649" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01732" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01737" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01830" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01840" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01861" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01872" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01895" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2011.11233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.06757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.03110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.13619" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.03188" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.04385" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.06267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11098" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16174" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19569" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04643" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05357" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11363" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15989" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00958" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01006" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2308.01319">
<title>Recent advancement in Disease Diagnostic using machine learning: Systematic survey of decades, comparisons, and challenges. (arXiv:2308.01319v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01319</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer-aided diagnosis (CAD), a vibrant medical imaging research field, is
expanding quickly. Because errors in medical diagnostic systems might lead to
seriously misleading medical treatments, major efforts have been made in recent
years to improve computer-aided diagnostics applications. The use of machine
learning in computer-aided diagnosis is crucial. A simple equation may result
in a false indication of items like organs. Therefore, learning from examples
is a vital component of pattern recognition. Pattern recognition and machine
learning in the biomedical area promise to increase the precision of disease
detection and diagnosis. They also support the decision-making process&apos;s
objectivity. Machine learning provides a practical method for creating elegant
and autonomous algorithms to analyze high-dimensional and multimodal
bio-medical data. This review article examines machine-learning algorithms for
detecting diseases, including hepatitis, diabetes, liver disease, dengue fever,
and heart disease. It draws attention to the collection of machine learning
techniques and algorithms employed in studying conditions and the ensuing
decision-making process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tajidini_F/0/1/0/all/0/1&quot;&gt;Farzaneh Tajidini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kheiri_M/0/1/0/all/0/1&quot;&gt;Mohammad-Javad Kheiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01320">
<title>DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales. (arXiv:2308.01320v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01320</link>
<description rdf:parseType="Literal">&lt;p&gt;ChatGPT-like models have revolutionized various applications in artificial
intelligence, from summarization and coding to translation, matching or even
surpassing human performance. However, the current landscape lacks an
accessible, efficient, and cost-effective end-to-end RLHF (Reinforcement
Learning with Human Feedback) training pipeline for these powerful models,
particularly when training at the scale of billions of parameters. This paper
introduces DeepSpeed-Chat, a novel system that democratizes RLHF training,
making it accessible to the AI community. DeepSpeed-Chat offers three key
capabilities: an easy-to-use training and inference experience for ChatGPT-like
models, a DeepSpeed-RLHF pipeline that replicates the training pipeline from
InstructGPT, and a robust DeepSpeed-RLHF system that combines various
optimizations for training and inference in a unified way. The system delivers
unparalleled efficiency and scalability, enabling training of models with
hundreds of billions of parameters in record time and at a fraction of the
cost. With this development, DeepSpeed-Chat paves the way for broader access to
advanced RLHF training, even for data scientists with limited resources,
thereby fostering innovation and further development in the field of AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1&quot;&gt;Zhewei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aminabadi_R/0/1/0/all/0/1&quot;&gt;Reza Yazdani Aminabadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruwase_O/0/1/0/all/0/1&quot;&gt;Olatunji Ruwase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajbhandari_S/0/1/0/all/0/1&quot;&gt;Samyam Rajbhandari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoxia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awan_A/0/1/0/all/0/1&quot;&gt;Ammar Ahmad Awan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasley_J/0/1/0/all/0/1&quot;&gt;Jeff Rasley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Minjia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Conglong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holmes_C/0/1/0/all/0/1&quot;&gt;Connor Holmes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhongzhu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wyatt_M/0/1/0/all/0/1&quot;&gt;Michael Wyatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1&quot;&gt;Molly Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurilenko_L/0/1/0/all/0/1&quot;&gt;Lev Kurilenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1&quot;&gt;Heyang Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanaka_M/0/1/0/all/0/1&quot;&gt;Masahiro Tanaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_S/0/1/0/all/0/1&quot;&gt;Shuai Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shuaiwen Leon Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuxiong He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01329">
<title>EmbeddingTree: Hierarchical Exploration of Entity Features in Embedding. (arXiv:2308.01329v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01329</link>
<description rdf:parseType="Literal">&lt;p&gt;Embedding learning transforms discrete data entities into continuous
numerical representations, encoding features/properties of the entities.
Despite the outstanding performance reported from different embedding learning
algorithms, few efforts were devoted to structurally interpreting how features
are encoded in the learned embedding space. This work proposes EmbeddingTree, a
hierarchical embedding exploration algorithm that relates the semantics of
entity features with the less-interpretable embedding vectors. An interactive
visualization tool is also developed based on EmbeddingTree to explore
high-dimensional embeddings. The tool helps users discover nuance features of
data entities, perform feature denoising/injecting in embedding training, and
generate embeddings for unseen entities. We demonstrate the efficacy of
EmbeddingTree and our visualization tool through embeddings generated for
industry-scale merchant data and the public 30Music listening/playlists
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1&quot;&gt;Chin-Chia Michael Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yujie Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huiyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01368">
<title>Empirical Translation Process Research: Past and Possible Future Perspectives. (arXiv:2308.01368v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.01368</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past four decades, efforts have been made to develop and evaluate
models for Empirical Translation Process Research (TPR), yet a comprehensive
framework remains elusive. This article traces the evolution of empirical TPR
within the CRITT TPR-DB tradition and proposes the Free Energy Principle (FEP)
and Active Inference (AIF) as a framework for modeling deeply embedded
translation processes. It introduces novel approaches for quantifying
fundamental concepts of Relevance Theory (relevance, s-mode, i-mode), and
establishes their relation to the Monitor Model, framing relevance maximization
as a special case of minimizing free energy. FEP/AIF provides a mathematically
rigorous foundation that enables modeling of deep temporal architectures in
which embedded translation processes unfold on different timelines. This
framework opens up exciting prospects for future research in predictive TPR,
likely to enrich our comprehension of human translation processes, and making
valuable contributions to the wider realm of translation studies and the design
of cognitive architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carl_M/0/1/0/all/0/1&quot;&gt;Michael Carl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01369">
<title>An enhanced motion planning approach by integrating driving heterogeneity and long-term trajectory prediction for automated driving systems. (arXiv:2308.01369v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2308.01369</link>
<description rdf:parseType="Literal">&lt;p&gt;Navigating automated driving systems (ADSs) through complex driving
environments is difficult. Predicting the driving behavior of surrounding
human-driven vehicles (HDVs) is a critical component of an ADS. This paper
proposes an enhanced motion-planning approach for an ADS in a highway-merging
scenario. The proposed enhanced approach utilizes the results of two aspects:
the driving behavior and long-term trajectory of surrounding HDVs, which are
coupled using a hierarchical model that is used for the motion planning of an
ADS to improve driving safety.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1&quot;&gt;Ni Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yina Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yiheng Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaobo Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01375">
<title>CausalOps -- Towards an Industrial Lifecycle for Causal Probabilistic Graphical Models. (arXiv:2308.01375v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.01375</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal probabilistic graph-based models have gained widespread utility,
enabling the modeling of cause-and-effect relationships across diverse domains.
With their rising adoption in new areas, such as automotive system safety and
machine learning, the need for an integrated lifecycle framework akin to DevOps
and MLOps has emerged. Currently, a process reference for organizations
interested in employing causal engineering is missing. To address this gap and
foster widespread industrial adoption, we propose CausalOps, a novel lifecycle
framework for causal model development and application. By defining key
entities, dependencies, and intermediate artifacts generated during causal
engineering, we establish a consistent vocabulary and workflow model. This work
contextualizes causal model usage across different stages and stakeholders,
outlining a holistic view of creating and maintaining them. CausalOps&apos; aim is
to drive the adoption of causal methods in practical applications within
interested organizations and the causality community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maier_R/0/1/0/all/0/1&quot;&gt;Robert Maier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlattl_A/0/1/0/all/0/1&quot;&gt;Andreas Schlattl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guess_T/0/1/0/all/0/1&quot;&gt;Thomas Guess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mottok_J/0/1/0/all/0/1&quot;&gt;J&amp;#xfc;rgen Mottok&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01390">
<title>OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models. (arXiv:2308.01390v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01390</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce OpenFlamingo, a family of autoregressive vision-language models
ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce
an open-source replication of DeepMind&apos;s Flamingo models. On seven
vision-language datasets, OpenFlamingo models average between 80 - 89% of
corresponding Flamingo performance. This technical report describes our models,
training data, hyperparameters, and evaluation suite. We share our models and
code at https://github.com/mlfoundations/open_flamingo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awadalla_A/0/1/0/all/0/1&quot;&gt;Anas Awadalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_I/0/1/0/all/0/1&quot;&gt;Irena Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1&quot;&gt;Josh Gardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1&quot;&gt;Jack Hessel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanafy_Y/0/1/0/all/0/1&quot;&gt;Yusuf Hanafy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wanrong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marathe_K/0/1/0/all/0/1&quot;&gt;Kalyani Marathe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1&quot;&gt;Samir Gadre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sagawa_S/0/1/0/all/0/1&quot;&gt;Shiori Sagawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jitsev_J/0/1/0/all/0/1&quot;&gt;Jenia Jitsev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1&quot;&gt;Simon Kornblith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1&quot;&gt;Pang Wei Koh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1&quot;&gt;Gabriel Ilharco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1&quot;&gt;Mitchell Wortsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01399">
<title>Learning to Model the World with Language. (arXiv:2308.01399v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.01399</link>
<description rdf:parseType="Literal">&lt;p&gt;To interact with humans in the world, agents need to understand the diverse
types of language that people use, relate them to the visual world, and act
based on them. While current agents learn to execute simple language
instructions from task rewards, we aim to build agents that leverage diverse
language that conveys general knowledge, describes the state of the world,
provides interactive feedback, and more. Our key idea is that language helps
agents predict the future: what will be observed, how the world will behave,
and which situations will be rewarded. This perspective unifies language
understanding with future prediction as a powerful self-supervised learning
objective. We present Dynalang, an agent that learns a multimodal world model
that predicts future text and image representations and learns to act from
imagined model rollouts. Unlike traditional agents that use language only to
predict actions, Dynalang acquires rich language understanding by using past
language also to predict future language, video, and rewards. In addition to
learning from online interaction in an environment, Dynalang can be pretrained
on datasets of text, video, or both without actions or rewards. From using
language hints in grid worlds to navigating photorealistic scans of homes,
Dynalang utilizes diverse types of language to improve task performance,
including environment descriptions, game rules, and instructions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jessy Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yuqing Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watkins_O/0/1/0/all/0/1&quot;&gt;Olivia Watkins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hafner_D/0/1/0/all/0/1&quot;&gt;Danijar Hafner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1&quot;&gt;Dan Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1&quot;&gt;Anca Dragan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01413">
<title>LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning. (arXiv:2308.01413v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.01413</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based models have revolutionized the performance of a wide range
of language tasks. Intuitively, one might expect text classification, which
does not necessitate as many high-level representations as generative tasks, to
be comprehensively addressed with the powerful representation capabilities of
Transformers. However, in reality, there remains significant potential for
enhancement, particularly in the areas of multi-class and multi-label
classification of lengthy textual documents and other large files. The
performance of Transformer-based models is mainly hindered by a major
limitation: a restricted input length, e.g., 512 tokens for BERT. While an
increase in GPU memory can marginally extend this limit, practical real-world
applications often operate under constrained GPU resources. In this work, we
tackle the input limit problem from the perspective of correlated multiple
instance learning. The proposed approach, LaFiCMIL, serves as a versatile
framework applicable to various large file classification tasks covering
binary, multi-class, and multi-label classification tasks, spanning various
domains including Natural Language Processing, Programming Language Processing,
and Android Analysis. To evaluate its effectiveness, we employ eight benchmark
datasets pertaining to Long Document Classification, Code Defect Detection, and
Android Malware Detection. Leveraging BERT-family models as feature extractors,
our experimental results demonstrate that LaFiCMIL achieves new
state-of-the-art performance across all benchmark datasets. This is largely
attributable to its capability of scaling BERT up to nearly 20K tokens, running
on a single Tesla V-100 GPU with 32G of memory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tiezhu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pian_W/0/1/0/all/0/1&quot;&gt;Weiguo Pian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daoudi_N/0/1/0/all/0/1&quot;&gt;Nadia Daoudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allix_K/0/1/0/all/0/1&quot;&gt;Kevin Allix&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bissyande_T/0/1/0/all/0/1&quot;&gt;Tegawend&amp;#xe9; F. Bissyand&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_J/0/1/0/all/0/1&quot;&gt;Jacques Klein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01414">
<title>HouYi: An open-source large language model specially designed for renewable energy and carbon neutrality field. (arXiv:2308.01414v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.01414</link>
<description rdf:parseType="Literal">&lt;p&gt;Renewable energy is important for achieving carbon neutrality goal. With the
great success of Large Language Models (LLMs) like ChatGPT in automatic content
generation, LLMs are playing an increasingly important role. However, there has
not been a specially designed LLM for renewable energy. Meanwhile, there has
not been any dataset of renewable energy for training LLMs. Therefore, this
paper published the first open-source Renewable Energy Academic Paper (REAP)
dataset for non-commercial LLM research of renewable energy. REAP dataset is
collected through searching the title and abstract of 1,168,970 academic
literatures from Web of Science. Based on REAP dataset, HouYi model, the first
LLM for renewable energy, is developed through finetuning general LLMs. HouYi
demonstrated powerful academic paper paragraph generation ability in renewable
energy field. Experiments show that its ability to generate academic papers on
renewable energy is comparable to ChatGPT, slightly outperforms Claude, ERNIE
Bot and SparkDesk, and significantly outperforms open-source LLaMA-13B model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_M/0/1/0/all/0/1&quot;&gt;Mingliang Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhihao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruidong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yusheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zizhen Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yunxiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_C/0/1/0/all/0/1&quot;&gt;Chunjin Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinfu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Daren Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01415">
<title>An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model. (arXiv:2308.01415v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.01415</link>
<description rdf:parseType="Literal">&lt;p&gt;At the beginning era of large language model, it is quite critical to
generate a high-quality financial dataset to fine-tune a large language model
for financial related tasks. Thus, this paper presents a carefully designed
data creation pipeline for this purpose. Particularly, we initiate a dialogue
between an AI investor and financial expert using ChatGPT and incorporate the
feedback of human financial experts, leading to the refinement of the dataset.
This pipeline yielded a robust instruction tuning dataset comprised of 103k
multi-turn chats. Extensive experiments have been conducted on this dataset to
evaluate the model&apos;s performance by adopting an external GPT-4 as the judge.
The promising experimental results verify that our approach led to significant
advancements in generating accurate, relevant, and financial-style responses
from AI models, and thus providing a powerful tool for applications within the
financial sector.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianning Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Junda Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01423">
<title>ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks. (arXiv:2308.01423v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.01423</link>
<description rdf:parseType="Literal">&lt;p&gt;ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to
predict and generate of metal-organic frameworks (MOFs). By leveraging a
large-scale language model (gpt-3.5-turbo), ChatMOF extracts key details from
textual inputs and delivers appropriate responses, thus eliminating the
necessity for rigid structured queries. The system is comprised of three core
components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust
pipeline that manages a variety of tasks, including data retrieval, property
prediction, and structure generation. The study further explores the merits and
constraints of using large language models (LLMs) AI system in material
sciences using and showcases its transformative potential for future
advancements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yeonghun Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jihan Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01438">
<title>Novel Physics-Based Machine-Learning Models for Indoor Air Quality Approximations. (arXiv:2308.01438v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01438</link>
<description rdf:parseType="Literal">&lt;p&gt;Cost-effective sensors are capable of real-time capturing a variety of air
quality-related modalities from different pollutant concentrations to
indoor/outdoor humidity and temperature. Machine learning (ML) models are
capable of performing air-quality &quot;ahead-of-time&quot; approximations. Undoubtedly,
accurate indoor air quality approximation significantly helps provide a healthy
indoor environment, optimize associated energy consumption, and offer human
comfort. However, it is crucial to design an ML architecture to capture the
domain knowledge, so-called problem physics. In this study, we propose six
novel physics-based ML models for accurate indoor pollutant concentration
approximations. The proposed models include an adroit combination of
state-space concepts in physics, Gated Recurrent Units, and Decomposition
techniques. The proposed models were illustrated using data collected from five
offices in a commercial building in California. The proposed models are shown
to be less complex, computationally more efficient, and more accurate than
similar state-of-the-art transformer-based models. The superiority of the
proposed models is due to their relatively light architecture (computational
efficiency) and, more importantly, their ability to capture the underlying
highly nonlinear patterns embedded in the often contaminated sensor-collected
indoor air quality temporal data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadshirazi_A/0/1/0/all/0/1&quot;&gt;Ahmad Mohammadshirazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nadafian_A/0/1/0/all/0/1&quot;&gt;Aida Nadafian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monsefi_A/0/1/0/all/0/1&quot;&gt;Amin Karimi Monsefi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rafiei_M/0/1/0/all/0/1&quot;&gt;Mohammad H. Rafiei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramnath_R/0/1/0/all/0/1&quot;&gt;Rajiv Ramnath&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01469">
<title>VertexSerum: Poisoning Graph Neural Networks for Link Inference. (arXiv:2308.01469v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01469</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) have brought superb performance to various
applications utilizing graph structural data, such as social analysis and fraud
detection. The graph links, e.g., social relationships and transaction history,
are sensitive and valuable information, which raises privacy concerns when
using GNNs. To exploit these vulnerabilities, we propose VertexSerum, a novel
graph poisoning attack that increases the effectiveness of graph link stealing
by amplifying the link connectivity leakage. To infer node adjacency more
accurately, we propose an attention mechanism that can be embedded into the
link detection network. Our experiments demonstrate that VertexSerum
significantly outperforms the SOTA link inference attack, improving the AUC
scores by an average of $9.8\%$ across four real-world datasets and three
different GNN structures. Furthermore, our experiments reveal the effectiveness
of VertexSerum in both black-box and online learning settings, further
validating its applicability in real-world scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1&quot;&gt;Ruyi Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1&quot;&gt;Shijin Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaolin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Y/0/1/0/all/0/1&quot;&gt;Yunsi Fei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01471">
<title>Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving. (arXiv:2308.01471v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01471</link>
<description rdf:parseType="Literal">&lt;p&gt;A self-driving vehicle (SDV) must be able to perceive its surroundings and
predict the future behavior of other traffic participants. Existing works
either perform object detection followed by trajectory forecasting of the
detected objects, or predict dense occupancy and flow grids for the whole
scene. The former poses a safety concern as the number of detections needs to
be kept low for efficiency reasons, sacrificing object recall. The latter is
computationally expensive due to the high-dimensionality of the output grid,
and suffers from the limited receptive field inherent to fully convolutional
networks. Furthermore, both approaches employ many computational resources
predicting areas or objects that might never be queried by the motion planner.
This motivates our unified approach to perception and future prediction that
implicitly represents occupancy and flow over time with a single neural
network. Our method avoids unnecessary computation, as it can be directly
queried by the motion planner at continuous spatio-temporal locations.
Moreover, we design an architecture that overcomes the limited receptive field
of previous explicit occupancy prediction methods by adding an efficient yet
effective global attention mechanism. Through extensive experiments in both
urban and highway settings, we demonstrate that our implicit model outperforms
the current state-of-the-art. For more information, visit the project website:
https://waabi.ai/research/implicito.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agro_B/0/1/0/all/0/1&quot;&gt;Ben Agro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sykora_Q/0/1/0/all/0/1&quot;&gt;Quinlan Sykora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casas_S/0/1/0/all/0/1&quot;&gt;Sergio Casas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01519">
<title>Quantum Multi-Agent Reinforcement Learning for Autonomous Mobility Cooperation. (arXiv:2308.01519v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2308.01519</link>
<description rdf:parseType="Literal">&lt;p&gt;For Industry 4.0 Revolution, cooperative autonomous mobility systems are
widely used based on multi-agent reinforcement learning (MARL). However, the
MARL-based algorithms suffer from huge parameter utilization and convergence
difficulties with many agents. To tackle these problems, a quantum MARL (QMARL)
algorithm based on the concept of actor-critic network is proposed, which is
beneficial in terms of scalability, to deal with the limitations in the noisy
intermediate-scale quantum (NISQ) era. Additionally, our QMARL is also
beneficial in terms of efficient parameter utilization and fast convergence due
to quantum supremacy. Note that the reward in our QMARL is defined as task
precision over computation time in multiple agents, thus, multi-agent
cooperation can be realized. For further improvement, an additional technique
for scalability is proposed, which is called projection value measure (PVM).
Based on PVM, our proposed QMARL can achieve the highest reward, by reducing
the action dimension into a logarithmic-scale. Finally, we can conclude that
our proposed QMARL with PVM outperforms the other algorithms in terms of
efficient parameter utilization, fast convergence, and scalability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Soohyun Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jae Pyoung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Chanyoung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1&quot;&gt;Soyi Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Joongheon Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01538">
<title>Non-equilibrium physics: from spin glasses to machine and neural learning. (arXiv:2308.01538v1 [cond-mat.dis-nn])</title>
<link>http://arxiv.org/abs/2308.01538</link>
<description rdf:parseType="Literal">&lt;p&gt;Disordered many-body systems exhibit a wide range of emergent phenomena
across different scales. These complex behaviors can be utilized for various
information processing tasks such as error correction, learning, and
optimization. Despite the empirical success of utilizing these systems for
intelligent tasks, the underlying principles that govern their emergent
intelligent behaviors remain largely unknown. In this thesis, we aim to
characterize such emergent intelligence in disordered systems through
statistical physics. We chart a roadmap for our efforts in this thesis based on
two axes: learning mechanisms (long-term memory vs. working memory) and
learning dynamics (artificial vs. natural). Throughout our journey, we uncover
relationships between learning mechanisms and physical dynamics that could
serve as guiding principles for designing intelligent systems. We hope that our
investigation into the emergent intelligence of seemingly disparate learning
systems can expand our current understanding of intelligence beyond neural
systems and uncover a wider range of computational substrates suitable for AI
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zhong_W/0/1/0/all/0/1&quot;&gt;Weishun Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01543">
<title>Lode Enhancer: Level Co-creation Through Scaling. (arXiv:2308.01543v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01543</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore AI-powered upscaling as a design assistance tool in the context of
creating 2D game levels. Deep neural networks are used to upscale artificially
downscaled patches of levels from the puzzle platformer game Lode Runner. The
trained networks are incorporated into a web-based editor, where the user can
create and edit levels at three different levels of resolution: 4x4, 8x8, and
16x16. An edit at any resolution instantly transfers to the other resolutions.
As upscaling requires inventing features that might not be present at lower
resolutions, we train neural networks to reproduce these features. We introduce
a neural network architecture that is capable of not only learning upscaling
but also giving higher priority to less frequent tiles. To investigate the
potential of this tool and guide further development, we conduct a qualitative
study with 3 designers to understand how they use it. Designers enjoyed
co-designing with the tool, liked its underlying concept, and provided feedback
for further improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhaumik_D/0/1/0/all/0/1&quot;&gt;Debosmita Bhaumik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1&quot;&gt;Julian Togelius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yannakakis_G/0/1/0/all/0/1&quot;&gt;Georgios N. Yannakakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalifa_A/0/1/0/all/0/1&quot;&gt;Ahmed Khalifa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01546">
<title>MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies. (arXiv:2308.01546v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2308.01546</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have shown promising results in cross-modal generation
tasks, including text-to-image and text-to-audio generation. However,
generating music, as a special type of audio, presents unique challenges due to
limited availability of music data and sensitive issues related to copyright
and plagiarism. In this paper, to tackle these challenges, we first construct a
state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion
and AudioLDM architectures to the music domain. We achieve this by retraining
the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN
vocoder, as components of MusicLDM, on a collection of music data samples.
Then, to address the limitations of training data and to avoid plagiarism, we
leverage a beat tracking model and propose two different mixup strategies for
data augmentation: beat-synchronous audio mixup and beat-synchronous latent
mixup, which recombine training audio directly or via a latent embeddings
space, respectively. Such mixup strategies encourage the model to interpolate
between musical training samples and generate new music within the convex hull
of the training data, making the generated music more diverse while still
staying faithful to the corresponding style. In addition to popular evaluation
metrics, we design several new evaluation metrics based on CLAP score to
demonstrate that our proposed MusicLDM and beat-synchronous mixup strategies
improve both the quality and novelty of generated music, as well as the
correspondence between input text and generated music.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Ke Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yusong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haohe Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nezhurina_M/0/1/0/all/0/1&quot;&gt;Marianna Nezhurina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1&quot;&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubnov_S/0/1/0/all/0/1&quot;&gt;Shlomo Dubnov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01551">
<title>Avoidance Navigation Based on Offline Pre-Training Reinforcement Learning. (arXiv:2308.01551v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2308.01551</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a Pre-Training Deep Reinforcement Learning(DRL) for
avoidance navigation without map for mobile robots which map raw sensor data to
control variable and navigate in an unknown environment. The efficient offline
training strategy is proposed to speed up the inefficient random explorations
in early stage and we also collect a universal dataset including expert
experience for offline training, which is of some significance for other
navigation training work. The pre-training and prioritized expert experience
are proposed to reduce 80\% training time and has been verified to improve the
2 times reward of DRL. The advanced simulation gazebo with real physical
modelling and dynamic equations reduce the gap between sim-to-real. We train
our model a corridor environment, and evaluate the model in different
environment getting the same effect. Compared to traditional method navigation,
we can confirm the trained model can be directly applied into different
scenarios and have the ability to no collision navigate. It was demonstrated
that our DRL model have universal general capacity in different environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1&quot;&gt;Yang Wenkai Ji Ruihang Zhang Yuxiang Lei Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zijie_Z/0/1/0/all/0/1&quot;&gt;Zhao Zijie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01552">
<title>InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent. (arXiv:2308.01552v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.01552</link>
<description rdf:parseType="Literal">&lt;p&gt;This research paper delves into the integration of OpenAI&apos;s ChatGPT into
embodied agent systems, evaluating its influence on interactive decision-making
benchmark. Drawing a parallel to the concept of people assuming roles according
to their unique strengths, we introduce InterAct. In this approach, we feed
ChatGPT with varied prompts, assigning it a numerous roles like a checker and a
sorter, then integrating them with the original language model. Our research
shows a remarkable success rate of 98% in AlfWorld, which consists of 6
different tasks in a simulated household environment, emphasizing the
significance of proficient prompt engineering. The results highlight ChatGPT&apos;s
competence in comprehending and performing intricate tasks effectively in
real-world settings, thus paving the way for further advancements in task
planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Po-Lin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Cheng-Shang Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01556">
<title>A Global Transport Capacity Risk Prediction Method for Rail Transit Based on Gaussian Bayesian Network. (arXiv:2308.01556v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.01556</link>
<description rdf:parseType="Literal">&lt;p&gt;Aiming at the prediction problem of transport capacity risk caused by the
mismatch between the carrying capacity of rail transit network and passenger
flow demand, this paper proposes an explainable prediction method of rail
transit network transport capacity risk based on linear Gaussian Bayesian
network. This method obtains the training data of the prediction model based on
the simulation model of the rail transit system with a three-layer structure
including rail transit network, train flow and passenger flow. A Bayesian
network structure construction method based on the topology of the rail transit
network is proposed, and the MLE (Maximum Likelihood Estimation) method is used
to realize the parameter learning of the Bayesian network. Finally, the
effectiveness of the proposed method is verified by simulation examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhengyang_Z/0/1/0/all/0/1&quot;&gt;Zhang Zhengyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1&quot;&gt;Dong Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+jun_L/0/1/0/all/0/1&quot;&gt;Liu jun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xinya_S/0/1/0/all/0/1&quot;&gt;Sun Xinya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yindong_J/0/1/0/all/0/1&quot;&gt;Ji Yindong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01557">
<title>Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models. (arXiv:2308.01557v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2308.01557</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning priors on trajectory distributions can help accelerate robot motion
planning optimization. Given previously successful plans, learning trajectory
generative models as priors for a new planning problem is highly desirable.
Prior works propose several ways on utilizing this prior to bootstrapping the
motion planning problem. Either sampling the prior for initializations or using
the prior distribution in a maximum-a-posterior formulation for trajectory
optimization. In this work, we propose learning diffusion models as priors. We
then can sample directly from the posterior trajectory distribution conditioned
on task goals, by leveraging the inverse denoising process of diffusion models.
Furthermore, diffusion has been recently shown to effectively encode data
multimodality in high-dimensional settings, which is particularly well-suited
for large trajectory dataset. To demonstrate our method efficacy, we compare
our proposed method - Motion Planning Diffusion - against several baselines in
simulated planar robot and 7-dof robot arm manipulator environments. To assess
the generalization capabilities of our method, we test it in environments with
previously unseen obstacles. Our experiments show that diffusion models are
strong priors to encode high-dimensional trajectory distributions of robot
motions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carvalho_J/0/1/0/all/0/1&quot;&gt;Joao Carvalho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_A/0/1/0/all/0/1&quot;&gt;An T. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baierl_M/0/1/0/all/0/1&quot;&gt;Mark Baierl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koert_D/0/1/0/all/0/1&quot;&gt;Dorothea Koert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1&quot;&gt;Jan Peters&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01578">
<title>Unsupervised Representation Learning for Time Series: A Review. (arXiv:2308.01578v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01578</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised representation learning approaches aim to learn discriminative
feature representations from unlabeled data, without the requirement of
annotating every sample. Enabling unsupervised representation learning is
extremely crucial for time series data, due to its unique annotation bottleneck
caused by its complex characteristics and lack of visual cues compared with
other data modalities. In recent years, unsupervised representation learning
techniques have advanced rapidly in various domains. However, there is a lack
of systematic analysis of unsupervised representation learning approaches for
time series. To fill the gap, we conduct a comprehensive literature review of
existing rapidly evolving unsupervised representation learning approaches for
time series. Moreover, we also develop a unified and standardized library,
named ULTS (i.e., Unsupervised Learning for Time Series), to facilitate fast
implementations and unified evaluations on various models. With ULTS, we
empirically evaluate state-of-the-art approaches, especially the rapidly
evolving contrastive learning methods, on 9 diverse real-world datasets. We
further discuss practical considerations as well as open research challenges on
unsupervised representation learning for time series to facilitate future
research in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1&quot;&gt;Qianwen Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1&quot;&gt;Hangwei Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yonghui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhiqi Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1&quot;&gt;Lizhen Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01589">
<title>Holy Grail 2.0: From Natural Language to Constraint Models. (arXiv:2308.01589v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.01589</link>
<description rdf:parseType="Literal">&lt;p&gt;Twenty-seven years ago, E. Freuder highlighted that &quot;Constraint programming
represents one of the closest approaches computer science has yet made to the
Holy Grail of programming: the user states the problem, the computer solves
it&quot;. Nowadays, CP users have great modeling tools available (like Minizinc and
CPMpy), allowing them to formulate the problem and then let a solver do the
rest of the job, getting closer to the stated goal. However, this still
requires the CP user to know the formalism and respect it. Another significant
challenge lies in the expertise required to effectively model combinatorial
problems. All this limits the wider adoption of CP. In this position paper, we
investigate a possible approach to leverage pre-trained Large Language Models
to extract models from textual problem descriptions. More specifically, we take
inspiration from the Natural Language Processing for Optimization (NL4OPT)
challenge and present early results with a decomposition-based prompting
approach to GPT Models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsouros_D/0/1/0/all/0/1&quot;&gt;Dimos Tsouros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verhaeghe_H/0/1/0/all/0/1&quot;&gt;H&amp;#xe9;l&amp;#xe8;ne Verhaeghe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadioglu_S/0/1/0/all/0/1&quot;&gt;Serdar Kad&amp;#x131;o&amp;#x11f;lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guns_T/0/1/0/all/0/1&quot;&gt;Tias Guns&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01597">
<title>DOLCE: A Descriptive Ontology for Linguistic and Cognitive Engineering. (arXiv:2308.01597v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.01597</link>
<description rdf:parseType="Literal">&lt;p&gt;DOLCE, the first top-level (foundational) ontology to be axiomatized, has
remained stable for twenty years and today is broadly used in a variety of
domains. DOLCE is inspired by cognitive and linguistic considerations and aims
to model a commonsense view of reality, like the one human beings exploit in
everyday life in areas as diverse as socio-technical systems, manufacturing,
financial transactions and cultural heritage. DOLCE clearly lists the
ontological choices it is based upon, relies on philosophical principles, is
richly formalized, and is built according to well-established ontological
methodologies, e.g. OntoClean. Because of these features, it has inspired most
of the existing top-level ontologies and has been used to develop or improve
standards and public domain resources (e.g. CIDOC CRM, DBpedia and WordNet).
Being a foundational ontology, DOLCE is not directly concerned with domain
knowledge. Its purpose is to provide the general categories and relations
needed to give a coherent view of reality, to integrate domain knowledge, and
to mediate across domains. In these 20 years DOLCE has shown that applied
ontologies can be stable and that interoperability across reference and domain
ontologies is a reality. This paper briefly introduces the ontology and shows
how to use it on a few modeling cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borgo_S/0/1/0/all/0/1&quot;&gt;Stefano Borgo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrario_R/0/1/0/all/0/1&quot;&gt;Roberta Ferrario&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gangemi_A/0/1/0/all/0/1&quot;&gt;Aldo Gangemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guarino_N/0/1/0/all/0/1&quot;&gt;Nicola Guarino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masolo_C/0/1/0/all/0/1&quot;&gt;Claudio Masolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Porello_D/0/1/0/all/0/1&quot;&gt;Daniele Porello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanfilippo_E/0/1/0/all/0/1&quot;&gt;Emilio M. Sanfilippo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vieu_L/0/1/0/all/0/1&quot;&gt;Laure Vieu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01614">
<title>Assessing Systematic Weaknesses of DNNs using Counterfactuals. (arXiv:2308.01614v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01614</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advancement of DNNs into safety-critical applications, testing
approaches for such models have gained more attention. A current direction is
the search for and identification of systematic weaknesses that put safety
assumptions based on average performance values at risk. Such weaknesses can
take on the form of (semantically coherent) subsets or areas in the input space
where a DNN performs systematically worse than its expected average. However,
it is non-trivial to attribute the reason for such observed low performances to
the specific semantic features that describe the subset. For instance,
inhomogeneities within the data w.r.t. other (non-considered) attributes might
distort results. However, taking into account all (available) attributes and
their interaction is often computationally highly expensive. Inspired by
counterfactual explanations, we propose an effective and computationally cheap
algorithm to validate the semantic attribution of existing subsets, i.e., to
check whether the identified attribute is likely to have caused the degraded
performance. We demonstrate this approach on an example from the autonomous
driving domain using highly annotated simulated data, where we show for a
semantic segmentation model that (i) performance differences among the
different pedestrian assets exist, but (ii) only in some cases is the asset
type itself the reason for this reduction in the performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gannamaneni_S/0/1/0/all/0/1&quot;&gt;Sujan Sai Gannamaneni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mock_M/0/1/0/all/0/1&quot;&gt;Michael Mock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akila_M/0/1/0/all/0/1&quot;&gt;Maram Akila&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01622">
<title>ReIDTrack: Multi-Object Track and Segmentation Without Motion. (arXiv:2308.01622v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01622</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, dominant Multi-object tracking (MOT) and segmentation (MOTS)
methods mainly follow the tracking-by-detection paradigm. Transformer-based
end-to-end (E2E) solutions bring some ideas to MOT and MOTS, but they cannot
achieve a new state-of-the-art (SOTA) performance in major MOT and MOTS
benchmarks. Detection and association are two main modules of the
tracking-by-detection paradigm. Association techniques mainly depend on the
combination of motion and appearance information. As deep learning has been
recently developed, the performance of the detection and appearance model is
rapidly improved. These trends made us consider whether we can achieve SOTA
based on only high-performance detection and appearance model. Our paper mainly
focuses on exploring this direction based on CBNetV2 with Swin-B as a detection
model and MoCo-v2 as a self-supervised appearance model. Motion information and
IoU mapping were removed during the association. Our method wins 1st place on
the MOTS track and wins 2nd on the MOT track in the CVPR2023 WAD workshop. We
hope our simple and effective method can give some insights to the MOT and MOTS
research community. Source code will be released under this git repository
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaer Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Bingchuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Feng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Twombly_C/0/1/0/all/0/1&quot;&gt;Christopher Walter Twombly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhepeng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01626">
<title>Interleaving GANs with knowledge graphs to support design creativity for book covers. (arXiv:2308.01626v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01626</link>
<description rdf:parseType="Literal">&lt;p&gt;An attractive book cover is important for the success of a book. In this
paper, we apply Generative Adversarial Networks (GANs) to the book covers
domain, using different methods for training in order to obtain better
generated images. We interleave GANs with knowledge graphs to alter the input
title to obtain multiple possible options for any given title, which are then
used as an augmented input to the generator. Finally, we use the discriminator
obtained during the training phase to select the best images generated with new
titles. Our method performed better at generating book covers than previous
attempts, and the knowledge graph gives better options to the book author or
editor compared to using GANs alone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Motogna_A/0/1/0/all/0/1&quot;&gt;Alexandru Motogna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groza_A/0/1/0/all/0/1&quot;&gt;Adrian Groza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01648">
<title>Improving Wind Resistance Performance of Cascaded PID Controlled Quadcopters using Residual Reinforcement Learning. (arXiv:2308.01648v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2308.01648</link>
<description rdf:parseType="Literal">&lt;p&gt;Wind resistance control is an essential feature for quadcopters to maintain
their position to avoid deviation from target position and prevent collisions
with obstacles. Conventionally, cascaded PID controller is used for the control
of quadcopters for its simplicity and ease of tuning its parameters. However,
it is weak against wind disturbances and the quadcopter can easily deviate from
target position. In this work, we propose a residual reinforcement learning
based approach to build a wind resistance controller of a quadcopter. By
learning only the residual that compensates the disturbance, we can continue
using the cascaded PID controller as the base controller of the quadcopter but
improve its performance against wind disturbances. To avoid unexpected crashes
and destructions of quadcopters, our method does not require real hardware for
data collection and training. The controller is trained only on a simulator and
directly applied to the target hardware without extra finetuning process. We
demonstrate the effectiveness of our approach through various experiments
including an experiment in an outdoor scene with wind speed greater than 13
m/s. Despite its simplicity, our controller reduces the position deviation by
approximately 50% compared to the quadcopter controlled with the conventional
cascaded PID controller. Furthermore, trained controller is robust and
preserves its performance even though the quadcopter&apos;s mass and propeller&apos;s
lift coefficient is changed between 50% to 150% from original training time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishihara_Y/0/1/0/all/0/1&quot;&gt;Yu Ishihara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hazama_Y/0/1/0/all/0/1&quot;&gt;Yuichi Hazama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suzuki_K/0/1/0/all/0/1&quot;&gt;Kousuke Suzuki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yokono_J/0/1/0/all/0/1&quot;&gt;Jerry Jun Yokono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabe_K/0/1/0/all/0/1&quot;&gt;Kohtaro Sabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawamoto_K/0/1/0/all/0/1&quot;&gt;Kenta Kawamoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01649">
<title>MARLIM: Multi-Agent Reinforcement Learning for Inventory Management. (arXiv:2308.01649v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01649</link>
<description rdf:parseType="Literal">&lt;p&gt;Maintaining a balance between the supply and demand of products by optimizing
replenishment decisions is one of the most important challenges in the supply
chain industry. This paper presents a novel reinforcement learning framework
called MARLIM, to address the inventory management problem for a single-echelon
multi-products supply chain with stochastic demands and lead-times. Within this
context, controllers are developed through single or multiple agents in a
cooperative setting. Numerical experiments on real data demonstrate the
benefits of reinforcement learning methods over traditional baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leluc_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Leluc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadoche_E/0/1/0/all/0/1&quot;&gt;Elie Kadoche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertoncello_A/0/1/0/all/0/1&quot;&gt;Antoine Bertoncello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gourvenec_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Gourv&amp;#xe9;nec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01681">
<title>NBIAS: A Natural Language Processing Framework for Bias Identification in Text. (arXiv:2308.01681v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.01681</link>
<description rdf:parseType="Literal">&lt;p&gt;Bias in textual data can lead to skewed interpretations and outcomes when the
data is used. These biases could perpetuate stereotypes, discrimination, or
other forms of unfair treatment. An algorithm trained on biased data ends up
making decisions that disproportionately impact a certain group of people.
Therefore, it is crucial to detect and remove these biases to ensure the fair
and ethical use of data. To this end, we develop a comprehensive and robust
framework \textsc{Nbias} that consists of a data layer, corpus contruction,
model development layer and an evaluation layer. The dataset is constructed by
collecting diverse data from various fields, including social media,
healthcare, and job hiring portals. As such, we applied a transformer-based
token classification model that is able to identify bias words/ phrases through
a unique named entity. In the assessment procedure, we incorporate a blend of
quantitative and qualitative evaluations to gauge the effectiveness of our
models. We achieve accuracy improvements ranging from 1% to 8% compared to
baselines. We are also able to generate a robust understanding of the model
functioning, capturing not only numerical data but also the quality and
intricacies of its performance. The proposed approach is applicable to a
variety of biases and contributes to the fair and ethical use of textual data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razaa_S/0/1/0/all/0/1&quot;&gt;Shaina Razaa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1&quot;&gt;Muskan Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reji_D/0/1/0/all/0/1&quot;&gt;Deepak John Reji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bashir_S/0/1/0/all/0/1&quot;&gt;Syed Raza Bashir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Chen Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01682">
<title>Evaluating Link Prediction Explanations for Graph Neural Networks. (arXiv:2308.01682v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01682</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Machine Learning (GML) has numerous applications, such as node/graph
classification and link prediction, in real-world domains. Providing
human-understandable explanations for GML models is a challenging yet
fundamental task to foster their adoption, but validating explanations for link
prediction models has received little attention. In this paper, we provide
quantitative metrics to assess the quality of link prediction explanations,
with or without ground-truth. State-of-the-art explainability methods for Graph
Neural Networks are evaluated using these metrics. We discuss how underlying
assumptions and technical details specific to the link prediction task, such as
the choice of distance between node embeddings, can influence the quality of
the explanations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borile_C/0/1/0/all/0/1&quot;&gt;Claudio Borile&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perotti_A/0/1/0/all/0/1&quot;&gt;Alan Perotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panisson_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Panisson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01686">
<title>LiDAR-Camera Panoptic Segmentation via Geometry-Consistent and Semantic-Aware Alignment. (arXiv:2308.01686v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01686</link>
<description rdf:parseType="Literal">&lt;p&gt;3D panoptic segmentation is a challenging perception task that requires both
semantic segmentation and instance segmentation. In this task, we notice that
images could provide rich texture, color, and discriminative information, which
can complement LiDAR data for evident performance improvement, but their fusion
remains a challenging problem. To this end, we propose LCPS, the first
LiDAR-Camera Panoptic Segmentation network. In our approach, we conduct
LiDAR-Camera fusion in three stages: 1) an Asynchronous Compensation Pixel
Alignment (ACPA) module that calibrates the coordinate misalignment caused by
asynchronous problems between sensors; 2) a Semantic-Aware Region Alignment
(SARA) module that extends the one-to-one point-pixel mapping to one-to-many
semantic relations; 3) a Point-to-Voxel feature Propagation (PVP) module that
integrates both geometric and semantic fusion information for the entire point
cloud. Our fusion strategy improves about 6.9% PQ performance over the
LiDAR-only baseline on NuScenes dataset. Extensive quantitative and qualitative
experiments further demonstrate the effectiveness of our novel framework. The
code will be released at https://github.com/zhangzw12319/lcps.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhizhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1&quot;&gt;Ran Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yuan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lizhuang Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01700">
<title>Bees Local Phase Quantization Feature Selection for RGB-D Facial Expressions Recognition. (arXiv:2308.01700v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01700</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature selection could be defined as an optimization problem and solved by
bio-inspired algorithms. Bees Algorithm (BA) shows decent performance in
feature selection optimization tasks. On the other hand, Local Phase
Quantization (LPQ) is a frequency domain feature which has excellent
performance on Depth images. Here, after extracting LPQ features out of RGB
(colour) and Depth images from the Iranian Kinect Face Database (IKFDB), the
Bees feature selection algorithm applies to select the desired number of
features for final classification tasks. IKFDB is recorded with Kinect sensor
V.2 and contains colour and depth images for facial and facial
micro-expressions recognition purposes. Here five facial expressions of Anger,
Joy, Surprise, Disgust and Fear are used for final validation. The proposed
Bees LPQ method is compared with Particle Swarm Optimization (PSO) LPQ, PCA
LPQ, Lasso LPQ, and just LPQ features for classification tasks with Support
Vector Machines (SVM), K-Nearest Neighbourhood (KNN), Shallow Neural Network
and Ensemble Subspace KNN. Returned results, show a decent performance of the
proposed algorithm (99 % accuracy) in comparison with others.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousavi_S/0/1/0/all/0/1&quot;&gt;Seyed Muhammad Hossein Mousavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilanloo_A/0/1/0/all/0/1&quot;&gt;Atiye Ilanloo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01727">
<title>Local Large Language Models for Complex Structured Medical Tasks. (arXiv:2308.01727v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.01727</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces an approach that combines the language reasoning
capabilities of large language models (LLMs) with the benefits of local
training to tackle complex, domain-specific tasks. Specifically, the authors
demonstrate their approach by extracting structured condition codes from
pathology reports. The proposed approach utilizes local LLMs, which can be
fine-tuned to respond to specific generative instructions and provide
structured outputs. The authors collected a dataset of over 150k uncurated
surgical pathology reports, containing gross descriptions, final diagnoses, and
condition codes. They trained different model architectures, including LLaMA,
BERT and LongFormer and evaluated their performance. The results show that the
LLaMA-based models significantly outperform BERT-style models across all
evaluated metrics, even with extremely reduced precision. The LLaMA models
performed especially well with large datasets, demonstrating their ability to
handle complex, multi-label tasks. Overall, this work presents an effective
approach for utilizing LLMs to perform domain-specific tasks using accessible
hardware, with potential applications in the medical domain, where complex data
extraction and classification are required.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bumgardner_V/0/1/0/all/0/1&quot;&gt;V. K. Cody Bumgardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mullen_A/0/1/0/all/0/1&quot;&gt;Aaron Mullen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armstrong_S/0/1/0/all/0/1&quot;&gt;Sam Armstrong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hickey_C/0/1/0/all/0/1&quot;&gt;Caylin Hickey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talbert_J/0/1/0/all/0/1&quot;&gt;Jeff Talbert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01732">
<title>Towards Self-organizing Personal Knowledge Assistants in Evolving Corporate Memories. (arXiv:2308.01732v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.01732</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a retrospective overview of a decade of research in our
department towards self-organizing personal knowledge assistants in evolving
corporate memories. Our research is typically inspired by real-world problems
and often conducted in interdisciplinary collaborations with research and
industry partners. We summarize past experiments and results comprising topics
like various ways of knowledge graph construction in corporate and personal
settings, Managed Forgetting and (Self-organizing) Context Spaces as a novel
approach to Personal Information Management (PIM) and knowledge work support.
Past results are complemented by an overview of related work and some of our
latest findings not published so far. Last, we give an overview of our related
industry use cases including a detailed look into CoMem, a Corporate Memory
based on our presented research already in productive use and providing
challenges for further research. Many contributions are only first steps in new
directions with still a lot of untapped potential, especially with regard to
further increasing the automation in PIM and knowledge work support.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jilek_C/0/1/0/all/0/1&quot;&gt;Christian Jilek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroder_M/0/1/0/all/0/1&quot;&gt;Markus Schr&amp;#xf6;der&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maus_H/0/1/0/all/0/1&quot;&gt;Heiko Maus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwarz_S/0/1/0/all/0/1&quot;&gt;Sven Schwarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1&quot;&gt;Andreas Dengel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01737">
<title>MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction. (arXiv:2308.01737v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2308.01737</link>
<description rdf:parseType="Literal">&lt;p&gt;With the widespread application of personalized online services,
click-through rate (CTR) prediction has received more and more attention and
research. The most prominent features of CTR prediction are its multi-field
categorical data format, and vast and daily-growing data volume. The large
capacity of neural models helps digest such massive amounts of data under the
supervised learning paradigm, yet they fail to utilize the substantial data to
its full potential, since the 1-bit click signal is not sufficient to guide the
model to learn capable representations of features and instances. The
self-supervised learning paradigm provides a more promising pretrain-finetune
solution to better exploit the large amount of user click logs, and learn more
generalized and effective representations. However, self-supervised learning
for CTR prediction is still an open question, since current works on this line
are only preliminary and rudimentary. To this end, we propose a Model-agnostic
pretraining (MAP) framework that applies feature corruption and recovery on
multi-field categorical data, and more specifically, we derive two practical
algorithms: masked feature prediction (MFP) and replaced feature detection
(RFD). MFP digs into feature interactions within each instance through masking
and predicting a small portion of input features, and introduces noise
contrastive estimation (NCE) to handle large feature spaces. RFD further turns
MFP into a binary classification mode through replacing and detecting changes
in input features, making it even simpler and more effective for CTR
pretraining. Our extensive experiments on two real-world large-scale datasets
(i.e., Avazu, Criteo) demonstrate the advantages of these two methods on
several strong backbones (e.g., DCNv2, DeepFM), and achieve new
state-of-the-art performance in terms of both effectiveness and efficiency for
CTR prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jianghao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1&quot;&gt;Yanru Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1&quot;&gt;Wei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xinyi Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1&quot;&gt;Ruiming Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01797">
<title>Job Shop Scheduling via Deep Reinforcement Learning: a Sequence to Sequence approach. (arXiv:2308.01797v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.01797</link>
<description rdf:parseType="Literal">&lt;p&gt;Job scheduling is a well-known Combinatorial Optimization problem with
endless applications. Well planned schedules bring many benefits in the context
of automated systems: among others, they limit production costs and waste.
Nevertheless, the NP-hardness of this problem makes it essential to use
heuristics whose design is difficult, requires specialized knowledge and often
produces methods tailored to the specific task. This paper presents an original
end-to-end Deep Reinforcement Learning approach to scheduling that
automatically learns dispatching rules. Our technique is inspired by natural
language encoder-decoder models for sequence processing and has never been
used, to the best of our knowledge, for scheduling purposes. We applied and
tested our method in particular to some benchmark instances of Job Shop
Problem, but this technique is general enough to be potentially used to tackle
other different optimal job scheduling tasks with minimal intervention. Results
demonstrate that we outperform many classical approaches exploiting priority
dispatching rules and show competitive results on state-of-the-art Deep
Reinforcement Learning ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonetta_G/0/1/0/all/0/1&quot;&gt;Giovanni Bonetta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zago_D/0/1/0/all/0/1&quot;&gt;Davide Zago&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cancelliere_R/0/1/0/all/0/1&quot;&gt;Rossella Cancelliere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosso_A/0/1/0/all/0/1&quot;&gt;Andrea Grosso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01813">
<title>Deep Neural Networks Fused with Textures for Image Classification. (arXiv:2308.01813v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01813</link>
<description rdf:parseType="Literal">&lt;p&gt;Fine-grained image classification (FGIC) is a challenging task in computer
vision for due to small visual differences among inter-subcategories, but,
large intra-class variations. Deep learning methods have achieved remarkable
success in solving FGIC. In this paper, we propose a fusion approach to address
FGIC by combining global texture with local patch-based information. The first
pipeline extracts deep features from various fixed-size non-overlapping patches
and encodes features by sequential modelling using the long short-term memory
(LSTM). Another path computes image-level textures at multiple scales using the
local binary patterns (LBP). The advantages of both streams are integrated to
represent an efficient feature vector for image classification. The method is
tested on eight datasets representing the human faces, skin lesions, food
dishes, marine lives, etc. using four standard backbone CNNs. Our method has
attained better classification accuracy over existing methods with notable
margins.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1&quot;&gt;Asish Bera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharjee_D/0/1/0/all/0/1&quot;&gt;Debotosh Bhattacharjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasipuri_M/0/1/0/all/0/1&quot;&gt;Mita Nasipuri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01823">
<title>Hard Adversarial Example Mining for Improving Robust Fairness. (arXiv:2308.01823v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01823</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial training (AT) is widely considered the state-of-the-art technique
for improving the robustness of deep neural networks (DNNs) against adversarial
examples (AE). Nevertheless, recent studies have revealed that adversarially
trained models are prone to unfairness problems, restricting their
applicability. In this paper, we empirically observe that this limitation may
be attributed to serious adversarial confidence overfitting, i.e., certain
adversarial examples with overconfidence. To alleviate this problem, we propose
HAM, a straightforward yet effective framework via adaptive Hard Adversarial
example Mining.HAM concentrates on mining hard adversarial examples while
discarding the easy ones in an adaptive fashion. Specifically, HAM identifies
hard AEs in terms of their step sizes needed to cross the decision boundary
when calculating loss value. Besides, an early-dropping mechanism is
incorporated to discard the easy examples at the initial stages of AE
generation, resulting in efficient AT. Extensive experimental results on
CIFAR-10, SVHN, and Imagenette demonstrate that HAM achieves significant
improvement in robust fairness while reducing computational cost compared to
several state-of-the-art adversarial training methods. The code will be made
publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chenhao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1&quot;&gt;Xiang Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yulong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Run Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1&quot;&gt;Liming Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01830">
<title>Learning beyond sensations: how dreams organize neuronal representations. (arXiv:2308.01830v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2308.01830</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic representations in higher sensory cortices form the basis for
robust, yet flexible behavior. These representations are acquired over the
course of development in an unsupervised fashion and continuously maintained
over an organism&apos;s lifespan. Predictive learning theories propose that these
representations emerge from predicting or reconstructing sensory inputs.
However, brains are known to generate virtual experiences, such as during
imagination and dreaming, that go beyond previously experienced inputs. Here,
we suggest that virtual experiences may be just as relevant as actual sensory
inputs in shaping cortical representations. In particular, we discuss two
complementary learning principles that organize representations through the
generation of virtual experiences. First, &quot;adversarial dreaming&quot; proposes that
creative dreams support a cortical implementation of adversarial learning in
which feedback and feedforward pathways engage in a productive game of trying
to fool each other. Second, &quot;contrastive dreaming&quot; proposes that the invariance
of neuronal representations to irrelevant factors of variation is acquired by
trying to map similar virtual experiences together via a contrastive learning
process. These principles are compatible with known cortical structure and
dynamics and the phenomenology of sleep thus providing promising directions to
explain cortical learning beyond the classical predictive learning paradigm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Deperrois_N/0/1/0/all/0/1&quot;&gt;Nicolas Deperrois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Petrovici_M/0/1/0/all/0/1&quot;&gt;Mihai A. Petrovici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Senn_W/0/1/0/all/0/1&quot;&gt;Walter Senn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jordan_J/0/1/0/all/0/1&quot;&gt;Jakob Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01834">
<title>The Capability of Large Language Models to Measure Psychiatric Functioning. (arXiv:2308.01834v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.01834</link>
<description rdf:parseType="Literal">&lt;p&gt;The current work investigates the capability of Large language models (LLMs)
that are explicitly trained on large corpuses of medical knowledge (Med-PaLM 2)
to predict psychiatric functioning from patient interviews and clinical
descriptions without being trained to do so. To assess this, n = 145 depression
and n =115 PTSD assessments and n = 46 clinical case studies across high
prevalence/high comorbidity disorders (Depressive, Anxiety, Psychotic, trauma
and stress, Addictive disorders) were analyzed using prompts to extract
estimated clinical scores and diagnoses. Results demonstrate that Med-PaLM 2 is
capable of assessing psychiatric functioning across a range of psychiatric
conditions with the strongest performance being the prediction of depression
scores based on standardized assessments (Accuracy range= 0.80 - 0.84) which
were statistically indistinguishable from human clinical raters t(1,144) =
1.20; p = 0.23. Results show the potential for general clinical language models
to flexibly predict psychiatric risk based on free descriptions of functioning
from both patients and clinicians.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galatzer_Levy_I/0/1/0/all/0/1&quot;&gt;Isaac R. Galatzer-Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1&quot;&gt;Daniel McDuff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natarajan_V/0/1/0/all/0/1&quot;&gt;Vivek Natarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1&quot;&gt;Alan Karthikesalingam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malgaroli_M/0/1/0/all/0/1&quot;&gt;Matteo Malgaroli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01840">
<title>URET: Universal Robustness Evaluation Toolkit (for Evasion). (arXiv:2308.01840v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01840</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models are known to be vulnerable to adversarial evasion
attacks as illustrated by image classification models. Thoroughly understanding
such attacks is critical in order to ensure the safety and robustness of
critical AI tasks. However, most evasion attacks are difficult to deploy
against a majority of AI systems because they have focused on image domain with
only few constraints. An image is composed of homogeneous, numerical,
continuous, and independent features, unlike many other input types to AI
systems used in practice. Furthermore, some input types include additional
semantic and functional constraints that must be observed to generate realistic
adversarial inputs. In this work, we propose a new framework to enable the
generation of adversarial inputs irrespective of the input type and task
domain. Given an input and a set of pre-defined input transformations, our
framework discovers a sequence of transformations that result in a semantically
correct and functional adversarial input. We demonstrate the generality of our
approach on several diverse machine learning tasks with various input
representations. We also show the importance of generating adversarial examples
as they enable the deployment of mitigation techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eykholt_K/0/1/0/all/0/1&quot;&gt;Kevin Eykholt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1&quot;&gt;Taesung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schales_D/0/1/0/all/0/1&quot;&gt;Douglas Schales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1&quot;&gt;Jiyong Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molloy_I/0/1/0/all/0/1&quot;&gt;Ian Molloy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zorin_M/0/1/0/all/0/1&quot;&gt;Masha Zorin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01850">
<title>Synthesizing Long-Term Human Motions with Diffusion Models via Coherent Sampling. (arXiv:2308.01850v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01850</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-motion generation has gained increasing attention, but most existing
methods are limited to generating short-term motions that correspond to a
single sentence describing a single action. However, when a text stream
describes a sequence of continuous motions, the generated motions corresponding
to each sentence may not be coherently linked. Existing long-term motion
generation methods face two main issues. Firstly, they cannot directly generate
coherent motions and require additional operations such as interpolation to
process the generated actions. Secondly, they generate subsequent actions in an
autoregressive manner without considering the influence of future actions on
previous ones. To address these issues, we propose a novel approach that
utilizes a past-conditioned diffusion model with two optional coherent sampling
methods: Past Inpainting Sampling and Compositional Transition Sampling. Past
Inpainting Sampling completes subsequent motions by treating previous motions
as conditions, while Compositional Transition Sampling models the distribution
of the transition as the composition of two adjacent motions guided by
different text prompts. Our experimental results demonstrate that our proposed
method is capable of generating compositional and coherent long-term 3D human
motions controlled by a user-instructed long text stream. The code is available
at
\href{https://github.com/yangzhao1230/PCMDM}{https://github.com/yangzhao1230/PCMDM}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1&quot;&gt;Bing Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01861">
<title>ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation. (arXiv:2308.01861v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.01861</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we make the first attempt to evaluate LLMs in a more
challenging code generation scenario, i.e. class-level code generation. We
first manually construct the first class-level code generation benchmark
ClassEval of 100 class-level Python code generation tasks with approximately
500 person-hours. Based on it, we then perform the first study of 11
state-of-the-art LLMs on class-level code generation. Based on our results, we
have the following main findings. First, we find that all existing LLMs show
much worse performance on class-level code generation compared to on standalone
method-level code generation benchmarks like HumanEval; and the method-level
coding ability cannot equivalently reflect the class-level coding ability among
LLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superior
than other LLMs on class-level code generation, and the second-tier models
includes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with very
similar performance. Third, we find that generating the entire class all at
once (i.e. holistic generation strategy) is the best generation strategy only
for GPT-4 and GPT-3.5, while method-by-method generation (i.e. incremental and
compositional) is better strategies for the other models with limited ability
of understanding long instructions and utilizing the middle information.
Lastly, we find the limited model ability of generating method-dependent code
and discuss the frequent error types in generated classes. Our benchmark is
available at https://github.com/FudanSELab/ClassEval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xueying Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mingwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaixin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanlin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Junwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yixuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiayi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sha_C/0/1/0/all/0/1&quot;&gt;Chaofeng Sha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xin Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1&quot;&gt;Yiling Lou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01872">
<title>Thespian: Multi-Character Text Role-Playing Game Agents. (arXiv:2308.01872v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.01872</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-adventure games and text role-playing games are grand challenges for
reinforcement learning game playing agents. Text role-playing games are
open-ended environments where an agent must faithfully play a particular
character. We consider the distinction between characters and actors, where an
actor agent has the ability to play multiple characters. We present a framework
we call a thespian agent that can learn to emulate multiple characters along
with a soft prompt that can be used to direct it as to which character to play
at any time. We further describe an attention mechanism that allows the agent
to learn new characters that are based on previously learned characters in a
few-shot fashion. We show that our agent outperforms the state of the art agent
framework in multi-character learning and few-shot learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1&quot;&gt;Christopher Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xiangyu Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1&quot;&gt;Mark Riedl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01895">
<title>Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning. (arXiv:2308.01895v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01895</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning seeks to enable deep learners to train on a series of
tasks of unknown length without suffering from the catastrophic forgetting of
previous tasks. One effective solution is replay, which involves storing few
previous experiences in memory and replaying them when learning the current
task. However, there is still room for improvement when it comes to selecting
the most informative samples for storage and determining the optimal number of
samples to be stored. This study aims to address these issues with a novel
comparison of the commonly used reservoir sampling to various alternative
population strategies and providing a novel detailed analysis of how to find
the optimal number of stored samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brignac_D/0/1/0/all/0/1&quot;&gt;Daniel Brignac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lobo_N/0/1/0/all/0/1&quot;&gt;Niels Lobo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahalanobis_A/0/1/0/all/0/1&quot;&gt;Abhijit Mahalanobis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01899">
<title>How many preprints have actually been printed and why: a case study of computer science preprints on arXiv. (arXiv:2308.01899v1 [cs.DL])</title>
<link>http://arxiv.org/abs/2308.01899</link>
<description rdf:parseType="Literal">&lt;p&gt;Preprints play an increasingly critical role in academic communities. There
are many reasons driving researchers to post their manuscripts to preprint
servers before formal submission to journals or conferences, but the use of
preprints has also sparked considerable controversy, especially surrounding the
claim of priority. In this paper, a case study of computer science preprints
submitted to arXiv from 2008 to 2017 is conducted to quantify how many
preprints have eventually been printed in peer-reviewed venues. Among those
published manuscripts, some are published under different titles and without an
update to their preprints on arXiv. In the case of these manuscripts, the
traditional fuzzy matching method is incapable of mapping the preprint to the
final published version. In view of this issue, we introduce a semantics-based
mapping method with the employment of Bidirectional Encoder Representations
from Transformers (BERT). With this new mapping method and a plurality of data
sources, we find that 66% of all sampled preprints are published under
unchanged titles and 11% are published under different titles and with other
modifications. A further analysis was then performed to investigate why these
preprints but not others were accepted for publication. Our comparison reveals
that in the field of computer science, published preprints feature adequate
revisions, multiple authorship, detailed abstract and introduction, extensive
and authoritative references and available source code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jialiang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhiyang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xiaodong Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01905">
<title>Revisiting Deformable Convolution for Depth Completion. (arXiv:2308.01905v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01905</link>
<description rdf:parseType="Literal">&lt;p&gt;Depth completion, which aims to generate high-quality dense depth maps from
sparse depth maps, has attracted increasing attention in recent years. Previous
work usually employs RGB images as guidance, and introduces iterative spatial
propagation to refine estimated coarse depth maps. However, most of the
propagation refinement methods require several iterations and suffer from a
fixed receptive field, which may contain irrelevant and useless information
with very sparse input. In this paper, we address these two challenges
simultaneously by revisiting the idea of deformable convolution. We propose an
effective architecture that leverages deformable kernel convolution as a
single-pass refinement module, and empirically demonstrate its superiority. To
better understand the function of deformable convolution and exploit it for
depth completion, we further systematically investigate a variety of
representative strategies. Our study reveals that, different from prior work,
deformable convolution needs to be applied on an estimated depth map with a
relatively high density for better performance. We evaluate our model on the
large-scale KITTI dataset and achieve state-of-the-art level performance in
both accuracy and inference speed. Our code is available at
https://github.com/AlexSunNik/ReDC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xinglong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1&quot;&gt;Jean Ponce&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01906">
<title>Reasoning in Large Language Models Through Symbolic Math Word Problems. (arXiv:2308.01906v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.01906</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have revolutionized NLP by solving downstream
tasks with little to no labeled data. Despite their versatile abilities, the
larger question of their ability to reason remains ill-understood. This paper
addresses reasoning in math word problems (MWPs) by studying symbolic versions
of the numeric problems, since a symbolic expression is a &quot;concise explanation&quot;
of the numeric answer. We create and use a symbolic version of the SVAMP
dataset and find that GPT-3&apos;s davinci-002 model also has good zero-shot
accuracy on symbolic MWPs. To evaluate the faithfulness of the model&apos;s
reasoning, we go beyond accuracy and additionally evaluate the alignment
between the final answer and the outputted reasoning, which correspond to
numeric and symbolic answers respectively for MWPs. We explore a self-prompting
approach to encourage the symbolic reasoning to align with the numeric answer,
thus equipping the LLM with the ability to provide a concise and verifiable
reasoning and making it more interpretable. Surprisingly, self-prompting also
improves the symbolic accuracy to be higher than both the numeric and symbolic
accuracies, thus providing an ensembling effect. The SVAMP_Sym dataset will be
released for future research on symbolic math problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaur_V/0/1/0/all/0/1&quot;&gt;Vedant Gaur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saunshi_N/0/1/0/all/0/1&quot;&gt;Nikunj Saunshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2011.11233">
<title>ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradient Accumulation. (arXiv:2011.11233v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2011.11233</link>
<description rdf:parseType="Literal">&lt;p&gt;Albeit being a prevalent architecture searching approach, differentiable
architecture search (DARTS) is largely hindered by its substantial memory cost
since the entire supernet resides in the memory. This is where the single-path
DARTS comes in, which only chooses a single-path submodel at each step. While
being memory-friendly, it also comes with low computational costs. Nonetheless,
we discover a critical issue of single-path DARTS that has not been primarily
noticed. Namely, it also suffers from severe performance collapse since too
many parameter-free operations like skip connections are derived, just like
DARTS does. In this paper, we propose a new algorithm called RObustifying
Memory-Efficient NAS (ROME) to give a cure. First, we disentangle the topology
search from the operation search to make searching and evaluation consistent.
We then adopt Gumbel-Top2 reparameterization and gradient accumulation to
robustify the unwieldy bi-level optimization. We verify ROME extensively across
15 benchmarks to demonstrate its effectiveness and robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoxing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1&quot;&gt;Xiangxiang Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yuda Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhexi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaokang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.06757">
<title>Auto-COP: Adaptation Generation in Context-Oriented Programming using Reinforcement Learning Options. (arXiv:2103.06757v2 [cs.PL] UPDATED)</title>
<link>http://arxiv.org/abs/2103.06757</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-adaptive software systems continuously adapt in response to internal and
external changes in their execution environment, captured as contexts. The COP
paradigm posits a technique for the development of self-adaptive systems,
capturing their main characteristics with specialized programming language
constructs. COP adaptations are specified as independent modules composed in
and out of the base system as contexts are activated and deactivated in
response to sensed circumstances from the surrounding environment. However, the
definition of adaptations, their contexts and associated specialized behavior,
need to be specified at design time. In complex CPS this is intractable due to
new unpredicted operating conditions. We propose Auto-COP, a new technique to
enable generation of adaptations at run time. Auto-COP uses RL options to build
action sequences, based on the previous instances of the system execution.
Options are explored in interaction with the environment, and the most suitable
options for each context are used to generate adaptations exploiting COP. To
validate Auto-COP, we present two case studies exhibiting different system
characteristics and application domains: a driving assistant and a robot
delivery system. We present examples of Auto-COP code generated at run time, to
illustrate the types of circumstances (contexts) requiring adaptation, and the
corresponding generated adaptations for each context. We confirm that the
generated adaptations exhibit correct system behavior measured by
domain-specific performance metrics, while reducing the number of required
execution/actuation steps by a factor of two showing that the adaptations are
regularly selected by the running system as adaptive behavior is more
appropriate than the execution of primitive actions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardozo_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xe1;s Cardozo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dusparic_I/0/1/0/all/0/1&quot;&gt;Ivana Dusparic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.03110">
<title>Successor Feature Neural Episodic Control. (arXiv:2111.03110v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2111.03110</link>
<description rdf:parseType="Literal">&lt;p&gt;A longstanding goal in reinforcement learning is to build intelligent agents
that show fast learning and a flexible transfer of skills akin to humans and
animals. This paper investigates the integration of two frameworks for tackling
those goals: episodic control and successor features. Episodic control is a
cognitively inspired approach relying on episodic memory, an instance-based
memory model of an agent&apos;s experiences. Meanwhile, successor features and
generalized policy improvement (SF&amp;amp;GPI) is a meta and transfer learning
framework allowing to learn policies for tasks that can be efficiently reused
for later tasks which have a different reward function. Individually, these two
techniques have shown impressive results in vastly improving sample efficiency
and the elegant reuse of previously learned policies. Thus, we outline a
combination of both approaches in a single reinforcement learning framework and
empirically illustrate its benefits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emukpere_D/0/1/0/all/0/1&quot;&gt;David Emukpere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1&quot;&gt;Xavier Alameda-Pineda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reinke_C/0/1/0/all/0/1&quot;&gt;Chris Reinke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.13619">
<title>Fairness in Recommendation: Foundations, Methods and Applications. (arXiv:2205.13619v6 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2205.13619</link>
<description rdf:parseType="Literal">&lt;p&gt;As one of the most pervasive applications of machine learning, recommender
systems are playing an important role on assisting human decision making. The
satisfaction of users and the interests of platforms are closely related to the
quality of the generated recommendation results. However, as a highly
data-driven system, recommender system could be affected by data or algorithmic
bias and thus generate unfair results, which could weaken the reliance of the
systems. As a result, it is crucial to address the potential unfairness
problems in recommendation settings. Recently, there has been growing attention
on fairness considerations in recommender systems with more and more literature
on approaches to promote fairness in recommendation. However, the studies are
rather fragmented and lack a systematic organization, thus making it difficult
to penetrate for new researchers to the domain. This motivates us to provide a
systematic survey of existing works on fairness in recommendation. This survey
focuses on the foundations for fairness in recommendation literature. It first
presents a brief introduction about fairness in basic machine learning tasks
such as classification and ranking in order to provide a general overview of
fairness research, as well as introduce the more complex situations and
challenges that need to be considered when studying fairness in recommender
systems. After that, the survey will introduce fairness in recommendation with
a focus on the taxonomies of current fairness definitions, the typical
techniques for improving fairness, as well as the datasets for fairness studies
in recommendation. The survey also talks about the challenges and opportunities
in fairness research with the hope of promoting the fair recommendation
research area and beyond.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanxiong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1&quot;&gt;Juntao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shuchang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02144">
<title>No Agreement Without Loss: Learning and Social Choice in Peer Review. (arXiv:2211.02144v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2211.02144</link>
<description rdf:parseType="Literal">&lt;p&gt;In peer review systems, reviewers are often asked to evaluate various
features of submissions, such as technical quality or novelty. A score is given
to each of the predefined features and based on these the reviewer has to
provide an overall quantitative recommendation. It may be assumed that each
reviewer has her own mapping from the set of features to a recommendation, and
that different reviewers have different mappings in mind. This introduces an
element of arbitrariness known as commensuration bias. In this paper we discuss
a framework, introduced by Noothigattu, Shah and Procaccia, and then applied by
the organizers of the AAAI 2022 conference. Noothigattu, Shah and Procaccia
proposed to aggregate reviewer&apos;s mapping by minimizing certain loss functions,
and studied axiomatic properties of this approach, in the sense of social
choice theory. We challenge several of the results and assumptions used in
their work and report a number of negative results. On the one hand, we study a
trade-off between some of the axioms proposed and the ability of the method to
properly capture agreements of the majority of reviewers. On the other hand, we
show that dropping a certain unrealistic assumption has dramatic effects,
including causing the method to be discontinuous.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barcelo_P/0/1/0/all/0/1&quot;&gt;Pablo Barcel&amp;#xf3;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duarte_M/0/1/0/all/0/1&quot;&gt;Mauricio Duarte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rojas_C/0/1/0/all/0/1&quot;&gt;Crist&amp;#xf3;bal Rojas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steifer_T/0/1/0/all/0/1&quot;&gt;Tomasz Steifer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.03188">
<title>An Unsupervised Machine Learning Approach for Ground-Motion Spectra Clustering and Selection. (arXiv:2212.03188v2 [physics.geo-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2212.03188</link>
<description rdf:parseType="Literal">&lt;p&gt;Clustering analysis of sequence data continues to address many applications
in engineering design, aided with the rapid growth of machine learning in
applied science. This paper presents an unsupervised machine learning algorithm
to extract defining characteristics of earthquake ground-motion spectra, also
called latent features, to aid in ground-motion selection (GMS). In this
context, a latent feature is a low-dimensional machine-discovered spectral
characteristic learned through nonlinear relationships of a neural network
autoencoder. Machine discovered latent features can be combined with
traditionally defined intensity measures and clustering can be performed to
select a representative subgroup from a large ground-motion suite. The
objective of efficient GMS is to choose characteristic records representative
of what the structure will probabilistically experience in its lifetime. Three
examples are presented to validate this approach, including the use of
synthetic and field recorded ground-motion datasets. The presented deep
embedding clustering of ground-motion spectra has three main advantages: 1.
defining characteristics the represent the sparse spectral content of
ground-motions are discovered efficiently through training of the autoencoder,
2. domain knowledge is incorporated into the machine learning framework with
conditional variables in the deep embedding scheme, and 3. method exhibits
excellent performance when compared to a benchmark seismic hazard analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bond_R/0/1/0/all/0/1&quot;&gt;R. Bailey Bond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ren_P/0/1/0/all/0/1&quot;&gt;Pu Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hajjar_J/0/1/0/all/0/1&quot;&gt;Jerome F. Hajjar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Hao Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.04385">
<title>BEVBert: Multimodal Map Pre-training for Language-guided Navigation. (arXiv:2212.04385v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.04385</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale pre-training has shown promising results on the
vision-and-language navigation (VLN) task. However, most existing pre-training
methods employ discrete panoramas to learn visual-textual associations. This
requires the model to implicitly correlate incomplete, duplicate observations
within the panoramas, which may impair an agent&apos;s spatial understanding. Thus,
we propose a new map-based pre-training paradigm that is spatial-aware for use
in VLN. Concretely, we build a local metric map to explicitly aggregate
incomplete observations and remove duplicates, while modeling navigation
dependency in a global topological map. This hybrid design can balance the
demand of VLN for both short-term reasoning and long-term planning. Then, based
on the hybrid map, we devise a pre-training framework to learn a multimodal map
representation, which enhances spatial-aware cross-modal reasoning thereby
facilitating the language-guided navigation goal. Extensive experiments
demonstrate the effectiveness of the map-based pre-training route for VLN, and
the proposed method achieves state-of-the-art on four VLN benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1&quot;&gt;Dong An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yuankai Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yangguang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1&quot;&gt;Tieniu Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jing Shao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.06267">
<title>Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models. (arXiv:2301.06267v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.06267</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to quickly learn a new task with minimal instruction - known as
few-shot learning - is a central aspect of intelligent agents. Classical
few-shot benchmarks make use of few-shot samples from a single modality, but
such samples may not be sufficient to characterize an entire concept class. In
contrast, humans use cross-modal information to learn new concepts efficiently.
In this work, we demonstrate that one can indeed build a better ${\bf visual}$
dog classifier by ${\bf read}$ing about dogs and ${\bf listen}$ing to them
bark. To do so, we exploit the fact that recent multimodal foundation models
such as CLIP are inherently cross-modal, mapping different modalities to the
same representation space. Specifically, we propose a simple cross-modal
adaptation approach that learns from few-shot examples spanning different
modalities. By repurposing class names as additional one-shot training samples,
we achieve SOTA results with an embarrassingly simple linear classifier for
vision-language adaptation. Furthermore, we show that our approach can benefit
existing methods such as prefix tuning, adapters, and classifier ensembling.
Finally, to explore other modalities beyond vision and language, we construct
the first (to our knowledge) audiovisual few-shot benchmark and use cross-modal
training to improve the performance of both image and audio classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhiqiu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Samuel Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1&quot;&gt;Zhiyi Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1&quot;&gt;Deepak Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1&quot;&gt;Deva Ramanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05118">
<title>SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model. (arXiv:2303.05118v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05118</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of continual learning is to improve the performance of recognition
models in learning sequentially arrived data. Although most existing works are
established on the premise of learning from scratch, growing efforts have been
devoted to incorporating the benefits of pre-training. However, how to
adaptively exploit the pre-trained knowledge for each incremental task while
maintaining its generalizability remains an open question. In this work, we
present an extensive analysis for continual learning on a pre-trained model
(CLPM), and attribute the key challenge to a progressive overfitting problem.
Observing that selectively reducing the learning rate can almost resolve this
issue in the representation layer, we propose a simple but extremely effective
approach named Slow Learner with Classifier Alignment (SLCA), which further
improves the classification layer by modeling the class-wise distributions and
aligning the classification layers in a post-hoc fashion. Across a variety of
scenarios, our proposal provides substantial improvements for CLPM (e.g., up to
49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split ImageNet-R, Split
CUB-200 and Split Cars-196, respectively), and thus outperforms
state-of-the-art approaches by a large margin. Based on such a strong baseline,
critical factors and promising directions are analyzed in-depth to facilitate
subsequent research. Code has been made available at:
https://github.com/GengDavid/SLCA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Gengwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1&quot;&gt;Guoliang Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Ling Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yunchao Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06050">
<title>Optimal foraging strategies can be learned. (arXiv:2303.06050v3 [cond-mat.stat-mech] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06050</link>
<description rdf:parseType="Literal">&lt;p&gt;The foraging behavior of animals is a paradigm of target search in nature.
Understanding which foraging strategies are optimal and how animals learn them
are central challenges in modeling animal foraging. While the question of
optimality has wide-ranging implications across fields such as economy,
physics, and ecology, the question of learnability is a topic of ongoing debate
in evolutionary biology. Recognizing the interconnected nature of these
challenges, this work addresses them simultaneously by exploring optimal
foraging strategies through a reinforcement learning framework. To this end, we
model foragers as learning agents. We first prove theoretically that maximizing
rewards in our reinforcement learning model is equivalent to optimizing
foraging efficiency. We then show with numerical experiments that, in the
paradigmatic model of non-destructive search, our agents learn foraging
strategies which outperform the efficiency of some of the best known strategies
such as L\&apos;evy walks. These findings highlight the potential of reinforcement
learning as a versatile framework not only for optimizing search strategies but
also to model the learning process, thus shedding light on the role of learning
in natural optimization processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Munoz_Gil_G/0/1/0/all/0/1&quot;&gt;Gorka Mu&amp;#xf1;oz-Gil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Lopez_Incera_A/0/1/0/all/0/1&quot;&gt;Andrea L&amp;#xf3;pez-Incera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Fiderer_L/0/1/0/all/0/1&quot;&gt;Lukas J. Fiderer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Briegel_H/0/1/0/all/0/1&quot;&gt;Hans J. Briegel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11098">
<title>A closer look at the training dynamics of knowledge distillation. (arXiv:2303.11098v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11098</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we revisit the efficacy of knowledge distillation as a function
matching and metric learning problem. In doing so we verify three important
design decisions, namely the normalisation, soft maximum function, and
projection layers as key ingredients. We theoretically show that the projector
implicitly encodes information on past examples, enabling relational gradients
for the student. We then show that the normalisation of representations is
tightly coupled with the training dynamics of this projector, which can have a
large impact on the students performance. Finally, we show that a simple soft
maximum function can be used to address any significant capacity gap problems.
Experimental results on various benchmark datasets demonstrate that using these
insights can lead to superior or comparable performance to state-of-the-art
knowledge distillation techniques, despite being much more computationally
efficient. In particular, we obtain these results across image classification
(CIFAR100 and ImageNet), object detection (COCO2017), and on more difficult
distillation objectives, such as training data efficient transformers, whereby
we attain a 77.2% top-1 accuracy with DeiT-Ti on ImageNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miles_R/0/1/0/all/0/1&quot;&gt;Roy Miles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1&quot;&gt;Krystian Mikolajczyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04370">
<title>OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04370</link>
<description rdf:parseType="Literal">&lt;p&gt;Human intelligence excels at combining basic skills to solve complex tasks.
This capability is vital for Artificial Intelligence (AI) and should be
embedded in comprehensive intelligent models, enabling them to harness expert
models for complex task-solving towards Artificial General Intelligence (AGI).
Large Language Models (LLMs) show promising learning and reasoning abilities,
and can effectively use external models, tools or APIs to tackle complex
problems. In this work, we introduce OpenAGI, an open-source AGI research
platform designed for multi-step, real-world tasks. Specifically, OpenAGI uses
a dual strategy, integrating standard benchmark tasks for benchmarking and
evaluation, and open-ended tasks including more expandable models, tools or
APIs for creative problem-solving. Tasks are presented as natural language
queries to the LLM, which then selects and executes appropriate models. We also
propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses
task results to improve the LLM&apos;s ability, which creates a self-improving AI
feedback loop. While we acknowledge that AGI is a broad and multifaceted
research challenge with no singularly defined solution path, the integration of
LLMs with domain-specific expert models, inspired by mirroring the blend of
general and specialized intelligence in humans, offers a promising approach
towards AGI. We are open-sourcing the OpenAGI project&apos;s code, dataset,
benchmarks, evaluation methods, and demo to foster community involvement in AGI
advancement: https://github.com/agiresearch/OpenAGI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1&quot;&gt;Wenyue Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_K/0/1/0/all/0/1&quot;&gt;Kai Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jianchao Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1&quot;&gt;Juntao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zelong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02897">
<title>An automatically discovered chain-of-thought prompt generalizes to novel models and datasets. (arXiv:2305.02897v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02897</link>
<description rdf:parseType="Literal">&lt;p&gt;Emergent chain-of-thought (CoT) reasoning capabilities promise to improve
performance and explainability of large language models (LLMs). However,
uncertainties remain about how reasoning strategies formulated for previous
model generations generalize to new model generations and different datasets.
In this small-scale study, we compare different reasoning strategies induced by
zero-shot prompting across six recently released LLMs (davinci-002,
davinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Cohere command-xlarge) on a
mixture of six question-answering datasets, including datasets from scientific
and medical domains. Our findings demonstrate that while some variations in
effectiveness occur, gains from CoT reasoning strategies remain robust across
different models and datasets. GPT-4 has the most benefit from current
state-of-the-art reasoning strategies and exhibits the best performance by
applying a prompt previously discovered through automated discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hebenstreit_K/0/1/0/all/0/1&quot;&gt;Konstantin Hebenstreit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Praas_R/0/1/0/all/0/1&quot;&gt;Robert Praas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiesewetter_L/0/1/0/all/0/1&quot;&gt;Louis P Kiesewetter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1&quot;&gt;Matthias Samwald&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04800">
<title>Mlinear: Rethink the Linear Model for Time-series Forecasting. (arXiv:2305.04800v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04800</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, significant advancements have been made in time-series forecasting
research, with an increasing focus on analyzing the nature of time-series data,
e.g, channel-independence (CI) and channel-dependence (CD), rather than solely
focusing on designing sophisticated forecasting models. However, current
research has primarily focused on either CI or CD in isolation, and the
challenge of effectively combining these two opposing properties to achieve a
synergistic effect remains an unresolved issue. In this paper, we carefully
examine the opposing properties of CI and CD, and raise a practical question
that has not been effectively answered, e.g.,&quot;How to effectively mix the CI and
CD properties of time series to achieve better predictive performance?&quot; To
answer this question, we propose Mlinear (MIX-Linear), a simple yet effective
method based mainly on linear layers. The design philosophy of Mlinear mainly
includes two aspects:(1) dynamically tuning the CI and CD properties based on
the time semantics of different input time series, and (2) providing deep
supervision to adjust the individual performance of the &quot;CI predictor&quot; and &quot;CD
predictor&quot;. In addition, empirically, we introduce a new loss function that
significantly outperforms the widely used mean squared error (MSE) on multiple
datasets. Experiments on time-series datasets covering multiple fields and
widely used have demonstrated the superiority of our method over PatchTST which
is the lateset Transformer-based method in terms of the MSE and MAE metrics on
7 datasets with identical sequence inputs (336 or 512). Specifically, our
method significantly outperforms PatchTST with a ratio of 21:3 at 336 sequence
length input and 29:10 at 512 sequence length input. Additionally, our approach
has a 10 $\times$ efficiency advantage at the unit level, taking into account
both training and inference times.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xiangxu Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chuhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianing Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10406">
<title>Variational Classification. (arXiv:2305.10406v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10406</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a latent variable generalisation of neural network softmax
classification trained with cross-entropy loss, referred to as variational
classification (VC). Our approach offers a novel probabilistic perspective on
the highly familiar softmax classification model, to which it relates similarly
to how variational and traditional autoencoders relate. We derive a training
objective based on the evidence lower bound (ELBO) that is non-trivial to
optimize, and therefore propose an adversarial approach to maximise it. We show
that VC addresses an inherent inconsistency within softmax classification,
whilst also allowing more flexible choices of prior distributions in the latent
space in place of implicit assumptions revealed within off-the-shelf softmax
classifiers. Empirical evaluation on image and text classification datasets
demonstrates that variational classification maintains prediction accuracy
while improving other desirable properties such as calibration and adversarial
robustness, particularly under distribution shift and low data settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1&quot;&gt;Shehzaad Dhuliawala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1&quot;&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1&quot;&gt;Carl Allen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13501">
<title>LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On. (arXiv:2305.13501v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13501</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapidly evolving fields of e-commerce and metaverse continue to seek
innovative approaches to enhance the consumer experience. At the same time,
recent advancements in the development of diffusion models have enabled
generative networks to create remarkably realistic images. In this context,
image-based virtual try-on, which consists in generating a novel image of a
target model wearing a given in-shop garment, has yet to capitalize on the
potential of these powerful generative solutions. This work introduces
LaDI-VTON, the first Latent Diffusion textual Inversion-enhanced model for the
Virtual Try-ON task. The proposed architecture relies on a latent diffusion
model extended with a novel additional autoencoder module that exploits
learnable skip connections to enhance the generation process preserving the
model&apos;s characteristics. To effectively maintain the texture and details of the
in-shop garment, we propose a textual inversion component that can map the
visual features of the garment to the CLIP token embedding space and thus
generate a set of pseudo-word token embeddings capable of conditioning the
generation process. Experimental results on Dress Code and VITON-HD datasets
demonstrate that our approach outperforms the competitors by a consistent
margin, achieving a significant milestone for the task. Source code and trained
models are publicly available at: https://github.com/miccunifi/ladi-vton.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morelli_D/0/1/0/all/0/1&quot;&gt;Davide Morelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldrati_A/0/1/0/all/0/1&quot;&gt;Alberto Baldrati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cartella_G/0/1/0/all/0/1&quot;&gt;Giuseppe Cartella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1&quot;&gt;Marcella Cornia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertini_M/0/1/0/all/0/1&quot;&gt;Marco Bertini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1&quot;&gt;Rita Cucchiara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16174">
<title>From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module. (arXiv:2305.16174v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16174</link>
<description rdf:parseType="Literal">&lt;p&gt;Latent Graph Inference (LGI) relaxed the reliance of Graph Neural Networks
(GNNs) on a given graph topology by dynamically learning it. However, most of
LGI methods assume to have a (noisy, incomplete, improvable, ...) input graph
to rewire and can solely learn regular graph topologies. In the wake of the
success of Topological Deep Learning (TDL), we study Latent Topology Inference
(LTI) for learning higher-order cell complexes (with sparse and not regular
topology) describing multi-way interactions between data points. To this aim,
we introduce the Differentiable Cell Complex Module (DCM), a novel learnable
function that computes cell probabilities in the complex to improve the
downstream task. We show how to integrate DCM with cell complex message passing
networks layers and train it in a end-to-end fashion, thanks to a two-step
inference procedure that avoids an exhaustive search across all possible cells
in the input, thus maintaining scalability. Our model is tested on several
homophilic and heterophilic graph datasets and it is shown to outperform other
state-of-the-art techniques, offering significant improvements especially in
cases where an input graph is not provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Battiloro_C/0/1/0/all/0/1&quot;&gt;Claudio Battiloro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spinelli_I/0/1/0/all/0/1&quot;&gt;Indro Spinelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Telyatnikov_L/0/1/0/all/0/1&quot;&gt;Lev Telyatnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1&quot;&gt;Michael Bronstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scardapane_S/0/1/0/all/0/1&quot;&gt;Simone Scardapane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorenzo_P/0/1/0/all/0/1&quot;&gt;Paolo Di Lorenzo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19569">
<title>Domain knowledge-informed Synthetic fault sample generation with Health Data Map for cross-domain Planetary Gearbox Fault Diagnosis. (arXiv:2305.19569v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19569</link>
<description rdf:parseType="Literal">&lt;p&gt;Extensive research has been conducted on fault diagnosis of planetary
gearboxes using vibration signals and deep learning (DL) approaches. However,
DL-based methods are susceptible to the domain shift problem caused by varying
operating conditions of the gearbox. Although domain adaptation and data
synthesis methods have been proposed to overcome such domain shifts, they are
often not directly applicable in real-world situations where only healthy data
is available in the target domain. To tackle the challenge of extreme domain
shift scenarios where only healthy data is available in the target domain, this
paper proposes two novel domain knowledge-informed data synthesis methods
utilizing the health data map (HDMap). The two proposed approaches are referred
to as scaled CutPaste and FaultPaste. The HDMap is used to physically represent
the vibration signal of the planetary gearbox as an image-like matrix, allowing
for visualization of fault-related features. CutPaste and FaultPaste are then
applied to generate faulty samples based on the healthy data in the target
domain, using domain knowledge and fault signatures extracted from the source
domain, respectively. In addition to generating realistic faults, the proposed
methods introduce scaling of fault signatures for controlled synthesis of
faults with various severity levels. A case study is conducted on a planetary
gearbox testbed to evaluate the proposed approaches. The results show that the
proposed methods are capable of accurately diagnosing faults, even in cases of
extreme domain shift, and can estimate the severity of faults that have not
been previously observed in the target domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1&quot;&gt;Jong Moon Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fink_O/0/1/0/all/0/1&quot;&gt;Olga Fink&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04643">
<title>Abnormal Trading Detection in the NFT Market. (arXiv:2306.04643v2 [q-fin.TR] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04643</link>
<description rdf:parseType="Literal">&lt;p&gt;The Non-Fungible-Token (NFT) market has experienced explosive growth in
recent years. According to DappRadar, the total transaction volume on OpenSea,
the largest NFT marketplace, reached 34.7 billion dollars in February 2023.
However, the NFT market is mostly unregulated and there are significant
concerns about money laundering, fraud and wash trading. The lack of
industry-wide regulations, and the fact that amateur traders and retail
investors comprise a significant fraction of the NFT market, make this market
particularly vulnerable to fraudulent activities. Therefore it is essential to
investigate and highlight the relevant risks involved in NFT trading. In this
paper, we attempted to uncover common fraudulent behaviors such as wash trading
that could mislead other traders. Using market data, we designed quantitative
features from the network, monetary, and temporal perspectives that were fed
into K-means clustering unsupervised learning algorithm to sort traders into
groups. Lastly, we discussed the clustering results&apos; significance and how
regulations can reduce undesired behaviors. Our work can potentially help
regulators narrow down their search space for bad actors in the market as well
as provide insights for amateur traders to protect themselves from unforeseen
frauds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Song_M/0/1/0/all/0/1&quot;&gt;Mingxiao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yunsong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Shah_A/0/1/0/all/0/1&quot;&gt;Agam Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Chava_S/0/1/0/all/0/1&quot;&gt;Sudheer Chava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05357">
<title>Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models. (arXiv:2306.05357v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05357</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image generative models have enabled high-resolution image synthesis
across different domains, but require users to specify the content they wish to
generate. In this paper, we consider the inverse problem -- given a collection
of different images, can we discover the generative concepts that represent
each image? We present an unsupervised approach to discover generative concepts
from a collection of images, disentangling different art styles in paintings,
objects, and lighting from kitchen scenes, and discovering image classes given
ImageNet images. We show how such generative concepts can accurately represent
the content of images, be recombined and composed to generate new artistic and
hybrid images, and be further used as a representation for downstream
classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Nan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yilun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1&quot;&gt;Antonio Torralba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06548">
<title>Inductive reasoning in humans and large language models. (arXiv:2306.06548v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06548</link>
<description rdf:parseType="Literal">&lt;p&gt;The impressive recent performance of large language models has led many to
wonder to what extent they can serve as models of general intelligence or are
similar to human cognition. We address this issue by applying GPT-3.5 and GPT-4
to a classic problem in human inductive reasoning known as property induction.
Over two experiments, we elicit human judgments on a range of property
induction tasks spanning multiple domains. Although GPT-3.5 struggles to
capture many aspects of human behaviour, GPT-4 is much more successful: for the
most part, its performance qualitatively matches that of humans, and the only
notable exception is its failure to capture the phenomenon of premise
non-monotonicity. Our work demonstrates that property induction allows for
interesting comparisons between human and machine intelligence and provides two
large datasets that can serve as benchmarks for future work in this vein.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Simon J. Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ransom_K/0/1/0/all/0/1&quot;&gt;Keith Ransom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perfors_A/0/1/0/all/0/1&quot;&gt;Andrew Perfors&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kemp_C/0/1/0/all/0/1&quot;&gt;Charles Kemp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11363">
<title>Masked Diffusion Models Are Fast and Privacy-Aware Learners. (arXiv:2306.11363v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11363</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have emerged as the \emph{de-facto} technique for image
generation, yet they entail significant computational overhead, hindering the
technique&apos;s broader application in the research community. We propose a
prior-based denoising training framework, the first to incorporate the
pre-train and fine-tune paradigm into the diffusion model training process,
which substantially improves training efficiency and shows potential in
facilitating various downstream tasks. Our approach centers on masking a high
proportion (e.g., up to 90\%) of the input image and employing masked denoising
score matching to denoise the visible areas, thereby guiding the diffusion
model to learn more salient features from training data as prior knowledge. By
utilizing masked learning in a pre-training stage, we efficiently train the
ViT-based diffusion model on CelebA-HQ $256 \times 256$ in the pixel space,
achieving a 4x acceleration and enhancing the quality of generated images
compared to denoising diffusion probabilistic model (DDPM). Moreover, our
masked pre-training technique can be universally applied to various diffusion
models that directly generate images in the pixel space, aiding in the learning
of pre-trained models with superior generalizability. For instance, a diffusion
model pre-trained on VGGFace2 attains a 46\% quality improvement through
fine-tuning with merely 10\% data from a different distribution. Moreover, our
method shows the potential to serve as a training paradigm for enhancing the
privacy protection capabilities of diffusion models. Our code is available at
\url{https://github.com/jiachenlei/maskdm}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1&quot;&gt;Jiachen Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1&quot;&gt;Peng Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ba_Z/0/1/0/all/0/1&quot;&gt;Zhongjie Ba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1&quot;&gt;Kui Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15989">
<title>Freespace Optical Flow Modeling for Automated Driving. (arXiv:2307.15989v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15989</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical flow and disparity are two informative visual features for autonomous
driving perception. They have been used for a variety of applications, such as
obstacle and lane detection. The concept of &quot;U-V-Disparity&quot; has been widely
explored in the literature, while its counterpart in optical flow has received
relatively little attention. Traditional motion analysis algorithms estimate
optical flow by matching correspondences between two successive video frames,
which limits the full utilization of environmental information and geometric
constraints. Therefore, we propose a novel strategy to model optical flow in
the collision-free space (also referred to as drivable area or simply
freespace) for intelligent vehicles, with the full utilization of geometry
information in a 3D driving environment. We provide explicit representations of
optical flow and deduce the quadratic relationship between the optical flow
component and the vertical coordinate. Through extensive experiments on several
public datasets, we demonstrate the high accuracy and robustness of our model.
Additionally, our proposed freespace optical flow model boasts a diverse array
of applications within the realm of automated driving, providing a geometric
constraint in freespace detection, vehicle localization, and more. We have made
our source code publicly available at https://mias.group/FSOF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;Jiayuan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qijun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1&quot;&gt;Rui Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16149">
<title>An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid. (arXiv:2307.16149v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16149</link>
<description rdf:parseType="Literal">&lt;p&gt;Energy theft detection (ETD) and energy consumption forecasting (ECF) are two
interconnected challenges in smart grid systems. Addressing these issues
collectively is crucial for ensuring system security. This paper addresses the
interconnected challenges of ETD and ECF in smart grid systems. The proposed
solution combines long short-term memory (LSTM) and a denoising diffusion
probabilistic model (DDPM) to generate input reconstruction and forecasting. By
leveraging the reconstruction and forecasting errors, the system identifies
instances of energy theft, with the methods based on reconstruction error and
forecasting error complementing each other in detecting different types of
attacks. Through extensive experiments on real-world and synthetic datasets,
the proposed scheme outperforms baseline methods in ETD and ECF problems. The
ensemble method significantly enhances ETD performance, accurately detecting
energy theft attacks that baseline methods fail to detect. The research offers
a comprehensive and effective solution for addressing ETD and ECF challenges,
demonstrating promising results and improved security in smart grid systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alromih_A/0/1/0/all/0/1&quot;&gt;Arwa Alromih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gope_P/0/1/0/all/0/1&quot;&gt;Prosanta Gope&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sikdar_B/0/1/0/all/0/1&quot;&gt;Biplab Sikdar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16173">
<title>Data-Driven Modeling with Experimental Augmentation for the Modulation Strategy of the Dual-Active-Bridge Converter. (arXiv:2307.16173v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16173</link>
<description rdf:parseType="Literal">&lt;p&gt;For the performance modeling of power converters, the mainstream approaches
are essentially knowledge-based, suffering from heavy manpower burden and low
modeling accuracy. Recent emerging data-driven techniques greatly relieve human
reliance by automatic modeling from simulation data. However, model discrepancy
may occur due to unmodeled parasitics, deficient thermal and magnetic models,
unpredictable ambient conditions, etc. These inaccurate data-driven models
based on pure simulation cannot represent the practical performance in physical
world, hindering their applications in power converter modeling. To alleviate
model discrepancy and improve accuracy in practice, this paper proposes a novel
data-driven modeling with experimental augmentation (D2EA), leveraging both
simulation data and experimental data. In D2EA, simulation data aims to
establish basic functional landscape, and experimental data focuses on matching
actual performance in real world. The D2EA approach is instantiated for the
efficiency optimization of a hybrid modulation for neutral-point-clamped
dual-active-bridge (NPC-DAB) converter. The proposed D2EA approach realizes
99.92% efficiency modeling accuracy, and its feasibility is comprehensively
validated in 2-kW hardware experiments, where the peak efficiency of 98.45% is
attained. Overall, D2EA is data-light and can achieve highly accurate and
highly practical data-driven models in one shot, and it is scalable to other
applications, effortlessly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinze Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pou_J/0/1/0/all/0/1&quot;&gt;Josep Pou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jiaxin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_F/0/1/0/all/0/1&quot;&gt;Fanfan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wen_C/0/1/0/all/0/1&quot;&gt;Changyun Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mukherjee_S/0/1/0/all/0/1&quot;&gt;Suvajit Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16387">
<title>Relation-Oriented: Toward Knowledge-Aligned Causal AI. (arXiv:2307.16387v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16387</link>
<description rdf:parseType="Literal">&lt;p&gt;In machine learning, we naturally apply an Observation-Oriented principle, in
which observational variables preexist and set the stage for constructing
relationships. While sufficient for traditional models, the integration of AI
with big data exposes the misalignment between the observational models and our
actual comprehension. Contrarily, humans shape cognitive entities defined by
relationships, enabling us to formulate knowledge across temporal and
hyper-dimensional spaces, rather than being confined to observational
constructs. From an innovative Relation-Oriented perspective, this study
examines the roots of this misalignment within our current modeling paradigm,
illuminated by intuitive examples from computer vision and health informatics.
We also introduce the relation-defined representation learning methodology as a
practical implementation of Relation-Oriented modeling, supported by extensive
experimental validation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16680">
<title>On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16680</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models and large language models have emerged as leading-edge
generative models and have sparked a revolutionary impact on various aspects of
human life. However, the practical implementation of these models has also
exposed inherent risks, highlighting their dual nature and raising concerns
regarding their trustworthiness. Despite the abundance of literature on this
subject, a comprehensive survey specifically delving into the intersection of
large-scale generative models and their trustworthiness remains largely absent.
To bridge this gap, This paper investigates both the long-standing and emerging
threats associated with these models across four fundamental dimensions:
privacy, security, fairness, and responsibility. In this way, we construct an
extensive map outlining the trustworthiness of these models, while also
providing practical recommendations and identifying future directions. These
efforts are crucial for promoting the trustworthy deployment of these models,
ultimately benefiting society as a whole.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1&quot;&gt;Mingyuan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jun Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00958">
<title>Isolation and Induction: Training Robust Deep Neural Networks against Model Stealing Attacks. (arXiv:2308.00958v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00958</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the broad application of Machine Learning models as a Service
(MLaaS), they are vulnerable to model stealing attacks. These attacks can
replicate the model functionality by using the black-box query process without
any prior knowledge of the target victim model. Existing stealing defenses add
deceptive perturbations to the victim&apos;s posterior probabilities to mislead the
attackers. However, these defenses are now suffering problems of high inference
computational overheads and unfavorable trade-offs between benign accuracy and
stealing robustness, which challenges the feasibility of deployed models in
practice. To address the problems, this paper proposes Isolation and Induction
(InI), a novel and effective training framework for model stealing defenses.
Instead of deploying auxiliary defense modules that introduce redundant
inference time, InI directly trains a defensive model by isolating the
adversary&apos;s training gradient from the expected gradient, which can effectively
reduce the inference computational cost. In contrast to adding perturbations
over model predictions that harm the benign accuracy, we train models to
produce uninformative outputs against stealing queries, which can induce the
adversary to extract little useful knowledge from victim models with minimal
impact on the benign performance. Extensive experiments on several visual
classification datasets (e.g., MNIST and CIFAR10) demonstrate the superior
robustness (up to 48% reduction on stealing accuracy) and speed (up to 25.4x
faster) of our InI over other state-of-the-art methods. Our codes can be found
in https://github.com/DIG-Beihang/InI-Model-Stealing-Defense.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jun Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Aishan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xingyu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1&quot;&gt;Siyuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yisong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yichao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianglong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01006">
<title>FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving. (arXiv:2308.01006v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01006</link>
<description rdf:parseType="Literal">&lt;p&gt;Building a multi-modality multi-task neural network toward accurate and
robust performance is a de-facto standard in perception task of autonomous
driving. However, leveraging such data from multiple sensors to jointly
optimize the prediction and planning tasks remains largely unexplored. In this
paper, we present FusionAD, to the best of our knowledge, the first unified
framework that fuse the information from two most critical sensors, camera and
LiDAR, goes beyond perception task. Concretely, we first build a transformer
based multi-modality fusion network to effectively produce fusion based
features. In constrast to camera-based end-to-end method UniAD, we then
establish a fusion aided modality-aware prediction and status-aware planning
modules, dubbed FMSPnP that take advantages of multi-modality features. We
conduct extensive experiments on commonly used benchmark nuScenes dataset, our
FusionAD achieves state-of-the-art performance and surpassing baselines on
average 15% on perception tasks like detection and tracking, 10% on occupancy
prediction accuracy, reducing prediction error from 0.708 to 0.389 in ADE score
and reduces the collision rate from 0.31% to only 0.12%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1&quot;&gt;Tengju Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_W/0/1/0/all/0/1&quot;&gt;Wei Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1&quot;&gt;Chunyong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shikun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Lingping Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fangzhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingke Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1&quot;&gt;Ke Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1&quot;&gt;Wencong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1&quot;&gt;Weibo Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junbo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Kaicheng Yu&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>