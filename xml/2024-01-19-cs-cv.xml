<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-17T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08581" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08584" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08593" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08599" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08615" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08623" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08639" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08714" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08732" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08734" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08740" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08787" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08809" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08825" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08837" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08840" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08847" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08860" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08876" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08903" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08943" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08965" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08968" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08972" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09029" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09047" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09049" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09083" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09140" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09146" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09160" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09180" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09252" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09266" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09271" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2003.06945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2006.07802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.09772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.02779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.10993" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.10973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.05109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04351" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14660" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07895" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15583" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00519" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.04001" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08480" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02992" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06031" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03996" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04960" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08846" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10165" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14238" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03145" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06637" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07825" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08332" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.08579">
<title>Curve-based Neural Style Transfer. (arXiv:2401.08579v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08579</link>
<description rdf:parseType="Literal">&lt;p&gt;This research presents a new parametric style transfer framework specifically
designed for curve-based design sketches. In this research, traditional
challenges faced by neural style transfer methods in handling binary sketch
transformations are effectively addressed through the utilization of parametric
shape-editing rules, efficient curve-to-pixel conversion techniques, and the
fine-tuning of VGG19 on ImageNet-Sketch, enhancing its role as a feature
pyramid network for precise style extraction. By harmonizing intuitive
curve-based imagery with rule-based editing, this study holds the potential to
significantly enhance design articulation and elevate the practice of style
transfer within the realm of product design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu-hsuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kara_L/0/1/0/all/0/1&quot;&gt;Levent Burak Kara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cagan_J/0/1/0/all/0/1&quot;&gt;Jonathan Cagan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08581">
<title>Temporal Embeddings: Scalable Self-Supervised Temporal Representation Learning from Spatiotemporal Data for Multimodal Computer Vision. (arXiv:2401.08581v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08581</link>
<description rdf:parseType="Literal">&lt;p&gt;There exists a correlation between geospatial activity temporal patterns and
type of land use. A novel self-supervised approach is proposed to stratify
landscape based on mobility activity time series. First, the time series signal
is transformed to the frequency domain and then compressed into task-agnostic
temporal embeddings by a contractive autoencoder, which preserves cyclic
temporal patterns observed in time series. The pixel-wise embeddings are
converted to image-like channels that can be used for task-based, multimodal
modeling of downstream geospatial tasks using deep semantic segmentation.
Experiments show that temporal embeddings are semantically meaningful
representations of time series data and are effective across different tasks
such as classifying residential area and commercial areas. Temporal embeddings
transform sequential, spatiotemporal motion trajectory data into semantically
meaningful image-like tensor representations that can be combined (multimodal
fusion) with other data modalities that are or can be transformed into
image-like tensor representations (for e.g., RBG imagery, graph embeddings of
road networks, passively collected imagery like SAR, etc.) to facilitate
multimodal learning in geospatial computer vision. Multimodal computer vision
is critical for training machine learning models for geospatial feature
detection to keep a geospatial mapping service up-to-date in real-time and can
significantly improve user experience and above all, user safety.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yi Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1&quot;&gt;Swetava Ganguli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_V/0/1/0/all/0/1&quot;&gt;Vipul Pandey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08584">
<title>Nahid: AI-based Algorithm for operating fully-automatic surgery. (arXiv:2401.08584v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08584</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, for the first time, a method is presented that can provide a
fully automated surgery based on software and computer vision techniques. Then,
the advantages and challenges of computerization of medical surgery are
examined. Finally, the surgery related to isolated ovarian endometriosis
disease has been examined, and based on the presented method, a more detailed
algorithm is presented that is capable of automatically diagnosing and treating
this disease during surgery as proof of our proposed method where a U-net is
trained to detect the endometriosis during surgery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saadati_S/0/1/0/all/0/1&quot;&gt;Sina Saadati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08587">
<title>Automatic extraction and 3D reconstruction of split wire from point cloud data based on improved DPC algorithm. (arXiv:2401.08587v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08587</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to solve the problem of point cloud data splitting improved by DPC
algorithm, a research on automatic separation and 3D reconstruction of point
cloud data split lines is proposed. First, the relative coordinates of each
point in the cloud point are calculated. Second, it is planned to develop a
relative ensemble-based DPC swarm algorithm for analyzing the number of
separation lines to determine all parts in the cloud content. Finally, fit each
separator using the least squares method. iron. The cloud point of the
resulting split subconductors has a clear demarcation line, and the distance
between adjacent split subconductors is 0.45 m, divided by the four vertices of
the square.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jia Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08588">
<title>Improved Pothole Detection Using YOLOv7 and ESRGAN. (arXiv:2401.08588v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08588</link>
<description rdf:parseType="Literal">&lt;p&gt;Potholes are common road hazards that is causing damage to vehicles and
posing a safety risk to drivers. The introduction of Convolutional Neural
Networks (CNNs) is widely used in the industry for object detection based on
Deep Learning methods and has achieved significant progress in hardware
improvement and software implementations. In this paper, a unique better
algorithm is proposed to warrant the use of low-resolution cameras or
low-resolution images and video feed for automatic pothole detection using
Super Resolution (SR) through Super Resolution Generative Adversarial Networks
(SRGANs). Then we have proceeded to establish a baseline pothole detection
performance on low quality and high quality dashcam images using a You Only
Look Once (YOLO) network, namely the YOLOv7 network. We then have illustrated
and examined the speed and accuracy gained above the benchmark after having
upscaling implementation on the low quality images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rout_N/0/1/0/all/0/1&quot;&gt;Nirmal Kumar Rout&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_G/0/1/0/all/0/1&quot;&gt;Gyanateet Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_V/0/1/0/all/0/1&quot;&gt;Varun Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1&quot;&gt;Arghadeep Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1&quot;&gt;Subhrangshu Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1&quot;&gt;Gopal Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08593">
<title>Automatic measurement of coverage area of water-based pesticides-surfactant formulation on plant leaves using deep learning tools. (arXiv:2401.08593v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08593</link>
<description rdf:parseType="Literal">&lt;p&gt;A method to efficiently and quantitatively study the delivery of a
pesticide-surfactant formulation in water solution over plants leaves is
presented. Instead of measuring the contact angle, the surface of the leaves
wet area is used as key parameter. To this goal, a deep learning model has been
trained and tested, to automatically measure the surface of area wet with water
solution over cucumber leaves, processing the frames of video footage. We have
individuated an existing deep learning model, reported in literature for other
applications, and we have applied it to this different task. We present the
measurement technique, some details of the deep learning model, its training
procedure and its image segmentation performance. Finally, we report the
results of the wet areas surface measurement as a function of the concentration
of a surfactant in the pesticide solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grazioso_F/0/1/0/all/0/1&quot;&gt;Fabio Grazioso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atsapina_A/0/1/0/all/0/1&quot;&gt;Anzhelika A. Atsapina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obaeed_G/0/1/0/all/0/1&quot;&gt;Gardoon L. O. Obaeed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ivanova_N/0/1/0/all/0/1&quot;&gt;Natalia A. Ivanova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08598">
<title>NutritionVerse-Real: An Open Access Manually Collected 2D Food Scene Dataset for Dietary Intake Estimation. (arXiv:2401.08598v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08598</link>
<description rdf:parseType="Literal">&lt;p&gt;Dietary intake estimation plays a crucial role in understanding the
nutritional habits of individuals and populations, aiding in the prevention and
management of diet-related health issues. Accurate estimation requires
comprehensive datasets of food scenes, including images, segmentation masks,
and accompanying dietary intake metadata. In this paper, we introduce
NutritionVerse-Real, an open access manually collected 2D food scene dataset
for dietary intake estimation with 889 images of 251 distinct dishes and 45
unique food types. The NutritionVerse-Real dataset was created by manually
collecting images of food scenes in real life, measuring the weight of every
ingredient and computing the associated dietary content of each dish using the
ingredient weights and nutritional information from the food packaging or the
Canada Nutrient File. Segmentation masks were then generated through human
labelling of the images. We provide further analysis on the data diversity to
highlight potential biases when using this data to develop models for dietary
intake estimation. NutritionVerse-Real is publicly available at
https://www.kaggle.com/datasets/nutritionverse/nutritionverse-real as part of
an open initiative to accelerate machine learning for dietary sensing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_C/0/1/0/all/0/1&quot;&gt;Chi-en Amy Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1&quot;&gt;Saeejith Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markham_O/0/1/0/all/0/1&quot;&gt;Olivia Markham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keller_M/0/1/0/all/0/1&quot;&gt;Matthew Keller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yifan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alexander Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08599">
<title>An annotated grain kernel image database for visual quality inspection. (arXiv:2401.08599v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08599</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a machine vision-based database named GrainSet for the purpose of
visual quality inspection of grain kernels. The database contains more than
350K single-kernel images with experts&apos; annotations. The grain kernels used in
the study consist of four types of cereal grains including wheat, maize,
sorghum and rice, and were collected from over 20 regions in 5 countries. The
surface information of each kernel is captured by our custom-built device
equipped with high-resolution optic sensor units, and corresponding sampling
information and annotations include collection location and time, morphology,
physical size, weight, and Damage &amp;amp; Unsound grain categories provided by senior
inspectors. In addition, we employed a commonly used deep learning model to
provide classification results as a benchmark. We believe that our GrainSet
will facilitate future research in fields such as assisting inspectors in grain
quality inspections, providing guidance for grain storage and trade, and
contributing to applications of smart agriculture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Lei Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yiwen Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1&quot;&gt;Dongdong Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1&quot;&gt;Hongxia Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pagnucco_M/0/1/0/all/0/1&quot;&gt;Maurice Pagnucco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yang Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08604">
<title>SAM4UDASS: When SAM Meets Unsupervised Domain Adaptive Semantic Segmentation in Intelligent Vehicles. (arXiv:2401.08604v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08604</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation plays a critical role in enabling intelligent vehicles
to comprehend their surrounding environments. However, deep learning-based
methods usually perform poorly in domain shift scenarios due to the lack of
labeled data for training. Unsupervised domain adaptation (UDA) techniques have
emerged to bridge the gap across different driving scenes and enhance model
performance on unlabeled target environments. Although self-training UDA
methods have achieved state-of-the-art results, the challenge of generating
precise pseudo-labels persists. These pseudo-labels tend to favor majority
classes, consequently sacrificing the performance of rare classes or small
objects like traffic lights and signs. To address this challenge, we introduce
SAM4UDASS, a novel approach that incorporates the Segment Anything Model (SAM)
into self-training UDA methods for refining pseudo-labels. It involves
Semantic-Guided Mask Labeling, which assigns semantic labels to unlabeled SAM
masks using UDA pseudo-labels. Furthermore, we devise fusion strategies aimed
at mitigating semantic granularity inconsistency between SAM masks and the
target domain. SAM4UDASS innovatively integrate SAM with UDA for semantic
segmentation in driving scenes and seamlessly complements existing
self-training UDA methodologies. Extensive experiments on synthetic-to-real and
normal-to-adverse driving datasets demonstrate its effectiveness. It brings
more than 3% mIoU gains on GTA5-to-Cityscapes, SYNTHIA-to-Cityscapes, and
Cityscapes-to-ACDC when using DAFormer and achieves SOTA when using MIC. The
code will be available at https://github.com/ywher/SAM4UDASS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1&quot;&gt;Weihao Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yeqiang Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xingyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_H/0/1/0/all/0/1&quot;&gt;Hanyang Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chunxiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08609">
<title>F4D: Factorized 4D Convolutional Neural Network for Efficient Video-level Representation Learning. (arXiv:2401.08609v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08609</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies have shown that video-level representation learning is crucial
to the capture and understanding of the long-range temporal structure for video
action recognition. Most existing 3D convolutional neural network (CNN)-based
methods for video-level representation learning are clip-based and focus only
on short-term motion and appearances. These CNN-based methods lack the capacity
to incorporate and model the long-range spatiotemporal representation of the
underlying video and ignore the long-range video-level context during training.
In this study, we propose a factorized 4D CNN architecture with attention (F4D)
that is capable of learning more effective, finer-grained, long-term
spatiotemporal video representations. We demonstrate that the proposed F4D
architecture yields significant performance improvements over the conventional
2D, and 3D CNN architectures proposed in the literature. Experiment evaluation
on five action recognition benchmark datasets, i.e., Something-Something-v1,
SomethingSomething-v2, Kinetics-400, UCF101, and HMDB51 demonstrate the
effectiveness of the proposed F4D network architecture for video-level action
recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Saad_M/0/1/0/all/0/1&quot;&gt;Mohammad Al-Saad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramaswamy_L/0/1/0/all/0/1&quot;&gt;Lakshmish Ramaswamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhandarkar_S/0/1/0/all/0/1&quot;&gt;Suchendra Bhandarkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08615">
<title>Online Anomaly Detection over Live Social Video Streaming. (arXiv:2401.08615v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08615</link>
<description rdf:parseType="Literal">&lt;p&gt;Social video anomaly is an observation in video streams that does not conform
to a common pattern of dataset&apos;s behaviour. Social video anomaly detection
plays a critical role in applications from e-commerce to e-learning.
Traditionally, anomaly detection techniques are applied to find anomalies in
video broadcasting. However, they neglect the live social video streams which
contain interactive talk, speech, or lecture with audience. In this paper, we
propose a generic framework for effectively online detecting Anomalies Over
social Video LIve Streaming (AOVLIS). Specifically, we propose a novel deep
neural network model called Coupling Long Short-Term Memory (CLSTM) that
adaptively captures the history behaviours of the presenters and audience, and
their mutual interactions to predict their behaviour at next time point over
streams. Then we well integrate the CLSTM with a decoder layer, and propose a
new reconstruction error-based scoring function $RE_{IA}$ to calculate the
anomaly score of each video segment for anomaly detection. After that, we
propose a novel model update scheme that incrementally maintains CLSTM and
decoder. Moreover, we design a novel upper bound and ADaptive Optimisation
Strategy (ADOS) for improving the efficiency of our solution. Extensive
experiments are conducted to prove the superiority of AOVLIS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1&quot;&gt;Chengkun He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiangmin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gondal_I/0/1/0/all/0/1&quot;&gt;Iqbal Gondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jie Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1&quot;&gt;Xun Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08623">
<title>Wake-Sleep Consolidated Learning. (arXiv:2401.08623v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2401.08623</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Wake-Sleep Consolidated Learning (WSCL), a learning strategy
leveraging Complementary Learning System theory and the wake-sleep phases of
the human brain to improve the performance of deep neural networks for visual
classification tasks in continual learning settings. Our method learns
continually via the synchronization between distinct wake and sleep phases.
During the wake phase, the model is exposed to sensory input and adapts its
representations, ensuring stability through a dynamic parameter freezing
mechanism and storing episodic memories in a short-term temporary memory
(similarly to what happens in the hippocampus). During the sleep phase, the
training process is split into NREM and REM stages. In the NREM stage, the
model&apos;s synaptic weights are consolidated using replayed samples from the
short-term and long-term memory and the synaptic plasticity mechanism is
activated, strengthening important connections and weakening unimportant ones.
In the REM stage, the model is exposed to previously-unseen realistic visual
sensory experience, and the dreaming process is activated, which enables the
model to explore the potential feature space, thus preparing synapses to future
knowledge. We evaluate the effectiveness of our approach on three benchmark
datasets: CIFAR-10, Tiny-ImageNet and FG-ImageNet. In all cases, our method
outperforms the baselines and prior work, yielding a significant performance
gain on continual visual classification tasks. Furthermore, we demonstrate the
usefulness of all processing stages and the importance of dreaming to enable
positive forward transfer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sorrenti_A/0/1/0/all/0/1&quot;&gt;Amelia Sorrenti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellitto_G/0/1/0/all/0/1&quot;&gt;Giovanni Bellitto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salanitri_F/0/1/0/all/0/1&quot;&gt;Federica Proietto Salanitri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pennisi_M/0/1/0/all/0/1&quot;&gt;Matteo Pennisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palazzo_S/0/1/0/all/0/1&quot;&gt;Simone Palazzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spampinato_C/0/1/0/all/0/1&quot;&gt;Concetto Spampinato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08629">
<title>Immature Green Apple Detection and Sizing in Commercial Orchards using YOLOv8 and Shape Fitting Techniques. (arXiv:2401.08629v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08629</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting and estimating size of apples during the early stages of growth is
crucial for predicting yield, pest management, and making informed decisions
related to crop-load management, harvest and post-harvest logistics, and
marketing. Traditional fruit size measurement methods are laborious and
time-consuming. This study employs the state-of-the-art YOLOv8 object detection
and instance segmentation algorithm in conjunction with geometric shape fitting
techniques on 3D point cloud data to accurately determine the size of immature
green apples (or fruitlet) in a commercial orchard environment. The methodology
utilized two RGB-D sensors: the Intel RealSense D435i and the Microsoft Azure
Kinect DK. Notably, the YOLOv8 instance segmentation models exhibited
proficiency in immature green apple detection, with the YOLOv8m-seg model
clinching the highest AP@0.5 and AP@0.75 scores of 0.94 and 0.91, respectively.
Leveraging the ellipsoid fitting technique on images from the Azure Kinect, we
observed remarkable metrics, including an RMSE of 2.35, MAE of 1.66, MAPE of
6.15, and an R-squared value of 0.9. Challenges such as partial occlusion,
where YOLOv8 sometimes misinterpreted immature green apple clusters, were
recognized. In a comparison of 102 outdoor samples, the Microsoft Azure Kinect
showed better performance than the Intel Realsense D435i, as supported by the
MAE data. This study emphasizes the combined effectiveness of shape-fitting
methods and 3D sensors in improving fruitlet sizing for agriculture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sapkota_R/0/1/0/all/0/1&quot;&gt;Ranjan Sapkota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_D/0/1/0/all/0/1&quot;&gt;Dawood Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Churuvija_M/0/1/0/all/0/1&quot;&gt;Martin Churuvija&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karkee_M/0/1/0/all/0/1&quot;&gt;Manoj Karkee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08633">
<title>Creating Visual Effects with Neural Radiance Fields. (arXiv:2401.08633v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08633</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a pipeline for integrating NeRFs into traditional compositing VFX
pipelines using Nerfstudio, an open-source framework for training and rendering
NeRFs. Our approach involves using Blender, a widely used open-source 3D
creation software, to align camera paths and composite NeRF renders with meshes
and other NeRFs, allowing for seamless integration of NeRFs into traditional
VFX pipelines. Our NeRF Blender add-on allows for more controlled camera
trajectories of photorealistic scenes, compositing meshes and other
environmental effects with NeRFs, and compositing multiple NeRFs in a single
scene.This approach of generating NeRF aligned camera paths can be adapted to
other 3D tool sets and workflows, enabling a more seamless integration of NeRFs
into visual effects and film production. Documentation can be found here:
https://docs.nerf.studio/extensions/blender_addon.html
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vachha_C/0/1/0/all/0/1&quot;&gt;Cyrus Vachha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08639">
<title>One-Step Diffusion Distillation via Deep Equilibrium Models. (arXiv:2401.08639v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08639</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models excel at producing high-quality samples but naively require
hundreds of iterations, prompting multiple attempts to distill the generation
process into a faster network. However, many existing approaches suffer from a
variety of challenges: the process for distillation training can be complex,
often requiring multiple training stages, and the resulting models perform
poorly when utilized in single-step generative applications. In this paper, we
introduce a simple yet effective means of distilling diffusion models directly
from initial noise to the resulting image. Of particular importance to our
approach is to leverage a new Deep Equilibrium (DEQ) model as the distilled
architecture: the Generative Equilibrium Transformer (GET). Our method enables
fully offline training with just noise/image pairs from the diffusion model
while achieving superior performance compared to existing one-step methods on
comparable training budgets. We demonstrate that the DEQ architecture is
crucial to this capability, as GET matches a $5\times$ larger ViT in terms of
FID scores while striking a critical balance of computational cost and image
quality. Code, checkpoints, and datasets are available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1&quot;&gt;Zhengyang Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pokle_A/0/1/0/all/0/1&quot;&gt;Ashwini Pokle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1&quot;&gt;J. Zico Kolter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08655">
<title>SAiD: Speech-driven Blendshape Facial Animation with Diffusion. (arXiv:2401.08655v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08655</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech-driven 3D facial animation is challenging due to the scarcity of
large-scale visual-audio datasets despite extensive research. Most prior works,
typically focused on learning regression models on a small dataset using the
method of least squares, encounter difficulties generating diverse lip
movements from speech and require substantial effort in refining the generated
outputs. To address these issues, we propose a speech-driven 3D facial
animation with a diffusion model (SAiD), a lightweight Transformer-based U-Net
with a cross-modality alignment bias between audio and visual to enhance lip
synchronization. Moreover, we introduce BlendVOCA, a benchmark dataset of pairs
of speech audio and parameters of a blendshape facial model, to address the
scarcity of public resources. Our experimental results demonstrate that the
proposed approach achieves comparable or superior performance in lip
synchronization to baselines, ensures more diverse lip movements, and
streamlines the animation editing process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_I/0/1/0/all/0/1&quot;&gt;Inkyu Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Jaewoong Cho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08686">
<title>Attention Modules Improve Modern Image-Level Anomaly Detection: A DifferNet Case Study. (arXiv:2401.08686v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08686</link>
<description rdf:parseType="Literal">&lt;p&gt;Within (semi-)automated visual inspection, learning-based approaches for
assessing visual defects, including deep neural networks, enable the processing
of otherwise small defect patterns in pixel size on high-resolution imagery.
The emergence of these often rarely occurring defect patterns explains the
general need for labeled data corpora. To not only alleviate this issue but to
furthermore advance the current state of the art in unsupervised visual
inspection, this contribution proposes a DifferNet-based solution enhanced with
attention modules utilizing SENet and CBAM as backbone - AttentDifferNet - to
improve the detection and classification capabilities on three different visual
inspection and anomaly detection datasets: MVTec AD, InsPLAD-fault, and
Semiconductor Wafer. In comparison to the current state of the art, it is shown
that AttentDifferNet achieves improved results, which are, in turn, highlighted
throughout our quantitative as well as qualitative evaluation, indicated by a
general improvement in AUC of 94.34 vs. 92.46, 96.67 vs. 94.69, and 90.20 vs.
88.74%. As our variants to AttentDifferNet show great prospects in the context
of currently investigated approaches, a baseline is formulated, emphasizing the
importance of attention for anomaly detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Luiz B. Vieira e Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simoes_F/0/1/0/all/0/1&quot;&gt;Francisco Sim&amp;#xf5;es&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kowerko_D/0/1/0/all/0/1&quot;&gt;Danny Kowerko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlosser_T/0/1/0/all/0/1&quot;&gt;Tobias Schlosser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Battisti_F/0/1/0/all/0/1&quot;&gt;Felipe Battisti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teichrieb_V/0/1/0/all/0/1&quot;&gt;Veronica Teichrieb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08687">
<title>DA-BEV: Unsupervised Domain Adaptation for Bird&apos;s Eye View Perception. (arXiv:2401.08687v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08687</link>
<description rdf:parseType="Literal">&lt;p&gt;Camera-only Bird&apos;s Eye View (BEV) has demonstrated great potential in
environment perception in a 3D space. However, most existing studies were
conducted under a supervised setup which cannot scale well while handling
various new data. Unsupervised domain adaptive BEV, which effective learning
from various unlabelled target data, is far under-explored. In this work, we
design DA-BEV, the first domain adaptive camera-only BEV framework that
addresses domain adaptive BEV challenges by exploiting the complementary nature
of image-view features and BEV features. DA-BEV introduces the idea of query
into the domain adaptation framework to derive useful information from
image-view and BEV features. It consists of two query-based designs, namely,
query-based adversarial learning (QAL) and query-based self-training (QST),
which exploits image-view features or BEV features to regularize the adaptation
of the other. Extensive experiments show that DA-BEV achieves superior domain
adaptive BEV perception performance consistently across multiple datasets and
tasks such as 3D object detection and 3D scene segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1&quot;&gt;Kai Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiaxing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weiying Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunsong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1&quot;&gt;Ling Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Shijian Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08689">
<title>NODI: Out-Of-Distribution Detection with Noise from Diffusion. (arXiv:2401.08689v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08689</link>
<description rdf:parseType="Literal">&lt;p&gt;Out-of-distribution (OOD) detection is a crucial part of deploying machine
learning models safely. It has been extensively studied with a plethora of
methods developed in the literature. This problem is tackled with an OOD score
computation, however, previous methods compute the OOD scores with limited
usage of the in-distribution dataset. For instance, the OOD scores are computed
with information from a small portion of the in-distribution data. Furthermore,
these methods encode images with a neural image encoder. The robustness of
these methods is rarely checked with respect to image encoders of different
training methods and architectures. In this work, we introduce the diffusion
process into the OOD task. The diffusion model integrates information on the
whole training set into the predicted noise vectors. What&apos;s more, we deduce a
closed-form solution for the noise vector (stable point). Then the noise vector
is converted into our OOD score, we test both the deep model predicted noise
vector and the closed-form noise vector on the OOD benchmarks \cite{openood}.
Our method outperforms previous OOD methods across all types of image encoders
(Table. \ref{main}). A $3.5\%$ performance gain is achieved with the MAE-based
image encoder. Moreover, we studied the robustness of OOD methods by applying
different types of image encoders. Some OOD methods failed to generalize well
when switching image encoders from ResNet to Vision Transformers, our method
performs exhibits good robustness with all the image encoders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingqiu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1&quot;&gt;Aojun Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongshen Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08695">
<title>Enabling Collaborative Clinical Diagnosis of Infectious Keratitis by Integrating Expert Knowledge and Interpretable Data-driven Intelligence. (arXiv:2401.08695v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.08695</link>
<description rdf:parseType="Literal">&lt;p&gt;Although data-driven artificial intelligence (AI) in medical image diagnosis
has shown impressive performance in silico, the lack of interpretability makes
it difficult to incorporate the &quot;black box&quot; into clinicians&apos; workflows. To make
the diagnostic patterns learned from data understandable by clinicians, we
develop an interpretable model, knowledge-guided diagnosis model (KGDM), that
provides a visualized reasoning process containing AI-based biomarkers and
retrieved cases that with the same diagnostic patterns. It embraces clinicians&apos;
prompts into the interpreted reasoning through human-AI interaction, leading to
potentially enhanced safety and more accurate predictions. This study
investigates the performance, interpretability, and clinical utility of KGDM in
the diagnosis of infectious keratitis (IK), which is the leading cause of
corneal blindness. The classification performance of KGDM is evaluated on a
prospective validation dataset, an external testing dataset, and an publicly
available testing dataset. The diagnostic odds ratios (DOR) of the interpreted
AI-based biomarkers are effective, ranging from 3.011 to 35.233 and exhibit
consistent diagnostic patterns with clinic experience. Moreover, a human-AI
collaborative diagnosis test is conducted and the participants with
collaboration achieved a performance exceeding that of both humans and AI. By
synergistically integrating interpretability and interaction, this study
facilitates the convergence of clinicians&apos; expertise and data-driven
intelligence. The promotion of inexperienced ophthalmologists with the aid of
AI-based biomarkers, as well as increased AI prediction by intervention from
experienced ones, demonstrate a promising diagnostic paradigm for infectious
keratitis using KGDM, which holds the potential for extension to other diseases
where experienced medical practitioners are limited and the safety of AI is
concerned.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zhengqing Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shuowen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zhouhang Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mengze Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinxu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yesheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Wenjia Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1&quot;&gt;Kun Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yu-Feng Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08699">
<title>On Image Search in Histopathology. (arXiv:2401.08699v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.08699</link>
<description rdf:parseType="Literal">&lt;p&gt;Pathology images of histopathology can be acquired from camera-mounted
microscopes or whole slide scanners. Utilizing similarity calculations to match
patients based on these images holds significant potential in research and
clinical contexts. Recent advancements in search technologies allow for nuanced
quantification of cellular structures across diverse tissue types, facilitating
comparisons and enabling inferences about diagnosis, prognosis, and predictions
for new patients when compared against a curated database of diagnosed and
treated cases. In this paper, we comprehensively review the latest developments
in image search technologies for histopathology, offering a concise overview
tailored for computational pathology researchers seeking effective, fast and
efficient image search methods in their work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tizhoosh_H/0/1/0/all/0/1&quot;&gt;H.R. Tizhoosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pantanowitz_L/0/1/0/all/0/1&quot;&gt;Liron Pantanowitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08714">
<title>Training program on sign language: social inclusion through Virtual Reality in ISENSE project. (arXiv:2401.08714v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.08714</link>
<description rdf:parseType="Literal">&lt;p&gt;Structured hand gestures that incorporate visual motions and signs are used
in sign language. Sign language is a valuable means of daily communication for
individuals who are deaf or have speech impairments, but it is still rare among
hearing people, and fewer are capable of understand it. Within the academic
context, parents and teachers play a crucial role in supporting deaf students
from childhood by facilitating their learning of sign language. In the last
years, among all the teaching tools useful for learning sign language, the use
of Virtual Reality (VR) has increased, as it has been demonstrated to improve
retention, memory and attention during the learning process. The ISENSE project
has been created to assist students with deafness during their academic life by
proposing different technological tools for teaching sign language to the
hearing community in the academic context. As part of the ISENSE project, this
work aims to develop an application for Spanish and Italian sign language
recognition that exploits the VR environment to quickly and easily create a
comprehensive database of signs and an Artificial Intelligence (AI)-based
software to accurately classify and recognize static and dynamic signs: from
letters to sentences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisio_A/0/1/0/all/0/1&quot;&gt;Alessia Bisio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeguas_Bolivar_E/0/1/0/all/0/1&quot;&gt;Enrique Yeguas-Bol&amp;#xed;var&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aparicio_Martinez_P/0/1/0/all/0/1&quot;&gt;Pilar Aparicio-Mart&amp;#xed;nez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redel_Macias_M/0/1/0/all/0/1&quot;&gt;Mar&amp;#xed;a Dolores Redel-Mac&amp;#xed;as&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinzi_S/0/1/0/all/0/1&quot;&gt;Sara Pinzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_S/0/1/0/all/0/1&quot;&gt;Stefano Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taborri_J/0/1/0/all/0/1&quot;&gt;Juri Taborri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08720">
<title>Unsupervised Pre-Training for 3D Leaf Instance Segmentation. (arXiv:2401.08720v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08720</link>
<description rdf:parseType="Literal">&lt;p&gt;Crops for food, feed, fiber, and fuel are key natural resources for our
society. Monitoring plants and measuring their traits is an important task in
agriculture often referred to as plant phenotyping. Traditionally, this task is
done manually, which is time- and labor-intensive. Robots can automate
phenotyping providing reproducible and high-frequency measurements. Today&apos;s
perception systems use deep learning to interpret these measurements, but
require a substantial amount of annotated data to work well. Obtaining such
labels is challenging as it often requires background knowledge on the side of
the labelers. This paper addresses the problem of reducing the labeling effort
required to perform leaf instance segmentation on 3D point clouds, which is a
first step toward phenotyping in 3D. Separating all leaves allows us to count
them and compute relevant traits as their areas, lengths, and widths. We
propose a novel self-supervised task-specific pre-training approach to
initialize the backbone of a network for leaf instance segmentation. We also
introduce a novel automatic postprocessing that considers the difficulty of
correctly segmenting the points close to the stem, where all the leaves petiole
overlap. The experiments presented in this paper suggest that our approach
boosts the performance over all the investigated scenarios. We also evaluate
the embeddings to assess the quality of the fully unsupervised approach and see
a higher performance of our domain-specific postprocessing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roggiolani_G/0/1/0/all/0/1&quot;&gt;Gianmarco Roggiolani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magistri_F/0/1/0/all/0/1&quot;&gt;Federico Magistri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guadagnino_T/0/1/0/all/0/1&quot;&gt;Tiziano Guadagnino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behley_J/0/1/0/all/0/1&quot;&gt;Jens Behley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stachniss_C/0/1/0/all/0/1&quot;&gt;Cyrill Stachniss&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08723">
<title>HierSFL: Local Differential Privacy-aided Split Federated Learning in Mobile Edge Computing. (arXiv:2401.08723v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.08723</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning is a promising approach for learning from user data while
preserving data privacy. However, the high requirements of the model training
process make it difficult for clients with limited memory or bandwidth to
participate. To tackle this problem, Split Federated Learning is utilized,
where clients upload their intermediate model training outcomes to a cloud
server for collaborative server-client model training. This methodology
facilitates resource-constrained clients&apos; participation in model training but
also increases the training time and communication overhead. To overcome these
limitations, we propose a novel algorithm, called Hierarchical Split Federated
Learning (HierSFL), that amalgamates models at the edge and cloud phases,
presenting qualitative directives for determining the best aggregation
timeframes to reduce computation and communication expenses. By implementing
local differential privacy at the client and edge server levels, we enhance
privacy during local model parameter updates. Our experiments using CIFAR-10
and MNIST datasets show that HierSFL outperforms standard FL approaches with
better training accuracy, training time, and communication-computing
trade-offs. HierSFL offers a promising solution to mobile edge computing&apos;s
challenges, ultimately leading to faster content delivery and improved mobile
service quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quan_M/0/1/0/all/0/1&quot;&gt;Minh K. Quan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Dinh C. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1&quot;&gt;Van-Dinh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wijayasundara_M/0/1/0/all/0/1&quot;&gt;Mayuri Wijayasundara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Setunge_S/0/1/0/all/0/1&quot;&gt;Sujeeva Setunge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathirana_P/0/1/0/all/0/1&quot;&gt;Pubudu N. Pathirana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08725">
<title>Revealing Vulnerabilities in Stable Diffusion via Targeted Attacks. (arXiv:2401.08725v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08725</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent developments in text-to-image models, particularly Stable Diffusion,
have marked significant achievements in various applications. With these
advancements, there are growing safety concerns about the vulnerability of the
model that malicious entities exploit to generate targeted harmful images.
However, the existing methods in the vulnerability of the model mainly evaluate
the alignment between the prompt and generated images, but fall short in
revealing the vulnerability associated with targeted image generation. In this
study, we formulate the problem of targeted adversarial attack on Stable
Diffusion and propose a framework to generate adversarial prompts.
Specifically, we design a gradient-based embedding optimization method to craft
reliable adversarial prompts that guide stable diffusion to generate specific
images. Furthermore, after obtaining successful adversarial prompts, we reveal
the mechanisms that cause the vulnerability of the model. Extensive experiments
on two targeted attack tasks demonstrate the effectiveness of our method in
targeted attacks. The code can be obtained in
https://github.com/datar001/Revealing-Vulnerabilities-in-Stable-Diffusion-via-Targeted-Attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lanjun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Anan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08732">
<title>Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information. (arXiv:2401.08732v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.08732</link>
<description rdf:parseType="Literal">&lt;p&gt;It is believed that in knowledge distillation (KD), the role of the teacher
is to provide an estimate for the unknown Bayes conditional probability
distribution (BCPD) to be used in the student training process. Conventionally,
this estimate is obtained by training the teacher using maximum log-likelihood
(MLL) method. To improve this estimate for KD, in this paper we introduce the
concept of conditional mutual information (CMI) into the estimation of BCPD and
propose a novel estimator called the maximum CMI (MCMI) method. Specifically,
in MCMI estimation, both the log-likelihood and CMI of the teacher are
simultaneously maximized when the teacher is trained. Through Eigen-CAM, it is
further shown that maximizing the teacher&apos;s CMI value allows the teacher to
capture more contextual information in an image cluster. Via conducting a
thorough set of experiments, we show that by employing a teacher trained via
MCMI estimation rather than one trained via MLL estimation in various
state-of-the-art KD frameworks, the student&apos;s classification accuracy
consistently increases, with the gain of up to 3.32\%. This suggests that the
teacher&apos;s BCPD estimate provided by MCMI method is more accurate than that
provided by MLL method. In addition, we show that such improvements in the
student&apos;s accuracy are more drastic in zero-shot and few-shot settings.
Notably, the student&apos;s accuracy increases with the gain of up to 5.72\% when
5\% of the training samples are available to the student (few-shot), and
increases from 0\% to as high as 84\% for an omitted class (zero-shot). The
code is available at \url{https://github.com/iclr2024mcmi/ICLRMCMI}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1&quot;&gt;Linfeng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamidi_S/0/1/0/all/0/1&quot;&gt;Shayan Mohajer Hamidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1&quot;&gt;Renhao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1&quot;&gt;En-Hui Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08734">
<title>Bag of Tricks to Boost Adversarial Transferability. (arXiv:2401.08734v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08734</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are widely known to be vulnerable to adversarial
examples. However, vanilla adversarial examples generated under the white-box
setting often exhibit low transferability across different models. Since
adversarial transferability poses more severe threats to practical
applications, various approaches have been proposed for better transferability,
including gradient-based, input transformation-based, and model-related
attacks, \etc. In this work, we find that several tiny changes in the existing
adversarial attacks can significantly affect the attack performance, \eg, the
number of iterations and step size. Based on careful studies of existing
adversarial attacks, we propose a bag of tricks to enhance adversarial
transferability, including momentum initialization, scheduled step size, dual
example, spectral-based input transformation, and several ensemble strategies.
Extensive experiments on the ImageNet dataset validate the high effectiveness
of our proposed tricks and show that combining them can further boost
adversarial transferability. Our work provides practical insights and
techniques to enhance adversarial transferability, and offers guidance to
improve the attack performance on the real-world application through simple
adjustments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zeliang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1&quot;&gt;Rongyi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaosen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chenliang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08739">
<title>EgoGen: An Egocentric Synthetic Data Generator. (arXiv:2401.08739v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08739</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the world in first-person view is fundamental in Augmented
Reality (AR). This immersive perspective brings dramatic visual changes and
unique challenges compared to third-person views. Synthetic data has empowered
third-person-view vision models, but its application to embodied egocentric
perception tasks remains largely unexplored. A critical challenge lies in
simulating natural human movements and behaviors that effectively steer the
embodied cameras to capture a faithful egocentric representation of the 3D
world. To address this challenge, we introduce EgoGen, a new synthetic data
generator that can produce accurate and rich ground-truth training data for
egocentric perception tasks. At the heart of EgoGen is a novel human motion
synthesis model that directly leverages egocentric visual inputs of a virtual
human to sense the 3D environment. Combined with collision-avoiding motion
primitives and a two-stage reinforcement learning approach, our motion
synthesis model offers a closed-loop solution where the embodied perception and
movement of the virtual human are seamlessly coupled. Compared to previous
works, our model eliminates the need for a pre-defined global path, and is
directly applicable to dynamic environments. Combined with our easy-to-use and
scalable data generation pipeline, we demonstrate EgoGen&apos;s efficacy in three
tasks: mapping and localization for head-mounted cameras, egocentric camera
tracking, and human mesh recovery from egocentric views. EgoGen will be fully
open-sourced, offering a practical solution for creating realistic egocentric
training data and aiming to serve as a useful tool for egocentric computer
vision research. Refer to our project page: https://ego-gen.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Kaifeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Siwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1&quot;&gt;Xiaozhong Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dusmanu_M/0/1/0/all/0/1&quot;&gt;Mihai Dusmanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1&quot;&gt;Marc Pollefeys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Siyu Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08740">
<title>SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers. (arXiv:2401.08740v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08740</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Scalable Interpolant Transformers (SiT), a family of generative
models built on the backbone of Diffusion Transformers (DiT). The interpolant
framework, which allows for connecting two distributions in a more flexible way
than standard diffusion models, makes possible a modular study of various
design choices impacting generative models built on dynamical transport: using
discrete vs. continuous time learning, deciding the objective for the model to
learn, choosing the interpolant connecting the distributions, and deploying a
deterministic or stochastic sampler. By carefully introducing the above
ingredients, SiT surpasses DiT uniformly across model sizes on the conditional
ImageNet 256x256 benchmark using the exact same backbone, number of parameters,
and GFLOPs. By exploring various diffusion coefficients, which can be tuned
separately from learning, SiT achieves an FID-50K score of 2.06.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1&quot;&gt;Nanye Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_M/0/1/0/all/0/1&quot;&gt;Mark Goldstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albergo_M/0/1/0/all/0/1&quot;&gt;Michael S. Albergo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boffi_N/0/1/0/all/0/1&quot;&gt;Nicholas M. Boffi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vanden_Eijnden_E/0/1/0/all/0/1&quot;&gt;Eric Vanden-Eijnden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Saining Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08741">
<title>Fixed Point Diffusion Models. (arXiv:2401.08741v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08741</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the Fixed Point Diffusion Model (FPDM), a novel approach to
image generation that integrates the concept of fixed point solving into the
framework of diffusion-based generative modeling. Our approach embeds an
implicit fixed point solving layer into the denoising network of a diffusion
model, transforming the diffusion process into a sequence of closely-related
fixed point problems. Combined with a new stochastic training method, this
approach significantly reduces model size, reduces memory usage, and
accelerates training. Moreover, it enables the development of two new
techniques to improve sampling efficiency: reallocating computation across
timesteps and reusing fixed point solutions between timesteps. We conduct
extensive experiments with state-of-the-art models on ImageNet, FFHQ,
CelebA-HQ, and LSUN-Church, demonstrating substantial improvements in
performance and efficiency. Compared to the state-of-the-art DiT model, FPDM
contains 87% fewer parameters, consumes 60% less memory during training, and
improves image generation quality in situations where sampling computation or
time is limited. Our code and pretrained models are available at
https://lukemelas.github.io/fixed-point-diffusion-models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xingjian Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melas_Kyriazi_L/0/1/0/all/0/1&quot;&gt;Luke Melas-Kyriazi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08742">
<title>Fast Dynamic 3D Object Generation from a Single-view Video. (arXiv:2401.08742v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08742</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating dynamic three-dimensional (3D) object from a single-view video is
challenging due to the lack of 4D labeled data. Existing methods extend
text-to-3D pipelines by transferring off-the-shelf image generation models such
as score distillation sampling, but they are slow and expensive to scale (e.g.,
150 minutes per object) due to the need for back-propagating the
information-limited supervision signals through a large pretrained model. To
address this limitation, we propose an efficient video-to-4D object generation
framework called Efficient4D. It generates high-quality spacetime-consistent
images under different camera views, and then uses them as labeled data to
directly train a novel 4D Gaussian splatting model with explicit point cloud
geometry, enabling real-time rendering under continuous camera trajectories.
Extensive experiments on synthetic and real videos show that Efficient4D offers
a remarkable 10-fold increase in speed when compared to prior art alternatives
while preserving the same level of innovative view synthesis quality. For
example, Efficient4D takes only 14 minutes to model a dynamic object.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zijie Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08743">
<title>MMToM-QA: Multimodal Theory of Mind Question Answering. (arXiv:2401.08743v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.08743</link>
<description rdf:parseType="Literal">&lt;p&gt;Theory of Mind (ToM), the ability to understand people&apos;s minds, is an
essential ingredient for developing machines with human-level social
intelligence. Recent machine learning models, particularly large language
models, seem to show some aspects of ToM understanding. However, existing ToM
benchmarks use unimodal datasets - either video or text. Human ToM, on the
other hand, is more than video or text understanding. People can flexibly
reason about another person&apos;s mind based on conceptual representations (e.g.,
goals, beliefs, plans) extracted from any available data, which can include
visual cues, linguistic narratives, or both. To address this, we introduce a
multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA
comprehensively evaluates machine ToM both on multimodal data and on different
kinds of unimodal data about a person&apos;s activity in a household environment. To
engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian
Inverse Planning Accelerated by Language Models). BIP-ALM extracts unified
representations from multimodal data and utilizes language models for scalable
Bayesian inverse planning. We conducted a systematic comparison of human
performance, BIP-ALM, and state-of-the-art models, including GPT-4. The
experiments demonstrate that large language models and large multimodal models
still lack robust ToM capacity. BIP-ALM, on the other hand, shows promising
results, by leveraging the power of both model-based mental inference and
language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Chuanyang Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yutong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jing Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1&quot;&gt;Jiannan Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuo_Y/0/1/0/all/0/1&quot;&gt;Yen-Ling Kuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhiting Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1&quot;&gt;Tomer Ullman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1&quot;&gt;Antonio Torralba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1&quot;&gt;Tianmin Shu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08787">
<title>Segment Anything Model Can Not Segment Anything: Assessing AI Foundation Model&apos;s Generalizability in Permafrost Mapping. (arXiv:2401.08787v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08787</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper assesses trending AI foundation models, especially emerging
computer vision foundation models and their performance in natural landscape
feature segmentation. While the term foundation model has quickly garnered
interest from the geospatial domain, its definition remains vague. Hence, this
paper will first introduce AI foundation models and their defining
characteristics. Built upon the tremendous success achieved by Large Language
Models (LLMs) as the foundation models for language tasks, this paper discusses
the challenges of building foundation models for geospatial artificial
intelligence (GeoAI) vision tasks. To evaluate the performance of large AI
vision models, especially Meta&apos;s Segment Anything Model (SAM), we implemented
different instance segmentation pipelines that minimize the changes to SAM to
leverage its power as a foundation model. A series of prompt strategies was
developed to test SAM&apos;s performance regarding its theoretical upper bound of
predictive accuracy, zero-shot performance, and domain adaptability through
fine-tuning. The analysis used two permafrost feature datasets, ice-wedge
polygons and retrogressive thaw slumps because (1) these landform features are
more challenging to segment than manmade features due to their complicated
formation mechanisms, diverse forms, and vague boundaries; (2) their presence
and changes are important indicators for Arctic warming and climate change. The
results show that although promising, SAM still has room for improvement to
support AI-augmented terrain mapping. The spatial and domain generalizability
of this finding is further validated using a more general dataset EuroCrop for
agricultural field mapping. Finally, we discuss future research directions that
strengthen SAM&apos;s applicability in challenging geospatial domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenwen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1&quot;&gt;Chia-Yu Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sizhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yezhou Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyunho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liljedahl_A/0/1/0/all/0/1&quot;&gt;Anna Liljedahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witharana_C/0/1/0/all/0/1&quot;&gt;Chandi Witharana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yili Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rogers_B/0/1/0/all/0/1&quot;&gt;Brendan M. Rogers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arundel_S/0/1/0/all/0/1&quot;&gt;Samantha T. Arundel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1&quot;&gt;Matthew B. Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McHenry_K/0/1/0/all/0/1&quot;&gt;Kenton McHenry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solis_P/0/1/0/all/0/1&quot;&gt;Patricia Solis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08809">
<title>Learning Implicit Representation for Reconstructing Articulated Objects. (arXiv:2401.08809v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08809</link>
<description rdf:parseType="Literal">&lt;p&gt;3D Reconstruction of moving articulated objects without additional
information about object structure is a challenging problem. Current methods
overcome such challenges by employing category-specific skeletal models.
Consequently, they do not generalize well to articulated objects in the wild.
We treat an articulated object as an unknown, semi-rigid skeletal structure
surrounded by nonrigid material (e.g., skin). Our method simultaneously
estimates the visible (explicit) representation (3D shapes, colors, camera
parameters) and the implicit skeletal representation, from motion cues in the
object video without 3D supervision. Our implicit representation consists of
four parts. (1) Skeleton, which specifies how semi-rigid parts are connected.
(2) \textcolor{black}{Skinning Weights}, which associates each surface vertex
with semi-rigid parts with probability. (3) Rigidity Coefficients, specifying
the articulation of the local surface. (4) Time-Varying Transformations, which
specify the skeletal motion and surface deformation parameters. We introduce an
algorithm that uses physical constraints as regularization terms and
iteratively estimates both implicit and explicit representations. Our method is
category-agnostic, thus eliminating the need for category-specific skeletons,
we show that our method outperforms state-of-the-art across standard video
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawlekar_S/0/1/0/all/0/1&quot;&gt;Samyak Rawlekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahuja_N/0/1/0/all/0/1&quot;&gt;Narendra Ahuja&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08815">
<title>Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive. (arXiv:2401.08815v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08815</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the recent advances in large-scale diffusion models, little progress
has been made on the layout-to-image (L2I) synthesis task. Current L2I models
either suffer from poor editability via text or weak alignment between the
generated image and the input layout. This limits their usability in practice.
To mitigate this, we propose to integrate adversarial supervision into the
conventional training pipeline of L2I diffusion models (ALDM). Specifically, we
employ a segmentation-based discriminator which provides explicit feedback to
the diffusion generator on the pixel-level alignment between the denoised image
and the input layout. To encourage consistent adherence to the input layout
over the sampling steps, we further introduce the multistep unrolling strategy.
Instead of looking at a single timestep, we unroll a few steps recursively to
imitate the inference process, and ask the discriminator to assess the
alignment of denoised images with the layout over a certain time window. Our
experiments show that ALDM enables layout faithfulness of the generated images,
while allowing broad editability via text prompts. Moreover, we showcase its
usefulness for practical applications: by synthesizing target distribution
samples via text control, we improve domain generalization of semantic
segmentation models by a large margin (~12 mIoU points).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yumeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1&quot;&gt;Margret Keuper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khoreva_A/0/1/0/all/0/1&quot;&gt;Anna Khoreva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08825">
<title>AiGen-FoodReview: A Multimodal Dataset of Machine-Generated Restaurant Reviews and Images on Social Media. (arXiv:2401.08825v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.08825</link>
<description rdf:parseType="Literal">&lt;p&gt;Online reviews in the form of user-generated content (UGC) significantly
impact consumer decision-making. However, the pervasive issue of not only human
fake content but also machine-generated content challenges UGC&apos;s reliability.
Recent advances in Large Language Models (LLMs) may pave the way to fabricate
indistinguishable fake generated content at a much lower cost. Leveraging
OpenAI&apos;s GPT-4-Turbo and DALL-E-2 models, we craft AiGen-FoodReview, a
multi-modal dataset of 20,144 restaurant review-image pairs divided into
authentic and machine-generated. We explore unimodal and multimodal detection
models, achieving 99.80% multimodal accuracy with FLAVA. We use attributes from
readability and photographic theories to score reviews and images,
respectively, demonstrating their utility as hand-crafted features in scalable
and interpretable detection models, with comparable performance. The paper
contributes by open-sourcing the dataset and releasing fake review detectors,
recommending its use in unimodal and multimodal fake review detection tasks,
and evaluating linguistic and visual features in synthetic versus authentic
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gambetti_A/0/1/0/all/0/1&quot;&gt;Alessandro Gambetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1&quot;&gt;Qiwei Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08837">
<title>Image Fusion in Remote Sensing: An Overview and Meta Analysis. (arXiv:2401.08837v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08837</link>
<description rdf:parseType="Literal">&lt;p&gt;Image fusion in Remote Sensing (RS) has been a consistent demand due to its
ability to turn raw images of different resolutions, sources, and modalities
into accurate, complete, and spatio-temporally coherent images. It greatly
facilitates downstream applications such as pan-sharpening, change detection,
land-cover classification, etc. Yet, image fusion solutions are highly
disparate to various remote sensing problems and thus are often narrowly
defined in existing reviews as topical applications, such as pan-sharpening,
and spatial-temporal image fusion. Considering that image fusion can be
theoretically applied to any gridded data through pixel-level operations, in
this paper, we expanded its scope by comprehensively surveying relevant works
with a simple taxonomy: 1) many-to-one image fusion; 2) many-to-many image
fusion. This simple taxonomy defines image fusion as a mapping problem that
turns either a single or a set of images into another single or set of images,
depending on the desired coherence, e.g., spectral, spatial/resolution
coherence, etc. We show that this simple taxonomy, despite the significant
modality difference it covers, can be presented by a conceptually easy
framework. In addition, we provide a meta-analysis to review the major papers
studying the various types of image fusion and their applications over the
years (from the 1980s to date), covering 5,926 peer-reviewed papers. Finally,
we discuss the main benefits and emerging challenges to provide open research
directions and potential future works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albanwan_H/0/1/0/all/0/1&quot;&gt;Hessah Albanwan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1&quot;&gt;Rongjun Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yang Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08840">
<title>Efficient Neural Representation of Volumetric Data using Coordinate-Based Networks. (arXiv:2401.08840v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08840</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose an efficient approach for the compression and
representation of volumetric data utilizing coordinate-based networks and
multi-resolution hash encoding. Efficient compression of volumetric data is
crucial for various applications, such as medical imaging and scientific
simulations. Our approach enables effective compression by learning a mapping
between spatial coordinates and intensity values. We compare different encoding
schemes and demonstrate the superiority of multi-resolution hash encoding in
terms of compression quality and training efficiency. Furthermore, we leverage
optimization-based meta-learning, specifically using the Reptile algorithm, to
learn weight initialization for neural representations tailored to volumetric
data, enabling faster convergence during optimization. Additionally, we compare
our approach with state-of-the-art methods to showcase improved image quality
and compression ratios. These findings highlight the potential of
coordinate-based networks and multi-resolution hash encoding for an efficient
and accurate representation of volumetric data, paving the way for advancements
in large-scale data visualization and other applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Devkota_S/0/1/0/all/0/1&quot;&gt;Sudarshan Devkota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pattanaik_S/0/1/0/all/0/1&quot;&gt;Sumanta Pattanaik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08847">
<title>RIDGE: Reproducibility, Integrity, Dependability, Generalizability, and Efficiency Assessment of Medical Image Segmentation Models. (arXiv:2401.08847v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.08847</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning techniques, despite their potential, often suffer from a lack
of reproducibility and generalizability, impeding their clinical adoption.
Image segmentation is one of the critical tasks in medical image analysis, in
which one or several regions/volumes of interest should be annotated. This
paper introduces the RIDGE checklist, a framework for assessing the
Reproducibility, Integrity, Dependability, Generalizability, and Efficiency of
deep learning-based medical image segmentation models. The checklist serves as
a guide for researchers to enhance the quality and transparency of their work,
ensuring that segmentation models are not only scientifically sound but also
clinically relevant.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maleki_F/0/1/0/all/0/1&quot;&gt;Farhad Maleki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moy_L/0/1/0/all/0/1&quot;&gt;Linda Moy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Forghani_R/0/1/0/all/0/1&quot;&gt;Reza Forghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ghosh_T/0/1/0/all/0/1&quot;&gt;Tapotosh Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ovens_K/0/1/0/all/0/1&quot;&gt;Katie Ovens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Langer_S/0/1/0/all/0/1&quot;&gt;Steve Langer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rouzrokh_P/0/1/0/all/0/1&quot;&gt;Pouria Rouzrokh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khosravi_B/0/1/0/all/0/1&quot;&gt;Bardia Khosravi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ganjizadeh_A/0/1/0/all/0/1&quot;&gt;Ali Ganjizadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Warren_D/0/1/0/all/0/1&quot;&gt;Daniel Warren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Daneshjou_R/0/1/0/all/0/1&quot;&gt;Roxana Daneshjou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moassefi_M/0/1/0/all/0/1&quot;&gt;Mana Moassefi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Avval_A/0/1/0/all/0/1&quot;&gt;Atlas Haddadi Avval&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sotardi_S/0/1/0/all/0/1&quot;&gt;Susan Sotardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tenenholtz_N/0/1/0/all/0/1&quot;&gt;Neil Tenenholtz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kitamura_F/0/1/0/all/0/1&quot;&gt;Felipe Kitamura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kline_T/0/1/0/all/0/1&quot;&gt;Timothy Kline&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08860">
<title>Cross-Level Multi-Instance Distillation for Self-Supervised Fine-Grained Visual Categorization. (arXiv:2401.08860v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08860</link>
<description rdf:parseType="Literal">&lt;p&gt;High-quality annotation of fine-grained visual categories demands great
expert knowledge, which is taxing and time consuming. Alternatively, learning
fine-grained visual representation from enormous unlabeled images (e.g.,
species, brands) by self-supervised learning becomes a feasible solution.
However, recent researches find that existing self-supervised learning methods
are less qualified to represent fine-grained categories. The bottleneck lies in
that the pre-text representation is built from every patch-wise embedding,
while fine-grained categories are only determined by several key patches of an
image. In this paper, we propose a Cross-level Multi-instance Distillation
(CMD) framework to tackle the challenge. Our key idea is to consider the
importance of each image patch in determining the fine-grained pre-text
representation by multiple instance learning. To comprehensively learn the
relation between informative patches and fine-grained semantics, the
multi-instance knowledge distillation is implemented on both the region/image
crop pairs from the teacher and student net, and the region-image crops inside
the teacher / student net, which we term as intra-level multi-instance
distillation and inter-level multi-instance distillation. Extensive experiments
on CUB-200-2011, Stanford Cars and FGVC Aircraft show that the proposed method
outperforms the contemporary method by upto 10.14% and existing
state-of-the-art self-supervised learning approaches by upto 19.78% on both
top-1 accuracy and Rank-1 retrieval metric.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_Q/0/1/0/all/0/1&quot;&gt;Qi Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1&quot;&gt;Wei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Jingjun Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1&quot;&gt;Haolan Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1&quot;&gt;Gui-Song Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08865">
<title>The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images. (arXiv:2401.08865v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08865</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates discrepancies in how neural networks learn from
different imaging domains, which are commonly overlooked when adopting computer
vision techniques from the domain of natural images to other specialized
domains such as medical images. Recent works have found that the generalization
error of a trained network typically increases with the intrinsic dimension
($d_{data}$) of its training set. Yet, the steepness of this relationship
varies significantly between medical (radiological) and natural imaging
domains, with no existing theoretical explanation. We address this gap in
knowledge by establishing and empirically validating a generalization scaling
law with respect to $d_{data}$, and propose that the substantial scaling
discrepancy between the two considered domains may be at least partially
attributed to the higher intrinsic &quot;label sharpness&quot; ($K_F$) of medical imaging
datasets, a metric which we propose. Next, we demonstrate an additional benefit
of measuring the label sharpness of a training set: it is negatively correlated
with the trained model&apos;s adversarial robustness, which notably leads to models
for medical images having a substantially higher vulnerability to adversarial
attack. Finally, we extend our $d_{data}$ formalism to the related metric of
learned representation intrinsic dimension ($d_{repr}$), derive a
generalization scaling law with respect to $d_{repr}$, and show that $d_{data}$
serves as an upper bound for $d_{repr}$. Our theoretical results are supported
by thorough experiments with six models and eleven natural and medical imaging
datasets over a range of training set sizes. Our findings offer insights into
the influence of intrinsic dataset properties on generalization, representation
learning, and robustness in deep neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konz_N/0/1/0/all/0/1&quot;&gt;Nicholas Konz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazurowski_M/0/1/0/all/0/1&quot;&gt;Maciej A. Mazurowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08868">
<title>B-Cos Aligned Transformers Learn Human-Interpretable Features. (arXiv:2401.08868v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08868</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) and Swin Transformers (Swin) are currently
state-of-the-art in computational pathology. However, domain experts are still
reluctant to use these models due to their lack of interpretability. This is
not surprising, as critical decisions need to be transparent and
understandable. The most common approach to understanding transformers is to
visualize their attention. However, attention maps of ViTs are often
fragmented, leading to unsatisfactory explanations. Here, we introduce a novel
architecture called the B-cos Vision Transformer (BvT) that is designed to be
more interpretable. It replaces all linear transformations with the B-cos
transform to promote weight-input alignment. In a blinded study, medical
experts clearly ranked BvTs above ViTs, suggesting that our network is better
at capturing biomedically relevant structures. This is also true for the B-cos
Swin Transformer (Bwin). Compared to the Swin Transformer, it even improves the
F1-score by up to 4.7% on two public datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1&quot;&gt;Manuel Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lahiani_A/0/1/0/all/0/1&quot;&gt;Amal Lahiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cid_Y/0/1/0/all/0/1&quot;&gt;Yashin Dicente Cid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boxberg_M/0/1/0/all/0/1&quot;&gt;Melanie Boxberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lienemann_P/0/1/0/all/0/1&quot;&gt;Peter Lienemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matek_C/0/1/0/all/0/1&quot;&gt;Christian Matek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_S/0/1/0/all/0/1&quot;&gt;Sophia J. Wagner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theis_F/0/1/0/all/0/1&quot;&gt;Fabian J. Theis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klaiman_E/0/1/0/all/0/1&quot;&gt;Eldad Klaiman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1&quot;&gt;Tingying Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08876">
<title>Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image Labeling. (arXiv:2401.08876v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.08876</link>
<description rdf:parseType="Literal">&lt;p&gt;As deep neural networks are more commonly deployed in high-stakes domains,
their lack of interpretability makes uncertainty quantification challenging. We
investigate the effects of presenting conformal prediction
sets$\unicode{x2013}$a method for generating valid confidence sets in
distribution-free uncertainty quantification$\unicode{x2013}$to express
uncertainty in AI-advised decision-making. Through a large pre-registered
experiment, we compare the utility of conformal prediction sets to displays of
Top-1 and Top-k predictions for AI-advised image labeling. We find that the
utility of prediction sets for accuracy varies with the difficulty of the task:
while they result in accuracy on par with or less than Top-1 and Top-k displays
for easy images, prediction sets excel at assisting humans in labeling
out-of-distribution (OOD) images especially when the set size is small. Our
results empirically pinpoint the practical challenges of conformal prediction
sets and provide implications on how to incorporate them for real-world
decision-making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dongping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatzimparmpas_A/0/1/0/all/0/1&quot;&gt;Angelos Chatzimparmpas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamali_N/0/1/0/all/0/1&quot;&gt;Negar Kamali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hullman_J/0/1/0/all/0/1&quot;&gt;Jessica Hullman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08903">
<title>PPR: Enhancing Dodging Attacks while Maintaining Impersonation Attacks on Face Recognition Systems. (arXiv:2401.08903v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08903</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial Attacks on Face Recognition (FR) encompass two types:
impersonation attacks and evasion attacks. We observe that achieving a
successful impersonation attack on FR does not necessarily ensure a successful
dodging attack on FR in the black-box setting. Introducing a novel attack
method named Pre-training Pruning Restoration Attack (PPR), we aim to enhance
the performance of dodging attacks whilst avoiding the degradation of
impersonation attacks. Our method employs adversarial example pruning, enabling
a portion of adversarial perturbations to be set to zero, while tending to
maintain the attack performance. By utilizing adversarial example pruning, we
can prune the pre-trained adversarial examples and selectively free up certain
adversarial perturbations. Thereafter, we embed adversarial perturbations in
the pruned area, which enhances the dodging performance of the adversarial face
examples. The effectiveness of our proposed attack method is demonstrated
through our experimental results, showcasing its superior performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Fengfan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1&quot;&gt;Heifei Ling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08913">
<title>Efficient Image Super-Resolution via Symmetric Visual Attention Network. (arXiv:2401.08913v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08913</link>
<description rdf:parseType="Literal">&lt;p&gt;An important development direction in the Single-Image Super-Resolution
(SISR) algorithms is to improve the efficiency of the algorithms. Recently,
efficient Super-Resolution (SR) research focuses on reducing model complexity
and improving efficiency through improved deep small kernel convolution,
leading to a small receptive field. The large receptive field obtained by large
kernel convolution can significantly improve image quality, but the
computational cost is too high. To improve the reconstruction details of
efficient super-resolution reconstruction, we propose a Symmetric Visual
Attention Network (SVAN) by applying large receptive fields. The SVAN
decomposes a large kernel convolution into three different combinations of
convolution operations and combines them with an attention mechanism to form a
Symmetric Large Kernel Attention Block (SLKAB), which forms a symmetric
attention block with a bottleneck structure by the size of the receptive field
in the convolution combination to extract depth features effectively as the
basic component of the SVAN. Our network gets a large receptive field while
minimizing the number of parameters and improving the perceptual ability of the
model. The experimental results show that the proposed SVAN can obtain
high-quality super-resolution reconstruction results using only about 30% of
the parameters of existing SOTA methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chengxu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1&quot;&gt;Qinrui Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jing Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08920">
<title>Idempotence and Perceptual Image Compression. (arXiv:2401.08920v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.08920</link>
<description rdf:parseType="Literal">&lt;p&gt;Idempotence is the stability of image codec to re-compression. At the first
glance, it is unrelated to perceptual image compression. However, we find that
theoretically: 1) Conditional generative model-based perceptual codec satisfies
idempotence; 2) Unconditional generative model with idempotence constraint is
equivalent to conditional generative codec. Based on this newfound equivalence,
we propose a new paradigm of perceptual image codec by inverting unconditional
generative model with idempotence constraints. Our codec is theoretically
equivalent to conditional generative codec, and it does not require training
new models. Instead, it only requires a pre-trained mean-square-error codec and
unconditional generative model. Empirically, we show that our proposed approach
outperforms state-of-the-art methods such as HiFiC and ILLM, in terms of
Fr\&apos;echet Inception Distance (FID). The source code is provided in
https://github.com/tongdaxu/Idempotence-and-Perceptual-Image-Compression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tongda Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Ziran Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_D/0/1/0/all/0/1&quot;&gt;Dailan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanghao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_L/0/1/0/all/0/1&quot;&gt;Lina Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuanyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qin_H/0/1/0/all/0/1&quot;&gt;Hongwei Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingjing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya-Qin Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08923">
<title>Subwavelength Imaging using a Solid-Immersion Diffractive Optical Processor. (arXiv:2401.08923v1 [physics.optics])</title>
<link>http://arxiv.org/abs/2401.08923</link>
<description rdf:parseType="Literal">&lt;p&gt;Phase imaging is widely used in biomedical imaging, sensing, and material
characterization, among other fields. However, direct imaging of phase objects
with subwavelength resolution remains a challenge. Here, we demonstrate
subwavelength imaging of phase and amplitude objects based on all-optical
diffractive encoding and decoding. To resolve subwavelength features of an
object, the diffractive imager uses a thin, high-index solid-immersion layer to
transmit high-frequency information of the object to a spatially-optimized
diffractive encoder, which converts/encodes high-frequency information of the
input into low-frequency spatial modes for transmission through air. The
subsequent diffractive decoder layers (in air) are jointly designed with the
encoder using deep-learning-based optimization, and communicate with the
encoder layer to create magnified images of input objects at its output,
revealing subwavelength features that would otherwise be washed away due to
diffraction limit. We demonstrate that this all-optical collaboration between a
diffractive solid-immersion encoder and the following decoder layers in air can
resolve subwavelength phase and amplitude features of input objects in a highly
compact design. To experimentally demonstrate its proof-of-concept, we used
terahertz radiation and developed a fabrication method for creating monolithic
multi-layer diffractive processors. Through these monolithically fabricated
diffractive encoder-decoder pairs, we demonstrated phase-to-intensity
transformations and all-optically reconstructed subwavelength phase features of
input objects by directly transforming them into magnified intensity features
at the output. This solid-immersion-based diffractive imager, with its compact
and cost-effective design, can find wide-ranging applications in bioimaging,
endoscopy, sensing and materials characterization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jingtian Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liao_K/0/1/0/all/0/1&quot;&gt;Kun Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Dinc_N/0/1/0/all/0/1&quot;&gt;Niyazi Ulas Dinc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gigli_C/0/1/0/all/0/1&quot;&gt;Carlo Gigli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bai_B/0/1/0/all/0/1&quot;&gt;Bijie Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gan_T/0/1/0/all/0/1&quot;&gt;Tianyi Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xurong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanlong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xilin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuhang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Isil_C/0/1/0/all/0/1&quot;&gt;Cagatay Isil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Md Sadman Sakib Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jingxi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaoyong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Jarrahi_M/0/1/0/all/0/1&quot;&gt;Mona Jarrahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Psaltis_D/0/1/0/all/0/1&quot;&gt;Demetri Psaltis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ozcan_A/0/1/0/all/0/1&quot;&gt;Aydogan Ozcan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08926">
<title>Uncertainty-aware No-Reference Point Cloud Quality Assessment. (arXiv:2401.08926v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08926</link>
<description rdf:parseType="Literal">&lt;p&gt;The evolution of compression and enhancement algorithms necessitates an
accurate quality assessment for point clouds. Previous works consistently
regard point cloud quality assessment (PCQA) as a MOS regression problem and
devise a deterministic mapping, ignoring the stochasticity in generating MOS
from subjective tests. Besides, the viewpoint switching of 3D point clouds in
subjective tests reinforces the judging stochasticity of different subjects
compared with traditional images. This work presents the first probabilistic
architecture for no-reference PCQA, motivated by the labeling process of
existing datasets. The proposed method can model the quality judging
stochasticity of subjects through a tailored conditional variational
autoencoder (CVAE) and produces multiple intermediate quality ratings. These
intermediate ratings simulate the judgments from different subjects and are
then integrated into an accurate quality prediction, mimicking the generation
process of a ground truth MOS. Specifically, our method incorporates a Prior
Module, a Posterior Module, and a Quality Rating Generator, where the former
two modules are introduced to model the judging stochasticity in subjective
tests, while the latter is developed to generate diverse quality ratings.
Extensive experiments indicate that our approach outperforms previous
cutting-edge methods by a large margin and exhibits gratifying cross-dataset
robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1&quot;&gt;Songlin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zixuan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wei Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Ge Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08930">
<title>3D Human Pose Analysis via Diffusion Synthesis. (arXiv:2401.08930v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08930</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have demonstrated remarkable success in generative modeling.
In this paper, we propose PADS (Pose Analysis by Diffusion Synthesis), a novel
framework designed to address various challenges in 3D human pose analysis
through a unified pipeline. Central to PADS are two distinctive strategies: i)
learning a task-agnostic pose prior using a diffusion synthesis process to
effectively capture the kinematic constraints in human pose data, and ii)
unifying multiple pose analysis tasks like estimation, completion, denoising,
etc, as instances of inverse problems. The learned pose prior will be treated
as a regularization imposing on task-specific constraints, guiding the
optimization process through a series of conditional denoising steps. PADS
represents the first diffusion-based framework for tackling general 3D human
pose analysis within the inverse problem framework. Its performance has been
validated on different benchmarks, signaling the adaptability and robustness of
this pipeline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Haorui Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongdong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08932">
<title>Learning to detect cloud and snow in remote sensing images from noisy labels. (arXiv:2401.08932v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08932</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting clouds and snow in remote sensing images is an essential
preprocessing task for remote sensing imagery. Previous works draw inspiration
from semantic segmentation models in computer vision, with most research
focusing on improving model architectures to enhance detection performance.
However, unlike natural images, the complexity of scenes and the diversity of
cloud types in remote sensing images result in many inaccurate labels in cloud
and snow detection datasets, introducing unnecessary noises into the training
and testing processes. By constructing a new dataset and proposing a novel
training strategy with the curriculum learning paradigm, we guide the model in
reducing overfitting to noisy labels. Additionally, we design a more
appropriate model performance evaluation method, that alleviates the
performance assessment bias caused by noisy labels. By conducting experiments
on models with UNet and Segformer, we have validated the effectiveness of our
proposed method. This paper is the first to consider the impact of label noise
on the detection of clouds and snow in remote sensing images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zili Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Keyan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1&quot;&gt;Zipeng Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1&quot;&gt;Zhengxia Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhenwei Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08937">
<title>ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization. (arXiv:2401.08937v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08937</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View
Synthesis (NVS) given a set of 2D images. However, NeRF training requires
accurate camera pose for each input view, typically obtained by
Structure-from-Motion (SfM) pipelines. Recent works have attempted to relax
this constraint, but they still often rely on decent initial poses which they
can refine. Here we aim at removing the requirement for pose initialization. We
present Incremental CONfidence (ICON), an optimization procedure for training
NeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate
initial guess for poses. Further, ICON introduces ``confidence&quot;: an adaptive
measure of model quality used to dynamically reweight gradients. ICON relies on
high-confidence poses to learn NeRF, and high-confidence 3D structure (as
encoded by NeRF) to learn poses. We show that ICON, without prior pose
initialization, achieves superior performance in both CO3D and HO3D versus
methods which use SfM pose.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weiyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gleize_P/0/1/0/all/0/1&quot;&gt;Pierre Gleize&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xingyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Kevin J Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feiszli_M/0/1/0/all/0/1&quot;&gt;Matt Feiszli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08943">
<title>Fluid Dynamic DNNs for Reliable and Adaptive Distributed Inference on Edge Devices. (arXiv:2401.08943v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08943</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed inference is a popular approach for efficient DNN inference at
the edge. However, traditional Static and Dynamic DNNs are not
distribution-friendly, causing system reliability and adaptability issues. In
this paper, we introduce Fluid Dynamic DNNs (Fluid DyDNNs), tailored for
distributed inference. Distinct from Static and Dynamic DNNs, Fluid DyDNNs
utilize a novel nested incremental training algorithm to enable independent and
combined operation of its sub-networks, enhancing system reliability and
adaptability. Evaluation on embedded Arm CPUs with a DNN model and the MNIST
dataset, shows that in scenarios of single device failure, Fluid DyDNNs ensure
continued inference, whereas Static and Dynamic DNNs fail. When devices are
fully operational, Fluid DyDNNs can operate in either a High-Accuracy mode and
achieve comparable accuracy with Static DNNs, or in a High-Throughput mode and
achieve 2.5x and 2x throughput compared with Static and Dynamic DNNs,
respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xun_L/0/1/0/all/0/1&quot;&gt;Lei Xun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1&quot;&gt;Mingyu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hengrui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Amit Kumar Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1&quot;&gt;Jonathon Hare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merrett_G/0/1/0/all/0/1&quot;&gt;Geoff V. Merrett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08965">
<title>Dynamic DNNs and Runtime Management for Efficient Inference on Mobile/Embedded Devices. (arXiv:2401.08965v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08965</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural network (DNN) inference is increasingly being executed on mobile
and embedded platforms due to several key advantages in latency, privacy and
always-on availability. However, due to limited computing resources, efficient
DNN deployment on mobile and embedded platforms is challenging. Although many
hardware accelerators and static model compression methods were proposed by
previous works, at system runtime, multiple applications are typically executed
concurrently and compete for hardware resources. This raises two main
challenges: Runtime Hardware Availability and Runtime Application Variability.
Previous works have addressed these challenges through either dynamic neural
networks that contain sub-networks with different performance trade-offs or
runtime hardware resource management. In this thesis, we proposed a combined
method, a system was developed for DNN performance trade-off management,
combining the runtime trade-off opportunities in both algorithms and hardware
to meet dynamically changing application performance targets and hardware
constraints in real time. We co-designed novel Dynamic Super-Networks to
maximise runtime system-level performance and energy efficiency on
heterogeneous hardware platforms. Compared with SOTA, our experimental results
using ImageNet on the GPU of Jetson Xavier NX show our model is 2.4x faster for
similar ImageNet Top-1 accuracy, or 5.1% higher accuracy at similar latency. We
also designed a hierarchical runtime resource manager that tunes both dynamic
neural networks and DVFS at runtime. Compared with the Linux DVFS governor
schedutil, our runtime approach achieves up to a 19% energy reduction and a 9%
latency reduction in single model deployment scenario, and an 89% energy
reduction and a 23% latency reduction in a two concurrent model deployment
scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xun_L/0/1/0/all/0/1&quot;&gt;Lei Xun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1&quot;&gt;Jonathon Hare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merrett_G/0/1/0/all/0/1&quot;&gt;Geoff V. Merrett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08968">
<title>COCO is &quot;ALL&apos;&apos; You Need for Visual Instruction Fine-tuning. (arXiv:2401.08968v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08968</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal Large Language Models (MLLMs) are increasingly prominent in the
field of artificial intelligence. Visual instruction fine-tuning (IFT) is a
vital process for aligning MLLMs&apos; output with user&apos;s intentions. High-quality
and diversified instruction following data is the key to this fine-tuning
process. Recent studies propose to construct visual IFT datasets through a
multifaceted approach: transforming existing datasets with rule-based
templates, employing GPT-4 for rewriting annotations, and utilizing GPT-4V for
visual dataset pseudo-labeling. LLaVA-1.5 adopted similar approach and
construct LLaVA-mix-665k, which is one of the simplest, most widely used, yet
most effective IFT datasets today. Notably, when properly fine-tuned with this
dataset, MLLMs can achieve state-of-the-art performance on several benchmarks.
However, we noticed that models trained with this dataset often struggle to
follow user instructions properly in multi-round dialog. In addition, tradition
caption and VQA evaluation benchmarks, with their closed-form evaluation
structure, are not fully equipped to assess the capabilities of modern
open-ended generative MLLMs. This problem is not unique to the LLaVA-mix-665k
dataset, but may be a potential issue in all IFT datasets constructed from
image captioning or VQA sources, though the extent of this issue may vary. We
argue that datasets with diverse and high-quality detailed instruction
following annotations are essential and adequate for MLLMs IFT. In this work,
we establish a new IFT dataset, with images sourced from the COCO dataset along
with more diverse instructions. Our experiments show that when fine-tuned with
out proposed dataset, MLLMs achieve better performance on open-ended evaluation
benchmarks in both single-round and multi-round dialog setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiaotian Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_B/0/1/0/all/0/1&quot;&gt;Bohan Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1&quot;&gt;Quanzeng You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hongxia Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08972">
<title>Hearing Loss Detection from Facial Expressions in One-on-one Conversations. (arXiv:2401.08972v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08972</link>
<description rdf:parseType="Literal">&lt;p&gt;Individuals with impaired hearing experience difficulty in conversations,
especially in noisy environments. This difficulty often manifests as a change
in behavior and may be captured via facial expressions, such as the expression
of discomfort or fatigue. In this work, we build on this idea and introduce the
problem of detecting hearing loss from an individual&apos;s facial expressions
during a conversation. Building machine learning models that can represent
hearing-related facial expression changes is a challenge. In addition, models
need to disentangle spurious age-related correlations from hearing-driven
expressions. To this end, we propose a self-supervised pre-training strategy
tailored for the modeling of expression variations. We also use adversarial
representation learning to mitigate the age bias. We evaluate our approach on a
large-scale egocentric dataset with real-world conversational scenarios
involving subjects with hearing loss and show that our method for hearing loss
detection achieves superior performance over baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1&quot;&gt;Yufeng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ananthabhotla_I/0/1/0/all/0/1&quot;&gt;Ishwarya Ananthabhotla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ithapu_V/0/1/0/all/0/1&quot;&gt;Vamsi Krishna Ithapu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1&quot;&gt;Stavros Petridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yu-Hsiang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_C/0/1/0/all/0/1&quot;&gt;Christi Miller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08973">
<title>OCTO+: A Suite for Automatic Open-Vocabulary Object Placement in Mixed Reality. (arXiv:2401.08973v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08973</link>
<description rdf:parseType="Literal">&lt;p&gt;One key challenge in Augmented Reality is the placement of virtual content in
natural locations. Most existing automated techniques can only work with a
closed-vocabulary, fixed set of objects. In this paper, we introduce and
evaluate several methods for automatic object placement using recent advances
in open-vocabulary vision-language models. Through a multifaceted evaluation,
we identify a new state-of-the-art method, OCTO+. We also introduce a benchmark
for automatically evaluating the placement of virtual objects in augmented
reality, alleviating the need for costly user studies. Through this, in
addition to human evaluations, we find that OCTO+ places objects in a valid
region over 70% of the time, outperforming other methods on a range of metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Aditya Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoffe_L/0/1/0/all/0/1&quot;&gt;Luke Yoffe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hollerer_T/0/1/0/all/0/1&quot;&gt;Tobias H&amp;#xf6;llerer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08998">
<title>Attack and Reset for Unlearning: Exploiting Adversarial Noise toward Machine Unlearning through Parameter Re-initialization. (arXiv:2401.08998v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.08998</link>
<description rdf:parseType="Literal">&lt;p&gt;With growing concerns surrounding privacy and regulatory compliance, the
concept of machine unlearning has gained prominence, aiming to selectively
forget or erase specific learned information from a trained model. In response
to this critical need, we introduce a novel approach called Attack-and-Reset
for Unlearning (ARU). This algorithm leverages meticulously crafted adversarial
noise to generate a parameter mask, effectively resetting certain parameters
and rendering them unlearnable. ARU outperforms current state-of-the-art
results on two facial machine-unlearning benchmark datasets, MUFAC and MUCAC.
In particular, we present the steps involved in attacking and masking that
strategically filter and re-initialize network parameters biased towards the
forget set. Our work represents a significant advancement in rendering data
unexploitable to deep learning models through parameter re-initialization,
achieved by harnessing adversarial noise to craft a mask.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1&quot;&gt;Yoonhwa Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_I/0/1/0/all/0/1&quot;&gt;Ikhyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_S/0/1/0/all/0/1&quot;&gt;Shun-Hsiang Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hockenmaier_J/0/1/0/all/0/1&quot;&gt;Julia Hockenmaier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09006">
<title>Generalized Face Liveness Detection via De-spoofing Face Generator. (arXiv:2401.09006v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09006</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous Face Anti-spoofing (FAS) works face the challenge of generalizing in
unseen domains. One of the major problems is that most existing FAS datasets
are relatively small and lack data diversity. However, we find that there are
numerous real faces that can be easily achieved under various conditions, which
are neglected by previous FAS works. In this paper, we conduct an Anomalous cue
Guided FAS (AG-FAS) method, which leverages real faces for improving model
generalization via a De-spoofing Face Generator (DFG). Specifically, the DFG
trained only on the real faces gains the knowledge of what a real face should
be like and can generate a &quot;real&quot; version of the face corresponding to any
given input face. The difference between the generated &quot;real&quot; face and the
input face can provide an anomalous cue for the downstream FAS task. We then
propose an Anomalous cue Guided FAS feature extraction Network (AG-Net) to
further improve the FAS feature generalization via a cross-attention
transformer. Extensive experiments on a total of nine public datasets show our
method achieves state-of-the-art results under cross-domain evaluations with
unseen scenarios and unknown presentation attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1&quot;&gt;Xingming Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1&quot;&gt;Shiguang Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09008">
<title>Hybrid of DiffStride and Spectral Pooling in Convolutional Neural Networks. (arXiv:2401.09008v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09008</link>
<description rdf:parseType="Literal">&lt;p&gt;Stride determines the distance between adjacent filter positions as the
filter moves across the input. A fixed stride causes important information
contained in the image can not be captured, so that important information is
not classified. Therefore, in previous research, the DiffStride Method was
applied, namely the Strided Convolution Method with which it can learn its own
stride value. Severe Quantization and a constraining lower bound on preserved
information are arises with Max Pooling Downsampling Method. Spectral Pooling
reduce the constraint lower bound on preserved information by cutting off the
representation in the frequency domain. In this research a CNN Model is
proposed with the Downsampling Learnable Stride Technique performed by
Backpropagation combined with the Spectral Pooling Technique. Diffstride and
Spectral Pooling techniques are expected to maintain most of the information
contained in the image. In this study, we compare the Hybrid Method, which is a
combined implementation of Spectral Pooling and DiffStride against the Baseline
Method, which is the DiffStride implementation on ResNet 18. The accuracy
result of the DiffStride combination with Spectral Pooling improves over
DiffStride which is baseline method by 0.0094. This shows that the Hybrid
Method can maintain most of the information by cutting of the representation in
the frequency domain and determine the stride of the learning result through
Backpropagation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rafif_S/0/1/0/all/0/1&quot;&gt;Sulthan Rafif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1&quot;&gt;Mochamad Arfan Ravy Wahyu Pratama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azhar_M/0/1/0/all/0/1&quot;&gt;Mohammad Faris Azhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibad_A/0/1/0/all/0/1&quot;&gt;Ahmad Mustafidul Ibad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muflikhah_L/0/1/0/all/0/1&quot;&gt;Lailil Muflikhah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1&quot;&gt;Novanto Yudistira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09019">
<title>Change Detection Between Optical Remote Sensing Imagery and Map Data via Segment Anything Model (SAM). (arXiv:2401.09019v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.09019</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised multimodal change detection is pivotal for time-sensitive tasks
and comprehensive multi-temporal Earth monitoring. In this study, we explore
unsupervised multimodal change detection between two key remote sensing data
sources: optical high-resolution imagery and OpenStreetMap (OSM) data.
Specifically, we propose to utilize the vision foundation model Segmentation
Anything Model (SAM), for addressing our task. Leveraging SAM&apos;s exceptional
zero-shot transfer capability, high-quality segmentation maps of optical images
can be obtained. Thus, we can directly compare these two heterogeneous data
forms in the so-called segmentation domain. We then introduce two strategies
for guiding SAM&apos;s segmentation process: the &apos;no-prompt&apos; and &apos;box/mask prompt&apos;
methods. The two strategies are designed to detect land-cover changes in
general scenarios and to identify new land-cover objects within existing
backgrounds, respectively. Experimental results on three datasets indicate that
the proposed approach can achieve more competitive results compared to
representative unsupervised multimodal change detection methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hongruixuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jian Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yokoya_N/0/1/0/all/0/1&quot;&gt;Naoto Yokoya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09029">
<title>Cross-modality Guidance-aided Multi-modal Learning with Dual Attention for MRI Brain Tumor Grading. (arXiv:2401.09029v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09029</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain tumor represents one of the most fatal cancers around the world, and is
very common in children and the elderly. Accurate identification of the type
and grade of tumor in the early stages plays an important role in choosing a
precise treatment plan. The Magnetic Resonance Imaging (MRI) protocols of
different sequences provide clinicians with important contradictory information
to identify tumor regions. However, manual assessment is time-consuming and
error-prone due to big amount of data and the diversity of brain tumor types.
Hence, there is an unmet need for MRI automated brain tumor diagnosis. We
observe that the predictive capability of uni-modality models is limited and
their performance varies widely across modalities, and the commonly used
modality fusion methods would introduce potential noise, which results in
significant performance degradation. To overcome these challenges, we propose a
novel cross-modality guidance-aided multi-modal learning with dual attention
for addressing the task of MRI brain tumor grading. To balance the tradeoff
between model efficiency and efficacy, we employ ResNet Mix Convolution as the
backbone network for feature extraction. Besides, dual attention is applied to
capture the semantic interdependencies in spatial and slice dimensions
respectively. To facilitate information interaction among modalities, we design
a cross-modality guidance-aided module where the primary modality guides the
other secondary modalities during the process of training, which can
effectively leverage the complementary information of different MRI modalities
and meanwhile alleviate the impact of the possible noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dunyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jinyue Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1&quot;&gt;Pheng-Ann Heng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09047">
<title>VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models. (arXiv:2401.09047v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09047</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-video generation aims to produce a video based on a given prompt.
Recently, several commercial video models have been able to generate plausible
videos with minimal noise, excellent details, and high aesthetic scores.
However, these models rely on large-scale, well-filtered, high-quality videos
that are not accessible to the community. Many existing research works, which
train models using the low-quality WebVid-10M dataset, struggle to generate
high-quality videos because the models are optimized to fit WebVid-10M. In this
work, we explore the training scheme of video models extended from Stable
Diffusion and investigate the feasibility of leveraging low-quality videos and
synthesized high-quality images to obtain a high-quality video model. We first
analyze the connection between the spatial and temporal modules of video models
and the distribution shift to low-quality videos. We observe that full training
of all modules results in a stronger coupling between spatial and temporal
modules than only training temporal modules. Based on this stronger coupling,
we shift the distribution to higher quality without motion degradation by
finetuning spatial modules with high-quality images, resulting in a generic
high-quality video model. Evaluations are conducted to demonstrate the
superiority of the proposed method, particularly in picture quality, motion,
and concept composition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haoxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cun_X/0/1/0/all/0/1&quot;&gt;Xiaodong Cun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1&quot;&gt;Menghan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1&quot;&gt;Chao Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09048">
<title>Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis. (arXiv:2401.09048v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09048</link>
<description rdf:parseType="Literal">&lt;p&gt;Addressing the limitations of text as a source of accurate layout
representation in text-conditional diffusion models, many works incorporate
additional signals to condition certain attributes within a generated image.
Although successful, previous works do not account for the specific
localization of said attributes extended into the three dimensional plane. In
this context, we present a conditional diffusion model that integrates control
over three-dimensional object placement with disentangled representations of
global stylistic semantics from multiple exemplar images. Specifically, we
first introduce \textit{depth disentanglement training} to leverage the
relative depth of objects as an estimator, allowing the model to identify the
absolute positions of unseen objects through the use of synthetic image
triplets. We also introduce \textit{soft guidance}, a method for imposing
global semantics onto targeted regions without the use of any additional
localization cues. Our integrated framework, \textsc{Compose and Conquer
(CnC)}, unifies these techniques to localize multiple conditions in a
disentangled manner. We demonstrate that our approach allows perception of
objects at varying depths while offering a versatile framework for composing
localized objects with different global semantics. Code:
https://github.com/tomtom1103/compose-and-conquer/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jonghyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1&quot;&gt;Hansam Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1&quot;&gt;Youngjoon Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seoung Bum Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1&quot;&gt;Yonghyun Jeong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09049">
<title>Enhancing Lidar-based Object Detection in Adverse Weather using Offset Sequences in Time. (arXiv:2401.09049v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09049</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated vehicles require an accurate perception of their surroundings for
safe and efficient driving. Lidar-based object detection is a widely used
method for environment perception, but its performance is significantly
affected by adverse weather conditions such as rain and fog. In this work, we
investigate various strategies for enhancing the robustness of lidar-based
object detection by processing sequential data samples generated by lidar
sensors. Our approaches leverage temporal information to improve a lidar object
detection model, without the need for additional filtering or pre-processing
steps. We compare $10$ different neural network architectures that process
point cloud sequences including a novel augmentation strategy introducing a
temporal offset between frames of a sequence during training and evaluate the
effectiveness of all strategies on lidar point clouds under adverse weather
conditions through experiments. Our research provides a comprehensive study of
effective methods for mitigating the effects of adverse weather on the
reliability of lidar-based object detection using sequential data that are
evaluated using public datasets such as nuScenes, Dense, and the Canadian
Adverse Driving Conditions Dataset. Our findings demonstrate that our novel
method, involving temporal offset augmentation through randomized frame
skipping in sequences, enhances object detection accuracy compared to both the
baseline model (Pillar-based Object Detection) and no augmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kempen_R/0/1/0/all/0/1&quot;&gt;Raphael van Kempen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rehbronn_T/0/1/0/all/0/1&quot;&gt;Tim Rehbronn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jose_A/0/1/0/all/0/1&quot;&gt;Abin Jose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stegmaier_J/0/1/0/all/0/1&quot;&gt;Johannes Stegmaier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lampe_B/0/1/0/all/0/1&quot;&gt;Bastian Lampe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woopen_T/0/1/0/all/0/1&quot;&gt;Timo Woopen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eckstein_L/0/1/0/all/0/1&quot;&gt;Lutz Eckstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09050">
<title>Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation with Deterministic Sampling Prior. (arXiv:2401.09050v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09050</link>
<description rdf:parseType="Literal">&lt;p&gt;Score distillation sampling (SDS) and its variants have greatly boosted the
development of text-to-3D generation, but are vulnerable to geometry collapse
and poor textures yet. To solve this issue, we first deeply analyze the SDS and
find that its distillation sampling process indeed corresponds to the
trajectory sampling of a stochastic differential equation (SDE): SDS samples
along an SDE trajectory to yield a less noisy sample which then serves as a
guidance to optimize a 3D model. However, the randomness in SDE sampling often
leads to a diverse and unpredictable sample which is not always less noisy, and
thus is not a consistently correct guidance, explaining the vulnerability of
SDS. Since for any SDE, there always exists an ordinary differential equation
(ODE) whose trajectory sampling can deterministically and consistently converge
to the desired target point as the SDE, we propose a novel and effective
&quot;Consistent3D&quot; method that explores the ODE deterministic sampling prior for
text-to-3D generation. Specifically, at each training iteration, given a
rendered image by a 3D model, we first estimate its desired 3D score function
by a pre-trained 2D diffusion model, and build an ODE for trajectory sampling.
Next, we design a consistency distillation sampling loss which samples along
the ODE trajectory to generate two adjacent samples and uses the less noisy
sample to guide another more noisy one for distilling the deterministic prior
into the 3D model. Experimental results show the efficacy of our Consistent3D
in generating high-fidelity and diverse 3D objects and large-scale scenes, as
shown in Fig. 1. The codes are available at
https://github.com/sail-sg/Consistent3D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zike Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1&quot;&gt;Xuanyu Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xiaoding Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanwang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09057">
<title>CrossVideo: Self-supervised Cross-modal Contrastive Learning for Point Cloud Video Understanding. (arXiv:2401.09057v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09057</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel approach named CrossVideo, which aims to
enhance self-supervised cross-modal contrastive learning in the field of point
cloud video understanding. Traditional supervised learning methods encounter
limitations due to data scarcity and challenges in label acquisition. To
address these issues, we propose a self-supervised learning method that
leverages the cross-modal relationship between point cloud videos and image
videos to acquire meaningful feature representations. Intra-modal and
cross-modal contrastive learning techniques are employed to facilitate
effective comprehension of point cloud video. We also propose a multi-level
contrastive approach for both modalities. Through extensive experiments, we
demonstrate that our method significantly surpasses previous state-of-the-art
approaches, and we conduct comprehensive ablation studies to validate the
effectiveness of our proposed designs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yunze Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changxi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1&quot;&gt;Li Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09059">
<title>Autonomous Catheterization with Open-source Simulator and Expert Trajectory. (arXiv:2401.09059v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.09059</link>
<description rdf:parseType="Literal">&lt;p&gt;Endovascular robots have been actively developed in both academia and
industry. However, progress toward autonomous catheterization is often hampered
by the widespread use of closed-source simulators and physical phantoms.
Additionally, the acquisition of large-scale datasets for training machine
learning algorithms with endovascular robots is usually infeasible due to
expensive medical procedures. In this chapter, we introduce CathSim, the first
open-source simulator for endovascular intervention to address these
limitations. CathSim emphasizes real-time performance to enable rapid
development and testing of learning algorithms. We validate CathSim against the
real robot and show that our simulator can successfully mimic the behavior of
the real robot. Based on CathSim, we develop a multimodal expert navigation
network and demonstrate its effectiveness in downstream endovascular navigation
tasks. The intensive experimental results suggest that CathSim has the
potential to significantly accelerate research in the autonomous
catheterization field. Our project is publicly available at
https://github.com/airvlab/cathsim.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jianu_T/0/1/0/all/0/1&quot;&gt;Tudor Jianu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Baoru Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_T/0/1/0/all/0/1&quot;&gt;Tuan Vo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_M/0/1/0/all/0/1&quot;&gt;Minh Nhat Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;Jingxuan Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hoan Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omisore_O/0/1/0/all/0/1&quot;&gt;Olatunji Omisore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berthet_Rayne_P/0/1/0/all/0/1&quot;&gt;Pierre Berthet-Rayne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fichera_S/0/1/0/all/0/1&quot;&gt;Sebastiano Fichera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09067">
<title>Towards Continual Learning Desiderata via HSIC-Bottleneck Orthogonalization and Equiangular Embedding. (arXiv:2401.09067v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09067</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are susceptible to catastrophic forgetting when trained
on sequential tasks. Various continual learning (CL) methods often rely on
exemplar buffers or/and network expansion for balancing model stability and
plasticity, which, however, compromises their practical value due to privacy
and memory concerns. Instead, this paper considers a strict yet realistic
setting, where the training data from previous tasks is unavailable and the
model size remains relatively constant during sequential training. To achieve
such desiderata, we propose a conceptually simple yet effective method that
attributes forgetting to layer-wise parameter overwriting and the resulting
decision boundary distortion. This is achieved by the synergy between two key
components: HSIC-Bottleneck Orthogonalization (HBO) implements non-overwritten
parameter updates mediated by Hilbert-Schmidt independence criterion in an
orthogonal space and EquiAngular Embedding (EAE) enhances decision boundary
adaptation between old and new tasks with predefined basis vectors. Extensive
experiments demonstrate that our method achieves competitive accuracy
performance, even with absolute superiority of zero exemplar buffer and 1.02x
the base model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Depeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junwei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1&quot;&gt;Qining Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1&quot;&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zhigang Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09083">
<title>Remote Sensing ChatGPT: Solving Remote Sensing Tasks with ChatGPT and Visual Models. (arXiv:2401.09083v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09083</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the flourishing large language models(LLM), especially ChatGPT,
have shown exceptional performance in language understanding, reasoning, and
interaction, attracting users and researchers from multiple fields and domains.
Although LLMs have shown great capacity to perform human-like task
accomplishment in natural language and natural image, their potential in
handling remote sensing interpretation tasks has not yet been fully explored.
Moreover, the lack of automation in remote sensing task planning hinders the
accessibility of remote sensing interpretation techniques, especially to
non-remote sensing experts from multiple research fields. To this end, we
present Remote Sensing ChatGPT, an LLM-powered agent that utilizes ChatGPT to
connect various AI-based remote sensing models to solve complicated
interpretation tasks. More specifically, given a user request and a remote
sensing image, we utilized ChatGPT to understand user requests, perform task
planning according to the tasks&apos; functions, execute each subtask iteratively,
and generate the final response according to the output of each subtask.
Considering that LLM is trained with natural language and is not capable of
directly perceiving visual concepts as contained in remote sensing images, we
designed visual cues that inject visual information into ChatGPT. With Remote
Sensing ChatGPT, users can simply send a remote sensing image with the
corresponding request, and get the interpretation results as well as language
feedback from Remote Sensing ChatGPT. Experiments and examples show that Remote
Sensing ChatGPT can tackle a wide range of remote sensing tasks and can be
extended to more tasks with more sophisticated models such as the remote
sensing foundation model. The code and demo of Remote Sensing ChatGPT is
publicly available at https://github.com/HaonanGuo/Remote-Sensing-ChatGPT .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Haonan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1&quot;&gt;Xin Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bo Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liangpei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Deren Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09084">
<title>UniVG: Towards UNIfied-modal Video Generation. (arXiv:2401.09084v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09084</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion based video generation has received extensive attention and
achieved considerable success within both the academic and industrial
communities. However, current efforts are mainly concentrated on
single-objective or single-task video generation, such as generation driven by
text, by image, or by a combination of text and image. This cannot fully meet
the needs of real-world application scenarios, as users are likely to input
images and text conditions in a flexible manner, either individually or in
combination. To address this, we propose a Unified-modal Video Genearation
system that is capable of handling multiple video generation tasks across text
and image modalities. To this end, we revisit the various video generation
tasks within our system from the perspective of generative freedom, and
classify them into high-freedom and low-freedom video generation categories.
For high-freedom video generation, we employ Multi-condition Cross Attention to
generate videos that align with the semantics of the input images or text. For
low-freedom video generation, we introduce Biased Gaussian Noise to replace the
pure random Gaussian Noise, which helps to better preserve the content of the
input conditions. Our method achieves the lowest Fr\&apos;echet Video Distance (FVD)
on the public academic benchmark MSR-VTT, surpasses the current open-source
methods in human evaluations, and is on par with the current close-source
method Gen2. For more samples, visit https://univg-baidu.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_L/0/1/0/all/0/1&quot;&gt;Ludan Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1&quot;&gt;Lei Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chuanwei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xinyan Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09101">
<title>PIN-SLAM: LiDAR SLAM Using a Point-Based Implicit Neural Representation for Achieving Global Map Consistency. (arXiv:2401.09101v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.09101</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate and robust localization and mapping are essential components for
most autonomous robots. In this paper, we propose a SLAM system for building
globally consistent maps, called PIN-SLAM, that is based on an elastic and
compact point-based implicit neural map representation. Taking range
measurements as input, our approach alternates between incremental learning of
the local implicit signed distance field and the pose estimation given the
current local map using a correspondence-free, point-to-implicit model
registration. Our implicit map is based on sparse optimizable neural points,
which are inherently elastic and deformable with the global pose adjustment
when closing a loop. Loops are also detected using the neural point features.
Extensive experiments validate that PIN-SLAM is robust to various environments
and versatile to different range sensors such as LiDAR and RGB-D cameras.
PIN-SLAM achieves pose estimation accuracy better or on par with the
state-of-the-art LiDAR odometry or SLAM systems and outperforms the recent
neural implicit SLAM approaches while maintaining a more consistent, and highly
compact implicit map that can be reconstructed as accurate and complete meshes.
Finally, thanks to the voxel hashing for efficient neural points indexing and
the fast implicit map-based registration without closest point association,
PIN-SLAM can run at the sensor frame rate on a moderate GPU. Codes will be
available at: https://github.com/PRBonn/PIN_SLAM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yue Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1&quot;&gt;Xingguang Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiesmann_L/0/1/0/all/0/1&quot;&gt;Louis Wiesmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Posewsky_T/0/1/0/all/0/1&quot;&gt;Thorbj&amp;#xf6;rn Posewsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behley_J/0/1/0/all/0/1&quot;&gt;Jens Behley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stachniss_C/0/1/0/all/0/1&quot;&gt;Cyrill Stachniss&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09109">
<title>Trapped in texture bias? A large scale comparison of deep instance segmentation. (arXiv:2401.09109v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09109</link>
<description rdf:parseType="Literal">&lt;p&gt;Do deep learning models for instance segmentation generalize to novel objects
in a systematic way? For classification, such behavior has been questioned. In
this study, we aim to understand if certain design decisions such as framework,
architecture or pre-training contribute to the semantic understanding of
instance segmentation. To answer this question, we consider a special case of
robustness and compare pre-trained models on a challenging benchmark for
object-centric, out-of-distribution texture. We do not introduce another method
in this work. Instead, we take a step back and evaluate a broad range of
existing literature. This includes Cascade and Mask R-CNN, Swin Transformer,
BMask, YOLACT(++), DETR, BCNet, SOTR and SOLOv2. We find that YOLACT++, SOTR
and SOLOv2 are significantly more robust to out-of-distribution texture than
other frameworks. In addition, we show that deeper and dynamic architectures
improve robustness whereas training schedules, data augmentation and
pre-training have only a minor impact. In summary we evaluate 68 models on 61
versions of MS COCO for a total of 4148 evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theodoridis_J/0/1/0/all/0/1&quot;&gt;Johannes Theodoridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofmann_J/0/1/0/all/0/1&quot;&gt;Jessica Hofmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maucher_J/0/1/0/all/0/1&quot;&gt;Johannes Maucher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schilling_A/0/1/0/all/0/1&quot;&gt;Andreas Schilling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09112">
<title>Stream Query Denoising for Vectorized HD Map Construction. (arXiv:2401.09112v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09112</link>
<description rdf:parseType="Literal">&lt;p&gt;To enhance perception performance in complex and extensive scenarios within
the realm of autonomous driving, there has been a noteworthy focus on temporal
modeling, with a particular emphasis on streaming methods. The prevailing trend
in streaming models involves the utilization of stream queries for the
propagation of temporal information. Despite the prevalence of this approach,
the direct application of the streaming paradigm to the construction of
vectorized high-definition maps (HD-maps) fails to fully harness the inherent
potential of temporal information. This paper introduces the Stream Query
Denoising (SQD) strategy as a novel approach for temporal modeling in
high-definition map (HD-map) construction. SQD is designed to facilitate the
learning of temporal consistency among map elements within the streaming model.
The methodology involves denoising the queries that have been perturbed by the
addition of noise to the ground-truth information from the preceding frame.
This denoising process aims to reconstruct the ground-truth information for the
current frame, thereby simulating the prediction process inherent in stream
queries. The SQD strategy can be applied to those streaming methods (e.g.,
StreamMapNet) to enhance the temporal modeling. The proposed SQD-MapNet is the
StreamMapNet equipped with SQD. Extensive experiments on nuScenes and
Argoverse2 show that our method is remarkably superior to other existing
methods across all settings of close range and long range. The code will be
available soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1&quot;&gt;Fan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yingfei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yucheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zehui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tiancai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1&quot;&gt;Feng Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09126">
<title>Objects With Lighting: A Real-World Dataset for Evaluating Reconstruction and Rendering for Object Relighting. (arXiv:2401.09126v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09126</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing an object from photos and placing it virtually in a new
environment goes beyond the standard novel view synthesis task as the
appearance of the object has to not only adapt to the novel viewpoint but also
to the new lighting conditions and yet evaluations of inverse rendering methods
rely on novel view synthesis data or simplistic synthetic datasets for
quantitative analysis. This work presents a real-world dataset for measuring
the reconstruction and rendering of objects for relighting. To this end, we
capture the environment lighting and ground truth images of the same objects in
multiple environments allowing to reconstruct the objects from images taken in
one environment and quantify the quality of the rendered views for the unseen
lighting environments. Further, we introduce a simple baseline composed of
off-the-shelf methods and test several state-of-the-art methods on the
relighting task and show that novel view synthesis is not a reliable proxy to
measure performance. Code and dataset are available at
https://github.com/isl-org/objects-with-lighting .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ummenhofer_B/0/1/0/all/0/1&quot;&gt;Benjamin Ummenhofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1&quot;&gt;Sanskar Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sepulveda_R/0/1/0/all/0/1&quot;&gt;Rene Sepulveda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lao_Y/0/1/0/all/0/1&quot;&gt;Yixing Lao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1&quot;&gt;Tianhang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richter_S/0/1/0/all/0/1&quot;&gt;Stephan Richter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shenlong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ros_G/0/1/0/all/0/1&quot;&gt;German Ros&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09133">
<title>SM$^3$: Self-Supervised Multi-task Modeling with Multi-view 2D Images for Articulated Objects. (arXiv:2401.09133v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09133</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing real-world objects and estimating their movable joint
structures are pivotal technologies within the field of robotics. Previous
research has predominantly focused on supervised approaches, relying on
extensively annotated datasets to model articulated objects within limited
categories. However, this approach falls short of effectively addressing the
diversity present in the real world. To tackle this issue, we propose a
self-supervised interaction perception method, referred to as SM$^3$, which
leverages multi-view RGB images captured before and after interaction to model
articulated objects, identify the movable parts, and infer the parameters of
their rotating joints. By constructing 3D geometries and textures from the
captured 2D images, SM$^3$ achieves integrated optimization of movable part and
joint parameters during the reconstruction process, obviating the need for
annotations. Furthermore, we introduce the MMArt dataset, an extension of
PartNet-Mobility, encompassing multi-view and multi-modal data of articulated
objects spanning diverse categories. Evaluations demonstrate that SM$^3$
surpasses existing benchmarks across various categories and objects, while its
adaptability in real-world scenarios has been thoroughly validated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haowen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zhao Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1&quot;&gt;Zhengping Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1&quot;&gt;Liang Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yakun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_X/0/1/0/all/0/1&quot;&gt;Xiuquan Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09140">
<title>Relative Pose for Nonrigid Multi-Perspective Cameras: The Static Case. (arXiv:2401.09140v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.09140</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-perspective cameras with potentially non-overlapping fields of view
have become an important exteroceptive sensing modality in a number of
applications such as intelligent vehicles, drones, and mixed reality headsets.
In this work, we challenge one of the basic assumptions made in these
scenarios, which is that the multi-camera rig is rigid. More specifically, we
are considering the problem of estimating the relative pose between a static
non-rigid rig in different spatial orientations while taking into account the
effect of gravity onto the system. The deformable physical connections between
each camera and the body center are approximated by a simple cantilever model,
and inserted into the generalized epipolar constraint. Our results lead us to
the important insight that the latent parameters of the deformation model,
meaning the gravity vector in both views, become observable. We present a
concise analysis of the observability of all variables based on noise,
outliers, and rig rigidity for two different algorithms. The first one is a
vision-only alternative, while the second one makes use of additional gravity
measurements. To conclude, we demonstrate the ability to sense gravity in a
real-world example, and discuss practical implications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Min Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1&quot;&gt;Laurent Kneip&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09146">
<title>Continuous Piecewise-Affine Based Motion Model for Image Animation. (arXiv:2401.09146v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09146</link>
<description rdf:parseType="Literal">&lt;p&gt;Image animation aims to bring static images to life according to driving
videos and create engaging visual content that can be used for various purposes
such as animation, entertainment, and education. Recent unsupervised methods
utilize affine and thin-plate spline transformations based on keypoints to
transfer the motion in driving frames to the source image. However, limited by
the expressive power of the transformations used, these methods always produce
poor results when the gap between the motion in the driving frame and the
source image is large. To address this issue, we propose to model motion from
the source image to the driving frame in highly-expressive diffeomorphism
spaces. Firstly, we introduce Continuous Piecewise-Affine based (CPAB)
transformation to model the motion and present a well-designed inference
algorithm to generate CPAB transformation from control keypoints. Secondly, we
propose a SAM-guided keypoint semantic loss to further constrain the keypoint
extraction process and improve the semantic consistency between the
corresponding keypoints on the source and driving images. Finally, we design a
structure alignment loss to align the structure-related features extracted from
driving and generated images, thus helping the generator generate results that
are more consistent with the driving action. Extensive experiments on four
datasets demonstrate the effectiveness of our method against state-of-the-art
competitors quantitatively and qualitatively. Code will be publicly available
at: https://github.com/DevilPG/AAAI2024-CPABMM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hexiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fengqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qianyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1&quot;&gt;Ran Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1&quot;&gt;Xin Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lizhuang Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09160">
<title>DK-SLAM: Monocular Visual SLAM with Deep Keypoints Adaptive Learning, Tracking and Loop-Closing. (arXiv:2401.09160v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.09160</link>
<description rdf:parseType="Literal">&lt;p&gt;Unreliable feature extraction and matching in handcrafted features undermine
the performance of visual SLAM in complex real-world scenarios. While learned
local features, leveraging CNNs, demonstrate proficiency in capturing
high-level information and excel in matching benchmarks, they encounter
challenges in continuous motion scenes, resulting in poor generalization and
impacting loop detection accuracy. To address these issues, we present DK-SLAM,
a monocular visual SLAM system with adaptive deep local features. MAML
optimizes the training of these features, and we introduce a coarse-to-fine
feature tracking approach. Initially, a direct method approximates the relative
pose between consecutive frames, followed by a feature matching method for
refined pose estimation. To counter cumulative positioning errors, a novel
online learning binary feature-based online loop closure module identifies loop
nodes within a sequence. Experimental results underscore DK-SLAM&apos;s efficacy,
outperforms representative SLAM solutions, such as ORB-SLAM3 on publicly
available datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1&quot;&gt;Hao Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lilian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1&quot;&gt;Jun Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tie_J/0/1/0/all/0/1&quot;&gt;Junbo Tie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaofeng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaoping Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yifei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changhao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09180">
<title>Unsupervised Multiple Domain Translation through Controlled Disentanglement in Variational Autoencoder. (arXiv:2401.09180v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09180</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised Multiple Domain Translation is the task of transforming data
from one domain to other domains without having paired data to train the
systems. Typically, methods based on Generative Adversarial Networks (GANs) are
used to address this task. However, our proposal exclusively relies on a
modified version of a Variational Autoencoder. This modification consists of
the use of two latent variables disentangled in a controlled way by design. One
of this latent variables is imposed to depend exclusively on the domain, while
the other one must depend on the rest of the variability factors of the data.
Additionally, the conditions imposed over the domain latent variable allow for
better control and understanding of the latent space. We empirically
demonstrate that our approach works on different vision datasets improving the
performance of other well known methods. Finally, we prove that, indeed, one of
the latent variables stores all the information related to the domain and the
other one hardly contains any domain information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antonio_A/0/1/0/all/0/1&quot;&gt;Almud&amp;#xe9;var Antonio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theo_M/0/1/0/all/0/1&quot;&gt;Mariotte Th&amp;#xe9;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alfonso_O/0/1/0/all/0/1&quot;&gt;Ortega Alfonso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marie_T/0/1/0/all/0/1&quot;&gt;Tahon Marie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09190">
<title>Exploring the Role of Convolutional Neural Networks (CNN) in Dental Radiography Segmentation: A Comprehensive Systematic Literature Review. (arXiv:2401.09190v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09190</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of dentistry, there is a growing demand for increased precision
in diagnostic tools, with a specific focus on advanced imaging techniques such
as computed tomography, cone beam computed tomography, magnetic resonance
imaging, ultrasound, and traditional intra-oral periapical X-rays. Deep
learning has emerged as a pivotal tool in this context, enabling the
implementation of automated segmentation techniques crucial for extracting
essential diagnostic data. This integration of cutting-edge technology
addresses the urgent need for effective management of dental conditions, which,
if left undetected, can have a significant impact on human health. The
impressive track record of deep learning across various domains, including
dentistry, underscores its potential to revolutionize early detection and
treatment of oral health issues. Objective: Having demonstrated significant
results in diagnosis and prediction, deep convolutional neural networks (CNNs)
represent an emerging field of multidisciplinary research. The goals of this
study were to provide a concise overview of the state of the art, standardize
the current debate, and establish baselines for future research. Method: In
this study, a systematic literature review is employed as a methodology to
identify and select relevant studies that specifically investigate the deep
learning technique for dental imaging analysis. This study elucidates the
methodological approach, including the systematic collection of data,
statistical analysis, and subsequent dissemination of outcomes. Conclusion:
This work demonstrates how Convolutional Neural Networks (CNNs) can be employed
to analyze images, serving as effective tools for detecting dental pathologies.
Although this research acknowledged some limitations, CNNs utilized for
segmenting and categorizing teeth exhibited their highest level of performance
overall.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brahmi_W/0/1/0/all/0/1&quot;&gt;Walid Brahmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jdey_I/0/1/0/all/0/1&quot;&gt;Imen Jdey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drira_F/0/1/0/all/0/1&quot;&gt;Fadoua Drira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09195">
<title>Training-Free Semantic Video Composition via Pre-trained Diffusion Model. (arXiv:2401.09195v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09195</link>
<description rdf:parseType="Literal">&lt;p&gt;The video composition task aims to integrate specified foregrounds and
backgrounds from different videos into a harmonious composite. Current
approaches, predominantly trained on videos with adjusted foreground color and
lighting, struggle to address deep semantic disparities beyond superficial
adjustments, such as domain gaps. Therefore, we propose a training-free
pipeline employing a pre-trained diffusion model imbued with semantic prior
knowledge, which can process composite videos with broader semantic
disparities. Specifically, we process the video frames in a cascading manner
and handle each frame in two processes with the diffusion model. In the
inversion process, we propose Balanced Partial Inversion to obtain generation
initial points that balance reversibility and modifiability. Then, in the
generation process, we further propose Inter-Frame Augmented attention to
augment foreground continuity across frames. Experimental results reveal that
our pipeline successfully ensures the visual harmony and inter-frame coherence
of the outputs, demonstrating efficacy in managing broader semantic
disparities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jiaqi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Sitong Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Junchen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Lianli Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jingkuan Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09232">
<title>Dynamic Relation Transformer for Contextual Text Block Detection. (arXiv:2401.09232v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09232</link>
<description rdf:parseType="Literal">&lt;p&gt;Contextual Text Block Detection (CTBD) is the task of identifying coherent
text blocks within the complexity of natural scenes. Previous methodologies
have treated CTBD as either a visual relation extraction challenge within
computer vision or as a sequence modeling problem from the perspective of
natural language processing. We introduce a new framework that frames CTBD as a
graph generation problem. This methodology consists of two essential
procedures: identifying individual text units as graph nodes and discerning the
sequential reading order relationships among these units as graph edges.
Leveraging the cutting-edge capabilities of DQ-DETR for node detection, our
framework innovates further by integrating a novel mechanism, a Dynamic
Relation Transformer (DRFormer), dedicated to edge generation. DRFormer
incorporates a dual interactive transformer decoder that deftly manages a
dynamic graph structure refinement process. Through this iterative process, the
model systematically enhances the graph&apos;s fidelity, ultimately resulting in
improved precision in detecting contextual text blocks. Comprehensive
experimental evaluations conducted on both SCUT-CTW-Context and ReCTS-Context
datasets substantiate that our method achieves state-of-the-art results,
underscoring the effectiveness and potential of our graph generation framework
in advancing the field of CTBD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiawei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shunchi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1&quot;&gt;Kai Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chixiang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1&quot;&gt;Zhuoyao Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_Q/0/1/0/all/0/1&quot;&gt;Qiang Huo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09239">
<title>DaFoEs: Mixing Datasets towards the generalization of vision-state deep-learning Force Estimation in Minimally Invasive Robotic Surgery. (arXiv:2401.09239v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09239</link>
<description rdf:parseType="Literal">&lt;p&gt;Precisely determining the contact force during safe interaction in Minimally
Invasive Robotic Surgery (MIRS) is still an open research challenge. Inspired
by post-operative qualitative analysis from surgical videos, the use of
cross-modality data driven deep neural network models has been one of the
newest approaches to predict sensorless force trends. However, these methods
required for large and variable datasets which are not currently available. In
this paper, we present a new vision-haptic dataset (DaFoEs) with variable soft
environments for the training of deep neural models. In order to reduce the
bias from a single dataset, we present a pipeline to generalize different
vision and state data inputs for mixed dataset training, using a previously
validated dataset with different setup. Finally, we present a variable
encoder-decoder architecture to predict the forces done by the laparoscopic
tool using single input or sequence of inputs. For input sequence, we use a
recurrent decoder, named with the prefix R, and a new temporal sampling to
represent the acceleration of the tool. During our training, we demonstrate
that single dataset training tends to overfit to the training data domain, but
has difficulties on translating the results across new domains. However,
dataset mixing presents a good translation with a mean relative estimated force
error of 5% and 12% for the recurrent and non-recurrent models respectively.
Our method, also marginally increase the effectiveness of transformers for
force estimation up to a maximum of ~15%, as the volume of available data is
increase by 150%. In conclusion, we demonstrate that mixing experimental set
ups for vision-state force estimation in MIRS is a possible approach towards
the general solution of the problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reyzabal_M/0/1/0/all/0/1&quot;&gt;Mikel De Iturrate Reyzabal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingcong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1&quot;&gt;Sebastien Ourselin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongbin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09245">
<title>Uncertainty estimates for semantic segmentation: providing enhanced reliability for automated motor claims handling. (arXiv:2401.09245v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09245</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural network models for image segmentation can be a powerful tool for
the automation of motor claims handling processes in the insurance industry. A
crucial aspect is the reliability of the model outputs when facing adverse
conditions, such as low quality photos taken by claimants to document damages.
We explore the use of a meta-classification model to assess the precision of
segments predicted by a model trained for the semantic segmentation of car body
parts. Different sets of features correlated with the quality of a segment are
compared, and an AUROC score of 0.915 is achieved for distinguishing between
high- and low-quality segments. By removing low-quality segments, the average
mIoU of the segmentation output is improved by 16 percentage points and the
number of wrongly predicted segments is reduced by 77%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuchler_J/0/1/0/all/0/1&quot;&gt;Jan K&amp;#xfc;chler&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kroll_D/0/1/0/all/0/1&quot;&gt;Daniel Kr&amp;#xf6;ll&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schoenen_S/0/1/0/all/0/1&quot;&gt;Sebastian Schoenen&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witte_A/0/1/0/all/0/1&quot;&gt;Andreas Witte&lt;/a&gt; (1) ((1) ControlExpert GmbH, Langenfeld, Germany)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09252">
<title>3D Scene Geometry Estimation from 360$^\circ$ Imagery: A Survey. (arXiv:2401.09252v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09252</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides a comprehensive survey on pioneer and state-of-the-art 3D
scene geometry estimation methodologies based on single, two, or multiple
images captured under the omnidirectional optics. We first revisit the basic
concepts of the spherical camera model, and review the most common acquisition
technologies and representation formats suitable for omnidirectional (also
called 360$^\circ$, spherical or panoramic) images and videos. We then survey
monocular layout and depth inference approaches, highlighting the recent
advances in learning-based solutions suited for spherical data. The classical
stereo matching is then revised on the spherical domain, where methodologies
for detecting and describing sparse and dense features become crucial. The
stereo matching concepts are then extrapolated for multiple view camera setups,
categorizing them among light fields, multi-view stereo, and structure from
motion (or visual simultaneous localization and mapping). We also compile and
discuss commonly adopted datasets and figures of merit indicated for each
purpose and list recent results for completeness. We conclude this paper by
pointing out current and future trends.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silveira_T/0/1/0/all/0/1&quot;&gt;Thiago Lopes Trugillo da Silveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinto_P/0/1/0/all/0/1&quot;&gt;Paulo Gamarra Lessa Pinto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Llerena_J/0/1/0/all/0/1&quot;&gt;Jeffri Erwin Murrugarra Llerena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1&quot;&gt;Claudio Rosito Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09258">
<title>An Efficient Generalizable Framework for Visuomotor Policies via Control-aware Augmentation and Privilege-guided Distillation. (arXiv:2401.09258v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.09258</link>
<description rdf:parseType="Literal">&lt;p&gt;Visuomotor policies, which learn control mechanisms directly from
high-dimensional visual observations, confront challenges in adapting to new
environments with intricate visual variations. Data augmentation emerges as a
promising method for bridging these generalization gaps by enriching data
variety. However, straightforwardly augmenting the entire observation shall
impose excessive burdens on policy learning and may even result in performance
degradation. In this paper, we propose to improve the generalization ability of
visuomotor policies as well as preserve training stability from two aspects: 1)
We learn a control-aware mask through a self-supervised reconstruction task
with three auxiliary losses and then apply strong augmentation only to those
control-irrelevant regions based on the mask to reduce the generalization gaps.
2) To address training instability issues prevalent in visual reinforcement
learning (RL), we distill the knowledge from a pretrained RL expert processing
low-level environment states, to the student visuomotor policy. The policy is
subsequently deployed to unseen environments without any further finetuning. We
conducted comparison and ablation studies across various benchmarks: the
DMControl Generalization Benchmark (DMC-GB), the enhanced Robot Manipulation
Distraction Benchmark (RMDB), and a specialized long-horizontal drawer-opening
robotic task. The extensive experimental results well demonstrate the
effectiveness of our method, e.g., showing a 17\% improvement over previous
methods in the video-hard setting of DMC-GB.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yinuo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1&quot;&gt;Kun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_T/0/1/0/all/0/1&quot;&gt;Tianjiao Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_X/0/1/0/all/0/1&quot;&gt;Xiaozhu Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1&quot;&gt;Zhengping Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1&quot;&gt;Qinru Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chi Harold Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09266">
<title>P$^2$OT: Progressive Partial Optimal Transport for Deep Imbalanced Clustering. (arXiv:2401.09266v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09266</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep clustering, which learns representation and semantic clustering without
labels information, poses a great challenge for deep learning-based approaches.
Despite significant progress in recent years, most existing methods focus on
uniformly distributed datasets, significantly limiting the practical
applicability of their methods. In this paper, we first introduce a more
practical problem setting named deep imbalanced clustering, where the
underlying classes exhibit an imbalance distribution. To tackle this problem,
we propose a novel pseudo-labeling-based learning framework. Our framework
formulates pseudo-label generation as a progressive partial optimal transport
problem, which progressively transports each sample to imbalanced clusters
under prior distribution constraints, thus generating imbalance-aware
pseudo-labels and learning from high-confident samples. In addition, we
transform the initial formulation into an unbalanced optimal transport problem
with augmented constraints, which can be solved efficiently by a fast matrix
scaling algorithm. Experiments on various datasets, including a human-curated
long-tailed CIFAR100, challenging ImageNet-R, and large-scale subsets of
fine-grained iNaturalist2018 datasets, demonstrate the superiority of our
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chuyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Hui Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xuming He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09271">
<title>PixelDINO: Semi-Supervised Semantic Segmentation for Detecting Permafrost Disturbances. (arXiv:2401.09271v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09271</link>
<description rdf:parseType="Literal">&lt;p&gt;Arctic Permafrost is facing significant changes due to global climate change.
As these regions are largely inaccessible, remote sensing plays a crucial rule
in better understanding the underlying processes not just on a local scale, but
across the Arctic. In this study, we focus on the remote detection of
retrogressive thaw slumps (RTS), a permafrost disturbance comparable to
landslides induced by thawing. For such analyses from space, deep learning has
become an indispensable tool, but limited labelled training data remains a
challenge for training accurate models. To improve model generalization across
the Arctic without the need for additional labelled data, we present a
semi-supervised learning approach to train semantic segmentation models to
detect RTS. Our framework called PixelDINO is trained in parallel on labelled
data as well as unlabelled data. For the unlabelled data, the model segments
the imagery into self-taught pseudo-classes and the training procedure ensures
consistency of these pseudo-classes across strong augmentations of the input
data. Our experimental results demonstrate that PixelDINO can improve model
performance both over supervised baseline methods as well as existing
semi-supervised semantic segmentation approaches, highlighting its potential
for training robust models that generalize well to regions that were not
included in the training data. The project page containing code and other
materials for this study can be found at
\url{https://khdlr.github.io/PixelDINO/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heidler_K/0/1/0/all/0/1&quot;&gt;Konrad Heidler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nitze_I/0/1/0/all/0/1&quot;&gt;Ingmar Nitze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosse_G/0/1/0/all/0/1&quot;&gt;Guido Grosse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiao Xiang Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2003.06945">
<title>Scene Completeness-Aware Lidar Depth Completion for Driving Scenario. (arXiv:2003.06945v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2003.06945</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces Scene Completeness-Aware Depth Completion (SCADC) to
complete raw lidar scans into dense depth maps with fine and complete scene
structures. Recent sparse depth completion for lidars only focuses on the lower
scenes and produces irregular estimations on the upper because existing
datasets, such as KITTI, do not provide groundtruth for upper areas. These
areas are considered less important since they are usually sky or trees of less
scene understanding interest. However, we argue that in several driving
scenarios such as large trucks or cars with loads, objects could extend to the
upper parts of scenes. Thus depth maps with structured upper scene estimation
are important for RGBD algorithms. SCADC adopts stereo images that produce
disparities with better scene completeness but are generally less precise than
lidars, to help sparse lidar depth completion. To our knowledge, we are the
first to focus on scene completeness of sparse depth completion. We validate
our SCADC on both depth estimate precision and scene-completeness on KITTI.
Moreover, we experiment on less-explored outdoor RGBD semantic segmentation
with scene completeness-aware D-input to validate our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Cho-Ying Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_U/0/1/0/all/0/1&quot;&gt;Ulrich Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2006.07802">
<title>Geometry-Aware Instance Segmentation with Disparity Maps. (arXiv:2006.07802v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2006.07802</link>
<description rdf:parseType="Literal">&lt;p&gt;Most previous works of outdoor instance segmentation for images only use
color information. We explore a novel direction of sensor fusion to exploit
stereo cameras. Geometric information from disparities helps separate
overlapping objects of the same or different classes. Moreover, geometric
information penalizes region proposals with unlikely 3D shapes thus suppressing
false positive detections. Mask regression is based on 2D, 2.5D, and 3D ROI
using the pseudo-lidar and image-based representations. These mask predictions
are fused by a mask scoring process. However, public datasets only adopt stereo
systems with shorter baseline and focal legnth, which limit measuring ranges of
stereo cameras. We collect and utilize High-Quality Driving Stereo (HQDS)
dataset, using much longer baseline and focal length with higher resolution.
Our performance attains state of the art. Please refer to our project page. The
full paper is available here.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Cho-Ying Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaoyan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Happold_M/0/1/0/all/0/1&quot;&gt;Michael Happold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiangeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_U/0/1/0/all/0/1&quot;&gt;Ulrich Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.09772">
<title>Synergy between 3DMM and 3D Landmarks for Accurate 3D Facial Geometry. (arXiv:2110.09772v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2110.09772</link>
<description rdf:parseType="Literal">&lt;p&gt;This work studies learning from a synergy process of 3D Morphable Models
(3DMM) and 3D facial landmarks to predict complete 3D facial geometry,
including 3D alignment, face orientation, and 3D face modeling. Our synergy
process leverages a representation cycle for 3DMM parameters and 3D landmarks.
3D landmarks can be extracted and refined from face meshes built by 3DMM
parameters. We next reverse the representation direction and show that
predicting 3DMM parameters from sparse 3D landmarks improves the information
flow. Together we create a synergy process that utilizes the relation between
3D landmarks and 3DMM parameters, and they collaboratively contribute to better
performance. We extensively validate our contribution on full tasks of facial
geometry prediction and show our superior and robust performance on these tasks
for various scenarios. Particularly, we adopt only simple and widely-used
network operations to attain fast and accurate facial geometry prediction.
Codes and data: https://choyingw.github.io/works/SynergyNet/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Cho-Ying Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiangeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_U/0/1/0/all/0/1&quot;&gt;Ulrich Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.02779">
<title>A Dempster-Shafer approach to trustworthy AI with application to fetal brain MRI segmentation. (arXiv:2204.02779v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.02779</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models for medical image segmentation can fail unexpectedly and
spectacularly for pathological cases and images acquired at different centers
than training images, with labeling errors that violate expert knowledge. Such
errors undermine the trustworthiness of deep learning models for medical image
segmentation. Mechanisms for detecting and correcting such failures are
essential for safely translating this technology into clinics and are likely to
be a requirement of future regulations on artificial intelligence (AI). In this
work, we propose a trustworthy AI theoretical framework and a practical system
that can augment any backbone AI system using a fallback method and a fail-safe
mechanism based on Dempster-Shafer theory. Our approach relies on an actionable
definition of trustworthy AI. Our method automatically discards the voxel-level
labeling predicted by the backbone AI that violate expert knowledge and relies
on a fallback for those voxels. We demonstrate the effectiveness of the
proposed trustworthy AI approach on the largest reported annotated dataset of
fetal MRI consisting of 540 manually annotated fetal brain 3D T2w MRIs from 13
centers. Our trustworthy AI method improves the robustness of a
state-of-the-art backbone AI for fetal brain MRIs acquired across various
centers and for fetuses with various brain abnormalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1&quot;&gt;Lucas Fidon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aertsen_M/0/1/0/all/0/1&quot;&gt;Michael Aertsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kofler_F/0/1/0/all/0/1&quot;&gt;Florian Kofler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bink_A/0/1/0/all/0/1&quot;&gt;Andrea Bink&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+David_A/0/1/0/all/0/1&quot;&gt;Anna L. David&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Deprest_T/0/1/0/all/0/1&quot;&gt;Thomas Deprest&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Emam_D/0/1/0/all/0/1&quot;&gt;Doaa Emam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guffens_F/0/1/0/all/0/1&quot;&gt;Fr&amp;#xe9;d&amp;#xe9;ric Guffens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jakab_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe1;s Jakab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kasprian_G/0/1/0/all/0/1&quot;&gt;Gregor Kasprian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kienast_P/0/1/0/all/0/1&quot;&gt;Patric Kienast&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Melbourne_A/0/1/0/all/0/1&quot;&gt;Andrew Melbourne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1&quot;&gt;Bjoern Menze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mufti_N/0/1/0/all/0/1&quot;&gt;Nada Mufti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pogledic_I/0/1/0/all/0/1&quot;&gt;Ivana Pogledic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prayer_D/0/1/0/all/0/1&quot;&gt;Daniela Prayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Stuempflen_M/0/1/0/all/0/1&quot;&gt;Marlene Stuempflen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Elslander_E/0/1/0/all/0/1&quot;&gt;Esther Van Elslander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Ourselin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Deprest_J/0/1/0/all/0/1&quot;&gt;Jan Deprest&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1&quot;&gt;Tom Vercauteren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.10993">
<title>TerrainMesh: Metric-Semantic Terrain Reconstruction from Aerial Images Using Joint 2D-3D Learning. (arXiv:2204.10993v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.10993</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers outdoor terrain mapping using RGB images obtained from
an aerial vehicle. While feature-based localization and mapping techniques
deliver real-time vehicle odometry and sparse keypoint depth reconstruction, a
dense model of the environment geometry and semantics (vegetation, buildings,
etc.) is usually recovered offline with significant computation and storage.
This paper develops a joint 2D-3D learning approach to reconstruct a local
metric-semantic mesh at each camera keyframe maintained by a visual odometry
algorithm. Given the estimated camera trajectory, the local meshes can be
assembled into a global environment model to capture the terrain topology and
semantics during online operation. A local mesh is reconstructed using an
initialization and refinement stage. In the initialization stage, we estimate
the mesh vertex elevation by solving a least squares problem relating the
vertex barycentric coordinates to the sparse keypoint depth measurements. In
the refinement stage, we associate 2D image and semantic features with the 3D
mesh vertices using camera projection and apply graph convolution to refine the
mesh vertex spatial coordinates and semantic features based on joint 2D and 3D
supervision. Quantitative and qualitative evaluation using real aerial images
show the potential of our method to support environmental monitoring and
surveillance applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1&quot;&gt;Qiaojun Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atanasov_N/0/1/0/all/0/1&quot;&gt;Nikolay Atanasov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.10973">
<title>Robust and Large-Payload DNN Watermarking via Fixed, Distribution-Optimized, Weights. (arXiv:2208.10973v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.10973</link>
<description rdf:parseType="Literal">&lt;p&gt;The design of an effective multi-bit watermarking algorithm hinges upon
finding a good trade-off between the three fundamental requirements forming the
watermarking trade-off triangle, namely, robustness against network
modifications, payload, and unobtrusiveness, ensuring minimal impact on the
performance of the watermarked network. In this paper, we first revisit the
nature of the watermarking trade-off triangle for the DNN case, then we exploit
our findings to propose a white-box, multi-bit watermarking method achieving
very large payload and strong robustness against network modification. In the
proposed system, the weights hosting the watermark are set prior to training,
making sure that their amplitude is large enough to bear the target payload and
survive network modifications, notably retraining, and are left unchanged
throughout the training process. The distribution of the weights carrying the
watermark is theoretically optimised to ensure the secrecy of the watermark and
make sure that the watermarked weights are indistinguishable from the
non-watermarked ones. The proposed method can achieve outstanding performance,
with no significant impact on network accuracy, including robustness against
network modifications, retraining and transfer learning, while ensuring a
payload which is out of reach of state of the art methods achieving a lower -
or at most comparable - robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tondi_B/0/1/0/all/0/1&quot;&gt;Benedetta Tondi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costanzo_A/0/1/0/all/0/1&quot;&gt;Andrea Costanzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barni_M/0/1/0/all/0/1&quot;&gt;Mauro Barni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.05109">
<title>Adjacent-Level Feature Cross-Fusion With 3-D CNN for Remote Sensing Image Change Detection. (arXiv:2302.05109v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.05109</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning-based change detection (CD) using remote sensing images has
received increasing attention in recent years. However, how to effectively
extract and fuse the deep features of bi-temporal images for improving the
accuracy of CD is still a challenge. To address that, a novel adjacent-level
feature fusion network with 3D convolution (named AFCF3D-Net) is proposed in
this article. First, through the inner fusion property of 3D convolution, we
design a new feature fusion way that can simultaneously extract and fuse the
feature information from bi-temporal images. Then, to alleviate the semantic
gap between low-level features and high-level features, we propose an
adjacent-level feature cross-fusion (AFCF) module to aggregate complementary
feature information between the adjacent levels. Furthermore, the full-scale
skip connection strategy is introduced to improve the capability of pixel-wise
prediction and the compactness of changed objects in the results. Finally, the
proposed AFCF3D-Net has been validated on the three challenging remote sensing
CD datasets: the Wuhan building dataset (WHU-CD), the LEVIR building dataset
(LEVIR-CD), and the Sun Yat-Sen University dataset (SYSU-CD). The results of
quantitative analysis and qualitative comparison demonstrate that the proposed
AFCF3D-Net achieves better performance compared to other state-of-the-art
methods. The code for this work is available at
https://github.com/wm-Githuber/AFCF3D-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yuanxin Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengmeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Liang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_G/0/1/0/all/0/1&quot;&gt;Guangyang Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jianwei Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yao Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00915">
<title>BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. (arXiv:2303.00915v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00915</link>
<description rdf:parseType="Literal">&lt;p&gt;Biomedical data is inherently multimodal, comprising physical measurements
and natural language narratives. A generalist biomedical AI model needs to
simultaneously process different modalities of data, including text and images.
Therefore, training an effective generalist biomedical model requires
high-quality multimodal data, such as parallel image-text pairs. Here, we
present PMC-15M, a novel dataset that is two orders of magnitude larger than
existing biomedical multimodal datasets such as MIMIC-CXR, and spans a diverse
range of biomedical image types. PMC-15M contains 15 million biomedical
image-text pairs collected from 4.4 million scientific articles. Based on
PMC-15M, we have pretrained BiomedCLIP, a multimodal foundation model, with
domain-specific adaptations tailored to biomedical vision-language processing.
We conducted extensive experiments and ablation studies on standard biomedical
imaging tasks from retrieval to classification to visual question-answering
(VQA). BiomedCLIP achieved new state-of-the-art results in a wide range of
standard datasets, substantially outperforming prior approaches. Intriguingly,
by large-scale pretraining on diverse biomedical image types, BiomedCLIP even
outperforms state-of-the-art radiology-specific models such as BioViL in
radiology-specific tasks such as RSNA pneumonia detection. In summary,
BiomedCLIP is a fully open-access foundation model that achieves
state-of-the-art performance on various biomedical tasks, paving the way for
transformative multimodal biomedical discovery and applications. We release our
models at https://aka.ms/biomedclip to facilitate future research in multimodal
biomedical AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yanbo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1&quot;&gt;Naoto Usuyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hanwen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagga_J/0/1/0/all/0/1&quot;&gt;Jaspreet Bagga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1&quot;&gt;Robert Tinn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preston_S/0/1/0/all/0/1&quot;&gt;Sam Preston&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_R/0/1/0/all/0/1&quot;&gt;Rajesh Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1&quot;&gt;Mu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valluri_N/0/1/0/all/0/1&quot;&gt;Naveen Valluri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1&quot;&gt;Cliff Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tupini_A/0/1/0/all/0/1&quot;&gt;Andrea Tupini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazzola_M/0/1/0/all/0/1&quot;&gt;Matt Mazzola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1&quot;&gt;Swadheen Shukla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liden_L/0/1/0/all/0/1&quot;&gt;Lars Liden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1&quot;&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1&quot;&gt;Tristan Naumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1&quot;&gt;Hoifung Poon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06920">
<title>Pixel-wise Gradient Uncertainty for Convolutional Neural Networks applied to Out-of-Distribution Segmentation. (arXiv:2303.06920v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06920</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, deep neural networks have defined the state-of-the-art in
semantic segmentation where their predictions are constrained to a predefined
set of semantic classes. They are to be deployed in applications such as
automated driving, although their categorically confined expressive power runs
contrary to such open world scenarios. Thus, the detection and segmentation of
objects from outside their predefined semantic space, i.e., out-of-distribution
(OoD) objects, is of highest interest. Since uncertainty estimation methods
like softmax entropy or Bayesian models are sensitive to erroneous predictions,
these methods are a natural baseline for OoD detection. Here, we present a
method for obtaining uncertainty scores from pixel-wise loss gradients which
can be computed efficiently during inference. Our approach is simple to
implement for a large class of models, does not require any additional training
or auxiliary data and can be readily used on pre-trained segmentation models.
Our experiments show the ability of our method to identify wrong pixel
classifications and to estimate prediction quality at negligible computational
overhead. In particular, we observe superior performance in terms of OoD
segmentation to comparable baselines on the SegmentMeIfYouCan benchmark,
clearly outperforming other methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maag_K/0/1/0/all/0/1&quot;&gt;Kira Maag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedlinger_T/0/1/0/all/0/1&quot;&gt;Tobias Riedlinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04351">
<title>Evaluate Geometry of Radiance Fields with Low-frequency Color Prior. (arXiv:2304.04351v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04351</link>
<description rdf:parseType="Literal">&lt;p&gt;A radiance field is an effective representation of 3D scenes, which has been
widely adopted in novel-view synthesis and 3D reconstruction. It is still an
open and challenging problem to evaluate the geometry, i.e., the density field,
as the ground-truth is almost impossible to obtain. One alternative indirect
solution is to transform the density field into a point-cloud and compute its
Chamfer Distance with the scanned ground-truth. However, many widely-used
datasets have no point-cloud ground-truth since the scanning process along with
the equipment is expensive and complicated. To this end, we propose a novel
metric, named Inverse Mean Residual Color (IMRC), which can evaluate the
geometry only with the observation images. Our key insight is that the better
the geometry, the lower-frequency the computed color field. From this insight,
given a reconstructed density field and observation images, we design a
closed-form method to approximate the color field with low-frequency spherical
harmonics, and compute the inverse mean residual color. Then the higher the
IMRC, the better the geometry. Qualitative and quantitative experimental
results verify the effectiveness of our proposed IMRC metric. We also benchmark
several state-of-the-art methods using IMRC to promote future related research.
Our code is available at https://github.com/qihangGH/IMRC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1&quot;&gt;Qihang Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yafei Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Keqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Huaiyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_G/0/1/0/all/0/1&quot;&gt;Gang Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1&quot;&gt;Liefeng Bo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07486">
<title>Region-Enhanced Feature Learning for Scene Semantic Segmentation. (arXiv:2304.07486v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07486</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation in complex scenes relies not only on object appearance
but also on object location and the surrounding environment. Nonetheless, it is
difficult to model long-range context in the format of pairwise point
correlations due to the huge computational cost for large-scale point clouds.
In this paper, we propose using regions as the intermediate representation of
point clouds instead of fine-grained points or voxels to reduce the
computational burden. We introduce a novel Region-Enhanced Feature Learning
Network (REFL-Net) that leverages region correlations to enhance point feature
learning. We design a region-based feature enhancement (RFE) module, which
consists of a Semantic-Spatial Region Extraction stage and a Region Dependency
Modeling stage. In the first stage, the input points are grouped into a set of
regions based on their semantic and spatial proximity. In the second stage, we
explore inter-region semantic and spatial relationships by employing a
self-attention block on region features and then fuse point features with the
region features to obtain more discriminative representations. Our proposed RFE
module is plug-and-play and can be integrated with common semantic segmentation
backbones. We conduct extensive experiments on ScanNetV2 and S3DIS datasets and
evaluate our RFE module with different segmentation backbones. Our REFL-Net
achieves 1.8% mIoU gain on ScanNetV2 and 1.7% mIoU gain on S3DIS with
negligible computational cost compared with backbone models. Both quantitative
and qualitative results show the powerful long-range context modeling ability
and strong generalization ability of our REFL-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1&quot;&gt;Xin Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaoqun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xuejin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10829">
<title>Deep Attention Unet: A Network Model with Global Feature Perception Ability. (arXiv:2304.10829v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10829</link>
<description rdf:parseType="Literal">&lt;p&gt;Remote sensing image segmentation is a specific task of remote sensing image
interpretation. A good remote sensing image segmentation algorithm can provide
guidance for environmental protection, agricultural production, and urban
construction. This paper proposes a new type of UNet image segmentation
algorithm based on channel self attention mechanism and residual connection
called . In my experiment, the new network model improved mIOU by 2.48%
compared to traditional UNet on the FoodNet dataset. The image segmentation
algorithm proposed in this article enhances the internal connections between
different items in the image, thus achieving better segmentation results for
remote sensing images with occlusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiacheng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14660">
<title>Segment Anything Model for Medical Images?. (arXiv:2304.14660v7 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14660</link>
<description rdf:parseType="Literal">&lt;p&gt;The Segment Anything Model (SAM) is the first foundation model for general
image segmentation. It has achieved impressive results on various natural image
segmentation tasks. However, medical image segmentation (MIS) is more
challenging because of the complex modalities, fine anatomical structures,
uncertain and complex object boundaries, and wide-range object scales. To fully
validate SAM&apos;s performance on medical data, we collected and sorted 53
open-source datasets and built a large medical segmentation dataset with 18
modalities, 84 objects, 125 object-modality paired targets, 1050K 2D images,
and 6033K masks. We comprehensively analyzed different models and strategies on
the so-called COSMOS 1050K dataset. Our findings mainly include the following:
1) SAM showed remarkable performance in some specific objects but was unstable,
imperfect, or even totally failed in other situations. 2) SAM with the large
ViT-H showed better overall performance than that with the small ViT-B. 3) SAM
performed better with manual hints, especially box, than the Everything mode.
4) SAM could help human annotation with high labeling quality and less time. 5)
SAM was sensitive to the randomness in the center point and tight box prompts,
and may suffer from a serious performance drop. 6) SAM performed better than
interactive methods with one or a few points, but will be outpaced as the
number of points increases. 7) SAM&apos;s performance correlated to different
factors, including boundary complexity, intensity differences, etc. 8)
Finetuning the SAM on specific medical tasks could improve its average DICE
performance by 4.39% and 6.68% for ViT-B and ViT-H, respectively. We hope that
this comprehensive report can help researchers explore the potential of SAM
applications in MIS, and guide how to appropriately use and develop SAM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Han Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_A/0/1/0/all/0/1&quot;&gt;Ao Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xinrui Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Rusi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Junxuan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiongquan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaoyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sijing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chi_H/0/1/0/all/0/1&quot;&gt;Haozhe Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xindi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yue_K/0/1/0/all/0/1&quot;&gt;Kejuan Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Grau_V/0/1/0/all/0/1&quot;&gt;Vicente Grau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fan_D/0/1/0/all/0/1&quot;&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_F/0/1/0/all/0/1&quot;&gt;Fajin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ni_D/0/1/0/all/0/1&quot;&gt;Dong Ni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07895">
<title>On the Hidden Mystery of OCR in Large Multimodal Models. (arXiv:2305.07895v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07895</link>
<description rdf:parseType="Literal">&lt;p&gt;Large models have recently played a dominant role in natural language
processing and multimodal vision-language learning. However, their
effectiveness in text-related visual tasks remains relatively unexplored. In
this paper, we conducted a comprehensive evaluation of Large Multimodal Models,
such as GPT4V and Gemini, in various text-related visual tasks including Text
Recognition, Scene Text-Centric Visual Question Answering (VQA),
Document-Oriented VQA, Key Information Extraction (KIE), and Handwritten
Mathematical Expression Recognition (HMER). To facilitate the assessment of
Optical Character Recognition (OCR) capabilities in Large Multimodal Models, we
propose OCRBench, a comprehensive evaluation benchmark.Our study encompasses 29
datasets, making it the most comprehensive OCR evaluation benchmark available.
Furthermore, our study reveals both the strengths and weaknesses of these
models, particularly in handling multilingual text, handwritten text,
non-semantic text, and mathematical expression recognition. Most importantly,
the baseline results showcased in this study could provide a foundational
framework for the conception and assessment of innovative strategies targeted
at enhancing zero-shot multimodal techniques. The evaluation pipeline and
benchmark are available at https://github.com/Yuliang-Liu/MultimodalOCR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Biao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xucheng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cheng-lin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1&quot;&gt;Lianwen Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xiang Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14243">
<title>Training Transitive and Commutative Multimodal Transformers with LoReTTa. (arXiv:2305.14243v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14243</link>
<description rdf:parseType="Literal">&lt;p&gt;Training multimodal foundation models is challenging due to the limited
availability of multimodal datasets. While many public datasets pair images
with text, few combine images with audio or text with audio. Even rarer are
datasets that align all three modalities at once. Critical domains such as
healthcare, infrastructure, or transportation are particularly affected by
missing modalities. This makes it difficult to integrate all modalities into a
large pre-trained neural network that can be used out-of-the-box or fine-tuned
for different downstream tasks. We introduce LoReTTa (Linking mOdalities with a
tRansitive and commutativE pre-Training sTrAtegy) to address this understudied
problem. Our self-supervised framework unifies causal modeling and masked
modeling with the rules of commutativity and transitivity. This allows us to
transition within and between modalities. As a result, our pre-trained models
are better at exploring the true underlying joint probability distribution.
Given a dataset containing only the disjoint combinations (A, B) and (B, C),
LoReTTa can model the relation A &amp;lt;-&amp;gt; C with A &amp;lt;-&amp;gt; B &amp;lt;-&amp;gt; C. In particular, we
show that a transformer pre-trained with LoReTTa can handle any mixture of
modalities at inference time, including the never-seen pair (A, C) and the
triplet (A, B, C). We extensively evaluate our approach on a synthetic,
medical, and reinforcement learning dataset. Across different domains, our
universal multimodal transformer consistently outperforms strong baselines such
as GPT, BERT, and CLIP on tasks involving the missing modality tuple.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1&quot;&gt;Manuel Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cid_Y/0/1/0/all/0/1&quot;&gt;Yashin Dicente Cid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lahiani_A/0/1/0/all/0/1&quot;&gt;Amal Lahiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theis_F/0/1/0/all/0/1&quot;&gt;Fabian J. Theis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1&quot;&gt;Tingying Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klaiman_E/0/1/0/all/0/1&quot;&gt;Eldad Klaiman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15583">
<title>Alleviating Exposure Bias in Diffusion Models through Sampling with Shifted Time Steps. (arXiv:2305.15583v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15583</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion Probabilistic Models (DPM) have shown remarkable efficacy in the
synthesis of high-quality images. However, their inference process
characteristically requires numerous, potentially hundreds, of iterative steps,
which could exaggerate the problem of exposure bias due to the training and
inference discrepancy. Previous work has attempted to mitigate this issue by
perturbing inputs during training, which consequently mandates the retraining
of the DPM. In this work, we conduct a systematic study of exposure bias in DPM
and, intriguingly, we find that the exposure bias could be alleviated with a
novel sampling method that we propose, without retraining the model. We
empirically and theoretically show that, during inference, for each backward
time step $t$ and corresponding state $\hat{x}_t$, there might exist another
time step $t_s$ which exhibits superior coupling with $\hat{x}_t$. Based on
this finding, we introduce a sampling method named Time-Shift Sampler. Our
framework can be seamlessly integrated to existing sampling algorithms, such as
DDPM, DDIM and other high-order solvers, inducing merely minimal additional
computations. Experimental results show our method brings significant and
consistent improvements in FID scores on different datasets and sampling
methods. For example, integrating Time-Shift Sampler to F-PNDM yields a
FID=3.88, achieving 44.49\% improvements as compared to F-PNDM, on CIFAR-10
with 10 sampling steps, which is more performant than the vanilla DDIM with 100
sampling steps. Our code is available at https://github.com/Mingxiao-Li/TS-DPM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mingxiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_T/0/1/0/all/0/1&quot;&gt;Tingyu Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_R/0/1/0/all/0/1&quot;&gt;Ruicong Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1&quot;&gt;Marie-Francine Moens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16494">
<title>Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability. (arXiv:2305.16494v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16494</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks are known to be susceptible to adversarial samples: small
variations of natural examples crafted to deliberately mislead the models.
While they can be easily generated using gradient-based techniques in digital
and physical scenarios, they often differ greatly from the actual data
distribution of natural images, resulting in a trade-off between strength and
stealthiness. In this paper, we propose a novel framework dubbed
Diffusion-Based Projected Gradient Descent (Diff-PGD) for generating realistic
adversarial samples. By exploiting a gradient guided by a diffusion model,
Diff-PGD ensures that adversarial samples remain close to the original data
distribution while maintaining their effectiveness. Moreover, our framework can
be easily customized for specific tasks such as digital attacks, physical-world
attacks, and style-based attacks. Compared with existing methods for generating
natural-style adversarial samples, our framework enables the separation of
optimizing adversarial loss from other surrogate losses (e.g.,
content/smoothness/style loss), making it more stable and controllable.
Finally, we demonstrate that the samples generated using Diff-PGD have better
transferability and anti-purification power than traditional gradient-based
methods. Code will be released in https://github.com/xavihart/Diff-PGD
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1&quot;&gt;Haotian Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araujo_A/0/1/0/all/0/1&quot;&gt;Alexandre Araujo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Bin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yongxin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18171">
<title>Improved Probabilistic Image-Text Representations. (arXiv:2305.18171v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18171</link>
<description rdf:parseType="Literal">&lt;p&gt;Image-Text Matching (ITM) task, a fundamental vision-language (VL) task,
suffers from the inherent ambiguity arising from multiplicity and imperfect
annotations. Deterministic functions are not sufficiently powerful to capture
ambiguity, prompting the exploration of probabilistic embeddings to tackle the
challenge. However, the existing probabilistic ITM approach encounters two key
shortcomings; the burden of heavy computations due to the Monte Carlo
approximation, and the loss saturation issue in the face of abundant false
negatives. To overcome the issues, this paper presents an improved
Probabilistic Cross-Modal Embeddings (named PCME++) by introducing a new
probabilistic distance with a closed-form solution. In addition, two
optimization techniques are proposed to enhance PCME++ further: first, the
incorporation of pseudo-positives to prevent the loss saturation problem under
massive false negatives; second, mixed sample data augmentation for
probabilistic matching. Experimental results on MS-COCO Caption and two
extended benchmarks, CxC and ECCV Caption, demonstrate the effectiveness of
PCME++ compared to state-of-the-art ITM methods. The robustness of PCME++ is
also evaluated under noisy image-text correspondences. In addition, the
potential applicability of PCME++ in automatic prompt tuning for zero-shot
classification is shown. The code is available at
https://github.com/naver-ai/pcmepp.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1&quot;&gt;Sanghyuk Chun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19858">
<title>Enhancing image quality prediction with self-supervised visual masking. (arXiv:2305.19858v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19858</link>
<description rdf:parseType="Literal">&lt;p&gt;Full-reference image quality metrics (FR-IQMs) aim to measure the visual
differences between a pair of reference and distorted images, with the goal of
accurately predicting human judgments. However, existing FR-IQMs, including
traditional ones like PSNR and SSIM and even perceptual ones such as HDR-VDP,
LPIPS, and DISTS, still fall short in capturing the complexities and nuances of
human perception. In this work, rather than devising a novel IQM model, we seek
to improve upon the perceptual quality of existing FR-IQM methods. We achieve
this by considering visual masking, an important characteristic of the human
visual system that changes its sensitivity to distortions as a function of
local image content. Specifically, for a given FR-IQM metric, we propose to
predict a visual masking model that modulates reference and distorted images in
a way that penalizes the visual errors based on their visibility. Since the
ground truth visual masks are difficult to obtain, we demonstrate how they can
be derived in a self-supervised manner solely based on mean opinion scores
(MOS) collected from an FR-IQM dataset. Our approach results in enhanced FR-IQM
metrics that are more in line with human prediction both visually and
quantitatively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cogalan_U/0/1/0/all/0/1&quot;&gt;U&amp;#x11f;ur &amp;#xc7;o&amp;#x11f;alan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bemana_M/0/1/0/all/0/1&quot;&gt;Mojtaba Bemana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seidel_H/0/1/0/all/0/1&quot;&gt;Hans-Peter Seidel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myszkowski_K/0/1/0/all/0/1&quot;&gt;Karol Myszkowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08834">
<title>ScrollTimes: Tracing the Provenance of Paintings as a Window into History. (arXiv:2306.08834v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08834</link>
<description rdf:parseType="Literal">&lt;p&gt;The study of cultural artifact provenance, tracing ownership and
preservation, holds significant importance in archaeology and art history.
Modern technology has advanced this field, yet challenges persist, including
recognizing evidence from diverse sources, integrating sociocultural context,
and enhancing interactive automation for comprehensive provenance analysis. In
collaboration with art historians, we examined the handscroll, a traditional
Chinese painting form that provides a rich source of historical data and a
unique opportunity to explore history through cultural artifacts. We present a
three-tiered methodology encompassing artifact, contextual, and provenance
levels, designed to create a &quot;Biography&quot; for handscroll. Our approach
incorporates the application of image processing techniques and language models
to extract, validate, and augment elements within handscroll using various
cultural heritage databases. To facilitate efficient analysis of non-contiguous
extracted elements, we have developed a distinctive layout. Additionally, we
introduce ScrollTimes, a visual analysis system tailored to support the
three-tiered analysis of handscroll, allowing art historians to interactively
create biographies tailored to their interests. Validated through case studies
and expert interviews, our approach offers a window into history, fostering a
holistic understanding of handscroll provenance and historical significance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kam_Kwai_W/0/1/0/all/0/1&quot;&gt;Wong Kam-Kwai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yitian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_A/0/1/0/all/0/1&quot;&gt;Ailing Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Luwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jian-Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lechao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1&quot;&gt;Huamin Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12048">
<title>Online Unsupervised Video Object Segmentation via Contrastive Motion Clustering. (arXiv:2306.12048v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12048</link>
<description rdf:parseType="Literal">&lt;p&gt;Online unsupervised video object segmentation (UVOS) uses the previous frames
as its input to automatically separate the primary object(s) from a streaming
video without using any further manual annotation. A major challenge is that
the model has no access to the future and must rely solely on the history,
i.e., the segmentation mask is predicted from the current frame as soon as it
is captured. In this work, a novel contrastive motion clustering algorithm with
an optical flow as its input is proposed for the online UVOS by exploiting the
common fate principle that visual elements tend to be perceived as a group if
they possess the same motion pattern. We build a simple and effective
auto-encoder to iteratively summarize non-learnable prototypical bases for the
motion pattern, while the bases in turn help learn the representation of the
embedding network. Further, a contrastive learning strategy based on a boundary
prior is developed to improve foreground and background feature discrimination
in the representation learning stage. The proposed algorithm can be optimized
on arbitrarily-scale data i.e., frame, clip, dataset) and performed in an
online fashion. Experiments on $\textit{DAVIS}_{\textit{16}}$, $\textit{FBMS}$,
and $\textit{SegTrackV2}$ datasets show that the accuracy of our method
surpasses the previous state-of-the-art (SoTA) online UVOS method by a margin
of 0.8%, 2.9%, and 1.1%, respectively. Furthermore, by using an online deep
subspace clustering to tackle the motion grouping, our method is able to
achieve higher accuracy at $3\times$ faster inference time compared to SoTA
online UVOS method, and making a good trade-off between effectiveness and
efficiency. Our code is available at https://github.com/xilin1991/ClusterNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_L/0/1/0/all/0/1&quot;&gt;Lin Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weihai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xingming Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhengguo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00519">
<title>Image Background Serves as Good Proxy for Out-of-distribution Data. (arXiv:2307.00519v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00519</link>
<description rdf:parseType="Literal">&lt;p&gt;Out-of-distribution (OOD) detection empowers the model trained on the closed
image set to identify unknown data in the open world. Though many prior
techniques have yielded considerable improvements in this research direction,
two crucial obstacles still remain. Firstly, a unified perspective has yet to
be presented to view the developed arts with individual designs, which is vital
for providing insights into future work. Secondly, we expect sufficient natural
OOD supervision to promote the generation of compact boundaries between the
in-distribution (ID) and OOD data without collecting explicit OOD samples. To
tackle these issues, we propose a general probabilistic framework to interpret
many existing methods and an OOD-data-free model, namely
\textbf{S}elf-supervised \textbf{S}ampling for \textbf{O}OD \textbf{D}etection
(SSOD). SSOD efficiently exploits natural OOD signals from the ID data based on
the local property of convolution. With these supervisions, it jointly
optimizes the OOD detection and conventional ID classification in an end-to-end
manner. Extensive experiments reveal that SSOD establishes competitive
state-of-the-art performance on many large-scale benchmarks, outperforming the
best previous method by a large margin, \eg, reporting \textbf{-6.28\%} FPR95
and \textbf{+0.77\%} AUROC on ImageNet, \textbf{-19.01\%} FPR95 and
\textbf{+3.04\%} AUROC on CIFAR-10, and top-ranked performance on hard OOD
datasets, \ie, ImageNet-O and OpenImage-O.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1&quot;&gt;Sen Pei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12143">
<title>A Probabilistic Fluctuation based Membership Inference Attack for Diffusion Models. (arXiv:2308.12143v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12143</link>
<description rdf:parseType="Literal">&lt;p&gt;Membership Inference Attack (MIA) identifies whether a record exists in a
machine learning model&apos;s training set by querying the model. MIAs on the
classic classification models have been well-studied, and recent works have
started to explore how to transplant MIA onto generative models. Our
investigation indicates that existing MIAs designed for generative models
mainly depend on the overfitting in target models. However, overfitting can be
avoided by employing various regularization techniques, whereas existing MIAs
demonstrate poor performance in practice. Unlike overfitting, memorization is
essential for deep learning models to attain optimal performance, making it a
more prevalent phenomenon. Memorization in generative models leads to an
increasing trend in the probability distribution of generating records around
the member record. Therefore, we propose a Probabilistic Fluctuation Assessing
Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by
detecting these trends via analyzing the overall probabilistic fluctuations
around given records. We conduct extensive experiments across multiple
generative models and datasets, which demonstrate PFAMI can improve the attack
success rate (ASR) by about 27.9% when compared with the best baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1&quot;&gt;Wenjie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huandong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chen Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guanghua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1&quot;&gt;Tao Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16518">
<title>MS23D: : A 3D Object Detection Method Using Multi-Scale Semantic Feature Points to Construct 3D Feature Layer. (arXiv:2308.16518v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16518</link>
<description rdf:parseType="Literal">&lt;p&gt;LiDAR point clouds can effectively depict the motion and posture of objects
in three-dimensional space. Many studies accomplish the 3D object detection by
voxelizing point clouds. However, in autonomous driving scenarios, the sparsity
and hollowness of point clouds create some difficulties for voxel-based
methods. The sparsity of point clouds makes it challenging to describe the
geometric features of objects. The hollowness of point clouds poses
difficulties for the aggregation of 3D features. We propose a two-stage 3D
object detection framework, called MS23D. (1) We propose a method using voxel
feature points from multi-branch to construct the 3D feature layer. Using voxel
feature points from different branches, we construct a relatively compact 3D
feature layer with rich semantic features. Additionally, we propose a
distance-weighted sampling method, reducing the loss of foreground points
caused by downsampling and allowing the 3D feature layer to retain more
foreground points. (2) In response to the hollowness of point clouds, we
predict the offsets between deep-level feature points and the object&apos;s
centroid, making them as close as possible to the object&apos;s centroid. This
enables the aggregation of these feature points with abundant semantic
features. For feature points from shallow-level, we retain them on the object&apos;s
surface to describe the geometric features of the object. To validate our
approach, we evaluated its effectiveness on both the KITTI and ONCE datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1&quot;&gt;Yongxin Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_A/0/1/0/all/0/1&quot;&gt;Aihong Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Binrui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1&quot;&gt;Tianhong Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhetao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yiyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaxin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.04001">
<title>MMSFormer: Multimodal Transformer for Material and Semantic Segmentation. (arXiv:2309.04001v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.04001</link>
<description rdf:parseType="Literal">&lt;p&gt;Leveraging information across diverse modalities is known to enhance
performance on multimodal segmentation tasks. However, effectively fusing
information from different modalities remains challenging due to the unique
characteristics of each modality. In this paper, we propose a novel fusion
strategy that can effectively fuse information from different modality
combinations. We also propose a new model named Multi-Modal Segmentation
TransFormer (MMSFormer) that incorporates the proposed fusion strategy to
perform multimodal material and semantic segmentation tasks. MMSFormer
outperforms current state-of-the-art models on three different datasets. As we
begin with only one input modality, performance improves progressively as
additional modalities are incorporated, showcasing the effectiveness of the
fusion block in combining useful information from diverse input modalities.
Ablation studies show that different modules in the fusion block are crucial
for overall model performance. Furthermore, our ablation studies also highlight
the capacity of different input modalities to improve performance in the
identification of different types of materials. The code and pretrained models
will be made available at https://github.com/csiplab/MMSFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reza_M/0/1/0/all/0/1&quot;&gt;Md Kaykobad Reza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prater_Bennette_A/0/1/0/all/0/1&quot;&gt;Ashley Prater-Bennette&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1&quot;&gt;M. Salman Asif&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08480">
<title>PoseFix: Correcting 3D Human Poses with Natural Language. (arXiv:2309.08480v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08480</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatically producing instructions to modify one&apos;s posture could open the
door to endless applications, such as personalized coaching and in-home
physical therapy. Tackling the reverse problem (i.e., refining a 3D pose based
on some natural language feedback) could help for assisted 3D character
animation or robot teaching, for instance. Although a few recent works explore
the connections between natural language and 3D human pose, none focus on
describing 3D body pose differences. In this paper, we tackle the problem of
correcting 3D human poses with natural language. To this end, we introduce the
PoseFix dataset, which consists of several thousand paired 3D poses and their
corresponding text feedback, that describe how the source pose needs to be
modified to obtain the target pose. We demonstrate the potential of this
dataset on two tasks: (1) text-based pose editing, that aims at generating
corrected 3D body poses given a query pose and a text modifier; and (2)
correctional text generation, where instructions are generated based on the
differences between two body poses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delmas_G/0/1/0/all/0/1&quot;&gt;Ginger Delmas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinzaepfel_P/0/1/0/all/0/1&quot;&gt;Philippe Weinzaepfel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1&quot;&gt;Francesc Moreno-Noguer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rogez_G/0/1/0/all/0/1&quot;&gt;Gr&amp;#xe9;gory Rogez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01812">
<title>PPT: Token Pruning and Pooling for Efficient Vision Transformers. (arXiv:2310.01812v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01812</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) have emerged as powerful models in the field of
computer vision, delivering superior performance across various vision tasks.
However, the high computational complexity poses a significant barrier to their
practical applications in real-world scenarios. Motivated by the fact that not
all tokens contribute equally to the final predictions and fewer tokens bring
less computational cost, reducing redundant tokens has become a prevailing
paradigm for accelerating vision transformers. However, we argue that it is not
optimal to either only reduce inattentive redundancy by token pruning, or only
reduce duplicative redundancy by token merging. To this end, in this paper we
propose a novel acceleration framework, namely token Pruning &amp;amp; Pooling
Transformers (PPT), to adaptively tackle these two types of redundancy in
different layers. By heuristically integrating both token pruning and token
pooling techniques in ViTs without additional trainable parameters, PPT
effectively reduces the model complexity while maintaining its predictive
accuracy. For example, PPT reduces over 37% FLOPs and improves the throughput
by over 45% for DeiT-S without any accuracy drop on the ImageNet dataset. The
code is available at https://github.com/xjwu1024/PPT and
https://github.com/mindspore-lab/models/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xinjian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1&quot;&gt;Fanhu Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiudong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinghao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04741">
<title>Balancing stability and plasticity in continual learning: the readout-decomposition of activation change (RDAC) framework. (arXiv:2310.04741v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04741</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning (CL) algorithms strive to acquire new knowledge while
preserving prior information. However, this stability-plasticity trade-off
remains a central challenge. This paper introduces a framework that dissects
this trade-off, offering valuable insights into CL algorithms. The
Readout-Decomposition of Activation Change (RDAC) framework first addresses the
stability-plasticity dilemma and its relation to catastrophic forgetting. It
relates learning-induced activation changes in the range of prior readouts to
the degree of stability and changes in the null space to the degree of
plasticity. In deep non-linear networks tackling split-CIFAR-110 tasks, the
framework clarifies the stability-plasticity trade-offs of the popular
regularization algorithms Synaptic intelligence (SI), Elastic-weight
consolidation (EWC), and learning without Forgetting (LwF), and replay-based
algorithms Gradient episodic memory (GEM), and data replay. GEM and data replay
preserved stability and plasticity, while SI, EWC, and LwF traded off
plasticity for stability. The inability of the regularization algorithms to
maintain plasticity was linked to them restricting the change of activations in
the null space of the prior readout. Additionally, for one-hidden-layer linear
neural networks, we derived a gradient decomposition algorithm to restrict
activation change only in the range of the prior readouts, to maintain high
stability while not further sacrificing plasticity. Results demonstrate that
the algorithm maintained stability without significant plasticity loss. The
RDAC framework informs the behavior of existing CL algorithms and paves the way
for novel CL approaches. Finally, it sheds light on the connection between
learning-induced activation/representation changes and the stability-plasticity
dilemma, also offering insights into representational drift in biological
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anthes_D/0/1/0/all/0/1&quot;&gt;Daniel Anthes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thorat_S/0/1/0/all/0/1&quot;&gt;Sushrut Thorat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konig_P/0/1/0/all/0/1&quot;&gt;Peter K&amp;#xf6;nig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kietzmann_T/0/1/0/all/0/1&quot;&gt;Tim C. Kietzmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05055">
<title>FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in Medical Image Analysis. (arXiv:2310.05055v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05055</link>
<description rdf:parseType="Literal">&lt;p&gt;Training models with robust group fairness properties is crucial in ethically
sensitive application areas such as medical diagnosis. Despite the growing body
of work aiming to minimise demographic bias in AI, this problem remains
challenging. A key reason for this challenge is the fairness generalisation
gap: High-capacity deep learning models can fit all training data nearly
perfectly, and thus also exhibit perfect fairness during training. In this
case, bias emerges only during testing when generalisation performance differs
across subgroups. This motivates us to take a bi-level optimisation perspective
on fair learning: Optimising the learning strategy based on validation
fairness. Specifically, we consider the highly effective workflow of adapting
pre-trained models to downstream medical imaging tasks using
parameter-efficient fine-tuning (PEFT) techniques. There is a trade-off between
updating more parameters, enabling a better fit to the task of interest vs.
fewer parameters, potentially reducing the generalisation gap. To manage this
tradeoff, we propose FairTune, a framework to optimise the choice of PEFT
parameters with respect to fairness. We demonstrate empirically that FairTune
leads to improved fairness on a range of medical imaging datasets. The code is
available at https://github.com/Raman1121/FairTune
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutt_R/0/1/0/all/0/1&quot;&gt;Raman Dutt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1&quot;&gt;Ondrej Bohdal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1&quot;&gt;Sotirios A. Tsaftaris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1&quot;&gt;Timothy Hospedales&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06234">
<title>Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing. (arXiv:2310.06234v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06234</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of high-capacity pre-trained models has revolutionized
problem-solving in computer vision, shifting the focus from training
task-specific models to adapting pre-trained models. Consequently, effectively
adapting large pre-trained models to downstream tasks in an efficient manner
has become a prominent research area. Existing solutions primarily concentrate
on designing lightweight adapters and their interaction with pre-trained
models, with the goal of minimizing the number of parameters requiring updates.
In this study, we propose a novel Adapter Re-Composing (ARC) strategy that
addresses efficient pre-trained model adaptation from a fresh perspective. Our
approach considers the reusability of adaptation parameters and introduces a
parameter-sharing scheme. Specifically, we leverage symmetric
down-/up-projections to construct bottleneck operations, which are shared
across layers. By learning low-dimensional re-scaling coefficients, we can
effectively re-compose layer-adaptive adapters. This parameter-sharing strategy
in adapter design allows us to significantly reduce the number of new
parameters while maintaining satisfactory performance, thereby offering a
promising approach to compress the adaptation cost. We conduct experiments on
24 downstream image classification tasks using various Vision Transformer
variants to evaluate our method. The results demonstrate that our approach
achieves compelling transfer learning performance with a reduced parameter
count. Our code is available at
\href{https://github.com/DavidYanAnDe/ARC}{https://github.com/DavidYanAnDe/ARC}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1&quot;&gt;Wei Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1&quot;&gt;Dawei Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhijun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10224">
<title>Generalizing Medical Image Representations via Quaternion Wavelet Networks. (arXiv:2310.10224v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10224</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural network generalizability is becoming a broad research field due to the
increasing availability of datasets from different sources and for various
tasks. This issue is even wider when processing medical data, where a lack of
methodological standards causes large variations being provided by different
imaging centers or acquired with various devices and cofactors. To overcome
these limitations, we introduce a novel, generalizable, data- and task-agnostic
framework able to extract salient features from medical images. The proposed
quaternion wavelet network (QUAVE) can be easily integrated with any
pre-existing medical image analysis or synthesis task, and it can be involved
with real, quaternion, or hypercomplex-valued models, generalizing their
adoption to single-channel data. QUAVE first extracts different sub-bands
through the quaternion wavelet transform, resulting in both
low-frequency/approximation bands and high-frequency/fine-grained features.
Then, it weighs the most representative set of sub-bands to be involved as
input to any other neural model for image processing, replacing standard data
samples. We conduct an extensive experimental evaluation comprising different
datasets, diverse image analysis, and synthesis tasks including reconstruction,
segmentation, and modality translation. We also evaluate QUAVE in combination
with both real and quaternion-valued models. Results demonstrate the
effectiveness and the generalizability of the proposed framework that improves
network performance while being flexible to be adopted in manifold scenarios
and robust to domain shifts. The full code is available at:
https://github.com/ispamm/QWT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sigillo_L/0/1/0/all/0/1&quot;&gt;Luigi Sigillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Grassucci_E/0/1/0/all/0/1&quot;&gt;Eleonora Grassucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Uncini_A/0/1/0/all/0/1&quot;&gt;Aurelio Uncini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Comminiello_D/0/1/0/all/0/1&quot;&gt;Danilo Comminiello&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16044">
<title>Stanford-ORB: A Real-World 3D Object Inverse Rendering Benchmark. (arXiv:2310.16044v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16044</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Stanford-ORB, a new real-world 3D Object inverse Rendering
Benchmark. Recent advances in inverse rendering have enabled a wide range of
real-world applications in 3D content generation, moving rapidly from research
and commercial use cases to consumer devices. While the results continue to
improve, there is no real-world benchmark that can quantitatively assess and
compare the performance of various inverse rendering methods. Existing
real-world datasets typically only consist of the shape and multi-view images
of objects, which are not sufficient for evaluating the quality of material
recovery and object relighting. Methods capable of recovering material and
lighting often resort to synthetic data for quantitative evaluation, which on
the other hand does not guarantee generalization to complex real-world
environments. We introduce a new dataset of real-world objects captured under a
variety of natural scenes with ground-truth 3D scans, multi-view images, and
environment lighting. Using this dataset, we establish the first comprehensive
real-world evaluation benchmark for object inverse rendering tasks from
in-the-wild scenes, and compare the performance of various existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1&quot;&gt;Zhengfei Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunzhi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hong-Xing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwala_S/0/1/0/all/0/1&quot;&gt;Samir Agarwala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shangzhe Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01475">
<title>Patch-Based Deep Unsupervised Image Segmentation using Graph Cuts. (arXiv:2311.01475v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01475</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised image segmentation aims at grouping different semantic patterns
in an image without the use of human annotation. Similarly, image clustering
searches for groupings of images based on their semantic content without
supervision. Classically, both problems have captivated researchers as they
drew from sound mathematical concepts to produce concrete applications. With
the emergence of deep learning, the scientific community turned its attention
to complex neural network-based solvers that achieved impressive results in
those domains but rarely leveraged the advances made by classical methods. In
this work, we propose a patch-based unsupervised image segmentation strategy
that bridges advances in unsupervised feature extraction from deep clustering
methods with the algorithmic help of classical graph-based methods. We show
that a simple convolutional neural network, trained to classify image patches
and iteratively regularized using graph cuts, naturally leads to a
state-of-the-art fully-convolutional unsupervised pixel-level segmenter.
Furthermore, we demonstrate that this is the ideal setting for leveraging the
patch-level pairwise features generated by vision transformer models. Our
results on real image data demonstrate the effectiveness of our proposed
methodology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasserman_I/0/1/0/all/0/1&quot;&gt;Isaac Wasserman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neto_J/0/1/0/all/0/1&quot;&gt;Jeova Farias Sales Rocha Neto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02992">
<title>NEURO HAND: A weakly supervised Hierarchical Attention Network for interpretable neuroimaging abnormality Detection. (arXiv:2311.02992v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02992</link>
<description rdf:parseType="Literal">&lt;p&gt;Clinical neuroimaging data is naturally hierarchical. Different magnetic
resonance imaging (MRI) sequences within a series, different slices covering
the head, and different regions within each slice all confer different
information. In this work we present a hierarchical attention network for
abnormality detection using MRI scans obtained in a clinical hospital setting.
The proposed network is suitable for non-volumetric data (i.e. stacks of
high-resolution MRI slices), and can be trained from binary examination-level
labels. We show that this hierarchical approach leads to improved
classification, while providing interpretability through either coarse inter-
and intra-slice abnormality localisation, or giving importance scores for
different slices and sequences, making our model suitable for use as an
automated triaging system in radiology departments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wood_D/0/1/0/all/0/1&quot;&gt;David A. Wood&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06031">
<title>Diagonal Hierarchical Consistency Learning for Semi-supervised Medical Image Segmentation. (arXiv:2311.06031v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06031</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image segmentation, which is essential for many clinical
applications, has achieved almost human-level performance via data-driven deep
learning technologies. Nevertheless, its performance is predicated upon the
costly process of manually annotating a vast amount of medical images. To this
end, we propose a novel framework for robust semi-supervised medical image
segmentation using diagonal hierarchical consistency learning (DiHC-Net).
First, it is composed of multiple sub-models with identical multi-scale
architecture but with distinct sub-layers, such as up-sampling and
normalisation layers. Second, with mutual consistency, a novel consistency
regularisation is enforced between one model&apos;s intermediate and final
prediction and soft pseudo labels from other models in a diagonal hierarchical
fashion. A series of experiments verifies the efficacy of our simple framework,
outperforming all previous approaches on public benchmark dataset on organ and
tumour.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koo_H/0/1/0/all/0/1&quot;&gt;Heejoon Koo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08747">
<title>Improved Dense Nested Attention Network Based on Transformer for Infrared Small Target Detection. (arXiv:2311.08747v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08747</link>
<description rdf:parseType="Literal">&lt;p&gt;Infrared small target detection based on deep learning offers unique
advantages in separating small targets from complex and dynamic backgrounds.
However, the features of infrared small targets gradually weaken as the depth
of convolutional neural network (CNN) increases. To address this issue, we
propose a novel method for detecting infrared small targets called improved
dense nested attention network (IDNANet), which is based on the transformer
architecture. We preserve the dense nested structure of dense nested attention
network (DNANet) and introduce the Swin-transformer during feature extraction
stage to enhance the continuity of features. Furthermore, we integrate the
ACmix attention structure into the dense nested structure to enhance the
features of intermediate layers. Additionally, we design a weighted dice binary
cross-entropy (WD-BCE) loss function to mitigate the negative impact of
foreground-background imbalance in the samples. Moreover, we develop a dataset
specifically for infrared small targets, called BIT-SIRST. The dataset
comprises a significant amount of real-world targets and manually annotated
labels, as well as synthetic data and corresponding labels. We have evaluated
the effectiveness of our method through experiments conducted on public
datasets. In comparison to other state-of-the-art methods, our approach
outperforms in terms of probability of detection ($P_d$), false-alarm rate
($F_a$), and mean intersection of union ($mIoU$). The $mIoU$ reaches 90.89\% on
the NUDT-SIRST dataset and 79.72\% on the SIRST dataset. The BIT-SIRST dataset
and codes are available openly at
\href{https://github.com/EdwardBao1006/bit\_sirst}{\color[HTML]{B22222}{https://github.com/EdwardBao1006/bit\_sirst}}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_C/0/1/0/all/0/1&quot;&gt;Chun Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jie Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1&quot;&gt;Yaqian Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tianhua Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhijun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zechen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Q/0/1/0/all/0/1&quot;&gt;Qun Hao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01689">
<title>Fast and accurate sparse-view CBCT reconstruction using meta-learned neural attenuation field and hash-encoding regularization. (arXiv:2312.01689v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01689</link>
<description rdf:parseType="Literal">&lt;p&gt;Cone beam computed tomography (CBCT) is an emerging medical imaging technique
to visualize the internal anatomical structures of patients. During a CBCT
scan, several projection images of different angles or views are collectively
utilized to reconstruct a tomographic image. However, reducing the number of
projections in a CBCT scan while preserving the quality of a reconstructed
image is challenging due to the nature of an ill-posed inverse problem.
Recently, a neural attenuation field (NAF) method was proposed by adopting a
neural radiance field algorithm as a new way for CBCT reconstruction,
demonstrating fast and promising results using only 50 views. However,
decreasing the number of projections is still preferable to reduce potential
radiation exposure, and a faster reconstruction time is required considering a
typical scan time. In this work, we propose a fast and accurate sparse-view
CBCT reconstruction (FACT) method to provide better reconstruction quality and
faster optimization speed in the minimal number of view acquisitions ($&amp;lt;$ 50
views). In the FACT method, we meta-trained a neural network and a hash-encoder
using a few scans (= 15), and a new regularization technique is utilized to
reconstruct the details of an anatomical structure. In conclusion, we have
shown that the FACT method produced better, and faster reconstruction results
over the other conventional algorithms based on CBCT scans of different body
parts (chest, head, and abdomen) and CT vendors (Siemens, Phillips, and GE).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shin_H/0/1/0/all/0/1&quot;&gt;Heejun Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taehee Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jongho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chun_S/0/1/0/all/0/1&quot;&gt;Se Young Chun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cho_S/0/1/0/all/0/1&quot;&gt;Seungryung Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shin_D/0/1/0/all/0/1&quot;&gt;Dongmyung Shin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03996">
<title>Stable Diffusion for Data Augmentation in COCO and Weed Datasets. (arXiv:2312.03996v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03996</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models have increasingly impacted relative tasks, from computer
vision to interior design and other fields. Stable diffusion is an outstanding
diffusion model that paves the way for producing high-resolution images with
thorough details from text prompts or reference images. It will be an
interesting topic about gaining improvements for small datasets with
image-sparse categories. This study utilized seven common categories and three
widespread weed species to evaluate the efficiency of a stable diffusion model.
In detail, Stable diffusion was used to generate synthetic images belonging to
these classes; three techniques (i.e., Image-to-image translation, Dreambooth,
and ControlNet) based on stable diffusion were leveraged for image generation
with different focuses. Then, classification and detection tasks were conducted
based on these synthetic images, whose performance was compared to the models
trained on original images. Promising results have been achieved in some
classes. This seminal study may expedite the adaption of stable diffusion
models to different fields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1&quot;&gt;Boyang Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04118">
<title>Caregiver Talk Shapes Toddler Vision: A Computational Study of Dyadic Play. (arXiv:2312.04118v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04118</link>
<description rdf:parseType="Literal">&lt;p&gt;Infants&apos; ability to recognize and categorize objects develops gradually. The
second year of life is marked by both the emergence of more semantic visual
representations and a better understanding of word meaning. This suggests that
language input may play an important role in shaping visual representations.
However, even in suitable contexts for word learning like dyadic play sessions,
caregivers utterances are sparse and ambiguous, often referring to objects that
are different from the one to which the child attends. Here, we systematically
investigate to what extent caregivers&apos; utterances can nevertheless enhance
visual representations. For this we propose a computational model of visual
representation learning during dyadic play. We introduce a synthetic dataset of
ego-centric images perceived by a toddler-agent that moves and rotates toy
objects in different parts of its home environment while hearing caregivers&apos;
utterances, modeled as captions. We propose to model toddlers&apos; learning as
simultaneously aligning representations for 1) close-in-time images and 2)
co-occurring images and utterances. We show that utterances with statistics
matching those of real caregivers give rise to representations supporting
improved category recognition. Our analysis reveals that a small
decrease/increase in object-relevant naming frequencies can drastically impact
the learned representations. This affects the attention on object names within
an utterance, which is required for efficient visuo-linguistic alignment.
Overall, our results support the hypothesis that caregivers&apos; naming utterances
can improve toddlers&apos; visual representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaumloffel_T/0/1/0/all/0/1&quot;&gt;Timothy Schauml&amp;#xf6;ffel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aubret_A/0/1/0/all/0/1&quot;&gt;Arthur Aubret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roig_G/0/1/0/all/0/1&quot;&gt;Gemma Roig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Triesch_J/0/1/0/all/0/1&quot;&gt;Jochen Triesch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04960">
<title>MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness. (arXiv:2312.04960v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04960</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) achieve superior performance on various tasks
compared to convolutional neural networks (CNNs), but ViTs are also vulnerable
to adversarial attacks. Adversarial training is one of the most successful
methods to build robust CNN models. Thus, recent works explored new
methodologies for adversarial training of ViTs based on the differences between
ViTs and CNNs, such as better training strategies, preventing attention from
focusing on a single block, or discarding low-attention embeddings. However,
these methods still follow the design of traditional supervised adversarial
training, limiting the potential of adversarial training on ViTs. This paper
proposes a novel defense method, MIMIR, which aims to build a different
adversarial training methodology by utilizing Masked Image Modeling at
pre-training. We create an autoencoder that accepts adversarial examples as
input but takes the clean examples as the modeling target. Then, we create a
mutual information (MI) penalty following the idea of the Information
Bottleneck. Among the two information source inputs and corresponding
adversarial perturbation, the perturbation information is eliminated due to the
constraint of the modeling target. Next, we provide a theoretical analysis of
MIMIR using the bounds of the MI penalty. We also design two adaptive attacks
when the adversary is aware of the MIMIR defense and show that MIMIR still
performs well. The experimental results show that MIMIR improves (natural and
adversarial) accuracy on average by 4.19% on CIFAR-10 and 5.52% on ImageNet-1K,
compared to baselines. On Tiny-ImageNet, we obtained improved natural accuracy
of 2.99\% on average and comparable adversarial accuracy. Our code and trained
models are publicly available https://github.com/xiaoyunxxy/MIMIR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaoyun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shujian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jingzheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picek_S/0/1/0/all/0/1&quot;&gt;Stjepan Picek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08846">
<title>TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training. (arXiv:2312.08846v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08846</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised Multi-modal Contrastive Learning (SMCL) remarkably advances
modern Vision-Language Pre-training (VLP) models by aligning visual and
linguistic modalities. Due to noises in web-harvested text-image pairs,
however, scaling up training data volume in SMCL presents considerable
obstacles in terms of computational cost and data inefficiency. To improve data
efficiency in VLP, we propose Text-aware Image Mixing (TiMix), which integrates
mix-based data augmentation techniques into SMCL, yielding significant
performance improvements without significantly increasing computational
overhead. We provide a theoretical analysis of TiMixfrom a mutual information
(MI) perspective, showing that mixed data samples for cross-modal contrastive
learning implicitly serve as a regularizer for the contrastive loss. The
experimental results demonstrate that TiMix exhibits a comparable performance
on downstream tasks, even with a reduced amount of training data and shorter
training time, when benchmarked against existing methods. This work empirically
and theoretically demonstrates the potential of data mixing for data-efficient
and computationally viable VLP, benefiting broader VLP model adoption in
practical scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Chaoya Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ye_W/0/1/0/all/0/1&quot;&gt;Wei ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haiyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qinghao Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1&quot;&gt;Ming Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Ji Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shikun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10165">
<title>Test-Time Domain Adaptation by Learning Domain-Aware Batch Normalization. (arXiv:2312.10165v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10165</link>
<description rdf:parseType="Literal">&lt;p&gt;Test-time domain adaptation aims to adapt the model trained on source domains
to unseen target domains using a few unlabeled images. Emerging research has
shown that the label and domain information is separately embedded in the
weight matrix and batch normalization (BN) layer. Previous works normally
update the whole network naively without explicitly decoupling the knowledge
between label and domain. As a result, it leads to knowledge interference and
defective distribution adaptation. In this work, we propose to reduce such
learning interference and elevate the domain knowledge learning by only
manipulating the BN layer. However, the normalization step in BN is
intrinsically unstable when the statistics are re-estimated from a few samples.
We find that ambiguities can be greatly reduced when only updating the two
affine parameters in BN while keeping the source domain statistics. To further
enhance the domain knowledge extraction from unlabeled data, we construct an
auxiliary branch with label-independent self-supervised learning (SSL) to
provide supervision. Moreover, we propose a bi-level optimization based on
meta-learning to enforce the alignment of two learning objectives of auxiliary
and main branches. The goal is to use the auxiliary branch to adapt the domain
and benefit main task for subsequent inference. Our method keeps the same
computational cost at inference as the auxiliary branch can be thoroughly
discarded after adaptation. Extensive experiments show that our method
outperforms the prior works on five WILDS real-world domain shift datasets. Our
method can also be integrated with methods with label-dependent optimization to
further push the performance boundary. Our code is available at
https://github.com/ynanwu/MABN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yanan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1&quot;&gt;Zhixiang Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1&quot;&gt;Konstantinos N. Plataniotis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Songhe Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13066">
<title>PPEA-Depth: Progressive Parameter-Efficient Adaptation for Self-Supervised Monocular Depth Estimation. (arXiv:2312.13066v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13066</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised monocular depth estimation is of significant importance with
applications spanning across autonomous driving and robotics. However, the
reliance on self-supervision introduces a strong static-scene assumption,
thereby posing challenges in achieving optimal performance in dynamic scenes,
which are prevalent in most real-world situations. To address these issues, we
propose PPEA-Depth, a Progressive Parameter-Efficient Adaptation approach to
transfer a pre-trained image model for self-supervised depth estimation. The
training comprises two sequential stages: an initial phase trained on a dataset
primarily composed of static scenes, succeeded by an expansion to more
intricate datasets involving dynamic scenes. To facilitate this process, we
design compact encoder and decoder adapters to enable parameter-efficient
tuning, allowing the network to adapt effectively. They not only uphold
generalized patterns from pre-trained image models but also retain knowledge
gained from the preceding phase into the subsequent one. Extensive experiments
demonstrate that PPEA-Depth achieves state-of-the-art performance on KITTI,
CityScapes and DDAD datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yue-Jiang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuan-Chen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Ying-Tian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fang-Lue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Song-Hai Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14238">
<title>InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks. (arXiv:2312.14238v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14238</link>
<description rdf:parseType="Literal">&lt;p&gt;The exponential growth of large language models (LLMs) has opened up numerous
possibilities for multimodal AGI systems. However, the progress in vision and
vision-language foundation models, which are also critical elements of
multi-modal AGI, has not kept pace with LLMs. In this work, we design a
large-scale vision-language foundation model (InternVL), which scales up the
vision foundation model to 6 billion parameters and progressively aligns it
with the LLM, using web-scale image-text data from various sources. This model
can be broadly applied to and achieve state-of-the-art performance on 32
generic visual-linguistic benchmarks including visual perception tasks such as
image-level or pixel-level recognition, vision-language tasks such as zero-shot
image/video classification, zero-shot image/video-text retrieval, and link with
LLMs to create multi-modal dialogue systems. It has powerful visual
capabilities and can be a good alternative to the ViT-22B. We hope that our
research could contribute to the development of multi-modal large models. Code
and models are available at https://github.com/OpenGVLab/InternVL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiannan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Weijie Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_S/0/1/0/all/0/1&quot;&gt;Sen Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1&quot;&gt;Muyan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qinglong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xizhou Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Lewei Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Tong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00436">
<title>Diff-PCR: Diffusion-Based Correspondence Searching in Doubly Stochastic Matrix Space for Point Cloud Registration. (arXiv:2401.00436v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00436</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiently finding optimal correspondences between point clouds is crucial
for solving both rigid and non-rigid point cloud registration problems.
Existing methods often rely on geometric or semantic feature embedding to
establish correspondences and estimate transformations or flow fields.
Recently, state-of-the-art methods have employed RAFT-like iterative updates to
refine the solution. However, these methods have certain limitations. Firstly,
their iterative refinement design lacks transparency, and their iterative
updates follow a fixed path during the refinement process, which can lead to
suboptimal results. Secondly, these methods overlook the importance of refining
or optimizing correspondences (or matching matrices) as a precursor to solving
transformations or flow fields. They typically compute candidate
correspondences based on distances in the point feature space. However, they
only project the candidate matching matrix into some matrix space once with
Sinkhorn or dual softmax operations to obtain final correspondences. This
one-shot projected matching matrix may be far from the globally optimal one,
and these approaches do not consider the distribution of the target matching
matrix. In this paper, we propose a novel approach that exploits the Denoising
Diffusion Model to predict a searching gradient for the optimal matching matrix
within the Doubly Stochastic Matrix Space. During the reverse denoising
process, our method iteratively searches for better solutions along this
denoising gradient, which points towards the maximum likelihood direction of
the target matching matrix. Our method offers flexibility by allowing the
search to start from any initial matching matrix provided by the online
backbone or white noise. Experimental evaluations on the 3DMatch/3DLoMatch and
4DMatch/4DLoMatch datasets demonstrate the effectiveness of our newly designed
framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qianliang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Haobo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yaqing Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Lei Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02906">
<title>MLLM-Protector: Ensuring MLLM&apos;s Safety without Hurting Performance. (arXiv:2401.02906v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02906</link>
<description rdf:parseType="Literal">&lt;p&gt;The deployment of multimodal large language models (MLLMs) has brought forth
a unique vulnerability: susceptibility to malicious attacks through visual
inputs. We delve into the novel challenge of defending MLLMs against such
attacks. We discovered that images act as a &quot;foreign language&quot; that is not
considered during alignment, which can make MLLMs prone to producing harmful
responses. Unfortunately, unlike the discrete tokens considered in text-based
LLMs, the continuous nature of image signals presents significant alignment
challenges, which poses difficulty to thoroughly cover the possible scenarios.
This vulnerability is exacerbated by the fact that open-source MLLMs are
predominantly fine-tuned on limited image-text pairs that is much less than the
extensive text-based pretraining corpus, which makes the MLLMs more prone to
catastrophic forgetting of their original abilities during explicit alignment
tuning. To tackle these challenges, we introduce MLLM-Protector, a
plug-and-play strategy combining a lightweight harm detector and a response
detoxifier. The harm detector&apos;s role is to identify potentially harmful outputs
from the MLLM, while the detoxifier corrects these outputs to ensure the
response stipulates to the safety standards. This approach effectively
mitigates the risks posed by malicious visual inputs without compromising the
model&apos;s overall performance. Our results demonstrate that MLLM-Protector offers
a robust solution to a previously unaddressed aspect of MLLM security.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pi_R/0/1/0/all/0/1&quot;&gt;Renjie Pi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1&quot;&gt;Tianyang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yueqi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1&quot;&gt;Rui Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_Q/0/1/0/all/0/1&quot;&gt;Qing Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hanze Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03145">
<title>Self-supervised Feature Adaptation for 3D Industrial Anomaly Detection. (arXiv:2401.03145v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03145</link>
<description rdf:parseType="Literal">&lt;p&gt;Industrial anomaly detection is generally addressed as an unsupervised task
that aims at locating defects with only normal training samples. Recently,
numerous 2D anomaly detection methods have been proposed and have achieved
promising results, however, using only the 2D RGB data as input is not
sufficient to identify imperceptible geometric surface anomalies. Hence, in
this work, we focus on multi-modal anomaly detection. Specifically, we
investigate early multi-modal approaches that attempted to utilize models
pre-trained on large-scale visual datasets, i.e., ImageNet, to construct
feature databases. And we empirically find that directly using these
pre-trained models is not optimal, it can either fail to detect subtle defects
or mistake abnormal features as normal ones. This may be attributed to the
domain gap between target industrial data and source data.Towards this problem,
we propose a Local-to-global Self-supervised Feature Adaptation (LSFA) method
to finetune the adaptors and learn task-oriented representation toward anomaly
detection.Both intra-modal adaptation and cross-modal alignment are optimized
from a local-to-global perspective in LSFA to ensure the representation quality
and consistency in the inference stage.Extensive experiments demonstrate that
our method not only brings a significant performance boost to feature embedding
based approaches, but also outperforms previous State-of-The-Art (SoTA) methods
prominently on both MVTec-3D AD and Eyecandies datasets, e.g., LSFA achieves
97.1% I-AUROC on MVTec-3D, surpass previous SoTA by +3.4%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1&quot;&gt;Yuanpeng Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Boshen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuxi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xuhai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiangning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yabiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Cai Rong Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05217">
<title>Exploring Vulnerabilities of No-Reference Image Quality Assessment Models: A Query-Based Black-Box Method. (arXiv:2401.05217v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05217</link>
<description rdf:parseType="Literal">&lt;p&gt;No-Reference Image Quality Assessment (NR-IQA) aims to predict image quality
scores consistent with human perception without relying on pristine reference
images, serving as a crucial component in various visual tasks. Ensuring the
robustness of NR-IQA methods is vital for reliable comparisons of different
image processing techniques and consistent user experiences in recommendations.
The attack methods for NR-IQA provide a powerful instrument to test the
robustness of NR-IQA. However, current attack methods of NR-IQA heavily rely on
the gradient of the NR-IQA model, leading to limitations when the gradient
information is unavailable. In this paper, we present a pioneering query-based
black box attack against NR-IQA methods. We propose the concept of score
boundary and leverage an adaptive iterative approach with multiple score
boundaries. Meanwhile, the initial attack directions are also designed to
leverage the characteristics of the Human Visual System (HVS). Experiments show
our method outperforms all compared state-of-the-art attack methods and is far
ahead of previous black-box methods. The effective NR-IQA model DBCNN suffers a
Spearman&apos;s rank-order correlation coefficient (SROCC) decline of 0.6381
attacked by our method, revealing the vulnerability of NR-IQA models to
black-box attacks. The proposed attack method also provides a potent tool for
further exploration into NR-IQA robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chenxi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yujia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dingquan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1&quot;&gt;Tingting Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05686">
<title>Self Expanding Convolutional Neural Networks. (arXiv:2401.05686v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05686</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a novel method for dynamically expanding
Convolutional Neural Networks (CNNs) during training, aimed at meeting the
increasing demand for efficient and sustainable deep learning models. Our
approach, drawing from the seminal work on Self-Expanding Neural Networks
(SENN), employs a natural expansion score as an expansion criteria to address
the common issue of over-parameterization in deep convolutional neural
networks, thereby ensuring that the model&apos;s complexity is finely tuned to the
task&apos;s specific needs. A significant benefit of this method is its eco-friendly
nature, as it obviates the necessity of training multiple models of different
sizes. We employ a strategy where a single model is dynamically expanded,
facilitating the extraction of checkpoints at various complexity levels,
effectively reducing computational resource use and energy consumption while
also expediting the development cycle by offering diverse model complexities
from a single training session. We evaluate our method on the CIFAR-10 dataset
and our experimental results validate this approach, demonstrating that
dynamically adding layers not only maintains but also improves CNN performance,
underscoring the effectiveness of our expansion criteria. This approach marks a
considerable advancement in developing adaptive, scalable, and environmentally
considerate neural network architectures, addressing key challenges in the
field of deep learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Appolinary_B/0/1/0/all/0/1&quot;&gt;Blaise Appolinary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deaconu_A/0/1/0/all/0/1&quot;&gt;Alex Deaconu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Sophia Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qingze/0/1/0/all/0/1&quot;&gt;Qingze&lt;/a&gt; (Eric)Li</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06506">
<title>Frequency Masking for Universal Deepfake Detection. (arXiv:2401.06506v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.06506</link>
<description rdf:parseType="Literal">&lt;p&gt;We study universal deepfake detection. Our goal is to detect synthetic images
from a range of generative AI approaches, particularly from emerging ones which
are unseen during training of the deepfake detector. Universal deepfake
detection requires outstanding generalization capability. Motivated by recently
proposed masked image modeling which has demonstrated excellent generalization
in self-supervised pre-training, we make the first attempt to explore masked
image modeling for universal deepfake detection. We study spatial and frequency
domain masking in training deepfake detectors. Based on empirical analysis, we
propose a novel deepfake detector via frequency masking. Our focus on frequency
domain is different from the majority, which primarily target spatial domain
detection. Our comparative analyses reveal substantial performance gains over
existing methods. Code and models are publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doloriel_C/0/1/0/all/0/1&quot;&gt;Chandler Timm Doloriel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1&quot;&gt;Ngai-Man Cheung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06637">
<title>Adversarial Examples are Misaligned in Diffusion Model Manifolds. (arXiv:2401.06637v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.06637</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, diffusion models (DMs) have drawn significant attention for
their success in approximating data distributions, yielding state-of-the-art
generative results. Nevertheless, the versatility of these models extends
beyond their generative capabilities to encompass various vision applications,
such as image inpainting, segmentation, adversarial robustness, among others.
This study is dedicated to the investigation of adversarial attacks through the
lens of diffusion models. However, our objective does not involve enhancing the
adversarial robustness of image classifiers. Instead, our focus lies in
utilizing the diffusion model to detect and analyze the anomalies introduced by
these attacks on images. To that end, we systematically examine the alignment
of the distributions of adversarial examples when subjected to the process of
transformation using diffusion models. The efficacy of this approach is
assessed across CIFAR-10 and ImageNet datasets, including varying image sizes
in the latter. The results demonstrate a notable capacity to discriminate
effectively between benign and attacked images, providing compelling evidence
that adversarial instances do not align with the learned manifold of the DMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorenz_P/0/1/0/all/0/1&quot;&gt;Peter Lorenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durall_R/0/1/0/all/0/1&quot;&gt;Ricard Durall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1&quot;&gt;Janis Keuper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07126">
<title>IVIM-Morph: Motion-compensated quantitative Intra-voxel Incoherent Motion (IVIM) analysis for functional fetal lung maturity assessment from diffusion-weighted MRI data. (arXiv:2401.07126v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07126</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantitative analysis of pseudo-diffusion in diffusion-weighted magnetic
resonance imaging (DWI) data shows potential for assessing fetal lung
maturation and generating valuable imaging biomarkers. Yet, the clinical
utility of DWI data is hindered by unavoidable fetal motion during acquisition.
We present IVIM-morph, a self-supervised deep neural network model for
motion-corrected quantitative analysis of DWI data using the Intra-voxel
Incoherent Motion (IVIM) model. IVIM-morph combines two sub-networks, a
registration sub-network, and an IVIM model fitting sub-network, enabling
simultaneous estimation of IVIM model parameters and motion. To promote
physically plausible image registration, we introduce a biophysically informed
loss function that effectively balances registration and model-fitting quality.
We validated the efficacy of IVIM-morph by establishing a correlation between
the predicted IVIM model parameters of the lung and gestational age (GA) using
fetal DWI data of 39 subjects. IVIM-morph exhibited a notably improved
correlation with gestational age (GA) when performing in-vivo quantitative
analysis of fetal lung DWI data during the canalicular phase. IVIM-morph shows
potential in developing valuable biomarkers for non-invasive assessment of
fetal lung maturity with DWI data. Moreover, its adaptability opens the door to
potential applications in other clinical contexts where motion compensation is
essential for quantitative DWI analysis. The IVIM-morph code is readily
available at: https://github.com/TechnionComputationalMRILab/qDWI-Morph.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kertes_N/0/1/0/all/0/1&quot;&gt;Noga Kertes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zaffrani_Reznikov_Y/0/1/0/all/0/1&quot;&gt;Yael Zaffrani-Reznikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Afacan_O/0/1/0/all/0/1&quot;&gt;Onur Afacan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kurugol_S/0/1/0/all/0/1&quot;&gt;Sila Kurugol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Warfield_S/0/1/0/all/0/1&quot;&gt;Simon K. Warfield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Freiman_M/0/1/0/all/0/1&quot;&gt;Moti Freiman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07825">
<title>Phenotyping calcification in vascular tissues using artificial intelligence. (arXiv:2401.07825v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07825</link>
<description rdf:parseType="Literal">&lt;p&gt;Vascular calcification is implicated as an important factor in major adverse
cardiovascular events (MACE), including heart attack and stroke. A controversy
remains over how to integrate the diverse forms of vascular calcification into
clinical risk assessment tools. Even the commonly used calcium score for
coronary arteries, which assumes risk scales positively with total
calcification, has important inconsistencies. Fundamental studies are needed to
determine how risk is influenced by the diverse calcification phenotypes.
However, studies of these kinds are hindered by the lack of high-throughput,
objective, and non-destructive tools for classifying calcification in imaging
data sets. Here, we introduce a new classification system for phenotyping
calcification along with a semi-automated, non-destructive pipeline that can
distinguish these phenotypes in even atherosclerotic tissues. The pipeline
includes a deep-learning-based framework for segmenting lipid pools in noisy
micro-CT images and an unsupervised clustering framework for categorizing
calcification based on size, clustering, and topology. This approach is
illustrated for five vascular specimens, providing phenotyping for thousands of
calcification particles across as many as 3200 images in less than seven hours.
Average Dice Similarity Coefficients of 0.96 and 0.87 could be achieved for
tissue and lipid pool, respectively, with training and validation needed on
only 13 images despite the high heterogeneity in these tissues. By introducing
an efficient and comprehensive approach to phenotyping calcification, this work
enables large-scale studies to identify a more reliable indicator of the risk
of cardiovascular events, a leading cause of global mortality and morbidity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramezanpour_M/0/1/0/all/0/1&quot;&gt;Mehdi Ramezanpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robertson_A/0/1/0/all/0/1&quot;&gt;Anne M. Robertson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tobe_Y/0/1/0/all/0/1&quot;&gt;Yasutaka Tobe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xiaowei Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cebral_J/0/1/0/all/0/1&quot;&gt;Juan R. Cebral&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08154">
<title>Learned Image Compression with ROI-Weighted Distortion and Bit Allocation. (arXiv:2401.08154v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08154</link>
<description rdf:parseType="Literal">&lt;p&gt;This one page paper describes our method for the track of image compression.
To achieve better perceptual quality, we use the adversarial loss to generate
realistic textures, use region of interest (ROI) mask to guide the bit
allocation for different regions. Our Team name is TLIC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1&quot;&gt;Yongqi Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hangyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ronggang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08332">
<title>Generative Denoise Distillation: Simple Stochastic Noises Induce Efficient Knowledge Transfer for Dense Prediction. (arXiv:2401.08332v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08332</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge distillation is the process of transferring knowledge from a more
powerful large model (teacher) to a simpler counterpart (student). Numerous
current approaches involve the student imitating the knowledge of the teacher
directly. However, redundancy still exists in the learned representations
through these prevalent methods, which tend to learn each spatial location&apos;s
features indiscriminately. To derive a more compact representation (concept
feature) from the teacher, inspired by human cognition, we suggest an
innovative method, termed Generative Denoise Distillation (GDD), where
stochastic noises are added to the concept feature of the student to embed them
into the generated instance feature from a shallow network. Then, the generated
instance feature is aligned with the knowledge of the instance from the
teacher. We extensively experiment with object detection, instance
segmentation, and semantic segmentation to demonstrate the versatility and
effectiveness of our method. Notably, GDD achieves new state-of-the-art
performance in the tasks mentioned above. We have achieved substantial
improvements in semantic segmentation by enhancing PspNet and DeepLabV3, both
of which are based on ResNet-18, resulting in mIoU scores of 74.67 and 77.69,
respectively, surpassing their previous scores of 69.85 and 73.20 on the
Cityscapes dataset of 20 categories. The source code is available at
https://github.com/ZhgLiu/GDD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhaoge Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaohao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yunkang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;Weiming Shen&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>