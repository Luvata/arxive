<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-24T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14201" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14216" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14223" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14238" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14280" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14301" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14389" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14395" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14400" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14407" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14427" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14432" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14457" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14465" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14474" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14481" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14502" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14556" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14570" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14574" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14606" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14619" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14626" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14635" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14650" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14664" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14697" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14706" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14733" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14830" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14871" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.02370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.05684" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14291" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02693" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05400" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17349" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05745" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15774" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17602" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16184" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15216" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02139" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09759" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13613" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16502" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18083" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06275" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06914" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07199" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07884" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08914" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11146" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11537" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13771" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13977" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.14198">
<title>ZeroShape: Regression-based Zero-shot Shape Reconstruction. (arXiv:2312.14198v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14198</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of single-image zero-shot 3D shape reconstruction.
Recent works learn zero-shot shape reconstruction through generative modeling
of 3D assets, but these models are computationally expensive at train and
inference time. In contrast, the traditional approach to this problem is
regression-based, where deterministic models are trained to directly regress
the object shape. Such regression methods possess much higher computational
efficiency than generative methods. This raises a natural question: is
generative modeling necessary for high performance, or conversely, are
regression-based approaches still competitive? To answer this, we design a
strong regression-based model, called ZeroShape, based on the converging
findings in this field and a novel insight. We also curate a large real-world
evaluation benchmark, with objects from three different real-world 3D datasets.
This evaluation benchmark is more diverse and an order of magnitude larger than
what prior works use to quantitatively evaluate their models, aiming at
reducing the evaluation variance in our field. We show that ZeroShape not only
achieves superior performance over state-of-the-art methods, but also
demonstrates significantly higher computational and data efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zixuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stojanov_S/0/1/0/all/0/1&quot;&gt;Stefan Stojanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thai_A/0/1/0/all/0/1&quot;&gt;Anh Thai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1&quot;&gt;Varun Jampani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1&quot;&gt;James M. Rehg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14200">
<title>Efficient Architecture Search via Bi-level Data Pruning. (arXiv:2312.14200v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14200</link>
<description rdf:parseType="Literal">&lt;p&gt;Improving the efficiency of Neural Architecture Search (NAS) is a challenging
but significant task that has received much attention. Previous works mainly
adopted the Differentiable Architecture Search (DARTS) and improved its search
strategies or modules to enhance search efficiency. Recently, some methods have
started considering data reduction for speedup, but they are not tightly
coupled with the architecture search process, resulting in sub-optimal
performance. To this end, this work pioneers an exploration into the critical
role of dataset characteristics for DARTS bi-level optimization, and then
proposes a novel Bi-level Data Pruning (BDP) paradigm that targets the weights
and architecture levels of DARTS to enhance efficiency from a data perspective.
Specifically, we introduce a new progressive data pruning strategy that
utilizes supernet prediction dynamics as the metric, to gradually prune
unsuitable samples for DARTS during the search. An effective automatic class
balance constraint is also integrated into BDP, to suppress potential class
imbalances resulting from data-efficient algorithms. Comprehensive evaluations
on the NAS-Bench-201 search space, DARTS search space, and MobileNet-like
search space validate that BDP reduces search costs by over 50% while achieving
superior performance when applied to baseline DARTS. Besides, we demonstrate
that BDP can harmoniously integrate with advanced DARTS variants, like PC-DARTS
and \b{eta}-DARTS, offering an approximately 2 times speedup with minimal
performance compromises.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_C/0/1/0/all/0/1&quot;&gt;Chongjun Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1&quot;&gt;Peng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weihao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1&quot;&gt;Hancheng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Baopu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14201">
<title>Towards Better Visualizing the Decision Basis of Networks via Unfold and Conquer Attribution Guidance. (arXiv:2312.14201v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14201</link>
<description rdf:parseType="Literal">&lt;p&gt;Revealing the transparency of Deep Neural Networks (DNNs) has been widely
studied to describe the decision mechanisms of network inner structures. In
this paper, we propose a novel post-hoc framework, Unfold and Conquer
Attribution Guidance (UCAG), which enhances the explainability of the network
decision by spatially scrutinizing the input features with respect to the model
confidence. Addressing the phenomenon of missing detailed descriptions, UCAG
sequentially complies with the confidence of slices of the image, leading to
providing an abundant and clear interpretation. Therefore, it is possible to
enhance the representation ability of explanation by preserving the detailed
descriptions of assistant input features, which are commonly overwhelmed by the
main meaningful regions. We conduct numerous evaluations to validate the
performance in several metrics: i) deletion and insertion, ii) (energy-based)
pointing games, and iii) positive and negative density maps. Experimental
results, including qualitative comparisons, demonstrate that our method
outperforms the existing methods with the nature of clear and detailed
explanations and applicability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Jung-Ho Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_W/0/1/0/all/0/1&quot;&gt;Woo-Jeoung Nam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_K/0/1/0/all/0/1&quot;&gt;Kyu-Sung Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seong-Whan Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14206">
<title>LLM4VG: Large Language Models Evaluation for Video Grounding. (arXiv:2312.14206v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14206</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, researchers have attempted to investigate the capability of LLMs in
handling videos and proposed several video LLM models. However, the ability of
LLMs to handle video grounding (VG), which is an important time-related video
task requiring the model to precisely locate the start and end timestamps of
temporal moments in videos that match the given textual queries, still remains
unclear and unexplored in literature. To fill the gap, in this paper, we
propose the LLM4VG benchmark, which systematically evaluates the performance of
different LLMs on video grounding tasks. Based on our proposed LLM4VG, we
design extensive experiments to examine two groups of video LLM models on video
grounding: (i) the video LLMs trained on the text-video pairs (denoted as
VidLLM), and (ii) the LLMs combined with pretrained visual description models
such as the video/image captioning model. We propose prompt methods to
integrate the instruction of VG and description from different kinds of
generators, including caption-based generators for direct visual description
and VQA-based generators for information enhancement. We also provide
comprehensive comparisons of various VidLLMs and explore the influence of
different choices of visual models, LLMs, prompt designs, etc, as well. Our
experimental evaluations lead to two conclusions: (i) the existing VidLLMs are
still far away from achieving satisfactory video grounding performance, and
more time-related video tasks should be included to further fine-tune these
models, and (ii) the combination of LLMs and visual models shows preliminary
abilities for video grounding with considerable potential for improvement by
resorting to more reliable models and further guidance of prompt instructions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1&quot;&gt;Wei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zeyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zihan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuwei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wenwu Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14209">
<title>TextFusion: Unveiling the Power of Textual Semantics for Controllable Image Fusion. (arXiv:2312.14209v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14209</link>
<description rdf:parseType="Literal">&lt;p&gt;Advanced image fusion methods are devoted to generating the fusion results by
aggregating the complementary information conveyed by the source images.
However, the difference in the source-specific manifestation of the imaged
scene content makes it difficult to design a robust and controllable fusion
process. We argue that this issue can be alleviated with the help of
higher-level semantics, conveyed by the text modality, which should enable us
to generate fused images for different purposes, such as visualisation and
downstream tasks, in a controllable way. This is achieved by exploiting a
vision-and-language model to build a coarse-to-fine association mechanism
between the text and image signals. With the guidance of the association maps,
an affine fusion unit is embedded in the transformer network to fuse the text
and vision modalities at the feature level. As another ingredient of this work,
we propose the use of textual attention to adapt image quality assessment to
the fusion task. To facilitate the implementation of the proposed text-guided
fusion paradigm, and its adoption by the wider research community, we release a
text-annotated image fusion dataset IVT. Extensive experiments demonstrate that
our approach (TextFusion) consistently outperforms traditional appearance-based
fusion methods. Our code and dataset will be publicly available on the project
homepage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Chunyang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tianyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiao-Jun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zhangyong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1&quot;&gt;Josef Kittler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14216">
<title>DreamDistribution: Prompt Distribution Learning for Text-to-Image Diffusion Models. (arXiv:2312.14216v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14216</link>
<description rdf:parseType="Literal">&lt;p&gt;The popularization of Text-to-Image (T2I) diffusion models enables the
generation of high-quality images from text descriptions. However, generating
diverse customized images with reference visual attributes remains challenging.
This work focuses on personalizing T2I diffusion models at a more abstract
concept or category level, adapting commonalities from a set of reference
images while creating new instances with sufficient variations. We introduce a
solution that allows a pretrained T2I diffusion model to learn a set of soft
prompts, enabling the generation of novel images by sampling prompts from the
learned distribution. These prompts offer text-guided editing capabilities and
additional flexibility in controlling variation and mixing between multiple
distributions. We also show the adaptability of the learned prompt distribution
to other tasks, such as text-to-3D. Finally we demonstrate effectiveness of our
approach through quantitative analysis including automatic evaluation and human
assessment. Project website: https://briannlongzhao.github.io/DreamDistribution
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Brian Nlong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yuhang Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiashu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xinyang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yifan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Itti_L/0/1/0/all/0/1&quot;&gt;Laurent Itti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1&quot;&gt;Vibhav Vineet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yunhao Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14218">
<title>AutoAugment Input Transformation for Highly Transferable Targeted Attacks. (arXiv:2312.14218v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14218</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks (DNNs) are widely acknowledged to be susceptible to
adversarial examples, wherein imperceptible perturbations are added to clean
examples through diverse input transformation attacks. However, these methods
originally designed for non-targeted attacks exhibit low success rates in
targeted attacks. Recent targeted adversarial attacks mainly pay attention to
gradient optimization, attempting to find the suitable perturbation direction.
However, few of them are dedicated to input transformation.In this work, we
observe a positive correlation between the logit/probability of the target
class and diverse input transformation methods in targeted attacks. To this
end, we propose a novel targeted adversarial attack called AutoAugment Input
Transformation (AAIT). Instead of relying on hand-made strategies, AAIT
searches for the optimal transformation policy from a transformation space
comprising various operations. Then, AAIT crafts adversarial examples using the
found optimal transformation policy to boost the adversarial transferability in
targeted attacks. Extensive experiments conducted on CIFAR-10 and
ImageNet-Compatible datasets demonstrate that the proposed AAIT surpasses other
transfer-based targeted attacks significantly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Haobo Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1&quot;&gt;Kun He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14223">
<title>Fast Diffusion-Based Counterfactuals for Shortcut Removal and Generation. (arXiv:2312.14223v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14223</link>
<description rdf:parseType="Literal">&lt;p&gt;Shortcut learning is when a model -- e.g. a cardiac disease classifier --
exploits correlations between the target label and a spurious shortcut feature,
e.g. a pacemaker, to predict the target label based on the shortcut rather than
real discriminative features. This is common in medical imaging, where
treatment and clinical annotations correlate with disease labels, making them
easy shortcuts to predict disease. We propose a novel detection and
quantification of the impact of potential shortcut features via a fast
diffusion-based counterfactual image generation that can synthetically remove
or add shortcuts. Via a novel inpainting-based modification we spatially limit
the changes made with no extra inference step, encouraging the removal of
spatially constrained shortcut features while ensuring that the shortcut-free
counterfactuals preserve their remaining image features to a high degree. Using
these, we assess how shortcut features influence model predictions.
&lt;/p&gt;
&lt;p&gt;This is enabled by our second contribution: An efficient diffusion-based
counterfactual explanation method with significant inference speed-up at
comparable image quality as state-of-the-art. We confirm this on two large
chest X-ray datasets, a skin lesion dataset, and CelebA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_N/0/1/0/all/0/1&quot;&gt;Nina Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pegios_P/0/1/0/all/0/1&quot;&gt;Paraskevas Pegios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1&quot;&gt;Aasa Feragen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petersen_E/0/1/0/all/0/1&quot;&gt;Eike Petersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bigdeli_S/0/1/0/all/0/1&quot;&gt;Siavash Bigdeli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14232">
<title>Parrot Captions Teach CLIP to Spot Text. (arXiv:2312.14232v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14232</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite CLIP being the foundation model in numerous vision-language
applications, the CLIP suffers from a severe text spotting bias. Such bias
causes CLIP models to `Parrot&apos; the visual text embedded within images while
disregarding the authentic visual semantics. We uncover that in the most
popular image-text dataset LAION-2B, the captions also densely parrot (spell)
the text embedded in images. Our analysis shows that around \textbf{50\%} of
images are embedded with visual text content, and \textbf{90\%} of their
captions more or less parrot the visual text. Based on such observation, we
thoroughly inspect the different release d versions of CLIP models and verify
that the visual text is the dominant factor in measuring the LAION-style
image-text similarity for these models. To examine whether these parrot
captions shape the text spotting bias, we train a series of CLIP models with
LAION subsets curated by different parrot-caption-oriented criteria. We show
that training with parrot captions easily shapes such bias but harms the
expected visual-language representation learning in CLIP models. This suggests
that it is urgent to revisit either the design of CLIP-like models or the
existing image-text dataset curation pipeline built on CLIP score filtering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yiqi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1&quot;&gt;Conghui He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Alex Jinpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weijia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14233">
<title>VCoder: Versatile Vision Encoders for Multimodal Large Language Models. (arXiv:2312.14233v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14233</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans possess the remarkable skill of Visual Perception, the ability to see
and understand the seen, helping them make sense of the visual world and, in
turn, reason. Multimodal Large Language Models (MLLM) have recently achieved
impressive performance on vision-language tasks ranging from visual
question-answering and image captioning to visual reasoning and image
generation. However, when prompted to identify or count (perceive) the entities
in a given image, existing MLLM systems fail. Working towards developing an
accurate MLLM system for perception and reasoning, we propose using Versatile
vision enCoders (VCoder) as perception eyes for Multimodal LLMs. We feed the
VCoder with perception modalities such as segmentation or depth maps, improving
the MLLM&apos;s perception abilities. Secondly, we leverage the images from COCO and
outputs from off-the-shelf vision perception models to create our COCO
Segmentation Text (COST) dataset for training and evaluating MLLMs on the
object perception task. Thirdly, we introduce metrics to assess the object
perception abilities in MLLMs on our COST dataset. Lastly, we provide extensive
experimental evidence proving the VCoder&apos;s improved object-level perception
skills over existing Multimodal LLMs, including GPT-4V. We open-source our
dataset, code, and models to promote research. We open-source our code at
https://github.com/SHI-Labs/VCoder
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_J/0/1/0/all/0/1&quot;&gt;Jitesh Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Humphrey Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14235">
<title>Neural Spline Fields for Burst Image Fusion and Layer Separation. (arXiv:2312.14235v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14235</link>
<description rdf:parseType="Literal">&lt;p&gt;Each photo in an image burst can be considered a sample of a complex 3D
scene: the product of parallax, diffuse and specular materials, scene motion,
and illuminant variation. While decomposing all of these effects from a stack
of misaligned images is a highly ill-conditioned task, the conventional
align-and-merge burst pipeline takes the other extreme: blending them into a
single image. In this work, we propose a versatile intermediate representation:
a two-layer alpha-composited image plus flow model constructed with neural
spline fields -- networks trained to map input coordinates to spline control
points. Our method is able to, during test-time optimization, jointly fuse a
burst image capture into one high-resolution reconstruction and decompose it
into transmission and obstruction layers. Then, by discarding the obstruction
layer, we can perform a range of tasks including seeing through occlusions,
reflection suppression, and shadow removal. Validated on complex synthetic and
in-the-wild captures we find that, with no post-processing steps or learned
priors, our generalizable model is able to outperform existing dedicated
single-image and multi-view obstruction removal approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chugunov_I/0/1/0/all/0/1&quot;&gt;Ilya Chugunov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shustin_D/0/1/0/all/0/1&quot;&gt;David Shustin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1&quot;&gt;Ruyu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1&quot;&gt;Chenyang Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1&quot;&gt;Felix Heide&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14238">
<title>InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks. (arXiv:2312.14238v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14238</link>
<description rdf:parseType="Literal">&lt;p&gt;The exponential growth of large language models (LLMs) has opened up numerous
possibilities for multi-modal AGI systems. However, the progress in vision and
vision-language foundation models, which are also critical elements of
multi-modal AGI, has not kept pace with LLMs. In this work, we design a
large-scale vision-language foundation model (InternVL), which scales up the
vision foundation model to 6 billion parameters and progressively aligns it
with the large language model, using web-scale image-text data from various
sources. This model can be broadly applied to and achieve state-of-the-art
performance on visual perception tasks such as image-level or pixel-level
recognition, vision-language tasks such as zero-shot image/video
classification, zero-shot image/video-text retrieval, and link with LLMs to
create multi-modal dialogue systems. We hope that our research could contribute
to the development of multi-modal large models. Code and models are available
at https://github.com/OpenGVLab/InternVL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiannan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Weijie Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_S/0/1/0/all/0/1&quot;&gt;Sen Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muyan_Z/0/1/0/all/0/1&quot;&gt;Zhong Muyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qinglong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xizhou Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Lewei Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Tong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14239">
<title>PlatoNeRF: 3D Reconstruction in Plato&apos;s Cave via Single-View Two-Bounce Lidar. (arXiv:2312.14239v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14239</link>
<description rdf:parseType="Literal">&lt;p&gt;3D reconstruction from a single-view is challenging because of the ambiguity
from monocular cues and lack of information about occluded regions. Neural
radiance fields (NeRF), while popular for view synthesis and 3D reconstruction,
are typically reliant on multi-view images. Existing methods for single-view 3D
reconstruction with NeRF rely on either data priors to hallucinate views of
occluded regions, which may not be physically accurate, or shadows observed by
RGB cameras, which are difficult to detect in ambient light and low albedo
backgrounds. We propose using time-of-flight data captured by a single-photon
avalanche diode to overcome these limitations. Our method models two-bounce
optical paths with NeRF, using lidar transient data for supervision. By
leveraging the advantages of both NeRF and two-bounce light measured by lidar,
we demonstrate that we can reconstruct visible and occluded geometry without
data priors or reliance on controlled ambient lighting or scene albedo. In
addition, we demonstrate improved generalization under practical constraints on
sensor spatial- and temporal-resolution. We believe our method is a promising
direction as single-photon lidars become ubiquitous on consumer devices, such
as phones, tablets, and headsets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klinghoffer_T/0/1/0/all/0/1&quot;&gt;Tzofi Klinghoffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Somasundaram_S/0/1/0/all/0/1&quot;&gt;Siddharth Somasundaram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yuchen Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richardt_C/0/1/0/all/0/1&quot;&gt;Christian Richardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1&quot;&gt;Ramesh Raskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranjan_R/0/1/0/all/0/1&quot;&gt;Rakesh Ranjan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14280">
<title>Fine-grained Forecasting Models Via Gaussian Process Blurring Effect. (arXiv:2312.14280v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.14280</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series forecasting is a challenging task due to the existence of complex
and dynamic temporal dependencies. This can lead to incorrect predictions by
even the best forecasting models. Using more training data is one way to
improve the accuracy, but this source is often limited. In contrast, we are
building on successful denoising approaches for image generation by advocating
for an end-to-end forecasting and denoising paradigm.
&lt;/p&gt;
&lt;p&gt;We propose an end-to-end forecast-blur-denoise forecasting framework by
encouraging a division of labors between the forecasting and the denoising
models. The initial forecasting model is directed to focus on accurately
predicting the coarse-grained behavior, while the denoiser model focuses on
capturing the fine-grained behavior that is locally blurred by integrating a
Gaussian Process model. All three parts are interacting for the best end-to-end
performance. Our extensive experiments demonstrate that our proposed approach
is able to improve the forecasting accuracy of several state-of-the-art
forecasting models as well as several other denoising approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koohfar_S/0/1/0/all/0/1&quot;&gt;Sepideh Koohfar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dietz_L/0/1/0/all/0/1&quot;&gt;Laura Dietz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14301">
<title>Autoencoder Based Face Verification System. (arXiv:2312.14301v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14301</link>
<description rdf:parseType="Literal">&lt;p&gt;The primary objective of this work is to present an alternative approach
aimed at reducing the dependency on labeled data. Our proposed method involves
utilizing autoencoder pre-training within a face image recognition task with
two step processes. Initially, an autoencoder is trained in an unsupervised
manner using a substantial amount of unlabeled training dataset. Subsequently,
a deep learning model is trained with initialized parameters from the
pre-trained autoencoder. This deep learning training process is conducted in a
supervised manner, employing relatively limited labeled training dataset.
During evaluation phase, face image embeddings is generated as the output of
deep neural network layer. Our training is executed on the CelebA dataset,
while evaluation is performed using benchmark face recognition datasets such as
Labeled Faces in the Wild (LFW) and YouTube Faces (YTF). Experimental results
demonstrate that by initializing the deep neural network with pre-trained
autoencoder parameters achieve comparable results to state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solomon_E/0/1/0/all/0/1&quot;&gt;Enoch Solomon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woubie_A/0/1/0/all/0/1&quot;&gt;Abraham Woubie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emiru_E/0/1/0/all/0/1&quot;&gt;Eyael Solomon Emiru&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14373">
<title>Learning Socio-Temporal Graphs for Multi-Agent Trajectory Prediction. (arXiv:2312.14373v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14373</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to predict a pedestrian&apos;s trajectory in a crowd accurately, one has
to take into account her/his underlying socio-temporal interactions with other
pedestrians consistently. Unlike existing work that represents the relevant
information separately, partially, or implicitly, we propose a complete
representation for it to be fully and explicitly captured and analyzed. In
particular, we introduce a Directed Acyclic Graph-based structure, which we
term Socio-Temporal Graph (STG), to explicitly capture pair-wise socio-temporal
interactions among a group of people across both space and time. Our model is
built on a time-varying generative process, whose latent variables determine
the structure of the STGs. We design an attention-based model named STGformer
that affords an end-to-end pipeline to learn the structure of the STGs for
trajectory prediction. Our solution achieves overall state-of-the-art
prediction accuracy in two large-scale benchmark datasets. Our analysis shows
that a person&apos;s past trajectory is critical for predicting another person&apos;s
future path. Our model learns this relationship with a strong notion of
socio-temporal localities. Statistics show that utilizing this information
explicitly for prediction yields a noticeable performance gain with respect to
the trajectory-only approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lixiong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guangyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1&quot;&gt;Ching-Yao Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anzellotti_S/0/1/0/all/0/1&quot;&gt;Stefano Anzellotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1&quot;&gt;Donglai Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14383">
<title>Removing Interference and Recovering Content Imaginatively for Visible Watermark Removal. (arXiv:2312.14383v1 [cs.MM])</title>
<link>http://arxiv.org/abs/2312.14383</link>
<description rdf:parseType="Literal">&lt;p&gt;Visible watermarks, while instrumental in protecting image copyrights,
frequently distort the underlying content, complicating tasks like scene
interpretation and image editing. Visible watermark removal aims to eliminate
the interference of watermarks and restore the background content. However,
existing methods often implement watermark component removal and background
restoration tasks within a singular branch, leading to residual watermarks in
the predictions and ignoring cases where watermarks heavily obscure the
background. To address these limitations, this study introduces the Removing
Interference and Recovering Content Imaginatively (RIRCI) framework. RIRCI
embodies a two-stage approach: the initial phase centers on discerning and
segregating the watermark component, while the subsequent phase focuses on
background content restoration. To achieve meticulous background restoration,
our proposed model employs a dual-path network capable of fully exploring the
intrinsic background information beneath semi-transparent watermarks and
peripheral contextual information from unaffected regions. Moreover, a Global
and Local Context Interaction module is built upon multi-layer perceptrons and
bidirectional feature transformation for comprehensive representation modeling
in the background restoration phase. The efficacy of our approach is
empirically validated across two large-scale datasets, and our findings reveal
a marked enhancement over existing watermark removal techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1&quot;&gt;Yicheng Leng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1&quot;&gt;Chaowei Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yixiang Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanbin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14387">
<title>Variance-insensitive and Target-preserving Mask Refinement for Interactive Image Segmentation. (arXiv:2312.14387v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14387</link>
<description rdf:parseType="Literal">&lt;p&gt;Point-based interactive image segmentation can ease the burden of mask
annotation in applications such as semantic segmentation and image editing.
However, fully extracting the target mask with limited user inputs remains
challenging. We introduce a novel method, Variance-Insensitive and
Target-Preserving Mask Refinement to enhance segmentation quality with fewer
user inputs. Regarding the last segmentation result as the initial mask, an
iterative refinement process is commonly employed to continually enhance the
initial mask. Nevertheless, conventional techniques suffer from sensitivity to
the variance in the initial mask. To circumvent this problem, our proposed
method incorporates a mask matching algorithm for ensuring consistent
inferences from different types of initial masks. We also introduce a
target-aware zooming algorithm to preserve object information during
downsampling, balancing efficiency and accuracy. Experiments on GrabCut,
Berkeley, SBD, and DAVIS datasets demonstrate our method&apos;s state-of-the-art
performance in interactive image segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1&quot;&gt;Chaowei Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Ziyin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junye Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hanjing Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qingyao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanbin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14389">
<title>StyleRetoucher: Generalized Portrait Image Retouching with GAN Priors. (arXiv:2312.14389v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14389</link>
<description rdf:parseType="Literal">&lt;p&gt;Creating fine-retouched portrait images is tedious and time-consuming even
for professional artists. There exist automatic retouching methods, but they
either suffer from over-smoothing artifacts or lack generalization ability. To
address such issues, we present StyleRetoucher, a novel automatic portrait
image retouching framework, leveraging StyleGAN&apos;s generation and generalization
ability to improve an input portrait image&apos;s skin condition while preserving
its facial details. Harnessing the priors of pretrained StyleGAN, our method
shows superior robustness: a). performing stably with fewer training samples
and b). generalizing well on the out-domain data. Moreover, by blending the
spatial features of the input image and intermediate features of the StyleGAN
layers, our method preserves the input characteristics to the largest extent.
We further propose a novel blemish-aware feature selection mechanism to
effectively identify and remove the skin blemishes, improving the image skin
condition. Qualitative and quantitative evaluations validate the great
generalization capability of our method. Further experiments show
StyleRetoucher&apos;s superior performance to the alternative solutions in the image
retouching task. We also conduct a user perceptive study to confirm the
superior retouching performance of our method over the existing
state-of-the-art alternatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Wanchao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Can Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1&quot;&gt;Hangzhou Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Hongbo Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1&quot;&gt;Jing Liao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14395">
<title>Unsupervised Deep Learning Image Verification Method. (arXiv:2312.14395v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14395</link>
<description rdf:parseType="Literal">&lt;p&gt;Although deep learning are commonly employed for image recognition, usually
huge amount of labeled training data is required, which may not always be
readily available. This leads to a noticeable performance disparity when
compared to state-of-the-art unsupervised face verification techniques. In this
work, we propose a method to narrow this gap by leveraging an autoencoder to
convert the face image vector into a novel representation. Notably, the
autoencoder is trained to reconstruct neighboring face image vectors rather
than the original input image vectors. These neighbor face image vectors are
chosen through an unsupervised process based on the highest cosine scores with
the training face image vectors. The proposed method achieves a relative
improvement of 56\% in terms of EER over the baseline system on Labeled Faces
in the Wild (LFW) dataset. This has successfully narrowed down the performance
gap between cosine and PLDA scoring systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solomon_E/0/1/0/all/0/1&quot;&gt;Enoch Solomon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woubie_A/0/1/0/all/0/1&quot;&gt;Abraham Woubie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emiru_E/0/1/0/all/0/1&quot;&gt;Eyael Solomon Emiru&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14400">
<title>Unveiling Backbone Effects in CLIP: Exploring Representational Synergies and Variances. (arXiv:2312.14400v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14400</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive Language-Image Pretraining (CLIP) stands out as a prominent
method for image representation learning. Various neural architectures,
spanning Transformer-based models like Vision Transformers (ViTs) to
Convolutional Networks (ConvNets) like ResNets, are trained with CLIP and serve
as universal backbones across diverse vision tasks. Despite utilizing the same
data and training objectives, the effectiveness of representations learned by
these architectures raises a critical question. Our investigation explores the
differences in CLIP performance among these backbone architectures, revealing
significant disparities in their classifications. Notably, normalizing these
representations results in substantial performance variations. Our findings
showcase a remarkable possible synergy between backbone predictions that could
reach an improvement of over 20% through informed selection of the appropriate
backbone. Moreover, we propose a simple, yet effective approach to combine
predictions from multiple backbones, leading to a notable performance boost of
up to 6.34\%. We will release the code for reproducing the results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_Opazo_C/0/1/0/all/0/1&quot;&gt;Cristian Rodriguez-Opazo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marrese_Taylor_E/0/1/0/all/0/1&quot;&gt;Edison Marrese-Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbasnejad_E/0/1/0/all/0/1&quot;&gt;Ehsan Abbasnejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damirchi_H/0/1/0/all/0/1&quot;&gt;Hamed Damirchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jara_I/0/1/0/all/0/1&quot;&gt;Ignacio M. Jara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bravo_Marquez_F/0/1/0/all/0/1&quot;&gt;Felipe Bravo-Marquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1&quot;&gt;Anton van den Hengel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14404">
<title>Cross-Covariate Gait Recognition: A Benchmark. (arXiv:2312.14404v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14404</link>
<description rdf:parseType="Literal">&lt;p&gt;Gait datasets are essential for gait research. However, this paper observes
that present benchmarks, whether conventional constrained or emerging
real-world datasets, fall short regarding covariate diversity. To bridge this
gap, we undertake an arduous 20-month effort to collect a cross-covariate gait
recognition (CCGR) dataset. The CCGR dataset has 970 subjects and about 1.6
million sequences; almost every subject has 33 views and 53 different
covariates. Compared to existing datasets, CCGR has both population and
individual-level diversity. In addition, the views and covariates are well
labeled, enabling the analysis of the effects of different factors. CCGR
provides multiple types of gait data, including RGB, parsing, silhouette, and
pose, offering researchers a comprehensive resource for exploration. In order
to delve deeper into addressing cross-covariate gait recognition, we propose
parsing-based gait recognition (ParsingGait) by utilizing the newly proposed
parsing data. We have conducted extensive experiments. Our main results show:
1) Cross-covariate emerges as a pivotal challenge for practical applications of
gait recognition. 2) ParsingGait demonstrates remarkable potential for further
advancement. 3) Alarmingly, existing SOTA methods achieve less than 43%
accuracy on the CCGR, highlighting the urgency of exploring cross-covariate
gait recognition. Link: https://github.com/ShinanZou/CCGR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1&quot;&gt;Shinan Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Chao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1&quot;&gt;Jianbo Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chuanfu Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shiqi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jin Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14407">
<title>AdvCloak: Customized Adversarial Cloak for Privacy Protection. (arXiv:2312.14407v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14407</link>
<description rdf:parseType="Literal">&lt;p&gt;With extensive face images being shared on social media, there has been a
notable escalation in privacy concerns. In this paper, we propose AdvCloak, an
innovative framework for privacy protection using generative models. AdvCloak
is designed to automatically customize class-wise adversarial masks that can
maintain superior image-level naturalness while providing enhanced
feature-level generalization ability. Specifically, AdvCloak sequentially
optimizes the generative adversarial networks by employing a two-stage training
strategy. This strategy initially focuses on adapting the masks to the unique
individual faces via image-specific training and then enhances their
feature-level generalization ability to diverse facial variations of
individuals via person-specific training. To fully utilize the limited training
data, we combine AdvCloak with several general geometric modeling methods, to
better describe the feature subspace of source identities. Extensive
quantitative and qualitative evaluations on both common and celebrity datasets
demonstrate that AdvCloak outperforms existing state-of-the-art methods in
terms of efficiency and effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuannan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yaoyao Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1&quot;&gt;Xing Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuhang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peipei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1&quot;&gt;Weihong Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14410">
<title>A Multi-Stage Adaptive Feature Fusion Neural Network for Multimodal Gait Recognition. (arXiv:2312.14410v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14410</link>
<description rdf:parseType="Literal">&lt;p&gt;Gait recognition is a biometric technology that has received extensive
attention. Most existing gait recognition algorithms are unimodal, and a few
multimodal gait recognition algorithms perform multimodal fusion only once.
None of these algorithms may fully exploit the complementary advantages of the
multiple modalities. In this paper, by considering the temporal and spatial
characteristics of gait data, we propose a multi-stage feature fusion strategy
(MSFFS), which performs multimodal fusions at different stages in the feature
extraction process. Also, we propose an adaptive feature fusion module (AFFM)
that considers the semantic association between silhouettes and skeletons. The
fusion process fuses different silhouette areas with their more related
skeleton joints. Since visual appearance changes and time passage co-occur in a
gait period, we propose a multiscale spatial-temporal feature extractor
(MSSTFE) to learn the spatial-temporal linkage features thoroughly.
Specifically, MSSTFE extracts and aggregates spatial-temporal linkages
information at different spatial scales. Combining the strategy and modules
mentioned above, we propose a multi-stage adaptive feature fusion (MSAFF)
neural network, which shows state-of-the-art performance in many experiments on
three datasets. Besides, MSAFF is equipped with feature dimensional pooling (FD
Pooling), which can significantly reduce the dimension of the gait
representations without hindering the accuracy.
https://github.com/ShinanZou/MSAFF
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1&quot;&gt;Shinan Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1&quot;&gt;Jianbo Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Chao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shiqi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jin Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14427">
<title>GROOD: GRadient-aware Out-Of-Distribution detection in interpolated manifolds. (arXiv:2312.14427v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14427</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) often fail silently with over-confident
predictions on out-of-distribution (OOD) samples, posing risks in real-world
deployments. Existing techniques predominantly emphasize either the feature
representation space or the gradient norms computed with respect to DNN
parameters, yet they overlook the intricate gradient distribution and the
topology of classification regions. To address this gap, we introduce
GRadient-aware Out-Of-Distribution detection in interpolated manifolds (GROOD),
a novel framework that relies on the discriminative power of gradient space to
distinguish between in-distribution (ID) and OOD samples. To build this space,
GROOD relies on class prototypes together with a prototype that specifically
captures OOD characteristics. Uniquely, our approach incorporates a targeted
mix-up operation at an early intermediate layer of the DNN to refine the
separation of gradient spaces between ID and OOD samples. We quantify OOD
detection efficacy using the distance to the nearest neighbor gradients derived
from the training set, yielding a robust OOD score. Experimental evaluations
substantiate that the introduction of targeted input mix-upamplifies the
separation between ID and OOD in the gradient space, yielding impressive
results across diverse datasets. Notably, when benchmarked against ImageNet-1k,
GROOD surpasses the established robustness of state-of-the-art baselines.
Through this work, we establish the utility of leveraging gradient spaces and
class prototypes for enhanced OOD detection for DNN in image classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ElAraby_M/0/1/0/all/0/1&quot;&gt;Mostafa ElAraby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1&quot;&gt;Sabyasachi Sahoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pequignot_Y/0/1/0/all/0/1&quot;&gt;Yann Pequignot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Novello_P/0/1/0/all/0/1&quot;&gt;Paul Novello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paull_L/0/1/0/all/0/1&quot;&gt;Liam Paull&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14432">
<title>Scalable 3D Reconstruction From Single Particle X-Ray Diffraction Images Based on Online Machine Learning. (arXiv:2312.14432v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14432</link>
<description rdf:parseType="Literal">&lt;p&gt;X-ray free-electron lasers (XFELs) offer unique capabilities for measuring
the structure and dynamics of biomolecules, helping us understand the basic
building blocks of life. Notably, high-repetition-rate XFELs enable single
particle imaging (X-ray SPI) where individual, weakly scattering biomolecules
are imaged under near-physiological conditions with the opportunity to access
fleeting states that cannot be captured in cryogenic or crystallized
conditions. Existing X-ray SPI reconstruction algorithms, which estimate the
unknown orientation of a particle in each captured image as well as its shared
3D structure, are inadequate in handling the massive datasets generated by
these emerging XFELs. Here, we introduce X-RAI, an online reconstruction
framework that estimates the structure of a 3D macromolecule from large X-ray
SPI datasets. X-RAI consists of a convolutional encoder, which amortizes pose
estimation over large datasets, as well as a physics-based decoder, which
employs an implicit neural representation to enable high-quality 3D
reconstruction in an end-to-end, self-supervised manner. We demonstrate that
X-RAI achieves state-of-the-art performance for small-scale datasets in
simulation and challenging experimental settings and demonstrate its
unprecedented ability to process large datasets containing millions of
diffraction images in an online fashion. These abilities signify a paradigm
shift in X-ray SPI towards real-time capture and reconstruction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shenoy_J/0/1/0/all/0/1&quot;&gt;Jay Shenoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_A/0/1/0/all/0/1&quot;&gt;Axel Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poitevin_F/0/1/0/all/0/1&quot;&gt;Fr&amp;#xe9;d&amp;#xe9;ric Poitevin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1&quot;&gt;Gordon Wetzstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14446">
<title>Cross-Modal Object Tracking via Modality-Aware Fusion Network and A Large-Scale Dataset. (arXiv:2312.14446v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14446</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual tracking often faces challenges such as invalid targets and decreased
performance in low-light conditions when relying solely on RGB image sequences.
While incorporating additional modalities like depth and infrared data has
proven effective, existing multi-modal imaging platforms are complex and lack
real-world applicability. In contrast, near-infrared (NIR) imaging, commonly
used in surveillance cameras, can switch between RGB and NIR based on light
intensity. However, tracking objects across these heterogeneous modalities
poses significant challenges, particularly due to the absence of modality
switch signals during tracking. To address these challenges, we propose an
adaptive cross-modal object tracking algorithm called Modality-Aware Fusion
Network (MAFNet). MAFNet efficiently integrates information from both RGB and
NIR modalities using an adaptive weighting mechanism, effectively bridging the
appearance gap and enabling a modality-aware target representation. It consists
of two key components: an adaptive weighting module and a modality-specific
representation module......
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Cheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenglong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jin Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14457">
<title>QUAR-VLA: Vision-Language-Action Model for Quadruped Robots. (arXiv:2312.14457v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.14457</link>
<description rdf:parseType="Literal">&lt;p&gt;The important manifestation of robot intelligence is the ability to naturally
interact and autonomously make decisions. Traditional approaches to robot
control often compartmentalize perception, planning, and decision-making,
simplifying system design but limiting the synergy between different
information streams. This compartmentalization poses challenges in achieving
seamless autonomous reasoning, decision-making, and action execution. To
address these limitations, a novel paradigm, named Vision-Language-Action tasks
for QUAdruped Robots (QUAR-VLA), has been introduced in this paper. This
approach tightly integrates visual information and instructions to generate
executable actions, effectively merging perception, planning, and
decision-making. The central idea is to elevate the overall intelligence of the
robot. Within this framework, a notable challenge lies in aligning fine-grained
instructions with visual perception information. This emphasizes the complexity
involved in ensuring that the robot accurately interprets and acts upon
detailed instructions in harmony with its visual observations. Consequently, we
propose QUAdruped Robotic Transformer (QUART), a family of VLA models to
integrate visual information and instructions from diverse modalities as input
and generates executable actions for real-world robots and present QUAdruped
Robot Dataset (QUARD), a large-scale multi-task dataset including navigation,
complex terrain locomotion, and whole-body manipulation tasks for training
QUART models. Our extensive evaluation (4000 evaluation trials) shows that our
approach leads to performant robotic policies and enables QUART to obtain a
range of emergent capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1&quot;&gt;Pengxiang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Han Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhitao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Shangke Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Donglin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14465">
<title>FM-OV3D: Foundation Model-based Cross-modal Knowledge Blending for Open-Vocabulary 3D Detection. (arXiv:2312.14465v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14465</link>
<description rdf:parseType="Literal">&lt;p&gt;The superior performances of pre-trained foundation models in various visual
tasks underscore their potential to enhance the 2D models&apos; open-vocabulary
ability. Existing methods explore analogous applications in the 3D space.
However, most of them only center around knowledge extraction from singular
foundation models, which limits the open-vocabulary ability of 3D models. We
hypothesize that leveraging complementary pre-trained knowledge from various
foundation models can improve knowledge transfer from 2D pre-trained visual
language models to the 3D space. In this work, we propose FM-OV3D, a method of
Foundation Model-based Cross-modal Knowledge Blending for Open-Vocabulary 3D
Detection, which improves the open-vocabulary localization and recognition
abilities of 3D model by blending knowledge from multiple pre-trained
foundation models, achieving true open-vocabulary without facing constraints
from original 3D datasets. Specifically, to learn the open-vocabulary 3D
localization ability, we adopt the open-vocabulary localization knowledge of
the Grounded-Segment-Anything model. For open-vocabulary 3D recognition
ability, We leverage the knowledge of generative foundation models, including
GPT-3 and Stable Diffusion models, and cross-modal discriminative models like
CLIP. The experimental results on two popular benchmarks for open-vocabulary 3D
object detection show that our model efficiently learns knowledge from multiple
foundation models to enhance the open-vocabulary ability of the 3D model and
successfully achieves state-of-the-art performance in open-vocabulary 3D object
detection tasks. Code is released at
https://github.com/dmzhang0425/FM-OV3D.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dongmei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ray Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Shenghao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1&quot;&gt;Wei Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaodong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shanghang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14471">
<title>Prototype-based Cross-Modal Object Tracking. (arXiv:2312.14471v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14471</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-modal object tracking is an important research topic in the field of
information fusion, and it aims to address imaging limitations in challenging
scenarios by integrating switchable visible and near-infrared modalities.
However, existing tracking methods face some difficulties in adapting to
significant target appearance variations in the presence of modality switch.
For instance, model update based tracking methods struggle to maintain stable
tracking results during modality switching, leading to error accumulation and
model drift. Template based tracking methods solely rely on the template
information from first frame and/or last frame, which lacks sufficient
representation ability and poses challenges in handling significant target
appearance changes. To address this problem, we propose a prototype-based
cross-modal object tracker called ProtoTrack, which introduces a novel
prototype learning scheme to adapt to significant target appearance variations,
for cross-modal object tracking. In particular, we design a multi-modal
prototype to represent target information by multi-kind samples, including a
fixed sample from the first frame and two representative samples from different
modalities. Moreover, we develop a prototype generation algorithm based on two
new modules to ensure the prototype representative in different
challenges......
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenglong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Futian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Longfeng Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jin Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14474">
<title>MonoLSS: Learnable Sample Selection For Monocular 3D Detection. (arXiv:2312.14474v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14474</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of autonomous driving, monocular 3D detection is a critical task
which estimates 3D properties (depth, dimension, and orientation) of objects in
a single RGB image. Previous works have used features in a heuristic way to
learn 3D properties, without considering that inappropriate features could have
adverse effects. In this paper, sample selection is introduced that only
suitable samples should be trained to regress the 3D properties. To select
samples adaptively, we propose a Learnable Sample Selection (LSS) module, which
is based on Gumbel-Softmax and a relative-distance sample divider. The LSS
module works under a warm-up strategy leading to an improvement in training
stability. Additionally, since the LSS module dedicated to 3D property sample
selection relies on object-level features, we further develop a data
augmentation method named MixUp3D to enrich 3D property samples which conforms
to imaging principles without introducing ambiguity. As two orthogonal methods,
the LSS module and MixUp3D can be utilized independently or in conjunction.
Sufficient experiments have shown that their combined use can lead to
synergistic effects, yielding improvements that transcend the mere sum of their
individual applications. Leveraging the LSS module and the MixUp3D, without any
extra data, our method named MonoLSS ranks 1st in all three categories (Car,
Cyclist, and Pedestrian) on KITTI 3D object detection benchmark, and achieves
competitive results on both the Waymo dataset and KITTI-nuScenes cross-dataset
evaluation. The code is included in the supplementary material and will be
released to facilitate related academic and industrial studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenjia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jinrang Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yifeng Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14481">
<title>Part to Whole: Collaborative Prompting for Surgical Instrument Segmentation. (arXiv:2312.14481v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14481</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation models like the Segment Anything Model (SAM) have demonstrated
promise in generic object segmentation. However, directly applying SAM to
surgical instrument segmentation presents key challenges. First, SAM relies on
per-frame point-or-box prompts which complicate surgeon-computer interaction.
Also, SAM yields suboptimal performance on segmenting surgical instruments,
owing to insufficient surgical data in its pre-training as well as the complex
structure and fine-grained details of various surgical instruments. To address
these challenges, in this paper, we investigate text promptable surgical
instrument segmentation and propose SP-SAM (SurgicalPart-SAM), a novel
efficient-tuning approach that integrates surgical instrument structure
knowledge with the generic segmentation knowledge of SAM. Specifically, we
achieve this by proposing (1) collaborative prompts in the text form &quot;[part
name] of [instrument category name]&quot; that decompose instruments into
fine-grained parts; (2) a Cross-Modal Prompt Encoder that encodes text prompts
jointly with visual embeddings into discriminative part-level representations;
and (3) a Part-to-Whole Selective Fusion and a Hierarchical Decoding strategy
that selectively assemble the part-level representations into a whole for
accurate instrument segmentation. Built upon them, SP-SAM acquires a better
capability to comprehend surgical instrument structures and distinguish between
various categories. Extensive experiments on both the EndoVis2018 and
EndoVis2017 datasets demonstrate SP-SAM&apos;s state-of-the-art performance with
minimal tunable parameters. Code is at
https://github.com/wenxi-yue/SurgicalPart-SAM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_W/0/1/0/all/0/1&quot;&gt;Wenxi Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1&quot;&gt;Kun Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qiuxia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1&quot;&gt;Zongyuan Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yong Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jiebo Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14492">
<title>Context Enhanced Transformer for Single Image Object Detection. (arXiv:2312.14492v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14492</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing importance of video data in real-world applications,
there is a rising need for efficient object detection methods that utilize
temporal information. While existing video object detection (VOD) techniques
employ various strategies to address this challenge, they typically depend on
locally adjacent frames or randomly sampled images within a clip. Although
recent Transformer-based VOD methods have shown promising results, their
reliance on multiple inputs and additional network complexity to incorporate
temporal information limits their practical applicability. In this paper, we
propose a novel approach to single image object detection, called Context
Enhanced TRansformer (CETR), by incorporating temporal context into DETR using
a newly designed memory module. To efficiently store temporal information, we
construct a class-wise memory that collects contextual information across data.
Additionally, we present a classification-based sampling technique to
selectively utilize the relevant memory for the current image. In the testing,
We introduce a test-time memory adaptation method that updates individual
memory functions by considering the test distribution. Experiments with CityCam
and ImageNet VID datasets exhibit the efficiency of the framework on various
video systems. The project page and code will be made available at:
https://ku-cvlab.github.io/CETR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1&quot;&gt;Seungjun An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Seonghoon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1&quot;&gt;Gyeongnyeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1&quot;&gt;Jeongyeol Baek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1&quot;&gt;Byeongwon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seungryong Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14494">
<title>Revisiting Few-Shot Object Detection with Vision-Language Models. (arXiv:2312.14494v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14494</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot object detection (FSOD) benchmarks have advanced techniques for
detecting new categories with limited annotations. Existing benchmarks
repurpose well-established datasets like COCO by partitioning categories into
base and novel classes for pre-training and fine-tuning respectively. However,
these benchmarks do not reflect how FSOD is deployed in practice. Rather than
only pre-training on a small number of base categories, we argue that it is
more practical to fine-tune a foundation model (e.g., a vision-language model
(VLM) pre-trained on web-scale data) for a target domain. Surprisingly, we find
that zero-shot inference from VLMs like GroundingDINO significantly outperforms
the state-of-the-art (48.3 vs. 33.1 AP) on COCO. However, such zero-shot models
can still be misaligned to target concepts of interest. For example, trailers
on the web may be different from trailers in the context of autonomous
vehicles. In this work, we propose Foundational FSOD, a new benchmark protocol
that evaluates detectors pre-trained on any external datasets and fine-tuned on
K-shots per target class. Further, we note that current FSOD benchmarks are
actually federated datasets containing exhaustive annotations for each category
on a subset of the data. We leverage this insight to propose simple strategies
for fine-tuning VLMs with federated losses. We demonstrate the effectiveness of
our approach on LVIS and nuImages, improving over prior work by 5.9 AP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madan_A/0/1/0/all/0/1&quot;&gt;Anish Madan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peri_N/0/1/0/all/0/1&quot;&gt;Neehar Peri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1&quot;&gt;Shu Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1&quot;&gt;Deva Ramanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14502">
<title>ViStripformer: A Token-Efficient Transformer for Versatile Video Restoration. (arXiv:2312.14502v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14502</link>
<description rdf:parseType="Literal">&lt;p&gt;Video restoration is a low-level vision task that seeks to restore clean,
sharp videos from quality-degraded frames. One would use the temporal
information from adjacent frames to make video restoration successful.
Recently, the success of the Transformer has raised awareness in the
computer-vision community. However, its self-attention mechanism requires much
memory, which is unsuitable for high-resolution vision tasks like video
restoration. In this paper, we propose ViStripformer (Video Stripformer), which
utilizes spatio-temporal strip attention to catch long-range data correlations,
consisting of intra-frame strip attention (Intra-SA) and inter-frame strip
attention (Inter-SA) for extracting spatial and temporal information. It
decomposes video frames into strip-shaped features in horizontal and vertical
directions for Intra-SA and Inter-SA to address degradation patterns with
various orientations and magnitudes. Besides, ViStripformer is an effective and
efficient transformer architecture with much lower memory usage than the
vanilla transformer. Extensive experiments show that the proposed model
achieves superior results with fast inference time on video restoration tasks,
including video deblurring, demoireing, and deraining.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsai_F/0/1/0/all/0/1&quot;&gt;Fu-Jen Tsai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yan-Tsung Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Chen-Yu Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chan-Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yen-Yu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsai_C/0/1/0/all/0/1&quot;&gt;Chung-Chi Tsai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chia-Wen Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14518">
<title>Joint Learning Neuronal Skeleton and Brain Circuit Topology with Permutation Invariant Encoders for Neuron Classification. (arXiv:2312.14518v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2312.14518</link>
<description rdf:parseType="Literal">&lt;p&gt;Determining the types of neurons within a nervous system plays a significant
role in the analysis of brain connectomics and the investigation of
neurological diseases. However, the efficiency of utilizing anatomical,
physiological, or molecular characteristics of neurons is relatively low and
costly. With the advancements in electron microscopy imaging and analysis
techniques for brain tissue, we are able to obtain whole-brain connectome
consisting neuronal high-resolution morphology and connectivity information.
However, few models are built based on such data for automated neuron
classification. In this paper, we propose NeuNet, a framework that combines
morphological information of neurons obtained from skeleton and topological
information between neurons obtained from neural circuit. Specifically, NeuNet
consists of three components, namely Skeleton Encoder, Connectome Encoder, and
Readout Layer. Skeleton Encoder integrates the local information of neurons in
a bottom-up manner, with a one-dimensional convolution in neural skeleton&apos;s
point data; Connectome Encoder uses a graph neural network to capture the
topological information of neural circuit; finally, Readout Layer fuses the
above two information and outputs classification results. We reprocess and
release two new datasets for neuron classification task from volume electron
microscopy(VEM) images of human brain cortex and Drosophila brain. Experiments
on these two datasets demonstrated the effectiveness of our model with accuracy
of 0.9169 and 0.9363, respectively. Code and data are available at:
https://github.com/WHUminghui/NeuNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liao_M/0/1/0/all/0/1&quot;&gt;Minghui Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wan_G/0/1/0/all/0/1&quot;&gt;Guojia Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bo Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14544">
<title>Inclusive normalization of face images to passport format. (arXiv:2312.14544v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14544</link>
<description rdf:parseType="Literal">&lt;p&gt;Face recognition has been used more and more in real world applications in
recent years. However, when the skin color bias is coupled with intra-personal
variations like harsh illumination, the face recognition task is more likely to
fail, even during human inspection. Face normalization methods try to deal with
such challenges by removing intra-personal variations from an input image while
keeping the identity the same. However, most face normalization methods can
only remove one or two variations and ignore dataset biases such as skin color
bias. The outputs of many face normalization methods are also not realistic to
human observers. In this work, a style based face normalization model
(StyleFNM) is proposed to remove most intra-personal variations including large
changes in pose, bad or harsh illumination, low resolution, blur, facial
expressions, and accessories like sunglasses among others. The dataset bias is
also dealt with in this paper by controlling a pretrained GAN to generate a
balanced dataset of passport-like images. The experimental results show that
StyleFNM can generate more realistic outputs and can improve significantly the
accuracy and fairness of face recognition systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1&quot;&gt;Hongliu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1&quot;&gt;Minh Nhat Do&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravanel_A/0/1/0/all/0/1&quot;&gt;Alexis Ravanel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomas_E/0/1/0/all/0/1&quot;&gt;Eoin Thomas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14556">
<title>CaptainCook4D: A dataset for understanding errors in procedural activities. (arXiv:2312.14556v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14556</link>
<description rdf:parseType="Literal">&lt;p&gt;Following step-by-step procedures is an essential component of various
activities carried out by individuals in their daily lives. These procedures
serve as a guiding framework that helps to achieve goals efficiently, whether
it is assembling furniture or preparing a recipe. However, the complexity and
duration of procedural activities inherently increase the likelihood of making
errors. Understanding such procedural activities from a sequence of frames is a
challenging task that demands an accurate interpretation of visual information
and the ability to reason about the structure of the activity. To this end, we
collect a new egocentric 4D dataset, CaptainCook4D, comprising 384 recordings
(94.5 hours) of people performing recipes in real kitchen environments. This
dataset consists of two distinct types of activity: one in which participants
adhere to the provided recipe instructions and another in which they deviate
and induce errors. We provide 5.3K step annotations and 10K fine-grained action
annotations and benchmark the dataset for the following tasks: supervised error
recognition, multistep localization, and procedure learning
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peddi_R/0/1/0/all/0/1&quot;&gt;Rohith Peddi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arya_S/0/1/0/all/0/1&quot;&gt;Shivvrat Arya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Challa_B/0/1/0/all/0/1&quot;&gt;Bharath Challa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pallapothula_L/0/1/0/all/0/1&quot;&gt;Likhitha Pallapothula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vyas_A/0/1/0/all/0/1&quot;&gt;Akshay Vyas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jikai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komaragiri_V/0/1/0/all/0/1&quot;&gt;Vasundhara Komaragiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ragan_E/0/1/0/all/0/1&quot;&gt;Eric Ragan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruozzi_N/0/1/0/all/0/1&quot;&gt;Nicholas Ruozzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1&quot;&gt;Yu Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gogate_V/0/1/0/all/0/1&quot;&gt;Vibhav Gogate&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14570">
<title>BSS-Bench: Towards Reproducible and Effective Band Selection Search. (arXiv:2312.14570v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14570</link>
<description rdf:parseType="Literal">&lt;p&gt;The key technology to overcome the drawbacks of hyperspectral imaging
(expensive, high capture delay, and low spatial resolution) and make it widely
applicable is to select only a few representative bands from hundreds of bands.
However, current band selection (BS) methods face challenges in fair
comparisons due to inconsistent train/validation settings, including the number
of bands, dataset splits, and retraining settings. To make BS methods easy and
reproducible, this paper presents the first band selection search benchmark
(BSS-Bench) containing 52k training and evaluation records of numerous band
combinations (BC) with different backbones for various hyperspectral analysis
tasks. The creation of BSS-Bench required a significant computational effort of
1.26k GPU days. By querying BSS-Bench, BS experiments can be performed easily
and reproducibly, and the gap between the searched result and the best
achievable performance can be measured. Based on BSS-Bench, we further discuss
the impact of various factors on BS, such as the number of bands, unsupervised
statistics, and different backbones. In addition to BSS-Bench, we present an
effective one-shot BS method called Single Combination One Shot (SCOS), which
learns the priority of any BCs through one-time training, eliminating the need
for repetitive retraining on different BCs. Furthermore, the search process of
SCOS is flexible and does not require training, making it efficient and
effective. Our extensive evaluations demonstrate that SCOS outperforms current
BS methods on multiple tasks, even with much fewer bands. Our BSS-Bench and
codes are available in the supplementary material and will be publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wenshuai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhenbo Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14574">
<title>MMGPL: Multimodal Medical Data Analysis with Graph Prompt Learning. (arXiv:2312.14574v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14574</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt learning has demonstrated impressive efficacy in the fine-tuning of
multimodal large models to a wide range of downstream tasks. Nonetheless,
applying existing prompt learning methods for the diagnosis of neurological
disorder still suffers from two issues: (i) existing methods typically treat
all patches equally, despite the fact that only a small number of patches in
neuroimaging are relevant to the disease, and (ii) they ignore the structural
information inherent in the brain connection network which is crucial for
understanding and diagnosing neurological disorders. To tackle these issues, we
introduce a novel prompt learning model by learning graph prompts during the
fine-tuning process of multimodal large models for diagnosing neurological
disorders. Specifically, we first leverage GPT-4 to obtain relevant disease
concepts and compute semantic similarity between these concepts and all
patches. Secondly, we reduce the weight of irrelevant patches according to the
semantic similarity between each patch and disease-related concepts. Moreover,
we construct a graph among tokens based on these concepts and employ a graph
convolutional network layer to extract the structural information of the graph,
which is used to prompt the pre-trained multimodal large models for diagnosing
neurological disorders. Extensive experiments demonstrate that our method
achieves superior performance for neurological disorder diagnosis compared with
state-of-the-art methods and validated by clinicians.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1&quot;&gt;Liang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1&quot;&gt;Songyue Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zongqian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_H/0/1/0/all/0/1&quot;&gt;Huifang Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14577">
<title>PoseViNet: Distracted Driver Action Recognition Framework Using Multi-View Pose Estimation and Vision Transformer. (arXiv:2312.14577v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14577</link>
<description rdf:parseType="Literal">&lt;p&gt;Driver distraction is a principal cause of traffic accidents. In a study
conducted by the National Highway Traffic Safety Administration, engaging in
activities such as interacting with in-car menus, consuming food or beverages,
or engaging in telephonic conversations while operating a vehicle can be
significant sources of driver distraction. From this viewpoint, this paper
introduces a novel method for detection of driver distraction using multi-view
driver action images. The proposed method is a vision transformer-based
framework with pose estimation and action inference, namely PoseViNet. The
motivation for adding posture information is to enable the transformer to focus
more on key features. As a result, the framework is more adept at identifying
critical actions. The proposed framework is compared with various
state-of-the-art models using SFD3 dataset representing 10 behaviors of
drivers. It is found from the comparison that the PoseViNet outperforms these
models. The proposed framework is also evaluated with the SynDD1 dataset
representing 16 behaviors of driver. As a result, the PoseViNet achieves 97.55%
validation accuracy and 90.92% testing accuracy with the challenging dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengar_N/0/1/0/all/0/1&quot;&gt;Neha Sengar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumari_I/0/1/0/all/0/1&quot;&gt;Indra Kumari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jihui Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Har_D/0/1/0/all/0/1&quot;&gt;Dongsoo Har&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14579">
<title>Environment-Specific People. (arXiv:2312.14579v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14579</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite significant progress in generative image synthesis and full-body
generation in particular, state-of-the-art methods are either
context-independent, overly reliant to text prompts, or bound to the curated
training datasets, such as fashion images with monotonous backgrounds. Here,
our goal is to generate people in clothing that is semantically appropriate for
a given scene. To this end, we present ESP, a novel method for context-aware
full-body generation, that enables photo-realistic inpainting of people into
existing &quot;in-the-wild&quot; photographs. ESP is conditioned on a 2D pose and
contextual cues that are extracted from the environment photograph and
integrated into the generation process. Our models are trained on a dataset
containing a set of in-the-wild photographs of people covering a wide range of
different environments. The method is analyzed quantitatively and
qualitatively, and we show that ESP outperforms state-of-the-art on the task of
contextual full-body generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ostrek_M/0/1/0/all/0/1&quot;&gt;Mirela Ostrek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1&quot;&gt;Soubhik Sanyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OSullivan_C/0/1/0/all/0/1&quot;&gt;Carol O&amp;#x27;Sullivan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1&quot;&gt;Michael J. Black&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1&quot;&gt;Justus Thies&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14606">
<title>Explainable Multi-Camera 3D Object Detection with Transformer-Based Saliency Maps. (arXiv:2312.14606v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14606</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) have achieved state-of-the-art results on various
computer vision tasks, including 3D object detection. However, their end-to-end
implementation also makes ViTs less explainable, which can be a challenge for
deploying them in safety-critical applications, such as autonomous driving,
where it is important for authorities, developers, and users to understand the
model&apos;s reasoning behind its predictions. In this paper, we propose a novel
method for generating saliency maps for a DetR-like ViT with multiple camera
inputs used for 3D object detection. Our method is based on the raw attention
and is more efficient than gradient-based methods. We evaluate the proposed
method on the nuScenes dataset using extensive perturbation tests and show that
it outperforms other explainability methods in terms of visual quality and
quantitative metrics. We also demonstrate the importance of aggregating
attention across different layers of the transformer. Our work contributes to
the development of explainable AI for ViTs, which can help increase trust in AI
applications by establishing more transparency regarding the inner workings of
AI models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beemelmanns_T/0/1/0/all/0/1&quot;&gt;Till Beemelmanns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zahr_W/0/1/0/all/0/1&quot;&gt;Wassim Zahr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eckstein_L/0/1/0/all/0/1&quot;&gt;Lutz Eckstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14611">
<title>Tuning-Free Inversion-Enhanced Control for Consistent Image Editing. (arXiv:2312.14611v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14611</link>
<description rdf:parseType="Literal">&lt;p&gt;Consistent editing of real images is a challenging task, as it requires
performing non-rigid edits (e.g., changing postures) to the main objects in the
input image without changing their identity or attributes. To guarantee
consistent attributes, some existing methods fine-tune the entire model or the
textual embedding for structural consistency, but they are time-consuming and
fail to perform non-rigid edits. Other works are tuning-free, but their
performances are weakened by the quality of Denoising Diffusion Implicit Model
(DDIM) reconstruction, which often fails in real-world scenarios. In this
paper, we present a novel approach called Tuning-free Inversion-enhanced
Control (TIC), which directly correlates features from the inversion process
with those from the sampling process to mitigate the inconsistency in DDIM
reconstruction. Specifically, our method effectively obtains inversion features
from the key and value features in the self-attention layers, and enhances the
sampling process by these inversion features, thus achieving accurate
reconstruction and content-consistent editing. To extend the applicability of
our method to general editing scenarios, we also propose a mask-guided
attention concatenation strategy that combines contents from both the inversion
and the naive DDIM editing processes. Experiments show that the proposed method
outperforms previous works in reconstruction and consistent editing, and
produces impressive results in various settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_X/0/1/0/all/0/1&quot;&gt;Xiaoyue Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1&quot;&gt;Shuhao Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1&quot;&gt;Guoliang Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Baochang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1&quot;&gt;Zhengcong Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1&quot;&gt;Mingyuan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Junshi Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14619">
<title>Towards Loose-Fitting Garment Animation via Generative Model of Deformation Decomposition. (arXiv:2312.14619v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14619</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing data-driven methods for garment animation, usually driven by linear
skinning, although effective on tight garments, do not handle loose-fitting
garments with complex deformations well. To address these limitations, we
develop a garment generative model based on deformation decomposition to
efficiently simulate loose garment deformation without directly using linear
skinning. Specifically, we learn a garment generative space with the proposed
generative model, where we decouple the latent representation into unposed
deformed garments and dynamic offsets during the decoding stage. With explicit
garment deformations decomposition, our generative model is able to generate
complex pose-driven deformations on canonical garment shapes. Furthermore, we
learn to transfer the body motions and previous state of the garment to the
latent space to regenerate dynamic results. In addition, we introduce a detail
enhancement module in an adversarial training setup to learn high-frequency
wrinkles. We demonstrate our method outperforms state-of-the-art data-driven
alternatives through extensive experiments and show qualitative and
quantitative analysis of results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yifu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoxia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhiling Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wei Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14626">
<title>DSAP: Analyzing Bias Through Demographic Comparison of Datasets. (arXiv:2312.14626v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14626</link>
<description rdf:parseType="Literal">&lt;p&gt;In the last few years, Artificial Intelligence systems have become
increasingly widespread. Unfortunately, these systems can share many biases
with human decision-making, including demographic biases. Often, these biases
can be traced back to the data used for training, where large uncurated
datasets have become the norm. Despite our knowledge of these biases, we still
lack general tools to detect and quantify them, as well as to compare the
biases in different datasets. Thus, in this work, we propose DSAP (Demographic
Similarity from Auxiliary Profiles), a two-step methodology for comparing the
demographic composition of two datasets. DSAP can be deployed in three key
applications: to detect and characterize demographic blind spots and bias
issues across datasets, to measure dataset demographic bias in single datasets,
and to measure dataset demographic shift in deployment scenarios. An essential
feature of DSAP is its ability to robustly analyze datasets without explicit
demographic labels, offering simplicity and interpretability for a wide range
of situations. To show the usefulness of the proposed methodology, we consider
the Facial Expression Recognition task, where demographic bias has previously
been found. The three applications are studied over a set of twenty datasets
with varying properties. The code is available at
https://github.com/irisdominguez/DSAP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dominguez_Catena_I/0/1/0/all/0/1&quot;&gt;Iris Dominguez-Catena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paternain_D/0/1/0/all/0/1&quot;&gt;Daniel Paternain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galar_M/0/1/0/all/0/1&quot;&gt;Mikel Galar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14630">
<title>A Language-based solution to enable Metaverse Retrieval. (arXiv:2312.14630v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14630</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the Metaverse is becoming increasingly attractive, with millions of
users accessing the many available virtual worlds. However, how do users find
the one Metaverse which best fits their current interests? So far, the search
process is mostly done by word of mouth, or by advertisement on
technology-oriented websites. However, the lack of search engines similar to
those available for other multimedia formats (e.g., YouTube for videos) is
showing its limitations, since it is often cumbersome to find a Metaverse based
on some specific interests using the available methods, while also making it
difficult to discover user-created ones which lack strong advertisement. To
address this limitation, we propose to use language to naturally describe the
desired contents of the Metaverse a user wishes to find. Second, we highlight
that, differently from more conventional 3D scenes, Metaverse scenarios
represent a more complex data format since they often contain one or more types
of multimedia which influence the relevance of the scenario itself to a user
query. Therefore, in this work, we create a novel task, called
Text-to-Metaverse retrieval, which aims at modeling these aspects while also
taking the cross-modal relations with the textual data into account. Since we
are the first ones to tackle this problem, we also collect a dataset of 33000
Metaverses, each of which consists of a 3D scene enriched with multimedia
content. Finally, we design and implement a deep learning framework based on
contrastive learning, resulting in a thorough experimental setup.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdari_A/0/1/0/all/0/1&quot;&gt;Ali Abdari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falcon_A/0/1/0/all/0/1&quot;&gt;Alex Falcon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serra_G/0/1/0/all/0/1&quot;&gt;Giuseppe Serra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14635">
<title>Fluid Simulation on Neural Flow Maps. (arXiv:2312.14635v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2312.14635</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Neural Flow Maps, a novel simulation method bridging the
emerging paradigm of implicit neural representations with fluid simulation
based on the theory of flow maps, to achieve state-of-the-art simulation of
inviscid fluid phenomena. We devise a novel hybrid neural field representation,
Spatially Sparse Neural Fields (SSNF), which fuses small neural networks with a
pyramid of overlapping, multi-resolution, and spatially sparse grids, to
compactly represent long-term spatiotemporal velocity fields at high accuracy.
With this neural velocity buffer in hand, we compute long-term, bidirectional
flow maps and their Jacobians in a mechanistically symmetric manner, to
facilitate drastic accuracy improvement over existing solutions. These
long-range, bidirectional flow maps enable high advection accuracy with low
dissipation, which in turn facilitates high-fidelity incompressible flow
simulations that manifest intricate vortical structures. We demonstrate the
efficacy of our neural fluid simulation in a variety of challenging simulation
scenarios, including leapfrogging vortices, colliding vortices, vortex
reconnections, as well as vortex generation from moving obstacles and density
differences. Our examples show increased performance over existing methods in
terms of energy conservation, visual complexity, adherence to experimental
observations, and preservation of detailed vortical structures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yitong Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hong-Xing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Diyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Bo Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14650">
<title>Global Occlusion-Aware Transformer for Robust Stereo Matching. (arXiv:2312.14650v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14650</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the remarkable progress facilitated by learning-based stereo-matching
algorithms, the performance in the ill-conditioned regions, such as the
occluded regions, remains a bottleneck. Due to the limited receptive field,
existing CNN-based methods struggle to handle these ill-conditioned regions
effectively. To address this issue, this paper introduces a novel
attention-based stereo-matching network called Global Occlusion-Aware
Transformer (GOAT) to exploit long-range dependency and occlusion-awareness
global context for disparity estimation. In the GOAT architecture, a parallel
disparity and occlusion estimation module PDO is proposed to estimate the
initial disparity map and the occlusion mask using a parallel attention
mechanism. To further enhance the disparity estimates in the occluded regions,
an occlusion-aware global aggregation module (OGA) is proposed. This module
aims to refine the disparity in the occluded regions by leveraging restricted
global correlation within the focus scope of the occluded areas. Extensive
experiments were conducted on several public benchmark datasets including
SceneFlow, KITTI 2015, and Middlebury. The results show that the proposed GOAT
demonstrates outstanding performance among all benchmarks, particularly in the
occluded regions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zihua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yizhou Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okutomi_M/0/1/0/all/0/1&quot;&gt;Masatoshi Okutomi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14664">
<title>Density Uncertainty Quantification with NeRF-Ensembles: Impact of Data and Scene Constraints. (arXiv:2312.14664v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14664</link>
<description rdf:parseType="Literal">&lt;p&gt;In the fields of computer graphics, computer vision and photogrammetry,
Neural Radiance Fields (NeRFs) are a major topic driving current research and
development. However, the quality of NeRF-generated 3D scene reconstructions
and subsequent surface reconstructions, heavily relies on the network output,
particularly the density. Regarding this critical aspect, we propose to utilize
NeRF-Ensembles that provide a density uncertainty estimate alongside the mean
density. We demonstrate that data constraints such as low-quality images and
poses lead to a degradation of the training process, increased density
uncertainty and decreased predicted density. Even with high-quality input data,
the density uncertainty varies based on scene constraints such as acquisition
constellations, occlusions and material properties. NeRF-Ensembles not only
provide a tool for quantifying the uncertainty but exhibit two promising
advantages: Enhanced robustness and artifact removal. Through the utilization
of NeRF-Ensembles instead of single NeRFs, small outliers are removed, yielding
a smoother output with improved completeness of structures. Furthermore,
applying percentile-based thresholds on density uncertainty outliers proves to
be effective for the removal of large (foggy) artifacts in post-processing. We
conduct our methodology on 3 different datasets: (i) synthetic benchmark
dataset, (ii) real benchmark dataset, (iii) real data under realistic recording
conditions and sensors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jager_M/0/1/0/all/0/1&quot;&gt;Miriam J&amp;#xe4;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Landgraf_S/0/1/0/all/0/1&quot;&gt;Steven Landgraf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jutzi_B/0/1/0/all/0/1&quot;&gt;Boris Jutzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14697">
<title>Pola4All: survey of polarimetric applications and an open-source toolkit to analyze polarization. (arXiv:2312.14697v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14697</link>
<description rdf:parseType="Literal">&lt;p&gt;Polarization information of the light can provide rich cues for computer
vision and scene understanding tasks, such as the type of material, pose, and
shape of the objects. With the advent of new and cheap polarimetric sensors,
this imaging modality is becoming accessible to a wider public for solving
problems such as pose estimation, 3D reconstruction, underwater navigation, and
depth estimation. However, we observe several limitations regarding the usage
of this sensorial modality, as well as a lack of standards and publicly
available tools to analyze polarization images. Furthermore, although
polarization camera manufacturers usually provide acquisition tools to
interface with their cameras, they rarely include processing algorithms that
make use of the polarization information. In this paper, we review recent
advances in applications that involve polarization imaging, including a
comprehensive survey of recent advances on polarization for vision and robotics
perception tasks. We also introduce a complete software toolkit that provides
common standards to communicate with and process information from most of the
existing micro-grid polarization cameras on the market. The toolkit also
implements several image processing algorithms for this modality, and it is
publicly available on GitHub: https://github.com/vibot-lab/Pola4all_JEI_2023.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_J/0/1/0/all/0/1&quot;&gt;Joaquin Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lew_Yan_Voon_L/0/1/0/all/0/1&quot;&gt;Lew-Fock-Chong Lew-Yan-Voon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martins_R/0/1/0/all/0/1&quot;&gt;Renato Martins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morel_O/0/1/0/all/0/1&quot;&gt;Olivier Morel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14705">
<title>SCUNet++: Assessment of Pulmonary Embolism CT Image Segmentation Leveraging Swin-UNet and CNN Bottleneck Hybrid Architecture with Multi-Fusion Dense Skip Connection. (arXiv:2312.14705v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.14705</link>
<description rdf:parseType="Literal">&lt;p&gt;Pulmonary embolism (PE) is a prevalent lung disease that can lead to right
ventricular hypertrophy and failure in severe cases, ranking second in severity
only to myocardial infarction and sudden death. Pulmonary artery CT angiography
(CTPA) is a widely used diagnostic method for PE. However, PE detection
presents challenges in clinical practice due to limitations in imaging
technology. CTPA can produce noises similar to PE, making confirmation of its
presence time-consuming and prone to overdiagnosis. Nevertheless, the
traditional segmentation method of PE can not fully consider the hierarchical
structure of features, local and global spatial features of PE CT images. In
this paper, we propose an automatic PE segmentation method called SCUNet++
(Swin Conv UNet++). This method incorporates multiple fusion dense skip
connections between the encoder and decoder, utilizing the Swin Transformer as
the encoder. And fuses features of different scales in the decoder subnetwork
to compensate for spatial information loss caused by the inevitable
downsampling in Swin-UNet or other state-of-the-art methods, effectively
solving the above problem. We provide a theoretical analysis of this method in
detail and validate it on publicly available PE CT image datasets FUMPE and
CAD-PE. The experimental results indicate that our proposed method achieved a
Dice similarity coefficient (DSC) of 83.47% and a Hausdorff distance 95th
percentile (HD95) of 3.83 on the FUMPE dataset, as well as a DSC of 83.42% and
an HD95 of 5.10 on the CAD-PE dataset. These findings demonstrate that our
method exhibits strong performance in PE segmentation tasks, potentially
enhancing the accuracy of automatic segmentation of PE and providing a powerful
diagnostic tool for clinical physicians. Our source code and new FUMPE dataset
are available at https://github.com/JustlfC03/SCUNet-plusplus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zou_B/0/1/0/all/0/1&quot;&gt;Binfeng Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhaoxin Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yifan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qin_F/0/1/0/all/0/1&quot;&gt;Feiwei Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qinhai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changmiao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14706">
<title>BonnBeetClouds3D: A Dataset Towards Point Cloud-based Organ-level Phenotyping of Sugar Beet Plants under Field Conditions. (arXiv:2312.14706v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14706</link>
<description rdf:parseType="Literal">&lt;p&gt;Agricultural production is facing severe challenges in the next decades
induced by climate change and the need for sustainability, reducing its impact
on the environment. Advancements in field management through non-chemical
weeding by robots in combination with monitoring of crops by autonomous
unmanned aerial vehicles (UAVs) and breeding of novel and more resilient crop
varieties are helpful to address these challenges. The analysis of plant
traits, called phenotyping, is an essential activity in plant breeding, it
however involves a great amount of manual labor. With this paper, we address
the problem of automatic fine-grained organ-level geometric analysis needed for
precision phenotyping. As the availability of real-world data in this domain is
relatively scarce, we propose a novel dataset that was acquired using UAVs
capturing high-resolution images of a real breeding trial containing 48 plant
varieties and therefore covering great morphological and appearance diversity.
This enables the development of approaches for autonomous phenotyping that
generalize well to different varieties. Based on overlapping high-resolution
images from multiple viewing angles, we compute photogrammetric dense point
clouds and provide detailed and accurate point-wise labels for plants, leaves,
and salient points as the tip and the base. Additionally, we include
measurements of phenotypic traits performed by experts from the German Federal
Plant Variety Office on the real plants, allowing the evaluation of new
approaches not only on segmentation and keypoint detection but also directly on
the downstream tasks. The provided labeled point clouds enable fine-grained
plant analysis and support further progress in the development of automatic
phenotyping approaches, but also enable further research in surface
reconstruction, point cloud completion, and semantic interpretation of point
clouds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marks_E/0/1/0/all/0/1&quot;&gt;Elias Marks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bomer_J/0/1/0/all/0/1&quot;&gt;Jonas B&amp;#xf6;mer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magistri_F/0/1/0/all/0/1&quot;&gt;Federico Magistri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sah_A/0/1/0/all/0/1&quot;&gt;Anurag Sah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behley_J/0/1/0/all/0/1&quot;&gt;Jens Behley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stachniss_C/0/1/0/all/0/1&quot;&gt;Cyrill Stachniss&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14724">
<title>Images in Discrete Choice Modeling: Addressing Data Isomorphism in Multi-Modality Inputs. (arXiv:2312.14724v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14724</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the intersection of Discrete Choice Modeling (DCM) and
machine learning, focusing on the integration of image data into DCM&apos;s utility
functions and its impact on model interpretability. We investigate the
consequences of embedding high-dimensional image data that shares isomorphic
information with traditional tabular inputs within a DCM framework. Our study
reveals that neural network (NN) components learn and replicate tabular
variable representations from images when co-occurrences exist, thereby
compromising the interpretability of DCM parameters. We propose and benchmark
two methodologies to address this challenge: architectural design adjustments
to segregate redundant information, and isomorphic information mitigation
through source information masking and inpainting. Our experiments, conducted
on a semi-synthetic dataset, demonstrate that while architectural modifications
prove inconclusive, direct mitigation at the data source shows to be a more
effective strategy in maintaining the integrity of DCM&apos;s interpretable
parameters. The paper concludes with insights into the applicability of our
findings in real-world settings and discusses the implications for future
research in hybrid modeling that combines complex data modalities. Full control
of tabular and image data congruence is attained by using the MIT moral machine
dataset, and both inputs are merged into a choice model by deploying the
Learning Multinomial Logit (L-MNL) framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sifringer_B/0/1/0/all/0/1&quot;&gt;Brian Sifringer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1&quot;&gt;Alexandre Alahi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14733">
<title>Harnessing Diffusion Models for Visual Perception with Meta Prompts. (arXiv:2312.14733v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14733</link>
<description rdf:parseType="Literal">&lt;p&gt;The issue of generative pretraining for vision models has persisted as a
long-standing conundrum. At present, the text-to-image (T2I) diffusion model
demonstrates remarkable proficiency in generating high-definition images
matching textual inputs, a feat made possible through its pre-training on
large-scale image-text pairs. This leads to a natural inquiry: can diffusion
models be utilized to tackle visual perception tasks? In this paper, we propose
a simple yet effective scheme to harness a diffusion model for visual
perception tasks. Our key insight is to introduce learnable embeddings (meta
prompts) to the pre-trained diffusion models to extract proper features for
perception. The effect of meta prompts are two-fold. First, as a direct
replacement of the text embeddings in the T2I models, it can activate
task-relevant features during feature extraction. Second, it will be used to
re-arrange the extracted features to ensures that the model focuses on the most
pertinent features for the task on hand. Additionally, we design a recurrent
refinement training strategy that fully leverages the property of diffusion
models, thereby yielding stronger visual features. Extensive experiments across
various benchmarks validate the effectiveness of our approach. Our approach
achieves new performance records in depth estimation tasks on NYU depth V2 and
KITTI, and in semantic segmentation task on CityScapes. Concurrently, the
proposed method attains results comparable to the current state-of-the-art in
semantic segmentation on ADE20K and pose estimation on COCO datasets, further
exemplifying its robustness and versatility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Q/0/1/0/all/0/1&quot;&gt;Qiang Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zilong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1&quot;&gt;Bingyi Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14773">
<title>Cross-Age and Cross-Site Domain Shift Impacts on Deep Learning-Based White Matter Fiber Estimation in Newborn and Baby Brains. (arXiv:2312.14773v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.14773</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models have shown great promise in estimating tissue
microstructure from limited diffusion magnetic resonance imaging data. However,
these models face domain shift challenges when test and train data are from
different scanners and protocols, or when the models are applied to data with
inherent variations such as the developing brains of infants and children
scanned at various ages. Several techniques have been proposed to address some
of these challenges, such as data harmonization or domain adaptation in the
adult brain. However, those techniques remain unexplored for the estimation of
fiber orientation distribution functions in the rapidly developing brains of
infants. In this work, we extensively investigate the age effect and domain
shift within and across two different cohorts of 201 newborns and 165 babies
using the Method of Moments and fine-tuning strategies. Our results show that
reduced variations in the microstructural development of babies in comparison
to newborns directly impact the deep learning models&apos; cross-age performance. We
also demonstrate that a small number of target domain samples can significantly
mitigate domain shift problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_R/0/1/0/all/0/1&quot;&gt;Rizhong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gholipour_A/0/1/0/all/0/1&quot;&gt;Ali Gholipour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thiran_J/0/1/0/all/0/1&quot;&gt;Jean-Philippe Thiran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Karimi_D/0/1/0/all/0/1&quot;&gt;Davood Karimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kebiri_H/0/1/0/all/0/1&quot;&gt;Hamza Kebiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cuadra_M/0/1/0/all/0/1&quot;&gt;Meritxell Bach Cuadra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14776">
<title>Compressing Image-to-Image Translation GANs Using Local Density Structures on Their Learned Manifold. (arXiv:2312.14776v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14776</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) have shown remarkable success in
modeling complex data distributions for image-to-image translation. Still,
their high computational demands prohibit their deployment in practical
scenarios like edge devices. Existing GAN compression methods mainly rely on
knowledge distillation or convolutional classifiers&apos; pruning techniques. Thus,
they neglect the critical characteristic of GANs: their local density structure
over their learned manifold. Accordingly, we approach GAN compression from a
new perspective by explicitly encouraging the pruned model to preserve the
density structure of the original parameter-heavy model on its learned
manifold. We facilitate this objective for the pruned model by partitioning the
learned manifold of the original generator into local neighborhoods around its
generated samples. Then, we propose a novel pruning objective to regularize the
pruned model to preserve the local density structure over each neighborhood,
resembling the kernel density estimation method. Also, we develop a
collaborative pruning scheme in which the discriminator and generator are
pruned by two pruning agents. We design the agents to capture interactions
between the generator and discriminator by exchanging their peer&apos;s feedback
when determining corresponding models&apos; architectures. Thanks to such a design,
our pruning method can efficiently find performant sub-networks and can
maintain the balance between the generator and discriminator more effectively
compared to baselines during pruning, thereby showing more stable pruning
dynamics. Our experiments on image translation GAN models, Pix2Pix and
CycleGAN, with various benchmark datasets and architectures demonstrate our
method&apos;s effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganjdanesh_A/0/1/0/all/0/1&quot;&gt;Alireza Ganjdanesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shangqian Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alipanah_H/0/1/0/all/0/1&quot;&gt;Hirad Alipanah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Heng Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14792">
<title>The Rate-Distortion-Perception-Classification Tradeoff: Joint Source Coding and Modulation via Inverse-Domain GANs. (arXiv:2312.14792v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.14792</link>
<description rdf:parseType="Literal">&lt;p&gt;The joint source coding and modulation (JSCM) framework was enabled by recent
developments in deep learning, which allows to automatically learn from data,
and in an end-to-end fashion, the best compression codes and modulation
schemes. In this paper, we show the existence of a strict tradeoff between
channel rate, distortion, perception, and classification accuracy in a JSCM
scenario. We then propose two image compression methods to navigate that
tradeoff: an inverse-domain generative adversarial network (ID-GAN), which
achieves extreme compression, and a simpler, heuristic method that reveals
insights about the performance of ID-GAN. Experiment results not only
corroborate the theoretical findings, but also demonstrate that the proposed
ID-GAN algorithm significantly improves system performance compared to
traditional separation-based methods and recent deep JSCM architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1&quot;&gt;Junli Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mota_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o F. C. Mota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1&quot;&gt;Baoshan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weicheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1&quot;&gt;Xuemin Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14812">
<title>PARDINUS: Weakly supervised discarding of photo-trapping empty images based on autoencoders. (arXiv:2312.14812v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14812</link>
<description rdf:parseType="Literal">&lt;p&gt;Photo-trapping cameras are widely employed for wildlife monitoring. Those
cameras take photographs when motion is detected to capture images where
animals appear. A significant portion of these images are empty - no wildlife
appears in the image. Filtering out those images is not a trivial task since it
requires hours of manual work from biologists. Therefore, there is a notable
interest in automating this task. Automatic discarding of empty photo-trapping
images is still an open field in the area of Machine Learning. Existing
solutions often rely on state-of-the-art supervised convolutional neural
networks that require the annotation of the images in the training phase.
PARDINUS (Weakly suPervised discARDINg of photo-trapping empty images based on
aUtoencoderS) is constructed on the foundation of weakly supervised learning
and proves that this approach equals or even surpasses other fully supervised
methods that require further labeling work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosa_D/0/1/0/all/0/1&quot;&gt;David de la Rosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivera_A/0/1/0/all/0/1&quot;&gt;Antonio J Rivera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jesus_M/0/1/0/all/0/1&quot;&gt;Mar&amp;#xed;a J del Jesus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charte_F/0/1/0/all/0/1&quot;&gt;Francisco Charte&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14828">
<title>Plan, Posture and Go: Towards Open-World Text-to-Motion Generation. (arXiv:2312.14828v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14828</link>
<description rdf:parseType="Literal">&lt;p&gt;Conventional text-to-motion generation methods are usually trained on limited
text-motion pairs, making them hard to generalize to open-world scenarios. Some
works use the CLIP model to align the motion space and the text space, aiming
to enable motion generation from natural language motion descriptions. However,
they are still constrained to generate limited and unrealistic in-place
motions. To address these issues, we present a divide-and-conquer framework
named PRO-Motion, which consists of three modules as motion planner,
posture-diffuser and go-diffuser. The motion planner instructs Large Language
Models (LLMs) to generate a sequence of scripts describing the key postures in
the target motion. Differing from natural languages, the scripts can describe
all possible postures following very simple text templates. This significantly
reduces the complexity of posture-diffuser, which transforms a script to a
posture, paving the way for open-world generation. Finally, go-diffuser,
implemented as another diffusion model, estimates whole-body translations and
rotations for all postures, resulting in realistic motions. Experimental
results have shown the superiority of our method with other counterparts, and
demonstrated its capability of generating diverse and realistic motions from
complex open-world prompts such as &quot;Experiencing a profound sense of joy&quot;. The
project page is available at https://moonsliu.github.io/Pro-Motion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinpeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Wenxun Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chunyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yiji Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yansong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1&quot;&gt;Xin Tong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14830">
<title>Dreaming of Electrical Waves: Generative Modeling of Cardiac Excitation Waves using Diffusion Models. (arXiv:2312.14830v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/2312.14830</link>
<description rdf:parseType="Literal">&lt;p&gt;Electrical waves in the heart form rotating spiral or scroll waves during
life-threatening arrhythmias such as atrial or ventricular fibrillation. The
wave dynamics are typically modeled using coupled partial differential
equations, which describe reaction-diffusion dynamics in excitable media. More
recently, data-driven generative modeling has emerged as an alternative to
generate spatio-temporal patterns in physical and biological systems. Here, we
explore denoising diffusion probabilistic models for the generative modeling of
electrical wave patterns in cardiac tissue. We trained diffusion models with
simulated electrical wave patterns to be able to generate such wave patterns in
unconditional and conditional generation tasks. For instance, we explored
inpainting tasks, such as reconstructing three-dimensional wave dynamics from
superficial two-dimensional measurements, and evolving and generating
parameter-specific dynamics. We characterized and compared the
diffusion-generated solutions to solutions obtained with biophysical models and
found that diffusion models learn to replicate spiral and scroll waves dynamics
so well that they could serve as an alternative data-driven approach for the
modeling of excitation waves in cardiac tissue. For instance, we found that it
is possible to initiate ventricular fibrillation (VF) dynamics instantaneously
without having to apply pacing protocols in order to induce wavebreak. The VF
dynamics can be created in arbitrary ventricular geometries and can be evolved
over time. However, we also found that diffusion models `hallucinate&apos; wave
patterns when given insufficient constraints. Regardless of these limitations,
diffusion models are an interesting and powerful tool with many potential
applications in cardiac arrhythmia research and diagnostics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Baranwal_T/0/1/0/all/0/1&quot;&gt;Tanish Baranwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lebert_J/0/1/0/all/0/1&quot;&gt;Jan Lebert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Christoph_J/0/1/0/all/0/1&quot;&gt;Jan Christoph&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14834">
<title>Prototype-Guided Text-based Person Search based on Rich Chinese Descriptions. (arXiv:2312.14834v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14834</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-based person search aims to simultaneously localize and identify the
target person based on query text from uncropped scene images, which can be
regarded as the unified task of person detection and text-based person
retrieval task. In this work, we propose a large-scale benchmark dataset named
PRW-TPS-CN based on the widely used person search dataset PRW. Our dataset
contains 47,102 sentences, which means there is quite more information than
existing dataset. These texts precisely describe the person images from top to
bottom, which in line with the natural description order. We also provide both
Chinese and English descriptions in our dataset for more comprehensive
evaluation. These characteristics make our dataset more applicable. To
alleviate the inconsistency between person detection and text-based person
retrieval, we take advantage of the rich texts in PRW-TPS-CN dataset. We
propose to aggregate multiple texts as text prototypes to maintain the
prominent text features of a person, which can better reflect the whole
character of a person. The overall prototypes lead to generating the image
attention map to eliminate the detection misalignment causing the decrease of
text-based person retrieval. Thus, the inconsistency between person detection
and text-based person retrieval is largely alleviated. We conduct extensive
experiments on the PRW-TPS-CN dataset. The experimental results show the
PRW-TPS-CN dataset&apos;s effectiveness and the state-of-the-art performance of our
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Ziqiang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1&quot;&gt;Bingpeng Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14867">
<title>VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation. (arXiv:2312.14867v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14867</link>
<description rdf:parseType="Literal">&lt;p&gt;In the rapidly advancing field of conditional image generation research,
challenges such as limited explainability lie in effectively evaluating the
performance and capabilities of various models. This paper introduces VIESCORE,
a Visual Instruction-guided Explainable metric for evaluating any conditional
image generation tasks. VIESCORE leverages general knowledge from Multimodal
Large Language Models (MLLMs) as the backbone and does not require training or
fine-tuning. We evaluate VIESCORE on seven prominent tasks in conditional image
tasks and found: (1) VIESCORE (GPT4-v) achieves a high Spearman correlation of
0.3 with human evaluations, while the human-to-human correlation is 0.45. (2)
VIESCORE (with open-source MLLM) is significantly weaker than GPT-4v in
evaluating synthetic images. (3) VIESCORE achieves a correlation on par with
human ratings in the generation tasks but struggles in editing tasks. With
these results, we believe VIESCORE shows its great potential to replace human
judges in evaluating image synthesis tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ku_M/0/1/0/all/0/1&quot;&gt;Max Ku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1&quot;&gt;Dongfu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1&quot;&gt;Cong Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xiang Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14871">
<title>BrainVis: Exploring the Bridge between Brain and Visual Signals via Image Reconstruction. (arXiv:2312.14871v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14871</link>
<description rdf:parseType="Literal">&lt;p&gt;Analyzing and reconstructing visual stimuli from brain signals effectively
advances understanding of the human visual system. However, the EEG signals are
complex and contain a amount of noise. This leads to substantial limitations in
existing works of visual stimuli reconstruction from EEG, such as difficulties
in aligning EEG embeddings with the fine-grained semantic information and a
heavy reliance on additional large self-collected dataset for training. To
address these challenges, we propose a novel approach called BrainVis. Firstly,
we divide the EEG signals into various units and apply a self-supervised
approach on them to obtain EEG time-domain features, in an attempt to ease the
training difficulty. Additionally, we also propose to utilize the
frequency-domain features to enhance the EEG representations. Then, we
simultaneously align EEG time-frequency embeddings with the interpolation of
the coarse and fine-grained semantics in the CLIP space, to highlight the
primary visual components and reduce the cross-modal alignment difficulty.
Finally, we adopt the cascaded diffusion models to reconstruct images. Our
proposed BrainVis outperforms state of the arts in both semantic fidelity
reconstruction and generation quality. Notably, we reduce the training data
scale to 10% of the previous work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Honghao Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhiqi Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chin_J/0/1/0/all/0/1&quot;&gt;Jing Jih Chin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14891">
<title>DRStageNet: Deep Learning for Diabetic Retinopathy Staging from Fundus Images. (arXiv:2312.14891v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.14891</link>
<description rdf:parseType="Literal">&lt;p&gt;Diabetic retinopathy (DR) is a prevalent complication of diabetes associated
with a significant risk of vision loss. Timely identification is critical to
curb vision impairment. Algorithms for DR staging from digital fundus images
(DFIs) have been recently proposed. However, models often fail to generalize
due to distribution shifts between the source domain on which the model was
trained and the target domain where it is deployed. A common and particularly
challenging shift is often encountered when the source- and target-domain
supports do not fully overlap. In this research, we introduce DRStageNet, a
deep learning model designed to mitigate this challenge. We used seven publicly
available datasets, comprising a total of 93,534 DFIs that cover a variety of
patient demographics, ethnicities, geographic origins and comorbidities. We
fine-tune DINOv2, a pretrained model of self-supervised vision transformer, and
implement a multi-source domain fine-tuning strategy to enhance generalization
performance. We benchmark and demonstrate the superiority of our method to two
state-of-the-art benchmarks, including a recently published foundation model.
We adapted the grad-rollout method to our regression task in order to provide
high-resolution explainability heatmaps. The error analysis showed that 59\% of
the main errors had incorrect reference labels. DRStageNet is accessible at URL
[upon acceptance of the manuscript].
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Men_Y/0/1/0/all/0/1&quot;&gt;Yevgeniy Men&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fhima_J/0/1/0/all/0/1&quot;&gt;Jonathan Fhima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Celi_L/0/1/0/all/0/1&quot;&gt;Leo Anthony Celi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ribeiro_L/0/1/0/all/0/1&quot;&gt;Lucas Zago Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nakayama_L/0/1/0/all/0/1&quot;&gt;Luis Filipe Nakayama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Behar_J/0/1/0/all/0/1&quot;&gt;Joachim A. Behar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14915">
<title>PoseGen: Learning to Generate 3D Human Pose Dataset with NeRF. (arXiv:2312.14915v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14915</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes an end-to-end framework for generating 3D human pose
datasets using Neural Radiance Fields (NeRF). Public datasets generally have
limited diversity in terms of human poses and camera viewpoints, largely due to
the resource-intensive nature of collecting 3D human pose data. As a result,
pose estimators trained on public datasets significantly underperform when
applied to unseen out-of-distribution samples. Previous works proposed
augmenting public datasets by generating 2D-3D pose pairs or rendering a large
amount of random data. Such approaches either overlook image rendering or
result in suboptimal datasets for pre-trained models. Here we propose PoseGen,
which learns to generate a dataset (human 3D poses and images) with a feedback
loss from a given pre-trained pose estimator. In contrast to prior art, our
generated data is optimized to improve the robustness of the pre-trained model.
The objective of PoseGen is to learn a distribution of data that maximizes the
prediction error of a given pre-trained model. As the learned data distribution
contains OOD samples of the pre-trained model, sampling data from such a
distribution for further fine-tuning a pre-trained model improves the
generalizability of the model. This is the first work that proposes NeRFs for
3D human data generation. NeRFs are data-driven and do not require 3D scans of
humans. Therefore, using NeRF for data generation is a new direction for
convenient user-specific data generation. Our extensive experiments show that
the proposed PoseGen improves two baseline models (SPIN and HybrIK) on four
datasets with an average 6% relative improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gholami_M/0/1/0/all/0/1&quot;&gt;Mohsen Gholami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ward_R/0/1/0/all/0/1&quot;&gt;Rabab Ward&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Z. Jane Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14919">
<title>Lift-Attend-Splat: Bird&apos;s-eye-view camera-lidar fusion using transformers. (arXiv:2312.14919v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14919</link>
<description rdf:parseType="Literal">&lt;p&gt;Combining complementary sensor modalities is crucial to providing robust
perception for safety-critical robotics applications such as autonomous driving
(AD). Recent state-of-the-art camera-lidar fusion methods for AD rely on
monocular depth estimation which is a notoriously difficult task compared to
using depth information from the lidar directly. Here, we find that this
approach does not leverage depth as expected and show that naively improving
depth estimation does not lead to improvements in object detection performance
and that, strikingly, removing depth estimation altogether does not degrade
object detection performance. This suggests that relying on monocular depth
could be an unnecessary architectural bottleneck during camera-lidar fusion. In
this work, we introduce a novel fusion method that bypasses monocular depth
estimation altogether and instead selects and fuses camera and lidar features
in a bird&apos;s-eye-view grid using a simple attention mechanism. We show that our
model can modulate its use of camera features based on the availability of
lidar features and that it yields better 3D object detection on the nuScenes
dataset than baselines relying on monocular depth estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunn_J/0/1/0/all/0/1&quot;&gt;James Gunn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lenyk_Z/0/1/0/all/0/1&quot;&gt;Zygmunt Lenyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Anuj Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donati_A/0/1/0/all/0/1&quot;&gt;Andrea Donati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buburuzan_A/0/1/0/all/0/1&quot;&gt;Alexandru Buburuzan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redford_J/0/1/0/all/0/1&quot;&gt;John Redford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mueller_R/0/1/0/all/0/1&quot;&gt;Romain Mueller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14924">
<title>Training Convolutional Neural Networks with the Forward-Forward algorithm. (arXiv:2312.14924v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14924</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent successes in analyzing images with deep neural networks are almost
exclusively achieved with Convolutional Neural Networks (CNNs). The training of
these CNNs, and in fact of all deep neural network architectures, uses the
backpropagation algorithm where the output of the network is compared with the
desired result and the difference is then used to tune the weights of the
network towards the desired outcome. In a 2022 preprint, Geoffrey Hinton
suggested an alternative way of training which passes the desired results
together with the images at the input of the network. This so called Forward
Forward (FF) algorithm has up to now only been used in fully connected
networks. In this paper, we show how the FF paradigm can be extended to CNNs.
Our FF-trained CNN, featuring a novel spatially-extended labeling technique,
achieves a classification accuracy of 99.0% on the MNIST hand-written digits
dataset. We show how different hyperparameters affect the performance of the
proposed algorithm and compare the results with CNN trained with the standard
backpropagation approach. Furthermore, we use Class Activation Maps to
investigate which type of features are learnt by the FF algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scodellaro_R/0/1/0/all/0/1&quot;&gt;Riccardo Scodellaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1&quot;&gt;Ajinkya Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alves_F/0/1/0/all/0/1&quot;&gt;Frauke Alves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroter_M/0/1/0/all/0/1&quot;&gt;Matthias Schr&amp;#xf6;ter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14929">
<title>MACS: Mass Conditioned 3D Hand and Object Motion Synthesis. (arXiv:2312.14929v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14929</link>
<description rdf:parseType="Literal">&lt;p&gt;The physical properties of an object, such as mass, significantly affect how
we manipulate it with our hands. Surprisingly, this aspect has so far been
neglected in prior work on 3D motion synthesis. To improve the naturalness of
the synthesized 3D hand object motions, this work proposes MACS the first MAss
Conditioned 3D hand and object motion Synthesis approach. Our approach is based
on cascaded diffusion models and generates interactions that plausibly adjust
based on the object mass and interaction type. MACS also accepts a manually
drawn 3D object trajectory as input and synthesizes the natural 3D hand motions
conditioned by the object mass. This flexibility enables MACS to be used for
various downstream applications, such as generating synthetic training data for
ML tasks, fast animation of hands for graphics workflows, and generating
character interactions for computer games. We show experimentally that a
small-scale dataset is sufficient for MACS to reasonably generalize across
interpolated and extrapolated object masses unseen during the training.
Furthermore, MACS shows moderate generalization to unseen objects, thanks to
the mass-conditioned contact labels generated by our surface contact synthesis
model ConNet. Our comprehensive user study confirms that the synthesized 3D
hand-object interactions are highly plausible and realistic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shimada_S/0/1/0/all/0/1&quot;&gt;Soshi Shimada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mueller_F/0/1/0/all/0/1&quot;&gt;Franziska Mueller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bednarik_J/0/1/0/all/0/1&quot;&gt;Jan Bednarik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doosti_B/0/1/0/all/0/1&quot;&gt;Bardia Doosti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bickel_B/0/1/0/all/0/1&quot;&gt;Bernd Bickel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1&quot;&gt;Danhang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1&quot;&gt;Vladislav Golyanik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_J/0/1/0/all/0/1&quot;&gt;Jonathan Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1&quot;&gt;Christian Theobalt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beeler_T/0/1/0/all/0/1&quot;&gt;Thabo Beeler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.02370">
<title>Self-Supervised Learning for Place Representation Generalization across Appearance Changes. (arXiv:2303.02370v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.02370</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual place recognition is a key to unlocking spatial navigation for
animals, humans and robots. While state-of-the-art approaches are trained in a
supervised manner and therefore hardly capture the information needed for
generalizing to unusual conditions, we argue that self-supervised learning may
help abstracting the place representation so that it can be foreseen,
irrespective of the conditions. More precisely, in this paper, we investigate
learning features that are robust to appearance modifications while sensitive
to geometric transformations in a self-supervised manner. This dual-purpose
training is made possible by combining the two self-supervision main paradigms,
\textit{i.e.} contrastive and predictive learning. Our results on standard
benchmarks reveal that jointly learning such appearance-robust and
geometry-sensitive image descriptors leads to competitive visual place
recognition results across adverse seasonal and illumination conditions,
without requiring any human-annotated labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musallam_M/0/1/0/all/0/1&quot;&gt;Mohamed Adel Musallam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaudilliere_V/0/1/0/all/0/1&quot;&gt;Vincent Gaudilli&amp;#xe8;re&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aouada_D/0/1/0/all/0/1&quot;&gt;Djamila Aouada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05915">
<title>Convolutional Cross-View Pose Estimation. (arXiv:2303.05915v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05915</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel end-to-end method for cross-view pose estimation. Given a
ground-level query image and an aerial image that covers the query&apos;s local
neighborhood, the 3 Degrees-of-Freedom camera pose of the query is estimated by
matching its image descriptor to descriptors of local regions within the aerial
image. The orientation-aware descriptors are obtained by using a
translationally equivariant convolutional ground image encoder and contrastive
learning. The Localization Decoder produces a dense probability distribution in
a coarse-to-fine manner with a novel Localization Matching Upsampling module. A
smaller Orientation Decoder produces a vector field to condition the
orientation estimate on the localization. Our method is validated on the VIGOR
and KITTI datasets, where it surpasses the state-of-the-art baseline by 72% and
36% in median localization error for comparable orientation estimation
accuracy. The predicted probability distribution can represent localization
ambiguity, and enables rejecting possible erroneous predictions. Without
re-training, the model can infer on ground images with different field of views
and utilize orientation priors if available. On the Oxford RobotCar dataset,
our method can reliably estimate the ego-vehicle&apos;s pose over time, achieving a
median localization error under 1 meter and a median orientation error of
around 1 degree at 14 FPS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1&quot;&gt;Zimin Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Booij_O/0/1/0/all/0/1&quot;&gt;Olaf Booij&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kooij_J/0/1/0/all/0/1&quot;&gt;Julian F. P. Kooij&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.05684">
<title>InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions. (arXiv:2304.05684v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.05684</link>
<description rdf:parseType="Literal">&lt;p&gt;We have recently seen tremendous progress in diffusion advances for
generating realistic human motions. Yet, they largely disregard the multi-human
interactions. In this paper, we present InterGen, an effective diffusion-based
approach that incorporates human-to-human interactions into the motion
diffusion process, which enables layman users to customize high-quality
two-person interaction motions, with only text guidance. We first contribute a
multimodal dataset, named InterHuman. It consists of about 107M frames for
diverse two-person interactions, with accurate skeletal motions and 23,337
natural language descriptions. For the algorithm side, we carefully tailor the
motion diffusion model to our two-person interaction setting. To handle the
symmetry of human identities during interactions, we propose two cooperative
transformer-based denoisers that explicitly share weights, with a mutual
attention mechanism to further connect the two denoising processes. Then, we
propose a novel representation for motion input in our interaction diffusion
model, which explicitly formulates the global relations between the two
performers in the world frame. We further introduce two novel regularization
terms to encode spatial relations, equipped with a corresponding damping scheme
during the training of our interaction diffusion model. Extensive experiments
validate the effectiveness and generalizability of InterGen. Notably, it can
generate more diverse and compelling two-person motions than previous methods
and enables various downstream applications for human interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Han Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenqian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jingyi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11241">
<title>AutoNeRF: Training Implicit Scene Representations with Autonomous Agents. (arXiv:2304.11241v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.11241</link>
<description rdf:parseType="Literal">&lt;p&gt;Implicit representations such as Neural Radiance Fields (NeRF) have been
shown to be very effective at novel view synthesis. However, these models
typically require manual and careful human data collection for training. In
this paper, we present AutoNeRF, a method to collect data required to train
NeRFs using autonomous embodied agents. Our method allows an agent to explore
an unseen environment efficiently and use the experience to build an implicit
map representation autonomously. We compare the impact of different exploration
strategies including handcrafted frontier-based exploration, end-to-end and
modular approaches composed of trained high-level planners and classical
low-level path followers. We train these models with different reward functions
tailored to this problem and evaluate the quality of the learned
representations on four different downstream tasks: classical viewpoint
rendering, map reconstruction, planning, and pose refinement. Empirical results
show that NeRFs can be trained on actively collected data using just a single
episode of experience in an unseen environment, and can be used for several
downstream robotic tasks, and that modular trained exploration models
outperform other classical and end-to-end baselines. Finally, we show that
AutoNeRF can reconstruct large-scale scenes, and is thus a useful tool to
perform scene-specific adaptation as the produced 3D environment models can be
loaded into a simulator to fine-tune a policy of interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marza_P/0/1/0/all/0/1&quot;&gt;Pierre Marza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matignon_L/0/1/0/all/0/1&quot;&gt;Laetitia Matignon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simonin_O/0/1/0/all/0/1&quot;&gt;Olivier Simonin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1&quot;&gt;Dhruv Batra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1&quot;&gt;Christian Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1&quot;&gt;Devendra Singh Chaplot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14291">
<title>EDAPS: Enhanced Domain-Adaptive Panoptic Segmentation. (arXiv:2304.14291v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14291</link>
<description rdf:parseType="Literal">&lt;p&gt;With autonomous industries on the rise, domain adaptation of the visual
perception stack is an important research direction due to the cost savings
promise. Much prior art was dedicated to domain-adaptive semantic segmentation
in the synthetic-to-real context. Despite being a crucial output of the
perception stack, panoptic segmentation has been largely overlooked by the
domain adaptation community. Therefore, we revisit well-performing domain
adaptation strategies from other fields, adapt them to panoptic segmentation,
and show that they can effectively enhance panoptic domain adaptation. Further,
we study the panoptic network design and propose a novel architecture (EDAPS)
designed explicitly for domain-adaptive panoptic segmentation. It uses a
shared, domain-robust transformer encoder to facilitate the joint adaptation of
semantic and instance features, but task-specific decoders tailored for the
specific requirements of both domain-adaptive semantic and instance
segmentation. As a result, the performance gap seen in challenging panoptic
benchmarks is substantially narrowed. EDAPS significantly improves the
state-of-the-art performance for panoptic segmentation UDA by a large margin of
20% on SYNTHIA-to-Cityscapes and even 72% on the more challenging
SYNTHIA-to-Mapillary Vistas. The implementation is available at
https://github.com/susaha/edaps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1&quot;&gt;Suman Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoyer_L/0/1/0/all/0/1&quot;&gt;Lukas Hoyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1&quot;&gt;Anton Obukhov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1&quot;&gt;Dengxin Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02693">
<title>Semi-supervised Domain Adaptation via Prototype-based Multi-level Learning. (arXiv:2305.02693v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02693</link>
<description rdf:parseType="Literal">&lt;p&gt;In semi-supervised domain adaptation (SSDA), a few labeled target samples of
each class help the model to transfer knowledge representation from the fully
labeled source domain to the target domain. Many existing methods ignore the
benefits of making full use of the labeled target samples from multi-level. To
make better use of this additional data, we propose a novel Prototype-based
Multi-level Learning (ProML) framework to better tap the potential of labeled
target samples. To achieve intra-domain adaptation, we first introduce a
pseudo-label aggregation based on the intra-domain optimal transport to help
the model align the feature distribution of unlabeled target samples and the
prototype. At the inter-domain level, we propose a cross-domain alignment loss
to help the model use the target prototype for cross-domain knowledge transfer.
We further propose a dual consistency based on prototype similarity and linear
classifier to promote discriminative learning of compact target feature
representation at the batch level. Extensive experiments on three datasets,
including DomainNet, VisDA2017, and Office-Home demonstrate that our proposed
method achieves state-of-the-art performance in SSDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xinyang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chuang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenkai Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05400">
<title>Investigating the Corruption Robustness of Image Classifiers with Random Lp-norm Corruptions. (arXiv:2305.05400v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05400</link>
<description rdf:parseType="Literal">&lt;p&gt;Robustness is a fundamental property of machine learning classifiers required
to achieve safety and reliability. In the field of adversarial robustness of
image classifiers, robustness is commonly defined as the stability of a model
to all input changes within a p-norm distance. However, in the field of random
corruption robustness, variations observed in the real world are used, while
p-norm corruptions are rarely considered. This study investigates the use of
random p-norm corruptions to augment the training and test data of image
classifiers. We evaluate the model robustness against imperceptible random
p-norm corruptions and propose a novel robustness metric. We empirically
investigate whether robustness transfers across different p-norms and derive
conclusions on which p-norm corruptions a model should be trained and
evaluated. We find that training data augmentation with a combination of p-norm
corruptions significantly improves corruption robustness, even on top of
state-of-the-art data augmentation schemes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siedel_G/0/1/0/all/0/1&quot;&gt;Georg Siedel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1&quot;&gt;Weijia Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vock_S/0/1/0/all/0/1&quot;&gt;Silvia Vock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morozov_A/0/1/0/all/0/1&quot;&gt;Andrey Morozov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17349">
<title>Condition-Invariant Semantic Segmentation. (arXiv:2305.17349v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17349</link>
<description rdf:parseType="Literal">&lt;p&gt;Adaptation of semantic segmentation networks to different visual conditions
is vital for robust perception in autonomous cars and robots. However, previous
work has shown that most feature-level adaptation methods, which employ
adversarial training and are validated on synthetic-to-real adaptation, provide
marginal gains in condition-level adaptation, being outperformed by simple
pixel-level adaptation via stylization. Motivated by these findings, we propose
to leverage stylization in performing feature-level adaptation by aligning the
internal network features extracted by the encoder of the network from the
original and the stylized view of each input image with a novel feature
invariance loss. In this way, we encourage the encoder to extract features that
are already invariant to the style of the input, allowing the decoder to focus
on parsing these features and not on further abstracting from the specific
style of the input. We implement our method, named Condition-Invariant Semantic
Segmentation (CISS), on the current state-of-the-art domain adaptation
architecture and achieve outstanding results on condition-level adaptation. In
particular, CISS sets the new state of the art in the popular
daytime-to-nighttime Cityscapes$\to$Dark Zurich benchmark. Furthermore, our
method achieves the second-best performance on the normal-to-adverse
Cityscapes$\to$ACDC benchmark. CISS is shown to generalize well to domains
unseen during training, such as BDD100K-night. Code is publicly available at
https://github.com/SysCV/CISS .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1&quot;&gt;Christos Sakaridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruggemann_D/0/1/0/all/0/1&quot;&gt;David Bruggemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fisher Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05745">
<title>Two Independent Teachers are Better Role Model. (arXiv:2306.05745v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05745</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent deep learning models have attracted substantial attention in infant
brain analysis. These models have performed state-of-the-art performance, such
as semi-supervised techniques (e.g., Temporal Ensembling, mean teacher).
However, these models depend on an encoder-decoder structure with stacked local
operators to gather long-range information, and the local operators limit the
efficiency and effectiveness. Besides, the $MRI$ data contain different tissue
properties ($TPs$) such as $T1$ and $T2$. One major limitation of these models
is that they use both data as inputs to the segment process, i.e., the models
are trained on the dataset once, and it requires much computational and memory
requirements during inference. In this work, we address the above limitations
by designing a new deep-learning model, called 3D-DenseUNet, which works as
adaptable global aggregation blocks in down-sampling to solve the issue of
spatial information loss. The self-attention module connects the down-sampling
blocks to up-sampling blocks, and integrates the feature maps in three
dimensions of spatial and channel, effectively improving the representation
potential and discriminating ability of the model. Additionally, we propose a
new method called Two Independent Teachers ($2IT$), that summarizes the model
weights instead of label predictions. Each teacher model is trained on
different types of brain data, $T1$ and $T2$, respectively. Then, a fuse model
is added to improve test accuracy and enable training with fewer parameters and
labels compared to the Temporal Ensembling method without modifying the network
architecture. Empirical results demonstrate the effectiveness of the proposed
method. The code is available at
https://github.com/AfifaKhaled/Two-Independent-Teachers-are-Better-Role-Model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khaled_A/0/1/0/all/0/1&quot;&gt;Afifa Khaled&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mubarak_A/0/1/0/all/0/1&quot;&gt;Ahmed A. Mubarak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_K/0/1/0/all/0/1&quot;&gt;Kun He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05832">
<title>Sketch Beautification: Learning Part Beautification and Structure Refinement for Sketches of Man-made Objects. (arXiv:2306.05832v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05832</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel freehand sketch beautification method, which takes as
input a freely drawn sketch of a man-made object and automatically beautifies
it both geometrically and structurally. Beautifying a sketch is challenging
because of its highly abstract and heavily diverse drawing manner. Existing
methods are usually confined to the distribution of their limited training
samples and thus cannot beautify freely drawn sketches with rich variations. To
address this challenge, we adopt a divide-and-combine strategy. Specifically,
we first parse an input sketch into semantic components, beautify individual
components by a learned part beautification module based on part-level implicit
manifolds, and then reassemble the beautified components through a structure
beautification module. With this strategy, our method can go beyond the
training samples and handle novel freehand sketches. We demonstrate the
effectiveness of our system with extensive experiments and a perceptive study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Deng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lau_M/0/1/0/all/0/1&quot;&gt;Manfred Lau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Lin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Hongbo Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06209">
<title>Backdoor Attack with Sparse and Invisible Trigger. (arXiv:2306.06209v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06209</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) are vulnerable to backdoor attacks, where the
adversary manipulates a small portion of training data such that the victim
model predicts normally on the benign samples but classifies the triggered
samples as the target class. The backdoor attack is an emerging yet threatening
training-phase threat, leading to serious risks in DNN-based applications. In
this paper, we revisit the trigger patterns of existing backdoor attacks. We
reveal that they are either visible or not sparse and therefore are not
stealthy enough. More importantly, it is not feasible to simply combine
existing methods to design an effective sparse and invisible backdoor attack.
To address this problem, we formulate the trigger generation as a bi-level
optimization problem with sparsity and invisibility constraints and propose an
effective method to solve it. The proposed method is dubbed sparse and
invisible backdoor attack (SIBA). We conduct extensive experiments on benchmark
datasets under different settings, which verify the effectiveness of our attack
and its resistance to existing backdoor defenses. The codes for reproducing
main experiments are available at \url{https://github.com/YinghuaGao/SIBA}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yinghua Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1&quot;&gt;Xueluan Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shu-Tao Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qian Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15774">
<title>Next Steps for Human-Centered Generative AI: A Technical Perspective. (arXiv:2306.15774v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15774</link>
<description rdf:parseType="Literal">&lt;p&gt;Through iterative, cross-disciplinary discussions, we define and propose
next-steps for Human-centered Generative AI (HGAI). We contribute a
comprehensive research agenda that lays out future directions of Generative AI
spanning three levels: aligning with human values; assimilating human intents;
and augmenting human abilities. By identifying these next-steps, we intend to
draw interdisciplinary research teams to pursue a coherent set of emergent
ideas in HGAI, focusing on their interested topics while maintaining a coherent
big picture of the future work landscape.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiang &amp;#x27;Anthony&amp;#x27; Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burke_J/0/1/0/all/0/1&quot;&gt;Jeff Burke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1&quot;&gt;Ruofei Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1&quot;&gt;Matthew K. Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobs_J/0/1/0/all/0/1&quot;&gt;Jennifer Jacobs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1&quot;&gt;Philippe Laban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dingzeyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1&quot;&gt;Nanyun Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willis_K/0/1/0/all/0/1&quot;&gt;Karl D. D. Willis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chien-Sheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bolei Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17602">
<title>S.T.A.R.-Track: Latent Motion Models for End-to-End 3D Object Tracking with Adaptive Spatio-Temporal Appearance Representations. (arXiv:2306.17602v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17602</link>
<description rdf:parseType="Literal">&lt;p&gt;Following the tracking-by-attention paradigm, this paper introduces an
object-centric, transformer-based framework for tracking in 3D. Traditional
model-based tracking approaches incorporate the geometric effect of object- and
ego motion between frames with a geometric motion model. Inspired by this, we
propose S.T.A.R.-Track, which uses a novel latent motion model (LMM) to
additionally adjust object queries to account for changes in viewing direction
and lighting conditions directly in the latent space, while still modeling the
geometric motion explicitly. Combined with a novel learnable track embedding
that aids in modeling the existence probability of tracks, this results in a
generic tracking framework that can be integrated with any query-based
detector. Extensive experiments on the nuScenes benchmark demonstrate the
benefits of our approach, showing \ac{sota} performance for DETR3D-based
trackers while drastically reducing the number of identity switches of tracks
at the same time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doll_S/0/1/0/all/0/1&quot;&gt;Simon Doll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanselmann_N/0/1/0/all/0/1&quot;&gt;Niklas Hanselmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_L/0/1/0/all/0/1&quot;&gt;Lukas Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_R/0/1/0/all/0/1&quot;&gt;Richard Schulz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Enzweiler_M/0/1/0/all/0/1&quot;&gt;Markus Enzweiler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1&quot;&gt;Hendrik P.A. Lensch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16184">
<title>UnIVAL: Unified Model for Image, Video, Audio and Language Tasks. (arXiv:2307.16184v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16184</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have made the ambitious quest for generalist
agents significantly far from being a fantasy. A key hurdle for building such
general models is the diversity and heterogeneity of tasks and modalities. A
promising solution is unification, allowing the support of a myriad of tasks
and modalities within one unified framework. While few large models (e.g.,
Flamingo (Alayrac et al., 2022), trained on massive datasets, can support more
than two modalities, current small to mid-scale unified models are still
limited to 2 modalities, usually image-text or video-text. The question that we
ask is: is it possible to build efficiently a unified model that can support
all modalities? To answer this, we propose UnIVAL, a step further towards this
ambitious goal. Without relying on fancy datasets sizes or models with billions
of parameters, the ~ 0.25B parameter UnIVAL model goes beyond two modalities
and unifies text, images, video, and audio into a single model. Our model is
efficiently pretrained on many tasks, based on task balancing and multimodal
curriculum learning. UnIVAL shows competitive performance to existing
state-of-the-art approaches, across image and video-text tasks. The feature
representations learned from image and video-text modalities, allows the model
to achieve competitive performance when finetuned on audio-text tasks, despite
not being pretrained on audio. Thanks to the unified model, we propose a novel
study on multimodal model merging via weight interpolation of models trained on
different multimodal tasks, showing their benefits in particular for
out-of-distribution generalization. Finally, we motivate unification by showing
the synergy between tasks. The model weights and code are released here:
https://github.com/mshukor/UnIVAL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shukor_M/0/1/0/all/0/1&quot;&gt;Mustafa Shukor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dancette_C/0/1/0/all/0/1&quot;&gt;Corentin Dancette&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rame_A/0/1/0/all/0/1&quot;&gt;Alexandre Rame&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1&quot;&gt;Matthieu Cord&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08806">
<title>Self-distillation Regularized Connectionist Temporal Classification Loss for Text Recognition: A Simple Yet Effective Approach. (arXiv:2308.08806v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08806</link>
<description rdf:parseType="Literal">&lt;p&gt;Text recognition methods are gaining rapid development. Some advanced
techniques, e.g., powerful modules, language models, and un- and
semi-supervised learning schemes, consecutively push the performance on public
benchmarks forward. However, the problem of how to better optimize a text
recognition model from the perspective of loss functions is largely overlooked.
CTC-based methods, widely used in practice due to their good balance between
performance and inference speed, still grapple with accuracy degradation. This
is because CTC loss emphasizes the optimization of the entire sequence target
while neglecting to learn individual characters. We propose a self-distillation
scheme for CTC-based model to address this issue. It incorporates a framewise
regularization term in CTC loss to emphasize individual supervision, and
leverages the maximizing-a-posteriori of latent alignment to solve the
inconsistency problem that arises in distillation between CTC-based models. We
refer to the regularized CTC loss as Distillation Connectionist Temporal
Classification (DCTC) loss. DCTC loss is module-free, requiring no extra
parameters, longer inference lag, or additional training data or phases.
Extensive experiments on public benchmarks demonstrate that DCTC can boost text
recognition model accuracy by up to 2.6%, without any of these drawbacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Ziyin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1&quot;&gt;Ning Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1&quot;&gt;Minghui Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yongshuai Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Cheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Min Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1&quot;&gt;Wei Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15216">
<title>On-the-Fly Guidance Training for Medical Image Registration. (arXiv:2308.15216v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15216</link>
<description rdf:parseType="Literal">&lt;p&gt;This research explores a novel approach in the realm of learning-based image
registration, addressing the limitations inherent in weakly-supervised and
unsupervised methods. Weakly-supervised techniques depend heavily on scarce
labeled data, while unsupervised strategies rely on indirect measures of
accuracy through image similarity. Notably, traditional supervised learning is
not utilized due to the lack of precise deformation ground-truth in medical
imaging. Our study introduces a unique training framework with On-the-Fly
Guidance (OFG) to enhance existing models. This framework, during training,
generates pseudo-ground truth a few steps ahead by refining the current
deformation prediction with our custom optimizer. This pseudo-ground truth then
serves to directly supervise the model in a supervised learning context. The
process involves optimizing the predicted deformation with a limited number of
steps, ensuring training efficiency and setting achievable goals for each
training phase. OFG notably boosts the precision of existing image registration
techniques while maintaining the speed of learning-based methods. We assessed
our approach using various pseudo-ground truth generation strategies, including
predictions and optimized outputs from established registration models. Our
experiments spanned three benchmark datasets and three cutting-edge models,
with OFG demonstrating significant and consistent enhancements, surpassing
previous state-of-the-arts in the field. OFG offers an easily integrable
plug-and-play solution to enhance the training effectiveness of learning-based
image registration models. Code at
https://github.com/miraclefactory/on-the-fly-guidance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yicheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shengxiang Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_Y/0/1/0/all/0/1&quot;&gt;Yuelin Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1&quot;&gt;Kun Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohui Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02139">
<title>Self-Supervised Pre-Training Boosts Semantic Scene Segmentation on LiDAR Data. (arXiv:2309.02139v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02139</link>
<description rdf:parseType="Literal">&lt;p&gt;Airborne LiDAR systems have the capability to capture the Earth&apos;s surface by
generating extensive point cloud data comprised of points mainly defined by 3D
coordinates. However, labeling such points for supervised learning tasks is
time-consuming. As a result, there is a need to investigate techniques that can
learn from unlabeled data to significantly reduce the number of annotated
samples. In this work, we propose to train a self-supervised encoder with
Barlow Twins and use it as a pre-trained network in the task of semantic scene
segmentation. The experimental results demonstrate that our unsupervised
pre-training boosts performance once fine-tuned on the supervised task,
especially for under-represented categories.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caros_M/0/1/0/all/0/1&quot;&gt;Mariona Car&amp;#xf3;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Just_A/0/1/0/all/0/1&quot;&gt;Ariadna Just&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Segui_S/0/1/0/all/0/1&quot;&gt;Santi Segu&amp;#xed;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vitria_J/0/1/0/all/0/1&quot;&gt;Jordi Vitri&amp;#xe0;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06978">
<title>Differentiable JPEG: The Devil is in the Details. (arXiv:2309.06978v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06978</link>
<description rdf:parseType="Literal">&lt;p&gt;JPEG remains one of the most widespread lossy image coding methods. However,
the non-differentiable nature of JPEG restricts the application in deep
learning pipelines. Several differentiable approximations of JPEG have recently
been proposed to address this issue. This paper conducts a comprehensive review
of existing diff. JPEG approaches and identifies critical details that have
been missed by previous methods. To this end, we propose a novel diff. JPEG
approach, overcoming previous limitations. Our approach is differentiable
w.r.t. the input image, the JPEG quality, the quantization tables, and the
color conversion parameters. We evaluate the forward and backward performance
of our diff. JPEG approach against existing methods. Additionally, extensive
ablations are performed to evaluate crucial design choices. Our proposed diff.
JPEG resembles the (non-diff.) reference implementation best, significantly
surpassing the recent-best diff. approach by $3.47$dB (PSNR) on average. For
strong compression rates, we can even improve PSNR by $9.51$dB. Strong
adversarial attack results are yielded by our diff. JPEG, demonstrating the
effective gradient approximation. Our code is available at
https://github.com/necla-ml/Diff-JPEG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1&quot;&gt;Christoph Reich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Debnath_B/0/1/0/all/0/1&quot;&gt;Biplob Debnath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1&quot;&gt;Deep Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakradhar_S/0/1/0/all/0/1&quot;&gt;Srimat Chakradhar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17093">
<title>Prototype-based Aleatoric Uncertainty Quantification for Cross-modal Retrieval. (arXiv:2309.17093v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17093</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-modal Retrieval methods build similarity relations between vision and
language modalities by jointly learning a common representation space. However,
the predictions are often unreliable due to the Aleatoric uncertainty, which is
induced by low-quality data, e.g., corrupt images, fast-paced videos, and
non-detailed texts. In this paper, we propose a novel Prototype-based Aleatoric
Uncertainty Quantification (PAU) framework to provide trustworthy predictions
by quantifying the uncertainty arisen from the inherent data ambiguity.
Concretely, we first construct a set of various learnable prototypes for each
modality to represent the entire semantics subspace. Then Dempster-Shafer
Theory and Subjective Logic Theory are utilized to build an evidential
theoretical framework by associating evidence with Dirichlet Distribution
parameters. The PAU model induces accurate uncertainty and reliable predictions
for cross-modal retrieval. Extensive experiments are performed on four major
benchmark datasets of MSR-VTT, MSVD, DiDeMo, and MS-COCO, demonstrating the
effectiveness of our method. The code is accessible at
https://github.com/leolee99/PAU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jingkuan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Lianli Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaosu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Heng Tao Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06000">
<title>Keystroke Verification Challenge (KVC): Biometric and Fairness Benchmark Evaluation. (arXiv:2311.06000v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06000</link>
<description rdf:parseType="Literal">&lt;p&gt;Analyzing keystroke dynamics (KD) for biometric verification has several
advantages: it is among the most discriminative behavioral traits; keyboards
are among the most common human-computer interfaces, being the primary means
for users to enter textual data; its acquisition does not require additional
hardware, and its processing is relatively lightweight; and it allows for
transparently recognizing subjects. However, the heterogeneity of experimental
protocols and metrics, and the limited size of the databases adopted in the
literature impede direct comparisons between different systems, thus
representing an obstacle in the advancement of keystroke biometrics. To
alleviate this aspect, we present a new experimental framework to benchmark
KD-based biometric verification performance and fairness based on tweet-long
sequences of variable transcript text from over 185,000 subjects, acquired
through desktop and mobile keyboards, extracted from the Aalto Keystroke
Databases. The framework runs on CodaLab in the form of the Keystroke
Verification Challenge (KVC). Moreover, we also introduce a novel fairness
metric, the Skewed Impostor Ratio (SIR), to capture inter- and
intra-demographic group bias patterns in the verification scores. We
demonstrate the usefulness of the proposed framework by employing two
state-of-the-art keystroke verification systems, TypeNet and TypeFormer, to
compare different sets of input features, achieving a less privacy-invasive
system, by discarding the analysis of text content (ASCII codes of the keys
pressed) in favor of extended features in the time domain. Our experiments show
that this approach allows to maintain satisfactory performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stragapede_G/0/1/0/all/0/1&quot;&gt;Giuseppe Stragapede&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1&quot;&gt;Ruben Vera-Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1&quot;&gt;Ruben Tolosana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1&quot;&gt;Aythami Morales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1&quot;&gt;Naser Damer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1&quot;&gt;Julian Fierrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1&quot;&gt;Javier Ortega-Garcia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08655">
<title>Review of AlexNet for Medical Image Classification. (arXiv:2311.08655v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08655</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the rapid development of deep learning has led to a wide
range of applications in the field of medical image classification. The
variants of neural network models with ever-increasing performance share some
commonalities: to try to mitigate overfitting, improve generalization, avoid
gradient vanishing and exploding, etc. AlexNet first utilizes the dropout
technique to mitigate overfitting and the ReLU activation function to avoid
gradient vanishing. Therefore, we focus our discussion on AlexNet, which has
contributed greatly to the development of CNNs in 2012. After reviewing over 40
papers, including journal papers and conference papers, we give a narrative on
the technical details, advantages, and application areas of AlexNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1&quot;&gt;Wenhao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Junding Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuihua Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yudong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09759">
<title>Scene Text Image Super-resolution based on Text-conditional Diffusion Models. (arXiv:2311.09759v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09759</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene Text Image Super-resolution (STISR) has recently achieved great success
as a preprocessing method for scene text recognition. STISR aims to transform
blurred and noisy low-resolution (LR) text images in real-world settings into
clear high-resolution (HR) text images suitable for scene text recognition. In
this study, we leverage text-conditional diffusion models (DMs), known for
their impressive text-to-image synthesis capabilities, for STISR tasks. Our
experimental results revealed that text-conditional DMs notably surpass
existing STISR methods. Especially when texts from LR text images are given as
input, the text-conditional DMs are able to produce superior quality
super-resolution text images. Utilizing this capability, we propose a novel
framework for synthesizing LR-HR paired text image datasets. This framework
consists of three specialized text-conditional DMs, each dedicated to text
image synthesis, super-resolution, and image degradation. These three modules
are vital for synthesizing distinct LR and HR paired images, which are more
suitable for training STISR methods. Our experiments confirmed that these
synthesized image pairs significantly enhance the performance of STISR methods
in the TextZoom evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noguchi_C/0/1/0/all/0/1&quot;&gt;Chihiro Noguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fukuda_S/0/1/0/all/0/1&quot;&gt;Shun Fukuda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamanaka_M/0/1/0/all/0/1&quot;&gt;Masao Yamanaka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13613">
<title>Spanning Training Progress: Temporal Dual-Depth Scoring (TDDS) for Enhanced Dataset Pruning. (arXiv:2311.13613v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13613</link>
<description rdf:parseType="Literal">&lt;p&gt;Dataset pruning aims to construct a coreset capable of achieving performance
comparable to the original, full dataset. Most existing dataset pruning methods
rely on snapshot-based criteria to identify representative samples, often
resulting in poor generalization across various pruning and cross-architecture
scenarios. Recent studies have addressed this issue by expanding the scope of
training dynamics considered, including factors such as forgetting event and
probability change, typically using an averaging approach. However, these works
struggle to integrate a broader range of training dynamics without overlooking
well-generalized samples, which may not be sufficiently highlighted in an
averaging manner. In this study, we propose a novel dataset pruning method
termed as Temporal Dual-Depth Scoring (TDDS), to tackle this problem. TDDS
utilizes a dual-depth strategy to achieve a balance between incorporating
extensive training dynamics and identifying representative samples for dataset
pruning. In the first depth, we estimate the series of each sample&apos;s individual
contributions spanning the training progress, ensuring comprehensive
integration of training dynamics. In the second depth, we focus on the
variability of the sample-wise contributions identified in the first depth to
highlight well-generalized samples. Extensive experiments conducted on CIFAR
and ImageNet datasets verify the superiority of TDDS over previous SOTA
methods. Specifically on CIFAR-100, our method achieves 54.51% accuracy with
only 10% training data, surpassing random selection by 7.83% and other
comparison methods by at least 12.69%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;Jiawei Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunsong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weiying Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Joey Tianyi Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16502">
<title>MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. (arXiv:2311.16502v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16502</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce MMMU: a new benchmark designed to evaluate multimodal models on
massive multi-discipline tasks demanding college-level subject knowledge and
deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal
questions from college exams, quizzes, and textbooks, covering six core
disciplines: Art &amp;amp; Design, Business, Science, Health &amp;amp; Medicine, Humanities &amp;amp;
Social Science, and Tech &amp;amp; Engineering. These questions span 30 subjects and
183 subfields, comprising 30 highly heterogeneous image types, such as charts,
diagrams, maps, tables, music sheets, and chemical structures. Unlike existing
benchmarks, MMMU focuses on advanced perception and reasoning with
domain-specific knowledge, challenging models to perform tasks akin to those
faced by experts. The evaluation of 14 open-source LMMs as well as the
proprietary GPT-4V(ision) and Gemini highlights the substantial challenges
posed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve
accuracies of 56% and 59% respectively, indicating significant room for
improvement. We believe MMMU will stimulate the community to build
next-generation multimodal foundation models towards expert artificial general
intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xiang Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1&quot;&gt;Yuansheng Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1&quot;&gt;Tianyu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ruoqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Ge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stevens_S/0/1/0/all/0/1&quot;&gt;Samuel Stevens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1&quot;&gt;Dongfu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1&quot;&gt;Weiming Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1&quot;&gt;Cong Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Botao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1&quot;&gt;Ruibin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1&quot;&gt;Renliang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1&quot;&gt;Ming Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1&quot;&gt;Boyuan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhenzhu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yibo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenhao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Huan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yu Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18083">
<title>Meta Co-Training: Two Views are Better than One. (arXiv:2311.18083v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18083</link>
<description rdf:parseType="Literal">&lt;p&gt;In many practical computer vision scenarios unlabeled data is plentiful, but
labels are scarce and difficult to obtain. As a result, semi-supervised
learning which leverages unlabeled data to boost the performance of supervised
classifiers have received significant attention in recent literature. One major
class of semi-supervised algorithms is co-training. In co-training two
different models leverage different independent and sufficient &quot;views&quot; of the
data to jointly make better predictions. During co-training each model creates
pseudo labels on unlabeled points which are used to improve the other model. We
show that in the common case when independent views are not available we can
construct such views inexpensively using pre-trained models. Co-training on the
constructed views yields a performance improvement over any of the individual
views we construct and performance comparable with recent approaches in
semi-supervised learning, but has some undesirable properties. To alleviate the
issues present with co-training we present Meta Co-Training which is an
extension of the successful Meta Pseudo Labels approach to two views. Our
method achieves new state-of-the-art performance on ImageNet-10% with very few
training resources, as well as outperforming prior semi-supervised work on
several other fine-grained image classification datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rothenberger_J/0/1/0/all/0/1&quot;&gt;Jay C. Rothenberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diochnos_D/0/1/0/all/0/1&quot;&gt;Dimitrios I. Diochnos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06275">
<title>DG-TTA: Out-of-domain medical image segmentation through Domain Generalization and Test-Time Adaptation. (arXiv:2312.06275v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06275</link>
<description rdf:parseType="Literal">&lt;p&gt;Applying pre-trained medical segmentation models on out-of-domain images
often yields predictions of insufficient quality. Several strategies have been
proposed to maintain model performance, such as finetuning or unsupervised- and
source-free domain adaptation. These strategies set restrictive requirements
for data availability. In this study, we propose to combine domain
generalization and test-time adaptation to create a highly effective approach
for reusing pre-trained models in unseen target domains. Domain-generalized
pre-training on source data is used to obtain the best initial performance in
the target domain. We introduce the MIND descriptor previously used in image
registration tasks as a further technique to achieve generalization and present
superior performance for small-scale datasets compared to existing approaches.
At test-time, high-quality segmentation for every single unseen scan is ensured
by optimizing the model weights for consistency given different image
augmentations. That way, our method enables separate use of source and target
data and thus removes current data availability barriers. Moreover, the
presented method is highly modular as it does not require specific model
architectures or prior knowledge of involved domains and labels. We demonstrate
this by integrating it into the nnUNet, which is currently the most popular and
accurate framework for medical image segmentation. We employ multiple datasets
covering abdominal, cardiac, and lumbar spine scans and compose several
out-of-domain scenarios in this study. We demonstrate that our method, combined
with pre-trained whole-body CT models, can effectively segment MR images with
high accuracy in all of the aforementioned scenarios. Open-source code can be
found here: https://github.com/multimodallearning/DG-TTA
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weihsbach_C/0/1/0/all/0/1&quot;&gt;Christian Weihsbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kruse_C/0/1/0/all/0/1&quot;&gt;Christian N. Kruse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bigalke_A/0/1/0/all/0/1&quot;&gt;Alexander Bigalke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinrich_M/0/1/0/all/0/1&quot;&gt;Mattias P. Heinrich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06709">
<title>AM-RADIO: Agglomerative Model -- Reduce All Domains Into One. (arXiv:2312.06709v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06709</link>
<description rdf:parseType="Literal">&lt;p&gt;A handful of visual foundation models (VFMs) have recently emerged as the
backbones for numerous downstream tasks. VFMs like CLIP, DINOv2, SAM are
trained with distinct objectives, exhibiting unique characteristics for various
downstream tasks. We find that despite their conceptual differences, these
models can be effectively merged into a unified model through multi-teacher
distillation. We name this approach AM-RADIO (Agglomerative Model -- Reduce All
Domains Into One). This integrative approach not only surpasses the performance
of individual teacher models but also amalgamates their distinctive features,
such as zero-shot vision-language comprehension, detailed pixel-level
understanding, and open vocabulary segmentation capabilities. In pursuit of the
most hardware-efficient backbone, we evaluated numerous architectures in our
multi-teacher distillation pipeline using the same training recipe. This led to
the development of a novel architecture (E-RADIO) that exceeds the performance
of its predecessors and is at least 7x faster than the teacher models. Our
comprehensive benchmarking process covers downstream tasks including ImageNet
classification, ADE20k semantic segmentation, COCO object detection and
LLaVa-1.5 framework.
&lt;/p&gt;
&lt;p&gt;Code: https://github.com/NVlabs/RADIO
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranzinger_M/0/1/0/all/0/1&quot;&gt;Mike Ranzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinrich_G/0/1/0/all/0/1&quot;&gt;Greg Heinrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1&quot;&gt;Jan Kautz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1&quot;&gt;Pavlo Molchanov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06914">
<title>Exploring Novel Object Recognition and Spontaneous Location Recognition Machine Learning Analysis Techniques in Alzheimer&apos;s Mice. (arXiv:2312.06914v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06914</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding object recognition patterns in mice is crucial for advancing
behavioral neuroscience and has significant implications for human health,
particularly in the realm of Alzheimer&apos;s research. This study is centered on
the development, application, and evaluation of a state-of-the-art
computational pipeline designed to analyze such behaviors, specifically
focusing on Novel Object Recognition (NOR) and Spontaneous Location Recognition
(SLR) tasks. The pipeline integrates three advanced computational models:
Any-Maze for initial data collection, DeepLabCut for detailed pose estimation,
and Convolutional Neural Networks (CNNs) for nuanced behavioral classification.
Employed across four distinct mouse groups, this pipeline demonstrated high
levels of accuracy and robustness. Despite certain challenges like video
quality limitations and the need for manual calculations, the results affirm
the pipeline&apos;s efficacy and potential for scalability. The study serves as a
proof of concept for a multidimensional computational approach to behavioral
neuroscience, emphasizing the pipeline&apos;s versatility and readiness for future,
more complex analyses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bafana_S/0/1/0/all/0/1&quot;&gt;Soham Bafana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07199">
<title>SeasFire as a Multivariate Earth System Datacube for Wildfire Dynamics. (arXiv:2312.07199v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07199</link>
<description rdf:parseType="Literal">&lt;p&gt;The global occurrence, scale, and frequency of wildfires pose significant
threats to ecosystem services and human livelihoods. To effectively quantify
and attribute the antecedent conditions for wildfires, a thorough understanding
of Earth system dynamics is imperative. In response, we introduce the SeasFire
datacube, a meticulously curated spatiotemporal dataset tailored for global
sub-seasonal to seasonal wildfire modeling via Earth observation. The SeasFire
datacube comprises of 59 variables encompassing climate, vegetation, oceanic
indices, and human factors, has an 8-day temporal resolution and a spatial
resolution of 0.25$^{\circ}$, and spans from 2001 to 2021. We showcase the
versatility of SeasFire for exploring the variability and seasonality of
wildfire drivers, modeling causal links between ocean-climate teleconnections
and wildfires, and predicting sub-seasonal wildfire patterns across multiple
timescales with a Deep Learning model. We publicly release the SeasFire
datacube and appeal to Earth system scientists and Machine Learning
practitioners to use it for an improved understanding and anticipation of
wildfires.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karasante_I/0/1/0/all/0/1&quot;&gt;Ilektra Karasante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_L/0/1/0/all/0/1&quot;&gt;Lazaro Alonso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prapas_I/0/1/0/all/0/1&quot;&gt;Ioannis Prapas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahuja_A/0/1/0/all/0/1&quot;&gt;Akanksha Ahuja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carvalhais_N/0/1/0/all/0/1&quot;&gt;Nuno Carvalhais&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papoutsis_I/0/1/0/all/0/1&quot;&gt;Ioannis Papoutsis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07661">
<title>CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor. (arXiv:2312.07661v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07661</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing open-vocabulary image segmentation methods require a fine-tuning
step on mask annotations and/or image-text datasets. Mask labels are
labor-intensive, which limits the number of categories in segmentation
datasets. As a result, the open-vocabulary capacity of pre-trained VLMs is
severely reduced after fine-tuning. However, without fine-tuning, VLMs trained
under weak image-text supervision tend to make suboptimal mask predictions when
there are text queries referring to non-existing concepts in the image. To
alleviate these issues, we introduce a novel recurrent framework that
progressively filters out irrelevant texts and enhances mask quality without
training efforts. The recurrent unit is a two-stage segmenter built upon a VLM
with frozen weights. Thus, our model retains the VLM&apos;s broad vocabulary space
and strengthens its segmentation capability. Experimental results show that our
method outperforms not only the training-free counterparts, but also those
fine-tuned with millions of additional data samples, and sets new
state-of-the-art records for both zero-shot semantic and referring image
segmentation tasks. Specifically, we improve the current record by 28.8, 16.0,
and 6.9 mIoU on Pascal VOC, COCO Object, and Pascal Context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shuyang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Runjia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip Torr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xiuye Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Siyang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07884">
<title>Mutual-Learning Knowledge Distillation for Nighttime UAV Tracking. (arXiv:2312.07884v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07884</link>
<description rdf:parseType="Literal">&lt;p&gt;Nighttime unmanned aerial vehicle (UAV) tracking has been facilitated with
indispensable plug-and-play low-light enhancers. However, the introduction of
low-light enhancers increases the extra computational burden for the UAV,
significantly hindering the development of real-time UAV applications.
Meanwhile, these state-of-the-art (SOTA) enhancers lack tight coupling with the
advanced daytime UAV tracking approach. To solve the above issues, this work
proposes a novel mutual-learning knowledge distillation framework for nighttime
UAV tracking, i.e., MLKD. This framework is constructed to learn a compact and
fast nighttime tracker via knowledge transferring from the teacher and
knowledge sharing among various students. Specifically, an advanced teacher
based on a SOTA enhancer and a superior tracking backbone is adopted for
guiding the student based only on the tight coupling-aware tracking backbone to
directly extract nighttime object features. To address the biased learning of a
single student, diverse lightweight students with different distillation
methods are constructed to focus on various aspects of the teacher&apos;s knowledge.
Moreover, an innovative mutual-learning room is designed to elect the superior
student candidate to assist the remaining students frame-by-frame in the
training phase. Furthermore, the final best student, i.e., MLKD-Track, is
selected through the testing dataset. Extensive experiments demonstrate the
effectiveness and superiority of MLKD and MLKD-Track. The practicality of the
MLKD-Track is verified in real-world tests with different challenging
situations. The code is available at https://github.com/lyfeng001/MLKD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yufeng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08914">
<title>CogAgent: A Visual Language Model for GUI Agents. (arXiv:2312.08914v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08914</link>
<description rdf:parseType="Literal">&lt;p&gt;People are spending an enormous amount of time on digital devices through
graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large
language models (LLMs) such as ChatGPT can assist people in tasks like writing
emails, but struggle to understand and interact with GUIs, thus limiting their
potential to increase automation levels. In this paper, we introduce CogAgent,
an 18-billion-parameter visual language model (VLM) specializing in GUI
understanding and navigation. By utilizing both low-resolution and
high-resolution image encoders, CogAgent supports input at a resolution of
1120*1120, enabling it to recognize tiny page elements and text. As a
generalist visual language model, CogAgent achieves the state of the art on
five text-rich and four general VQA benchmarks, including VQAv2, OK-VQA,
Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using
only screenshots as input, outperforms LLM-based methods that consume extracted
HTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW,
advancing the state of the art. The model and codes are available at
https://github.com/THUDM/CogVLM .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1&quot;&gt;Wenyi Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weihan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1&quot;&gt;Qingsong Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiazheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wenmeng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Junhui Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Juanzi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Bin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Ming Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jie Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09854">
<title>Q-Segment: Segmenting Images In-Sensor for Vessel-Based Medical Diagnosis. (arXiv:2312.09854v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09854</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the growing interest in deploying deep learning models
directly in-sensor. We present &quot;Q-Segment&quot;, a quantized real-time segmentation
algorithm, and conduct a comprehensive evaluation on a low-power edge vision
platform with an in-sensors processor, the Sony IMX500. One of the main goals
of the model is to achieve end-to-end image segmentation for vessel-based
medical diagnosis. Deployed on the IMX500 platform, Q-Segment achieves
ultra-low inference time in-sensor only 0.23 ms and power consumption of only
72mW. We compare the proposed network with state-of-the-art models, both float
and quantized, demonstrating that the proposed solution outperforms existing
networks on various platforms in computing efficiency, e.g., by a factor of 75x
compared to ERFNet. The network employs an encoder-decoder structure with skip
connections, and results in a binary accuracy of 97.25% and an Area Under the
Receiver Operating Characteristic Curve (AUC) of 96.97% on the CHASE dataset.
We also present a comparison of the IMX500 processing core with the Sony
Spresense, a low-power multi-core ARM Cortex-M microcontroller, and a
single-core ARM Cortex-M4 showing that it can achieve in-sensor processing with
end-to-end low latency (17 ms) and power concumption (254mW). This research
contributes valuable insights into edge-based image segmentation, laying the
foundation for efficient algorithms tailored to low-power environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bonazzi_P/0/1/0/all/0/1&quot;&gt;Pietro Bonazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moosmann_J/0/1/0/all/0/1&quot;&gt;Julian Moosmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bian_S/0/1/0/all/0/1&quot;&gt;Sizhen Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Magno_M/0/1/0/all/0/1&quot;&gt;Michele Magno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11146">
<title>OsmLocator: locating overlapping scatter marks with a non-training generative perspective. (arXiv:2312.11146v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11146</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated mark localization in scatter images, greatly helpful for
discovering knowledge and understanding enormous document images and reasoning
in visual question answering AI systems, is a highly challenging problem
because of the ubiquity of overlapping marks. Locating overlapping marks faces
many difficulties such as no texture, less contextual information, hallow shape
and tiny size. Here, we formulate it as a combinatorial optimization problem on
clustering-based re-visualization from a non-training generative perspective,
to locate scatter marks by finding the status of multi-variables when an
objective function reaches a minimum. The objective function is constructed on
difference between binarized scatter images and corresponding generated
re-visualization based on their clustering. Fundamentally, re-visualization
tries to generate a new scatter graph only taking a rasterized scatter image as
an input, and clustering is employed to provide the information for such
re-visualization. This method could stably locate severely-overlapping,
variable-size and variable-shape marks in scatter images without dependence of
any training dataset or reference. Meanwhile, we propose an adaptive variant of
simulated annealing which can works on various connected regions. In addition,
we especially built a dataset named SML2023 containing hundreds of scatter
images with different markers and various levels of overlapping severity, and
tested the proposed method and compared it to existing methods. The results
show that it can accurately locate most marks in scatter images with different
overlapping severity and marker types, with about 0.3 absolute increase on an
assignment-cost-based metric in comparison with state-of-the-art methods. This
work is of value to data mining on massive web pages and literatures, and
shedding new light on image measurement such as bubble counting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1&quot;&gt;Yuming Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pizurica_A/0/1/0/all/0/1&quot;&gt;Aleksandra Pizurica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ming_Q/0/1/0/all/0/1&quot;&gt;Qi Ming&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nadisic_N/0/1/0/all/0/1&quot;&gt;Nicolas Nadisic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11537">
<title>FastSR-NeRF: Improving NeRF Efficiency on Consumer Devices with A Simple Super-Resolution Pipeline. (arXiv:2312.11537v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11537</link>
<description rdf:parseType="Literal">&lt;p&gt;Super-resolution (SR) techniques have recently been proposed to upscale the
outputs of neural radiance fields (NeRF) and generate high-quality images with
enhanced inference speeds. However, existing NeRF+SR methods increase training
overhead by using extra input features, loss functions, and/or expensive
training procedures such as knowledge distillation. In this paper, we aim to
leverage SR for efficiency gains without costly training or architectural
changes. Specifically, we build a simple NeRF+SR pipeline that directly
combines existing modules, and we propose a lightweight augmentation technique,
random patch sampling, for training. Compared to existing NeRF+SR methods, our
pipeline mitigates the SR computing overhead and can be trained up to 23x
faster, making it feasible to run on consumer devices such as the Apple
MacBook. Experiments show our pipeline can upscale NeRF outputs by 2-4x while
maintaining high quality, increasing inference speeds by up to 18x on an NVIDIA
V100 GPU and 12.8x on an M1 Pro chip. We conclude that SR can be a simple but
effective technique for improving the efficiency of NeRF models for consumer
devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chien-Yu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1&quot;&gt;Qichen Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merth_T/0/1/0/all/0/1&quot;&gt;Thomas Merth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Karren Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranjan_A/0/1/0/all/0/1&quot;&gt;Anurag Ranjan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12865">
<title>RadEdit: stress-testing biomedical vision models via diffusion image editing. (arXiv:2312.12865v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12865</link>
<description rdf:parseType="Literal">&lt;p&gt;Biomedical imaging datasets are often small and biased, meaning that
real-world performance of predictive models can be substantially lower than
expected from internal testing. This work proposes using generative image
editing to simulate dataset shifts and diagnose failure modes of biomedical
vision models; this can be used in advance of deployment to assess readiness,
potentially reducing cost and patient harm. Existing editing methods can
produce undesirable changes, with spurious correlations learned due to the
co-occurrence of disease and treatment interventions, limiting practical
applicability. To address this, we train a text-to-image diffusion model on
multiple chest X-ray datasets and introduce a new editing method RadEdit that
uses multiple masks, if present, to constrain changes and ensure consistency in
the edited images. We consider three types of dataset shifts: acquisition
shift, manifestation shift, and population shift, and demonstrate that our
approach can diagnose failures and quantify model robustness without additional
data collection, complementing more qualitative tools for explainable AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_Garcia_F/0/1/0/all/0/1&quot;&gt;Fernando P&amp;#xe9;rez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bond_Taylor_S/0/1/0/all/0/1&quot;&gt;Sam Bond-Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1&quot;&gt;Pedro P. Sanchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breugel_B/0/1/0/all/0/1&quot;&gt;Boris van Breugel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1&quot;&gt;Daniel C. Castro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_H/0/1/0/all/0/1&quot;&gt;Harshita Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salvatelli_V/0/1/0/all/0/1&quot;&gt;Valentina Salvatelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wetscherek_M/0/1/0/all/0/1&quot;&gt;Maria T. A. Wetscherek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richardson_H/0/1/0/all/0/1&quot;&gt;Hannah Richardson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1&quot;&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nori_A/0/1/0/all/0/1&quot;&gt;Aditya Nori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_Valle_J/0/1/0/all/0/1&quot;&gt;Javier Alvarez-Valle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oktay_O/0/1/0/all/0/1&quot;&gt;Ozan Oktay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilse_M/0/1/0/all/0/1&quot;&gt;Maximilian Ilse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13016">
<title>DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis. (arXiv:2312.13016v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13016</link>
<description rdf:parseType="Literal">&lt;p&gt;We present DiffPortrait3D, a conditional diffusion model that is capable of
synthesizing 3D-consistent photo-realistic novel views from as few as a single
in-the-wild portrait. Specifically, given a single RGB input, we aim to
synthesize plausible but consistent facial details rendered from novel camera
views with retained both identity and facial expression. In lieu of
time-consuming optimization and fine-tuning, our zero-shot method generalizes
well to arbitrary face portraits with unposed camera views, extreme facial
expressions, and diverse artistic depictions. At its core, we leverage the
generative prior of 2D diffusion models pre-trained on large-scale image
datasets as our rendering backbone, while the denoising is guided with
disentangled attentive control of appearance and camera pose. To achieve this,
we first inject the appearance context from the reference image into the
self-attention layers of the frozen UNets. The rendering view is then
manipulated with a novel conditional control module that interprets the camera
pose by watching a condition image of a crossed subject from the same view.
Furthermore, we insert a trainable cross-view attention module to enhance view
consistency, which is further strengthened with a novel 3D-aware noise
generation process during inference. We demonstrate state-of-the-art results
both qualitatively and quantitatively on our challenging in-the-wild and
multi-view benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yuming Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;You Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hongyi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1&quot;&gt;Guoxian Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yichun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1&quot;&gt;Di Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Linjie Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13091">
<title>MoSAR: Monocular Semi-Supervised Model for Avatar Reconstruction using Differentiable Shading. (arXiv:2312.13091v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13091</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing an avatar from a portrait image has many applications in
multimedia, but remains a challenging research problem. Extracting reflectance
maps and geometry from one image is ill-posed: recovering geometry is a
one-to-many mapping problem and reflectance and light are difficult to
disentangle. Accurate geometry and reflectance can be captured under the
controlled conditions of a light stage, but it is costly to acquire large
datasets in this fashion. Moreover, training solely with this type of data
leads to poor generalization with in-the-wild images. This motivates the
introduction of MoSAR, a method for 3D avatar generation from monocular images.
We propose a semi-supervised training scheme that improves generalization by
learning from both light stage and in-the-wild datasets. This is achieved using
a novel differentiable shading formulation. We show that our approach
effectively disentangles the intrinsic face parameters, producing relightable
avatars. As a result, MoSAR estimates a richer set of skin reflectance maps,
and generates more realistic avatars than existing state-of-the-art methods. We
also introduce a new dataset, named FFHQ-UV-Intrinsics, the first public
dataset providing intrinsic face attributes at scale (diffuse, specular,
ambient occlusion and translucency maps) for a total of 10k subjects. The
project website and the dataset are available on the following link:
https://ubisoft-laforge.github.io/character/mosar/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dib_A/0/1/0/all/0/1&quot;&gt;Abdallah Dib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hafemann_L/0/1/0/all/0/1&quot;&gt;Luiz Gustavo Hafemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Got_E/0/1/0/all/0/1&quot;&gt;Emeline Got&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_T/0/1/0/all/0/1&quot;&gt;Trevor Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fadaeinejad_A/0/1/0/all/0/1&quot;&gt;Amin Fadaeinejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_R/0/1/0/all/0/1&quot;&gt;Rafael M. O. Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carbonneau_M/0/1/0/all/0/1&quot;&gt;Marc-Andre Carbonneau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13646">
<title>Weakly Supervised Semantic Segmentation for Driving Scenes. (arXiv:2312.13646v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13646</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art techniques in weakly-supervised semantic segmentation (WSSS)
using image-level labels exhibit severe performance degradation on driving
scene datasets such as Cityscapes. To address this challenge, we develop a new
WSSS framework tailored to driving scene datasets. Based on extensive analysis
of dataset characteristics, we employ Contrastive Language-Image Pre-training
(CLIP) as our baseline to obtain pseudo-masks. However, CLIP introduces two key
challenges: (1) pseudo-masks from CLIP lack in representing small object
classes, and (2) these masks contain notable noise. We propose solutions for
each issue as follows. (1) We devise Global-Local View Training that seamlessly
incorporates small-scale patches during model training, thereby enhancing the
model&apos;s capability to handle small-sized yet critical objects in driving scenes
(e.g., traffic light). (2) We introduce Consistency-Aware Region Balancing
(CARB), a novel technique that discerns reliable and noisy regions through
evaluating the consistency between CLIP masks and segmentation predictions. It
prioritizes reliable pixels over noisy pixels via adaptive loss weighting.
Notably, the proposed method achieves 51.8\% mIoU on the Cityscapes test
dataset, showcasing its potential as a strong WSSS baseline on driving scene
datasets. Experimental results on CamVid and WildDash2 demonstrate the
effectiveness of our method across diverse datasets, even with small-scale
datasets or visually challenging conditions. The code is available at
https://github.com/k0u-id/CARB.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dongseob Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seungho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1&quot;&gt;Junsuk Choe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1&quot;&gt;Hyunjung Shim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13729">
<title>Gaussian Splatting with NeRF-based Color and Opacity. (arXiv:2312.13729v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13729</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRFs) have demonstrated the remarkable potential of
neural networks to capture the intricacies of 3D objects. By encoding the shape
and color information within neural network weights, NeRFs excel at producing
strikingly sharp novel views of 3D objects. Recently, numerous generalizations
of NeRFs utilizing generative models have emerged, expanding its versatility.
In contrast, Gaussian Splatting (GS) offers a similar renders quality with
faster training and inference as it does not need neural networks to work. We
encode information about the 3D objects in the set of Gaussian distributions
that can be rendered in 3D similarly to classical meshes. Unfortunately, GS are
difficult to condition since they usually require circa hundred thousand
Gaussian components. To mitigate the caveats of both models, we propose a
hybrid model that uses GS representation of the 3D object&apos;s shape and
NeRF-based encoding of color and opacity. Our model uses Gaussian distributions
with trainable positions (i.e. means of Gaussian), shape (i.e. covariance of
Gaussian), color and opacity, and neural network, which takes parameters of
Gaussian and viewing direction to produce changes in color and opacity.
Consequently, our model better describes shadows, light reflections, and
transparency of 3D objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malarz_D/0/1/0/all/0/1&quot;&gt;Dawid Malarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smolak_W/0/1/0/all/0/1&quot;&gt;Weronika Smolak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1&quot;&gt;Jacek Tabor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tadeja_S/0/1/0/all/0/1&quot;&gt;S&amp;#x142;awomir Tadeja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spurek_P/0/1/0/all/0/1&quot;&gt;Przemys&amp;#x142;aw Spurek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13771">
<title>AppAgent: Multimodal Agents as Smartphone Users. (arXiv:2312.13771v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13771</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in large language models (LLMs) have led to the creation
of intelligent agents capable of performing complex tasks. This paper
introduces a novel LLM-based multimodal agent framework designed to operate
smartphone applications. Our framework enables the agent to operate smartphone
applications through a simplified action space, mimicking human-like
interactions such as tapping and swiping. This novel approach bypasses the need
for system back-end access, thereby broadening its applicability across diverse
apps. Central to our agent&apos;s functionality is its innovative learning method.
The agent learns to navigate and use new apps either through autonomous
exploration or by observing human demonstrations. This process generates a
knowledge base that the agent refers to for executing complex tasks across
different applications. To demonstrate the practicality of our agent, we
conducted extensive testing over 50 tasks in 10 different applications,
including social media, email, maps, shopping, and sophisticated image editing
tools. The results affirm our agent&apos;s proficiency in handling a diverse array
of high-level tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaxuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yucheng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zebiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1&quot;&gt;Bin Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Gang Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13913">
<title>Paint3D: Paint Anything 3D with Lighting-Less Texture Diffusion Models. (arXiv:2312.13913v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13913</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents Paint3D, a novel coarse-to-fine generative framework that
is capable of producing high-resolution, lighting-less, and diverse 2K UV
texture maps for untextured 3D meshes conditioned on text or image inputs. The
key challenge addressed is generating high-quality textures without embedded
illumination information, which allows the textures to be re-lighted or
re-edited within modern graphics pipelines. To achieve this, our method first
leverages a pre-trained depth-aware 2D diffusion model to generate
view-conditional images and perform multi-view texture fusion, producing an
initial coarse texture map. However, as 2D models cannot fully represent 3D
shapes and disable lighting effects, the coarse texture map exhibits incomplete
areas and illumination artifacts. To resolve this, we train separate UV
Inpainting and UVHD diffusion models specialized for the shape-aware refinement
of incomplete areas and the removal of illumination artifacts. Through this
coarse-to-fine process, Paint3D can produce high-quality 2K UV textures that
maintain semantic consistency while being lighting-less, significantly
advancing the state-of-the-art in texturing 3D objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xianfang Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1&quot;&gt;Zhongqi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zibo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhibin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1&quot;&gt;Bin Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Gang Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13977">
<title>NeuSurf: On-Surface Priors for Neural Surface Reconstruction from Sparse Input Views. (arXiv:2312.13977v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13977</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, neural implicit functions have demonstrated remarkable results in
the field of multi-view reconstruction. However, most existing methods are
tailored for dense views and exhibit unsatisfactory performance when dealing
with sparse views. Several latest methods have been proposed for generalizing
implicit reconstruction to address the sparse view reconstruction task, but
they still suffer from high training costs and are merely valid under carefully
selected perspectives. In this paper, we propose a novel sparse view
reconstruction framework that leverages on-surface priors to achieve highly
faithful surface reconstruction. Specifically, we design several constraints on
global geometry alignment and local geometry refinement for jointly optimizing
coarse shapes and fine details. To achieve this, we train a neural network to
learn a global implicit field from the on-surface points obtained from SfM and
then leverage it as a coarse geometric constraint. To exploit local geometric
consistency, we project on-surface points onto seen and unseen views, treating
the consistent loss of projected features as a fine geometric constraint. The
experimental results with DTU and BlendedMVS datasets in two prevalent sparse
settings demonstrate significant improvements over the state-of-the-art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Han Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yulun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Junsheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1&quot;&gt;Ge Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_M/0/1/0/all/0/1&quot;&gt;Ming Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu-Shen Liu&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>