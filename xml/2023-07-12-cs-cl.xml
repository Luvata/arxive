<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CL updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-11T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computation and Language</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04804" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04821" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04827" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04907" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05034" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05081" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05082" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05083" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05113" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05131" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05174" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05228" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05260" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05354" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05355" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05414" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05440" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05454" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2107.00439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.09523" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.03318" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.06960" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.06385" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09603" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.06862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.02168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09841" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02738" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03109" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.04804">
<title>S2vNTM: Semi-supervised vMF Neural Topic Modeling. (arXiv:2307.04804v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.04804</link>
<description rdf:parseType="Literal">&lt;p&gt;Language model based methods are powerful techniques for text classification.
However, the models have several shortcomings. (1) It is difficult to integrate
human knowledge such as keywords. (2) It needs a lot of resources to train the
models. (3) It relied on large text data to pretrain. In this paper, we propose
Semi-Supervised vMF Neural Topic Modeling (S2vNTM) to overcome these
difficulties. S2vNTM takes a few seed keywords as input for topics. S2vNTM
leverages the pattern of keywords to identify potential topics, as well as
optimize the quality of topics&apos; keywords sets. Across a variety of datasets,
S2vNTM outperforms existing semi-supervised topic modeling methods in
classification accuracy with limited keywords provided. S2vNTM is at least
twice as fast as baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weijie Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desai_J/0/1/0/all/0/1&quot;&gt;Jay Desai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengamedu_S/0/1/0/all/0/1&quot;&gt;Srinivasan Sengamedu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iannacci_F/0/1/0/all/0/1&quot;&gt;Francis Iannacci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04821">
<title>Amplifying Limitations, Harms and Risks of Large Language Models. (arXiv:2307.04821v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.04821</link>
<description rdf:parseType="Literal">&lt;p&gt;We present this article as a small gesture in an attempt to counter what
appears to be exponentially growing hype around Artificial Intelligence (AI)
and its capabilities, and the distraction provided by the associated talk of
science-fiction scenarios that might arise if AI should become sentient and
super-intelligent. It may also help those outside of the field to become more
informed about some of the limitations of AI technology. In the current context
of popular discourse AI defaults to mean foundation and large language models
(LLMs) such as those used to create ChatGPT. This in itself is a
misrepresentation of the diversity, depth and volume of research, researchers,
and technology that truly represents the field of AI. AI being a field of
research that has existed in software artefacts since at least the 1950&apos;s. We
set out to highlight a number of limitations of LLMs, and in so doing highlight
that harms have already arisen and will continue to arise due to these
limitations. Along the way we also highlight some of the associated risks for
individuals and organisations in using this technology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ONeill_M/0/1/0/all/0/1&quot;&gt;Michael O&amp;#x27;Neill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Connor_M/0/1/0/all/0/1&quot;&gt;Mark Connor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04827">
<title>LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad. (arXiv:2307.04827v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.04827</link>
<description rdf:parseType="Literal">&lt;p&gt;Launchpad is a musical instrument that allows users to create and perform
music by pressing illuminated buttons. To assist and inspire the design of the
Launchpad light effect, and provide a more accessible approach for beginners to
create music visualization with this instrument, we proposed the LaunchpadGPT
model to generate music visualization designs on Launchpad automatically. Based
on the language model with excellent generation ability, our proposed
LaunchpadGPT takes an audio piece of music as input and outputs the lighting
effects of Launchpad-playing in the form of a video (Launchpad-playing video).
We collect Launchpad-playing videos and process them to obtain music and
corresponding video frame of Launchpad-playing as prompt-completion pairs, to
train the language model. The experiment result shows the proposed method can
create better music visualization than random generation methods and hold the
potential for a broader range of music visualization applications. Our code is
available at https://github.com/yunlong10/LaunchpadGPT/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Siting Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yunlong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1&quot;&gt;Feng Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04892">
<title>Entity Identifier: A Natural Text Parsing-based Framework For Entity Relation Extraction. (arXiv:2307.04892v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.04892</link>
<description rdf:parseType="Literal">&lt;p&gt;The field of programming has a diversity of paradigms that are used according
to the working framework. While current neural code generation methods are able
to learn and generate code directly from text, we believe that this approach is
not optimal for certain code tasks, particularly the generation of classes in
an object-oriented project. Specifically, we use natural language processing
techniques to extract structured information from requirements descriptions, in
order to automate the generation of CRUD (Create, Read, Update, Delete) class
code. To facilitate this process, we introduce a pipeline for extracting entity
and relation information, as well as a representation called an &quot;Entity Tree&quot;
to model this information. We also create a dataset to evaluate the
effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chouham_E/0/1/0/all/0/1&quot;&gt;El Mehdi Chouham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Espejel_J/0/1/0/all/0/1&quot;&gt;Jessica L&amp;#xf3;pez Espejel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alassan_M/0/1/0/all/0/1&quot;&gt;Mahaman Sanoussi Yahaya Alassan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahhane_W/0/1/0/all/0/1&quot;&gt;Walid Dahhane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ettifouri_E/0/1/0/all/0/1&quot;&gt;El Hassane Ettifouri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04907">
<title>SimpleMTOD: A Simple Language Model for Multimodal Task-Oriented Dialogue with Symbolic Scene Representation. (arXiv:2307.04907v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.04907</link>
<description rdf:parseType="Literal">&lt;p&gt;SimpleMTOD is a simple language model which recasts several sub-tasks in
multimodal task-oriented dialogues as sequence prediction tasks. SimpleMTOD is
built on a large-scale transformer-based auto-regressive architecture, which
has already proven to be successful in uni-modal task-oriented dialogues, and
effectively leverages transfer learning from pre-trained GPT-2. In-order to
capture the semantics of visual scenes, we introduce both local and
de-localized tokens for objects within a scene. De-localized tokens represent
the type of an object rather than the specific object itself and so possess a
consistent meaning across the dataset. SimpleMTOD achieves a state-of-the-art
BLEU score (0.327) in the Response Generation sub-task of the SIMMC 2.0
test-std dataset while performing on par in other multimodal sub-tasks:
Disambiguation, Coreference Resolution, and Dialog State Tracking. This is
despite taking a minimalist approach for extracting visual (and non-visual)
information. In addition the model does not rely on task-specific architectural
changes such as classification heads.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemanthage_B/0/1/0/all/0/1&quot;&gt;Bhathiya Hemanthage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dondrup_C/0/1/0/all/0/1&quot;&gt;Christian Dondrup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartie_P/0/1/0/all/0/1&quot;&gt;Phil Bartie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemon_O/0/1/0/all/0/1&quot;&gt;Oliver Lemon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04963">
<title>DyCL: Dynamic Neural Network Compilation Via Program Rewriting and Graph Optimization. (arXiv:2307.04963v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.04963</link>
<description rdf:parseType="Literal">&lt;p&gt;DL compiler&apos;s primary function is to translate DNN programs written in
high-level DL frameworks such as PyTorch and TensorFlow into portable
executables. These executables can then be flexibly executed by the deployed
host programs. However, existing DL compilers rely on a tracing mechanism,
which involves feeding a runtime input to a neural network program and tracing
the program execution paths to generate the computational graph necessary for
compilation. Unfortunately, this mechanism falls short when dealing with modern
dynamic neural networks (DyNNs) that possess varying computational graphs
depending on the inputs. Consequently, conventional DL compilers struggle to
accurately compile DyNNs into executable code. To address this limitation, we
propose \tool, a general approach that enables any existing DL compiler to
successfully compile DyNNs. \tool tackles the dynamic nature of DyNNs by
introducing a compilation mechanism that redistributes the control and data
flow of the original DNN programs during the compilation process. Specifically,
\tool develops program analysis and program transformation techniques to
convert a dynamic neural network into multiple sub-neural networks. Each
sub-neural network is devoid of conditional statements and is compiled
independently. Furthermore, \tool synthesizes a host module that models the
control flow of the DyNNs and facilitates the invocation of the sub-neural
networks. Our evaluation demonstrates the effectiveness of \tool, achieving a
100\% success rate in compiling all dynamic neural networks. Moreover, the
compiled executables generated by \tool exhibit significantly improved
performance, running between $1.12\times$ and $20.21\times$ faster than the
original DyNNs executed on general-purpose DL frameworks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Simin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1&quot;&gt;Shiyi Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wei Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04964">
<title>Secrets of RLHF in Large Language Models Part I: PPO. (arXiv:2307.04964v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.04964</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have formulated a blueprint for the advancement
of artificial general intelligence. Its primary objective is to function as a
human-centric (helpful, honest, and harmless) assistant. Alignment with humans
assumes paramount significance, and reinforcement learning with human feedback
(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.
Current technical routes usually include \textbf{reward models} to measure
human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize
policy model outputs, and \textbf{process supervision} to improve step-by-step
reasoning capabilities. However, due to the challenges of reward design,
environment interaction, and agent training, coupled with huge trial and error
cost of large language models, there is a significant barrier for AI
researchers to motivate the development of technical alignment and safe landing
of LLMs. The stable training of RLHF has still been a puzzle. In the first
report, we dissect the framework of RLHF, re-evaluate the inner workings of
PPO, and explore how the parts comprising PPO algorithms impact policy agent
training. We identify policy constraints being the key factor for the effective
implementation of the PPO algorithm. Therefore, we explore the PPO-max, an
advanced version of PPO algorithm, to efficiently improve the training
stability of the policy model. Based on our main results, we perform a
comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.
The absence of open-source implementations has posed significant challenges to
the investigation of LLMs alignment. Therefore, we are eager to release
technical reports, reward models and PPO codes
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1&quot;&gt;Rui Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1&quot;&gt;Shihan Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Songyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;Wei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Binghai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1&quot;&gt;Senjie Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1&quot;&gt;Limao Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1&quot;&gt;Zhiheng Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1&quot;&gt;Nuo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1&quot;&gt;Wenbin Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Minghao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_R/0/1/0/all/0/1&quot;&gt;Rongxiang Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1&quot;&gt;Wensen Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Cheng Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhangyue Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1&quot;&gt;Yuan Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haoran Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tianxiang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1&quot;&gt;Tao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05006">
<title>Improving RNN-Transducers with Acoustic LookAhead. (arXiv:2307.05006v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05006</link>
<description rdf:parseType="Literal">&lt;p&gt;RNN-Transducers (RNN-Ts) have gained widespread acceptance as an end-to-end
model for speech to text conversion because of their high accuracy and
streaming capabilities. A typical RNN-T independently encodes the input audio
and the text context, and combines the two encodings by a thin joint network.
While this architecture provides SOTA streaming accuracy, it also makes the
model vulnerable to strong LM biasing which manifests as multi-step
hallucination of text without acoustic evidence. In this paper we propose
LookAhead that makes text representations more acoustically grounded by looking
ahead into the future within the audio input. This technique yields a
significant 5%-20% relative reduction in word error rate on both in-domain and
out-of-domain evaluation sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unni_V/0/1/0/all/0/1&quot;&gt;Vinit S. Unni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1&quot;&gt;Ashish Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1&quot;&gt;Preethi Jyothi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1&quot;&gt;Sunita Sarawagi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05034">
<title>Synthetic Dataset for Evaluating Complex Compositional Knowledge for Natural Language Inference. (arXiv:2307.05034v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05034</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a synthetic dataset called Sentences Involving Complex
Compositional Knowledge (SICCK) and a novel analysis that investigates the
performance of Natural Language Inference (NLI) models to understand
compositionality in logic. We produce 1,304 sentence pairs by modifying 15
examples from the SICK dataset (Marelli et al., 2014). To this end, we modify
the original texts using a set of phrases - modifiers that correspond to
universal quantifiers, existential quantifiers, negation, and other concept
modifiers in Natural Logic (NL) (MacCartney, 2009). We use these phrases to
modify the subject, verb, and object parts of the premise and hypothesis.
Lastly, we annotate these modified texts with the corresponding entailment
labels following NL rules. We conduct a preliminary verification of how well
the change in the structural and semantic composition is captured by neural NLI
models, in both zero-shot and fine-tuned scenarios. We found that the
performance of NLI models under the zero-shot setting is poor, especially for
modified sentences with negation and existential quantifiers. After fine-tuning
this dataset, we observe that models continue to perform poorly over negation,
existential and universal modifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akoju_S/0/1/0/all/0/1&quot;&gt;Sushma Anand Akoju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vacareanu_R/0/1/0/all/0/1&quot;&gt;Robert Vacareanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riaz_H/0/1/0/all/0/1&quot;&gt;Haris Riaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blanco_E/0/1/0/all/0/1&quot;&gt;Eduardo Blanco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1&quot;&gt;Mihai Surdeanu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05052">
<title>Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps. (arXiv:2307.05052v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05052</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the role of various demonstration components in the in-context
learning (ICL) performance of large language models (LLMs). Specifically, we
explore the impacts of ground-truth labels, input distribution, and
complementary explanations, particularly when these are altered or perturbed.
We build on previous work, which offers mixed findings on how these elements
influence ICL. To probe these questions, we employ explainable NLP (XNLP)
methods and utilize saliency maps of contrastive demonstrations for both
qualitative and quantitative analysis. Our findings reveal that flipping
ground-truth labels significantly affects the saliency, though it&apos;s more
noticeable in larger LLMs. Our analysis of the input distribution at a granular
level reveals that changing sentiment-indicative terms in a sentiment analysis
task to neutral ones does not have as substantial an impact as altering
ground-truth labels. Finally, we find that the effectiveness of complementary
explanations in boosting ICL performance is task-dependent, with limited
benefits seen in sentiment analysis tasks compared to symbolic reasoning tasks.
These insights are critical for understanding the functionality of LLMs and
guiding the development of effective demonstrations, which is increasingly
relevant in light of the growing use of LLMs in applications such as ChatGPT.
Our research code is publicly available at https://github.com/paihengxu/XICL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zongxia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Paiheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fuxiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Hyemi Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05081">
<title>Argumentative Segmentation Enhancement for Legal Summarization. (arXiv:2307.05081v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05081</link>
<description rdf:parseType="Literal">&lt;p&gt;We use the combination of argumentative zoning [1] and a legal argumentative
scheme to create legal argumentative segments. Based on the argumentative
segmentation, we propose a novel task of classifying argumentative segments of
legal case decisions. GPT-3.5 is used to generate summaries based on
argumentative segments. In terms of automatic evaluation metrics, our method
generates higher quality argumentative summaries while leaving out less
relevant context as compared to GPT-4 and non-GPT models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Huihui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashley_K/0/1/0/all/0/1&quot;&gt;Kevin Ashley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05082">
<title>OntoChatGPT Information System: Ontology-Driven Structured Prompts for ChatGPT Meta-Learning. (arXiv:2307.05082v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.05082</link>
<description rdf:parseType="Literal">&lt;p&gt;This research presents a comprehensive methodology for utilizing an
ontology-driven structured prompts system in interplay with ChatGPT, a widely
used large language model (LLM). The study develops formal models, both
information and functional, and establishes the methodological foundations for
integrating ontology-driven prompts with ChatGPT&apos;s meta-learning capabilities.
The resulting productive triad comprises the methodological foundations,
advanced information technology, and the OntoChatGPT system, which collectively
enhance the effectiveness and performance of chatbot systems. The
implementation of this technology is demonstrated using the Ukrainian language
within the domain of rehabilitation. By applying the proposed methodology, the
OntoChatGPT system effectively extracts entities from contexts, classifies
them, and generates relevant responses. The study highlights the versatility of
the methodology, emphasizing its applicability not only to ChatGPT but also to
other chatbot systems based on LLMs, such as Google&apos;s Bard utilizing the PaLM 2
LLM. The underlying principles of meta-learning, structured prompts, and
ontology-driven information retrieval form the core of the proposed
methodology, enabling their adaptation and utilization in various LLM-based
systems. This versatile approach opens up new possibilities for NLP and
dialogue systems, empowering developers to enhance the performance and
functionality of chatbot systems across different domains and languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palagin_O/0/1/0/all/0/1&quot;&gt;Oleksandr Palagin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaverinskiy_V/0/1/0/all/0/1&quot;&gt;Vladislav Kaverinskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Litvin_A/0/1/0/all/0/1&quot;&gt;Anna Litvin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malakhov_K/0/1/0/all/0/1&quot;&gt;Kyrylo Malakhov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05083">
<title>Vacaspati: A Diverse Corpus of Bangla Literature. (arXiv:2307.05083v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05083</link>
<description rdf:parseType="Literal">&lt;p&gt;Bangla (or Bengali) is the fifth most spoken language globally; yet, the
state-of-the-art NLP in Bangla is lagging for even simple tasks such as
lemmatization, POS tagging, etc. This is partly due to lack of a varied quality
corpus. To alleviate this need, we build Vacaspati, a diverse corpus of Bangla
literature. The literary works are collected from various websites; only those
works that are publicly available without copyright violations or restrictions
are collected. We believe that published literature captures the features of a
language much better than newspapers, blogs or social media posts which tend to
follow only a certain literary pattern and, therefore, miss out on language
variety. Our corpus Vacaspati is varied from multiple aspects, including type
of composition, topic, author, time, space, etc. It contains more than 11
million sentences and 115 million words. We also built a word embedding model,
Vac-FT, using FastText from Vacaspati as well as trained an Electra model,
Vac-BERT, using the corpus. Vac-BERT has far fewer parameters and requires only
a fraction of resources compared to other state-of-the-art transformer models
and yet performs either better or similar on various downstream tasks. On
multiple downstream tasks, Vac-FT outperforms other FastText-based models. We
also demonstrate the efficacy of Vacaspati as a corpus by showing that similar
models built from other corpora are not as effective. The models are available
at https://bangla.iitk.ac.in/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1&quot;&gt;Pramit Bhattacharyya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondal_J/0/1/0/all/0/1&quot;&gt;Joydeep Mondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1&quot;&gt;Subhadip Maji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1&quot;&gt;Arnab Bhattacharya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05113">
<title>Beyond the Obvious: Evaluating the Reasoning Ability In Real-life Scenarios of Language Models on Life Scapes Reasoning Benchmark~(LSR-Benchmark). (arXiv:2307.05113v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05113</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces the Life Scapes Reasoning Benchmark (LSR-Benchmark), a
novel dataset targeting real-life scenario reasoning, aiming to close the gap
in artificial neural networks&apos; ability to reason in everyday contexts. In
contrast to domain knowledge reasoning datasets, LSR-Benchmark comprises
free-text formatted questions with rich information on real-life scenarios,
human behaviors, and character roles. The dataset consists of 2,162 questions
collected from open-source online sources and is manually annotated to improve
its quality. Experiments are conducted using state-of-the-art language models,
such as gpt3.5-turbo and instruction fine-tuned llama models, to test the
performance in LSR-Benchmark. The results reveal that humans outperform these
models significantly, indicating a persisting challenge for machine learning
models in comprehending daily human life.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1&quot;&gt;Zhouhong Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zihan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zhuozhi Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Sihang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaoxuan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shusen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zili Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianchen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1&quot;&gt;Haoning Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenhao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yikai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Hongwei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yanghua Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05131">
<title>Overview of BioASQ 2023: The eleventh BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering. (arXiv:2307.05131v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05131</link>
<description rdf:parseType="Literal">&lt;p&gt;This is an overview of the eleventh edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2023. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks b and Synergy, and a new
task (MedProcNER) on semantic annotation of clinical content in Spanish with
medical procedures, which have a critical role in medical practice. In this
edition of BioASQ, 28 competing teams submitted the results of more than 150
distinct systems in total for the three different shared tasks of the
challenge. Similarly to previous editions, most of the participating systems
achieved competitive performance, suggesting the continuous advancement of the
state-of-the-art in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nentidis_A/0/1/0/all/0/1&quot;&gt;Anastasios Nentidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katsimpras_G/0/1/0/all/0/1&quot;&gt;Georgios Katsimpras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krithara_A/0/1/0/all/0/1&quot;&gt;Anastasia Krithara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_S/0/1/0/all/0/1&quot;&gt;Salvador Lima L&amp;#xf3;pez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farre_Maduell_E/0/1/0/all/0/1&quot;&gt;Eul&amp;#xe1;lia Farr&amp;#xe9;-Maduell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gasco_L/0/1/0/all/0/1&quot;&gt;Luis Gasco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krallinger_M/0/1/0/all/0/1&quot;&gt;Martin Krallinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1&quot;&gt;Georgios Paliouras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05134">
<title>TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation. (arXiv:2307.05134v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05134</link>
<description rdf:parseType="Literal">&lt;p&gt;The progress in the generation of synthetic images has made it crucial to
assess their quality. While several metrics have been proposed to assess the
rendering of images, it is crucial for Text-to-Image (T2I) models, which
generate images based on a prompt, to consider additional aspects such as to
which extent the generated image matches the important content of the prompt.
Moreover, although the generated images usually result from a random starting
point, the influence of this one is generally not considered. In this article,
we propose a new metric based on prompt templates to study the alignment
between the content specified in the prompt and the corresponding generated
images. It allows us to better characterize the alignment in terms of the type
of the specified objects, their number, and their color. We conducted a study
on several recent T2I models about various aspects. An additional interesting
result we obtained with our approach is that image quality can vary drastically
depending on the latent noise used as a seed for the images. We also quantify
the influence of the number of concepts in the prompt, their order as well as
their (color) attributes. Finally, our method allows us to identify some latent
seeds that produce better images than others, opening novel directions of
research on this understudied topic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grimal_P/0/1/0/all/0/1&quot;&gt;Paul Grimal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borgne_H/0/1/0/all/0/1&quot;&gt;Herv&amp;#xe9; Le Borgne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferret_O/0/1/0/all/0/1&quot;&gt;Olivier Ferret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tourille_J/0/1/0/all/0/1&quot;&gt;Julien Tourille&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05162">
<title>SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue Summarization. (arXiv:2307.05162v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05162</link>
<description rdf:parseType="Literal">&lt;p&gt;Finetuning Large Language Models helps improve the results for
domain-specific use cases. End-to-end finetuning of large language models is
time and resource intensive and has high storage requirements to store the
finetuned version of the large language model. Parameter Efficient Fine Tuning
(PEFT) methods address the time and resource challenges by keeping the large
language model as a fixed base and add additional layers, which the PEFT
methods finetune. This paper demonstrates the evaluation results for one such
PEFT method Low Rank Adaptation (LoRA), for Clinical Dialogue Summarization.
The evaluation results show that LoRA works at par with end-to-end finetuning
for a large language model. The paper presents the evaluations done for solving
both the Subtask A and B from ImageCLEFmedical
{https://www.imageclef.org/2023/medical}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suri_K/0/1/0/all/0/1&quot;&gt;Kunal Suri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1&quot;&gt;Prakhar Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1&quot;&gt;Saumajit Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Atul Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05174">
<title>Mao-Zedong At SemEval-2023 Task 4: Label Represention Multi-Head Attention Model With Contrastive Learning-Enhanced Nearest Neighbor Mechanism For Multi-Label Text Classification. (arXiv:2307.05174v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05174</link>
<description rdf:parseType="Literal">&lt;p&gt;The study of human values is essential in both practical and theoretical
domains. With the development of computational linguistics, the creation of
large-scale datasets has made it possible to automatically recognize human
values accurately. SemEval 2023 Task 4\cite{kiesel:2023} provides a set of
arguments and 20 types of human values that are implicitly expressed in each
argument. In this paper, we present our team&apos;s solution. We use the
Roberta\cite{liu_roberta_2019} model to obtain the word vector encoding of the
document and propose a multi-head attention mechanism to establish connections
between specific labels and semantic components. Furthermore, we use a
contrastive learning-enhanced K-nearest neighbor
mechanism\cite{su_contrastive_2022} to leverage existing instance information
for prediction. Our approach achieved an F1 score of 0.533 on the test set and
ranked fourth on the leaderboard.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Che Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Ping&amp;#x27;an Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zhenyang Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1&quot;&gt;Haojun Fei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05228">
<title>Attribute Controlled Dialogue Prompting. (arXiv:2307.05228v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05228</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt-tuning has become an increasingly popular parameter-efficient method
for adapting large pretrained language models to downstream tasks. However,
both discrete prompting and continuous prompting assume fixed prompts for all
data samples within a task, neglecting the fact that inputs vary greatly in
some tasks such as open-domain dialogue generation. In this paper, we present a
novel, instance-specific prompt-tuning algorithm for dialogue generation.
Specifically, we generate prompts based on instance-level control code, rather
than the conversation history, to explore their impact on controlled dialogue
generation. Experiments on popular open-domain dialogue datasets, evaluated on
both automated metrics and human evaluation, demonstrate that our method is
superior to prompting baselines and comparable to fine-tuning with only 5%-6%
of total parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Runcheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1&quot;&gt;Ahmad Rashid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobyzev_I/0/1/0/all/0/1&quot;&gt;Ivan Kobyzev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1&quot;&gt;Mehdi Rezagholizadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poupart_P/0/1/0/all/0/1&quot;&gt;Pascal Poupart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05260">
<title>U-CREAT: Unsupervised Case Retrieval using Events extrAcTion. (arXiv:2307.05260v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.05260</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of Prior Case Retrieval (PCR) in the legal domain is about
automatically citing relevant (based on facts and precedence) prior legal cases
in a given query case. To further promote research in PCR, in this paper, we
propose a new large benchmark (in English) for the PCR task: IL-PCR (Indian
Legal Prior Case Retrieval) corpus. Given the complex nature of case relevance
and the long size of legal documents, BM25 remains a strong baseline for
ranking the cited prior documents. In this work, we explore the role of events
in legal case retrieval and propose an unsupervised retrieval method-based
pipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction). We find
that the proposed unsupervised retrieval method significantly increases
performance compared to BM25 and makes retrieval faster by a considerable
margin, making it applicable to real-time case retrieval systems. Our proposed
system is generic, we show that it generalizes across two different legal
systems (Indian and Canadian), and it shows state-of-the-art performance on the
benchmarks for both the legal systems (IL-PCR and COLIEE corpora).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1&quot;&gt;Abhinav Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Akshat Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanikella_S/0/1/0/all/0/1&quot;&gt;Sai Kiran Tanikella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1&quot;&gt;Ashutosh Modi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05300">
<title>Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. (arXiv:2307.05300v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.05300</link>
<description rdf:parseType="Literal">&lt;p&gt;Human intelligence thrives on the concept of cognitive synergy, where
collaboration and information integration among different cognitive processes
yield superior outcomes compared to individual cognitive processes in
isolation. Although Large Language Models (LLMs) have demonstrated promising
performance as general task-solving agents, they still struggle with tasks that
require intensive domain knowledge and complex reasoning. In this work, we
propose Solo Performance Prompting (SPP), which transforms a single LLM into a
cognitive synergist by engaging in multi-turn self-collaboration with multiple
personas. A cognitive synergist refers to an intelligent agent that
collaborates with multiple minds, combining their individual strengths and
knowledge, to enhance problem-solving and overall performance in complex tasks.
By dynamically identifying and simulating different personas based on task
inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have
discovered that assigning multiple, fine-grained personas in LLMs elicits
better problem-solving abilities compared to using a single or fixed number of
personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,
Codenames Collaborative, and Logic Grid Puzzle, encompassing both
knowledge-intensive and reasoning-intensive types. Unlike previous works, such
as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, SPP
effectively elicits internal knowledge acquisition abilities, reduces
hallucination, and maintains strong reasoning capabilities. Code, data, and
prompts can be found at:
https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenhailong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1&quot;&gt;Shaoguang Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenshan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1&quot;&gt;Tao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1&quot;&gt;Furu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05329">
<title>Decoding the Popularity of TV Series: A Network Analysis Perspective. (arXiv:2307.05329v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2307.05329</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we analyze the character networks extracted from three popular
television series and explore the relationship between a TV show episode&apos;s
character network metrics and its review from IMDB. Character networks are
graphs created from the plot of a TV show that represents the interactions of
characters in scenes, indicating the presence of a connection between them. We
calculate various network metrics for each episode, such as node degree and
graph density, and use these metrics to explore the potential relationship
between network metrics and TV series reviews from IMDB. Our results show that
certain network metrics of character interactions in episodes have a strong
correlation with the review score of TV series. Our research aims to provide
more quantitative information that can help TV producers understand how to
adjust the character dynamics of future episodes to appeal to their audience.
By understanding the impact of character interactions on audience engagement
and enjoyment, producers can make informed decisions about the development of
their shows.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1&quot;&gt;Melody Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05337">
<title>Explaining Competitive-Level Programming Solutions using LLMs. (arXiv:2307.05337v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05337</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we approach competitive-level programming problem-solving as a
composite task of reasoning and code generation. We propose a novel method to
automatically annotate natural language explanations to \textit{&amp;lt;problem,
solution&amp;gt;} pairs. We show that despite poor performance in solving
competitive-level programming problems, state-of-the-art LLMs exhibit a strong
capacity in describing and explaining solutions. Our explanation generation
methodology can generate a structured solution explanation for the problem
containing descriptions and analysis. To evaluate the quality of the annotated
explanations, we examine their effectiveness in two aspects: 1) satisfying the
human programming expert who authored the oracle solution, and 2) aiding LLMs
in solving problems more effectively. The experimental results on the
CodeContests dataset demonstrate that while LLM GPT3.5&apos;s and GPT-4&apos;s abilities
in describing the solution are comparable, GPT-4 shows a better understanding
of the key idea behind the solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jierui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tworkowski_S/0/1/0/all/0/1&quot;&gt;Szymon Tworkowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yingying Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1&quot;&gt;Raymond Mooney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05354">
<title>GujiBERT and GujiGPT: Construction of Intelligent Information Processing Foundation Language Models for Ancient Texts. (arXiv:2307.05354v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05354</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of the rapid development of large language models, we have
meticulously trained and introduced the GujiBERT and GujiGPT language models,
which are foundational models specifically designed for intelligent information
processing of ancient texts. These models have been trained on an extensive
dataset that encompasses both simplified and traditional Chinese characters,
allowing them to effectively handle various natural language processing tasks
related to ancient books, including but not limited to automatic sentence
segmentation, punctuation, word segmentation, part-of-speech tagging, entity
recognition, and automatic translation. Notably, these models have exhibited
exceptional performance across a range of validation tasks using publicly
available datasets. Our research findings highlight the efficacy of employing
self-supervised methods to further train the models using classical text
corpora, thus enhancing their capability to tackle downstream tasks. Moreover,
it is worth emphasizing that the choice of font, the scale of the corpus, and
the initial model selection all exert significant influence over the ultimate
experimental outcomes. To cater to the diverse text processing preferences of
researchers in digital humanities and linguistics, we have developed three
distinct categories comprising a total of nine model variations. We believe
that by sharing these foundational language models specialized in the domain of
ancient texts, we can facilitate the intelligent processing and scholarly
exploration of ancient literary works and, consequently, contribute to the
global dissemination of China&apos;s rich and esteemed traditional culture in this
new era.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dongbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhixiao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1&quot;&gt;Si Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Haotian Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Mengcheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Litao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xue Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiyu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05355">
<title>UniCoRN: Unified Cognitive Signal ReconstructioN bridging cognitive signals and human language. (arXiv:2307.05355v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2307.05355</link>
<description rdf:parseType="Literal">&lt;p&gt;Decoding text stimuli from cognitive signals (e.g. fMRI) enhances our
understanding of the human language system, paving the way for building
versatile Brain-Computer Interface. However, existing studies largely focus on
decoding individual word-level fMRI volumes from a restricted vocabulary, which
is far too idealized for real-world application. In this paper, we propose
fMRI2text, the first openvocabulary task aiming to bridge fMRI time series and
human language. Furthermore, to explore the potential of this new task, we
present a baseline solution, UniCoRN: the Unified Cognitive Signal
ReconstructioN for Brain Decoding. By reconstructing both individual time
points and time series, UniCoRN establishes a robust encoder for cognitive
signals (fMRI &amp;amp; EEG). Leveraging a pre-trained language model as decoder,
UniCoRN proves its efficacy in decoding coherent text from fMRI series across
various split settings. Our model achieves a 34.77% BLEU score on fMRI2text,
and a 37.04% BLEU when generalized to EEGto-text decoding, thereby surpassing
the former baseline. Experimental results indicate the feasibility of decoding
consecutive fMRI volumes, and the effectiveness of decoding different cognitive
signals using a unified structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xi_N/0/1/0/all/0/1&quot;&gt;Nuwa Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Sendong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haochun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qin_B/0/1/0/all/0/1&quot;&gt;Bing Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05360">
<title>Unmasking the giant: A comprehensive evaluation of ChatGPT&apos;s proficiency in coding algorithms and data structures. (arXiv:2307.05360v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.05360</link>
<description rdf:parseType="Literal">&lt;p&gt;The transformative influence of Large Language Models (LLMs) is profoundly
reshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT
distinguishes itself within these models, demonstrating remarkable performance
in multi-turn conversations and exhibiting code proficiency across an array of
languages. In this paper, we carry out a comprehensive evaluation of ChatGPT&apos;s
coding capabilities based on what is to date the largest catalog of coding
challenges. Our focus is on the python programming language and problems
centered on data structures and algorithms, two topics at the very foundations
of Computer Science. We evaluate ChatGPT for its ability to generate correct
solutions to the problems fed to it, its code quality, and nature of run-time
errors thrown by its code. Where ChatGPT code successfully executes, but fails
to solve the problem at hand, we look into patterns in the test cases passed in
order to gain some insights into how wrong ChatGPT code is in these kinds of
situations. To infer whether ChatGPT might have directly memorized some of the
data that was used to train it, we methodically design an experiment to
investigate this phenomena. Making comparisons with human performance whenever
feasible, we investigate all the above questions from the context of both its
underlying learning models (GPT-3.5 and GPT-4), on a vast array sub-topics
within the main topics, and on problems having varying degrees of difficulty.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arefin_S/0/1/0/all/0/1&quot;&gt;Sayed Erfan Arefin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heya_T/0/1/0/all/0/1&quot;&gt;Tasnia Ashrafi Heya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Qudah_H/0/1/0/all/0/1&quot;&gt;Hasan Al-Qudah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ineza_Y/0/1/0/all/0/1&quot;&gt;Ynes Ineza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serwadda_A/0/1/0/all/0/1&quot;&gt;Abdul Serwadda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05410">
<title>BLUEX: A benchmark based on Brazilian Leading Universities Entrance eXams. (arXiv:2307.05410v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05410</link>
<description rdf:parseType="Literal">&lt;p&gt;One common trend in recent studies of language models (LMs) is the use of
standardized tests for evaluation. However, despite being the fifth most spoken
language worldwide, few such evaluations have been conducted in Portuguese.
This is mainly due to the lack of high-quality datasets available to the
community for carrying out evaluations in Portuguese. To address this gap, we
introduce the Brazilian Leading Universities Entrance eXams (BLUEX), a dataset
of entrance exams from the two leading universities in Brazil: UNICAMP and USP.
The dataset includes annotated metadata for evaluating the performance of NLP
models on a variety of subjects. Furthermore, BLUEX includes a collection of
recently administered exams that are unlikely to be included in the training
data of many popular LMs as of 2023. The dataset is also annotated to indicate
the position of images in each question, providing a valuable resource for
advancing the state-of-the-art in multimodal language understanding and
reasoning. We describe the creation and characteristics of BLUEX and establish
a benchmark through experiments with state-of-the-art LMs, demonstrating its
potential for advancing the state-of-the-art in natural language understanding
and reasoning in Portuguese. The data and relevant code can be found at
https://github.com/Portuguese-Benchmark-Datasets/BLUEX
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almeida_T/0/1/0/all/0/1&quot;&gt;Thales Sales Almeida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laitz_T/0/1/0/all/0/1&quot;&gt;Thiago Laitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonas_G/0/1/0/all/0/1&quot;&gt;Giovana K. Bon&amp;#xe1;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1&quot;&gt;Rodrigo Nogueira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05414">
<title>Duncode Characters Shorter. (arXiv:2307.05414v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05414</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates the employment of various encoders in text
transformation, converting characters into bytes. It discusses local encoders
such as ASCII and GB-2312, which encode specific characters into shorter bytes,
and universal encoders like UTF-8 and UTF-16, which can encode the complete
Unicode set with greater space requirements and are gaining widespread
acceptance. Other encoders, including SCSU, BOCU-1, and binary encoders,
however, lack self-synchronizing capabilities. Duncode is introduced as an
innovative encoding method that aims to encode the entire Unicode character set
with high space efficiency, akin to local encoders. It has the potential to
compress multiple characters of a string into a Duncode unit using fewer bytes.
Despite offering less self-synchronizing identification information, Duncode
surpasses UTF8 in terms of space efficiency. The application is available at
\url{https://github.com/laohur/duncode}. Additionally, we have developed a
benchmark for evaluating character encoders across different languages. It
encompasses 179 languages and can be accessed at
\url{https://github.com/laohur/wiki2txt}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1&quot;&gt;Changshang Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05440">
<title>ISLTranslate: Dataset for Translating Indian Sign Language. (arXiv:2307.05440v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05440</link>
<description rdf:parseType="Literal">&lt;p&gt;Sign languages are the primary means of communication for many
hard-of-hearing people worldwide. Recently, to bridge the communication gap
between the hard-of-hearing community and the rest of the population, several
sign language translation datasets have been proposed to enable the development
of statistical sign language translation systems. However, there is a dearth of
sign language resources for the Indian sign language. This resource paper
introduces ISLTranslate, a translation dataset for continuous Indian Sign
Language (ISL) consisting of 31k ISL-English sentence/phrase pairs. To the best
of our knowledge, it is the largest translation dataset for continuous Indian
Sign Language. We provide a detailed analysis of the dataset. To validate the
performance of existing end-to-end Sign language to spoken language translation
systems, we benchmark the created dataset with a transformer-based model for
ISL translation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1&quot;&gt;Abhinav Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1&quot;&gt;Susmit Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1&quot;&gt;Ashutosh Modi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05454">
<title>Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features. (arXiv:2307.05454v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05454</link>
<description rdf:parseType="Literal">&lt;p&gt;A challenge towards developing NLP systems for the world&apos;s languages is
understanding how they generalize to typological differences relevant for
real-world applications. To this end, we propose M2C, a morphologically-aware
framework for behavioral testing of NLP models. We use M2C to generate tests
that probe models&apos; behavior in light of specific linguistic features in 12
typologically diverse languages. We evaluate state-of-the-art language models
on the generated tests. While models excel at most tests in English, we
highlight generalization failures to specific typological characteristics such
as temporal expressions in Swahili and compounding possessives in Finish. Our
findings motivate the development of models that address these blind spots.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hlavnova_E/0/1/0/all/0/1&quot;&gt;Ester Hlavnova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1&quot;&gt;Sebastian Ruder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2107.00439">
<title>What do End-to-End Speech Models Learn about Speaker, Language and Channel Information? A Layer-wise and Neuron-level Analysis. (arXiv:2107.00439v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2107.00439</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are inherently opaque and challenging to interpret.
Unlike hand-crafted feature-based models, we struggle to comprehend the
concepts learned and how they interact within these models. This understanding
is crucial not only for debugging purposes but also for ensuring fairness in
ethical decision-making. In our study, we conduct a post-hoc functional
interpretability analysis of pretrained speech models using the probing
framework [1]. Specifically, we analyze utterance-level representations of
speech models trained for various tasks such as speaker recognition and dialect
identification. We conduct layer and neuron-wise analyses, probing for speaker,
language, and channel properties. Our study aims to answer the following
questions: i) what information is captured within the representations? ii) how
is it represented and distributed? and iii) can we identify a minimal subset of
the network that possesses this information?
&lt;/p&gt;
&lt;p&gt;Our results reveal several novel findings, including: i) channel and gender
information are distributed across the network, ii) the information is
redundantly available in neurons with respect to a task, iii) complex
properties such as dialectal information are encoded only in the task-oriented
pretrained network, iv) and is localised in the upper layers, v) we can extract
a minimal subset of neurons encoding the pre-defined property, vi) salient
neurons are sometimes shared between properties, vii) our analysis highlights
the presence of biases (for example gender) in the network. Our
cross-architectural comparison indicates that: i) the pretrained models capture
speaker-invariant information, and ii) CNN models are competitive with
Transformer models in encoding various understudied properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1&quot;&gt;Shammur Absar Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1&quot;&gt;Nadir Durrani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1&quot;&gt;Ahmed Ali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.09523">
<title>BTPK-based interpretable method for NER tasks based on Talmudic Public Announcement Logic. (arXiv:2201.09523v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2201.09523</link>
<description rdf:parseType="Literal">&lt;p&gt;As one of the basic tasks in natural language processing (NLP), named entity
recognition (NER) is an important basic tool for downstream tasks of NLP, such
as information extraction, syntactic analysis, machine translation and so on.
The internal operation logic of current name entity recognition model is
black-box to the user, so the user has no basis to determine which name entity
makes more sense. Therefore, a user-friendly explainable recognition process
would be very useful for many people. In this paper, we propose a novel
interpretable method, BTPK (Binary Talmudic Public Announcement Logic model),
to help users understand the internal recognition logic of the name entity
recognition tasks based on Talmudic Public Announcement Logic. BTPK model can
also capture the semantic information in the input sentences, that is, the
context dependency of the sentence. We observed the public announcement of BTPK
presents the inner decision logic of BRNNs, and the explanations obtained from
a BTPK model show us how BRNNs essentially handle NER tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yulin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1&quot;&gt;Beishui Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bentzen_B/0/1/0/all/0/1&quot;&gt;Bruno Bentzen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1&quot;&gt;Bo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1&quot;&gt;Zelai Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1&quot;&gt;Haixiao Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabbay_D/0/1/0/all/0/1&quot;&gt;Dov Gabbay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.03318">
<title>LegoNN: Building Modular Encoder-Decoder Models. (arXiv:2206.03318v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2206.03318</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art encoder-decoder models (e.g. for machine translation (MT) or
automatic speech recognition (ASR)) are constructed and trained end-to-end as
an atomic unit. No component of the model can be (re-)used without the others,
making it impossible to share parts, e.g. a high resourced decoder, across
tasks. We describe LegoNN, a procedure for building encoder-decoder
architectures in a way so that its parts can be applied to other tasks without
the need for any fine-tuning. To achieve this reusability, the interface
between encoder and decoder modules is grounded to a sequence of marginal
distributions over a pre-defined discrete vocabulary. We present two approaches
for ingesting these marginals; one is differentiable, allowing the flow of
gradients across the entire network, and the other is gradient-isolating. To
enable the portability of decoder modules between MT tasks for different source
languages and across other tasks like ASR, we introduce a modality agnostic
encoder which consists of a length control mechanism to dynamically adapt
encoders&apos; output lengths in order to match the expected input length range of
pre-trained decoders. We present several experiments to demonstrate the
effectiveness of LegoNN models: a trained language generation LegoNN decoder
module from German-English (De-En) MT task can be reused without any
fine-tuning for the Europarl English ASR and the Romanian-English (Ro-En) MT
tasks, matching or beating the performance of baseline. After fine-tuning,
LegoNN models improve the Ro-En MT task by 1.5 BLEU points and achieve 12.5%
relative WER reduction on the Europarl ASR task. To show how the approach
generalizes, we compose a LegoNN ASR model from three modules -- each has been
learned within different end-to-end trained models on three different datasets
-- achieving an overall WER reduction of 19.5%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalmia_S/0/1/0/all/0/1&quot;&gt;Siddharth Dalmia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1&quot;&gt;Dmytro Okhonko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1&quot;&gt;Mike Lewis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edunov_S/0/1/0/all/0/1&quot;&gt;Sergey Edunov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1&quot;&gt;Shinji Watanabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1&quot;&gt;Florian Metze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1&quot;&gt;Luke Zettlemoyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1&quot;&gt;Abdelrahman Mohamed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.06960">
<title>Forming Trees with Treeformers. (arXiv:2207.06960v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2207.06960</link>
<description rdf:parseType="Literal">&lt;p&gt;Human language is known to exhibit a nested, hierarchical structure, allowing
us to form complex sentences out of smaller pieces. However, many
state-of-the-art neural networks models such as Transformers have no explicit
hierarchical structure in its architecture -- that is, they don&apos;t have an
inductive bias toward hierarchical structure. Additionally, Transformers are
known to perform poorly on compositional generalization tasks which require
such structures. In this paper, we introduce Treeformer, a general-purpose
encoder module inspired by the CKY algorithm which learns a composition
operator and pooling function to construct hierarchical encodings for phrases
and sentences. Our extensive experiments demonstrate the benefits of
incorporating hierarchical structure into the Transformer and show significant
improvements in compositional generalization as well as in downstream tasks
such as machine translation, abstractive summarization, and various natural
language understanding tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_N/0/1/0/all/0/1&quot;&gt;Nilay Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flanigan_J/0/1/0/all/0/1&quot;&gt;Jeffrey Flanigan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.06385">
<title>TencentPretrain: A Scalable and Flexible Toolkit for Pre-training Models of Different Modalities. (arXiv:2212.06385v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.06385</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the success of pre-training in text domain has been fully extended
to vision, audio, and cross-modal scenarios. The proposed pre-training models
of different modalities are showing a rising trend of homogeneity in their
model structures, which brings the opportunity to implement different
pre-training models within a uniform framework. In this paper, we present
TencentPretrain, a toolkit supporting pre-training models of different
modalities. The core feature of TencentPretrain is the modular design. The
toolkit uniformly divides pre-training models into 5 components: embedding,
encoder, target embedding, decoder, and target. As almost all of common modules
are provided in each component, users can choose the desired modules from
different components to build a complete pre-training model. The modular design
enables users to efficiently reproduce existing pre-training models or build
brand-new one. We test the toolkit on text, vision, and audio benchmarks and
show that it can match the performance of the original implementations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhe Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yudong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_C/0/1/0/all/0/1&quot;&gt;Cheng Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1&quot;&gt;Rong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weijie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiren Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_N/0/1/0/all/0/1&quot;&gt;Ningyuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haoyan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1&quot;&gt;Weiquan Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Han Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1&quot;&gt;Weigang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Taiqiang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1&quot;&gt;Tao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Wenhang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sihong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liqun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feifei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoshuai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xingwu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1&quot;&gt;Zhanhui Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xiaoyong Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Linlin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Kimmo Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09603">
<title>Explanation Regeneration via Information Bottleneck. (arXiv:2212.09603v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09603</link>
<description rdf:parseType="Literal">&lt;p&gt;Explaining the black-box predictions of NLP models naturally and accurately
is an important open problem in natural language generation. These free-text
explanations are expected to contain sufficient and carefully-selected evidence
to form supportive arguments for predictions. Due to the superior generative
capacity of large pretrained language models, recent work built on prompt
engineering enables explanation generation without specific training. However,
explanation generated through single-pass prompting often lacks sufficiency and
conciseness. To address this problem, we develop an information bottleneck
method EIB to produce refined explanations that are sufficient and concise. Our
approach regenerates the free-text explanation by polishing the single-pass
output from the pretrained language model but retaining the information that
supports the contents being explained. Experiments on two out-of-domain tasks
verify the effectiveness of EIB through automatic evaluation and
thoroughly-conducted human evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qintong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1&quot;&gt;Lingpeng Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1&quot;&gt;Wei Bi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09651">
<title>Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages. (arXiv:2212.09651v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09651</link>
<description rdf:parseType="Literal">&lt;p&gt;Multilingual Pretrained Language Models (MPLMs) have shown their strong
multilinguality in recent empirical cross-lingual transfer studies. In this
paper, we propose the Prompts Augmented by Retrieval Crosslingually (PARC)
pipeline to improve the zero-shot performance on low-resource languages (LRLs)
by augmenting the context with semantically similar sentences retrieved from a
high-resource language (HRL) as prompts. PARC improves the zero-shot
performance on three downstream tasks (binary sentiment classification, topic
categorization and natural language inference) with multilingual parallel test
sets across 10 LRLs covering 6 language families in both unlabeled settings
(+5.1%) and labeled settings (+16.3%). PARC-labeled also outperforms the
finetuning baseline by 3.7%. We find a significant positive correlation between
cross-lingual transfer performance on one side, and the similarity between the
high- and low-resource languages as well as the amount of low-resource
pretraining data on the other side. A robustness analysis suggests that PARC
has the potential to achieve even stronger performance with more powerful
MPLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_E/0/1/0/all/0/1&quot;&gt;Ercong Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1&quot;&gt;Sheng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmid_H/0/1/0/all/0/1&quot;&gt;Helmut Schmid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1&quot;&gt;Hinrich Sch&amp;#xfc;tze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10722">
<title>Contrastive Error Attribution for Finetuned Language Models. (arXiv:2212.10722v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10722</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work has identified noisy and misannotated data as a core cause of
hallucinations and unfaithful outputs in Natural Language Generation (NLG)
tasks. Consequently, identifying and removing these examples is a key open
challenge in creating reliable NLG systems. In this work, we introduce a
framework to identify and remove low-quality training instances that lead to
undesirable outputs, such as faithfulness errors in text summarization. We show
that existing approaches for error tracing, such as gradient-based influence
measures, do not perform reliably for detecting faithfulness errors in NLG
datasets. We overcome the drawbacks of existing error tracing methods through a
new, contrast-based estimate that compares undesired generations to
human-corrected outputs. Our proposed method can achieve a mean average
precision of 0.93 at detecting known data errors across synthetic tasks with
known ground truth, substantially outperforming existing approaches. Using this
approach and re-training models on cleaned data leads to a 70% reduction in
entity hallucinations on the NYT dataset and a 55% reduction in semantic errors
on the E2E dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ladhak_F/0/1/0/all/0/1&quot;&gt;Faisal Ladhak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1&quot;&gt;Esin Durmus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1&quot;&gt;Tatsunori Hashimoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.06862">
<title>Algorithms for Acyclic Weighted Finite-State Automata with Failure Arcs. (arXiv:2301.06862v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2301.06862</link>
<description rdf:parseType="Literal">&lt;p&gt;Weighted finite-state automata (WSFAs) are commonly used in NLP. Failure
transitions are a useful extension for compactly representing backoffs or
interpolation in $n$-gram models and CRFs, which are special cases of WFSAs.
The pathsum in ordinary acyclic WFSAs is efficiently computed by the backward
algorithm in time $O(|E|)$, where $E$ is the set of transitions. However, this
does not allow failure transitions, and preprocessing the WFSA to eliminate
failure transitions could greatly increase $|E|$. We extend the backward
algorithm to handle failure transitions directly. Our approach is efficient
when the average state has outgoing arcs for only a small fraction $s \ll 1$ of
the alphabet $\Sigma$. We propose an algorithm for general acyclic WFSAs which
runs in $O{\left(|E| + s |\Sigma| |Q| T_\text{max} \log{|\Sigma|}\right)}$,
where $Q$ is the set of states and $T_\text{max}$ is the size of the largest
connected component of failure transitions. When the failure transition
topology satisfies a condition exemplified by CRFs, the $T_\text{max}$ factor
can be dropped, and when the weight semiring is a ring, the $\log{|\Sigma|}$
factor can be dropped. In the latter case (ring-weighted acyclic WFSAs), we
also give an alternative algorithm with complexity $\displaystyle O{\left(|E| +
|\Sigma| |Q| \min(1,s\pi_\text{max}) \right)}$, where $\pi_\text{max}$ is the
size of the longest failure path.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Svete_A/0/1/0/all/0/1&quot;&gt;Anej Svete&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dayan_B/0/1/0/all/0/1&quot;&gt;Benjamin Dayan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1&quot;&gt;Tim Vieira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1&quot;&gt;Ryan Cotterell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1&quot;&gt;Jason Eisner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.02168">
<title>I2I: Initializing Adapters with Improvised Knowledge. (arXiv:2304.02168v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.02168</link>
<description rdf:parseType="Literal">&lt;p&gt;Adapters present a promising solution to the catastrophic forgetting problem
in continual learning. However, training independent Adapter modules for every
new task misses an opportunity for cross-task knowledge transfer. We propose
Improvise to Initialize (I2I), a continual learning algorithm that initializes
Adapters for incoming tasks by distilling knowledge from previously-learned
tasks&apos; Adapters. We evaluate I2I on CLiMB, a multimodal continual learning
benchmark, by conducting experiments on sequences of visual question answering
tasks. Adapters trained with I2I consistently achieve better task accuracy than
independently-trained Adapters, demonstrating that our algorithm facilitates
knowledge transfer between task Adapters. I2I also results in better cross-task
knowledge transfer than the state-of-the-art AdapterFusion without incurring
the associated parametric cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_T/0/1/0/all/0/1&quot;&gt;Tejas Srinivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1&quot;&gt;Furong Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1&quot;&gt;Mohammad Rostami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1&quot;&gt;Jesse Thomason&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14793">
<title>Faithful Low-Resource Data-to-Text Generation through Cycle Training. (arXiv:2305.14793v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14793</link>
<description rdf:parseType="Literal">&lt;p&gt;Methods to generate text from structured data have advanced significantly in
recent years, primarily due to fine-tuning of pre-trained language models on
large datasets. However, such models can fail to produce output faithful to the
input data, particularly on out-of-domain data. Sufficient annotated data is
often not available for specific domains, leading us to seek an unsupervised
approach to improve the faithfulness of output text. Since the problem is
fundamentally one of consistency between the representations of the structured
data and text, we evaluate the effectiveness of cycle training in this work.
Cycle training uses two models which are inverses of each other: one that
generates text from structured data, and one which generates the structured
data from natural language text. We show that cycle training, when initialized
with a small amount of supervised data (100 samples in our case), achieves
nearly the same performance as fully supervised approaches for the data-to-text
generation task on the WebNLG, E2E, WTQ, and WSQL datasets. We perform
extensive empirical analysis with automated evaluation metrics and a newly
designed human evaluation schema to reveal different cycle training strategies&apos;
effectiveness of reducing various types of generation errors. Our code is
publicly available at https://github.com/Edillower/CycleNLG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhuoer Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1&quot;&gt;Marcus Collins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vedula_N/0/1/0/all/0/1&quot;&gt;Nikhita Vedula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filice_S/0/1/0/all/0/1&quot;&gt;Simone Filice&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malmasi_S/0/1/0/all/0/1&quot;&gt;Shervin Malmasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rokhlenko_O/0/1/0/all/0/1&quot;&gt;Oleg Rokhlenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15066">
<title>GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking. (arXiv:2305.15066v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15066</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models~(LLM) like ChatGPT have become indispensable to
artificial general intelligence~(AGI), demonstrating excellent performance in
various natural language processing tasks. In the real world, graph data is
ubiquitous and an essential part of AGI and prevails in domains like social
network analysis, bioinformatics and recommender systems. The training corpus
of large language models often includes some algorithmic components, which
allows them to achieve certain effects on some graph data-related problems.
However, there is still little research on their performance on a broader range
of graph-structured data. In this study, we conduct an extensive investigation
to assess the proficiency of LLMs in comprehending graph data, employing a
diverse range of structural and semantic-related tasks. Our analysis
encompasses 10 distinct tasks that evaluate the LLMs&apos; capabilities in graph
understanding. Through our study, we not only uncover the current limitations
of language models in comprehending graph structures and performing associated
reasoning tasks but also emphasize the necessity for further advancements and
novel approaches to enhance their graph processing capabilities. Our findings
contribute valuable insights towards bridging the gap between language models
and graph understanding, paving the way for more effective graph mining and
knowledge extraction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jiayan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1&quot;&gt;Lun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hengyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mengyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xinyi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Shi Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15542">
<title>TOAST: Transfer Learning via Attention Steering. (arXiv:2305.15542v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15542</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning involves adapting a pre-trained model to novel downstream
tasks. However, we observe that current transfer learning methods often fail to
focus on task-relevant features. In this work, we explore refocusing model
attention for transfer learning. We introduce Top-Down Attention Steering
(TOAST), a novel transfer learning algorithm that keeps the pre-trained
backbone frozen, selects task-relevant features in the output, and feeds those
features back to the model to steer the attention to the task-specific
features. By refocusing the attention only, TOAST achieves state-of-the-art
results on a number of transfer learning benchmarks, while having a small
number of tunable parameters. Compared to fully fine-tuning, LoRA, and prompt
tuning, TOAST substantially improves performance across a range of fine-grained
visual classification datasets (e.g., 81.1% -&amp;gt; 86.2% on FGVC). TOAST also
outperforms the fully fine-tuned Alpaca and Vicuna models on
instruction-following language generation. Code is available at
https://github.com/bfshi/TOAST.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Baifeng Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gai_S/0/1/0/all/0/1&quot;&gt;Siyu Gai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09841">
<title>Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond. (arXiv:2306.09841v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09841</link>
<description rdf:parseType="Literal">&lt;p&gt;Logical reasoning consistently plays a fundamental and significant role in
the domains of knowledge engineering and artificial intelligence. Recently,
Large Language Models (LLMs) have emerged as a noteworthy innovation in natural
language processing (NLP), exhibiting impressive achievements across various
classic NLP tasks. However, the question of whether LLMs can effectively
address the task of logical reasoning, which requires gradual cognitive
inference similar to human intelligence, remains unanswered. To this end, we
aim to bridge this gap and provide comprehensive evaluations in this paper.
Firstly, to offer systematic evaluations, we select fifteen typical logical
reasoning datasets and organize them into deductive, inductive, abductive and
mixed-form reasoning settings. Considering the comprehensiveness of
evaluations, we include three representative LLMs (i.e., text-davinci-003,
ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot,
one-shot and three-shot settings. Secondly, different from previous evaluations
relying only on simple metrics (e.g., accuracy), we propose fine-level
evaluations from objective and subjective manners, covering both answers and
explanations. Additionally, to uncover the logical flaws of LLMs, problematic
cases will be attributed to five error types from two dimensions, i.e.,
evidence selection process and reasoning process. Thirdly, to avoid the
influences of knowledge bias and purely focus on benchmarking the logical
reasoning capability of LLMs, we propose a new dataset with neutral content. It
contains 3,000 samples and covers deductive, inductive and abductive settings.
Based on the in-depth evaluations, this paper finally forms a general
evaluation scheme of logical reasoning capability from six dimensions. It
reflects the pros and cons of LLMs and gives guiding directions for future
works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1&quot;&gt;Fangzhi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Qika Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiawei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tianzhe Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1&quot;&gt;Erik Cambria&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02738">
<title>RecallM: An Architecture for Temporal Context Understanding and Question Answering. (arXiv:2307.02738v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02738</link>
<description rdf:parseType="Literal">&lt;p&gt;The ideal long-term memory mechanism for Large Language Model (LLM) based
chatbots, would lay the foundation for continual learning, complex reasoning
and allow sequential and temporal dependencies to be learnt. Creating this type
of memory mechanism is an extremely challenging problem. In this paper we
explore different methods of achieving the effect of long-term memory. We
propose a new architecture focused on creating adaptable and updatable
long-term memory for AGI systems. We demonstrate through various experiments
the benefits of the RecallM architecture, particularly the improved temporal
understanding of knowledge it provides.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kynoch_B/0/1/0/all/0/1&quot;&gt;Brandon Kynoch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latapie_H/0/1/0/all/0/1&quot;&gt;Hugo Latapie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03109">
<title>A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03109</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are gaining increasing popularity in both
academia and industry, owing to their unprecedented performance in various
applications. As LLMs continue to play a vital role in both research and daily
use, their evaluation becomes increasingly critical, not only at the task
level, but also at the society level for better understanding of their
potential risks. Over the past years, significant efforts have been made to
examine LLMs from various perspectives. This paper presents a comprehensive
review of these evaluation methods for LLMs, focusing on three key dimensions:
what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide
an overview from the perspective of evaluation tasks, encompassing general
natural language processing tasks, reasoning, medical usage, ethics,
educations, natural and social sciences, agent applications, and other areas.
Secondly, we answer the `where&apos; and `how&apos; questions by diving into the
evaluation methods and benchmarks, which serve as crucial components in
assessing performance of LLMs. Then, we summarize the success and failure cases
of LLMs in different tasks. Finally, we shed light on several future challenges
that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to
researchers in the realm of LLMs evaluation, thereby aiding the development of
more proficient LLMs. Our key point is that evaluation should be treated as an
essential discipline to better assist the development of LLMs. We consistently
maintain the related open-source materials at:
https://github.com/MLGroupJLU/LLM-eval-survey.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yupeng Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kaijie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Linyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1&quot;&gt;Xiaoyuan Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cunxiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yidong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wei Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yi Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>