<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-18T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10047" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10053" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10064" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10073" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10075" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10078" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10079" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10080" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10083" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10089" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10094" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10095" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10107" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10170" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10181" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10201" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10219" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10230" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2104.03617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.05075" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.06754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.12787" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.09147" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.15657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11650" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01071" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.12212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.13036" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11098" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08049" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14196" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19972" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13803" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15222" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01504" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08536" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.10639" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15293" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02161" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02601" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15318" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17658" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04498" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04666" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07745" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10599" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00377" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02674" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02855" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06833" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06901" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07392" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07398" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07696" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08064" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08533" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09857" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09983" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10029" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.01402" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.07317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09910" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06330" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09196" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.10045">
<title>Interpretable Knowledge Tracing via Response Influence-based Counterfactual Reasoning. (arXiv:2312.10045v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.10045</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge tracing (KT) plays a crucial role in computer-aided education and
intelligent tutoring systems, aiming to assess students&apos; knowledge proficiency
by predicting their future performance on new questions based on their past
response records. While existing deep learning knowledge tracing (DLKT) methods
have significantly improved prediction accuracy and achieved state-of-the-art
results, they often suffer from a lack of interpretability. To address this
limitation, current approaches have explored incorporating psychological
influences to achieve more explainable predictions, but they tend to overlook
the potential influences of historical responses. In fact, understanding how
models make predictions based on response influences can enhance the
transparency and trustworthiness of the knowledge tracing process, presenting
an opportunity for a new paradigm of interpretable KT. However, measuring
unobservable response influences is challenging. In this paper, we resort to
counterfactual reasoning that intervenes in each response to answer
\textit{what if a student had answered a question incorrectly that he/she
actually answered correctly, and vice versa}. Based on this, we propose RCKT, a
novel response influence-based counterfactual knowledge tracing framework. RCKT
generates response influences by comparing prediction outcomes from factual
sequences and constructed counterfactual sequences after interventions.
Additionally, we introduce maximization and inference techniques to leverage
accumulated influences from different past responses, further improving the
model&apos;s performance and credibility. Extensive experimental results demonstrate
that our RCKT method outperforms state-of-the-art knowledge tracing methods on
four datasets against six baselines, and provides credible interpretations of
response influences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jiajun Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1&quot;&gt;Minghe Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Aimin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianyong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10046">
<title>Deep Metric Learning for Computer Vision: A Brief Overview. (arXiv:2312.10046v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10046</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective functions that optimize deep neural networks play a vital role in
creating an enhanced feature representation of the input data. Although
cross-entropy-based loss formulations have been extensively used in a variety
of supervised deep-learning applications, these methods tend to be less
adequate when there is large intra-class variance and low inter-class variance
in input data distribution. Deep Metric Learning seeks to develop methods that
aim to measure the similarity between data samples by learning a representation
function that maps these data samples into a representative embedding space. It
leverages carefully designed sampling strategies and loss functions that aid in
optimizing the generation of a discriminative embedding space even for
distributions having low inter-class and high intra-class variances. In this
chapter, we will provide an overview of recent progress in this area and
discuss state-of-the-art Deep Metric Learning approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohan_D/0/1/0/all/0/1&quot;&gt;Deen Dayal Mohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jawade_B/0/1/0/all/0/1&quot;&gt;Bhavin Jawade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Setlur_S/0/1/0/all/0/1&quot;&gt;Srirangaraj Setlur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Govindaraj_V/0/1/0/all/0/1&quot;&gt;Venu Govindaraj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10047">
<title>Clustering Students According to their Academic Achievement Using Fuzzy Logic. (arXiv:2312.10047v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.10047</link>
<description rdf:parseType="Literal">&lt;p&gt;The software for clustering students according to their educational
achievements using fuzzy logic was developed in Python using the Google Colab
cloud service. In the process of analyzing educational data, the problems of
Data Mining are solved, since only some characteristics of the educational
process are obtained from a large sample of data. Data clustering was performed
using the classic K-Means method, which is characterized by simplicity and high
speed. Cluster analysis was performed in the space of two features using the
machine learning library scikit-learn (Python). The obtained clusters are
described by fuzzy triangular membership functions, which allowed to correctly
determine the membership of each student to a certain cluster. Creation of
fuzzy membership functions is done using the scikit-fuzzy library. The
development of fuzzy functions of objects belonging to clusters is also useful
for educational purposes, as it allows a better understanding of the principles
of using fuzzy logic. As a result of processing test educational data using the
developed software, correct results were obtained. It is shown that the use of
fuzzy membership functions makes it possible to correctly determine the
belonging of students to certain clusters, even if such clusters are not
clearly separated. Due to this, it is possible to more accurately determine the
recommended level of difficulty of tasks for each student, depending on his
previous evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balovsyak_S/0/1/0/all/0/1&quot;&gt;Serhiy Balovsyak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derevyanchuk_O/0/1/0/all/0/1&quot;&gt;Oleksandr Derevyanchuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kravchenko_H/0/1/0/all/0/1&quot;&gt;Hanna Kravchenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ushenko_Y/0/1/0/all/0/1&quot;&gt;Yuriy Ushenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhengbing Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10053">
<title>Towards Goal-oriented Intelligent Tutoring Systems in Online Education. (arXiv:2312.10053v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.10053</link>
<description rdf:parseType="Literal">&lt;p&gt;Interactive Intelligent Tutoring Systems (ITSs) enhance traditional ITSs by
promoting effective learning through interactions and problem resolution in
online education. Yet, proactive engagement, prioritizing resource optimization
with planning and assessment capabilities, is often overlooked in current ITS
designs. In this work, we investigate a new task, named Goal-oriented
Intelligent Tutoring Systems (GITS), which aims to enable the student&apos;s mastery
of a designated concept by strategically planning a customized sequence of
exercises and assessment. To address the problem of goal-oriented policy
learning in GITS, we propose a novel graph-based reinforcement learning
framework, named Planning-Assessment-Interaction (PAI). Specifically, we first
leverage cognitive structure information to improve state representation
learning and action selection for planning the next action, which can be either
to tutor an exercise or to assess the target concept. Further, we use a
dynamically updated cognitive diagnosis model to simulate student responses to
exercises and concepts. Three benchmark datasets across different subjects are
constructed for enabling offline academic research on GITS. Experimental
results demonstrate the effectiveness and efficiency of PAI and extensive
analyses of various types of students are conducted to showcase the challenges
in this task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zifeng Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;An Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1&quot;&gt;Wenqiang Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1&quot;&gt;Tat-Seng Chua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10055">
<title>Next-Step Hint Generation for Introductory Programming Using Large Language Models. (arXiv:2312.10055v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.10055</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models possess skills such as answering questions, writing
essays or solving programming exercises. Since these models are easily
accessible, researchers have investigated their capabilities and risks for
programming education. This work explores how LLMs can contribute to
programming education by supporting students with automated next-step hints. We
investigate prompt practices that lead to effective next-step hints and use
these insights to build our StAP-tutor. We evaluate this tutor by conducting an
experiment with students, and performing expert assessments. Our findings show
that most LLM-generated feedback messages describe one specific next step and
are personalised to the student&apos;s code and approach. However, the hints may
contain misleading information and lack sufficient detail when students
approach the end of the assignment. This work demonstrates the potential for
LLM-generated feedback, but further research is required to explore its
practical implementation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roest_L/0/1/0/all/0/1&quot;&gt;Lianne Roest&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuning_H/0/1/0/all/0/1&quot;&gt;Hieke Keuning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeuring_J/0/1/0/all/0/1&quot;&gt;Johan Jeuring&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10064">
<title>Dynamic Collaborative Filtering for Matrix- and Tensor-based Recommender Systems. (arXiv:2312.10064v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2312.10064</link>
<description rdf:parseType="Literal">&lt;p&gt;In production applications of recommender systems, a continuous data flow is
employed to update models in real-time. Many recommender models often require
complete retraining to adapt to new data. In this work, we introduce a novel
collaborative filtering model for sequential problems known as Tucker
Integrator Recommender - TIRecA. TIRecA efficiently updates its parameters
using only the new data segment, allowing incremental addition of new users and
items to the recommender system. To demonstrate the effectiveness of the
proposed model, we conducted experiments on four publicly available datasets:
MovieLens 20M, Amazon Beauty, Amazon Toys and Games, and Steam. Our comparison
with general matrix and tensor-based baselines in terms of prediction quality
and computational time reveals that TIRecA achieves comparable quality to the
baseline methods, while being 10-20 times faster in training time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saiapin_A/0/1/0/all/0/1&quot;&gt;Albert Saiapin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1&quot;&gt;Ivan Oseledets&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frolov_E/0/1/0/all/0/1&quot;&gt;Evgeny Frolov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10065">
<title>Exploring Social Bias in Downstream Applications of Text-to-Image Foundation Models. (arXiv:2312.10065v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.10065</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image diffusion models have been adopted into key commercial
workflows, such as art generation and image editing. Characterising the
implicit social biases they exhibit, such as gender and racial stereotypes, is
a necessary first step in avoiding discriminatory outcomes. While existing
studies on social bias focus on image generation, the biases exhibited in
alternate applications of diffusion-based foundation models remain
under-explored. We propose methods that use synthetic images to probe two
applications of diffusion models, image editing and classification, for social
bias. Using our methodology, we uncover meaningful and significant
inter-sectional social biases in \textit{Stable Diffusion}, a state-of-the-art
open-source text-to-image model. Our findings caution against the uninformed
adoption of text-to-image foundation models for downstream tasks and services.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saravanan_A/0/1/0/all/0/1&quot;&gt;Adhithya Prakash Saravanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kocielnik_R/0/1/0/all/0/1&quot;&gt;Rafal Kocielnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1&quot;&gt;Roy Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_P/0/1/0/all/0/1&quot;&gt;Pengrui Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1&quot;&gt;Anima Anandkumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10067">
<title>Teenagers and Artificial Intelligence: Bootcamp Experience and Lessons Learned. (arXiv:2312.10067v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.10067</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) stands out as a game-changer in today&apos;s
technology landscape. However, the integration of AI education in classroom
curricula currently lags behind, leaving teenagers inadequately prepared for an
imminent AI-driven future.
&lt;/p&gt;
&lt;p&gt;In this pilot study, we designed a three-day bootcamp offered in the summer
of 2023 to a cohort of 60 high school students. The curriculum was delivered in
person through animated video content, easy-to-follow slides, interactive
playgrounds, and quizzes. These were packaged in the early version of an online
learning platform we are developing. Results from the post-bootcamp survey
conveyed a 91.4% overall satisfaction. Despite the short bootcamp duration,
88.5% and 71.4% of teenagers responded that they had an improved understanding
of AI concepts and programming, respectively.
&lt;/p&gt;
&lt;p&gt;Overall, we found that employing diverse modalities effectively engaged
students, and building foundational modules proved beneficial for introducing
more complex topics. Furthermore, using Google Colab notebooks for coding
assignments proved challenging to most students. Students&apos; activity on the
platform and their answers to quizzes showed proficient engagement and a grasp
of the material.
&lt;/p&gt;
&lt;p&gt;Our results strongly highlight the need for compelling and accessible AI
education methods for the next generation and the potential for informal
learning to fill the gap of providing early AI education to teenagers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macar_U/0/1/0/all/0/1&quot;&gt;Uzay Macar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castleman_B/0/1/0/all/0/1&quot;&gt;Blake Castleman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mauchly_N/0/1/0/all/0/1&quot;&gt;Noah Mauchly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Michael Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aouissi_A/0/1/0/all/0/1&quot;&gt;Asma Aouissi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aouissi_S/0/1/0/all/0/1&quot;&gt;Salma Aouissi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maayah_X/0/1/0/all/0/1&quot;&gt;Xena Maayah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erdem_K/0/1/0/all/0/1&quot;&gt;Kaan Erdem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravindranath_R/0/1/0/all/0/1&quot;&gt;Rohith Ravindranath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_Sevilla_A/0/1/0/all/0/1&quot;&gt;Andrea Clark-Sevilla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salleb_Aouissi_A/0/1/0/all/0/1&quot;&gt;Ansaf Salleb-Aouissi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10068">
<title>Estimation of Physical Parameters of Waveforms With Neural Networks. (arXiv:2312.10068v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2312.10068</link>
<description rdf:parseType="Literal">&lt;p&gt;Light Detection and Ranging (LiDAR) are fast emerging sensors in the field of
Earth Observation. It is a remote sensing technology that utilizes laser beams
to measure distances and create detailed three-dimensional representations of
objects and environments. The potential of Full Waveform LiDAR is much greater
than just height estimation and 3D reconstruction only. Overall shape of signal
provides important information about properties of water body. However, the
shape of FWL is unexplored as most LiDAR software work on point cloud by
utilizing the maximum value within the waveform. Existing techniques in the
field of LiDAR data analysis include depth estimation through inverse modeling
and regression of logarithmic intensity and depth for approximating the
attenuation coefficient. However, these methods suffer from limitations in
accuracy. Depth estimation through inverse modeling provides only approximate
values and does not account for variations in surface properties, while the
regression approach for the attenuation coefficient is only able to generalize
a value through several data points which lacks precision and may lead to
significant errors in estimation. Additionally, there is currently no
established modeling method available for predicting bottom reflectance. This
research proposed a novel solution based on neural networks for parameter
estimation in LIDAR data analysis. By leveraging the power of neural networks,
the proposed solution successfully learned the inversion model, was able to do
prediction of parameters such as depth, attenuation coefficient, and bottom
reflectance. Performance of model was validated by testing it on real LiDAR
data. In future, more data availability would enable more accuracy and
reliability of such models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jamal_S/0/1/0/all/0/1&quot;&gt;Saad Ahmed Jamal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Corpetti_T/0/1/0/all/0/1&quot;&gt;Thomas Corpetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tiede_D/0/1/0/all/0/1&quot;&gt;Dirk Tiede&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Letard_M/0/1/0/all/0/1&quot;&gt;Mathilde Letard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lague_D/0/1/0/all/0/1&quot;&gt;Dimitri Lague&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10072">
<title>Assessing the Usability of GutGPT: A Simulation Study of an AI Clinical Decision Support System for Gastrointestinal Bleeding Risk. (arXiv:2312.10072v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.10072</link>
<description rdf:parseType="Literal">&lt;p&gt;Applications of large language models (LLMs) like ChatGPT have potential to
enhance clinical decision support through conversational interfaces. However,
challenges of human-algorithmic interaction and clinician trust are poorly
understood. GutGPT, a LLM for gastrointestinal (GI) bleeding risk prediction
and management guidance, was deployed in clinical simulation scenarios
alongside the electronic health record (EHR) with emergency medicine
physicians, internal medicine physicians, and medical students to evaluate its
effect on physician acceptance and trust in AI clinical decision support
systems (AI-CDSS). GutGPT provides risk predictions from a validated machine
learning model and evidence-based answers by querying extracted clinical
guidelines. Participants were randomized to GutGPT and an interactive
dashboard, or the interactive dashboard and a search engine. Surveys and
educational assessments taken before and after measured technology acceptance
and content mastery. Preliminary results showed mixed effects on acceptance
after using GutGPT compared to the dashboard or search engine but appeared to
improve content mastery based on simulation performance. Overall, this study
demonstrates LLMs like GutGPT could enhance effective AI-CDSS if implemented
optimally and paired with interactive interfaces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1&quot;&gt;Colleen Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_K/0/1/0/all/0/1&quot;&gt;Kisung You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1&quot;&gt;Sunny Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giuffre_M/0/1/0/all/0/1&quot;&gt;Mauro Giuffr&amp;#xe8;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saarinen_T/0/1/0/all/0/1&quot;&gt;Theo Saarinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajashekar_N/0/1/0/all/0/1&quot;&gt;Niroop Rajashekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1&quot;&gt;Yuan Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1&quot;&gt;Yeo Eun Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laine_L/0/1/0/all/0/1&quot;&gt;Loren Laine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Ambrose Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kizilcec_R/0/1/0/all/0/1&quot;&gt;Ren&amp;#xe9; Kizilcec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sekhon_J/0/1/0/all/0/1&quot;&gt;Jasjeet Sekhon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shung_D/0/1/0/all/0/1&quot;&gt;Dennis Shung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10073">
<title>Data Scarcity in Recommendation Systems: A Survey. (arXiv:2312.10073v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2312.10073</link>
<description rdf:parseType="Literal">&lt;p&gt;The prevalence of online content has led to the widespread adoption of
recommendation systems (RSs), which serve diverse purposes such as news,
advertisements, and e-commerce recommendations. Despite their significance,
data scarcity issues have significantly impaired the effectiveness of existing
RS models and hindered their progress. To address this challenge, the concept
of knowledge transfer, particularly from external sources like pre-trained
language models, emerges as a potential solution to alleviate data scarcity and
enhance RS development. However, the practice of knowledge transfer in RSs is
intricate. Transferring knowledge between domains introduces data disparities,
and the application of knowledge transfer in complex RS scenarios can yield
negative consequences if not carefully designed. Therefore, this article
contributes to this discourse by addressing the implications of data scarcity
on RSs and introducing various strategies, such as data augmentation,
self-supervised learning, transfer learning, broad learning, and knowledge
graph utilization, to mitigate this challenge. Furthermore, it delves into the
challenges and future direction within the RS domain, offering insights that
are poised to facilitate the development and implementation of robust RSs,
particularly when confronted with data scarcity. We aim to provide valuable
guidance and inspiration for researchers and practitioners, ultimately driving
advancements in the field of RS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zefeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1&quot;&gt;Wensheng Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiayang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1&quot;&gt;Kaixia Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hong Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10075">
<title>Assessing LLMs for Moral Value Pluralism. (arXiv:2312.10075v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.10075</link>
<description rdf:parseType="Literal">&lt;p&gt;The fields of AI current lacks methods to quantitatively assess and
potentially alter the moral values inherent in the output of large language
models (LLMs). However, decades of social science research has developed and
refined widely-accepted moral value surveys, such as the World Values Survey
(WVS), eliciting value judgments from direct questions in various geographies.
We have turned those questions into value statements and use NLP to compute to
how well popular LLMs are aligned with moral values for various demographics
and cultures. While the WVS is accepted as an explicit assessment of values, we
lack methods for assessing implicit moral and cultural values in media, e.g.,
encountered in social media, political rhetoric, narratives, and generated by
AI systems such as LLMs that are increasingly present in our daily lives. As we
consume online content and utilize LLM outputs, we might ask, which moral
values are being implicitly promoted or undercut, or -- in the case of LLMs --
if they are intending to represent a cultural identity, are they doing so
consistently? In this paper we utilize a Recognizing Value Resonance (RVR) NLP
model to identify WVS values that resonate and conflict with a given passage of
output text. We apply RVR to the text generated by LLMs to characterize
implicit moral values, allowing us to quantify the moral/cultural distance
between LLMs and various demographics that have been surveyed using the WVS. In
line with other work we find that LLMs exhibit several Western-centric value
biases; they overestimate how conservative people in non-Western countries are,
they are less accurate in representing gender for non-Western countries, and
portray older populations as having more traditional values. Our results
highlight value misalignment and age groups, and a need for social science
informed technological solutions addressing value plurality in LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benkler_N/0/1/0/all/0/1&quot;&gt;Noam Benkler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mosaphir_D/0/1/0/all/0/1&quot;&gt;Drisana Mosaphir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedman_S/0/1/0/all/0/1&quot;&gt;Scott Friedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smart_A/0/1/0/all/0/1&quot;&gt;Andrew Smart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmer_Galunder_S/0/1/0/all/0/1&quot;&gt;Sonja Schmer-Galunder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10078">
<title>Early ChatGPT User Portrait through the Lens of Data. (arXiv:2312.10078v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.10078</link>
<description rdf:parseType="Literal">&lt;p&gt;Since its launch, ChatGPT has achieved remarkable success as a versatile
conversational AI platform, drawing millions of users worldwide and garnering
widespread recognition across academic, industrial, and general communities.
This paper aims to point a portrait of early GPT users and understand how they
evolved. Specific questions include their topics of interest and their
potential careers; and how this changes over time. We conduct a detailed
analysis of real-world ChatGPT datasets with multi-turn conversations between
users and ChatGPT. Through a multi-pronged approach, we quantify conversation
dynamics by examining the number of turns, then gauge sentiment to understand
user sentiment variations, and finally employ Latent Dirichlet Allocation (LDA)
to discern overarching topics within the conversation. By understanding shifts
in user demographics and interests, we aim to shed light on the changing nature
of human-AI interaction and anticipate future trends in user engagement with
language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yuyang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1&quot;&gt;Ni Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xin Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10079">
<title>Music Recommendation on Spotify using Deep Learning. (arXiv:2312.10079v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2312.10079</link>
<description rdf:parseType="Literal">&lt;p&gt;Hosting about 50 million songs and 4 billion playlists, there is an enormous
amount of data generated at Spotify every single day - upwards of 600 gigabytes
of data (harvard.edu). Since the algorithms that Spotify uses in recommendation
systems is proprietary and confidential, code for big data analytics and
recommendation can only be speculated. However, it is widely theorized that
Spotify uses two main strategies to target users&apos; playlists and personalized
mixes that are infamous for their retention - exploration and exploitation
(kaggle.com). This paper aims to appropriate filtering using the approach of
deep learning for maximum user likeability. The architecture achieves 98.57%
and 80% training and validation accuracy respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maheshwari_C/0/1/0/all/0/1&quot;&gt;Chhavi Maheshwari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10080">
<title>No prejudice! Fair Federated Graph Neural Networks for Personalized Recommendation. (arXiv:2312.10080v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2312.10080</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensuring fairness in Recommendation Systems (RSs) across demographic groups
is critical due to the increased integration of RSs in applications such as
personalized healthcare, finance, and e-commerce. Graph-based RSs play a
crucial role in capturing intricate higher-order interactions among entities.
However, integrating these graph models into the Federated Learning (FL)
paradigm with fairness constraints poses formidable challenges as this requires
access to the entire interaction graph and sensitive user information (such as
gender, age, etc.) at the central server. This paper addresses the pervasive
issue of inherent bias within RSs for different demographic groups without
compromising the privacy of sensitive user attributes in FL environment with
the graph-based model. To address the group bias, we propose F2PGNN (Fair
Federated Personalized Graph Neural Network), a novel framework that leverages
the power of Personalized Graph Neural Network (GNN) coupled with fairness
considerations. Additionally, we use differential privacy techniques to fortify
privacy protection. Experimental evaluation on three publicly available
datasets showcases the efficacy of F2PGNN in mitigating group unfairness by 47%
- 99% compared to the state-of-the-art while preserving privacy and maintaining
the utility. The results validate the significance of our framework in
achieving equitable and personalized recommendations using GNN within the FL
landscape.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_N/0/1/0/all/0/1&quot;&gt;Nimesh Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sirohi_A/0/1/0/all/0/1&quot;&gt;Anuj Kumar Sirohi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayadeva/0/1/0/all/0/1&quot;&gt;Jayadeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sandeep Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10083">
<title>The Limits of Fair Medical Imaging AI In The Wild. (arXiv:2312.10083v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.10083</link>
<description rdf:parseType="Literal">&lt;p&gt;As artificial intelligence (AI) rapidly approaches human-level performance in
medical imaging, it is crucial that it does not exacerbate or propagate
healthcare disparities. Prior research has established AI&apos;s capacity to infer
demographic data from chest X-rays, leading to a key concern: do models using
demographic shortcuts have unfair predictions across subpopulations? In this
study, we conduct a thorough investigation into the extent to which medical AI
utilizes demographic encodings, focusing on potential fairness discrepancies
within both in-distribution training sets and external test sets. Our analysis
covers three key medical imaging disciplines: radiology, dermatology, and
ophthalmology, and incorporates data from six global chest X-ray datasets. We
confirm that medical imaging AI leverages demographic shortcuts in disease
classification. While correcting shortcuts algorithmically effectively
addresses fairness gaps to create &quot;locally optimal&quot; models within the original
data distribution, this optimality is not true in new test settings.
Surprisingly, we find that models with less encoding of demographic attributes
are often most &quot;globally optimal&quot;, exhibiting better fairness during model
evaluation in new test environments. Our work establishes best practices for
medical imaging models which maintain their performance and fairness in
deployments beyond their initial training contexts, underscoring critical
considerations for AI clinical deployments across populations and sites.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haoran Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gichoya_J/0/1/0/all/0/1&quot;&gt;Judy W Gichoya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katabi_D/0/1/0/all/0/1&quot;&gt;Dina Katabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1&quot;&gt;Marzyeh Ghassemi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10089">
<title>Advancements in Content-Based Image Retrieval: A Comprehensive Survey of Relevance Feedback Techniques. (arXiv:2312.10089v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10089</link>
<description rdf:parseType="Literal">&lt;p&gt;Content-based image retrieval (CBIR) systems have emerged as crucial tools in
the field of computer vision, allowing for image search based on visual content
rather than relying solely on metadata. This survey paper presents a
comprehensive overview of CBIR, emphasizing its role in object detection and
its potential to identify and retrieve visually similar images based on content
features. Challenges faced by CBIR systems, including the semantic gap and
scalability, are discussed, along with potential solutions. It elaborates on
the semantic gap, which arises from the disparity between low-level features
and high-level semantic concepts, and explores approaches to bridge this gap.
One notable solution is the integration of relevance feedback (RF), empowering
users to provide feedback on retrieved images and refine search results
iteratively. The survey encompasses long-term and short-term learning
approaches that leverage RF for enhanced CBIR accuracy and relevance. These
methods focus on weight optimization and the utilization of active learning
algorithms to select samples for training classifiers. Furthermore, the paper
investigates machine learning techniques and the utilization of deep learning
and convolutional neural networks to enhance CBIR performance. This survey
paper plays a significant role in advancing the understanding of CBIR and RF
techniques. It guides researchers and practitioners in comprehending existing
methodologies, challenges, and potential solutions while fostering knowledge
dissemination and identifying research gaps. By addressing future research
directions, it sets the stage for advancements in CBIR that will enhance
retrieval accuracy, usability, and effectiveness in various application
domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qazanfari_H/0/1/0/all/0/1&quot;&gt;Hamed Qazanfari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+AlyanNezhadi_M/0/1/0/all/0/1&quot;&gt;Mohammad M. AlyanNezhadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khoshdaregi_Z/0/1/0/all/0/1&quot;&gt;Zohreh Nozari Khoshdaregi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10094">
<title>Evaluative Item-Contrastive Explanations in Rankings. (arXiv:2312.10094v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2312.10094</link>
<description rdf:parseType="Literal">&lt;p&gt;The remarkable success of Artificial Intelligence in advancing automated
decision-making is evident both in academia and industry. Within the plethora
of applications, ranking systems hold significant importance in various
domains. This paper advocates for the application of a specific form of
Explainable AI -- namely, contrastive explanations -- as particularly
well-suited for addressing ranking problems. This approach is especially potent
when combined with an Evaluative AI methodology, which conscientiously
evaluates both positive and negative aspects influencing a potential ranking.
Therefore, the present work introduces Evaluative Item-Contrastive Explanations
tailored for ranking systems and illustrates its application and
characteristics through an experiment conducted on publicly available data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castelnovo_A/0/1/0/all/0/1&quot;&gt;Alessandro Castelnovo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crupi_R/0/1/0/all/0/1&quot;&gt;Riccardo Crupi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mombelli_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xf2; Mombelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nanino_G/0/1/0/all/0/1&quot;&gt;Gabriele Nanino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Regoli_D/0/1/0/all/0/1&quot;&gt;Daniele Regoli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10095">
<title>Casual Social Media Use among the Youth: Effects on Online and Offline Political Participation. (arXiv:2312.10095v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.10095</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: Previous studies suggest that social media use among the youth is
correlated with online and offline political participation. There is also a
mixed and inconclusive debate on whether more online political participation in
the youth increases their offline political participation. Methods: This study
uses three models of OLS, two-way fixed effects, and an instrumental variable
approach to make causal inferences about social media use, online, and offline
political participation of the youth. Findings: The analyses provide evidence
of a large effect of casual social media use on online political participation,
and no effect or negligible effect on offline political participation and
voting behavior. The results from fixed effects and instrumental variable
models provide strong evidence of elasticity between online and offline
political participation in young individuals. On average, a one percent
increase in online political participation increases the offline political
activity index by 0.12 percent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barati_M/0/1/0/all/0/1&quot;&gt;Mehdi Barati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10107">
<title>Towards Context-Aware Domain Generalization: Representing Environments with Permutation-Invariant Networks. (arXiv:2312.10107v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.10107</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we show that information about the context of an input $X$ can
improve the predictions of deep learning models when applied in new domains or
production environments. We formalize the notion of context as a
permutation-invariant representation of a set of data points that originate
from the same environment/domain as the input itself. These representations are
jointly learned with a standard supervised learning objective, providing
incremental information about the unknown outcome. Furthermore, we offer a
theoretical analysis of the conditions under which our approach can, in
principle, yield benefits, and formulate two necessary criteria that can be
easily verified in practice. Additionally, we contribute insights into the kind
of distribution shifts for which our approach promises robustness. Our
empirical evaluation demonstrates the effectiveness of our approach for both
low-dimensional and high-dimensional data sets. Finally, we demonstrate that we
can reliably detect scenarios where a model is tasked with unwarranted
extrapolation in out-of-distribution (OOD) domains, identifying potential
failure cases. Consequently, we showcase a method to select between the most
predictive and the most robust model, circumventing the well-known trade-off
between predictive performance and robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_J/0/1/0/all/0/1&quot;&gt;Jens M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhmichel_L/0/1/0/all/0/1&quot;&gt;Lars K&amp;#xfc;hmichel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rohbeck_M/0/1/0/all/0/1&quot;&gt;Martin Rohbeck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radev_S/0/1/0/all/0/1&quot;&gt;Stefan T. Radev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kothe_U/0/1/0/all/0/1&quot;&gt;Ullrich K&amp;#xf6;the&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10108">
<title>Privacy-Aware Document Visual Question Answering. (arXiv:2312.10108v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10108</link>
<description rdf:parseType="Literal">&lt;p&gt;Document Visual Question Answering (DocVQA) is a fast growing branch of
document understanding. Despite the fact that documents contain sensitive or
copyrighted information, none of the current DocVQA methods offers strong
privacy guarantees.
&lt;/p&gt;
&lt;p&gt;In this work, we explore privacy in the domain of DocVQA for the first time.
We highlight privacy issues in state of the art multi-modal LLM models used for
DocVQA, and explore possible solutions.
&lt;/p&gt;
&lt;p&gt;Specifically, we focus on the invoice processing use case as a realistic,
widely used scenario for document understanding, and propose a large scale
DocVQA dataset comprising invoice documents and associated questions and
answers. We employ a federated learning scheme, that reflects the real-life
distribution of documents in different businesses, and we explore the use case
where the ID of the invoice issuer is the sensitive information to be
protected.
&lt;/p&gt;
&lt;p&gt;We demonstrate that non-private models tend to memorise, behaviour that can
lead to exposing private information. We then evaluate baseline training
schemes employing federated learning and differential privacy in this
multi-modal scenario, where the sensitive information might be exposed through
any of the two input modalities: vision (document image) or language (OCR
tokens).
&lt;/p&gt;
&lt;p&gt;Finally, we design an attack exploiting the memorisation effect of the model,
and demonstrate its effectiveness in probing different DocVQA models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tito_R/0/1/0/all/0/1&quot;&gt;Rub&amp;#xe8;n Tito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Khanh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tobaben_M/0/1/0/all/0/1&quot;&gt;Marlon Tobaben&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerkouche_R/0/1/0/all/0/1&quot;&gt;Raouf Kerkouche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souibgui_M/0/1/0/all/0/1&quot;&gt;Mohamed Ali Souibgui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1&quot;&gt;Kangsoo Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1&quot;&gt;Lei Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1&quot;&gt;Ernest Valveny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honkela_A/0/1/0/all/0/1&quot;&gt;Antti Honkela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1&quot;&gt;Mario Fritz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1&quot;&gt;Dimosthenis Karatzas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10109">
<title>Enlighten-Your-Voice: When Multimodal Meets Zero-shot Low-light Image Enhancement. (arXiv:2312.10109v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10109</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-light image enhancement is a crucial visual task, and many unsupervised
methods tend to overlook the degradation of visible information in low-light
scenes, which adversely affects the fusion of complementary information and
hinders the generation of satisfactory results. To address this, our study
introduces ``Enlighten-Your-Voice&apos;&apos;, a multimodal enhancement framework that
innovatively enriches user interaction through voice and textual commands. This
approach does not merely signify a technical leap but also represents a
paradigm shift in user engagement. Our model is equipped with a Dual
Collaborative Attention Module (DCAM) that meticulously caters to distinct
content and color discrepancies, thereby facilitating nuanced enhancements.
Complementarily, we introduce a Semantic Feature Fusion (SFM) plug-and-play
module that synergizes semantic context with low-light enhancement operations,
sharpening the algorithm&apos;s efficacy. Crucially, ``Enlighten-Your-Voice&apos;&apos;
showcases remarkable generalization in unsupervised zero-shot scenarios. The
source code can be accessed from
https://github.com/zhangbaijin/Enlighten-Your-Voice
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zishan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1&quot;&gt;Chaochen Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Shanying Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1&quot;&gt;Xinping Guan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10128">
<title>An Information-Flow Perspective on Algorithmic Fairness. (arXiv:2312.10128v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.10128</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents insights gained by investigating the relationship between
algorithmic fairness and the concept of secure information flow. The problem of
enforcing secure information flow is well-studied in the context of information
security: If secret information may &quot;flow&quot; through an algorithm or program in
such a way that it can influence the program&apos;s output, then that is considered
insecure information flow as attackers could potentially observe (parts of) the
secret.
&lt;/p&gt;
&lt;p&gt;There is a strong correspondence between secure information flow and
algorithmic fairness: if protected attributes such as race, gender, or age are
treated as secret program inputs, then secure information flow means that these
``secret&apos;&apos; attributes cannot influence the result of a program. While most
research in algorithmic fairness evaluation concentrates on studying the impact
of algorithms (often treating the algorithm as a black-box), the concepts
derived from information flow can be used both for the analysis of disparate
treatment as well as disparate impact w.r.t. a structural causal model.
&lt;/p&gt;
&lt;p&gt;In this paper, we examine the relationship between quantitative as well as
qualitative information-flow properties and fairness. Moreover, based on this
duality, we derive a new quantitative notion of fairness called fairness
spread, which can be easily analyzed using quantitative information flow and
which strongly relates to counterfactual fairness. We demonstrate that
off-the-shelf tools for information-flow properties can be used in order to
formally analyze a program&apos;s algorithmic fairness properties, including the new
notion of fairness spread as well as established notions such as demographic
parity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teuber_S/0/1/0/all/0/1&quot;&gt;Samuel Teuber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beckert_B/0/1/0/all/0/1&quot;&gt;Bernhard Beckert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10144">
<title>Data-Efficient Multimodal Fusion on a Single GPU. (arXiv:2312.10144v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.10144</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of multimodal alignment is to learn a single latent space that is
shared between multimodal inputs. The most powerful models in this space have
been trained using massive datasets of paired inputs and large-scale
computational resources, making them prohibitively expensive to train in many
practical scenarios. We surmise that existing unimodal encoders pre-trained on
large amounts of unimodal data should provide an effective bootstrap to create
multimodal models from unimodal ones at much lower costs. We therefore propose
FuseMix, a multimodal augmentation scheme that operates on the latent spaces of
arbitrary pre-trained unimodal encoders. Using FuseMix for multimodal
alignment, we achieve competitive performance -- and in certain cases
outperform state-of-the art methods -- in both image-text and audio-text
retrieval, with orders of magnitude less compute and data: for example, we
outperform CLIP on the Flickr30K text-to-image retrieval task with $\sim \!
600\times$ fewer GPU days and $\sim \! 80\times$ fewer image-text pairs.
Additionally, we show how our method can be applied to convert pre-trained
text-to-image generative models into audio-to-image ones. Code is available at:
https://github.com/layer6ai-labs/fusemix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vouitsis_N/0/1/0/all/0/1&quot;&gt;No&amp;#xeb;l Vouitsis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhaoyan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorti_S/0/1/0/all/0/1&quot;&gt;Satya Krishna Gorti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villecroze_V/0/1/0/all/0/1&quot;&gt;Valentin Villecroze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cresswell_J/0/1/0/all/0/1&quot;&gt;Jesse C. Cresswell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Guangwei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loaiza_Ganem_G/0/1/0/all/0/1&quot;&gt;Gabriel Loaiza-Ganem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volkovs_M/0/1/0/all/0/1&quot;&gt;Maksims Volkovs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10153">
<title>Bayesian Metaplasticity from Synaptic Uncertainty. (arXiv:2312.10153v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.10153</link>
<description rdf:parseType="Literal">&lt;p&gt;Catastrophic forgetting remains a challenge for neural networks, especially
in lifelong learning scenarios. In this study, we introduce MEtaplasticity from
Synaptic Uncertainty (MESU), inspired by metaplasticity and Bayesian inference
principles. MESU harnesses synaptic uncertainty to retain information over
time, with its update rule closely approximating the diagonal Newton&apos;s method
for synaptic updates. Through continual learning experiments on permuted MNIST
tasks, we demonstrate MESU&apos;s remarkable capability to maintain learning
performance across 100 tasks without the need of explicit task boundaries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonnet_D/0/1/0/all/0/1&quot;&gt;Djohan Bonnet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirtzlin_T/0/1/0/all/0/1&quot;&gt;Tifenn Hirtzlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Januel_T/0/1/0/all/0/1&quot;&gt;Tarcisius Januel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalgaty_T/0/1/0/all/0/1&quot;&gt;Thomas Dalgaty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Querlioz_D/0/1/0/all/0/1&quot;&gt;Damien Querlioz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vianello_E/0/1/0/all/0/1&quot;&gt;Elisa Vianello&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10170">
<title>UINav: A maker of UI automation agents. (arXiv:2312.10170v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.10170</link>
<description rdf:parseType="Literal">&lt;p&gt;An automation system that can execute natural language instructions by
driving the user interface (UI) of an application can benefit users, especially
when situationally or permanently impaired. Traditional automation systems
(manual scripting, programming by demonstration tools, etc.) do not produce
generalizable models that can tolerate changes in the UI or task workflow.
Machine-learned automation agents generalize better, but either work only in
simple, hand-crafted applications or rely on large pre-trained models, which
may be too computationally expensive to run on mobile devices. In this paper,
we propose \emph{UINav}, a demonstration-based agent maker system. UINav agents
are lightweight enough to run on mobile devices, yet they achieve high success
rates with a modest number of task demonstrations. To minimize the number of
task demonstrations, UINav includes a referee model that allows users to
receive immediate feedback on tasks where the agent is failing to best guide
efforts to collect additional demonstrations. Further, UINav adopts macro
actions to reduce an agent&apos;s state space, and augments human demonstrations to
increase the diversity of training data. Our evaluation demonstrates that with
an average of 10 demonstrations per task UINav can achieve an accuracy of 70\%
or higher, and that with enough demonstrations it can achieve near-perfect
success rates on 40+ different tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_F/0/1/0/all/0/1&quot;&gt;Fu-Lin Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bishop_W/0/1/0/all/0/1&quot;&gt;Will Bishop&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campbell_Ajala_F/0/1/0/all/0/1&quot;&gt;Folawiyo Campbell-Ajala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riva_O/0/1/0/all/0/1&quot;&gt;Oriana Riva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Max Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10181">
<title>Coupling Fairness and Pruning in a Single Run: a Bi-level Optimization Perspective. (arXiv:2312.10181v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.10181</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have demonstrated remarkable performance in various
tasks. With a growing need for sparse deep learning, model compression
techniques, especially pruning, have gained significant attention. However,
conventional pruning techniques can inadvertently exacerbate algorithmic bias,
resulting in unequal predictions. To address this, we define a fair pruning
task where a sparse model is derived subject to fairness requirements. In
particular, we propose a framework to jointly optimize the pruning mask and
weight update processes with fairness constraints. This framework is engineered
to compress models that maintain performance while ensuring fairness in a
single execution. To this end, we formulate the fair pruning problem as a novel
constrained bi-level optimization task and derive efficient and effective
solving strategies. We design experiments spanning various datasets and
settings to validate our proposed method. Our empirical analysis contrasts our
framework with several mainstream pruning strategies, emphasizing our method&apos;s
superiority in maintaining model fairness, performance, and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yucong Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1&quot;&gt;Feng Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaolong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yongkai Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10195">
<title>SoloPose: One-Shot Kinematic 3D Human Pose Estimation with Video Data Augmentation. (arXiv:2312.10195v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10195</link>
<description rdf:parseType="Literal">&lt;p&gt;While recent two-stage many-to-one deep learning models have demonstrated
great success in 3D human pose estimation, such models are inefficient ways to
detect 3D key points in a sequential video relative to one-shot and
many-to-many models. Another key drawback of two-stage and many-to-one models
is that errors in the first stage will be passed onto the second stage. In this
paper, we introduce SoloPose, a novel one-shot, many-to-many spatio-temporal
transformer model for kinematic 3D human pose estimation of video. SoloPose is
further fortified by HeatPose, a 3D heatmap based on Gaussian Mixture Model
distributions that factors target key points as well as kinematically adjacent
key points. Finally, we address data diversity constraints with the 3D
AugMotion Toolkit, a methodology to augment existing 3D human pose datasets,
specifically by projecting four top public 3D human pose datasets (Humans3.6M,
MADS, AIST Dance++, MPI INF 3DHP) into a novel dataset (Humans7.1M) with a
universal coordinate system. Extensive experiments are conducted on Human3.6M
as well as the augmented Humans7.1M dataset, and SoloPose demonstrates superior
results relative to the state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_D/0/1/0/all/0/1&quot;&gt;David C. Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongji Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salazar_S/0/1/0/all/0/1&quot;&gt;Saunder Salazar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jessie Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitts_C/0/1/0/all/0/1&quot;&gt;Christopher A. Kitts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10201">
<title>CARAT: Contrastive Feature Reconstruction and Aggregation for Multi-modal Multi-label Emotion Recognition. (arXiv:2312.10201v1 [cs.MM])</title>
<link>http://arxiv.org/abs/2312.10201</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal multi-label emotion recognition (MMER) aims to identify relevant
emotions from multiple modalities. The challenge of MMER is how to effectively
capture discriminative features for multiple labels from heterogeneous data.
Recent studies are mainly devoted to exploring various fusion strategies to
integrate multi-modal information into a unified representation for all labels.
However, such a learning scheme not only overlooks the specificity of each
modality but also fails to capture individual discriminative features for
different labels. Moreover, dependencies of labels and modalities cannot be
effectively modeled. To address these issues, this paper presents ContrAstive
feature Reconstruction and AggregaTion (CARAT) for the MMER task. Specifically,
we devise a reconstruction-based fusion mechanism to better model fine-grained
modality-to-label dependencies by contrastively learning modal-separated and
label-specific features. To further exploit the modality complementarity, we
introduce a shuffle-based aggregation strategy to enrich co-occurrence
collaboration among labels. Experiments on two benchmark datasets CMU-MOSEI and
M3ED demonstrate the effectiveness of CARAT over state-of-the-art methods. Code
is available at https://github.com/chengzju/CARAT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1&quot;&gt;Cheng Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Ke Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1&quot;&gt;Lidan Shou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Gang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10219">
<title>The Complexity of Optimizing Atomic Congestion. (arXiv:2312.10219v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2312.10219</link>
<description rdf:parseType="Literal">&lt;p&gt;Atomic congestion games are a classic topic in network design, routing, and
algorithmic game theory, and are capable of modeling congestion and flow
optimization tasks in various application areas. While both the price of
anarchy for such games as well as the computational complexity of computing
their Nash equilibria are by now well-understood, the computational complexity
of computing a system-optimal set of strategies -- that is, a centrally planned
routing that minimizes the average cost of agents -- is severely understudied
in the literature. We close this gap by identifying the exact boundaries of
tractability for the problem through the lens of the parameterized complexity
paradigm. After showing that the problem remains highly intractable even on
extremely simple networks, we obtain a set of results which demonstrate that
the structural parameters which control the computational (in)tractability of
the problem are not vertex-separator based in nature (such as, e.g.,
treewidth), but rather based on edge separators. We conclude by extending our
analysis towards the (even more challenging) min-max variant of the problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brand_C/0/1/0/all/0/1&quot;&gt;Cornelius Brand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganian_R/0/1/0/all/0/1&quot;&gt;Robert Ganian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalyanasundaram_S/0/1/0/all/0/1&quot;&gt;Subrahmanyam Kalyanasundaram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inerney_F/0/1/0/all/0/1&quot;&gt;Fionn Mc Inerney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10230">
<title>Constrained Meta-Reinforcement Learning for Adaptable Safety Guarantee with Differentiable Convex Programming. (arXiv:2312.10230v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.10230</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite remarkable achievements in artificial intelligence, the deployability
of learning-enabled systems in high-stakes real-world environments still faces
persistent challenges. For example, in safety-critical domains like autonomous
driving, robotic manipulation, and healthcare, it is crucial not only to
achieve high performance but also to comply with given constraints.
Furthermore, adaptability becomes paramount in non-stationary domains, where
environmental parameters are subject to change. While safety and adaptability
are recognized as key qualities for the new generation of AI, current
approaches have not demonstrated effective adaptable performance in constrained
settings. Hence, this paper breaks new ground by studying the unique challenges
of ensuring safety in non-stationary environments by solving constrained
problems through the lens of the meta-learning approach (learning-to-learn).
While unconstrained meta-learning al-ready encounters complexities in
end-to-end differentiation of the loss due to the bi-level nature, its
constrained counterpart introduces an additional layer of difficulty, since the
constraints imposed on task-level updates complicate the differentiation
process. To address the issue, we first employ successive convex-constrained
policy updates across multiple tasks with differentiable convexprogramming,
which allows meta-learning in constrained scenarios by enabling end-to-end
differentiation. This approach empowers the agent to rapidly adapt to new tasks
under non-stationarity while ensuring compliance with safety constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1&quot;&gt;Minjae Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Chuangchuang Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2104.03617">
<title>Half-Truth: A Partially Fake Audio Detection Dataset. (arXiv:2104.03617v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2104.03617</link>
<description rdf:parseType="Literal">&lt;p&gt;Diverse promising datasets have been designed to hold back the development of
fake audio detection, such as ASVspoof databases. However, previous datasets
ignore an attacking situation, in which the hacker hides some small fake clips
in real speech audio. This poses a serious threat since that it is difficult to
distinguish the small fake clip from the whole speech utterance. Therefore,
this paper develops such a dataset for half-truth audio detection (HAD).
Partially fake audio in the HAD dataset involves only changing a few words in
an utterance.The audio of the words is generated with the very latest
state-of-the-art speech synthesis technology. We can not only detect fake
uttrances but also localize manipulated regions in a speech using this dataset.
Some benchmark results are presented on this dataset. The results show that
partially fake audio presents much more challenging than fully fake audio for
fake audio detection. The HAD dataset is publicly available:
https://zenodo.org/records/10377492.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Jiangyan Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Ye Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1&quot;&gt;Jianhua Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Haoxin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1&quot;&gt;Zhengkun Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chenglong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1&quot;&gt;Ruibo Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.05075">
<title>On the Compression of Neural Networks Using $\ell_0$-Norm Regularization and Weight Pruning. (arXiv:2109.05075v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2109.05075</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the growing availability of high-capacity computational platforms,
implementation complexity still has been a great concern for the real-world
deployment of neural networks. This concern is not exclusively due to the huge
costs of state-of-the-art network architectures, but also due to the recent
push towards edge intelligence and the use of neural networks in embedded
applications. In this context, network compression techniques have been gaining
interest due to their ability for reducing deployment costs while keeping
inference accuracy at satisfactory levels. The present paper is dedicated to
the development of a novel compression scheme for neural networks. To this end,
a new form of $\ell_0$-norm-based regularization is firstly developed, which is
capable of inducing strong sparseness in the network during training. Then,
targeting the smaller weights of the trained network with pruning techniques,
smaller yet highly effective networks can be obtained. The proposed compression
scheme also involves the use of $\ell_2$-norm regularization to avoid
overfitting as well as fine tuning to improve the performance of the pruned
network. Experimental results are presented aiming to show the effectiveness of
the proposed scheme as well as to make comparisons with competing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_F/0/1/0/all/0/1&quot;&gt;Felipe Dennis de Resende Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batista_E/0/1/0/all/0/1&quot;&gt;Eduardo Luiz Ortiz Batista&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seara_R/0/1/0/all/0/1&quot;&gt;Rui Seara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.06754">
<title>RecurSeed and EdgePredictMix: Pseudo-Label Refinement Learning for Weakly Supervised Semantic Segmentation across Single- and Multi-Stage Frameworks. (arXiv:2204.06754v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.06754</link>
<description rdf:parseType="Literal">&lt;p&gt;Although weakly supervised semantic segmentation using only image-level
labels (WSSS-IL) is potentially useful, its low performance and implementation
complexity still limit its application. The main causes are (a) non-detection
and (b) false-detection phenomena: (a) The class activation maps refined from
existing WSSS-IL methods still only represent partial regions for large-scale
objects, and (b) for small-scale objects, over-activation causes them to
deviate from the object edges. We propose RecurSeed, which alternately reduces
non- and false detections through recursive iterations, thereby implicitly
finding an optimal junction that minimizes both errors. We also propose a novel
data augmentation (DA) approach called EdgePredictMix, which further expresses
an object&apos;s edge by utilizing the probability difference information between
adjacent pixels in combining the segmentation results, thereby compensating for
the shortcomings when applying the existing DA methods to WSSS. We achieved new
state-of-the-art performances on both the PASCAL VOC 2012 and MS COCO 2014
benchmarks (VOC val: 74.4%, COCO val: 46.4%). The code is available at
https://github.com/shjo-april/RecurSeed_and_EdgePredictMix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jo_S/0/1/0/all/0/1&quot;&gt;Sanghyun Jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_I/0/1/0/all/0/1&quot;&gt;In-Jae Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kyungsu Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.12787">
<title>Impartial Games: A Challenge for Reinforcement Learning. (arXiv:2205.12787v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.12787</link>
<description rdf:parseType="Literal">&lt;p&gt;While AlphaZero-style reinforcement learning (RL) algorithms excel in various
board games, in this paper we show that they face challenges on impartial games
where players share pieces. We present a concrete example of a game - namely
the children&apos;s game of Nim - and other impartial games that seem to be a
stumbling block for AlphaZero-style and similar self-play reinforcement
learning algorithms.
&lt;/p&gt;
&lt;p&gt;Our work is built on the challenges posed by the intricacies of data
distribution on the ability of neural networks to learn parity functions,
exacerbated by the noisy labels issue. Our findings are consistent with recent
studies showing that AlphaZero-style algorithms are vulnerable to adversarial
attacks and adversarial perturbations, showing the difficulty of learning to
master the games in all legal states.
&lt;/p&gt;
&lt;p&gt;We show that Nim can be learned on small boards, but the learning progress of
AlphaZero-style algorithms dramatically slows down when the board size
increases. Intuitively, the difference between impartial games like Nim and
partisan games like Chess and Go can be explained by the fact that if a small
part of the board is covered for impartial games it is typically not possible
to predict whether the position is won or lost as there is often zero
correlation between the visible part of a partly blanked-out position and its
correct evaluation. This situation starkly contrasts partisan games where a
partly blanked-out board position typically provides abundant or at least
non-trifle information about the value of the fully uncovered position.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riis_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf8;ren Riis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.09147">
<title>Disentangled Representation with Causal Constraints for Counterfactual Fairness. (arXiv:2208.09147v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.09147</link>
<description rdf:parseType="Literal">&lt;p&gt;Much research has been devoted to the problem of learning fair
representations; however, they do not explicitly the relationship between
latent representations. In many real-world applications, there may be causal
relationships between latent representations. Furthermore, most fair
representation learning methods focus on group-level fairness and are based on
correlations, ignoring the causal relationships underlying the data. In this
work, we theoretically demonstrate that using the structured representations
enable downstream predictive models to achieve counterfactual fairness, and
then we propose the Counterfactual Fairness Variational AutoEncoder (CF-VAE) to
obtain structured representations with respect to domain knowledge. The
experimental results show that the proposed method achieves better fairness and
accuracy performance than the benchmark fairness methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Ziqi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jixue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1&quot;&gt;Debo Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiuyong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Ke Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.15657">
<title>Detecting fake accounts through Generative Adversarial Network in online social media. (arXiv:2210.15657v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2210.15657</link>
<description rdf:parseType="Literal">&lt;p&gt;Online social media is integral to human life, facilitating messaging,
information sharing, and confidential communication while preserving privacy.
Platforms like Twitter, Instagram, and Facebook exemplify this phenomenon.
However, users face challenges due to network anomalies, often stemming from
malicious activities such as identity theft for financial gain or harm. This
paper proposes a novel method using user similarity measures and the Generative
Adversarial Network (GAN) algorithm to identify fake user accounts in the
Twitter dataset. Despite the problem&apos;s complexity, the method achieves an AUC
rate of 80\% in classifying and detecting fake accounts. Notably, the study
builds on previous research, highlighting advancements and insights into the
evolving landscape of anomaly detection in online social networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bordbar_J/0/1/0/all/0/1&quot;&gt;Jinus Bordbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadrezaie_M/0/1/0/all/0/1&quot;&gt;Mohammadreza Mohammadrezaie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ardalan_S/0/1/0/all/0/1&quot;&gt;Saman Ardalan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shiri_M/0/1/0/all/0/1&quot;&gt;Mohammad Ebrahim Shiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11650">
<title>Neural Meta-Symbolic Reasoning and Learning. (arXiv:2211.11650v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11650</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural learning uses an increasing amount of computation and data to
solve very specific problems. By stark contrast, human minds solve a wide range
of problems using a fixed amount of computation and limited experience. One
ability that seems crucial to this kind of general intelligence is
meta-reasoning, i.e., our ability to reason about reasoning. To make deep
learning do more from less, we propose the first neural meta-symbolic system
(NEMESYS) for reasoning and learning: meta programming using differentiable
forward-chaining reasoning in first-order logic. Differentiable meta
programming naturally allows NEMESYS to reason and learn several tasks
efficiently. This is different from performing object-level deep reasoning and
learning, which refers in some way to entities external to the system. In
contrast, NEMESYS enables self-introspection, lifting from object- to
meta-level reasoning and vice versa. In our extensive experiments, we
demonstrate that NEMESYS can solve different kinds of tasks by adapting the
meta-level programs without modifying the internal reasoning system. Moreover,
we show that NEMESYS can learn meta-level programs given examples. This is
difficult, if not impossible, for standard differentiable logic programming
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1&quot;&gt;Zihan Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shindo_H/0/1/0/all/0/1&quot;&gt;Hikaru Shindo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhami_D/0/1/0/all/0/1&quot;&gt;Devendra Singh Dhami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01071">
<title>Fake detection in imbalance dataset by Semi-supervised learning with GAN. (arXiv:2212.01071v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01071</link>
<description rdf:parseType="Literal">&lt;p&gt;As social media continues to grow rapidly, the prevalence of harassment on
these platforms has also increased. This has piqued the interest of researchers
in the field of fake detection. Social media data, often forms complex graphs
with numerous nodes, posing several challenges. These challenges and
limitations include dealing with a significant amount of irrelevant features in
matrices and addressing issues such as high data dispersion and an imbalanced
class distribution within the dataset. To overcome these challenges and
limitations, researchers have employed auto-encoders and a combination of
semi-supervised learning with a GAN algorithm, referred to as SGAN. Our
proposed method utilizes auto-encoders for feature extraction and incorporates
SGAN. By leveraging an unlabeled dataset, the unsupervised layer of SGAN
compensates for the limited availability of labeled data, making efficient use
of the limited number of labeled instances. Multiple evaluation metrics were
employed, including the Confusion Matrix and the ROC curve. The dataset was
divided into training and testing sets, with 100 labeled samples for training
and 1,000 samples for testing. The novelty of our research lies in applying
SGAN to address the issue of imbalanced datasets in fake account detection. By
optimizing the use of a smaller number of labeled instances and reducing the
need for extensive computational power, our method offers a more efficient
solution. Additionally, our study contributes to the field by achieving an 81%
accuracy in detecting fake accounts using only 100 labeled samples. This
demonstrates the potential of SGAN as a powerful tool for handling minority
classes and addressing big data challenges in fake account detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bordbar_J/0/1/0/all/0/1&quot;&gt;Jinus Bordbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ardalan_S/0/1/0/all/0/1&quot;&gt;Saman Ardalan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadrezaie_M/0/1/0/all/0/1&quot;&gt;Mohammadreza Mohammadrezaie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghasemi_Z/0/1/0/all/0/1&quot;&gt;Zahra Ghasemi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10343">
<title>ClimaX: A foundation model for weather and climate. (arXiv:2301.10343v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10343</link>
<description rdf:parseType="Literal">&lt;p&gt;Most state-of-the-art approaches for weather and climate modeling are based
on physics-informed numerical models of the atmosphere. These approaches aim to
model the non-linear dynamics and complex interactions between multiple
variables, which are challenging to approximate. Additionally, many such
numerical models are computationally intensive, especially when modeling the
atmospheric phenomenon at a fine-grained spatial and temporal resolution.
Recent data-driven approaches based on machine learning instead aim to directly
solve a downstream forecasting or projection task by learning a data-driven
functional mapping using deep neural networks. However, these networks are
trained using curated and homogeneous climate datasets for specific
spatiotemporal tasks, and thus lack the generality of numerical models. We
develop and demonstrate ClimaX, a flexible and generalizable deep learning
model for weather and climate science that can be trained using heterogeneous
datasets spanning different variables, spatio-temporal coverage, and physical
groundings. ClimaX extends the Transformer architecture with novel encoding and
aggregation blocks that allow effective use of available compute while
maintaining general utility. ClimaX is pre-trained with a self-supervised
learning objective on climate datasets derived from CMIP6. The pre-trained
ClimaX can then be fine-tuned to address a breadth of climate and weather
tasks, including those that involve atmospheric variables and spatio-temporal
scales unseen during pretraining. Compared to existing data-driven baselines,
we show that this generality in ClimaX results in superior performance on
benchmarks for weather forecasting and climate projections, even when
pretrained at lower resolutions and compute budgets. The source code is
available at https://github.com/microsoft/ClimaX.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tung Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brandstetter_J/0/1/0/all/0/1&quot;&gt;Johannes Brandstetter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1&quot;&gt;Ashish Kapoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1&quot;&gt;Jayesh K. Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1&quot;&gt;Aditya Grover&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10405">
<title>Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v7 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10405</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently decades have witnessed the empirical success of framing Knowledge
Graph (KG) embeddings via language models. However, language model-based KG
embeddings are usually deployed as static artifacts, making them difficult to
modify post-deployment without re-training after deployment. To address this
issue, we propose a new task of editing language model-based KG embeddings in
this paper. This task is designed to facilitate rapid, data-efficient updates
to KG embeddings without compromising the performance of other aspects. We
build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and
evaluate several knowledge editing baselines demonstrating the limited ability
of previous models to handle the proposed challenging task. We further propose
a simple yet strong baseline dubbed KGEditor, which utilizes additional
parametric layers of the hypernetwork to edit/add facts. Our comprehensive
experimental results reveal that KGEditor excels in updating specific facts
without impacting the overall performance, even when faced with limited
training resources. Code and datasets are available in
https://github.com/zjunlp/PromptKG/tree/main/deltaKG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Siyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1&quot;&gt;Bozhong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qingbing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.12212">
<title>Efficient Enumeration of Markov Equivalent DAGs. (arXiv:2301.12212v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2301.12212</link>
<description rdf:parseType="Literal">&lt;p&gt;Enumerating the directed acyclic graphs (DAGs) of a Markov equivalence class
(MEC) is an important primitive in causal analysis. The central resource from
the perspective of computational complexity is the delay, that is, the time an
algorithm that lists all members of the class requires between two consecutive
outputs. Commonly used algorithms for this task utilize the rules proposed by
Meek (1995) or the transformational characterization by Chickering (1995), both
resulting in superlinear delay. In this paper, we present the first linear-time
delay algorithm. On the theoretical side, we show that our algorithm can be
generalized to enumerate DAGs represented by models that incorporate background
knowledge, such as MPDAGs; on the practical side, we provide an efficient
implementation and evaluate it in a series of experiments. Complementary to the
linear-time delay algorithm, we also provide intriguing insights into Markov
equivalence itself: All members of an MEC can be enumerated such that two
successive DAGs have structural Hamming distance at most three.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wienobst_M/0/1/0/all/0/1&quot;&gt;Marcel Wien&amp;#xf6;bst&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luttermann_M/0/1/0/all/0/1&quot;&gt;Malte Luttermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bannach_M/0/1/0/all/0/1&quot;&gt;Max Bannach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liskiewicz_M/0/1/0/all/0/1&quot;&gt;Maciej Li&amp;#x15b;kiewicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.13036">
<title>Limited Query Graph Connectivity Test. (arXiv:2302.13036v3 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2302.13036</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a combinatorial optimisation model called Limited Query Graph
Connectivity Test. We consider a graph whose edges have two possible states
(On/Off). The edges&apos; states are hidden initially. We could query an edge to
reveal its state. Given a source s and a destination t, we aim to test s-t
connectivity by identifying either a path (consisting of only On edges) or a
cut (consisting of only Off edges). We are limited to B queries, after which we
stop regardless of whether graph connectivity is established. We aim to design
a query policy that minimizes the expected number of queries.
&lt;/p&gt;
&lt;p&gt;Our model is mainly motivated by a cyber security use case where we need to
establish whether an attack path exists in a network, between a source and a
destination. Edge query is resolved by manual effort from the IT admin, which
is the motivation behind query minimization.
&lt;/p&gt;
&lt;p&gt;Our model is highly related to monotone Stochastic Boolean Function
Evaluation (SBFE). There are two existing exact algorithms for SBFE that are
prohibitively expensive. We propose a significantly more scalable exact
algorithm. While previous exact algorithms only scale for trivial graphs (i.e.,
past works experimented on at most 20 edges), we empirically demonstrate that
our algorithm is scalable for a wide range of much larger practical graphs
(i.e., Windows domain network graphs with tens of thousands of edges).
&lt;/p&gt;
&lt;p&gt;We propose three heuristics. Our best-performing heuristic is via reducing
the search horizon of the exact algorithm. The other two are via reinforcement
learning (RL) and Monte Carlo tree search (MCTS). We also derive an anytime
algorithm for computing the performance lower bound. Experimentally, we show
that all our heuristics are near optimal. The exact algorithm based heuristic
outperforms all, surpassing RL, MCTS and 8 existing heuristics ported from SBFE
and related literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1&quot;&gt;Mingyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jialiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_A/0/1/0/all/0/1&quot;&gt;Aneta Neumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_F/0/1/0/all/0/1&quot;&gt;Frank Neumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hung Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03932">
<title>FFT-based Dynamic Token Mixer for Vision. (arXiv:2303.03932v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03932</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-head-self-attention (MHSA)-equipped models have achieved notable
performance in computer vision. Their computational complexity is proportional
to quadratic numbers of pixels in input feature maps, resulting in slow
processing, especially when dealing with high-resolution images. New types of
token-mixer are proposed as an alternative to MHSA to circumvent this problem:
an FFT-based token-mixer involves global operations similar to MHSA but with
lower computational complexity. However, despite its attractive properties, the
FFT-based token-mixer has not been carefully examined in terms of its
compatibility with the rapidly evolving MetaFormer architecture. Here, we
propose a novel token-mixer called Dynamic Filter and novel image recognition
models, DFFormer and CDFFormer, to close the gaps above. The results of image
classification and downstream tasks, analysis, and visualization show that our
models are helpful. Notably, their throughput and memory efficiency when
dealing with high-resolution image recognition is remarkable. Our results
indicate that Dynamic Filter is one of the token-mixer options that should be
seriously considered. The code is available at
https://github.com/okojoalg/dfformer
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tatsunami_Y/0/1/0/all/0/1&quot;&gt;Yuki Tatsunami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taki_M/0/1/0/all/0/1&quot;&gt;Masato Taki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11098">
<title>Understanding the Role of the Projector in Knowledge Distillation. (arXiv:2303.11098v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11098</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we revisit the efficacy of knowledge distillation as a function
matching and metric learning problem. In doing so we verify three important
design decisions, namely the normalisation, soft maximum function, and
projection layers as key ingredients. We theoretically show that the projector
implicitly encodes information on past examples, enabling relational gradients
for the student. We then show that the normalisation of representations is
tightly coupled with the training dynamics of this projector, which can have a
large impact on the students performance. Finally, we show that a simple soft
maximum function can be used to address any significant capacity gap problems.
Experimental results on various benchmark datasets demonstrate that using these
insights can lead to superior or comparable performance to state-of-the-art
knowledge distillation techniques, despite being much more computationally
efficient. In particular, we obtain these results across image classification
(CIFAR100 and ImageNet), object detection (COCO2017), and on more difficult
distillation objectives, such as training data efficient transformers, whereby
we attain a 77.2% top-1 accuracy with DeiT-Ti on ImageNet. Code and models are
publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miles_R/0/1/0/all/0/1&quot;&gt;Roy Miles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1&quot;&gt;Krystian Mikolajczyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17561">
<title>SoftCLIP: Softer Cross-modal Alignment Makes CLIP Stronger. (arXiv:2303.17561v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17561</link>
<description rdf:parseType="Literal">&lt;p&gt;During the preceding biennium, vision-language pre-training has achieved
noteworthy success on several downstream tasks. Nevertheless, acquiring
high-quality image-text pairs, where the pairs are entirely exclusive of each
other, remains a challenging task, and noise exists in the commonly used
datasets. To address this issue, we propose SoftCLIP, a novel approach that
relaxes the strict one-to-one constraint and achieves a soft cross-modal
alignment by introducing a softened target, which is generated from the
fine-grained intra-modal self-similarity. The intra-modal guidance is
indicative to enable two pairs have some local similarities and model
many-to-many relationships between the two modalities. Besides, since the
positive still dominates in the softened target distribution, we disentangle
the negatives in the distribution to further boost the relation alignment with
the negatives in the cross-modal learning. Extensive experiments demonstrate
the effectiveness of SoftCLIP. In particular, on ImageNet zero-shot
classification task, using CC3M/CC12M as pre-training dataset, SoftCLIP brings
a top-1 accuracy improvement of 6.8%/7.2% over the CLIP baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yuting Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinfeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zihan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Wu Enwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Ke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xing Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05091">
<title>Knowledge-enhanced Agents for Interactive Text Games. (arXiv:2305.05091v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05091</link>
<description rdf:parseType="Literal">&lt;p&gt;Communication via natural language is a key aspect of machine intelligence,
and it requires computational models to learn and reason about world concepts,
with varying levels of supervision. Significant progress has been made on
fully-supervised non-interactive tasks, such as question-answering and
procedural text understanding. Yet, various sequential interactive tasks, as in
text-based games, have revealed limitations of existing approaches in terms of
coherence, contextual awareness, and their ability to learn effectively from
the environment. In this paper, we propose a knowledge-injection framework for
improved functional grounding of agents in text-based games. Specifically, we
consider two forms of domain knowledge that we inject into learning-based
agents: memory of previous correct actions and affordances of relevant objects
in the environment. Our framework supports two representative model classes:
reinforcement learning agents and language model agents. Furthermore, we devise
multiple injection strategies for the above domain knowledge types and agent
architectures, including injection via knowledge graphs and augmentation of the
existing input encoding strategies. We experiment with four models on the 10
tasks in the ScienceWorld text-based game environment, to illustrate the impact
of knowledge injection on various model configurations and challenging task
settings. Our findings provide crucial insights into the interplay between task
properties, model architectures, and domain knowledge for interactive contexts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chhikara_P/0/1/0/all/0/1&quot;&gt;Prateek Chhikara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiarui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1&quot;&gt;Filip Ilievski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1&quot;&gt;Jonathan Francis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1&quot;&gt;Kaixin Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08049">
<title>A Surprisingly Simple Continuous-Action POMDP Solver: Lazy Cross-Entropy Search Over Policy Trees. (arXiv:2305.08049v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08049</link>
<description rdf:parseType="Literal">&lt;p&gt;The Partially Observable Markov Decision Process (POMDP) provides a
principled framework for decision making in stochastic partially observable
environments. However, computing good solutions for problems with continuous
action spaces remains challenging. To ease this challenge, we propose a simple
online POMDP solver, called Lazy Cross-Entropy Search Over Policy Trees
(LCEOPT). At each planning step, our method uses a novel lazy Cross-Entropy
method to search the space of policy trees, which provide a simple policy
representation. Specifically, we maintain a distribution on promising
finite-horizon policy trees. The distribution is iteratively updated by
sampling policies, evaluating them via Monte Carlo simulation, and refitting
them to the top-performing ones. Our method is lazy in the sense that it
exploits the policy tree representation to avoid redundant computations in
policy sampling, evaluation, and distribution update. This leads to
computational savings of up to two orders of magnitude. Our LCEOPT is
surprisingly simple as compared to existing state-of-the-art methods, yet
empirically outperforms them on several continuous-action POMDP problems,
particularly for problems with higher-dimensional action spaces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoerger_M/0/1/0/all/0/1&quot;&gt;Marcus Hoerger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurniawati_H/0/1/0/all/0/1&quot;&gt;Hanna Kurniawati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kroese_D/0/1/0/all/0/1&quot;&gt;Dirk Kroese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_N/0/1/0/all/0/1&quot;&gt;Nan Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14196">
<title>ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding. (arXiv:2305.14196v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14196</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce ZeroSCROLLS, a zero-shot benchmark for natural language
understanding over long texts, which contains only test and small validation
sets, without training data. We adapt six tasks from the SCROLLS benchmark, and
add four new datasets, including two novel information fusing tasks, such as
aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a
comprehensive evaluation of both open-source and closed large language models,
finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest
average score. However, there is still room for improvement on multiple open
challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to
pass the naive baseline. As the state of the art is a moving target, we invite
researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaham_U/0/1/0/all/0/1&quot;&gt;Uri Shaham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ivgi_M/0/1/0/all/0/1&quot;&gt;Maor Ivgi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efrat_A/0/1/0/all/0/1&quot;&gt;Avia Efrat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1&quot;&gt;Jonathan Berant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1&quot;&gt;Omer Levy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14561">
<title>Negative Feedback Training: A Novel Concept to Improve Robustness of NVCIM DNN Accelerators. (arXiv:2305.14561v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14561</link>
<description rdf:parseType="Literal">&lt;p&gt;Compute-in-memory (CIM) accelerators built upon non-volatile memory (NVM)
devices excel in energy efficiency and latency when performing Deep Neural
Network (DNN) inference, thanks to their in-situ data processing capability.
However, the stochastic nature and intrinsic variations of NVM devices often
result in performance degradation in DNN inference. Introducing these non-ideal
device behaviors during DNN training enhances robustness, but drawbacks include
limited accuracy improvement, reduced prediction confidence, and convergence
issues. This arises from a mismatch between the deterministic training and
non-deterministic device variations, as such training, though considering
variations, relies solely on the model&apos;s final output. In this work, we draw
inspiration from the control theory and propose a novel training concept:
Negative Feedback Training (NFT) leveraging the multi-scale noisy information
captured from network. We develop two specific NFT instances, Oriented
Variational Forward (OVF) and Intermediate Representation Snapshot (IRS).
Extensive experiments show that our methods outperform existing
state-of-the-art methods with up to a 46.71% improvement in inference accuracy
while reducing epistemic uncertainty, boosting output confidence, and improving
convergence probability. Their effectiveness highlights the generality and
practicality of our NFT concept in enhancing DNN robustness against device
variations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yifan Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zheyu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1&quot;&gt;Wujie Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaobo Sharon Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yiyu Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17021">
<title>GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations. (arXiv:2305.17021v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17021</link>
<description rdf:parseType="Literal">&lt;p&gt;Counterfactual explanations have been widely studied in explainability, with
a range of application dependent methods prominent in fairness, recourse and
model understanding. The major shortcoming associated with these methods,
however, is their inability to provide explanations beyond the local or
instance-level. While many works touch upon the notion of a global explanation,
typically suggesting to aggregate masses of local explanations in the hope of
ascertaining global properties, few provide frameworks that are both reliable
and computationally tractable. Meanwhile, practitioners are requesting more
efficient and interactive explainability tools. We take this opportunity to
propose Global &amp;amp; Efficient Counterfactual Explanations (GLOBE-CE), a flexible
framework that tackles the reliability and scalability issues associated with
current state-of-the-art, particularly on higher dimensional datasets and in
the presence of continuous features. Furthermore, we provide a unique
mathematical analysis of categorical feature translations, utilising it in our
method. Experimental evaluation with publicly available datasets and user
studies demonstrate that GLOBE-CE performs significantly better than the
current state-of-the-art across multiple metrics (e.g., speed, reliability).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ley_D/0/1/0/all/0/1&quot;&gt;Dan Ley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Saumitra Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magazzeni_D/0/1/0/all/0/1&quot;&gt;Daniele Magazzeni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19972">
<title>VILAS: Exploring the Effects of Vision and Language Context in Automatic Speech Recognition. (arXiv:2305.19972v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19972</link>
<description rdf:parseType="Literal">&lt;p&gt;Enhancing automatic speech recognition (ASR) performance by leveraging
additional multimodal information has shown promising results in previous
studies. However, most of these works have primarily focused on utilizing
visual cues derived from human lip motions. In fact, context-dependent visual
and linguistic cues can also benefit in many scenarios. In this paper, we first
propose ViLaS (Vision and Language into Automatic Speech Recognition), a novel
multimodal ASR model based on the continuous integrate-and-fire (CIF)
mechanism, which can integrate visual and textual context simultaneously or
separately, to facilitate speech recognition. Next, we introduce an effective
training strategy that improves performance in modal-incomplete test scenarios.
Then, to explore the effects of integrating vision and language, we create
VSDial, a multimodal ASR dataset with multimodal context cues in both Chinese
and English versions. Finally, empirical results are reported on the public
Flickr8K and self-constructed VSDial datasets. We explore various cross-modal
fusion schemes, analyze fine-grained crossmodal alignment on VSDial, and
provide insights into the effects of integrating multimodal information on
speech recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ni_Z/0/1/0/all/0/1&quot;&gt;Ziyi Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_M/0/1/0/all/0/1&quot;&gt;Minglun Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Feilong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meng_L/0/1/0/all/0/1&quot;&gt;Linghui Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jing Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lv_P/0/1/0/all/0/1&quot;&gt;Pin Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Bo Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00249">
<title>BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned Approximations. (arXiv:2306.00249v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00249</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world planning problems, including autonomous driving and sustainable
energy applications like carbon storage and resource exploration, have recently
been modeled as partially observable Markov decision processes (POMDPs) and
solved using approximate methods. To solve high-dimensional POMDPs in practice,
state-of-the-art methods use online planning with problem-specific heuristics
to reduce planning horizons and make the problems tractable. Algorithms that
learn approximations to replace heuristics have recently found success in
large-scale fully observable domains. The key insight is the combination of
online Monte Carlo tree search with offline neural network approximations of
the optimal policy and value function. In this work, we bring this insight to
partially observed domains and propose BetaZero, a belief-state planning
algorithm for high-dimensional POMDPs. BetaZero learns offline approximations
that replace heuristics to enable online decision making in long-horizon
problems. We address several challenges inherent in large-scale partially
observable domains; namely challenges of transitioning in stochastic
environments, prioritizing action branching with a limited search budget, and
representing beliefs as input to the network. To formalize the use of all
limited search information we train against a novel Q-weighted policy vector
target. We test BetaZero on various well-established benchmark POMDPs found in
the literature and a real-world, high-dimensional problem of critical mineral
exploration. Experiments show that BetaZero outperforms state-of-the-art POMDP
solvers on a variety of tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moss_R/0/1/0/all/0/1&quot;&gt;Robert J. Moss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corso_A/0/1/0/all/0/1&quot;&gt;Anthony Corso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caers_J/0/1/0/all/0/1&quot;&gt;Jef Caers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1&quot;&gt;Mykel J. Kochenderfer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00800">
<title>FigGen: Text to Scientific Figure Generation. (arXiv:2306.00800v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00800</link>
<description rdf:parseType="Literal">&lt;p&gt;The generative modeling landscape has experienced tremendous growth in recent
years, particularly in generating natural images and art. Recent techniques
have shown impressive potential in creating complex visual compositions while
delivering impressive realism and quality. However, state-of-the-art methods
have been focusing on the narrow domain of natural images, while other
distributions remain unexplored. In this paper, we introduce the problem of
text-to-figure generation, that is creating scientific figures of papers from
text descriptions. We present FigGen, a diffusion-based approach for
text-to-figure as well as the main challenges of the proposed task. Code and
models are available at https://github.com/joanrod/figure-diffusion
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_J/0/1/0/all/0/1&quot;&gt;Juan A Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1&quot;&gt;David Vazquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1&quot;&gt;Issam Laradji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1&quot;&gt;Marco Pedersoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1&quot;&gt;Pau Rodriguez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02235">
<title>Learning Linear Causal Representations from Interventions under General Nonlinear Mixing. (arXiv:2306.02235v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02235</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of learning causal representations from unknown, latent
interventions in a general setting, where the latent distribution is Gaussian
but the mixing function is completely general. We prove strong identifiability
results given unknown single-node interventions, i.e., without having access to
the intervention targets. This generalizes prior works which have focused on
weaker classes, such as linear maps or paired counterfactual data. This is also
the first instance of causal identifiability from non-paired interventions for
deep neural network embeddings. Our proof relies on carefully uncovering the
high-dimensional geometric structure present in the data distribution after a
non-linear density transformation, which we capture by analyzing quadratic
forms of precision matrices of the latent distributions. Finally, we propose a
contrastive algorithm to identify the latent variables in practice and evaluate
its performance on various tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buchholz_S/0/1/0/all/0/1&quot;&gt;Simon Buchholz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajendran_G/0/1/0/all/0/1&quot;&gt;Goutham Rajendran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenfeld_E/0/1/0/all/0/1&quot;&gt;Elan Rosenfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aragam_B/0/1/0/all/0/1&quot;&gt;Bryon Aragam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1&quot;&gt;Pradeep Ravikumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13803">
<title>Elephants and Algorithms: A Review of the Current and Future Role of AI in Elephant Monitoring. (arXiv:2306.13803v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13803</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) and machine learning (ML) present revolutionary
opportunities to enhance our understanding of animal behavior and conservation
strategies. Using elephants, a crucial species in Africa&apos;s protected areas, as
our focal point, we delve into the role of AI and ML in their conservation.
Given the increasing amounts of data gathered from a variety of sensors like
cameras, microphones, geophones, drones, and satellites, the challenge lies in
managing and interpreting this vast data. New AI and ML techniques offer
solutions to streamline this process, helping us extract vital information that
might otherwise be overlooked. This paper focuses on the different AI-driven
monitoring methods and their potential for improving elephant conservation.
Collaborative efforts between AI experts and ecological researchers are
essential in leveraging these innovative technologies for enhanced wildlife
conservation, setting a precedent for numerous other species.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brickson_L/0/1/0/all/0/1&quot;&gt;Leandra Brickson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vollrath_F/0/1/0/all/0/1&quot;&gt;Fritz Vollrath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Titus_A/0/1/0/all/0/1&quot;&gt;Alexander J. Titus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15222">
<title>Learning to Rank in Generative Retrieval. (arXiv:2306.15222v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15222</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative retrieval stands out as a promising new paradigm in text retrieval
that aims to generate identifier strings of relevant passages as the retrieval
target. This generative paradigm taps into powerful generative language models,
distinct from traditional sparse or dense retrieval methods. However, only
learning to generate is insufficient for generative retrieval. Generative
retrieval learns to generate identifiers of relevant passages as an
intermediate goal and then converts predicted identifiers into the final
passage rank list. The disconnect between the learning objective of
autoregressive models and the desired passage ranking target leads to a
learning gap. To bridge this gap, we propose a learning-to-rank framework for
generative retrieval, dubbed LTRGR. LTRGR enables generative retrieval to learn
to rank passages directly, optimizing the autoregressive model toward the final
passage ranking target via a rank loss. This framework only requires an
additional learning-to-rank training phase to enhance current generative
retrieval systems and does not add any burden to the inference stage. We
conducted experiments on three public benchmarks, and the results demonstrate
that LTRGR achieves state-of-the-art performance among generative retrieval
methods. The code and checkpoints are released at
https://github.com/liyongqi67/LTRGR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yongqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1&quot;&gt;Nan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1&quot;&gt;Furu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenjie Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01504">
<title>All in One: Multi-task Prompting for Graph Neural Networks. (arXiv:2307.01504v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01504</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, &apos;&apos;pre-training and fine-tuning&apos;&apos; has been adopted as a standard
workflow for many graph tasks since it can take general graph knowledge to
relieve the lack of graph annotations from each application. However, graph
tasks with node level, edge level, and graph level are far diversified, making
the pre-training pretext often incompatible with these multiple tasks. This gap
may even cause a &apos;&apos;negative transfer&apos;&apos; to the specific application, leading to
poor results. Inspired by the prompt learning in natural language processing
(NLP), which has presented significant effectiveness in leveraging prior
knowledge for various NLP tasks, we study the prompting topic for graphs with
the motivation of filling the gap between pre-trained models and various graph
tasks. In this paper, we propose a novel multi-task prompting method for graph
models. Specifically, we first unify the format of graph prompts and language
prompts with the prompt token, token structure, and inserting pattern. In this
way, the prompting idea from NLP can be seamlessly introduced to the graph
area. Then, to further narrow the gap between various graph tasks and
state-of-the-art pre-training strategies, we further study the task space of
various graph applications and reformulate downstream problems to the
graph-level task. Afterward, we introduce meta-learning to efficiently learn a
better initialization for the multi-task prompt of graphs so that our prompting
framework can be more reliable and general for different tasks. We conduct
extensive experiments, results from which demonstrate the superiority of our
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiangguo Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hong Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1&quot;&gt;Jihong Guan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02507">
<title>STS-CCL: Spatial-Temporal Synchronous Contextual Contrastive Learning for Urban Traffic Forecasting. (arXiv:2307.02507v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02507</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiently capturing the complex spatiotemporal representations from
large-scale unlabeled traffic data remains to be a challenging task. In
considering of the dilemma, this work employs the advanced contrastive learning
and proposes a novel Spatial-Temporal Synchronous Contextual Contrastive
Learning (STS-CCL) model. First, we elaborate the basic and strong augmentation
methods for spatiotemporal graph data, which not only perturb the data in terms
of graph structure and temporal characteristics, but also employ a
learning-based dynamic graph view generator for adaptive augmentation. Second,
we introduce a Spatial-Temporal Synchronous Contrastive Module (STS-CM) to
simultaneously capture the decent spatial-temporal dependencies and realize
graph-level contrasting. To further discriminate node individuals in negative
filtering, a Semantic Contextual Contrastive method is designed based on
semantic features and spatial heterogeneity, achieving node-level contrastive
learning along with negative filtering. Finally, we present a hard mutual-view
contrastive training scheme and extend the classic contrastive loss to an
integrated objective function, yielding better performance. Extensive
experiments and evaluations demonstrate that building a predictor upon STS-CCL
contrastive learning model gains superior performance than existing traffic
forecasting benchmarks. The proposed STS-CCL is highly suitable for large
datasets with only a few labeled data and other spatiotemporal tasks with data
scarcity issue.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lincan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kaixiang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1&quot;&gt;Fengji Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1&quot;&gt;Jichao Bi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08506">
<title>Does Visual Pretraining Help End-to-End Reasoning?. (arXiv:2307.08506v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08506</link>
<description rdf:parseType="Literal">&lt;p&gt;We aim to investigate whether end-to-end learning of visual reasoning can be
achieved with general-purpose neural networks, with the help of visual
pretraining. A positive result would refute the common belief that explicit
visual abstraction (e.g. object detection) is essential for compositional
generalization on visual reasoning, and confirm the feasibility of a neural
network &quot;generalist&quot; to solve visual recognition and reasoning tasks. We
propose a simple and general self-supervised framework which &quot;compresses&quot; each
video frame into a small set of tokens with a transformer network, and
reconstructs the remaining frames based on the compressed temporal context. To
minimize the reconstruction loss, the network must learn a compact
representation for each image, as well as capture temporal dynamics and object
permanence from temporal context. We perform evaluation on two visual reasoning
benchmarks, CATER and ACRE. We observe that pretraining is essential to achieve
compositional generalization for end-to-end visual reasoning. Our proposed
framework outperforms traditional supervised pretraining, including image
classification and explicit object detection, by large margins.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Chen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Calvin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xingyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1&quot;&gt;Anurag Arnab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1&quot;&gt;Cordelia Schmid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13709">
<title>Neural Bradley-Terry Rating: Quantifying Properties from Comparisons. (arXiv:2307.13709v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13709</link>
<description rdf:parseType="Literal">&lt;p&gt;Many properties in the real world doesn&apos;t have metrics and can&apos;t be
numerically observed, making them difficult to learn. To deal with this
challenging problem, prior works have primarily focused on estimating those
properties by using graded human scores as the target label in the training.
Meanwhile, rating algorithms based on the Bradley-Terry model are extensively
studied to evaluate the competitiveness of players based on their match
history. In this paper, we introduce the Neural Bradley-Terry Rating (NBTR), a
novel machine learning framework designed to quantify and evaluate properties
of unknown items. Our method seamlessly integrates the Bradley-Terry model into
the neural network structure. Moreover, we generalize this architecture further
to asymmetric environments with unfairness, a condition more commonly
encountered in real-world settings. Through experimental analysis, we
demonstrate that NBTR successfully learns to quantify and estimate desired
properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujii_S/0/1/0/all/0/1&quot;&gt;Satoru Fujii&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15484">
<title>Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding. (arXiv:2307.15484v3 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15484</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, there has been a growing interest in text-to-speech (TTS) methods
that can be trained with minimal supervision by combining two types of discrete
speech representations and using two sequence-to-sequence tasks to decouple
TTS. However, existing methods suffer from three problems: the high
dimensionality and waveform distortion of discrete speech representations, the
prosodic averaging problem caused by the duration prediction model in
non-autoregressive frameworks, and the information redundancy and dimension
explosion problems of existing semantic encoding methods. To address these
problems, three progressive methods are proposed. First, we propose
Diff-LM-Speech, an autoregressive structure consisting of a language model and
diffusion models, which models the semantic embedding into the mel-spectrogram
based on a diffusion model to achieve higher audio quality. We also introduce a
prompt encoder structure based on a variational autoencoder and a prosody
bottleneck to improve prompt representation ability. Second, we propose
Tetra-Diff-Speech, a non-autoregressive structure consisting of four diffusion
model-based modules that design a duration diffusion model to achieve diverse
prosodic expressions. Finally, we propose Tri-Diff-Speech, a non-autoregressive
structure consisting of three diffusion model-based modules that verify the
non-necessity of existing semantic encoding models and achieve the best
results. Experimental results show that our proposed methods outperform
baseline methods. We provide a website with audio samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_C/0/1/0/all/0/1&quot;&gt;Chunyu Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_H/0/1/0/all/0/1&quot;&gt;Hao Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1&quot;&gt;He Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1&quot;&gt;Ruibo Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longbiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_J/0/1/0/all/0/1&quot;&gt;Jianwu Dang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15494">
<title>ETHER: Aligning Emergent Communication for Hindsight Experience Replay. (arXiv:2307.15494v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15494</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural language instruction following is paramount to enable collaboration
between artificial agents and human beings. Natural language-conditioned
reinforcement learning (RL) agents have shown how natural languages&apos;
properties, such as compositionality, can provide a strong inductive bias to
learn complex policies. Previous architectures like HIGhER combine the benefit
of language-conditioning with Hindsight Experience Replay (HER) to deal with
sparse rewards environments. Yet, like HER, HIGhER relies on an oracle
predicate function to provide a feedback signal highlighting which linguistic
description is valid for which state. This reliance on an oracle limits its
application. Additionally, HIGhER only leverages the linguistic information
contained in successful RL trajectories, thus hurting its final performance and
data-efficiency. Without early successful trajectories, HIGhER is no better
than DQN upon which it is built. In this paper, we propose the Emergent Textual
Hindsight Experience Replay (ETHER) agent, which builds on HIGhER and addresses
both of its limitations by means of (i) a discriminative visual referential
game, commonly studied in the subfield of Emergent Communication (EC), used
here as an unsupervised auxiliary task and (ii) a semantic grounding scheme to
align the emergent language with the natural language of the
instruction-following benchmark. We show that the referential game&apos;s agents
make an artificial language emerge that is aligned with the natural-like
language used to describe goals in the BabyAI benchmark and that it is
expressive enough so as to also describe unsuccessful RL trajectories and thus
provide feedback to the RL agent to leverage the linguistic, structured
information contained in all trajectories. Our work shows that EC is a viable
unsupervised auxiliary task for RL and provides missing pieces to make HER more
widely applicable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denamganai_K/0/1/0/all/0/1&quot;&gt;Kevin Denamgana&amp;#xef;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_D/0/1/0/all/0/1&quot;&gt;Daniel Hernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vardal_O/0/1/0/all/0/1&quot;&gt;Ozan Vardal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Missaoui_S/0/1/0/all/0/1&quot;&gt;Sondess Missaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1&quot;&gt;James Alfred Walker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04586">
<title>Bootstrapping Developmental AIs: From Simple Competences to Intelligent Human-Compatible AIs. (arXiv:2308.04586v13 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04586</link>
<description rdf:parseType="Literal">&lt;p&gt;Mainstream approaches for creating AIs include deep learning and generative
approaches (e.g., large language models) and manually constructed symbolic
approaches. These approaches have led to valuable AI systems and impressive
feats, but they can create risks when their operations affect people. Manually
constructed AIs are brittle even in circumscribed domains. Generative AIs make
strange mistakes and do not notice them. These AIs cannot be instructed easily,
fail to use common sense, and lack curiosity. They lack social alignment.
Developmental AI is a bootstrapping approach that uses embodied AIs. The AIs
start with innate competences and learn by interacting with their environment.
The AIs develop abilities in small steps along a bio-inspired trajectory.
Developmental AIs have shown capabilities for multimodal perception, object
recognition, and manipulation. Powerful computational models for hierarchical
planning, abstraction discovery, curiosity, and language acquisition exist but
need to be adapted to an embodied approach. This research aims to produce AIs
that learn to communicate, establish common ground, read critically, consider
the provenance of information, test hypotheses, and collaborate. However,
developmental AI systems have not yet passed the abilities of young children.
They need to bridge competence gaps involving nonverbal communication, speech,
reading, and writing. Scaling to practical applications also requires reducing
hardware costs. This position paper lays out prospects, gaps, and challenges
for this approach. The ambition is to create data-rich experientially based
foundation models for human-compatible, resilient, and trustworthy AIs. The AIs
would learn, share what they learn, and collaborate to achieve high standards.
The approach would make AI technology more democratic and enable more people to
train, test, build on, and replicate AIs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stefik_M/0/1/0/all/0/1&quot;&gt;Mark Stefik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Price_R/0/1/0/all/0/1&quot;&gt;Robert Price&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06764">
<title>Few-shot Class-incremental Learning: A Survey. (arXiv:2308.06764v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06764</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot Class-Incremental Learning (FSCIL) presents a unique challenge in
Machine Learning (ML), as it necessitates the Incremental Learning (IL) of new
classes from sparsely labeled training samples without forgetting previous
knowledge. While this field has seen recent progress, it remains an active
exploration area. This paper aims to provide a comprehensive and systematic
review of FSCIL. In our in-depth examination, we delve into various facets of
FSCIL, encompassing the problem definition, the discussion of the primary
challenges of unreliable empirical risk minimization and the
stability-plasticity dilemma, general schemes, and relevant problems of IL and
Few-shot Learning (FSL). Besides, we offer an overview of benchmark datasets
and evaluation metrics. Furthermore, we introduce the Few-shot
Class-incremental Classification (FSCIC) methods from data-based,
structure-based, and optimization-based approaches and the Few-shot
Class-incremental Object Detection (FSCIOD) methods from anchor-free and
anchor-based approaches. Beyond these, we present several promising research
directions within FSCIL that merit further investigation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jinghua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Li Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silven_O/0/1/0/all/0/1&quot;&gt;Olli Silv&amp;#xe9;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pietikainen_M/0/1/0/all/0/1&quot;&gt;Matti Pietik&amp;#xe4;inen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dewen Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06931">
<title>FusionPlanner: A Multi-task Motion Planner for Mining Trucks via Multi-sensor Fusion. (arXiv:2308.06931v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06931</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, significant achievements have been made in motion planning
for intelligent vehicles. However, as a typical unstructured environment,
open-pit mining attracts limited attention due to its complex operational
conditions and adverse environmental factors. A comprehensive paradigm for
unmanned transportation in open-pit mines is proposed in this research.
Firstly, we propose a multi-task motion planning algorithm, called
FusionPlanner, for autonomous mining trucks by the multi-sensor fusion method
to adapt both lateral and longitudinal control tasks for unmanned
transportation. Then, we develop a novel benchmark called MiningNav, which
offers three validation approaches to evaluate the trustworthiness and
robustness of well-trained algorithms in transportation roads of open-pit
mines. Finally, we introduce the Parallel Mining Simulator (PMS), a new
high-fidelity simulator specifically designed for open-pit mining scenarios.
PMS enables the users to manage and control open-pit mine transportation from
both the single-truck control and multi-truck scheduling perspectives. The
performance of FusionPlanner is tested by MiningNav in PMS, and the empirical
results demonstrate a significant reduction in the number of collisions and
takeovers of our planner. We anticipate our unmanned transportation paradigm
will bring mining trucks one step closer to trustworthiness and robustness in
continuous round-the-clock unmanned transportation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1&quot;&gt;Siyu Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Luxi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuchen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xuemin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lingxi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ai_Y/0/1/0/all/0/1&quot;&gt;Yunfeng Ai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Long Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08536">
<title>Can Transformers Learn Optimal Filtering for Unknown Systems?. (arXiv:2308.08536v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08536</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer models have shown great success in natural language processing;
however, their potential remains mostly unexplored for dynamical systems. In
this work, we investigate the optimal output estimation problem using
transformers, which generate output predictions using all the past ones.
Particularly, we train the transformer using various distinct systems and then
evaluate the performance on unseen systems with unknown dynamics. Empirically,
the trained transformer adapts exceedingly well to different unseen systems and
even matches the optimal performance given by the Kalman filter for linear
systems. In more complex settings with non-i.i.d. noise, time-varying dynamics,
and nonlinear dynamics like a quadrotor system with unknown parameters,
transformers also demonstrate promising results. To support our experimental
findings, we provide statistical guarantees that quantify the amount of
training data required for the transformer to achieve a desired excess risk.
Finally, we point out some limitations by identifying two classes of problems
that lead to degraded performance, highlighting the need for caution when using
transformers for control and estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Balim_H/0/1/0/all/0/1&quot;&gt;Haldun Balim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Du_Z/0/1/0/all/0/1&quot;&gt;Zhe Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oymak_S/0/1/0/all/0/1&quot;&gt;Samet Oymak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ozay_N/0/1/0/all/0/1&quot;&gt;Necmiye Ozay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09437">
<title>From Hope to Safety: Unlearning Biases of Deep Models via Gradient Penalization in Latent Space. (arXiv:2308.09437v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09437</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks are prone to learning spurious correlations embedded in
the training data, leading to potentially biased predictions. This poses risks
when deploying these models for high-stake decision-making, such as in medical
applications. Current methods for post-hoc model correction either require
input-level annotations which are only possible for spatially localized biases,
or augment the latent feature space, thereby hoping to enforce the right
reasons. We present a novel method for model correction on the concept level
that explicitly reduces model sensitivity towards biases via gradient
penalization. When modeling biases via Concept Activation Vectors, we highlight
the importance of choosing robust directions, as traditional regression-based
approaches such as Support Vector Machines tend to result in diverging
directions. We effectively mitigate biases in controlled and real-world
settings on the ISIC, Bone Age, ImageNet and CelebA datasets using VGG, ResNet
and EfficientNet architectures. Code is available on
https://github.com/frederikpahde/rrclarc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1&quot;&gt;Maximilian Dreyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pahde_F/0/1/0/all/0/1&quot;&gt;Frederik Pahde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anders_C/0/1/0/all/0/1&quot;&gt;Christopher J. Anders&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1&quot;&gt;Wojciech Samek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1&quot;&gt;Sebastian Lapuschkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09720">
<title>On the Unexpected Abilities of Large Language Models. (arXiv:2308.09720v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09720</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) are capable of displaying a wide range of
abilities that are not directly connected with the task for which they are
trained: predicting the next words of human-written texts. In this article, I
review recent research investigating the cognitive abilities developed by LLMs
and their relation to human cognition. I discuss the nature of the indirect
process that leads to the acquisition of these cognitive abilities, their
relation to other indirect processes, and the implications for the acquisition
of integrated abilities. Moreover, I propose the factors that enable the
development of abilities that are related only very indirectly to the proximal
objective of the training task. Finally, I discuss whether the full set of
capabilities that LLMs could possibly develop is predictable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nolfi_S/0/1/0/all/0/1&quot;&gt;Stefano Nolfi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09936">
<title>BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09936</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Language Models (VLMs), which extend Large Language Models (LLM) by
incorporating visual understanding capability, have demonstrated significant
advancements in addressing open-ended visual question-answering (VQA) tasks.
However, these models cannot accurately interpret images infused with text, a
common occurrence in real-world scenarios. Standard procedures for extracting
information from images often involve learning a fixed set of query embeddings.
These embeddings are designed to encapsulate image contexts and are later used
as soft prompt inputs in LLMs. Yet, this process is limited to the token count,
potentially curtailing the recognition of scenes with text-rich context. To
improve upon them, the present study introduces BLIVA: an augmented version of
InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings
from InstructBLIP and also directly projects encoded patch embeddings into the
LLM, a technique inspired by LLaVA. This approach assists the model to capture
intricate details potentially missed during the query decoding process.
Empirical evidence demonstrates that our model, BLIVA, significantly enhances
performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA
benchmark) and in undertaking general (not particularly text-rich) VQA
benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), and achieved
17.72% overall improvement in a comprehensive multimodal LLM benchmark (MME),
comparing to our baseline InstructBLIP. BLIVA demonstrates significant
capability in decoding real-world images, irrespective of text presence. To
demonstrate the broad industry applications enabled by BLIVA, we evaluate the
model using a new dataset comprising YouTube thumbnails paired with
question-answer sets across 11 diverse categories. Our code and models are
freely accessible at https://github.com/mlpc-ucsd/BLIVA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenbo Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yifan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weiyue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zeyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhuowen Tu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10144">
<title>ExpeL: LLM Agents Are Experiential Learners. (arXiv:2308.10144v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10144</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent surge in research interest in applying large language models
(LLMs) to decision-making tasks has flourished by leveraging the extensive
world knowledge embedded in LLMs. While there is a growing demand to tailor
LLMs for custom decision-making tasks, finetuning them for specific tasks is
resource-intensive and may diminish the model&apos;s generalization capabilities.
Moreover, state-of-the-art language models like GPT-4 and Claude are primarily
accessible through API calls, with their parametric weights remaining
proprietary and unavailable to the public. This scenario emphasizes the growing
need for new methodologies that allow learning from agent experiences without
requiring parametric updates. To address these problems, we introduce the
Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences
and extracts knowledge using natural language from a collection of training
tasks. At inference, the agent recalls its extracted insights and past
experiences to make informed decisions. Our empirical results highlight the
robust learning efficacy of the ExpeL agent, indicating a consistent
enhancement in its performance as it accumulates experiences. We further
explore the emerging capabilities and transfer learning potential of the ExpeL
agent through qualitative observations and additional experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_A/0/1/0/all/0/1&quot;&gt;Andrew Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Daniel Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Quentin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Matthieu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong-Jin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Gao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10997">
<title>MarkovGen: Structured Prediction for Efficient Text-to-Image Generation. (arXiv:2308.10997v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10997</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern text-to-image generation models produce high-quality images that are
both photorealistic and faithful to the text prompts. However, this quality
comes at significant computational cost: nearly all of these models are
iterative and require running sampling multiple times with large models. This
iterative process is needed to ensure that different regions of the image are
not only aligned with the text prompt, but also compatible with each other. In
this work, we propose a light-weight approach to achieving this compatibility
between different regions of an image, using a Markov Random Field (MRF) model.
We demonstrate the effectiveness of this method on top of the latent
token-based Muse text-to-image model. The MRF richly encodes the compatibility
among image tokens at different spatial locations to improve quality and
significantly reduce the required number of Muse sampling steps. Inference with
the MRF is significantly cheaper, and its parameters can be quickly learned
through back-propagation by modeling MRF inference as a differentiable
neural-network layer. Our full model, MarkovGen, uses this proposed MRF model
to both speed up Muse by 1.5X and produce higher quality images by decreasing
undesirable image artifacts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayasumana_S/0/1/0/all/0/1&quot;&gt;Sadeep Jayasumana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glasner_D/0/1/0/all/0/1&quot;&gt;Daniel Glasner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1&quot;&gt;Srikumar Ramalingam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1&quot;&gt;Andreas Veit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1&quot;&gt;Ayan Chakrabarti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sanjiv Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11730">
<title>Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11730</link>
<description rdf:parseType="Literal">&lt;p&gt;The `pre-train, prompt, predict&apos; paradigm of large language models (LLMs) has
achieved remarkable success in open-domain question answering (OD-QA). However,
few works explore this paradigm in the scenario of multi-document question
answering (MD-QA), a task demanding a thorough understanding of the logical
associations among the contents and structures of different documents. To fill
this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to
formulate the right context in prompting LLMs for MD-QA, which consists of a
graph construction module and a graph traversal module. For graph construction,
we create a knowledge graph (KG) over multiple documents with nodes symbolizing
passages or document structures (e.g., pages/tables), and edges denoting the
semantic/lexical similarity between passages or intra-document structural
relations. For graph traversal, we design an LLM-based graph traversal agent
that navigates across nodes and gathers supporting passages assisting LLMs in
MD-QA. The constructed graph serves as the global ruler that regulates the
transitional space among passages and reduces retrieval latency. Concurrently,
the graph traversal agent acts as a local navigator that gathers pertinent
context to progressively approach the question and guarantee retrieval quality.
Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the
potential of leveraging graphs in enhancing the prompt design for LLMs. Our
code: https://github.com/YuWVandy/KG-LLM-MDQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1&quot;&gt;Nedim Lipka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1&quot;&gt;Ryan A. Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siu_A/0/1/0/all/0/1&quot;&gt;Alexa Siu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruiyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derr_T/0/1/0/all/0/1&quot;&gt;Tyler Derr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11978">
<title>Will More Expressive Graph Neural Networks do Better on Generative Tasks?. (arXiv:2308.11978v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11978</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph generation poses a significant challenge as it involves predicting a
complete graph with multiple nodes and edges based on simply a given label.
This task also carries fundamental importance to numerous real-world
applications, including de-novo drug and molecular design. In recent years,
several successful methods have emerged in the field of graph generation.
However, these approaches suffer from two significant shortcomings: (1) the
underlying Graph Neural Network (GNN) architectures used in these methods are
often underexplored; and (2) these methods are often evaluated on only a
limited number of metrics. To fill this gap, we investigate the expressiveness
of GNNs under the context of the molecular graph generation task, by replacing
the underlying GNNs of graph generative models with more expressive GNNs.
Specifically, we analyse the performance of six GNNs on six different molecular
generative objectives on the ZINC-250k dataset in two different generative
frameworks: autoregressive generation models, such as GCPN and GraphAF, and
one-shot generation models, such as GraphEBM. Through our extensive
experiments, we demonstrate that advanced GNNs can indeed improve the
performance of GCPN, GraphAF, and GraphEBM on molecular generation tasks, but
GNN expressiveness is not a necessary condition for a good GNN-based generative
model. Moreover, we show that GCPN and GraphAF with advanced GNNs can achieve
state-of-the-art results across 17 other non-GNN-based graph generative
approaches, such as variational autoencoders and Bayesian optimisation models,
on the proposed molecular generative objectives (DRD2, Median1, Median2), which
are important metrics for de-novo molecular design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xiandong Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1&quot;&gt;Pietro Li&amp;#xf2;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yiren Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15452">
<title>When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v6 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15452</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of embodied artificial intelligence, the reasoning capabilities
of Large Language Models (LLMs) play a pivotal role. Although there are
effective methods like program-of-thought prompting for LLMs which uses
programming language to tackle complex reasoning tasks, the specific impact of
code data on the improvement of reasoning capabilities remains under-explored.
To address this gap, we propose complexity-impacted reasoning score (CIRS),
which combines structural and logical attributes, to measure the correlation
between code and reasoning abilities. Specifically, we use the abstract syntax
tree to encode the structural information and calculate logical complexity by
considering the difficulty and the cyclomatic complexity. Through an empirical
analysis, we find not all code data of complexity can be learned or understood
by LLMs. Optimal level of complexity is critical to the improvement of
reasoning abilities by program-aided prompting. Then we design an
auto-synthesizing and stratifying algorithm, and apply it to instruction
generation for mathematical reasoning and code data filtering for code
generation tasks. Extensive results demonstrates the effectiveness of our
proposed approach. Code will be integrated into the EasyInstruct framework at
https://github.com/zjunlp/EasyInstruct.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1&quot;&gt;Zhen Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yinuo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Shumin Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1&quot;&gt;Guozhou Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00424">
<title>Learning Speech Representation From Contrastive Token-Acoustic Pretraining. (arXiv:2309.00424v5 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00424</link>
<description rdf:parseType="Literal">&lt;p&gt;For fine-grained generation and recognition tasks such as
minimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic
speech recognition (ASR), the intermediate representations extracted from
speech should serve as a &quot;bridge&quot; between text and acoustic information,
containing information from both modalities. The semantic content is
emphasized, while the paralinguistic information such as speaker identity and
acoustic details should be de-emphasized. However, existing methods for
extracting fine-grained intermediate representations from speech suffer from
issues of excessive redundancy and dimension explosion. Contrastive learning is
a good method for modeling intermediate representations from two modalities.
However, existing contrastive learning methods in the audio field focus on
extracting global descriptive information for downstream audio classification
tasks, making them unsuitable for TTS, VC, and ASR tasks. To address these
issues, we propose a method named &quot;Contrastive Token-Acoustic Pretraining
(CTAP)&quot;, which uses two encoders to bring phoneme and speech into a joint
multimodal space, learning how to connect phoneme and speech at the frame
level. The CTAP model is trained on 210k speech and phoneme pairs, achieving
minimally-supervised TTS, VC, and ASR. The proposed CTAP method offers a
promising solution for fine-grained generation and recognition downstream tasks
in speech processing. We provide a website with audio samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qiang_C/0/1/0/all/0/1&quot;&gt;Chunyu Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yixin Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fu_R/0/1/0/all/0/1&quot;&gt;Ruibo Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longbiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dang_J/0/1/0/all/0/1&quot;&gt;Jianwu Dang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05109">
<title>Deep Learning-Aided Subspace-Based DOA Recovery for Sparse Arrays. (arXiv:2309.05109v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05109</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse arrays enable resolving more direction of arrivals (DoAs) than antenna
elements using non-uniform arrays. This is typically achieved by reconstructing
the covariance of a virtual large uniform linear array (ULA), which is then
processed by subspace DoA estimators. However, these method assume that the
signals are non-coherent and the array is calibrated; the latter often
challenging to achieve in sparse arrays, where one cannot access the virtual
array elements. In this work, we propose Sparse-SubspaceNet, which leverages
deep learning to enable subspace-based DoA recovery from sparse miscallibrated
arrays with coherent sources. Sparse- SubspaceNet utilizes a dedicated deep
network to learn from data how to compute a surrogate virtual array covariance
that is divisible into distinguishable subspaces. By doing so, we learn to cope
with coherent sources and miscalibrated sparse arrays, while preserving the
interpretability and the suitability of model-based subspace DoA estimators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Amiel_Y/0/1/0/all/0/1&quot;&gt;Yoav Amiel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shmuel_D/0/1/0/all/0/1&quot;&gt;Dor H. Shmuel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shlezinger_N/0/1/0/all/0/1&quot;&gt;Nir Shlezinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huleihel_W/0/1/0/all/0/1&quot;&gt;Wasim Huleihel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05173">
<title>DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05173</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt tuning (PT), where a small amount of trainable soft (continuous)
prompt vectors is affixed to the input of language models (LM), has shown
promising results across various tasks and models for parameter-efficient
fine-tuning (PEFT). PT stands out from other PEFT approaches because it
maintains competitive performance with fewer trainable parameters and does not
drastically scale up its parameters as the model size expands. However, PT
introduces additional soft prompt tokens, leading to longer input sequences,
which significantly impacts training and inference time and memory usage due to
the Transformer&apos;s quadratic complexity. Particularly concerning for Large
Language Models (LLMs) that face heavy daily querying. To address this issue,
we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt
into a shorter soft prompt and a pair of low-rank matrices that are then
optimised with two different learning rates. This allows DePT to achieve better
performance while saving over 20% memory and time costs compared to vanilla PT
and its variants, without changing trainable parameter sizes. Through extensive
experiments on 23 natural language processing (NLP) and vision-language (VL)
tasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches,
including the full fine-tuning baseline in some scenarios. Additionally, we
empirically show that DEPT grows more efficient as the model size increases.
Our further study reveals that DePT integrates seamlessly with
parameter-efficient transfer learning in the few-shot learning setting and
highlights its adaptability to various model architectures and sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhengxiang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1&quot;&gt;Aldo Lipani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06810">
<title>Leveraging SE(3) Equivariance for Learning 3D Geometric Shape Assembly. (arXiv:2309.06810v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06810</link>
<description rdf:parseType="Literal">&lt;p&gt;Shape assembly aims to reassemble parts (or fragments) into a complete
object, which is a common task in our daily life. Different from the semantic
part assembly (e.g., assembling a chair&apos;s semantic parts like legs into a whole
chair), geometric part assembly (e.g., assembling bowl fragments into a
complete bowl) is an emerging task in computer vision and robotics. Instead of
semantic information, this task focuses on geometric information of parts. As
the both geometric and pose space of fractured parts are exceptionally large,
shape pose disentanglement of part representations is beneficial to geometric
shape assembly. In our paper, we propose to leverage SE(3) equivariance for
such shape pose disentanglement. Moreover, while previous works in vision and
robotics only consider SE(3) equivariance for the representations of single
objects, we move a step forward and propose leveraging SE(3) equivariance for
representations considering multi-part correlations, which further boosts the
performance of the multi-part assembly. Experiments demonstrate the
significance of SE(3) equivariance and our proposed method for geometric shape
assembly. Project page: https://crtie.github.io/SE-3-part-assembly/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruihai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tie_C/0/1/0/all/0/1&quot;&gt;Chenrui Tie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yushi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06961">
<title>Towards Reliable Dermatology Evaluation Benchmarks. (arXiv:2309.06961v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06961</link>
<description rdf:parseType="Literal">&lt;p&gt;Benchmark datasets for digital dermatology unwittingly contain inaccuracies
that reduce trust in model performance estimates. We propose a
resource-efficient data-cleaning protocol to identify issues that escaped
previous curation. The protocol leverages an existing algorithmic cleaning
strategy and is followed by a confirmation process terminated by an intuitive
stopping criterion. Based on confirmation by multiple dermatologists, we remove
irrelevant samples and near duplicates and estimate the percentage of label
errors in six dermatology image datasets for model evaluation promoted by the
International Skin Imaging Collaboration. Along with this paper, we publish
revised file lists for each dataset which should be used for model evaluation.
Our work paves the way for more trustworthy performance assessment in digital
dermatology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groger_F/0/1/0/all/0/1&quot;&gt;Fabian Gr&amp;#xf6;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lionetti_S/0/1/0/all/0/1&quot;&gt;Simone Lionetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottfrois_P/0/1/0/all/0/1&quot;&gt;Philippe Gottfrois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_Jimenez_A/0/1/0/all/0/1&quot;&gt;Alvaro Gonzalez-Jimenez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groh_M/0/1/0/all/0/1&quot;&gt;Matthew Groh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daneshjou_R/0/1/0/all/0/1&quot;&gt;Roxana Daneshjou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Consortium_L/0/1/0/all/0/1&quot;&gt;Labelling Consortium&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navarini_A/0/1/0/all/0/1&quot;&gt;Alexander A. Navarini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pouly_M/0/1/0/all/0/1&quot;&gt;Marc Pouly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.10639">
<title>Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers. (arXiv:2309.10639v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.10639</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we provide a geometric interpretation of the structure of Deep
Learning (DL) networks, characterized by $L$ hidden layers, a ReLU ramp
activation function, an $\mathcal{L}^2$ Schatten class (or Hilbert-Schmidt)
cost function, and input and output spaces $\mathbb{R}^Q$ with equal dimension
$Q\geq1$. The hidden layers are also defined on $\mathbb{R}^{Q}$; the training
input size $N$ can be arbitrarily large - thus, we are considering the
underparametrized regime. We apply our recent results on shallow neural
networks to construct an explicit family of minimizers for the global minimum
of the cost function in the case $L\geq Q$, which we show to be degenerate. In
the context presented here, the hidden layers of the DL network &quot;curate&quot; the
training inputs by recursive application of a truncation map that minimizes the
noise to signal ratio of the training inputs. Moreover, we determine a set of
$2^Q-1$ distinct degenerate local minima of the cost function. Our
constructions make no use of gradient descent algorithms at all.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Thomas Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ewald_P/0/1/0/all/0/1&quot;&gt;Patricia Mu&amp;#xf1;oz Ewald&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13782">
<title>On the Computational Benefit of Multimodal Learning. (arXiv:2309.13782v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13782</link>
<description rdf:parseType="Literal">&lt;p&gt;Human perception inherently operates in a multimodal manner. Similarly, as
machines interpret the empirical world, their learning processes ought to be
multimodal. The recent, remarkable successes in empirical multimodal learning
underscore the significance of understanding this paradigm. Yet, a solid
theoretical foundation for multimodal learning has eluded the field for some
time. While a recent study by Lu (2023) has shown the superior sample
complexity of multimodal learning compared to its unimodal counterpart, another
basic question remains: does multimodal learning also offer computational
advantages over unimodal learning? This work initiates a study on the
computational benefit of multimodal learning. We demonstrate that, under
certain conditions, multimodal learning can outpace unimodal learning
exponentially in terms of computation. Specifically, we present a learning task
that is NP-hard for unimodal learning but is solvable in polynomial time by a
multimodal algorithm. Our construction is based on a novel modification to the
intersection of two half-spaces problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhou Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14950">
<title>Multi-Source Domain Adaptation for Object Detection with Prototype-based Mean-teacher. (arXiv:2309.14950v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14950</link>
<description rdf:parseType="Literal">&lt;p&gt;Adapting visual object detectors to operational target domains is a
challenging task, commonly achieved using unsupervised domain adaptation (UDA)
methods. Recent studies have shown that when the labeled dataset comes from
multiple source domains, treating them as separate domains and performing a
multi-source domain adaptation (MSDA) improves the accuracy and robustness over
blending these source domains and performing a UDA. For adaptation, existing
MSDA methods learn domain-invariant and domain-specific parameters (for each
source domain). However, unlike single-source UDA methods, learning
domain-specific parameters makes them grow significantly in proportion to the
number of source domains. This paper proposes a novel MSDA method called
Prototype-based Mean Teacher (PMT), which uses class prototypes instead of
domain-specific subnets to encode domain-specific information. These prototypes
are learned using a contrastive loss, aligning the same categories across
domains and separating different categories far apart. Given the use of
prototypes, the number of parameters required for our PMT method does not
increase significantly with the number of source domains, thus reducing memory
issues and possible overfitting. Empirical studies indicate that PMT
outperforms state-of-the-art MSDA methods on several challenging object
detection datasets. Our code is available at
https://github.com/imatif17/Prototype-Mean-Teacher.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belal_A/0/1/0/all/0/1&quot;&gt;Atif Belal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meethal_A/0/1/0/all/0/1&quot;&gt;Akhil Meethal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_F/0/1/0/all/0/1&quot;&gt;Francisco Perdigon Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1&quot;&gt;Marco Pedersoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1&quot;&gt;Eric Granger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15293">
<title>Maximum diffusion reinforcement learning. (arXiv:2309.15293v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15293</link>
<description rdf:parseType="Literal">&lt;p&gt;The assumption that data are independent and identically distributed
underpins all machine learning. When data are collected sequentially from agent
experiences this assumption does not generally hold, as in reinforcement
learning. Here, we derive a method that overcomes these limitations by
exploiting the statistical mechanics of ergodic processes, which we term
maximum diffusion reinforcement learning. By decorrelating agent experiences,
our approach provably enables single-shot learning in continuous deployments
over the course of individual task attempts. Moreover, we prove our approach
generalizes well-known maximum entropy techniques, and robustly exceeds
state-of-the-art performance across popular benchmarks. Our results at the
nexus of physics, learning, and control pave the way towards more transparent
and reliable decision-making in reinforcement learning agents, such as
locomoting robots and self-driving cars.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berrueta_T/0/1/0/all/0/1&quot;&gt;Thomas A. Berrueta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinosky_A/0/1/0/all/0/1&quot;&gt;Allison Pinosky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphey_T/0/1/0/all/0/1&quot;&gt;Todd D. Murphey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15512">
<title>High-Fidelity Speech Synthesis with Minimal Supervision: All Using Diffusion Models. (arXiv:2309.15512v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15512</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-speech (TTS) methods have shown promising results in voice cloning,
but they require a large number of labeled text-speech pairs.
Minimally-supervised speech synthesis decouples TTS by combining two types of
discrete speech representations(semantic \&amp;amp; acoustic) and using two
sequence-to-sequence tasks to enable training with minimal supervision.
However, existing methods suffer from information redundancy and dimension
explosion in semantic representation, and high-frequency waveform distortion in
discrete acoustic representation. Autoregressive frameworks exhibit typical
instability and uncontrollability issues. And non-autoregressive frameworks
suffer from prosodic averaging caused by duration prediction models. To address
these issues, we propose a minimally-supervised high-fidelity speech synthesis
method, where all modules are constructed based on the diffusion models. The
non-autoregressive framework enhances controllability, and the duration
diffusion model enables diversified prosodic expression. Contrastive
Token-Acoustic Pretraining (CTAP) is used as an intermediate semantic
representation to solve the problems of information redundancy and dimension
explosion in existing semantic coding methods. Mel-spectrogram is used as the
acoustic representation. Both semantic and acoustic representations are
predicted by continuous variable regression tasks to solve the problem of
high-frequency fine-grained waveform distortion. Experimental results show that
our proposed method outperforms the baseline method. We provide audio samples
on our website.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_C/0/1/0/all/0/1&quot;&gt;Chunyu Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yixin Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ying Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longbiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_J/0/1/0/all/0/1&quot;&gt;Jianwu Dang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17176">
<title>AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback. (arXiv:2309.17176v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17176</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have demonstrated significant success across
various domains. However, their application in complex decision-making tasks
frequently necessitates intricate prompt engineering or fine-tuning, leading to
challenges in unseen downstream tasks and heavy demands on computational
resources. Meanwhile, Reinforcement Learning (RL) has been recognized as
effective in decision-making problems but struggles in environments with sparse
rewards, such as open-world games. To overcome these challenges, we introduce
AdaRefiner, a novel framework designed to enhance the synergy between LLMs and
RL feedback. The key component of AdaRefiner is a lightweight Adapter Language
Model (LM), which automatically refines task comprehension based on feedback
from RL agents. This method mitigates the need for intricate prompt engineering
and intensive LLM fine-tuning while maintaining the LLMs&apos; generalization
abilities and enhancing their decision-making capabilities in downstream tasks.
Empirical evaluations of AdaRefiner on 22 diverse tasks within the open-world
game Crafter have demonstrated its superior effectiveness, especially in
guiding agents towards higher-level and common-sense skills. Our work makes
contributions to the automatic self-refinement of LLMs with RL feedback,
offering a more adaptable and efficient solution for complex decision-making
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wanpeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zongqing Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02161">
<title>Selenite: Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models. (arXiv:2310.02161v3 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02161</link>
<description rdf:parseType="Literal">&lt;p&gt;Sensemaking in unfamiliar domains can be challenging, demanding considerable
user effort to compare different options with respect to various criteria.
Prior research and our formative study found that people would benefit from
reading an overview of an information space upfront, including the criteria
others previously found useful. However, existing sensemaking tools struggle
with the &quot;cold-start&quot; problem -- not only requiring significant input from
previous users to generate and share these overviews, but also that such
overviews may turn out to be biased and incomplete. In this work, we introduce
a novel system, Selenite, which leverages LLMs as reasoning machines and
knowledge retrievers to automatically produce a comprehensive overview of
options and criteria to jumpstart users&apos; sensemaking processes. Subsequently,
Selenite also adapts as people use it, helping users find, read, and navigate
unfamiliar information in a systematic yet personalized manner. Through three
studies, we found that Selenite produced accurate and high-quality overviews
reliably, significantly accelerated users&apos; information processing, and
effectively improved their overall comprehension and sensemaking experience.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Michael Xieyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tongshuang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianying Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Franklin Mingzhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kittur_A/0/1/0/all/0/1&quot;&gt;Aniket Kittur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myers_B/0/1/0/all/0/1&quot;&gt;Brad A. Myers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02601">
<title>MagicDrive: Street View Generation with Diverse 3D Geometry Control. (arXiv:2310.02601v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02601</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in diffusion models have significantly enhanced the data
synthesis with 2D control. Yet, precise 3D control in street view generation,
crucial for 3D perception tasks, remains elusive. Specifically, utilizing
Bird&apos;s-Eye View (BEV) as the primary condition often leads to challenges in
geometry control (e.g., height), affecting the representation of object shapes,
occlusion patterns, and road surface elevations, all of which are essential to
perception data synthesis, especially for 3D object detection tasks. In this
paper, we introduce MagicDrive, a novel street view generation framework
offering diverse 3D geometry controls, including camera poses, road maps, and
3D bounding boxes, together with textual descriptions, achieved through
tailored encoding strategies. Besides, our design incorporates a cross-view
attention module, ensuring consistency across multiple camera views. With
MagicDrive, we achieve high-fidelity street-view synthesis that captures
nuanced 3D geometry and various scene descriptions, enhancing tasks like BEV
segmentation and 3D object detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Ruiyuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1&quot;&gt;Lanqing Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1&quot;&gt;Dit-Yan Yeung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03059">
<title>Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03059</link>
<description rdf:parseType="Literal">&lt;p&gt;The popularity of pre-trained large models has revolutionized downstream
tasks across diverse fields, such as language, vision, and multi-modality. To
minimize the adaption cost for downstream tasks, many Parameter-Efficient
Fine-Tuning (PEFT) techniques are proposed for language and 2D image
pre-trained models. However, the specialized PEFT method for 3D pre-trained
models is still under-explored. To this end, we introduce Point-PEFT, a novel
framework for adapting point cloud pre-trained models with minimal learnable
parameters. Specifically, for a pre-trained 3D model, we freeze most of its
parameters, and only tune the newly added PEFT modules on downstream tasks,
which consist of a Point-prior Prompt and a Geometry-aware Adapter. The
Point-prior Prompt adopts a set of learnable prompt tokens, for which we
propose to construct a memory bank with domain-specific knowledge, and utilize
a parameter-free attention to enhance the prompt tokens. The Geometry-aware
Adapter aims to aggregate point cloud features within spatial neighborhoods to
capture fine-grained geometric information through local interactions.
Extensive experiments indicate that our Point-PEFT can achieve better
performance than the full fine-tuning on various downstream tasks, while using
only 5% of the trainable parameters, demonstrating the efficiency and
effectiveness of our approach. Code is released at
https://github.com/Ivan-Tang-3D/PEFT-3D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yiwen Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ray Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zoey Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xianzheng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhigang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuelong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04469">
<title>Taming Binarized Neural Networks and Mixed-Integer Programs. (arXiv:2310.04469v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04469</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been a great deal of recent interest in binarized neural networks,
especially because of their explainability. At the same time, automatic
differentiation algorithms such as backpropagation fail for binarized neural
networks, which limits their applicability. By reformulating the problem of
training binarized neural networks as a subadditive dual of a mixed-integer
program, we show that binarized neural networks admit a tame representation.
This, in turn, makes it possible to use the framework of Bolte et al. for
implicit differentiation, which offers the possibility for practical
implementation of backpropagation in the context of binarized neural networks.
This approach could also be used for a broader class of mixed-integer programs,
beyond the training of binarized neural networks, as encountered in symbolic
approaches to AI and beyond.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aspman_J/0/1/0/all/0/1&quot;&gt;Johannes Aspman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korpas_G/0/1/0/all/0/1&quot;&gt;Georgios Korpas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marecek_J/0/1/0/all/0/1&quot;&gt;Jakub Marecek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10059">
<title>Flow Dynamics Correction for Action Recognition. (arXiv:2310.10059v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10059</link>
<description rdf:parseType="Literal">&lt;p&gt;Various research studies indicate that action recognition performance highly
depends on the types of motions being extracted and how accurate the human
actions are represented. In this paper, we investigate different optical flow,
and features extracted from these optical flow that capturing both short-term
and long-term motion dynamics. We perform power normalization on the magnitude
component of optical flow for flow dynamics correction to boost subtle or
dampen sudden motions. We show that existing action recognition models which
rely on optical flow are able to get performance boosted with our corrected
optical flow. To further improve performance, we integrate our corrected flow
dynamics into popular models through a simple hallucination step by selecting
only the best performing optical flow features, and we show that by
&apos;translating&apos; the CNN feature maps into these optical flow features with
different scales of motions leads to the new state-of-the-art performance on
several benchmarks including HMDB-51, YUP++, fine-grained action recognition on
MPII Cooking Activities, and large-scale Charades.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1&quot;&gt;Piotr Koniusz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12439">
<title>PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models. (arXiv:2310.12439v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12439</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompts have significantly improved the performance of pretrained Large
Language Models (LLMs) on various downstream tasks recently, making them
increasingly indispensable for a diverse range of LLM application scenarios.
However, the backdoor vulnerability, a serious security threat that can
maliciously alter the victim model&apos;s normal predictions, has not been
sufficiently explored for prompt-based LLMs. In this paper, we present
POISONPROMPT, a novel backdoor attack capable of successfully compromising both
hard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and
robustness of POISONPROMPT through extensive experiments on three popular
prompt methods, using six datasets and three widely used LLMs. Our findings
highlight the potential security threats posed by backdoor attacks on
prompt-based LLMs and emphasize the need for further research in this area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Hongwei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1&quot;&gt;Jian Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zhan Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15318">
<title>HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks. (arXiv:2310.15318v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15318</link>
<description rdf:parseType="Literal">&lt;p&gt;Graphs have emerged as a natural choice to represent and analyze the
intricate patterns and rich information of the Web, enabling applications such
as online page classification and social recommendation. The prevailing
&quot;pre-train, fine-tune&quot; paradigm has been widely adopted in graph machine
learning tasks, particularly in scenarios with limited labeled nodes. However,
this approach often exhibits a misalignment between the training objectives of
pretext tasks and those of downstream tasks. This gap can result in the
&quot;negative transfer&quot; problem, wherein the knowledge gained from pre-training
adversely affects performance in the downstream tasks. The surge in
prompt-based learning within Natural Language Processing (NLP) suggests the
potential of adapting a &quot;pre-train, prompt&quot; paradigm to graphs as an
alternative. However, existing graph prompting techniques are tailored to
homogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To
bridge this gap, we propose HetGPT, a general post-training prompting framework
to improve the predictive performance of pre-trained heterogeneous graph neural
networks (HGNNs). The key is the design of a novel prompting function that
integrates a virtual class prompt and a heterogeneous feature prompt, with the
aim to reformulate downstream tasks to mirror pretext tasks. Moreover, HetGPT
introduces a multi-view neighborhood aggregation mechanism, capturing the
complex neighborhood structure in heterogeneous graphs. Extensive experiments
on three benchmark datasets demonstrate HetGPT&apos;s capability to enhance the
performance of state-of-the-art HGNNs on semi-supervised node classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yihong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_N/0/1/0/all/0/1&quot;&gt;Ning Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiayu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mortazavi_M/0/1/0/all/0/1&quot;&gt;Masood Mortazavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chawla_N/0/1/0/all/0/1&quot;&gt;Nitesh V. Chawla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17589">
<title>An Open Source Data Contamination Report for Large Language Models. (arXiv:2310.17589v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17589</link>
<description rdf:parseType="Literal">&lt;p&gt;Data contamination in language model evaluation is increasingly prevalent as
the popularity of large language models. It allows models to &quot;cheat&quot; via
memorisation instead of displaying true capabilities. Therefore, contamination
analysis has became an crucial part of reliable model evaluation to validate
results. However, existing contamination analysis is usually conducted
internally by LLM developers and often lacks transparency and completeness.
This paper present an open source data contamination reports for the Llama
series models. We analyse six popular multi-choice QA benchmarks and quantify
their overlapping with the training set of Llama. Various levels of
contamination ranging from 1\% to 8.7\% are found across benchmarks. Our
comparison also reveals that Llama models can gain over 5\% higher accuracy on
contaminated subsets versus clean subsets. Data and code are available at:
https://github.com/liyucheng09/Contamination_Detector.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yucheng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17658">
<title>Is Channel Independent strategy optimal for Time Series Forecasting?. (arXiv:2310.17658v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17658</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been an emergence of various models for long-term time series
forecasting. Recent studies have demonstrated that a single linear layer, using
Channel Dependent (CD) or Channel Independent (CI) modeling, can even
outperform a large number of sophisticated models. However, current research
primarily considers CD and CI as two complementary yet mutually exclusive
approaches, unable to harness these two extremes simultaneously. And it is also
a challenging issue that both CD and CI are static strategies that cannot be
determined to be optimal for a specific dataset without extensive experiments.
In this paper, we reconsider whether the current CI strategy is the best
solution for time series forecasting. First, we propose a simple yet effective
strategy called CSC, which stands for $\mathbf{C}$hannel
$\mathbf{S}$elf-$\mathbf{C}$lustering strategy, for linear models. Our Channel
Self-Clustering (CSC) enhances CI strategy&apos;s performance improvements while
reducing parameter size, for exmpale by over 10 times on electricity dataset,
and significantly cutting training time. Second, we further propose Channel
Rearrangement (CR), a method for deep models inspired by the self-clustering.
CR attains competitive performance against baselines. Finally, we also discuss
whether it is best to forecast the future values using the historical values of
the same channel as inputs. We hope our findings and methods could inspire new
solutions beyond CD/CI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peiwen_Y/0/1/0/all/0/1&quot;&gt;Yuan Peiwen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Changsheng_Z/0/1/0/all/0/1&quot;&gt;Zhu Changsheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18021">
<title>FormalGeo: The First Step Toward Human-like IMO-level Geometric Automated Reasoning. (arXiv:2310.18021v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18021</link>
<description rdf:parseType="Literal">&lt;p&gt;This is the first paper in a series of work we have accomplished over the
past three years. In this paper, we have constructed a complete and compatible
formal plane geometry system. This will serve as a crucial bridge between
IMO-level plane geometry challenges and readable AI automated reasoning. Within
this formal framework, we have been able to seamlessly integrate modern AI
models with our formal system. AI is now capable of providing deductive
reasoning solutions to IMO-level plane geometry problems, just like handling
other natural languages, and these proofs are readable, traceable, and
verifiable. We propose the geometry formalization theory (GFT) to guide the
development of the geometry formal system. Based on the GFT, we have
established the FormalGeo, which consists of 88 geometric predicates and 196
theorems. It can represent, validate, and solve IMO-level geometry problems. we
also have crafted the FGPS (formal geometry problem solver) in Python. It
serves as both an interactive assistant for verifying problem-solving processes
and an automated problem solver. We&apos;ve annotated the formalgeo7k and
formalgeo-imo datasets. The former contains 6,981 (expand to 133,818 through
data augmentation) geometry problems, while the latter includes 18 (expand to
2,627 and continuously increasing) IMO-level challenging geometry problems. All
annotated problems include detailed formal language descriptions and solutions.
Implementation of the formal system and experiments validate the correctness
and utility of the GFT. The backward depth-first search method only yields a
2.42% problem-solving failure rate, and we can incorporate deep learning
techniques to achieve lower one. The source code of FGPS and datasets are
available at https://github.com/BitSecret/FGPS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaokai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_N/0/1/0/all/0/1&quot;&gt;Na Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yiming He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;Jia Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qike Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yanjun Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1&quot;&gt;Chenyang Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhe Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_D/0/1/0/all/0/1&quot;&gt;Dengfeng Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Fangzhen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiwen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Runan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1&quot;&gt;Cheng Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zhenbing Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Shaorong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiangfeng Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leng_T/0/1/0/all/0/1&quot;&gt;Tuo Leng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18541">
<title>ReConTab: Regularized Contrastive Representation Learning for Tabular Data. (arXiv:2310.18541v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18541</link>
<description rdf:parseType="Literal">&lt;p&gt;Representation learning stands as one of the critical machine learning
techniques across various domains. Through the acquisition of high-quality
features, pre-trained embeddings significantly reduce input space redundancy,
benefiting downstream pattern recognition tasks such as classification,
regression, or detection. Nonetheless, in the domain of tabular data, feature
engineering and selection still heavily rely on manual intervention, leading to
time-consuming processes and necessitating domain expertise. In response to
this challenge, we introduce ReConTab, a deep automatic representation learning
framework with regularized contrastive learning. Agnostic to any type of
modeling task, ReConTab constructs an asymmetric autoencoder based on the same
raw features from model inputs, producing low-dimensional representative
embeddings. Specifically, regularization techniques are applied for raw feature
selection. Meanwhile, ReConTab leverages contrastive learning to distill the
most pertinent information for downstream tasks. Experiments conducted on
extensive real-world datasets substantiate the framework&apos;s capacity to yield
substantial and robust performance improvements. Furthermore, we empirically
demonstrate that pre-trained embeddings can seamlessly integrate as easily
adaptable features, enhancing the performance of various traditional methods
such as XGBoost and Random Forest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Suiyao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jing Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hovakimyan_N/0/1/0/all/0/1&quot;&gt;Naira Hovakimyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Handong Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18890">
<title>Towards Generalized Multi-stage Clustering: Multi-view Self-distillation. (arXiv:2310.18890v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18890</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing multi-stage clustering methods independently learn the salient
features from multiple views and then perform the clustering task.
Particularly, multi-view clustering (MVC) has attracted a lot of attention in
multi-view or multi-modal scenarios. MVC aims at exploring common semantics and
pseudo-labels from multiple views and clustering in a self-supervised manner.
However, limited by noisy data and inadequate feature learning, such a
clustering paradigm generates overconfident pseudo-labels that mis-guide the
model to produce inaccurate predictions. Therefore, it is desirable to have a
method that can correct this pseudo-label mistraction in multi-stage clustering
to avoid the bias accumulation. To alleviate the effect of overconfident
pseudo-labels and improve the generalization ability of the model, this paper
proposes a novel multi-stage deep MVC framework where multi-view
self-distillation (DistilMVC) is introduced to distill dark knowledge of label
distribution. Specifically, in the feature subspace at different hierarchies,
we explore the common semantics of multiple views through contrastive learning
and obtain pseudo-labels by maximizing the mutual information between views.
Additionally, a teacher network is responsible for distilling pseudo-labels
into dark knowledge, supervising the student network and improving its
predictive capabilities to enhance the robustness. Extensive experiments on
real-world multi-view datasets show that our method has better clustering
performance than state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiatai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19251">
<title>Pre-trained Recommender Systems: A Causal Debiasing Perspective. (arXiv:2310.19251v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19251</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies on pre-trained vision/language models have demonstrated the
practical benefit of a new, promising solution-building paradigm in AI where
models can be pre-trained on broad data describing a generic task space and
then adapted successfully to solve a wide range of downstream tasks, even when
training data is severely limited (e.g., in zero- or few-shot learning
scenarios). Inspired by such progress, we investigate in this paper the
possibilities and challenges of adapting such a paradigm to the context of
recommender systems, which is less investigated from the perspective of
pre-trained model. In particular, we propose to develop a generic recommender
that captures universal interaction patterns by training on generic user-item
interaction data extracted from different domains, which can then be fast
adapted to improve few-shot learning performance in unseen new domains (with
limited data).
&lt;/p&gt;
&lt;p&gt;However, unlike vision/language data which share strong conformity in the
semantic space, universal patterns underlying recommendation data collected
across different domains (e.g., different countries or different E-commerce
platforms) are often occluded by both in-domain and cross-domain biases
implicitly imposed by the cultural differences in their user and item bases, as
well as their uses of different e-commerce platforms. As shown in our
experiments, such heterogeneous biases in the data tend to hinder the
effectiveness of the pre-trained model. To address this challenge, we further
introduce and formalize a causal debiasing perspective, which is substantiated
via a hierarchical Bayesian deep learning model, named PreRec. Our empirical
studies on real-world data show that the proposed model could significantly
improve the recommendation performance in zero- and few-shot learning settings
under both cross-market and cross-platform scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Ziqian Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Hao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_N/0/1/0/all/0/1&quot;&gt;Nghia Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1&quot;&gt;Branislav Kveton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deoras_A/0/1/0/all/0/1&quot;&gt;Anoop Deoras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19629">
<title>RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency. (arXiv:2310.19629v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19629</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the problem of continuous 3D shape representations.
The majority of existing successful methods are coordinate-based implicit
neural representations. However, they are inefficient to render novel views or
recover explicit surface points. A few works start to formulate 3D shapes as
ray-based neural functions, but the learned structures are inferior due to the
lack of multi-view geometry consistency. To tackle these challenges, we propose
a new framework called RayDF. It consists of three major components: 1) the
simple ray-surface distance field, 2) the novel dual-ray visibility classifier,
and 3) a multi-view consistency optimization module to drive the learned
ray-surface distances to be multi-view geometry consistent. We extensively
evaluate our method on three public datasets, demonstrating remarkable
performance in 3D surface point reconstruction on both synthetic and
challenging real-world 3D scenes, clearly surpassing existing coordinate-based
and ray-based baselines. Most notably, our method achieves a 1000x faster speed
than coordinate-based methods to render an 800x800 depth image, showing the
superiority of our method for 3D shape representation. Our code and data are
available at https://github.com/vLAR-group/RayDF
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhuoman Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luximon_Y/0/1/0/all/0/1&quot;&gt;Yan Luximon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Ajay Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinxi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04498">
<title>NExT-Chat: An LMM for Chat, Detection and Segmentation. (arXiv:2311.04498v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04498</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of large language models (LLMs) has greatly advanced the
field of multimodal understanding, leading to the emergence of large multimodal
models (LMMs). In order to enhance the level of visual comprehension, recent
studies have equipped LMMs with region-level understanding capabilities by
representing object bounding box coordinates as a series of text sequences
(pix2seq). In this paper, we introduce a novel paradigm for object location
modeling called pix2emb method, where we ask the LMM to output the location
embeddings and then decode them with different decoders. This paradigm allows
us to use different location formats (such as bounding boxes and masks) in
multimodal conversations. Leveraging the proposed pix2emb method, we train an
LMM named NExT-Chat and demonstrate its capability of handling multiple tasks
like visual grounding, region captioning, and grounded reasoning. Comprehensive
experiments show the effectiveness of our NExT-Chat on various tasks, e.g.,
NExT-Chat (87.7) vs. Shikra (86.9) on POPE-Random, NExT-Chat (68.9) vs. LISA
(67.9) on referring expression segmentation task, and NExT-Chat (79.6) vs.
Kosmos-2 (62.3) on region caption task. The code and model are released at
https://github.com/NExT-ChatV/NExT-Chat.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Ao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuan Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1&quot;&gt;Wei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1&quot;&gt;Tat-Seng Chua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04666">
<title>Pre-training LLMs using human-like development data corpus. (arXiv:2311.04666v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04666</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained Large Language Models (LLMs) have shown success in a diverse set
of language inference and understanding tasks. The pre-training stage of LLMs
looks at a large corpus of raw textual data. The BabyLM shared task compares
LLM pre-training to human language acquisition, where the number of tokens seen
by 13-year-old kids is magnitudes smaller than the number of tokens seen by
LLMs. In this work, we pre-train and evaluate LLMs on their ability to learn
contextual word representations using roughly the same number of tokens as seen
by children. We provide a strong set of baselines; with different
architectures, evaluation of changes in performance across epochs, and reported
pre-training metrics for the strict small and strict tracks of the task. We
also try to loosely replicate the RoBERTa baseline given by the task organizers
to observe the training robustness to hyperparameter selection and
replicability. We provide the submission details to the strict and strict-small
tracks in this report.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhardwaj_K/0/1/0/all/0/1&quot;&gt;Khushi Bhardwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Raj Sanjay Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varma_S/0/1/0/all/0/1&quot;&gt;Sashank Varma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07723">
<title>Generalization Analogies: A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains. (arXiv:2311.07723v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07723</link>
<description rdf:parseType="Literal">&lt;p&gt;As AI systems become more intelligent and their behavior becomes more
challenging to assess, they may learn to game the flaws of human feedback
instead of genuinely striving to follow instructions; however, this risk can be
mitigated by controlling how LLMs generalize human feedback to situations where
it is unreliable. To better understand how reward models generalize, we craft
69 distribution shifts spanning 8 categories. We find that reward models do not
learn to evaluate `instruction-following&apos; by default and instead favor personas
that resemble internet text. Techniques for interpreting reward models&apos;
internal representations achieve better generalization than standard
fine-tuning, but still frequently fail to distinguish instruction-following
from conflated behaviors. We consolidate the 15 most challenging distribution
shifts into the GENeralization analogIES (GENIES) benchmark, which we hope will
enable progress toward controlling reward model generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clymer_J/0/1/0/all/0/1&quot;&gt;Joshua Clymer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baker_G/0/1/0/all/0/1&quot;&gt;Garrett Baker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramani_R/0/1/0/all/0/1&quot;&gt;Rohan Subramani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sam Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07745">
<title>Simplifying Complex Observation Models in Continuous POMDP Planning with Probabilistic Guarantees and Practice. (arXiv:2311.07745v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07745</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving partially observable Markov decision processes (POMDPs) with high
dimensional and continuous observations, such as camera images, is required for
many real life robotics and planning problems. Recent researches suggested
machine learned probabilistic models as observation models, but their use is
currently too computationally expensive for online deployment. We deal with the
question of what would be the implication of using simplified observation
models for planning, while retaining formal guarantees on the quality of the
solution. Our main contribution is a novel probabilistic bound based on a
statistical total variation distance of the simplified model. We show that it
bounds the theoretical POMDP value w.r.t. original model, from the empirical
planned value with the simplified model, by generalizing recent results of
particle-belief MDP concentration bounds. Our calculations can be separated
into offline and online parts, and we arrive at formal guarantees without
having to access the costly model at all during planning, which is also a novel
result. Finally, we demonstrate in simulation how to integrate the bound into
the routine of an existing continuous online POMDP solver.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lev_Yehudi_I/0/1/0/all/0/1&quot;&gt;Idan Lev-Yehudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barenboim_M/0/1/0/all/0/1&quot;&gt;Moran Barenboim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Indelman_V/0/1/0/all/0/1&quot;&gt;Vadim Indelman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10522">
<title>Enhancing Object Coherence in Layout-to-Image Synthesis. (arXiv:2311.10522v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10522</link>
<description rdf:parseType="Literal">&lt;p&gt;Layout-to-image synthesis is an emerging technique in conditional image
generation. It aims to generate complex scenes, where users require fine
control over the layout of the objects in a scene. However, it remains
challenging to control the object coherence, including semantic coherence
(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the
hand and the racket should not be misaligned). In this paper, we propose a
novel diffusion model with effective global semantic fusion (GSF) and
self-similarity feature enhancement modules to guide the object coherence for
this task. For semantic coherence, we argue that the image caption contains
rich information for defining the semantic relationship within the objects in
the images. Instead of simply employing cross-attention between captions and
generated images, which addresses the highly relevant layout restriction and
semantic coherence separately and thus leads to unsatisfying results shown in
our experiments, we develop GSF to fuse the supervision from the layout
restriction and semantic coherence requirement and exploit it to guide the
image synthesis process. Moreover, to improve the physical coherence, we
develop a Self-similarity Coherence Attention (SCA) module to explicitly
integrate local contextual physical coherence into each pixel&apos;s generation
process. Specifically, we adopt a self-similarity map to encode the coherence
restrictions and employ it to extract coherent features from text embedding.
Through visualization of our self-similarity map, we explore the essence of
SCA, revealing that its effectiveness is not only in capturing reliable
physical coherence patterns but also in enhancing complex texture generation.
Extensive experiments demonstrate the superiority of our proposed method in
both image generation quality and controllability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yibin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weizhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jianwei Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Cheng Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10599">
<title>Chatbots as social companions: How people perceive consciousness, human likeness, and social health benefits in machines. (arXiv:2311.10599v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10599</link>
<description rdf:parseType="Literal">&lt;p&gt;As artificial intelligence (AI) becomes more widespread, one question that
arises is how human-AI interaction might impact human-human interaction.
Chatbots, for example, are increasingly used as social companions, but little
is known about how their use impacts human relationships. A common hypothesis
is that these companion bots are detrimental to social health by harming or
replacing human interaction. To understand how companion bots impact social
health, we studied people who used companion bots and people who did not.
Contrary to expectations, companion bot users indicated that these
relationships were beneficial to their social health, whereas nonusers viewed
them as harmful. Another common assumption is that people perceive conscious,
humanlike AI as disturbing and threatening. Among both users and nonusers,
however, we found the opposite: perceiving companion bots as more conscious and
humanlike correlated with more positive opinions and better social health
benefits. Humanlike bots may aid social health by supplying reliable and safe
interactions, without necessarily harming human relationships.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guingrich_R/0/1/0/all/0/1&quot;&gt;Rose Guingrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graziano_M/0/1/0/all/0/1&quot;&gt;Michael S. A. Graziano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12435">
<title>Fair Enough? A map of the current limitations of the requirements to have &quot;fair&quot; algorithms. (arXiv:2311.12435v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12435</link>
<description rdf:parseType="Literal">&lt;p&gt;In the recent years, the raise in the usage and efficiency of Artificial
Intelligence and, more in general, of Automated Decision-Making systems has
brought with it an increasing and welcome awareness of the risks associated
with such systems. One of such risks is that of perpetuating or even amplifying
bias and unjust disparities present in the data from which many of these
systems learn to adjust and optimise their decisions. This awareness has on one
side encouraged several scientific communities to come up with more and more
appropriate ways and methods to assess, quantify, and possibly mitigate such
biases and disparities. On the other hand, it has prompted more and more layers
of society, including policy makers, to call for &quot;fair&quot; algorithms. We believe
that while a lot of excellent and multidisciplinary research is currently being
conducted, what is still fundamentally missing is the awareness that having
&quot;fair&quot; algorithms is per se a nearly meaningless requirement, that needs to be
complemented with a lot of additional societal choices to become actionable.
Namely, there is a hiatus between what the society is demanding from Automated
Decision-Making systems, and what this demand actually means in real-world
scenarios. In this work, we outline the key features of such a hiatus, and
pinpoint a list of fundamental ambiguities and attention points that we as a
society must address in order to give a concrete meaning to the increasing
demand of fairness in Automated Decision-Making systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castelnovo_A/0/1/0/all/0/1&quot;&gt;Alessandro Castelnovo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inverardi_N/0/1/0/all/0/1&quot;&gt;Nicole Inverardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nanino_G/0/1/0/all/0/1&quot;&gt;Gabriele Nanino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Penco_I/0/1/0/all/0/1&quot;&gt;Ilaria Giuseppina Penco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Regoli_D/0/1/0/all/0/1&quot;&gt;Daniele Regoli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14865">
<title>Improving Cross-Domain Hate Speech Generalizability with Emotion Knowledge. (arXiv:2311.14865v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14865</link>
<description rdf:parseType="Literal">&lt;p&gt;Reliable automatic hate speech (HS) detection systems must adapt to the
in-flow of diverse new data to curtail hate speech. However, hate speech
detection systems commonly lack generalizability in identifying hate speech
dissimilar to data used in training, impeding their robustness in real-world
deployments. In this work, we propose a hate speech generalization framework
that leverages emotion knowledge in a multitask architecture to improve the
generalizability of hate speech detection in a cross-domain setting. We
investigate emotion corpora with varying emotion categorical scopes to
determine the best corpus scope for supplying emotion knowledge to foster
generalized hate speech detection. We further assess the relationship between
using pretrained Transformers models adapted for hate speech and its effect on
our emotion-enriched hate speech generalization model. We perform extensive
experiments on six publicly available datasets sourced from different online
domains and show that our emotion-enriched HS detection generalization method
demonstrates consistent generalization improvement in cross-domain evaluation,
increasing generalization performance up to 18.1% and average cross-domain
performance up to 8.5%, according to the F1 measure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Shi Yin Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gauch_S/0/1/0/all/0/1&quot;&gt;Susan Gauch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15786">
<title>YUAN 2.0: A Large Language Model with Localized Filtering-based Attention. (arXiv:2311.15786v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15786</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we develop and release Yuan 2.0, a series of large language
models with parameters ranging from 2.1 billion to 102.6 billion. The Localized
Filtering-based Attention (LFA) is introduced to incorporate prior knowledge of
local dependencies of natural language into Attention. A data filtering and
generating system is presented to build pre-training and fine-tuning dataset in
high quality. A distributed training method with non-uniform pipeline parallel,
data parallel, and optimizer parallel is proposed, which greatly reduces the
bandwidth requirements of intra-node communication, and achieves good
performance in large-scale distributed training. Yuan 2.0 models display
impressive ability in code generation, math problem-solving, and chatting
compared with existing models. The latest version of YUAN 2.0, including model
weights and source code, is accessible at Github.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shaohua Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xudong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shenling Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jiangang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lingjun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rongguo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiahua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17401">
<title>Gene-MOE: A sparsely gated prognosis and classification framework exploiting pan-cancer genomic information. (arXiv:2311.17401v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17401</link>
<description rdf:parseType="Literal">&lt;p&gt;Benefiting from the advancements in deep learning, various genomic analytical
techniques, such as survival analysis, classification of tumors and their
subtypes, and exploration of specific pathways, have significantly enhanced our
understanding of the biological mechanisms driving cancer. However, the
overfitting issue, arising from the limited number of patient samples, poses a
challenge in improving the accuracy of genome analysis by deepening the neural
network. Furthermore, it remains uncertain whether novel approaches such as the
sparsely gated mixture of expert (MOE) and self-attention mechanisms can
improve the accuracy of genomic analysis. In this paper, we introduce a novel
sparsely gated RNA-seq analysis framework called Gene-MOE. This framework
exploits the potential of the MOE layers and the proposed mixture of attention
expert (MOAE) layers to enhance the analysis accuracy. Additionally, it
addresses overfitting challenges by integrating pan-cancer information from 33
distinct cancer types through pre-training.We pre-trained Gene-MOE on TCGA
pan-cancer RNA-seq dataset with 33 cancer types. Subsequently, we conducted
experiments involving cancer classification and survival analysis based on the
pre-trained Gene-MOE. According to the survival analysis results on 14 cancer
types, Gene-MOE outperformed state-of-the-art models on 12 cancer types.
Through detailed feature analysis, we found that the Gene-MOE model could learn
rich feature representations of high-dimensional genes. According to the
classification results, the total accuracy of the classification model for 33
cancer classifications reached 95.8%, representing the best performance
compared to state-of-the-art models. These results indicate that Gene-MOE holds
strong potential for use in cancer classification and survival analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xiangyu Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Huanhuan Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1&quot;&gt;Lian Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Hongzhen Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_L/0/1/0/all/0/1&quot;&gt;Long Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18550">
<title>Search Still Matters: Information Retrieval in the Era of Generative AI. (arXiv:2311.18550v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18550</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: Information retrieval (IR, also known as search) systems are
ubiquitous in modern times. How does the emergence of generative artificial
intelligence (AI), based on large language models (LLMs), fit into the IR
process? Process: This perspective explores the use of generative AI in the
context of the motivations, considerations, and outcomes of the IR process with
a focus on the academic use of such systems. Conclusions: There are many
information needs, from simple to complex, that motivate use of IR. Users of
such systems, particularly academics, have concerns for authoritativeness,
timeliness, and contextualization of search. While LLMs may provide
functionality that aids the IR process, the continued need for search systems,
and research into their improvement, remains essential.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hersh_W/0/1/0/all/0/1&quot;&gt;William R. Hersh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00377">
<title>SynFundus: A synthetic fundus images dataset with millions of samples and multi-disease annotations. (arXiv:2312.00377v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00377</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of medical imaging, there are seldom large-scale public datasets
with high-quality annotations due to data privacy and annotation cost. To
address this issue, we release SynFundus-1M, a high-quality synthetic dataset
containing over \textbf{1 million} fundus images w.r.t. 11 disease types.
Moreover, we intentionally diversify the readability of the images and
accordingly provide 4 types of the quality score for each image. To the best of
our knowledge, SynFundus-1M is currently the largest fundus dataset with the
most sophisticated annotations. All the images are generated by a Denoising
Diffusion Probabilistic Model, named SynFundus-Generator. Trained with over 1.3
million private fundus images, our SynFundus-Generator achieves significant
superior performance in generating fundus images compared to some recent
related works. Furthermore, we blend some synthetic images from SynFundus-1M
with real fundus images, and ophthalmologists can hardly distinguish the
synthetic images from real ones. Through extensive experiments, we demonstrate
that both convolutional neural networs (CNN) and Vision Transformer (ViT) can
benefit from SynFundus-1M by pretraining or training directly. Compared to
datasets like ImageNet or EyePACS, models trained on SynFundus-1M not only
achieve better performance but also faster convergence on various downstream
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1&quot;&gt;Fangxin Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yehui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haifeng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Junwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lei Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01090">
<title>Self Generated Wargame AI: Double Layer Agent Task Planning Based on Large Language Model. (arXiv:2312.01090v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01090</link>
<description rdf:parseType="Literal">&lt;p&gt;The large language models represented by ChatGPT have a disruptive impact on
the field of artificial intelligence. But it mainly focuses on natural language
processing, speech recognition, machine learning and natural language
understanding. This paper innovatively applies the large language model to the
field of intelligent decision-making, places the large language model in the
decision-making center, and constructs an agent architecture with the large
language model as the core. Based on this, it further proposes a two-layer
agent task planning, issues and executes decision commands through the
interaction of natural language, and carries out simulation verification
through the wargame simulation environment. Through the game confrontation
simulation experiment, it is found that the intelligent decision-making ability
of the large language model is significantly stronger than the commonly used
reinforcement learning AI and rule AI, and the intelligence, understandability
and generalization are all better. And through experiments, it was found that
the intelligence of the large language model is closely related to prompt. This
work also extends the large language model from previous human-computer
interaction to the field of intelligent decision-making, which has important
reference value and significance for the development of intelligent
decision-making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Y.Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;J.Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;C.Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;W.Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;X.Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02246">
<title>Conditional Variational Diffusion Models. (arXiv:2312.02246v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02246</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse problems aim to determine parameters from observations, a crucial
task in engineering and science. Lately, generative models, especially
diffusion models, have gained popularity in this area for their ability to
produce realistic solutions and their good mathematical properties. Despite
their success, an important drawback of diffusion models is their sensitivity
to the choice of variance schedule, which controls the dynamics of the
diffusion process. Fine-tuning this schedule for specific applications is
crucial but time-costly and does not guarantee an optimal result. We propose a
novel approach for learning the schedule as part of the training process. Our
method supports probabilistic conditioning on data, provides high-quality
solutions, and is flexible, proving able to adapt to different applications
with minimum overhead. This approach is tested in two unrelated inverse
problems: super-resolution microscopy and quantitative phase imaging, yielding
comparable or superior results to previous methods and fine-tuned diffusion
models. We conclude that fine-tuning the schedule by experimentation should be
avoided because it can be learned during training in a stable way that yields
better results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggiora_G/0/1/0/all/0/1&quot;&gt;Gabriel della Maggiora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Croquevielle_L/0/1/0/all/0/1&quot;&gt;Luis Alberto Croquevielle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desphande_N/0/1/0/all/0/1&quot;&gt;Nikita Desphande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horsley_H/0/1/0/all/0/1&quot;&gt;Harry Horsley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinis_T/0/1/0/all/0/1&quot;&gt;Thomas Heinis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yakimovich_A/0/1/0/all/0/1&quot;&gt;Artur Yakimovich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02674">
<title>Amortized Bayesian Decision Making for simulation-based models. (arXiv:2312.02674v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02674</link>
<description rdf:parseType="Literal">&lt;p&gt;Simulation-based inference (SBI) provides a powerful framework for inferring
posterior distributions of stochastic simulators in a wide range of domains. In
many settings, however, the posterior distribution is not the end goal itself
-- rather, the derived parameter values and their uncertainties are used as a
basis for deciding what actions to take. Unfortunately, because posterior
distributions provided by SBI are (potentially crude) approximations of the
true posterior, the resulting decisions can be suboptimal. Here, we address the
question of how to perform Bayesian decision making on stochastic simulators,
and how one can circumvent the need to compute an explicit approximation to the
posterior. Our method trains a neural network on simulated data and can predict
the expected cost given any data and action, and can, thus, be directly used to
infer the action with lowest cost. We apply our method to several benchmark
problems and demonstrate that it induces similar cost as the true posterior
distribution. We then apply the method to infer optimal actions in a real-world
simulator in the medical neurosciences, the Bayesian Virtual Epileptic Patient,
and demonstrate that it allows to infer actions associated with low cost after
few simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorecki_M/0/1/0/all/0/1&quot;&gt;Mila Gorecki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macke_J/0/1/0/all/0/1&quot;&gt;Jakob H. Macke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deistler_M/0/1/0/all/0/1&quot;&gt;Michael Deistler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02855">
<title>Exploring Error Bits for Memory Failure Prediction: An In-Depth Correlative Study. (arXiv:2312.02855v2 [cs.AR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02855</link>
<description rdf:parseType="Literal">&lt;p&gt;In large-scale datacenters, memory failure is a common cause of server
crashes, with Uncorrectable Errors (UEs) being a major indicator of Dual Inline
Memory Module (DIMM) defects. Existing approaches primarily focus on predicting
UEs using Correctable Errors (CEs), without fully considering the information
provided by error bits. However, error bit patterns have a strong correlation
with the occurrence of UEs. In this paper, we present a comprehensive study on
the correlation between CEs and UEs, specifically emphasizing the importance of
spatio-temporal error bit information. Our analysis reveals a strong
correlation between spatio-temporal error bits and UE occurrence. Through
evaluations using real-world datasets, we demonstrate that our approach
significantly improves prediction performance by 15% in F1-score compared to
the state-of-the-art algorithms. Overall, our approach effectively reduces the
number of virtual machine interruptions caused by UEs by approximately 59%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qiao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wengui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardoso_J/0/1/0/all/0/1&quot;&gt;Jorge Cardoso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kao_O/0/1/0/all/0/1&quot;&gt;Odej Kao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03360">
<title>Teaching Specific Scientific Knowledge into Large Language Models through Additional Training. (arXiv:2312.03360v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03360</link>
<description rdf:parseType="Literal">&lt;p&gt;Through additional training, we explore embedding specialized scientific
knowledge into the Llama 2 Large Language Model (LLM). Key findings reveal that
effective knowledge integration requires reading texts from multiple
perspectives, especially in instructional formats. We utilize text augmentation
to tackle the scarcity of specialized texts, including style conversions and
translations. Hyperparameter optimization proves crucial, with different size
models (7b, 13b, and 70b) reasonably undergoing additional training. Validating
our methods, we construct a dataset of 65,000 scientific papers. Although we
have succeeded in partially embedding knowledge, the study highlights the
complexities and limitations of incorporating specialized information into
LLMs, suggesting areas for further improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatakeyama_Sato_K/0/1/0/all/0/1&quot;&gt;Kan Hatakeyama-Sato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Igarashi_Y/0/1/0/all/0/1&quot;&gt;Yasuhiko Igarashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katakami_S/0/1/0/all/0/1&quot;&gt;Shun Katakami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nabae_Y/0/1/0/all/0/1&quot;&gt;Yuta Nabae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayakawa_T/0/1/0/all/0/1&quot;&gt;Teruaki Hayakawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06833">
<title>The unreasonable effectiveness of AI CADe polyp detectors to generalize to new countries. (arXiv:2312.06833v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06833</link>
<description rdf:parseType="Literal">&lt;p&gt;$\textbf{Background and aims}$: Artificial Intelligence (AI) Computer-Aided
Detection (CADe) is commonly used for polyp detection, but data seen in
clinical settings can differ from model training. Few studies evaluate how well
CADe detectors perform on colonoscopies from countries not seen during
training, and none are able to evaluate performance without collecting
expensive and time-intensive labels.
&lt;/p&gt;
&lt;p&gt;$\textbf{Methods}$: We trained a CADe polyp detector on Israeli colonoscopy
videos (5004 videos, 1106 hours) and evaluated on Japanese videos (354 videos,
128 hours) by measuring the True Positive Rate (TPR) versus false alarms per
minute (FAPM). We introduce a colonoscopy dissimilarity measure called &quot;MAsked
mediCal Embedding Distance&quot; (MACE) to quantify differences between
colonoscopies, without labels. We evaluated CADe on all Japan videos and on
those with the highest MACE.
&lt;/p&gt;
&lt;p&gt;$\textbf{Results}$: MACE correctly quantifies that narrow-band imaging (NBI)
and chromoendoscopy (CE) frames are less similar to Israel data than Japan
whitelight (bootstrapped z-test, |z| &amp;gt; 690, p &amp;lt; $10^{-8}$ for both). Despite
differences in the data, CADe performance on Japan colonoscopies was
non-inferior to Israel ones without additional training (TPR at 0.5 FAPM: 0.957
and 0.972 for Israel and Japan; TPR at 1.0 FAPM: 0.972 and 0.989 for Israel and
Japan; superiority test t &amp;gt; 45.2, p &amp;lt; $10^{-8}$). Despite not being trained on
NBI or CE, TPR on those subsets were non-inferior to Japan overall
(non-inferiority test t &amp;gt; 47.3, p &amp;lt; $10^{-8}$, $\delta$ = 1.5% for both).
&lt;/p&gt;
&lt;p&gt;$\textbf{Conclusion}$: Differences that prevent CADe detectors from
performing well in non-medical settings do not degrade the performance of our
AI CADe polyp detector when applied to data from a new country. MACE can help
medical AI models internationalize by identifying the most &quot;dissimilar&quot; data on
which to evaluate models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shor_J/0/1/0/all/0/1&quot;&gt;Joel Shor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamano_H/0/1/0/all/0/1&quot;&gt;Hiro-o Yamano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsurumaru_D/0/1/0/all/0/1&quot;&gt;Daisuke Tsurumaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Intrator_Y/0/1/0/all/0/1&quot;&gt;Yotami Intrator&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kayama_H/0/1/0/all/0/1&quot;&gt;Hiroki Kayama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ledsam_J/0/1/0/all/0/1&quot;&gt;Joe Ledsam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamabe_A/0/1/0/all/0/1&quot;&gt;Atsushi Hamabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ando_K/0/1/0/all/0/1&quot;&gt;Koji Ando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ota_M/0/1/0/all/0/1&quot;&gt;Mitsuhiko Ota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ogino_H/0/1/0/all/0/1&quot;&gt;Haruei Ogino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakase_H/0/1/0/all/0/1&quot;&gt;Hiroshi Nakase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobayashi_K/0/1/0/all/0/1&quot;&gt;Kaho Kobayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oki_E/0/1/0/all/0/1&quot;&gt;Eiji Oki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldenberg_R/0/1/0/all/0/1&quot;&gt;Roman Goldenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivlin_E/0/1/0/all/0/1&quot;&gt;Ehud Rivlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takemasa_I/0/1/0/all/0/1&quot;&gt;Ichiro Takemasa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06901">
<title>Unsupervised Extractive Summarization with Learnable Length Control Strategies. (arXiv:2312.06901v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06901</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised extractive summarization is an important technique in
information extraction and retrieval. Compared with supervised method, it does
not require high-quality human-labelled summaries for training and thus can be
easily applied for documents with different types, domains or languages. Most
of existing unsupervised methods including TextRank and PACSUM rely on
graph-based ranking on sentence centrality. However, this scorer can not be
directly applied in end-to-end training, and the positional-related prior
assumption is often needed for achieving good summaries. In addition, less
attention is paid to length-controllable extractor, where users can decide to
summarize texts under particular length constraint. This paper introduces an
unsupervised extractive summarization model based on a siamese network, for
which we develop a trainable bidirectional prediction objective between the
selected summary and the original document. Different from the centrality-based
ranking methods, our extractive scorer can be trained in an end-to-end manner,
with no other requirement of positional assumption. In addition, we introduce a
differentiable length control module by approximating 0-1 knapsack solver for
end-to-end length-controllable extracting. Experiments show that our
unsupervised method largely outperforms the centrality-based baseline using a
same sentence encoder. In terms of length control ability, via our trainable
knapsack module, the performance consistently outperforms the strong baseline
without utilizing end-to-end training. Human evaluation further evidences that
our method performs the best among baselines in terms of relevance and
consistency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jie_R/0/1/0/all/0/1&quot;&gt;Renlong Jie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xiaojun Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qun Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07144">
<title>The Parameterized Complexity of Coordinated Motion Planning. (arXiv:2312.07144v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07144</link>
<description rdf:parseType="Literal">&lt;p&gt;In Coordinated Motion Planning (CMP), we are given a rectangular-grid on
which $k$ robots occupy $k$ distinct starting gridpoints and need to reach $k$
distinct destination gridpoints. In each time step, any robot may move to a
neighboring gridpoint or stay in its current gridpoint, provided that it does
not collide with other robots. The goal is to compute a schedule for moving the
$k$ robots to their destinations which minimizes a certain objective target -
prominently the number of time steps in the schedule, i.e., the makespan, or
the total length traveled by the robots. We refer to the problem arising from
minimizing the former objective target as CMP-M and the latter as CMP-L. Both
CMP-M and CMP-L are fundamental problems that were posed as the computational
geometry challenge of SoCG 2021, and CMP also embodies the famous
$(n^2-1)$-puzzle as a special case.
&lt;/p&gt;
&lt;p&gt;In this paper, we settle the parameterized complexity of CMP-M and CMP-L with
respect to their two most fundamental parameters: the number of robots, and the
objective target. We develop a new approach to establish the fixed-parameter
tractability of both problems under the former parameterization that relies on
novel structural insights into optimal solutions to the problem. When
parameterized by the objective target, we show that CMP-L remains
fixed-parameter tractable while CMP-M becomes para-NP-hard. The latter result
is noteworthy, not only because it improves the previously-known boundaries of
intractability for the problem, but also because the underlying reduction
allows us to establish - as a simpler case - the NP-hardness of the classical
Vertex Disjoint and Edge Disjoint Paths problems with constant path-lengths on
grids.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eiben_E/0/1/0/all/0/1&quot;&gt;Eduard Eiben&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganian_R/0/1/0/all/0/1&quot;&gt;Robert Ganian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanj_I/0/1/0/all/0/1&quot;&gt;Iyad Kanj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07392">
<title>ReRoGCRL: Representation-based Robustness in Goal-Conditioned Reinforcement Learning. (arXiv:2312.07392v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07392</link>
<description rdf:parseType="Literal">&lt;p&gt;While Goal-Conditioned Reinforcement Learning (GCRL) has gained attention,
its algorithmic robustness against adversarial perturbations remains
unexplored. The attacks and robust representation training methods that are
designed for traditional RL become less effective when applied to GCRL. To
address this challenge, we first propose the Semi-Contrastive Representation
attack, a novel approach inspired by the adversarial contrastive attack. Unlike
existing attacks in RL, it only necessitates information from the policy
function and can be seamlessly implemented during deployment. Then, to mitigate
the vulnerability of existing GCRL algorithms, we introduce Adversarial
Representation Tactics, which combines Semi-Contrastive Adversarial
Augmentation with Sensitivity-Aware Regularizer to improve the adversarial
robustness of the underlying RL agent against various types of perturbations.
Extensive experiments validate the superior performance of our attack and
defence methods across multiple state-of-the-art GCRL algorithms. Our tool
ReRoGCRL is available at https://github.com/TrustAI/ReRoGCRL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xiangyu Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Sihao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaxu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1&quot;&gt;Meng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xingyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1&quot;&gt;Wenjie Ruan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07398">
<title>LLMEval: A Preliminary Study on How to Evaluate Large Language Models. (arXiv:2312.07398v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07398</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the evaluation of Large Language Models has emerged as a popular
area of research. The three crucial questions for LLM evaluation are ``what,
where, and how to evaluate&apos;&apos;. However, the existing research mainly focuses on
the first two questions, which are basically what tasks to give the LLM during
testing and what kind of knowledge it should deal with. As for the third
question, which is about what standards to use, the types of evaluators, how to
score, and how to rank, there hasn&apos;t been much discussion. In this paper, we
analyze evaluation methods by comparing various criteria with both manual and
automatic evaluation, utilizing onsite, crowd-sourcing, public annotators and
GPT-4, with different scoring methods and ranking systems. We propose a new
dataset, LLMEval and conduct evaluations on 20 LLMs. A total of 2,186
individuals participated, leading to the generation of 243,337 manual
annotations and 57,511 automatic evaluation results. We perform comparisons and
analyses of different settings and conduct 10 conclusions that can provide some
insights for evaluating LLM in the future. The dataset and the results are
publicly available at https://github.com/llmeval .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Haipeng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shichun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yongyao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1&quot;&gt;Tao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07492">
<title>SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models. (arXiv:2312.07492v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07492</link>
<description rdf:parseType="Literal">&lt;p&gt;Current datasets for unwanted social bias auditing are limited to studying
protected demographic features such as race and gender. In this work, we
introduce a comprehensive benchmark that is meant to capture the amplification
of social bias, via stigmas, in generative language models. Taking inspiration
from social science research, we start with a documented list of 93 US-centric
stigmas and curate a question-answering (QA) dataset which involves simple
social situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts,
with a variety of prompt styles, carefully constructed to systematically test
for both social bias and model robustness. We present results for
SocialStigmaQA with two open source generative language models and we find that
the proportion of socially biased output ranges from 45% to 59% across a
variety of decoding strategies and prompting styles. We demonstrate that the
deliberate design of the templates in our benchmark (e.g., adding biasing text
to the prompt or using different verbs that change the answer that indicates
bias) impacts the model tendencies to generate socially biased output.
Additionally, through manual evaluation, we discover problematic patterns in
the generated chain-of-thought output that range from subtle bias to lack of
reasoning.
&lt;/p&gt;
&lt;p&gt;Warning: This paper contains examples of text which are toxic, biased, and
potentially harmful.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagireddy_M/0/1/0/all/0/1&quot;&gt;Manish Nagireddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiazor_L/0/1/0/all/0/1&quot;&gt;Lamogha Chiazor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Moninder Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1&quot;&gt;Ioana Baldini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07696">
<title>Real-time Network Intrusion Detection via Decision Transformers. (arXiv:2312.07696v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07696</link>
<description rdf:parseType="Literal">&lt;p&gt;Many cybersecurity problems that require real-time decision-making based on
temporal observations can be abstracted as a sequence modeling problem, e.g.,
network intrusion detection from a sequence of arriving packets. Existing
approaches like reinforcement learning may not be suitable for such
cybersecurity decision problems, since the Markovian property may not
necessarily hold and the underlying network states are often not observable. In
this paper, we cast the problem of real-time network intrusion detection as
casual sequence modeling and draw upon the power of the transformer
architecture for real-time decision-making. By conditioning a causal decision
transformer on past trajectories, consisting of the rewards, network packets,
and detection decisions, our proposed framework will generate future detection
decisions to achieve the desired return. It enables decision transformers to be
applied to real-time network intrusion detection, as well as a novel tradeoff
between the accuracy and timeliness of detection. The proposed solution is
evaluated on public network intrusion detection datasets and outperforms
several baseline algorithms using reinforcement learning and sequence modeling,
in terms of detection accuracy and timeliness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jingdi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hanhan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_Y/0/1/0/all/0/1&quot;&gt;Yongsheng Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adam_G/0/1/0/all/0/1&quot;&gt;Gina Adam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bastian_N/0/1/0/all/0/1&quot;&gt;Nathaniel D. Bastian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1&quot;&gt;Tian Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07753">
<title>Polynomial-based Self-Attention for Table Representation learning. (arXiv:2312.07753v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07753</link>
<description rdf:parseType="Literal">&lt;p&gt;Structured data, which constitutes a significant portion of existing data
types, has been a long-standing research topic in the field of machine
learning. Various representation learning methods for tabular data have been
proposed, ranging from encoder-decoder structures to Transformers. Among these,
Transformer-based methods have achieved state-of-the-art performance not only
in tabular data but also in various other fields, including computer vision and
natural language processing. However, recent studies have revealed that
self-attention, a key component of Transformers, can lead to an oversmoothing
issue. We show that Transformers for tabular data also face this problem, and
to address the problem, we propose a novel matrix polynomial-based
self-attention layer as a substitute for the original self-attention layer,
which enhances model scalability. In our experiments with three representative
table learning models equipped with our proposed layer, we illustrate that the
layer effectively mitigates the oversmoothing problem and enhances the
representation performance of the existing methods, outperforming the
state-of-the-art table representation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jayoung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1&quot;&gt;Yehjin Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jeongwhan Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wi_H/0/1/0/all/0/1&quot;&gt;Hyowon Wi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1&quot;&gt;Noseong Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08064">
<title>Exploring the Impact of Lay User Feedback for Improving AI Fairness. (arXiv:2312.08064v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08064</link>
<description rdf:parseType="Literal">&lt;p&gt;Fairness in AI is a growing concern for high-stakes decision making. Engaging
stakeholders, especially lay users, in fair AI development is promising yet
overlooked. Recent efforts explore enabling lay users to provide AI
fairness-related feedback, but there is still a lack of understanding of how to
integrate users&apos; feedback into an AI model and the impacts of doing so. To
bridge this gap, we collected feedback from 58 lay users on the fairness of a
XGBoost model trained on the Home Credit dataset, and conducted offline
experiments to investigate the effects of retraining models on accuracy, and
individual and group fairness. Our work contributes baseline results of
integrating user fairness feedback in XGBoost, and a dataset and code framework
to bootstrap research in engaging stakeholders in AI fairness. Our discussion
highlights the challenges of employing user feedback in AI fairness and points
the way to a future application area of interactive machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taka_E/0/1/0/all/0/1&quot;&gt;Evdoxia Taka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakao_Y/0/1/0/all/0/1&quot;&gt;Yuri Nakao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonoda_R/0/1/0/all/0/1&quot;&gt;Ryosuke Sonoda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yokota_T/0/1/0/all/0/1&quot;&gt;Takuya Yokota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Lin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stumpf_S/0/1/0/all/0/1&quot;&gt;Simone Stumpf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08533">
<title>World Models via Policy-Guided Trajectory Diffusion. (arXiv:2312.08533v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08533</link>
<description rdf:parseType="Literal">&lt;p&gt;World models are a powerful tool for developing intelligent agents. By
predicting the outcome of a sequence of actions, world models enable policies
to be optimised via on-policy reinforcement learning (RL) using synthetic data,
i.e. in &quot;in imagination&quot;. Existing world models are autoregressive in that they
interleave predicting the next state with sampling the next action from the
policy. Prediction error inevitably compounds as the trajectory length grows.
In this work, we propose a novel world modelling approach that is not
autoregressive and generates entire on-policy trajectories in a single pass
through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion
(PolyGRAD), leverages a denoising model in addition to the gradient of the
action distribution of the policy to diffuse a trajectory of initially random
states and actions into an on-policy synthetic trajectory. We analyse the
connections between PolyGRAD, score-based generative models, and
classifier-guided diffusion models. Our results demonstrate that PolyGRAD
outperforms state-of-the-art baselines in terms of trajectory prediction error
for moderate-length trajectories, with the exception of autoregressive
diffusion. At short horizons, PolyGRAD obtains comparable errors to
autoregressive diffusion, but with significantly lower computational
requirements. Our experiments also demonstrate that PolyGRAD enables performant
policies to be trained via on-policy RL in imagination for MuJoCo continuous
control domains. Thus, PolyGRAD introduces a new paradigm for scalable and
non-autoregressive on-policy world modelling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rigter_M/0/1/0/all/0/1&quot;&gt;Marc Rigter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamada_J/0/1/0/all/0/1&quot;&gt;Jun Yamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Posner_I/0/1/0/all/0/1&quot;&gt;Ingmar Posner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08656">
<title>MaxK-GNN: Towards Theoretical Speed Limits for Accelerating Graph Neural Networks Training. (arXiv:2312.08656v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08656</link>
<description rdf:parseType="Literal">&lt;p&gt;In the acceleration of deep neural network training, the GPU has become the
mainstream platform. GPUs face substantial challenges on GNNs, such as workload
imbalance and memory access irregularities, leading to underutilized hardware.
Existing solutions such as PyG, DGL with cuSPARSE, and GNNAdvisor frameworks
partially address these challenges but memory traffic is still significant.
&lt;/p&gt;
&lt;p&gt;We argue that drastic performance improvements can only be achieved by the
vertical optimization of algorithm and system innovations, rather than treating
the speedup optimization as an &quot;after-thought&quot; (i.e., (i) given a GNN
algorithm, designing an accelerator, or (ii) given hardware, mainly optimizing
the GNN algorithm). In this paper, we present MaxK-GNN, an advanced
high-performance GPU training system integrating algorithm and system
innovation. (i) We introduce the MaxK nonlinearity and provide a theoretical
analysis of MaxK nonlinearity as a universal approximator, and present the
Compressed Balanced Sparse Row (CBSR) format, designed to store the data and
index of the feature matrix after nonlinearity; (ii) We design a coalescing
enhanced forward computation with row-wise product-based SpGEMM Kernel using
CBSR for input feature matrix fetching and strategic placement of a sparse
output accumulation buffer in shared memory; (iii) We develop an optimized
backward computation with outer product-based and SSpMM Kernel.
&lt;/p&gt;
&lt;p&gt;We conduct extensive evaluations of MaxK-GNN and report the end-to-end system
run-time. Experiments show that MaxK-GNN system could approach the theoretical
speedup limit according to Amdahl&apos;s law. We achieve comparable accuracy to SOTA
GNNs, but at a significantly increased speed: 3.22/4.24 times speedup (vs.
theoretical limits, 5.52/7.27 times) on Reddit compared to DGL and GNNAdvisor
implementations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Hongwu Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shivdikar_K/0/1/0/all/0/1&quot;&gt;Kaustubh Shivdikar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1&quot;&gt;MD Amit Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jiahui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shaoyi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_O/0/1/0/all/0/1&quot;&gt;Omer Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaeli_D/0/1/0/all/0/1&quot;&gt;David Kaeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Caiwen Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08726">
<title>Labels Need Prompts Too: Mask Matching for Natural Language Understanding Tasks. (arXiv:2312.08726v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08726</link>
<description rdf:parseType="Literal">&lt;p&gt;Textual label names (descriptions) are typically semantically rich in many
natural language understanding (NLU) tasks. In this paper, we incorporate the
prompting methodology, which is widely used to enrich model input, into the
label side for the first time. Specifically, we propose a Mask Matching method,
which equips an input with a prompt and its label with another, and then makes
predictions by matching their mask representations. We evaluate our method
extensively on 8 NLU tasks with 14 datasets. The experimental results show that
Mask Matching significantly outperforms its counterparts of fine-tuning and
conventional prompt-tuning, setting up state-of-the-art performances in several
datasets. Mask Matching is particularly good at handling NLU tasks with large
label counts and informative label names. As pioneering efforts that
investigate the label-side prompt, we also discuss open issues for future
study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wei Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Quansen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shikun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08763">
<title>Learning from Polar Representation: An Extreme-Adaptive Model for Long-Term Time Series Forecasting. (arXiv:2312.08763v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08763</link>
<description rdf:parseType="Literal">&lt;p&gt;In the hydrology field, time series forecasting is crucial for efficient
water resource management, improving flood and drought control and increasing
the safety and quality of life for the general population. However, predicting
long-term streamflow is a complex task due to the presence of extreme events.
It requires the capture of long-range dependencies and the modeling of rare but
important extreme values. Existing approaches often struggle to tackle these
dual challenges simultaneously. In this paper, we specifically delve into these
issues and propose Distance-weighted Auto-regularized Neural network (DAN), a
novel extreme-adaptive model for long-range forecasting of stremflow enhanced
by polar representation learning. DAN utilizes a distance-weighted multi-loss
mechanism and stackable blocks to dynamically refine indicator sequences from
exogenous data, while also being able to handle uni-variate time-series by
employing Gaussian Mixture probability modeling to improve robustness to severe
events. We also introduce Kruskal-Wallis sampling and gate control vectors to
handle imbalanced extreme data. On four real-life hydrologic streamflow
datasets, we demonstrate that DAN significantly outperforms both
state-of-the-art hydrologic time series prediction methods and general methods
designed for long-term time series prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanhong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jack Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anastasiu_D/0/1/0/all/0/1&quot;&gt;David C. Anastasiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08793">
<title>Forbidden Facts: An Investigation of Competing Objectives in Llama-2. (arXiv:2312.08793v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08793</link>
<description rdf:parseType="Literal">&lt;p&gt;LLMs often face competing pressures (for example helpfulness vs.
harmlessness). To understand how models resolve such conflicts, we study
Llama-2-chat models on the forbidden fact task. Specifically, we instruct
Llama-2 to truthfully complete a factual recall statement while forbidding it
from saying the correct answer. This often makes the model give incorrect
answers. We decompose Llama-2 into 1000+ components, and rank each one with
respect to how useful it is for forbidding the correct answer. We find that in
aggregate, around 35 components are enough to reliably implement the full
suppression behavior. However, these components are fairly heterogeneous and
many operate using faulty heuristics. We discover that one of these heuristics
can be exploited via a manually designed adversarial attack which we call The
California Attack. Our results highlight some roadblocks standing in the way of
being able to successfully interpret advanced ML systems. Project website
available at https://forbiddenfacts.github.io .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tony T. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Miles Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hariharan_K/0/1/0/all/0/1&quot;&gt;Kaivalya Hariharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shavit_N/0/1/0/all/0/1&quot;&gt;Nir Shavit&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08926">
<title>Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent. (arXiv:2312.08926v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08926</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) face challenges in solving complex mathematical
problems that require comprehensive capacities to parse the statements,
associate domain knowledge, perform compound logical reasoning, and integrate
the intermediate rationales. Tackling all these problems once could be arduous
for LLMs, thus leading to confusion in generation. In this work, we explore the
potential of enhancing LLMs with agents by meticulous decomposition and
modeling of mathematical reasoning process. Specifically, we propose a formal
description of the mathematical solving and extend LLMs with an agent-based
zero-shot framework named
$\bf{P}$lanner-$\bf{R}$easoner-$\bf{E}$xecutor-$\bf{R}$eflector (PRER). We
further provide and implement two MathAgents that define the logical forms and
inherent relations via a pool of actions in different grains and orientations:
MathAgent-M adapts its actions to LLMs, while MathAgent-H aligns with
humankind. Experiments on miniF2F and MATH have demonstrated the effectiveness
of PRER and proposed MathAgents, achieving an increase of
$12.3\%$($53.9\%\xrightarrow{}66.2\%$) on the MiniF2F, $9.2\%$
($49.8\%\xrightarrow{}59.0\%$) on MATH, and
$13.2\%$($23.2\%\xrightarrow{}35.4\%$) for level-5 problems of MATH against
GPT-4. Further analytical results provide more insightful perspectives on
exploiting the behaviors of LLMs as agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1&quot;&gt;Haoran Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1&quot;&gt;Qinyi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shaohua Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Hao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yanyan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jidong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yaohui Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09323">
<title>Perspectives on the State and Future of Deep Learning -- 2023. (arXiv:2312.09323v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09323</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of this series is to chronicle opinions and issues in the field of
machine learning as they stand today and as they change over time. The plan is
to host this survey periodically until the AI singularity
paperclip-frenzy-driven doomsday, keeping an updated list of topical questions
and interviewing new community members for each edition. In this issue, we
probed people&apos;s opinions on interpretable AI, the value of benchmarking in
modern NLP, the state of progress towards understanding deep learning, and the
future of academia.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1&quot;&gt;Micah Goldblum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1&quot;&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1&quot;&gt;Richard Baraniuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1&quot;&gt;Tom Goldstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1&quot;&gt;Zachary C Lipton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1&quot;&gt;Melanie Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakkiran_P/0/1/0/all/0/1&quot;&gt;Preetum Nakkiran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1&quot;&gt;Max Welling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1&quot;&gt;Andrew Gordon Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09857">
<title>Deep Unsupervised Domain Adaptation for Time Series Classification: a Benchmark. (arXiv:2312.09857v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09857</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised Domain Adaptation (UDA) aims to harness labeled source data to
train models for unlabeled target data. Despite extensive research in domains
like computer vision and natural language processing, UDA remains underexplored
for time series data, which has widespread real-world applications ranging from
medicine and manufacturing to earth observation and human activity recognition.
Our paper addresses this gap by introducing a comprehensive benchmark for
evaluating UDA techniques for time series classification, with a focus on deep
learning methods. We provide seven new benchmark datasets covering various
domain shifts and temporal dynamics, facilitating fair and standardized UDA
method assessments with state of the art neural network backbones (e.g.
Inception) for time series data. This benchmark offers insights into the
strengths and limitations of the evaluated approaches while preserving the
unsupervised nature of domain adaptation, making it directly applicable to
practical problems. Our paper serves as a vital resource for researchers and
practitioners, advancing domain adaptation solutions for time series data and
fostering innovation in this critical field. The implementation code of this
benchmark is available at https://github.com/EricssonResearch/UDA-4-TSC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fawaz_H/0/1/0/all/0/1&quot;&gt;Hassan Ismail Fawaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosso_G/0/1/0/all/0/1&quot;&gt;Ganesh Del Grosso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerdoncuff_T/0/1/0/all/0/1&quot;&gt;Tanguy Kerdoncuff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boisbunon_A/0/1/0/all/0/1&quot;&gt;Aurelie Boisbunon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saffar_I/0/1/0/all/0/1&quot;&gt;Illyyne Saffar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09983">
<title>Toward Computationally Efficient Inverse Reinforcement Learning via Reward Shaping. (arXiv:2312.09983v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09983</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse reinforcement learning (IRL) is computationally challenging, with
common approaches requiring the solution of multiple reinforcement learning
(RL) sub-problems. This work motivates the use of potential-based reward
shaping to reduce the computational burden of each RL sub-problem. This work
serves as a proof-of-concept and we hope will inspire future developments
towards computationally efficient IRL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cooke_L/0/1/0/all/0/1&quot;&gt;Lauren H. Cooke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klyne_H/0/1/0/all/0/1&quot;&gt;Harvey Klyne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Edwin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laidlaw_C/0/1/0/all/0/1&quot;&gt;Cassidy Laidlaw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1&quot;&gt;Milind Tambe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1&quot;&gt;Finale Doshi-Velez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10029">
<title>Challenges with unsupervised LLM knowledge discovery. (arXiv:2312.10029v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10029</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that existing unsupervised methods on large language model (LLM)
activations do not discover knowledge -- instead they seem to discover whatever
feature of the activations is most prominent. The idea behind unsupervised
knowledge elicitation is that knowledge satisfies a consistency structure,
which can be used to discover knowledge. We first prove theoretically that
arbitrary features (not just knowledge) satisfy the consistency structure of a
particular leading unsupervised knowledge-elicitation method,
contrast-consistent search (Burns et al. - &lt;a href=&quot;/abs/2212.03827&quot;&gt;arXiv:2212.03827&lt;/a&gt;). We then present a
series of experiments showing settings in which unsupervised methods result in
classifiers that do not predict knowledge, but instead predict a different
prominent feature. We conclude that existing unsupervised methods for
discovering latent knowledge are insufficient, and we contribute sanity checks
to apply to evaluating future knowledge elicitation methods. Conceptually, we
hypothesise that the identification issues explored here, e.g. distinguishing a
model&apos;s knowledge from that of a simulated character&apos;s, will persist for future
unsupervised methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farquhar_S/0/1/0/all/0/1&quot;&gt;Sebastian Farquhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varma_V/0/1/0/all/0/1&quot;&gt;Vikrant Varma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kenton_Z/0/1/0/all/0/1&quot;&gt;Zachary Kenton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gasteiger_J/0/1/0/all/0/1&quot;&gt;Johannes Gasteiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikulik_V/0/1/0/all/0/1&quot;&gt;Vladimir Mikulik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Rohin Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.01402">
<title>GALAXY: Graph-based Active Learning at the Extreme. (arXiv:2202.01402v2 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2202.01402</link>
<description rdf:parseType="Literal">&lt;p&gt;Active learning is a label-efficient approach to train highly effective
models while interactively selecting only small subsets of unlabelled data for
labelling and training. In &quot;open world&quot; settings, the classes of interest can
make up a small fraction of the overall dataset -- most of the data may be
viewed as an out-of-distribution or irrelevant class. This leads to extreme
class-imbalance, and our theory and methods focus on this core issue. We
propose a new strategy for active learning called GALAXY (Graph-based Active
Learning At the eXtrEme), which blends ideas from graph-based active learning
and deep learning. GALAXY automatically and adaptively selects more
class-balanced examples for labeling than most other methods for active
learning. Our theory shows that GALAXY performs a refined form of uncertainty
sampling that gathers a much more class-balanced dataset than vanilla
uncertainty sampling. Experimentally, we demonstrate GALAXY&apos;s superiority over
existing state-of-art deep active learning algorithms in unbalanced vision
classification settings generated from popular datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katz_Samuels_J/0/1/0/all/0/1&quot;&gt;Julian Katz-Samuels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1&quot;&gt;Robert Nowak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.07317">
<title>Algorithm Selection for Deep Active Learning with Imbalanced Datasets. (arXiv:2302.07317v3 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2302.07317</link>
<description rdf:parseType="Literal">&lt;p&gt;Label efficiency has become an increasingly important objective in deep
learning applications. Active learning aims to reduce the number of labeled
examples needed to train deep networks, but the empirical performance of active
learning algorithms can vary dramatically across datasets and applications. It
is difficult to know in advance which active learning strategy will perform
well or best in a given application. To address this, we propose the first
adaptive algorithm selection strategy for deep active learning. For any
unlabeled dataset, our (meta) algorithm TAILOR (Thompson ActIve Learning
algORithm selection) iteratively and adaptively chooses among a set of
candidate active learning algorithms. TAILOR uses novel reward functions aimed
at gathering class-balanced examples. Extensive experiments in multi-class and
multi-label applications demonstrate TAILOR&apos;s effectiveness in achieving
accuracy comparable or better than that of the best of the candidate
algorithms. Our implementation of TAILOR is open-sourced at
https://github.com/jifanz/TAILOR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1&quot;&gt;Shuai Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1&quot;&gt;Saurabh Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1&quot;&gt;Robert Nowak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08870">
<title>UPGPT: Universal Diffusion Model for Person Image Generation, Editing and Pose Transfer. (arXiv:2304.08870v2 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2304.08870</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image models (T2I) such as StableDiffusion have been used to generate
high quality images of people. However, due to the random nature of the
generation process, the person has a different appearance e.g. pose, face, and
clothing, despite using the same text prompt. The appearance inconsistency
makes T2I unsuitable for pose transfer. We address this by proposing a
multimodal diffusion model that accepts text, pose, and visual prompting. Our
model is the first unified method to perform all person image tasks -
generation, pose transfer, and mask-less edit. We also pioneer using small
dimensional 3D body model parameters directly to demonstrate new capability -
simultaneous pose and camera view interpolation while maintaining the person&apos;s
appearance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheong_S/0/1/0/all/0/1&quot;&gt;Soon Yau Cheong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustafa_A/0/1/0/all/0/1&quot;&gt;Armin Mustafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1&quot;&gt;Andrew Gilbert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09910">
<title>LabelBench: A Comprehensive Framework for Benchmarking Adaptive Label-Efficient Learning. (arXiv:2306.09910v2 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2306.09910</link>
<description rdf:parseType="Literal">&lt;p&gt;Labeled data are critical to modern machine learning applications, but
obtaining labels can be expensive. To mitigate this cost, machine learning
methods, such as transfer learning, semi-supervised learning and active
learning, aim to be label-efficient: achieving high predictive performance from
relatively few labeled examples. While obtaining the best label-efficiency in
practice often requires combinations of these techniques, existing benchmark
and evaluation frameworks do not capture a concerted combination of all such
techniques. This paper addresses this deficiency by introducing LabelBench, a
new computationally-efficient framework for joint evaluation of multiple
label-efficient learning techniques. As an application of LabelBench, we
introduce a novel benchmark of state-of-the-art active learning methods in
combination with semi-supervised learning for fine-tuning pretrained vision
transformers. Our benchmark demonstrates better label-efficiencies than
previously reported in active learning. LabelBench&apos;s modular codebase is
open-sourced for the broader community to contribute label-efficient learning
methods and benchmarks. The repository can be found at:
https://github.com/EfficientTraining/LabelBench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canal_G/0/1/0/all/0/1&quot;&gt;Gregory Canal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mussmann_S/0/1/0/all/0/1&quot;&gt;Stephen Mussmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Arnav M. Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1&quot;&gt;Gantavya Bhatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yinglun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Simon Shaolei Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1&quot;&gt;Kevin Jamieson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1&quot;&gt;Robert D Nowak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06330">
<title>Smart Agent-Based Modeling: On the Use of Large Language Models in Computer Simulations. (arXiv:2311.06330v4 [cs.AI] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2311.06330</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer simulations offer a robust toolset for exploring complex systems
across various disciplines. A particularly impactful approach within this realm
is Agent-Based Modeling (ABM), which harnesses the interactions of individual
agents to emulate intricate system dynamics. ABM&apos;s strength lies in its
bottom-up methodology, illuminating emergent phenomena by modeling the
behaviors of individual components of a system. Yet, ABM has its own set of
challenges, notably its struggle with modeling natural language instructions
and common sense in mathematical equations or rules. This paper seeks to
transcend these boundaries by integrating Large Language Models (LLMs) like GPT
into ABM. This amalgamation gives birth to a novel framework, Smart Agent-Based
Modeling (SABM). Building upon the concept of smart agents -- entities
characterized by their intelligence, adaptability, and computation ability --
we explore in the direction of utilizing LLM-powered agents to simulate
real-world scenarios with increased nuance and realism. In this comprehensive
exploration, we elucidate the state of the art of ABM, introduce SABM&apos;s
potential and methodology, and present three case studies (source codes
available at https://github.com/Roihn/SABM), demonstrating the SABM methodology
and validating its effectiveness in modeling real-world systems. Furthermore,
we cast a vision towards several aspects of the future of SABM, anticipating a
broader horizon for its applications. Through this endeavor, we aspire to
redefine the boundaries of computer simulations, enabling a more profound
understanding of complex systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zengqing Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1&quot;&gt;Run Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shuyuan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yixin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chuan Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03154">
<title>ViscoNet: Bridging and Harmonizing Visual and Textual Conditioning for ControlNet. (arXiv:2312.03154v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2312.03154</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces ViscoNet, a novel method that enhances text-to-image
human generation models with visual prompting. Unlike existing methods that
rely on lengthy text descriptions to control the image structure, ViscoNet
allows users to specify the visual appearance of the target object with a
reference image. ViscoNet disentangles the object&apos;s appearance from the image
background and injects it into a pre-trained latent diffusion model (LDM) model
via a ControlNet branch. This way, ViscoNet mitigates the style mode collapse
problem and enables precise and flexible visual control. We demonstrate the
effectiveness of ViscoNet on human image generation, where it can manipulate
visual attributes and artistic styles with text and image prompts. We also show
that ViscoNet can learn visual conditioning from small and specific object
domains while preserving the generative power of the LDM backbone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheong_S/0/1/0/all/0/1&quot;&gt;Soon Yau Cheong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustafa_A/0/1/0/all/0/1&quot;&gt;Armin Mustafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1&quot;&gt;Andrew Gilbert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09196">
<title>DIRECT: Deep Active Learning under Imbalance and Label Noise. (arXiv:2312.09196v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2312.09196</link>
<description rdf:parseType="Literal">&lt;p&gt;Class imbalance is a prevalent issue in real world machine learning
applications, often leading to poor performance in rare and minority classes.
With an abundance of wild unlabeled data, active learning is perhaps the most
effective technique in solving the problem at its root -- collecting a more
balanced and informative set of labeled examples during annotation. In this
work, we propose a novel algorithm that first identifies the class separation
threshold and then annotate the most uncertain examples from the minority
classes, close to the separation threshold. Through a novel reduction to
one-dimensional active learning, our algorithm DIRECT is able to leverage the
classic active learning literature to address issues such as batch labeling and
tolerance towards label noise. Compared to existing algorithms, our algorithm
saves more than 15\% of the annotation budget compared to state-of-art active
learning algorithm and more than 90\% of annotation budget compared to random
sampling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nuggehalli_S/0/1/0/all/0/1&quot;&gt;Shyam Nuggehalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_L/0/1/0/all/0/1&quot;&gt;Lalit Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1&quot;&gt;Robert Nowak&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>