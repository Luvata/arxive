<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-21T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10264" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10266" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10268" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10271" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10272" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10278" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10279" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10280" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10282" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10284" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10289" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10299" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10341" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10348" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10359" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10372" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10394" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10415" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10428" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10431" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10447" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10474" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10480" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10510" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10603" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10640" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10641" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10642" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10643" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10660" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10685" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10690" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10712" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10733" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10744" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10745" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10759" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10816" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10839" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10840" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10841" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10882" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10889" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1812.01243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.06826" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.05359" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13359" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.02506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15954" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.00746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.13189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08565" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.10444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04965" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12955" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03976" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15497" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01185" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03263" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04336" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08326" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09566" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09691" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.10241">
<title>Zero Bubble Pipeline Parallelism. (arXiv:2401.10241v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2401.10241</link>
<description rdf:parseType="Literal">&lt;p&gt;Pipeline parallelism is one of the key components for large-scale distributed
training, yet its efficiency suffers from pipeline bubbles which were deemed
inevitable. In this work, we introduce a scheduling strategy that, to our
knowledge, is the first to successfully achieve zero pipeline bubbles under
synchronous training semantics. The key idea behind this improvement is to
split the backward computation into two parts, one that computes gradient for
the input and another that computes for the parameters. Based on this idea, we
handcraft novel pipeline schedules that significantly outperform the baseline
methods. We further develop an algorithm that automatically finds an optimal
schedule based on specific model configuration and memory limit. Additionally,
to truly achieve zero bubble, we introduce a novel technique to bypass
synchronizations during the optimizer step. Experimental evaluations show that
our method outperforms the 1F1B schedule up to 23% in throughput under a
similar memory limit. This number can be further pushed to 31% when the memory
constraint is relaxed. We believe our results mark a major step forward in
harnessing the true potential of pipeline parallelism. We open sourced our
implementation based on the popular Megatron-LM repository on
https://github.com/sail-sg/zero-bubble-pipeline-parallelism.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_P/0/1/0/all/0/1&quot;&gt;Penghui Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xinyi Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Guangxing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Min Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10262">
<title>Null Space Properties of Neural Networks with Applications to Image Steganography. (arXiv:2401.10262v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10262</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the null space properties of neural networks. We extend
the null space definition from linear to nonlinear maps and discuss the
presence of a null space in neural networks. The null space of a given neural
network can tell us the part of the input data that makes no contribution to
the final prediction so that we can use it to trick the neural network. This
reveals an inherent weakness in neural networks that can be exploited. One
application described here leads to a method of image steganography. Through
experiments on image datasets such as MNIST, we show that we can use null space
components to force the neural network to choose a selected hidden image class,
even though the overall image can be made to look like a completely different
image. We conclude by showing comparisons between what a human viewer would
see, and the part of the image that the neural network is actually using to
make predictions and, hence, show that what the neural network ``sees&apos;&apos; is
completely different than what we would expect.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Short_K/0/1/0/all/0/1&quot;&gt;Kevin M. Short&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10264">
<title>Harnessing Transparent Learning Analytics for Individualized Support through Auto-detection of Engagement in Face-to-Face Collaborative Learning. (arXiv:2401.10264v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.10264</link>
<description rdf:parseType="Literal">&lt;p&gt;Using learning analytics to investigate and support collaborative learning
has been explored for many years. Recently, automated approaches with various
artificial intelligence approaches have provided promising results for
modelling and predicting student engagement and performance in collaborative
learning tasks. However, due to the lack of transparency and interpretability
caused by the use of &quot;black box&quot; approaches in learning analytics design and
implementation, guidance for teaching and learning practice may become a
challenge. On the one hand, the black box created by machine learning
algorithms and models prevents users from obtaining educationally meaningful
learning and teaching suggestions. On the other hand, focusing on group and
cohort level analysis only can make it difficult to provide specific support
for individual students working in collaborative groups. This paper proposes a
transparent approach to automatically detect student&apos;s individual engagement in
the process of collaboration. The results show that the proposed approach can
reflect student&apos;s individual engagement and can be used as an indicator to
distinguish students with different collaborative learning challenges
(cognitive, behavioural and emotional) and learning outcomes. The potential of
the proposed collaboration analytics approach for scaffolding collaborative
learning practice in face-to-face contexts is discussed and future research
suggestions are provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suraworachet_W/0/1/0/all/0/1&quot;&gt;Wannapon Suraworachet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cukurova_M/0/1/0/all/0/1&quot;&gt;Mutlu Cukurova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10266">
<title>Intelligent Condition Monitoring of Industrial Plants: An Overview of Methodologies and Uncertainty Management Strategies. (arXiv:2401.10266v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10266</link>
<description rdf:parseType="Literal">&lt;p&gt;Condition monitoring plays a significant role in the safety and reliability
of modern industrial systems. Artificial intelligence (AI) approaches are
gaining attention from academia and industry as a growing subject in industrial
applications and as a powerful way of identifying faults. This paper provides
an overview of intelligent condition monitoring and fault detection and
diagnosis methods for industrial plants with a focus on the open-source
benchmark Tennessee Eastman Process (TEP). In this survey, the most popular and
state-of-the-art deep learning (DL) and machine learning (ML) algorithms for
industrial plant condition monitoring, fault detection, and diagnosis are
summarized and the advantages and disadvantages of each algorithm are studied.
Challenges like imbalanced data, unlabelled samples and how deep learning
models can handle them are also covered. Finally, a comparison of the
accuracies and specifications of different algorithms utilizing the Tennessee
Eastman Process (TEP) is conducted. This research will be beneficial for both
researchers who are new to the field and experts, as it covers the literature
on condition monitoring and state-of-the-art methods alongside the challenges
and possible solutions to them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahang_M/0/1/0/all/0/1&quot;&gt;Maryam Ahang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charter_T/0/1/0/all/0/1&quot;&gt;Todd Charter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ogunfowora_O/0/1/0/all/0/1&quot;&gt;Oluwaseyi Ogunfowora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khadivi_M/0/1/0/all/0/1&quot;&gt;Maziyar Khadivi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbasi_M/0/1/0/all/0/1&quot;&gt;Mostafa Abbasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najjaran_H/0/1/0/all/0/1&quot;&gt;Homayoun Najjaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10267">
<title>HyperSense: Accelerating Hyper-Dimensional Computing for Intelligent Sensor Data Processing. (arXiv:2401.10267v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2401.10267</link>
<description rdf:parseType="Literal">&lt;p&gt;Introducing HyperSense, our co-designed hardware and software system
efficiently controls Analog-to-Digital Converter (ADC) modules&apos; data generation
rate based on object presence predictions in sensor data. Addressing challenges
posed by escalating sensor quantities and data rates, HyperSense reduces
redundant digital data using energy-efficient low-precision ADC, diminishing
machine learning system costs. Leveraging neurally-inspired HyperDimensional
Computing (HDC), HyperSense analyzes real-time raw low-precision sensor data,
offering advantages in handling noise, memory-centricity, and real-time
learning.
&lt;/p&gt;
&lt;p&gt;Our proposed HyperSense model combines high-performance software for object
detection with real-time hardware prediction, introducing the novel concept of
Intelligent Sensor Control. Comprehensive software and hardware evaluations
demonstrate our solution&apos;s superior performance, evidenced by the highest Area
Under the Curve (AUC) and sharpest Receiver Operating Characteristic (ROC)
curve among lightweight models. Hardware-wise, our FPGA-based domain-specific
accelerator tailored for HyperSense achieves a 5.6x speedup compared to YOLOv4
on NVIDIA Jetson Orin while showing up to 92.1% energy saving compared to the
conventional system. These results underscore HyperSense&apos;s effectiveness and
efficiency, positioning it as a promising solution for intelligent sensing and
real-time data processing across diverse applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1&quot;&gt;Sanggeon Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanning Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masukawa_R/0/1/0/all/0/1&quot;&gt;Ryozo Masukawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barkam_H/0/1/0/all/0/1&quot;&gt;Hamza Errahmouni Barkam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_A/0/1/0/all/0/1&quot;&gt;Andrew Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenjun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezvani_A/0/1/0/all/0/1&quot;&gt;Arghavan Rezvani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Angizi_S/0/1/0/all/0/1&quot;&gt;Shaahin Angizi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imani_M/0/1/0/all/0/1&quot;&gt;Mohsen Imani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10268">
<title>The complementary contributions of academia and industry to AI research. (arXiv:2401.10268v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.10268</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) has seen tremendous development in industry and
academia. However, striking recent advances by industry have stunned the world,
inviting a fresh perspective on the role of academic research in this field.
Here, we characterize the impact and type of AI produced by both environments
over the last 25 years and establish several patterns. We find that articles
published by teams consisting exclusively of industry researchers tend to get
greater attention, with a higher chance of being highly cited and
citation-disruptive, and several times more likely to produce state-of-the-art
models. In contrast, we find that exclusively academic teams publish the bulk
of AI research and tend to produce higher novelty work, with single papers
having several times higher likelihood of being unconventional and atypical.
The respective impact-novelty advantages of industry and academia are robust to
controls for subfield, team size, seniority, and prestige. We find that
academic-industry collaborations struggle to replicate the novelty of academic
teams and tend to look similar to industry teams. Together, our findings
identify the unique and nearly irreplaceable contributions that both academia
and industry make toward the healthy progress of AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Lizhen Liang&lt;/a&gt; (Syracuse University), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_H/0/1/0/all/0/1&quot;&gt;Han Zhuang&lt;/a&gt; (Northeastern University), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt; (Stanford University), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acuna_D/0/1/0/all/0/1&quot;&gt;Daniel E. Acuna&lt;/a&gt; (University of Colorado at Boulder)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10271">
<title>Querying Triadic Concepts through Partial or Complete Matching of Triples. (arXiv:2401.10271v1 [cs.DB])</title>
<link>http://arxiv.org/abs/2401.10271</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a new method for querying triadic concepts
through partial or complete matching of triples using an inverted index, to
retrieve already computed triadic concepts that contain a set of terms in their
extent, intent, and/or modus. As opposed to the approximation approach
described in Ananias, this method (i) does not need to keep the initial triadic
context or its three dyadic counterparts, (ii) avoids the application of
derivation operators on the triple components through context exploration, and
(iii) eliminates the requirement for a factorization phase to get triadic
concepts as the answer to one-dimensional queries. Additionally, our solution
introduces a novel metric for ranking the retrieved triadic concepts based on
their similarity to a given query. Lastly, an empirical study is primarily done
to illustrate the effectiveness and scalability of our approach against the
approximation one. Our solution not only showcases superior efficiency, but
also highlights a better scalability, making it suitable for big data
scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruas_P/0/1/0/all/0/1&quot;&gt;Pedro Henrique B. Ruas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Missaoui_R/0/1/0/all/0/1&quot;&gt;Rokia Missaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1&quot;&gt;Mohamed Hamza Ibrahim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10272">
<title>Multi-Source Collaborative Gradient Discrepancy Minimization for Federated Domain Generalization. (arXiv:2401.10272v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10272</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Domain Generalization aims to learn a domain-invariant model from
multiple decentralized source domains for deployment on unseen target domain.
Due to privacy concerns, the data from different source domains are kept
isolated, which poses challenges in bridging the domain gap. To address this
issue, we propose a Multi-source Collaborative Gradient Discrepancy
Minimization (MCGDM) method for federated domain generalization. Specifically,
we propose intra-domain gradient matching between the original images and
augmented images to avoid overfitting the domain-specific information within
isolated domains. Additionally, we propose inter-domain gradient matching with
the collaboration of other domains, which can further reduce the domain shift
across decentralized domains. Combining intra-domain and inter-domain gradient
matching, our method enables the learned model to generalize well on unseen
domains. Furthermore, our method can be extended to the federated domain
adaptation task by fine-tuning the target model on the pseudo-labeled target
domain. The extensive experiments on federated domain generalization and
adaptation indicate that our method outperforms the state-of-the-art methods
significantly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yikang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yahong Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10273">
<title>Revolutionizing Pharma: Unveiling the AI and LLM Trends in the Pharmaceutical Industry. (arXiv:2401.10273v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.10273</link>
<description rdf:parseType="Literal">&lt;p&gt;This document offers a critical overview of the emerging trends and
significant advancements in artificial intelligence (AI) within the
pharmaceutical industry. Detailing its application across key operational
areas, including research and development, animal testing, clinical trials,
hospital clinical stages, production, regulatory affairs, quality control and
other supporting areas, the paper categorically examines AI&apos;s role in each
sector. Special emphasis is placed on cutting-edge AI technologies like machine
learning algorithms and their contributions to various aspects of
pharmaceutical operations. Through this comprehensive analysis, the paper
highlights the transformative potential of AI in reshaping the pharmaceutical
industry&apos;s future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1&quot;&gt;Jingwen Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10274">
<title>Knowledge-Assisted Dual-Stage Evolutionary Optimization of Large-Scale Crude Oil Scheduling. (arXiv:2401.10274v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2401.10274</link>
<description rdf:parseType="Literal">&lt;p&gt;With the scaling up of crude oil scheduling in modern refineries, large-scale
crude oil scheduling problems (LSCOSPs) emerge with thousands of binary
variables and non-linear constraints, which are challenging to be optimized by
traditional optimization methods. To solve LSCOSPs, we take the practical crude
oil scheduling from a marine-access refinery as an example and start with
modeling LSCOSPs from crude unloading, transportation, crude distillation unit
processing, and inventory management of intermediate products. On the basis of
the proposed model, a dual-stage evolutionary algorithm driven by heuristic
rules (denoted by DSEA/HR) is developed, where the dual-stage search mechanism
consists of global search and local refinement. In the global search stage, we
devise several heuristic rules based on the empirical operating knowledge to
generate a well-performing initial population and accelerate convergence in the
mixed variables space. In the local refinement stage, a repair strategy is
proposed to move the infeasible solutions towards feasible regions by further
optimizing the local continuous variables. During the whole evolutionary
process, the proposed dual-stage framework plays a crucial role in balancing
exploration and exploitation. Experimental results have shown that DSEA/HR
outperforms the state-of-the-art and widely-used mathematical programming
methods and metaheuristic algorithms on LSCOSP instances within a reasonable
time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wanting Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1&quot;&gt;Wei Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Guo Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Renchu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1&quot;&gt;Wenli Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yaochu Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10278">
<title>EEGFormer: Towards Transferable and Interpretable Large-Scale EEG Foundation Model. (arXiv:2401.10278v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2401.10278</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning has emerged as a highly effective approach in the
fields of natural language processing and computer vision. It is also
applicable to brain signals such as electroencephalography (EEG) data, given
the abundance of available unlabeled data that exist in a wide spectrum of
real-world medical applications ranging from seizure detection to wave
analysis. The existing works leveraging self-supervised learning on EEG
modeling mainly focus on pretraining upon each individual dataset corresponding
to a single downstream task, which cannot leverage the power of abundant data,
and they may derive sub-optimal solutions with a lack of generalization.
Moreover, these methods rely on end-to-end model learning which is not easy for
humans to understand. In this paper, we present a novel EEG foundation model,
namely EEGFormer, pretrained on large-scale compound EEG data. The pretrained
model cannot only learn universal representations on EEG signals with adaptable
performance on various downstream tasks but also provide interpretable outcomes
of the useful patterns within the data. To validate the effectiveness of our
model, we extensively evaluate it on various downstream tasks and assess the
performance under different transfer settings. Furthermore, we demonstrate how
the learned model exhibits transferable anomaly detection performance and
provides valuable interpretability of the acquired patterns via self-supervised
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ren_K/0/1/0/all/0/1&quot;&gt;Kan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_K/0/1/0/all/0/1&quot;&gt;Kaitao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yansen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Lili Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10279">
<title>A systematic review of geospatial location embedding approaches in large language models: A path to spatial AI systems. (arXiv:2401.10279v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2401.10279</link>
<description rdf:parseType="Literal">&lt;p&gt;Geospatial Location Embedding (GLE) helps a Large Language Model (LLM)
assimilate and analyze spatial data. GLE emergence in Geospatial Artificial
Intelligence (GeoAI) is precipitated by the need for deeper geospatial
awareness in our complex contemporary spaces and the success of LLMs in
extracting deep meaning in Generative AI. We searched Google Scholar, Science
Direct, and arXiv for papers on geospatial location embedding and LLM and
reviewed articles focused on gaining deeper spatial &quot;knowing&quot; through LLMs. We
screened 304 titles, 30 abstracts, and 18 full-text papers that reveal four GLE
themes - Entity Location Embedding (ELE), Document Location Embedding (DLE),
Sequence Location Embedding (SLE), and Token Location Embedding (TLE).
Synthesis is tabular and narrative, including a dialogic conversation between
&quot;Space&quot; and &quot;LLM.&quot; Though GLEs aid spatial understanding by superimposing
spatial data, they emphasize the need to advance in the intricacies of spatial
modalities and generalized reasoning. GLEs signal the need for a Spatial
Foundation/Language Model (SLM) that embeds spatial knowing within the model
architecture. The SLM framework advances Spatial Artificial Intelligence
Systems (SPAIS), establishing a Spatial Vector Space (SVS) that maps to
physical space. The resulting spatially imbued Language Model is unique. It
simultaneously represents actual space and an AI-capable space, paving the way
for AI native geo storage, analysis, and multi-modality as the basis for
Spatial Artificial Intelligence Systems (SPAIS).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tucker_S/0/1/0/all/0/1&quot;&gt;Sean Tucker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10280">
<title>GANs for EVT Based Model Parameter Estimation in Real-time Ultra-Reliable Communication. (arXiv:2401.10280v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2401.10280</link>
<description rdf:parseType="Literal">&lt;p&gt;The Ultra-Reliable Low-Latency Communications (URLLC) paradigm in
sixth-generation (6G) systems heavily relies on precise channel modeling,
especially when dealing with rare and extreme events within wireless
communication channels. This paper explores a novel methodology integrating
Extreme Value Theory (EVT) and Generative Adversarial Networks (GANs) to
achieve the precise channel modeling in real-time. The proposed approach
harnesses EVT by employing the Generalized Pareto Distribution (GPD) to model
the distribution of extreme events. Subsequently, Generative Adversarial
Networks (GANs) are employed to estimate the parameters of the GPD. In contrast
to conventional GAN configurations that focus on estimating the overall
distribution, the proposed approach involves the incorporation of an additional
block within the GAN structure. This specific augmentation is designed with the
explicit purpose of directly estimating the parameters of the Generalized
Pareto Distribution (GPD). Through extensive simulations across different
sample sizes, the proposed GAN based approach consistently demonstrates
superior adaptability, surpassing Maximum Likelihood Estimation (MLE),
particularly in scenarios with limited sample sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Valiahdi_P/0/1/0/all/0/1&quot;&gt;Parmida Valiahdi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Coleri_S/0/1/0/all/0/1&quot;&gt;Sinem Coleri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10282">
<title>BioDiffusion: A Versatile Diffusion Model for Biomedical Signal Synthesis. (arXiv:2401.10282v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2401.10282</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning tasks involving biomedical signals frequently grapple with
issues such as limited data availability, imbalanced datasets, labeling
complexities, and the interference of measurement noise. These challenges often
hinder the optimal training of machine learning algorithms. Addressing these
concerns, we introduce BioDiffusion, a diffusion-based probabilistic model
optimized for the synthesis of multivariate biomedical signals. BioDiffusion
demonstrates excellence in producing high-fidelity, non-stationary,
multivariate signals for a range of tasks including unconditional,
label-conditional, and signal-conditional generation. Leveraging these
synthesized signals offers a notable solution to the aforementioned challenges.
Our research encompasses both qualitative and quantitative assessments of the
synthesized data quality, underscoring its capacity to bolster accuracy in
machine learning tasks tied to biomedical signals. Furthermore, when juxtaposed
with current leading time-series generative models, empirical evidence suggests
that BioDiffusion outperforms them in biomedical signal generation quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaomin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sakevych_M/0/1/0/all/0/1&quot;&gt;Mykhailo Sakevych&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Atkinson_G/0/1/0/all/0/1&quot;&gt;Gentry Atkinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Metsis_V/0/1/0/all/0/1&quot;&gt;Vangelis Metsis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10284">
<title>MorpheusNet: Resource efficient sleep stage classifier for embedded on-line systems. (arXiv:2401.10284v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2401.10284</link>
<description rdf:parseType="Literal">&lt;p&gt;Sleep Stage Classification (SSC) is a labor-intensive task, requiring experts
to examine hours of electrophysiological recordings for manual classification.
This is a limiting factor when it comes to leveraging sleep stages for
therapeutic purposes. With increasing affordability and expansion of wearable
devices, automating SSC may enable deployment of sleep-based therapies at
scale. Deep Learning has gained increasing attention as a potential method to
automate this process. Previous research has shown accuracy comparable to
manual expert scores. However, previous approaches require sizable amount of
memory and computational resources. This constrains the ability to classify in
real time and deploy models on the edge. To address this gap, we aim to provide
a model capable of predicting sleep stages in real-time, without requiring
access to external computational sources (e.g., mobile phone, cloud). The
algorithm is power efficient to enable use on embedded battery powered systems.
Our compact sleep stage classifier can be deployed on most off-the-shelf
microcontrollers (MCU) with constrained hardware settings. This is due to the
memory footprint of our approach requiring significantly fewer operations. The
model was tested on three publicly available data bases and achieved
performance comparable to the state of the art, whilst reducing model
complexity by orders of magnitude (up to 280 times smaller compared to state of
the art). We further optimized the model with quantization of parameters to 8
bits with only an average drop of 0.95% in accuracy. When implemented in
firmware, the quantized model achieves a latency of 1.6 seconds on an Arm
CortexM4 processor, allowing its use for on-line SSC-based therapies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kavoosi_A/0/1/0/all/0/1&quot;&gt;Ali Kavoosi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mitchell_M/0/1/0/all/0/1&quot;&gt;Morgan P. Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kariyawasam_R/0/1/0/all/0/1&quot;&gt;Raveen Kariyawasam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fleming_J/0/1/0/all/0/1&quot;&gt;John E. Fleming&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lewis_P/0/1/0/all/0/1&quot;&gt;Penny Lewis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Johansen_Berg_H/0/1/0/all/0/1&quot;&gt;Heidi Johansen-Berg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cagnan_H/0/1/0/all/0/1&quot;&gt;Hayriye Cagnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Denison_T/0/1/0/all/0/1&quot;&gt;Timothy Denison&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10286">
<title>Top in Chinese Data Processing: English Code Models. (arXiv:2401.10286v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10286</link>
<description rdf:parseType="Literal">&lt;p&gt;While the alignment between tasks and training corpora is a fundamental
consensus in the application of language models, our series of experiments and
the metrics we designed reveal that code-based Large Language Models (LLMs)
significantly outperform models trained on data that is closely matched to the
tasks in non-coding Chinese tasks. Moreover, in tasks high sensitivity to
Chinese hallucinations, models exhibiting fewer linguistic features of the
Chinese language achieve better performance. Our experimental results can be
easily replicated in Chinese data processing tasks, such as preparing data for
Retrieval-Augmented Generation (RAG), by simply replacing the base model with a
code-based model. Additionally, our research offers a distinct perspective for
discussion on the philosophical &quot;Chinese Room&quot; thought experiment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Linghan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xiaojun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jiayuan Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1&quot;&gt;Yue Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1&quot;&gt;Gang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hongwei Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10289">
<title>Design and development of opto-neural processors for simulation of neural networks trained in image detection for potential implementation in hybrid robotics. (arXiv:2401.10289v1 [cs.ET])</title>
<link>http://arxiv.org/abs/2401.10289</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks have been employed for a wide range of processing
applications like image processing, motor control, object detection and many
others. Living neural networks offer advantages of lower power consumption,
faster processing, and biological realism. Optogenetics offers high spatial and
temporal control over biological neurons and presents potential in training
live neural networks. This work proposes a simulated living neural network
trained indirectly by backpropagating STDP based algorithms using precision
activation by optogenetics achieving accuracy comparable to traditional neural
network training algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shetty_S/0/1/0/all/0/1&quot;&gt;Sanjana Shetty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10299">
<title>An attempt to generate new bridge types from latent space of generative flow. (arXiv:2401.10299v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10299</link>
<description rdf:parseType="Literal">&lt;p&gt;Through examples of coordinate and probability transformation between
different distributions, the basic principle of normalizing flow is introduced
in a simple and concise manner. From the perspective of the distribution of
random variable function, the essence of probability transformation is
explained, and the scaling factor Jacobian determinant of probability
transformation is introduced. Treating the dataset as a sample from the
population, obtaining normalizing flow is essentially through sampling surveys
to statistically infer the numerical features of the population, and then the
loss function is established by using the maximum likelihood estimation method.
This article introduces how normalizing flow cleverly solves the two major
application challenges of high-dimensional matrix determinant calculation and
neural network reversible transformation. Using symmetric structured image
dataset of three-span beam bridge, arch bridge, cable-stayed bridge and
suspension bridge, constructing and training normalizing flow based on the Glow
API in the TensorFlow Probability library. The model can smoothly transform the
complex distribution of the bridge dataset into a standard normal distribution,
and from the obtained latent space sampling, it can generate new bridge types
that are different from the training dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongjun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10300">
<title>A Hierarchical Framework with Spatio-Temporal Consistency Learning for Emergence Detection in Complex Adaptive Systems. (arXiv:2401.10300v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2401.10300</link>
<description rdf:parseType="Literal">&lt;p&gt;Emergence, a global property of complex adaptive systems (CASs) constituted
by interactive agents, is prevalent in real-world dynamic systems, e.g.,
network-level traffic congestions. Detecting its formation and evaporation
helps to monitor the state of a system, allowing to issue a warning signal for
harmful emergent phenomena. Since there is no centralized controller of CAS,
detecting emergence based on each agent&apos;s local observation is desirable but
challenging. Existing works are unable to capture emergence-related spatial
patterns, and fail to model the nonlinear relationships among agents. This
paper proposes a hierarchical framework with spatio-temporal consistency
learning to solve these two problems by learning the system representation and
agent representations, respectively. Especially, spatio-temporal encoders are
tailored to capture agents&apos; nonlinear relationships and the system&apos;s complex
evolution. Representations of the agents and the system are learned by
preserving the intrinsic spatio-temporal consistency in a self-supervised
manner. Our method achieves more accurate detection than traditional methods
and deep learning methods on three datasets with well-known yet hard-to-detect
emergent behaviors. Notably, our hierarchical framework is generic, which can
employ other deep learning methods for agent-level and system-level detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Siyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xin Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiahai Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10304">
<title>On the Readiness of Scientific Data for a Fair and Transparent Use in Machine Learning. (arXiv:2401.10304v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10304</link>
<description rdf:parseType="Literal">&lt;p&gt;To ensure the fairness and trustworthiness of machine learning (ML) systems,
recent legislative initiatives and relevant research in the ML community have
pointed out the need to document the data used to train ML models. Besides,
data-sharing practices in many scientific domains have evolved in recent years
for reproducibility purposes. In this sense, the adoption of these practices by
academic institutions has encouraged researchers to publish their data and
technical documentation in peer-reviewed publications such as data papers. In
this study, we analyze how this scientific data documentation meets the needs
of the ML community and regulatory bodies for its use in ML technologies. We
examine a sample of 4041 data papers of different domains, assessing their
completeness and coverage of the requested dimensions, and trends in recent
years, putting special emphasis on the most and least documented dimensions. As
a result, we propose a set of recommendation guidelines for data creators and
scientific data publishers to increase their data&apos;s preparedness for its
transparent and fairer use in ML technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giner_Miguelez_J/0/1/0/all/0/1&quot;&gt;Joan Giner-Miguelez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1&quot;&gt;Abel G&amp;#xf3;mez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cabot_J/0/1/0/all/0/1&quot;&gt;Jordi Cabot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10310">
<title>Mathematical Algorithm Design for Deep Learning under Societal and Judicial Constraints: The Algorithmic Transparency Requirement. (arXiv:2401.10310v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10310</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning still has drawbacks in terms of trustworthiness, which
describes a comprehensible, fair, safe, and reliable method. To mitigate the
potential risk of AI, clear obligations associated to trustworthiness have been
proposed via regulatory guidelines, e.g., in the European AI Act. Therefore, a
central question is to what extent trustworthy deep learning can be realized.
Establishing the described properties constituting trustworthiness requires
that the factors influencing an algorithmic computation can be retraced, i.e.,
the algorithmic implementation is transparent. Motivated by the observation
that the current evolution of deep learning models necessitates a change in
computing technology, we derive a mathematical framework which enables us to
analyze whether a transparent implementation in a computing model is feasible.
We exemplarily apply our trustworthiness framework to analyze deep learning
approaches for inverse problems in digital and analog computing models
represented by Turing and Blum-Shub-Smale Machines, respectively. Based on
previous results, we find that Blum-Shub-Smale Machines have the potential to
establish trustworthy solvers for inverse problems under fairly general
conditions, whereas Turing machines cannot guarantee trustworthiness to the
same degree.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boche_H/0/1/0/all/0/1&quot;&gt;Holger Boche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fono_A/0/1/0/all/0/1&quot;&gt;Adalbert Fono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1&quot;&gt;Gitta Kutyniok&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10314">
<title>LangProp: A code optimization framework using Language Models applied to driving. (arXiv:2401.10314v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.10314</link>
<description rdf:parseType="Literal">&lt;p&gt;LangProp is a framework for iteratively optimizing code generated by large
language models (LLMs) in a supervised/reinforcement learning setting. While
LLMs can generate sensible solutions zero-shot, the solutions are often
sub-optimal. Especially for code generation tasks, it is likely that the
initial code will fail on certain edge cases. LangProp automatically evaluates
the code performance on a dataset of input-output pairs, as well as catches any
exceptions, and feeds the results back to the LLM in the training loop, so that
the LLM can iteratively improve the code it generates. By adopting a metric-
and data-driven training paradigm for this code optimization procedure, one
could easily adapt findings from traditional machine learning techniques such
as imitation learning, DAgger, and reinforcement learning. We demonstrate the
first proof of concept of automated code optimization for autonomous driving in
CARLA, showing that LangProp can generate interpretable and transparent driving
policies that can be verified and improved in a metric- and data-driven way.
Our code will be open-sourced and is available at
https://github.com/shuishida/LangProp.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishida_S/0/1/0/all/0/1&quot;&gt;Shu Ishida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corrado_G/0/1/0/all/0/1&quot;&gt;Gianluca Corrado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fedoseev_G/0/1/0/all/0/1&quot;&gt;George Fedoseev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1&quot;&gt;Hudson Yeo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russell_L/0/1/0/all/0/1&quot;&gt;Lloyd Russell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shotton_J/0/1/0/all/0/1&quot;&gt;Jamie Shotton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o F. Henriques&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1&quot;&gt;Anthony Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10316">
<title>Improving One-class Recommendation with Multi-tasking on Various Preference Intensities. (arXiv:2401.10316v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2401.10316</link>
<description rdf:parseType="Literal">&lt;p&gt;In the one-class recommendation problem, it&apos;s required to make
recommendations basing on users&apos; implicit feedback, which is inferred from
their action and inaction. Existing works obtain representations of users and
items by encoding positive and negative interactions observed from training
data. However, these efforts assume that all positive signals from implicit
feedback reflect a fixed preference intensity, which is not realistic.
Consequently, representations learned with these methods usually fail to
capture informative entity features that reflect various preference
intensities.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a multi-tasking framework taking various preference
intensities of each signal from implicit feedback into consideration.
Representations of entities are required to satisfy the objective of each
subtask simultaneously, making them more robust and generalizable. Furthermore,
we incorporate attentive graph convolutional layers to explore high-order
relationships in the user-item bipartite graph and dynamically capture the
latent tendencies of users toward the items they interact with. Experimental
results show that our method performs better than state-of-the-art methods by a
large margin on three large-scale real-world benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1&quot;&gt;Chu-Jen Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Hao-Ming Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1&quot;&gt;Pu-Jen Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10334">
<title>DrugAssist: A Large Language Model for Molecule Optimization. (arXiv:2401.10334v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2401.10334</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the impressive performance of large language models (LLMs) on a
wide range of tasks has attracted an increasing number of attempts to apply
LLMs in drug discovery. However, molecule optimization, a critical task in the
drug discovery pipeline, is currently an area that has seen little involvement
from LLMs. Most of existing approaches focus solely on capturing the underlying
patterns in chemical structures provided by the data, without taking advantage
of expert feedback. These non-interactive approaches overlook the fact that the
drug discovery process is actually one that requires the integration of expert
experience and iterative refinement. To address this gap, we propose
DrugAssist, an interactive molecule optimization model which performs
optimization through human-machine dialogue by leveraging LLM&apos;s strong
interactivity and generalizability. DrugAssist has achieved leading results in
both single and multiple property optimization, simultaneously showcasing
immense potential in transferability and iterative optimization. In addition,
we publicly release a large instruction-based dataset called
MolOpt-Instructions for fine-tuning language models on molecule optimization
tasks. We have made our code and data publicly available at
https://github.com/blazerye/DrugAssist, which we hope to pave the way for
future research in LLMs&apos; application for drug discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ye_G/0/1/0/all/0/1&quot;&gt;Geyan Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cai_X/0/1/0/all/0/1&quot;&gt;Xibao Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lai_H/0/1/0/all/0/1&quot;&gt;Houtim Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Junhong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xiangxiang Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10337">
<title>Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition. (arXiv:2401.10337v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10337</link>
<description rdf:parseType="Literal">&lt;p&gt;Tactics, Techniques and Procedures (TTPs) represent sophisticated attack
patterns in the cybersecurity domain, described encyclopedically in textual
knowledge bases. Identifying TTPs in cybersecurity writing, often called TTP
mapping, is an important and challenging task. Conventional learning approaches
often target the problem in the classical multi-class or multilabel
classification setting. This setting hinders the learning ability of the model
due to a large number of classes (i.e., TTPs), the inevitable skewness of the
label distribution and the complex hierarchical structure of the label space.
We formulate the problem in a different learning paradigm, where the assignment
of a text to a TTP label is decided by the direct semantic similarity between
the two, thus reducing the complexity of competing solely over the large
labeling space. To that end, we propose a neural matching architecture with an
effective sampling-based learn-to-compare mechanism, facilitating the learning
process of the matching model despite constrained resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tu Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srndic_N/0/1/0/all/0/1&quot;&gt;Nedim Srndic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neth_A/0/1/0/all/0/1&quot;&gt;Alexander Neth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10341">
<title>ELRT: Efficient Low-Rank Training for Compact Convolutional Neural Networks. (arXiv:2401.10341v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10341</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-rank compression, a popular model compression technique that produces
compact convolutional neural networks (CNNs) with low rankness, has been
well-studied in the literature. On the other hand, low-rank training, as an
alternative way to train low-rank CNNs from scratch, has been exploited little
yet. Unlike low-rank compression, low-rank training does not need pre-trained
full-rank models, and the entire training phase is always performed on the
low-rank structure, bringing attractive benefits for practical applications.
However, the existing low-rank training solutions still face several
challenges, such as a considerable accuracy drop and/or still needing to update
full-size models during the training. In this paper, we perform a systematic
investigation on low-rank CNN training. By identifying the proper low-rank
format and performance-improving strategy, we propose ELRT, an efficient
low-rank training solution for high-accuracy, high-compactness, low-rank CNN
models. Our extensive evaluation results for training various CNNs on different
datasets demonstrate the effectiveness of ELRT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1&quot;&gt;Yang Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1&quot;&gt;Miao Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yu Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jinqi Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1&quot;&gt;Huy Phan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1&quot;&gt;Bo Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10348">
<title>Exploring General Intelligence via Gated Graph Transformer in Functional Connectivity Studies. (arXiv:2401.10348v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2401.10348</link>
<description rdf:parseType="Literal">&lt;p&gt;Functional connectivity (FC) as derived from fMRI has emerged as a pivotal
tool in elucidating the intricacies of various psychiatric disorders and
delineating the neural pathways that underpin cognitive and behavioral dynamics
inherent to the human brain. While Graph Neural Networks (GNNs) offer a
structured approach to represent neuroimaging data, they are limited by their
need for a predefined graph structure to depict associations between brain
regions, a detail not solely provided by FCs. To bridge this gap, we introduce
the Gated Graph Transformer (GGT) framework, designed to predict cognitive
metrics based on FCs. Empirical validation on the Philadelphia
Neurodevelopmental Cohort (PNC) underscores the superior predictive prowess of
our model, further accentuating its potential in identifying pivotal neural
connectivities that correlate with human cognitive processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Qu_G/0/1/0/all/0/1&quot;&gt;Gang Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Orlichenko_A/0/1/0/all/0/1&quot;&gt;Anton Orlichenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Gemeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xiao_L/0/1/0/all/0/1&quot;&gt;Li Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Aiying Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Zhengming Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Ping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10359">
<title>Keeping Deep Learning Models in Check: A History-Based Approach to Mitigate Overfitting. (arXiv:2401.10359v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.10359</link>
<description rdf:parseType="Literal">&lt;p&gt;In software engineering, deep learning models are increasingly deployed for
critical tasks such as bug detection and code review. However, overfitting
remains a challenge that affects the quality, reliability, and trustworthiness
of software systems that utilize deep learning models. Overfitting can be (1)
prevented (e.g., using dropout or early stopping) or (2) detected in a trained
model (e.g., using correlation-based approaches). Both overfitting detection
and prevention approaches that are currently used have constraints (e.g.,
requiring modification of the model structure, and high computing resources).
In this paper, we propose a simple, yet powerful approach that can both detect
and prevent overfitting based on the training history (i.e., validation
losses). Our approach first trains a time series classifier on training
histories of overfit models. This classifier is then used to detect if a
trained model is overfit. In addition, our trained classifier can be used to
prevent overfitting by identifying the optimal point to stop a model&apos;s
training. We evaluate our approach on its ability to identify and prevent
overfitting in real-world samples. We compare our approach against
correlation-based detection approaches and the most commonly used prevention
approach (i.e., early stopping). Our approach achieves an F1 score of 0.91
which is at least 5% higher than the current best-performing non-intrusive
overfitting detection approach. Furthermore, our approach can stop training to
avoid overfitting at least 32% of the times earlier than early stopping and has
the same or a better rate of returning the best model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajbahadur_G/0/1/0/all/0/1&quot;&gt;Gopi Krishnan Rajbahadur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dayi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bezemer_C/0/1/0/all/0/1&quot;&gt;Cor-Paul Bezemer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1&quot;&gt;Zhen Ming&lt;/a&gt; (Jack) &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang/0/1/0/all/0/1&quot;&gt;Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10361">
<title>Hierarchical Federated Learning in Multi-hop Cluster-Based VANETs. (arXiv:2401.10361v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10361</link>
<description rdf:parseType="Literal">&lt;p&gt;The usage of federated learning (FL) in Vehicular Ad hoc Networks (VANET) has
garnered significant interest in research due to the advantages of reducing
transmission overhead and protecting user privacy by communicating local
dataset gradients instead of raw data. However, implementing FL in VANETs faces
challenges, including limited communication resources, high vehicle mobility,
and the statistical diversity of data distributions. In order to tackle these
issues, this paper introduces a novel framework for hierarchical federated
learning (HFL) over multi-hop clustering-based VANET. The proposed method
utilizes a weighted combination of the average relative speed and cosine
similarity of FL model parameters as a clustering metric to consider both data
diversity and high vehicle mobility. This metric ensures convergence with
minimum changes in cluster heads while tackling the complexities associated
with non-independent and identically distributed (non-IID) data scenarios.
Additionally, the framework includes a novel mechanism to manage seamless
transitions of cluster heads (CHs), followed by transferring the most recent FL
model parameter to the designated CH. Furthermore, the proposed approach
considers the option of merging CHs, aiming to reduce their count and,
consequently, mitigate associated overhead. Through extensive simulations, the
proposed hierarchical federated learning over clustered VANET has been
demonstrated to improve accuracy and convergence time significantly while
maintaining an acceptable level of packet overhead compared to previously
proposed clustering algorithms and non-clustered VANET.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+HaghighiFard_M/0/1/0/all/0/1&quot;&gt;M. Saeid HaghighiFard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coleri_S/0/1/0/all/0/1&quot;&gt;Sinem Coleri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10372">
<title>MutaBot: A Mutation Testing Approach for Chatbots. (arXiv:2401.10372v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.10372</link>
<description rdf:parseType="Literal">&lt;p&gt;Mutation testing is a technique aimed at assessing the effectiveness of test
suites by seeding artificial faults into programs. Although available for many
platforms and languages, no mutation testing tool is currently available for
conversational chatbots, which represent an increasingly popular solution to
design systems that can interact with users through a natural language
interface. Note that since conversations must be explicitly engineered by the
developers of conversational chatbots, these systems are exposed to specific
types of faults not supported by existing mutation testing tools.
&lt;/p&gt;
&lt;p&gt;In this paper, we present MutaBot, a mutation testing tool for conversational
chatbots. MutaBot addresses mutations at multiple levels, including
conversational flows, intents, and contexts. We designed the tool to
potentially target multiple platforms, while we implemented initial support for
Google Dialogflow chatbots. We assessed the tool with three Dialogflow chatbots
and test cases generated with Botium, revealing weaknesses in the test suites.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urrico_M/0/1/0/all/0/1&quot;&gt;Michael Ferdinando Urrico&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clerissi_D/0/1/0/all/0/1&quot;&gt;Diego Clerissi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mariani_L/0/1/0/all/0/1&quot;&gt;Leonardo Mariani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10379">
<title>Agricultural Object Detection with You Look Only Once (YOLO) Algorithm: A Bibliometric and Systematic Literature Review. (arXiv:2401.10379v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10379</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision is a major component in several digital technologies and tools used in
agriculture. The object detector, You Look Only Once (YOLO), has gained
popularity in agriculture in a relatively short span due to its
state-of-the-art performance. YOLO offers real-time detection with good
accuracy and is implemented in various agricultural tasks, including
monitoring, surveillance, sensing, automation, and robotics. The research and
application of YOLO in agriculture are accelerating rapidly but are fragmented
and multidisciplinary. Moreover, the performance characteristics (i.e.,
accuracy, speed, computation) of the object detector influence the rate of
technology implementation and adoption in agriculture. Thus, the study aims to
collect extensive literature to document and critically evaluate the advances
and application of YOLO for agricultural object recognition. First, we
conducted a bibliometric review of 257 articles to understand the scholarly
landscape of YOLO in agricultural domain. Secondly, we conducted a systematic
review of 30 articles to identify current knowledge, gaps, and modifications in
YOLO for specific agricultural tasks. The study critically assesses and
summarizes the information on YOLO&apos;s end-to-end learning approach, including
data acquisition, processing, network modification, integration, and
deployment. We also discussed task-specific YOLO algorithm modification and
integration to meet the agricultural object or environment-specific challenges.
In general, YOLO-integrated digital tools and technologies show the potential
for real-time, automated monitoring, surveillance, and object handling to
reduce labor, production cost, and environmental impact while maximizing
resource efficiency. The study provides detailed documentation and
significantly advances the existing knowledge on applying YOLO in agriculture,
which can greatly benefit the scientific community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badgujar_C/0/1/0/all/0/1&quot;&gt;Chetan M Badgujar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poulose_A/0/1/0/all/0/1&quot;&gt;Alwin Poulose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_H/0/1/0/all/0/1&quot;&gt;Hao Gan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10393">
<title>Catastrophic Interference is Mitigated in Naturalistic Power-Law Learning Environments. (arXiv:2401.10393v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10393</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks often suffer from catastrophic interference (CI): performance
on previously learned tasks drops off significantly when learning a new task.
This contrasts strongly with humans, who can sequentially learn new tasks
without appreciably forgetting previous tasks. Prior work has explored various
techniques for mitigating CI such as regularization, rehearsal, generative
replay, and distillation methods. The current work takes a different approach,
one guided by cognitive science research showing that in naturalistic
environments, the probability of encountering a task decreases as a power-law
of the time since it was last performed. We argue that a realistic evaluation
of techniques for the mitigation of CI should be performed in simulated
naturalistic learning environments. Thus, we evaluate the extent of mitigation
of CI when training simple rehearsal-based methods in power-law environments
similar to the ones humans face. Our work explores this novel rehearsal-based
approach for a domain-incremental task: learning permutations in the MNIST
task. We compare our rehearsal environment with other baselines to show its
efficacy in promoting continual learning. Additionally, we investigate whether
this environment shows forward facilitation, i.e., faster learning of later
tasks. Next, we explore the robustness of our learning environment to the
number of tasks, model size, and amount of data rehearsed after each task.
Notably, our results show that the performance is comparable or superior to
that of models trained using popular regularization methods and also to
rehearsals in non-power-law environments. The benefits of this training
paradigm include simplicity and the lack of a need for extra neural circuitry.
In addition, because our method is orthogonal to other methods, future research
can combine training in power-law environments with other continual learning
mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandhi_A/0/1/0/all/0/1&quot;&gt;Atith Gandhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Raj Sanjay Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marupudi_V/0/1/0/all/0/1&quot;&gt;Vijay Marupudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varma_S/0/1/0/all/0/1&quot;&gt;Sashank Varma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10394">
<title>Distribution Consistency based Self-Training for Graph Neural Networks with Sparse Labels. (arXiv:2401.10394v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10394</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot node classification poses a significant challenge for Graph Neural
Networks (GNNs) due to insufficient supervision and potential distribution
shifts between labeled and unlabeled nodes. Self-training has emerged as a
widely popular framework to leverage the abundance of unlabeled data, which
expands the training set by assigning pseudo-labels to selected unlabeled
nodes. Efforts have been made to develop various selection strategies based on
confidence, information gain, etc. However, none of these methods takes into
account the distribution shift between the training and testing node sets. The
pseudo-labeling step may amplify this shift and even introduce new ones,
hindering the effectiveness of self-training. Therefore, in this work, we
explore the potential of explicitly bridging the distribution shift between the
expanded training set and test set during self-training. To this end, we
propose a novel Distribution-Consistent Graph Self-Training (DC-GST) framework
to identify pseudo-labeled nodes that are both informative and capable of
redeeming the distribution discrepancy and formulate it as a differentiable
optimization task. A distribution-shift-aware edge predictor is further adopted
to augment the graph and increase the model&apos;s generalizability in assigning
pseudo labels. We evaluate our proposed method on four publicly available
benchmark datasets and extensive experiments demonstrate that our framework
consistently outperforms state-of-the-art baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fali Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tianxiang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Suhang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10415">
<title>Can Large Language Model Summarizers Adapt to Diverse Scientific Communication Goals?. (arXiv:2401.10415v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10415</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we investigate the controllability of large language models
(LLMs) on scientific summarization tasks. We identify key stylistic and content
coverage factors that characterize different types of summaries such as paper
reviews, abstracts, and lay summaries. By controlling stylistic features, we
find that non-fine-tuned LLMs outperform humans in the MuP review generation
task, both in terms of similarity to reference summaries and human preferences.
Also, we show that we can improve the controllability of LLMs with
keyword-based classifier-free guidance (CFG) while achieving lexical overlap
comparable to strong fine-tuned baselines on arXiv and PubMed. However, our
results also indicate that LLMs cannot consistently generate long summaries
with more than 8 sentences. Furthermore, these models exhibit limited capacity
to produce highly abstractive lay summaries. Although LLMs demonstrate strong
generic summarization competency, sophisticated content control without costly
fine-tuning remains an open problem for domain-specific applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fonseca_M/0/1/0/all/0/1&quot;&gt;Marcio Fonseca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1&quot;&gt;Shay B. Cohen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10420">
<title>Generalized Nested Rollout Policy Adaptation with Limited Repetitions. (arXiv:2401.10420v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.10420</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalized Nested Rollout Policy Adaptation (GNRPA) is a Monte Carlo search
algorithm for optimizing a sequence of choices. We propose to improve on GNRPA
by avoiding too deterministic policies that find again and again the same
sequence of choices. We do so by limiting the number of repetitions of the best
sequence found at a given level. Experiments show that it improves the
algorithm for three different combinatorial problems: Inverse RNA Folding, the
Traveling Salesman Problem with Time Windows and the Weak Schur problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cazenave_T/0/1/0/all/0/1&quot;&gt;Tristan Cazenave&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10428">
<title>Understanding Learning through the Lens of Dynamical Invariants. (arXiv:2401.10428v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.10428</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel perspective on learning, positing it as the
pursuit of dynamical invariants -- data combinations that remain constant or
exhibit minimal change over time as a system evolves. This concept is
underpinned by both informational and physical principles, rooted in the
inherent properties of these invariants. Firstly, their stability makes them
ideal for memorization and integration into associative networks, forming the
basis of our knowledge structures. Secondly, the predictability of these stable
invariants makes them valuable sources of usable energy, quantifiable as kTln2
per bit of accurately predicted information. This energy can be harnessed to
explore new transformations, rendering learning systems energetically
autonomous and increasingly effective. Such systems are driven to continuously
seek new data invariants as energy sources. The paper further explores several
meta-architectures of autonomous, self-propelled learning agents that utilize
predictable information patterns as a source of usable energy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ushveridze_A/0/1/0/all/0/1&quot;&gt;Alex Ushveridze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10431">
<title>Learning a Prior for Monte Carlo Search by Replaying Solutions to Combinatorial Problems. (arXiv:2401.10431v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.10431</link>
<description rdf:parseType="Literal">&lt;p&gt;Monte Carlo Search gives excellent results in multiple difficult
combinatorial problems. Using a prior to perform non uniform playouts during
the search improves a lot the results compared to uniform playouts. Handmade
heuristics tailored to the combinatorial problem are often used as priors. We
propose a method to automatically compute a prior. It uses statistics on solved
problems. It is a simple and general method that incurs no computational cost
at playout time and that brings large performance gains. The method is applied
to three difficult combinatorial problems: Latin Square Completion, Kakuro, and
Inverse RNA Folding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cazenave_T/0/1/0/all/0/1&quot;&gt;Tristan Cazenave&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10444">
<title>Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?. (arXiv:2401.10444v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.10444</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper discusses what is needed to address the limitations of current
LLM-centered AI systems. The paper argues that incorporating insights from
human cognition and psychology, as embodied by a computational cognitive
architecture, can help develop systems that are more capable, more reliable,
and more human-like. It emphasizes the importance of the dual-process
architecture and the hybrid neuro-symbolic approach in addressing the
limitations of current LLMs. In the opposite direction, the paper also
highlights the need for an overhaul of computational cognitive architectures to
better reflect advances in AI and computing technology. Overall, the paper
advocates for a multidisciplinary, mutually beneficial approach towards
developing better models both for AI and for understanding the human mind.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1&quot;&gt;Ron Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10446">
<title>Large Language Models are Efficient Learners of Noise-Robust Speech Recognition. (arXiv:2401.10446v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10446</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in large language models (LLMs) have promoted generative
error correction (GER) for automatic speech recognition (ASR), which leverages
the rich linguistic knowledge and powerful reasoning ability of LLMs to improve
recognition results. The latest work proposes a GER benchmark with HyPoradise
dataset to learn the mapping from ASR N-best hypotheses to ground-truth
transcription by efficient LLM finetuning, which shows great effectiveness but
lacks specificity on noise-robust ASR. In this work, we extend the benchmark to
noisy conditions and investigate if we can teach LLMs to perform denoising for
GER just like what robust ASR do}, where one solution is introducing noise
information as a conditioner into LLM. However, directly incorporating noise
embeddings from audio encoder could harm the LLM tuning due to cross-modality
gap. To this end, we propose to extract a language-space noise embedding from
the N-best list to represent the noise conditions of source speech, which can
promote the denoising process in GER. Furthermore, in order to enhance its
representation ability of audio noise, we design a knowledge distillation (KD)
approach via mutual information estimation to distill the real noise
information in audio embeddings to our language embedding. Experiments on
various latest LLMs demonstrate our approach achieves a new breakthrough with
up to 53.9% correction improvement in terms of word error rate while with
limited training data. Analysis shows that our language-space noise embedding
can well represent the noise conditions of source speech, under which
off-the-shelf LLMs show strong ability of language-space denoising.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yuchen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chao-Han Huck Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruizhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chng_E/0/1/0/all/0/1&quot;&gt;EnSiong Chng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10447">
<title>Investigating Training Strategies and Model Robustness of Low-Rank Adaptation for Language Modeling in Speech Recognition. (arXiv:2401.10447v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10447</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of low-rank adaptation (LoRA) with frozen pretrained language models
(PLMs) has become increasing popular as a mainstream, resource-efficient
modeling approach for memory-constrained hardware. In this study, we first
explore how to enhance model performance by introducing various LoRA training
strategies, achieving relative word error rate reductions of 3.50\% on the
public Librispeech dataset and of 3.67\% on an internal dataset in the
messaging domain. To further characterize the stability of LoRA-based
second-pass speech recognition models, we examine robustness against input
perturbations. These perturbations are rooted in homophone replacements and a
novel metric called N-best Perturbation-based Rescoring Robustness (NPRR), both
designed to measure the relative degradation in the performance of rescoring
models. Our experimental results indicate that while advanced variants of LoRA,
such as dynamic rank-allocated LoRA, lead to performance degradation in
$1$-best perturbation, they alleviate the degradation in $N$-best perturbation.
This finding is in comparison to fully-tuned models and vanilla LoRA tuning
baselines, suggesting that a comprehensive selection is needed when using
LoRA-based adaptation for compute-cost savings and robust language modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yu Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chao-Han Huck Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1&quot;&gt;Tuan Dinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_S/0/1/0/all/0/1&quot;&gt;Sungho Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolehmainen_J/0/1/0/all/0/1&quot;&gt;Jari Kolehmainen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1&quot;&gt;Roger Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filimonov_D/0/1/0/all/0/1&quot;&gt;Denis Filimonov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shivakumar_P/0/1/0/all/0/1&quot;&gt;Prashanth G. Shivakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandhe_A/0/1/0/all/0/1&quot;&gt;Ankur Gandhe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastow_A/0/1/0/all/0/1&quot;&gt;Ariya Rastow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jia Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1&quot;&gt;Ivan Bulyko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1&quot;&gt;Andreas Stolcke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10463">
<title>Critical Data Size of Language Models from a Grokking Perspective. (arXiv:2401.10463v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10463</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the critical data size in language models, a threshold that marks
a fundamental shift from quick memorization to slow generalization. We
formalize the phase transition under the grokking configuration into the Data
Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus
regimes in language models training dynamics. We develop a grokking
configuration to reproduce grokking on simplistic language models stably by
rescaling initialization and weight decay. We show that generalization occurs
only when language models reach a critical size. We analyze grokking across
sample-wise and model-wise, verifying the proposed data efficiency hypothesis.
Our experiments reveal smoother phase transitions occurring at the critical
dataset size for language datasets. As the model size increases, this critical
point also becomes larger, indicating that larger models require more data. Our
results deepen the understanding of language model training, offering a novel
perspective on the role of data in the learning mechanism of language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xuekai Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yao Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bowen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhouhan Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10467">
<title>Learning Backdoors for Mixed Integer Programs with Contrastive Learning. (arXiv:2401.10467v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.10467</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real-world problems can be efficiently modeled as Mixed Integer Programs
(MIPs) and solved with the Branch-and-Bound method. Prior work has shown the
existence of MIP backdoors, small sets of variables such that prioritizing
branching on them when possible leads to faster running times. However, finding
high-quality backdoors that improve running times remains an open question.
Previous work learns to estimate the relative solver speed of randomly sampled
backdoors through ranking and then decide whether to use it. In this paper, we
utilize the Monte-Carlo tree search method to collect backdoors for training,
rather than relying on random sampling, and adapt a contrastive learning
framework to train a Graph Attention Network model to predict backdoors. Our
method, evaluated on four common MIP problem domains, demonstrates performance
improvements over both Gurobi and previous models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Junyang Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Taoan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1&quot;&gt;Bistra Dilkina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10471">
<title>DeepEdit: Knowledge Editing as Decoding with Constraints. (arXiv:2401.10471v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10471</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a new perspective of knowledge editing for large language models
(LLMs) as decoding with constraints. We propose DeepEdit (Depth-first Search
based Progressive Decoding for Knowledge Editing), a neuro-symbolic method that
improves knowledge editing with better coherence of reasoning, relevance to the
question, and awareness of updated knowledge. DeepEdit can be flexibly applied
to all black-box LLMs: it does not require any access to the model parameters,
representations, or output vocabulary distributions. DeepEdit progressively
produces the high-quality reasoning steps towards effective knowledge editing.
It utilizes a depth-first search to revise the LLMs&apos; output, which improves the
output&apos;s informativeness to the input question and awareness of the updated
knowledge. Qualitatively, DeepEdit effectively controls LLMs to produce more
succinct reasoning in accord with knowledge editing. Quantitatively, DeepEdit
yields significant gains on MQuaKE, a challenging multi-hop question-answering
dataset with knowledge editing. We release the source code at
https://github.com/wangywUST/DeepEdit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Muhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1&quot;&gt;Nanyun Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10474">
<title>LDReg: Local Dimensionality Regularized Self-Supervised Learning. (arXiv:2401.10474v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10474</link>
<description rdf:parseType="Literal">&lt;p&gt;Representations learned via self-supervised learning (SSL) can be susceptible
to dimensional collapse, where the learned representation subspace is of
extremely low dimensionality and thus fails to represent the full data
distribution and modalities. Dimensional collapse also known as the
&quot;underfilling&quot; phenomenon is one of the major causes of degraded performance on
downstream tasks. Previous work has investigated the dimensional collapse
problem of SSL at a global level. In this paper, we demonstrate that
representations can span over high dimensional space globally, but collapse
locally. To address this, we propose a method called $\textit{local
dimensionality regularization (LDReg)}$. Our formulation is based on the
derivation of the Fisher-Rao metric to compare and optimize local distance
distributions at an asymptotically small radius for each data point. By
increasing the local intrinsic dimensionality, we demonstrate through a range
of experiments that LDReg improves the representation quality of SSL. The
results also show that LDReg can regularize dimensionality at both local and
global levels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hanxun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campello_R/0/1/0/all/0/1&quot;&gt;Ricardo J. G. B. Campello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1&quot;&gt;Sarah Monazam Erfani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xingjun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houle_M/0/1/0/all/0/1&quot;&gt;Michael E. Houle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1&quot;&gt;James Bailey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10480">
<title>Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step Reasoning. (arXiv:2401.10480v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10480</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-consistency (SC) has been a widely used decoding strategy for
chain-of-thought reasoning. Despite bringing significant performance
improvements across a variety of multi-step reasoning tasks, it is a high-cost
method that requires multiple sampling with the preset size. In this paper, we
propose a simple and scalable sampling process, \textbf{E}arly-Stopping
\textbf{S}elf-\textbf{C}onsistency (ESC), to greatly reduce the cost of SC
without sacrificing performance. On this basis, one control scheme for ESC is
further derivated to dynamically choose the performance-cost balance for
different tasks and models. To demonstrate ESC&apos;s effectiveness, we conducted
extensive experiments on three popular categories of reasoning tasks:
arithmetic, commonsense and symbolic reasoning over language models with
varying scales. The empirical results show that ESC reduces the average number
of sampling of chain-of-thought reasoning by a significant margin on six
benchmarks, including MATH (-33.8%), GSM8K (-80.1%), StrategyQA (-76.8%),
CommonsenseQA (-78.5%), Coin Flip (-84.2%) and Last Letters (-67.4%), while
attaining comparable performances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_P/0/1/0/all/0/1&quot;&gt;Peiwen Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shaoxiong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1&quot;&gt;Boyuan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinglin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Bin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Heda Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10484">
<title>Enhancing Scalability in Recommender Systems through Lottery Ticket Hypothesis and Knowledge Distillation-based Neural Network Pruning. (arXiv:2401.10484v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2401.10484</link>
<description rdf:parseType="Literal">&lt;p&gt;This study introduces an innovative approach aimed at the efficient pruning
of neural networks, with a particular focus on their deployment on edge
devices. Our method involves the integration of the Lottery Ticket Hypothesis
(LTH) with the Knowledge Distillation (KD) framework, resulting in the
formulation of three distinct pruning models. These models have been developed
to address scalability issue in recommender systems, whereby the complexities
of deep learning models have hindered their practical deployment. With
judicious application of the pruning techniques, we effectively curtail the
power consumption and model dimensions without compromising on accuracy.
Empirical evaluation has been performed using two real world datasets from
diverse domains against two baselines. Gratifyingly, our approaches yielded a
GPU computation-power reduction of up to 66.67%. Notably, our study contributes
to the field of recommendation system by pioneering the application of LTH and
KD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+R_R/0/1/0/all/0/1&quot;&gt;Rajaram R&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bharadhwaj_M/0/1/0/all/0/1&quot;&gt;Manoj Bharadhwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+VS_V/0/1/0/all/0/1&quot;&gt;Vasan VS&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pervin_N/0/1/0/all/0/1&quot;&gt;Nargis Pervin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10495">
<title>Causal Layering via Conditional Entropy. (arXiv:2401.10495v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10495</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal discovery aims to recover information about an unobserved causal graph
from the observable data it generates. Layerings are orderings of the variables
which place causes before effects. In this paper, we provide ways to recover
layerings of a graph by accessing the data via a conditional entropy oracle,
when distributions are discrete. Our algorithms work by repeatedly removing
sources or sinks from the graph. Under appropriate assumptions and
conditioning, we can separate the sources or sinks from the remainder of the
nodes by comparing their conditional entropy to the unconditional entropy of
their noise. Our algorithms are provably correct and run in worst-case
quadratic time. The main assumptions are faithfulness and injective noise, and
either known noise entropies or weakly monotonically increasing noise entropies
along directed paths. In addition, we require one of either a very mild
extension of faithfulness, or strictly monotonically increasing noise
entropies, or expanding noise injectivity to include an additional single
argument in the structural functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feigenbaum_I/0/1/0/all/0/1&quot;&gt;Itai Feigenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arpit_D/0/1/0/all/0/1&quot;&gt;Devansh Arpit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinecke_S/0/1/0/all/0/1&quot;&gt;Shelby Heinecke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niebles_J/0/1/0/all/0/1&quot;&gt;Juan Carlos Niebles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Weiran Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1&quot;&gt;Silvio Savarese&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10506">
<title>FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis. (arXiv:2401.10506v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10506</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-SQL, which provides zero-code interface for operating relational
databases, has gained much attention in financial analysis; because, financial
professionals may not well-skilled in SQL programming. However, until now,
there is no practical Text-to-SQL benchmark dataset for financial analysis, and
existing Text-to-SQL methods have not considered the unique characteristics of
databases in financial applications, such as commonly existing wide tables. To
address these issues, we collect a practical Text-to-SQL benchmark dataset and
propose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL
framework for financial analysis. The benchmark dataset, BULL, is collected
from the practical financial analysis business of Hundsun Technologies Inc.,
including databases for fund, stock, and macro economy. Besides, the proposed
LLMs-based Text-to-SQL framework, FinSQL, provides a systematic treatment for
financial Text-to-SQL from the perspectives of prompt construction,
parameter-efficient fine-tuning and output calibration. Extensive experimental
results on BULL demonstrate that FinSQL achieves the state-of-the-art
Text-to-SQL performance at a small cost; furthermore, FinSQL can bring up to
36.64% performance improvement in scenarios requiring few-shot cross-database
model transfer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yuren Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yijiang Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_Y/0/1/0/all/0/1&quot;&gt;Yu Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yunjun Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_D/0/1/0/all/0/1&quot;&gt;Dongfang Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jinshu Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10510">
<title>A match made in consistency heaven: when large language models meet evolutionary algorithms. (arXiv:2401.10510v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2401.10510</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained large language models (LLMs) have powerful capabilities for
generating creative natural text. Evolutionary algorithms (EAs) can discover
diverse solutions to complex real-world problems. Motivated by the common
collective and directionality of text sequence generation and evolution, this
paper illustrates the strong consistency of LLMs and EAs, which includes
multiple one-to-one key characteristics: token embedding and genotype-phenotype
mapping, position encoding and fitness shaping, position embedding and
selection, attention and crossover, feed-forward neural network and mutation,
model training and parameter update, and multi-task learning and
multi-objective optimization. Based on this consistency perspective, existing
coupling studies are analyzed, including evolutionary fine-tuning and
LLM-enhanced EAs. Leveraging these insights, we outline a fundamental roadmap
for future research in coupling LLMs and EAs, while highlighting key challenges
along the way. The consistency not only reveals the evolution mechanism behind
LLMs but also facilitates the development of evolved artificial agents that
approach or surpass biological organisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1&quot;&gt;Wang Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jiaxuan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1&quot;&gt;Licheng Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lingling Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shuyuan Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10516">
<title>Episodic Reinforcement Learning with Expanded State-reward Space. (arXiv:2401.10516v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10516</link>
<description rdf:parseType="Literal">&lt;p&gt;Empowered by deep neural networks, deep reinforcement learning (DRL) has
demonstrated tremendous empirical successes in various domains, including
games, health care, and autonomous driving. Despite these advancements, DRL is
still identified as data-inefficient as effective policies demand vast numbers
of environmental samples. Recently, episodic control (EC)-based model-free DRL
methods enable sample efficiency by recalling past experiences from episodic
memory. However, existing EC-based methods suffer from the limitation of
potential misalignment between the state and reward spaces for neglecting the
utilization of (past) retrieval states with extensive information, which
probably causes inaccurate value estimation and degraded policy performance. To
tackle this issue, we introduce an efficient EC-based DRL framework with
expanded state-reward space, where the expanded states used as the input and
the expanded rewards used in the training both contain historical and current
information. To be specific, we reuse the historical states retrieved by EC as
part of the input states and integrate the retrieved MC-returns into the
immediate reward in each interactive transition. As a result, our method is
able to simultaneously achieve the full utilization of retrieval information
and the better evaluation of state values by a Temporal Difference (TD) loss.
Empirical results on challenging Box2d and Mujoco tasks demonstrate the
superiority of our method over a recent sibling method and common baselines.
Further, we also verify our method&apos;s effectiveness in alleviating Q-value
overestimation by additional experiments of Q-value comparison.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1&quot;&gt;Dayang Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yaru Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yunlong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10521">
<title>Cross-lingual Editing in Multilingual Language Models. (arXiv:2401.10521v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10521</link>
<description rdf:parseType="Literal">&lt;p&gt;The training of large language models (LLMs) necessitates substantial data
and computational resources, and updating outdated LLMs entails significant
efforts and resources. While numerous model editing techniques (METs) have
emerged to efficiently update model outputs without retraining, their
effectiveness in multilingual LLMs, where knowledge is stored in diverse
languages, remains an underexplored research area. This research paper
introduces the cross-lingual model editing (\textbf{XME}) paradigm, wherein a
fact is edited in one language, and the subsequent update propagation is
observed across other languages. To investigate the XME paradigm, we conducted
experiments using BLOOM, mBERT, and XLM-RoBERTa using the two writing scripts:
\textit{Latin} (English, French, and Spanish) and \textit{Indic} (Hindi,
Gujarati, and Bengali). The results reveal notable performance limitations of
state-of-the-art METs under the XME setting, mainly when the languages involved
belong to two distinct script families. These findings highlight the need for
further research and development of XME techniques to address these challenges.
For more comprehensive information, the dataset used in this research and the
associated code are publicly available at the following
URL\url{https://github.com/lingo-iitgn/XME}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beniwal_H/0/1/0/all/0/1&quot;&gt;Himanshu Beniwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+D_K/0/1/0/all/0/1&quot;&gt;Kowsik Nandagopan D&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Mayank Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10529">
<title>Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences. (arXiv:2401.10529v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10529</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal Large Language Models (MLLMs) have demonstrated proficiency in
handling a variety of visual-language tasks. However, current MLLM benchmarks
are predominantly designed to evaluate reasoning based on static information
about a single image, and the ability of modern MLLMs to extrapolate from image
sequences, which is essential for understanding our ever-changing world, has
been less investigated. To address this challenge, this paper introduces
Mementos, a new benchmark designed to assess MLLMs&apos; sequential image reasoning
abilities. Mementos features 4,761 diverse image sequences with varying
lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning
performance. Through a careful evaluation of nine recent MLLMs on Mementos,
including GPT-4V and Gemini, we find that they struggle to accurately describe
dynamic information about given image sequences, often leading to
hallucinations/misrepresentations of objects and their corresponding behaviors.
Our quantitative analysis and case studies identify three key factors impacting
MLLMs&apos; sequential image reasoning: the correlation between object and
behavioral hallucinations, the influence of cooccurring behaviors, and the
compounding impact of behavioral hallucinations. Our dataset is available at
https://github.com/umd-huang-lab/Mementos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuhang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hongjin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuancheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1&quot;&gt;Feihong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jaehong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Taixi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1&quot;&gt;Gedas Bertasius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Huaxiu Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Furong Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10544">
<title>AAT: Adapting Audio Transformer for Various Acoustics Recognition Tasks. (arXiv:2401.10544v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.10544</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Transformers have been introduced into the field of acoustics
recognition. They are pre-trained on large-scale datasets using methods such as
supervised learning and semi-supervised learning, demonstrating robust
generality--It fine-tunes easily to downstream tasks and shows more robust
performance. However, the predominant fine-tuning method currently used is
still full fine-tuning, which involves updating all parameters during training.
This not only incurs significant memory usage and time costs but also
compromises the model&apos;s generality. Other fine-tuning methods either struggle
to address this issue or fail to achieve matching performance. Therefore, we
conducted a comprehensive analysis of existing fine-tuning methods and proposed
an efficient fine-tuning approach based on Adapter tuning, namely AAT. The core
idea is to freeze the audio Transformer model and insert extra learnable
Adapters, efficiently acquiring downstream task knowledge without compromising
the model&apos;s original generality. Extensive experiments have shown that our
method achieves performance comparable to or even superior to full fine-tuning
while optimizing only 7.118% of the parameters. It also demonstrates
superiority over other fine-tuning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yun Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hai Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1&quot;&gt;Shaojian Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yihang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10559">
<title>OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy. (arXiv:2401.10559v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10559</link>
<description rdf:parseType="Literal">&lt;p&gt;We advance the field of Parameter-Efficient Fine-Tuning (PEFT) with our novel
multi-adapter method, OrchMoE, which capitalizes on modular skill architecture
for enhanced forward transfer in neural networks. Unlike prior models that
depend on explicit task identification inputs, OrchMoE automatically discerns
task categories, streamlining the learning process. This is achieved through an
integrated mechanism comprising an Automatic Task Classification module and a
Task-Skill Allocation module, which collectively deduce task-specific
classifications and tailor skill allocation matrices. Our extensive evaluations
on the &apos;Super Natural Instructions&apos; dataset, featuring 1,600 diverse
instructional tasks, indicate that OrchMoE substantially outperforms comparable
multi-adapter baselines in terms of both performance and sample utilization
efficiency, all while operating within the same parameter constraints. These
findings suggest that OrchMoE offers a significant leap forward in multi-task
learning efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haowen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_K/0/1/0/all/0/1&quot;&gt;Kaixiang Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Cong Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jinjie Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10568">
<title>CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents. (arXiv:2401.10568v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.10568</link>
<description rdf:parseType="Literal">&lt;p&gt;The generalization of decision-making agents encompasses two fundamental
elements: learning from past experiences and reasoning in novel contexts.
However, the predominant emphasis in most interactive environments is on
learning, often at the expense of complexity in reasoning. In this paper, we
introduce CivRealm, an environment inspired by the Civilization game.
Civilization&apos;s profound alignment with human history and society necessitates
sophisticated learning, while its ever-changing situations demand strong
reasoning to generalize. Particularly, CivRealm sets up an
imperfect-information general-sum game with a changing number of players; it
presents a plethora of complex features, challenging the agent to deal with
open-ended stochastic environments that require diplomacy and negotiation
skills. Within CivRealm, we provide interfaces for two typical agent types:
tensor-based agents that focus on learning, and language-based agents that
emphasize reasoning. To catalyze further research, we present initial results
for both paradigms. The canonical RL-based agents exhibit reasonable
performance in mini-games, whereas both RL- and LLM-based agents struggle to
make substantial progress in the full game. Overall, CivRealm stands as a
unique learning and reasoning challenge for decision-making agents. The code is
available at https://github.com/bigai-ai/civrealm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1&quot;&gt;Siyuan Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yexin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1&quot;&gt;Xiangyu Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bangcheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_P/0/1/0/all/0/1&quot;&gt;Pring Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhaowei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Nian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yaodong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Song-Chun Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10586">
<title>PuriDefense: Randomized Local Implicit Adversarial Purification for Defending Black-box Query-based Attacks. (arXiv:2401.10586v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.10586</link>
<description rdf:parseType="Literal">&lt;p&gt;Black-box query-based attacks constitute significant threats to Machine
Learning as a Service (MLaaS) systems since they can generate adversarial
examples without accessing the target model&apos;s architecture and parameters.
Traditional defense mechanisms, such as adversarial training, gradient masking,
and input transformations, either impose substantial computational costs or
compromise the test accuracy of non-adversarial inputs. To address these
challenges, we propose an efficient defense mechanism, PuriDefense, that
employs random patch-wise purifications with an ensemble of lightweight
purification models at a low level of inference cost. These models leverage the
local implicit function and rebuild the natural image manifold. Our theoretical
analysis suggests that this approach slows down the convergence of query-based
attacks by incorporating randomness into purifications. Extensive experiments
on CIFAR-10 and ImageNet validate the effectiveness of our proposed
purifier-based defense mechanism, demonstrating significant improvements in
robustness against query-based attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1&quot;&gt;Ping Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qingchuan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qingfu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10589">
<title>Rethinking the Soft Conflict Pseudo Boolean Constraint on MaxSAT Local Search Solvers. (arXiv:2401.10589v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.10589</link>
<description rdf:parseType="Literal">&lt;p&gt;MaxSAT is an optimization version of the famous NP-complete Satisfiability
problem (SAT). Algorithms for MaxSAT mainly include complete solvers and local
search incomplete solvers. In many complete solvers, once a better solution is
found, a Soft conflict Pseudo Boolean (SPB) constraint will be generated to
enforce the algorithm to find better solutions. In many local search
algorithms, clause weighting is a key technique for effectively guiding the
search directions. In this paper, we propose to transfer the SPB constraint
into the clause weighting system of the local search method, leading the
algorithm to better solutions. We further propose an adaptive clause weighting
strategy that breaks the tradition of using constant values to adjust clause
weights. Based on the above methods, we propose a new local search algorithm
called SPB-MaxSAT that provides new perspectives for clause weighting on MaxSAT
local search solvers. Extensive experiments demonstrate the excellent
performance of the proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jiongzhi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chu-Min Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1&quot;&gt;Kun He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10603">
<title>ZnTrack -- Data as Code. (arXiv:2401.10603v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.10603</link>
<description rdf:parseType="Literal">&lt;p&gt;The past decade has seen tremendous breakthroughs in computation and there is
no indication that this will slow any time soon. Machine learning, large-scale
computing resources, and increased industry focus have resulted in rising
investments in computer-driven solutions for data management, simulations, and
model generation. However, with this growth in computation has come an even
larger expansion of data and with it, complexity in data storage, sharing, and
tracking. In this work, we introduce ZnTrack, a Python-driven data versioning
tool. ZnTrack builds upon established version control systems to provide a
user-friendly and easy-to-use interface for tracking parameters in experiments,
designing workflows, and storing and sharing data. From this ability to reduce
large datasets to a simple Python script emerges the concept of Data as Code, a
core component of the work presented here and an undoubtedly important concept
as the age of computation continues to evolve. ZnTrack offers an open-source,
FAIR data compatible Python package to enable users to harness these concepts
of the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zills_F/0/1/0/all/0/1&quot;&gt;Fabian Zills&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schafer_M/0/1/0/all/0/1&quot;&gt;Moritz Sch&amp;#xe4;fer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tovey_S/0/1/0/all/0/1&quot;&gt;Samuel Tovey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kastner_J/0/1/0/all/0/1&quot;&gt;Johannes K&amp;#xe4;stner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holm_C/0/1/0/all/0/1&quot;&gt;Christian Holm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10640">
<title>A comprehensive study on fidelity metrics for XAI. (arXiv:2401.10640v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10640</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of eXplainable Artificial Intelligence (XAI) systems has introduced a
set of challenges that need resolution. Herein, we focus on how to correctly
select an XAI method, an open questions within the field. The inherent
difficulty of this task is due to the lack of a ground truth. Several authors
have proposed metrics to approximate the fidelity of different XAI methods.
These metrics lack verification and have concerning disagreements. In this
study, we proposed a novel methodology to verify fidelity metrics, using a
well-known transparent model, namely a decision tree. This model allowed us to
obtain explanations with perfect fidelity. Our proposal constitutes the first
objective benchmark for these metrics, facilitating a comparison of existing
proposals, and surpassing existing methods. We applied our benchmark to assess
the existing fidelity metrics in two different experiments, each using public
datasets comprising 52,000 images. The images from these datasets had a size a
128 by 128 pixels and were synthetic data that simplified the training process.
All metric values, indicated a lack of fidelity, with the best one showing a 30
\% deviation from the expected values for perfect explanation. Our
experimentation led us to conclude that the current fidelity metrics are not
reliable enough to be used in real scenarios. From this finding, we deemed it
necessary to development new metrics, to avoid the detected problems, and we
recommend the usage of our proposal as a benchmark within the scientific
community to address these limitations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miro_Nicolau_M/0/1/0/all/0/1&quot;&gt;Miquel Mir&amp;#xf3;-Nicolau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaume_i_Capo_A/0/1/0/all/0/1&quot;&gt;Antoni Jaume-i-Cap&amp;#xf3;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moya_Alcover_G/0/1/0/all/0/1&quot;&gt;Gabriel Moy&amp;#xe0;-Alcover&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10641">
<title>An Effective Index for Truss-based Community Search on Large Directed Graphs. (arXiv:2401.10641v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2401.10641</link>
<description rdf:parseType="Literal">&lt;p&gt;Community search is a derivative of community detection that enables online
and personalized discovery of communities and has found extensive applications
in massive real-world networks. Recently, there needs to be more focus on the
community search issue within directed graphs, even though substantial research
has been carried out on undirected graphs. The recently proposed D-truss model
has achieved good results in the quality of retrieved communities. However,
existing D-truss-based work cannot perform efficient community searches on
large graphs because it consumes too many computing resources to retrieve the
maximal D-truss. To overcome this issue, we introduce an innovative merge
relation known as D-truss-connected to capture the inherent density and
cohesiveness of edges within D-truss. This relation allows us to partition all
the edges in the original graph into a series of D-truss-connected classes.
Then, we construct a concise and compact index, ConDTruss, based on
D-truss-connected. Using ConDTruss, the efficiency of maximum D-truss retrieval
will be greatly improved, making it a theoretically optimal approach.
Experimental evaluations conducted on large directed graph certificate the
effectiveness of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ai_W/0/1/0/all/0/1&quot;&gt;Wei Ai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;CanHao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_T/0/1/0/all/0/1&quot;&gt;Tao Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yinghao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;KeQin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10642">
<title>Fast Butterfly-Core Community Search For Large Labeled Graphs. (arXiv:2401.10642v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2401.10642</link>
<description rdf:parseType="Literal">&lt;p&gt;Community Search (CS) aims to identify densely interconnected subgraphs
corresponding to query vertices within a graph. However, existing heterogeneous
graph-based community search methods need help identifying cross-group
communities and suffer from efficiency issues, making them unsuitable for large
graphs. This paper presents a fast community search model based on the
Butterfly-Core Community (BCC) structure for heterogeneous graphs. The Random
Walk with Restart (RWR) algorithm and butterfly degree comprehensively evaluate
the importance of vertices within communities, allowing leader vertices to be
rapidly updated to maintain cross-group cohesion. Moreover, we devised a more
efficient method for updating vertex distances, which minimizes vertex visits
and enhances operational efficiency. Extensive experiments on several
real-world temporal graphs demonstrate the effectiveness and efficiency of this
solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;JiaYi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yinghao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ai_W/0/1/0/all/0/1&quot;&gt;Wei Ai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_T/0/1/0/all/0/1&quot;&gt;Tao Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;CanHao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;KeQin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10643">
<title>A Comprehensive Survey on Deep-Learning-based Vehicle Re-Identification: Models, Data Sets and Challenges. (arXiv:2401.10643v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10643</link>
<description rdf:parseType="Literal">&lt;p&gt;Vehicle re-identification (ReID) endeavors to associate vehicle images
collected from a distributed network of cameras spanning diverse traffic
environments. This task assumes paramount importance within the spectrum of
vehicle-centric technologies, playing a pivotal role in deploying Intelligent
Transportation Systems (ITS) and advancing smart city initiatives. Rapid
advancements in deep learning have significantly propelled the evolution of
vehicle ReID technologies in recent years. Consequently, undertaking a
comprehensive survey of methodologies centered on deep learning for vehicle
re-identification has become imperative and inescapable. This paper extensively
explores deep learning techniques applied to vehicle ReID. It outlines the
categorization of these methods, encompassing supervised and unsupervised
approaches, delves into existing research within these categories, introduces
datasets and evaluation criteria, and delineates forthcoming challenges and
potential research directions. This comprehensive assessment examines the
landscape of deep learning in vehicle ReID and establishes a foundation and
starting point for future works. It aims to serve as a complete reference by
highlighting challenges and emerging trends, fostering advancements and
applications in vehicle ReID utilizing deep learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amiri_A/0/1/0/all/0/1&quot;&gt;Ali Amiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaya_A/0/1/0/all/0/1&quot;&gt;Aydin Kaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keceli_A/0/1/0/all/0/1&quot;&gt;Ali Seydi Keceli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10660">
<title>A Simple Framework to Accelerate Multilingual Language Model for Monolingual Text Generation. (arXiv:2401.10660v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10660</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in large language models have facilitated the execution
of complex language tasks, not only in English but also in non-English
languages. However, the tokenizers of most language models, such as Llama,
trained on English-centric corpora, tend to excessively fragment tokens in
non-English languages. This issue is especially pronounced in non-roman
alphabetic languages, which are often divided at a character or even Unicode
level, leading to slower text generation. To address this, our study introduces
a novel framework designed to expedite text generation in these languages. This
framework predicts larger linguistic units than those of conventional
multilingual tokenizers and is specifically tailored to the target language,
thereby reducing the number of decoding steps required. Our empirical results
demonstrate that the proposed framework increases the generation speed by a
factor of 1.9 compared to standard decoding while maintaining the performance
of a pre-trained multilingual model on monolingual tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Jimin Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gibbeum Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Jaewoong Cho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10685">
<title>Towards End-to-End GPS Localization with Neural Pseudorange Correction. (arXiv:2401.10685v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10685</link>
<description rdf:parseType="Literal">&lt;p&gt;Pseudorange errors are the root cause of localization inaccuracy in GPS.
Previous data-driven methods regress and eliminate pseudorange errors using
handcrafted intermediate labels. Unlike them, we propose an end-to-end GPS
localization framework, E2E-PrNet, to train a neural network for pseudorange
correction (PrNet) directly using the final task loss calculated with the
ground truth of GPS receiver states. The gradients of the loss with respect to
learnable parameters are backpropagated through a differentiable nonlinear
least squares optimizer to PrNet. The feasibility is verified with GPS data
collected by Android phones, showing that E2E-PrNet outperforms the
state-of-the-art end-to-end GPS localization methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1&quot;&gt;Xu Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_K/0/1/0/all/0/1&quot;&gt;KV Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haochen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_K/0/1/0/all/0/1&quot;&gt;Kun Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10690">
<title>Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models. (arXiv:2401.10690v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10690</link>
<description rdf:parseType="Literal">&lt;p&gt;Dyadic regression models, which predict real-valued outcomes for pairs of
entities, are fundamental in many domains (e.g. predicting the rating of a user
to a product in Recommender Systems) and promising and under exploration in
many others (e.g. approximating the adequate dosage of a drug for a patient in
personalized pharmacology). In this work, we demonstrate that non-uniformity in
the observed value distributions of individual entities leads to severely
biased predictions in state-of-the-art models, skewing predictions towards the
average of observed past values for the entity and providing worse-than-random
predictive power in eccentric yet equally important cases. We show that the
usage of global error metrics like Root Mean Squared Error (RMSE) and Mean
Absolute Error (MAE) is insufficient to capture this phenomenon, which we name
eccentricity bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as
a new complementary metric that can quantify it in all studied models and
datasets. We also prove the adequateness of EAUC by using naive de-biasing
corrections to demonstrate that a lower model bias correlates with a lower EAUC
and vice-versa. This work contributes a bias-aware evaluation of dyadic
regression models to avoid potential unfairness and risks in critical
real-world applications of such systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paz_Ruza_J/0/1/0/all/0/1&quot;&gt;Jorge Paz-Ruza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_Betanzos_A/0/1/0/all/0/1&quot;&gt;Amparo Alonso-Betanzos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guijarro_Berdinas_B/0/1/0/all/0/1&quot;&gt;Bertha Guijarro-Berdi&amp;#xf1;as&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cancela_B/0/1/0/all/0/1&quot;&gt;Brais Cancela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eiras_Franco_C/0/1/0/all/0/1&quot;&gt;Carlos Eiras-Franco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10700">
<title>Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model. (arXiv:2401.10700v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10700</link>
<description rdf:parseType="Literal">&lt;p&gt;Safe offline RL is a promising way to bypass risky online interactions
towards safe policy learning. Most existing methods only enforce soft
constraints, i.e., constraining safety violations in expectation below
thresholds predetermined. This can lead to potentially unsafe outcomes, thus
unacceptable in safety-critical scenarios. An alternative is to enforce the
hard constraint of zero violation. However, this can be challenging in offline
setting, as it needs to strike the right balance among three highly intricate
and correlated aspects: safety constraint satisfaction, reward maximization,
and behavior regularization imposed by offline datasets. Interestingly, we
discover that via reachability analysis of safe-control theory, the hard safety
constraint can be equivalently translated to identifying the largest feasible
region given the offline dataset. This seamlessly converts the original trilogy
problem to a feasibility-dependent objective, i.e., maximizing reward value
within the feasible region while minimizing safety risks in the infeasible
region. Inspired by these, we propose FISOR (FeasIbility-guided Safe Offline
RL), which allows safety constraint adherence, reward maximization, and offline
policy learning to be realized via three decoupled processes, while offering
strong safety performance and stability. In FISOR, the optimal policy for the
translated optimization problem can be derived in a special form of weighted
behavior cloning. Thus, we propose a novel energy-guided diffusion model that
does not require training a complicated time-dependent classifier to extract
the policy, greatly simplifying the training. We compare FISOR against
baselines on DSRL benchmark for safe offline RL. Evaluation results show that
FISOR is the only method that can guarantee safety satisfaction in all tasks,
while achieving top returns in most tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yinan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianxiong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dongjie Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yujie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shengbo Eben Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1&quot;&gt;Xianyuan Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingjing Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10711">
<title>Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering. (arXiv:2401.10711v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10711</link>
<description rdf:parseType="Literal">&lt;p&gt;Video Question Answering (VideoQA) aims to answer natural language questions
based on the information observed in videos. Despite the recent success of
Large Multimodal Models (LMMs) in image-language understanding and reasoning,
they deal with VideoQA insufficiently by simply taking uniformly sampled frames
as visual inputs, which ignores question-relevant visual clues. Moreover, there
are no human annotations for question-critical timestamps in existing VideoQA
datasets. In light of this, we propose a novel weakly supervised framework to
enforce the LMMs to reason out the answers with question-critical moments as
visual inputs. Specifically, we fuse the question and answer pairs as event
descriptions to find multiple keyframes as target moments, which will be
pseudo-labels. With these pseudo-labels as additionally weak supervision, we
devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG
learns multiple Gaussian functions to characterize the temporal structure of
the video, and sample question-critical frames as positive moments to be the
visual inputs of LMMs. Extensive experiments on several VideoQA benchmarks
verify the effectiveness of our framework, and we achieve substantial
improvements compared to previous state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haibo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1&quot;&gt;Chenghang Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yixuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1&quot;&gt;Weifeng Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10712">
<title>Q&amp;A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge. (arXiv:2401.10712v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10712</link>
<description rdf:parseType="Literal">&lt;p&gt;With the breakthrough of multi-modal large language models, answering complex
visual questions that demand advanced reasoning abilities and world knowledge
has become a much more important testbed for developing AI models than ever.
However, equipping AI models with robust cross-modality reasoning ability
remains challenging since the cognition scheme of humans has not been
understood systematically. In this paper, we believe that if we can collect
visual clues in the given image as much as possible, we will recognize the
image more accurately, understand the question better, recall relevant
knowledge more easily, and finally reason out the answer. We discover these
rich visual clues by mining question-answer pairs in images and sending them
into multi-modal large language models as prompts. We call the proposed method
Q&amp;amp;A Prompts. Specifically, we first use the image-answer pairs and the
corresponding questions in the training set as inputs and outputs to train a
visual question generation model. Then, we use an image tagging model to
identify various instances and send packaged image-tag pairs into the visual
question generation model to generate relevant questions with the extracted
image tags as answers. Finally, we encode these generated question-answer pairs
as prompts with a visual-aware prompting module and send them into pre-trained
multi-modal large language models to reason out the final answers. Experimental
results show that, compared with state-of-the-art methods, our Q&amp;amp;A Prompts
achieves substantial improvements on the challenging visual question answering
datasets requiring reasoning over diverse world knowledge, such as OK-VQA and
A-OKVQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haibi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1&quot;&gt;Weifeng Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10725">
<title>Proceedings 14th International Conference on Automated Deduction in Geometry. (arXiv:2401.10725v1 [cs.LO])</title>
<link>http://arxiv.org/abs/2401.10725</link>
<description rdf:parseType="Literal">&lt;p&gt;ADG is a forum to exchange ideas and views, to present research results and
progress, and to demonstrate software tools at the intersection between
geometry and automated deduction. The conference is held every two years. The
previous editions of ADG were held in Hagenberg in 2021 (online, postponed from
2020 due to COVID-19), Nanning in 2018, Strasbourg in 2016, Coimbra in 2014,
Edinburgh in 2012, Munich in 2010, Shanghai in 2008, Pontevedra in 2006,
Gainesville in 2004, Hagenberg in 2002, Zurich in 2000, Beijing in 1998, and
Toulouse in 1996.
&lt;/p&gt;
&lt;p&gt;The 14th edition, ADG 2023, was held in Belgrade, Serbia, in September 20-22,
2023. This edition of ADG had an additional special focus topic, Deduction in
Education.
&lt;/p&gt;
&lt;p&gt;Invited Speakers: Julien Narboux, University of Strasbourg, France
&quot;Formalisation, arithmetization and automatisation of geometry&quot;; Filip Mari\&apos;c,
University of Belgrade, Serbia, &quot;Automatization, formalization and
visualization of hyperbolic geometry&quot;; Zlatan Magajna, University of Ljubljana,
Slovenia, &quot;Workshop OK Geometry&quot;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quaresma_P/0/1/0/all/0/1&quot;&gt;Pedro Quaresma&lt;/a&gt; (University of Coimbra, Portugal), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovacs_Z/0/1/0/all/0/1&quot;&gt;Zolt&amp;#xe1;n Kov&amp;#xe1;cs&lt;/a&gt; (The Private University College of Education of the Diocese of Linz, Austria)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10733">
<title>Dynamic Q&amp;A of Clinical Documents with Large Language Models. (arXiv:2401.10733v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2401.10733</link>
<description rdf:parseType="Literal">&lt;p&gt;Electronic health records (EHRs) house crucial patient data in clinical
notes. As these notes grow in volume and complexity, manual extraction becomes
challenging. This work introduces a natural language interface using large
language models (LLMs) for dynamic question-answering on clinical notes. Our
chatbot, powered by Langchain and transformer-based LLMs, allows users to query
in natural language, receiving relevant answers from clinical notes.
Experiments, utilizing various embedding models and advanced LLMs, show Wizard
Vicuna&apos;s superior accuracy, albeit with high compute demands. Model
optimization, including weight quantization, improves latency by approximately
48 times. Promising results indicate potential, yet challenges such as model
hallucinations and limited diverse medical case evaluations remain. Addressing
these gaps is crucial for unlocking the value in clinical notes and advancing
AI-driven clinical decision-making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elgedawy_R/0/1/0/all/0/1&quot;&gt;Ran Elgedawy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1&quot;&gt;Sudarshan Srinivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danciu_I/0/1/0/all/0/1&quot;&gt;Ioana Danciu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10744">
<title>FinLLMs: A Framework for Financial Reasoning Dataset Generation with Large Language Models. (arXiv:2401.10744v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.10744</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language models (LLMs) usually rely on extensive training datasets. In
the financial domain, creating numerical reasoning datasets that include a mix
of tables and long text often involves substantial manual annotation expenses.
To address the limited data resources and reduce the annotation cost, we
introduce FinLLMs, a method for generating financial question-answering data
based on common financial formulas using Large Language Models. First, we
compile a list of common financial formulas and construct a graph based on the
variables these formulas employ. We then augment the formula set by combining
those that share identical variables as new elements. Specifically, we explore
formulas obtained by manual annotation and merge those formulas with shared
variables by traversing the constructed graph. Finally, utilizing GPT-3.5, we
generate financial question-answering data that encompasses both tabular
information and long textual content, building on the collected formula set.
Our experiments demonstrate that synthetic data generated by FinLLMs
effectively enhances the performance of several large-scale numerical reasoning
models in the financial domain, outperforming two established benchmark
financial question-answering datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Ziqiang Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaiyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Shoutai Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Ye Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingya Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yanlin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wenqi Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10745">
<title>Ethical Artificial Intelligence Principles and Guidelines for the Governance and Utilization of Highly Advanced Large Language Models. (arXiv:2401.10745v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.10745</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the success of ChatGPT, LaMDA and other large language models (LLMs),
there has been an increase in development and usage of LLMs within the
technology sector and other sectors. While the level in which LLMs has not
reached a level where it has surpassed human intelligence, there will be a time
when it will. Such LLMs can be referred to as advanced LLMs. Currently, there
are limited usage of ethical artificial intelligence (AI) principles and
guidelines addressing advanced LLMs due to the fact that we have not reached
that point yet. However, this is a problem as once we do reach that point, we
will not be adequately prepared to deal with the aftermath of it in an ethical
and optimal way, which will lead to undesired and unexpected consequences. This
paper addresses this issue by discussing what ethical AI principles and
guidelines can be used to address highly advanced LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hossain_S/0/1/0/all/0/1&quot;&gt;Soaad Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1&quot;&gt;Syed Ishtiaque Ahmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10746">
<title>A Systematic Evaluation of Euclidean Alignment with Deep Learning for EEG Decoding. (arXiv:2401.10746v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2401.10746</link>
<description rdf:parseType="Literal">&lt;p&gt;Electroencephalography (EEG) signals are frequently used for various
Brain-Computer Interface (BCI) tasks. While Deep Learning (DL) techniques have
shown promising results, they are hindered by the substantial data
requirements. By leveraging data from multiple subjects, transfer learning
enables more effective training of DL models. A technique that is gaining
popularity is Euclidean Alignment (EA) due to its ease of use, low
computational complexity, and compatibility with Deep Learning models. However,
few studies evaluate its impact on the training performance of shared and
individual DL models. In this work, we systematically evaluate the effect of EA
combined with DL for decoding BCI signals. We used EA to train shared models
with data from multiple subjects and evaluated its transferability to new
subjects. Our experimental results show that it improves decoding in the target
subject by 4.33% and decreases convergence time by more than 70%. We also
trained individual models for each subject to use as a majority-voting ensemble
classifier. In this scenario, using EA improved the 3-model ensemble accuracy
by 3.7%. However, when compared to the shared model with EA, the ensemble
accuracy was 3.62% lower.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Junqueira_B/0/1/0/all/0/1&quot;&gt;Bruna Junqueira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aristimunha_B/0/1/0/all/0/1&quot;&gt;Bruno Aristimunha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chevallier_S/0/1/0/all/0/1&quot;&gt;Sylvain Chevallier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Camargo_R/0/1/0/all/0/1&quot;&gt;Raphael Y. de Camargo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10747">
<title>Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach. (arXiv:2401.10747v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.10747</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal sentiment analysis aims to identify the emotions expressed by
individuals through visual, language, and acoustic cues. However, most of the
existing research efforts assume that all modalities are available during both
training and testing, making their algorithms susceptible to the missing
modality scenario. In this paper, we propose a novel knowledge-transfer network
to translate between different modalities to reconstruct the missing audio
modalities. Moreover, we develop a cross-modality attention mechanism to retain
the maximal information of the reconstructed and observed modalities for
sentiment prediction. Extensive experiments on three publicly available
datasets demonstrate significant improvements over baselines and achieve
comparable results to the previous methods with complete multi-modality
supervision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weide Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1&quot;&gt;Huijing Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_F/0/1/0/all/0/1&quot;&gt;Fengmao Lv&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10751">
<title>EFO: the Emotion Frame Ontology. (arXiv:2401.10751v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.10751</link>
<description rdf:parseType="Literal">&lt;p&gt;Emotions are a subject of intense debate in various disciplines. Despite the
proliferation of theories and definitions, there is still no consensus on what
emotions are, and how to model the different concepts involved when we talk
about - or categorize - them. In this paper, we propose an OWL frame-based
ontology of emotions: the Emotion Frames Ontology (EFO). EFO treats emotions as
semantic frames, with a set of semantic roles that capture the different
aspects of emotional experience. EFO follows pattern-based ontology design, and
is aligned to the DOLCE foundational ontology. EFO is used to model multiple
emotion theories, which can be cross-linked as modules in an Emotion Ontology
Network. In this paper, we exemplify it by modeling Ekman&apos;s Basic Emotions (BE)
Theory as an EFO-BE module, and demonstrate how to perform automated inferences
on the representation of emotion situations. EFO-BE has been evaluated by
lexicalizing the BE emotion frames from within the Framester knowledge graph,
and implementing a graph-based emotion detector from text. In addition, an EFO
integration of multimodal datasets, including emotional speech and emotional
face expressions, has been performed to enable further inquiry into crossmodal
emotion semantics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giorgis_S/0/1/0/all/0/1&quot;&gt;Stefano De Giorgis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gangemi_A/0/1/0/all/0/1&quot;&gt;Aldo Gangemi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10753">
<title>BoolGebra: Attributed Graph-learning for Boolean Algebraic Manipulation. (arXiv:2401.10753v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2401.10753</link>
<description rdf:parseType="Literal">&lt;p&gt;Boolean algebraic manipulation is at the core of logic synthesis in
Electronic Design Automation (EDA) design flow. Existing methods struggle to
fully exploit optimization opportunities, and often suffer from an explosive
search space and limited scalability efficiency. This work presents BoolGebra,
a novel attributed graph-learning approach for Boolean algebraic manipulation
that aims to improve fundamental logic synthesis. BoolGebra incorporates Graph
Neural Networks (GNNs) and takes initial feature embeddings from both
structural and functional information as inputs. A fully connected neural
network is employed as the predictor for direct optimization result
predictions, significantly reducing the search space and efficiently locating
the optimization space. The experiments involve training the BoolGebra model
w.r.t design-specific and cross-design inferences using the trained model,
where BoolGebra demonstrates generalizability for cross-design inference and
its potential to scale from small, simple training datasets to large, complex
inference datasets. Finally, BoolGebra is integrated with existing synthesis
tool ABC to perform end-to-end logic minimization evaluation w.r.t SOTA
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agnesina_A/0/1/0/all/0/1&quot;&gt;Anthony Agnesina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Haoxing Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Cunxi Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10759">
<title>Interactions with Prompt Problems: A New Way to Teach Programming with Large Language Models. (arXiv:2401.10759v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.10759</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have upended decades of pedagogy in computing
education. Students previously learned to code through \textit{writing} many
small problems with less emphasis on code reading and comprehension. Recent
research has shown that free code generation tools powered by LLMs can solve
introductory programming problems presented in natural language with ease. In
this paper, we propose a new way to teach programming with Prompt Problems.
Students receive a problem visually, indicating how input should be transformed
to output, and must translate that to a prompt for an LLM to decipher. The
problem is considered correct when the code that is generated by the student
prompt can pass all test cases. In this paper we present the design of this
tool, discuss student interactions with it as they learn, and provide insights
into this new class of programming problems as well as the design tools that
integrate LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prather_J/0/1/0/all/0/1&quot;&gt;James Prather&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1&quot;&gt;Paul Denny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leinonen_J/0/1/0/all/0/1&quot;&gt;Juho Leinonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_D/0/1/0/all/0/1&quot;&gt;David H. Smith IV&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reeves_B/0/1/0/all/0/1&quot;&gt;Brent N. Reeves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacNeil_S/0/1/0/all/0/1&quot;&gt;Stephen MacNeil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Becker_B/0/1/0/all/0/1&quot;&gt;Brett A. Becker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luxton_Reilly_A/0/1/0/all/0/1&quot;&gt;Andrew Luxton-Reilly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amarouche_T/0/1/0/all/0/1&quot;&gt;Thezyrie Amarouche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kimmel_B/0/1/0/all/0/1&quot;&gt;Bailey Kimmel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10781">
<title>Metric Dynamic Equilibrium Logic. (arXiv:2401.10781v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.10781</link>
<description rdf:parseType="Literal">&lt;p&gt;In temporal extensions of Answer Set Programming (ASP) based on linear-time,
the behavior of dynamic systems is captured by sequences of states. While this
representation reflects their relative order, it abstracts away the specific
times associated with each state. In many applications, however, timing
constraints are important like, for instance, when planning and scheduling go
hand in hand. We address this by developing a metric extension of linear-time
Dynamic Equilibrium Logic, in which dynamic operators are constrained by
intervals over integers. The resulting Metric Dynamic Equilibrium Logic
provides the foundation of an ASP-based approach for specifying qualitative and
quantitative dynamic constraints. As such, it constitutes the most general
among a whole spectrum of temporal extensions of Equilibrium Logic. In detail,
we show that it encompasses Temporal, Dynamic, Metric, and regular Equilibrium
Logic, as well as its classic counterparts once the law of the excluded middle
is added.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Becker_A/0/1/0/all/0/1&quot;&gt;Arvid Becker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cabalar_P/0/1/0/all/0/1&quot;&gt;Pedro Cabalar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dieguez_M/0/1/0/all/0/1&quot;&gt;Mart&amp;#xed;n Di&amp;#xe9;guez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farinas_L/0/1/0/all/0/1&quot;&gt;Luis Fari&amp;#xf1;as&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaub_T/0/1/0/all/0/1&quot;&gt;Torsten Schaub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuhmann_A/0/1/0/all/0/1&quot;&gt;Anna Schuhmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10805">
<title>Learning to Visually Connect Actions and their Effects. (arXiv:2401.10805v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10805</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we introduce the novel concept of visually Connecting Actions
and Their Effects (CATE) in video understanding. CATE can have applications in
areas like task planning and learning from demonstration. We propose different
CATE-based task formulations, such as action selection and action
specification, where video understanding models connect actions and effects at
semantic and fine-grained levels. We observe that different formulations
produce representations capturing intuitive action properties. We also design
various baseline models for action selection and action specification. Despite
the intuitive nature of the task, we observe that models struggle, and humans
outperform them by a large margin. The study aims to establish a foundation for
future efforts, showcasing the flexibility and versatility of connecting
actions and effects in video understanding, with the hope of inspiring advanced
formulations and models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peh_E/0/1/0/all/0/1&quot;&gt;Eric Peh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parmar_P/0/1/0/all/0/1&quot;&gt;Paritosh Parmar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1&quot;&gt;Basura Fernando&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10816">
<title>Co-Pilot for Health: Personalized Algorithmic AI Nudging to Improve Health Outcomes. (arXiv:2401.10816v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.10816</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to shape health behaviors of large populations automatically,
across wearable types and disease conditions at scale has tremendous potential
to improve global health outcomes. We designed and implemented an AI driven
platform for digital algorithmic nudging, enabled by a Graph-Neural Network
(GNN) based Recommendation System, and granular health behavior data from
wearable fitness devices. Here we describe the efficacy results of this
platform with its capabilities of personalized and contextual nudging to
$n=84,764$ individuals over a 12-week period in Singapore. We statistically
validated that participants in the target group who received such AI optimized
daily nudges increased daily physical activity like step count by 6.17% ($p =
3.09\times10^{-4}$) and weekly minutes of Moderate to Vigorous Physical
Activity (MVPA) by 7.61% ($p = 1.16\times10^{-2}$), compared to matched
participants in control group who did not receive any nudges. Further, such
nudges were very well received, with a 13.1% of nudges sent being opened (open
rate), and 11.7% of the opened nudges rated useful compared to 1.9% rated as
not useful thereby demonstrating significant improvement in population level
engagement metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiam_J/0/1/0/all/0/1&quot;&gt;Jodi Chiam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_A/0/1/0/all/0/1&quot;&gt;Aloysius Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nott_C/0/1/0/all/0/1&quot;&gt;Cheryl Nott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mark_N/0/1/0/all/0/1&quot;&gt;Nicholas Mark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teredesai_A/0/1/0/all/0/1&quot;&gt;Ankur Teredesai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shinde_S/0/1/0/all/0/1&quot;&gt;Sunil Shinde&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10819">
<title>Optimisation in Neurosymbolic Learning Systems. (arXiv:2401.10819v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.10819</link>
<description rdf:parseType="Literal">&lt;p&gt;Neurosymbolic AI aims to integrate deep learning with symbolic AI. This
integration has many promises, such as decreasing the amount of data required
to train a neural network, improving the explainability and interpretability of
answers given by models and verifying the correctness of trained systems. We
study neurosymbolic learning, where we have both data and background knowledge
expressed using symbolic languages. How do we connect the symbolic and neural
components to communicate this knowledge? One option is fuzzy reasoning, which
studies degrees of truth. For example, being tall is not a binary concept.
Instead, probabilistic reasoning studies the probability that something is true
or will happen. Our first research question studies how different forms of
fuzzy reasoning combine with learning. We find surprising results like a
connection to the Raven paradox stating we confirm &quot;ravens are black&quot; when we
observe a green apple. In this study, we did not use the background knowledge
when we deployed our models after training. In our second research question, we
studied how to use background knowledge in deployed models. We developed a new
neural network layer based on fuzzy reasoning. Probabilistic reasoning is a
natural fit for neural networks, which we usually train to be probabilistic.
However, they are expensive to compute and do not scale well to large tasks. In
our third research question, we study how to connect probabilistic reasoning
with neural networks by sampling to estimate averages, while in the final
research question, we study scaling probabilistic neurosymbolic learning to
much larger problems than before. Our insight is to train a neural network with
synthetic data to predict the result of probabilistic reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krieken_E/0/1/0/all/0/1&quot;&gt;Emile van Krieken&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10831">
<title>Understanding Video Transformers via Universal Concept Discovery. (arXiv:2401.10831v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10831</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the problem of concept-based interpretability of
transformer representations for videos. Concretely, we seek to explain the
decision-making process of video transformers based on high-level,
spatiotemporal concepts that are automatically discovered. Prior research on
concept-based interpretability has concentrated solely on image-level tasks.
Comparatively, video models deal with the added temporal dimension, increasing
complexity and posing challenges in identifying dynamic concepts over time. In
this work, we systematically address these challenges by introducing the first
Video Transformer Concept Discovery (VTCD) algorithm. To this end, we propose
an efficient approach for unsupervised identification of units of video
transformer representations - concepts, and ranking their importance to the
output of a model. The resulting concepts are highly interpretable, revealing
spatio-temporal reasoning mechanisms and object-centric representations in
unstructured video models. Performing this analysis jointly over a diverse set
of supervised and self-supervised representations, we discover that some of
these mechanism are universal in video transformers. Finally, we demonstrate
that VTCDcan be used to improve model performance for fine-grained tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kowal_M/0/1/0/all/0/1&quot;&gt;Matthew Kowal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dave_A/0/1/0/all/0/1&quot;&gt;Achal Dave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1&quot;&gt;Rares Ambrus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1&quot;&gt;Adrien Gaidon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1&quot;&gt;Konstantinos G. Derpanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tokmakov_P/0/1/0/all/0/1&quot;&gt;Pavel Tokmakov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10839">
<title>Holonic Learning: A Flexible Agent-based Distributed Machine Learning Framework. (arXiv:2401.10839v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2401.10839</link>
<description rdf:parseType="Literal">&lt;p&gt;Ever-increasing ubiquity of data and computational resources in the last
decade have propelled a notable transition in the machine learning paradigm
towards more distributed approaches. Such a transition seeks to not only tackle
the scalability and resource distribution challenges but also to address
pressing privacy and security concerns. To contribute to the ongoing discourse,
this paper introduces Holonic Learning (HoL), a collaborative and
privacy-focused learning framework designed for training deep learning models.
By leveraging holonic concepts, the HoL framework establishes a structured
self-similar hierarchy in the learning process, enabling more nuanced control
over collaborations through the individual model aggregation approach of each
holon, along with their intra-holon commitment and communication patterns. HoL,
in its general form, provides extensive design and flexibility potentials. For
empirical analysis and to demonstrate its effectiveness, this paper implements
HoloAvg, a special variant of HoL that employs weighted averaging for model
aggregation across all holons. The convergence of the proposed method is
validated through experiments on both IID and Non-IID settings of the standard
MNISt dataset. Furthermore, the performance behaviors of HoL are investigated
under various holarchical designs and data distribution scenarios. The
presented results affirm HoL&apos;s prowess in delivering competitive performance
particularly, in the context of the Non-IID data distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esmaeili_A/0/1/0/all/0/1&quot;&gt;Ahmad Esmaeili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghorrati_Z/0/1/0/all/0/1&quot;&gt;Zahra Ghorrati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matson_E/0/1/0/all/0/1&quot;&gt;Eric T. Matson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10840">
<title>Symbolic Cognitive Diagnosis via Hybrid Optimization for Intelligent Education Systems. (arXiv:2401.10840v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.10840</link>
<description rdf:parseType="Literal">&lt;p&gt;Cognitive diagnosis assessment is a fundamental and crucial task for student
learning. It models the student-exercise interaction, and discovers the
students&apos; proficiency levels on each knowledge attribute. In real-world
intelligent education systems, generalization and interpretability of cognitive
diagnosis methods are of equal importance. However, most existing methods can
hardly make the best of both worlds due to the complicated student-exercise
interaction. To this end, this paper proposes a symbolic cognitive
diagnosis~(SCD) framework to simultaneously enhance generalization and
interpretability. The SCD framework incorporates the symbolic tree to
explicably represent the complicated student-exercise interaction function, and
utilizes gradient-based optimization methods to effectively learn the student
and exercise parameters. Meanwhile, the accompanying challenge is that we need
to tunnel the discrete symbolic representation and continuous parameter
optimization. To address this challenge, we propose to hybridly optimize the
representation and parameters in an alternating manner. To fulfill SCD, it
alternately learns the symbolic tree by derivative-free genetic programming and
learns the student and exercise parameters via gradient-based Adam. The
extensive experimental results on various real-world datasets show the
superiority of SCD on both generalization and interpretability. The ablation
study verifies the efficacy of each ingredient in SCD, and the case study
explicitly showcases how the interpretable ability of SCD works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Junhao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1&quot;&gt;Hong Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Aimin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10841">
<title>Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media. (arXiv:2401.10841v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10841</link>
<description rdf:parseType="Literal">&lt;p&gt;Online hate speech proliferation has created a difficult problem for social
media platforms. A particular challenge relates to the use of coded language by
groups interested in both creating a sense of belonging for its users and
evading detection. Coded language evolves quickly and its use varies over time.
This paper proposes a methodology for detecting emerging coded hate-laden
terminology. The methodology is tested in the context of online antisemitic
discourse. The approach considers posts scraped from social media platforms,
often used by extremist users. The posts are scraped using seed expressions
related to previously known discourse of hatred towards Jews. The method begins
by identifying the expressions most representative of each post and calculating
their frequency in the whole corpus. It filters out grammatically incoherent
expressions as well as previously encountered ones so as to focus on emergent
well-formed terminology. This is followed by an assessment of semantic
similarity to known antisemitic terminology using a fine-tuned large language
model, and subsequent filtering out of the expressions that are too distant
from known expressions of hatred. Emergent antisemitic expressions containing
terms clearly relating to Jewish topics are then removed to return only coded
expressions of hatred.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kikkisetti_D/0/1/0/all/0/1&quot;&gt;Dhanush Kikkisetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustafa_R/0/1/0/all/0/1&quot;&gt;Raza Ul Mustafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melillo_W/0/1/0/all/0/1&quot;&gt;Wendy Melillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corizzo_R/0/1/0/all/0/1&quot;&gt;Roberto Corizzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boukouvalas_Z/0/1/0/all/0/1&quot;&gt;Zois Boukouvalas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gill_J/0/1/0/all/0/1&quot;&gt;Jeff Gill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Japkowicz_N/0/1/0/all/0/1&quot;&gt;Nathalie Japkowicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10848">
<title>Source-Free and Image-Only Unsupervised Domain Adaptation for Category Level Object Pose Estimation. (arXiv:2401.10848v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10848</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of source-free unsupervised category-level pose
estimation from only RGB images to a target domain without any access to source
domain data or 3D annotations during adaptation. Collecting and annotating
real-world 3D data and corresponding images is laborious, expensive, yet
unavoidable process, since even 3D pose domain adaptation methods require 3D
data in the target domain. We introduce 3DUDA, a method capable of adapting to
a nuisance-ridden target domain without 3D or depth data. Our key insight stems
from the observation that specific object subparts remain stable across
out-of-domain (OOD) scenarios, enabling strategic utilization of these
invariant subcomponents for effective model updates. We represent object
categories as simple cuboid meshes, and harness a generative model of neural
feature activations modeled at each mesh vertex learnt using differential
rendering. We focus on individual locally robust mesh vertex features and
iteratively update them based on their proximity to corresponding features in
the target domain even when the global pose is not correct. Our model is then
trained in an EM fashion, alternating between updating the vertex features and
the feature extractor. We show that our method simulates fine-tuning on a
global pseudo-labeled dataset under mild assumptions, which converges to the
target domain asymptotically. Through extensive empirical validation, including
a complex extreme UDA setup which combines real nuisances, synthetic noise, and
occlusion, we demonstrate the potency of our simple approach in addressing the
domain shift challenge and significantly improving pose estimation accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaushik_P/0/1/0/all/0/1&quot;&gt;Prakhar Kaushik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1&quot;&gt;Aayush Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1&quot;&gt;Adam Kortylewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1&quot;&gt;Alan Yuille&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10850">
<title>Advancements in eHealth Data Analytics through Natural Language Processing and Deep Learning. (arXiv:2401.10850v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10850</link>
<description rdf:parseType="Literal">&lt;p&gt;The healthcare environment is commonly referred to as &quot;information-rich&quot; but
also &quot;knowledge poor&quot;. Healthcare systems collect huge amounts of data from
various sources: lab reports, medical letters, logs of medical tools or
programs, medical prescriptions, etc. These massive sets of data can provide
great knowledge and information that can improve the medical services, and
overall the healthcare domain, such as disease prediction by analyzing the
patient&apos;s symptoms or disease prevention, by facilitating the discovery of
behavioral factors for diseases. Unfortunately, only a relatively small volume
of the textual eHealth data is processed and interpreted, an important factor
being the difficulty in efficiently performing Big Data operations. In the
medical field, detecting domain-specific multi-word terms is a crucial task as
they can define an entire concept with a few words. A term can be defined as a
linguistic structure or a concept, and it is composed of one or more words with
a specific meaning to a domain. All the terms of a domain create its
terminology. This chapter offers a critical study of the current, most
performant solutions for analyzing unstructured (image and textual) eHealth
data. This study also provides a comparison of the current Natural Language
Processing and Deep Learning techniques in the eHealth context. Finally, we
examine and discuss some of the current issues, and we define a set of research
directions in this area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apostol_E/0/1/0/all/0/1&quot;&gt;Elena-Simona Apostol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truica_C/0/1/0/all/0/1&quot;&gt;Ciprian-Octavian Truic&amp;#x103;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10862">
<title>Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning. (arXiv:2401.10862v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10862</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) are vulnerable to `Jailbreaking&apos; prompts, a type
of attack that can coax these models into generating harmful and illegal
content. In this paper, we show that pruning up to 20% of LLM parameters
markedly increases their resistance to such attacks without additional training
and without sacrificing their performance in standard benchmarks. Intriguingly,
we discovered that the enhanced safety observed post-pruning correlates to the
initial safety training level of the model, hinting that the effect of pruning
could be more general and may hold for other LLM behaviors beyond safety.
Additionally, we introduce a curated dataset of 225 harmful tasks across five
categories, inserted into ten different Jailbreaking prompts, showing that
pruning aids LLMs in concentrating attention on task-relevant tokens in
jailbreaking prompts. Lastly, our experiments reveal that the prominent chat
models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high
susceptibility to jailbreaking attacks, with some categories achieving nearly
70-100% success rate. These insights underline the potential of pruning as a
generalizable approach for improving LLM safety, reliability, and potentially
other desired behaviors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1&quot;&gt;Adib Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rugina_I/0/1/0/all/0/1&quot;&gt;Ileana Rugina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Alex Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10882">
<title>Reinforcement learning for question answering in programming domain using public community scoring as a human feedback. (arXiv:2401.10882v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10882</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we investigate the enhancement of the GPT Neo 125M performance
in Community Question Answering (CQA) with a focus on programming, through the
integration of Reinforcement Learning from Human Feedback (RLHF) and the
utilization of scores from Stack Overflow. Two distinct reward model training
strategies are employed for fine-tuning with Proximal Policy Optimization
(PPO). Notably, the improvements in performance achieved through this method
are comparable to those of GPT Neo 2.7B parameter variant. Additionally, an
auxiliary scoring mechanism is introduced, which demonstrates the limitations
of conventional linguistic metrics in evaluating responses in the programming
domain. Through accurate analysis, this paper looks at the divergence between
traditional linguistic metrics and our human-preferences-based reward model,
underscoring the imperative for domain-specific evaluation methods. By
elucidating the complexities involved in applying RLHF to programming CQA and
accentuating the significance of context-aware evaluation, this study
contributes to the ongoing efforts in refining Large Language Models through
focused human feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorbatovski_A/0/1/0/all/0/1&quot;&gt;Alexey Gorbatovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovalchuk_S/0/1/0/all/0/1&quot;&gt;Sergey Kovalchuk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10886">
<title>SCENES: Subpixel Correspondence Estimation With Epipolar Supervision. (arXiv:2401.10886v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10886</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracting point correspondences from two or more views of a scene is a
fundamental computer vision problem with particular importance for relative
camera pose estimation and structure-from-motion. Existing local feature
matching approaches, trained with correspondence supervision on large-scale
datasets, obtain highly-accurate matches on the test sets. However, they do not
generalise well to new datasets with different characteristics to those they
were trained on, unlike classic feature extractors. Instead, they require
finetuning, which assumes that ground-truth correspondences or ground-truth
camera poses and 3D structure are available. We relax this assumption by
removing the requirement of 3D structure, e.g., depth maps or point clouds, and
only require camera pose information, which can be obtained from odometry. We
do so by replacing correspondence losses with epipolar losses, which encourage
putative matches to lie on the associated epipolar line. While weaker than
correspondence supervision, we observe that this cue is sufficient for
finetuning existing models on new data. We then further relax the assumption of
known camera poses by using pose estimates in a novel bootstrapping approach.
We evaluate on highly challenging datasets, including an indoor drone dataset
and an outdoor smartphone camera dataset, and obtain state-of-the-art results
without strong supervision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kloepfer_D/0/1/0/all/0/1&quot;&gt;Dominik A. Kloepfer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o F. Henriques&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1&quot;&gt;Dylan Campbell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10889">
<title>Synthesizing Moving People with 3D Control. (arXiv:2401.10889v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10889</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a diffusion model-based framework for animating
people from a single image for a given target 3D motion sequence. Our approach
has two core components: a) learning priors about invisible parts of the human
body and clothing, and b) rendering novel body poses with proper clothing and
texture. For the first part, we learn an in-filling diffusion model to
hallucinate unseen parts of a person given a single image. We train this model
on texture map space, which makes it more sample-efficient since it is
invariant to pose and viewpoint. Second, we develop a diffusion-based rendering
pipeline, which is controlled by 3D human poses. This produces realistic
renderings of novel poses of the person, including clothing, hair, and
plausible in-filling of unseen regions. This disentangled approach allows our
method to generate a sequence of images that are faithful to the target motion
in the 3D pose and, to the input image in terms of visual similarity. In
addition to that, the 3D control allows various synthetic camera trajectories
to render a person. Our experiments show that our method is resilient in
generating prolonged motions and varied challenging and complex poses compared
to prior methods. Please check our website for more details:
https://boyiliee.github.io/3DHM.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Boyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajasegaran_J/0/1/0/all/0/1&quot;&gt;Jathushan Rajasegaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandelsman_Y/0/1/0/all/0/1&quot;&gt;Yossi Gandelsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1&quot;&gt;Alexei A. Efros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1&quot;&gt;Jitendra Malik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1812.01243">
<title>Efficient Attention: Attention with Linear Complexities. (arXiv:1812.01243v10 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1812.01243</link>
<description rdf:parseType="Literal">&lt;p&gt;Dot-product attention has wide applications in computer vision and natural
language processing. However, its memory and computational costs grow
quadratically with the input size. Such growth prohibits its application on
high-resolution inputs. To remedy this drawback, this paper proposes a novel
efficient attention mechanism equivalent to dot-product attention but with
substantially less memory and computational costs. Its resource efficiency
allows more widespread and flexible integration of attention modules into a
network, which leads to better accuracies. Empirical evaluations demonstrated
the effectiveness of its advantages. Efficient attention modules brought
significant performance boosts to object detectors and instance segmenters on
MS-COCO 2017. Further, the resource efficiency democratizes attention to
complex models, where high costs prohibit the use of dot-product attention. As
an exemplar, a model with efficient attention achieved state-of-the-art
accuracies for stereo depth estimation on the Scene Flow dataset. Code is
available at https://github.com/cmsflash/efficient-attention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haiyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1&quot;&gt;Shuai Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.06826">
<title>Few-shot Quality-Diversity Optimization. (arXiv:2109.06826v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2109.06826</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past few years, a considerable amount of research has been dedicated
to the exploitation of previous learning experiences and the design of Few-shot
and Meta Learning approaches, in problem domains ranging from Computer Vision
to Reinforcement Learning based control. A notable exception, where to the best
of our knowledge, little to no effort has been made in this direction is
Quality-Diversity (QD) optimization. QD methods have been shown to be effective
tools in dealing with deceptive minima and sparse rewards in Reinforcement
Learning. However, they remain costly due to their reliance on inherently
sample inefficient evolutionary processes. We show that, given examples from a
task distribution, information about the paths taken by optimization in
parameter space can be leveraged to build a prior population, which when used
to initialize QD methods in unseen environments, allows for few-shot
adaptation. Our proposed method does not require backpropagation. It is simple
to implement and scale, and furthermore, it is agnostic to the underlying
models that are being trained. Experiments carried in both sparse and dense
reward settings using robotic manipulation and navigation benchmarks show that
it considerably reduces the number of generations that are required for QD
optimization in these environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salehi_A/0/1/0/all/0/1&quot;&gt;Achkan Salehi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coninx_A/0/1/0/all/0/1&quot;&gt;Alexandre Coninx&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doncieux_S/0/1/0/all/0/1&quot;&gt;Stephane Doncieux&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.05359">
<title>Exploring Local Explanations of Nonlinear Models Using Animated Linear Projections. (arXiv:2205.05359v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2205.05359</link>
<description rdf:parseType="Literal">&lt;p&gt;The increased predictive power of machine learning models comes at the cost
of increased complexity and loss of interpretability, particularly in
comparison to parametric statistical models. This trade-off has led to the
emergence of eXplainable AI (XAI) which provides methods, such as local
explanations (LEs) and local variable attributions (LVAs), to shed light on how
a model use predictors to arrive at a prediction. These provide a point
estimate of the linear variable importance in the vicinity of a single
observation. However, LVAs tend not to effectively handle association between
predictors. To understand how the interaction between predictors affects the
variable importance estimate, we can convert LVAs into linear projections and
use the radial tour. This is also useful for learning how a model has made a
mistake, or the effect of outliers, or the clustering of observations. The
approach is illustrated with examples from categorical (penguin species,
chocolate types) and quantitative (soccer/football salaries, house prices)
response models. The methods are implemented in the R package cheem, available
on CRAN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Spyrison_N/0/1/0/all/0/1&quot;&gt;Nicholas Spyrison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cook_D/0/1/0/all/0/1&quot;&gt;Dianne Cook&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Biecek_P/0/1/0/all/0/1&quot;&gt;Przemyslaw Biecek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13350">
<title>Choreographer: Learning and Adapting Skills in Imagination. (arXiv:2211.13350v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13350</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised skill learning aims to learn a rich repertoire of behaviors
without external supervision, providing artificial agents with the ability to
control and influence the environment. However, without appropriate knowledge
and exploration, skills may provide control only over a restricted area of the
environment, limiting their applicability. Furthermore, it is unclear how to
leverage the learned skill behaviors for adapting to downstream tasks in a
data-efficient manner. We present Choreographer, a model-based agent that
exploits its world model to learn and adapt skills in imagination. Our method
decouples the exploration and skill learning processes, being able to discover
skills in the latent state space of the model. During adaptation, the agent
uses a meta-controller to evaluate and adapt the learned skills efficiently by
deploying them in parallel in imagination. Choreographer is able to learn
skills both from offline data, and by collecting data simultaneously with an
exploration policy. The skills can be used to effectively adapt to downstream
tasks, as we show in the URL benchmark, where we outperform previous approaches
from both pixels and states inputs. The learned skills also explore the
environment thoroughly, finding sparse rewards more frequently, as shown in
goal-reaching tasks from the DMC Suite and Meta-World. Website and code:
https://skillchoreographer.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazzaglia_P/0/1/0/all/0/1&quot;&gt;Pietro Mazzaglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbelen_T/0/1/0/all/0/1&quot;&gt;Tim Verbelen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhoedt_B/0/1/0/all/0/1&quot;&gt;Bart Dhoedt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacoste_A/0/1/0/all/0/1&quot;&gt;Alexandre Lacoste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajeswar_S/0/1/0/all/0/1&quot;&gt;Sai Rajeswar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01521">
<title>Distribution Fitting for Combating Mode Collapse in Generative Adversarial Networks. (arXiv:2212.01521v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01521</link>
<description rdf:parseType="Literal">&lt;p&gt;Mode collapse is a significant unsolved issue of generative adversarial
networks. In this work, we examine the causes of mode collapse from a novel
perspective. Due to the nonuniform sampling in the training process, some
sub-distributions may be missed when sampling data. As a result, even when the
generated distribution differs from the real one, the GAN objective can still
achieve the minimum. To address the issue, we propose a global distribution
fitting (GDF) method with a penalty term to confine the generated data
distribution. When the generated distribution differs from the real one, GDF
will make the objective harder to reach the minimal value, while the original
global minimum is not changed. To deal with the circumstance when the overall
real data is unreachable, we also propose a local distribution fitting (LDF)
method. Experiments on several benchmarks demonstrate the effectiveness and
competitive performance of GDF and LDF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yanxiang Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_G/0/1/0/all/0/1&quot;&gt;Guozhen Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zheng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1&quot;&gt;Mei Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09726">
<title>Improving Faithfulness of Abstractive Summarization by Controlling Confounding Effect of Irrelevant Sentences. (arXiv:2212.09726v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09726</link>
<description rdf:parseType="Literal">&lt;p&gt;Lack of factual correctness is an issue that still plagues state-of-the-art
summarization systems despite their impressive progress on generating seemingly
fluent summaries. In this paper, we show that factual inconsistency can be
caused by irrelevant parts of the input text, which act as confounders. To that
end, we leverage information-theoretic measures of causal effects to quantify
the amount of confounding and precisely quantify how they affect the
summarization performance. Based on insights derived from our theoretical
results, we design a simple multi-task model to control such confounding by
leveraging human-annotated relevant sentences when available. Crucially, we
give a principled characterization of data distributions where such confounding
can be large thereby necessitating the use of human annotated relevant
sentences to generate factual summaries. Our approach improves faithfulness
scores by 20\% over strong baselines on AnswerSumm
\citep{fabbri2021answersumm}, a conversation summarization dataset where lack
of faithfulness is a significant issue due to the subjective nature of the
task. Our best method achieves the highest faithfulness score while also
achieving state-of-the-art results on standard metrics like ROUGE and METEOR.
We corroborate these improvements through human evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghoshal_A/0/1/0/all/0/1&quot;&gt;Asish Ghoshal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Einolghozati_A/0/1/0/all/0/1&quot;&gt;Arash Einolghozati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arun_A/0/1/0/all/0/1&quot;&gt;Ankit Arun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lili Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gor_V/0/1/0/all/0/1&quot;&gt;Vera Gor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1&quot;&gt;Yashar Mehdad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yih_S/0/1/0/all/0/1&quot;&gt;Scott Wen-tau Yih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1&quot;&gt;Asli Celikyilmaz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10766">
<title>On the Adversarial Robustness of Camera-based 3D Object Detection. (arXiv:2301.10766v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10766</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, camera-based 3D object detection has gained widespread
attention for its ability to achieve high performance with low computational
cost. However, the robustness of these methods to adversarial attacks has not
been thoroughly examined, especially when considering their deployment in
safety-critical domains like autonomous driving. In this study, we conduct the
first comprehensive investigation of the robustness of leading camera-based 3D
object detection approaches under various adversarial conditions. We
systematically analyze the resilience of these models under two attack
settings: white-box and black-box; focusing on two primary objectives:
classification and localization. Additionally, we delve into two types of
adversarial attack techniques: pixel-based and patch-based. Our experiments
yield four interesting findings: (a) bird&apos;s-eye-view-based representations
exhibit stronger robustness against localization attacks; (b)
depth-estimation-free approaches have the potential to show stronger
robustness; (c) accurate depth estimation effectively improves robustness for
depth-estimation-based methods; (d) incorporating multi-frame benign inputs can
effectively mitigate adversarial attacks. We hope our findings can steer the
development of future camera-based object detection models with enhanced
adversarial robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Shaoyuan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zichao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Cihang Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13359">
<title>IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing. (arXiv:2301.13359v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13359</link>
<description rdf:parseType="Literal">&lt;p&gt;Image anomaly detection (IAD) is an emerging and vital computer vision task
in industrial manufacturing (IM). Recently, many advanced algorithms have been
reported, but their performance deviates considerably with various IM settings.
We realize that the lack of a uniform IM benchmark is hindering the development
and usage of IAD methods in real-world applications. In addition, it is
difficult for researchers to analyze IAD algorithms without a uniform
benchmark. To solve this problem, we propose a uniform IM benchmark, for the
first time, to assess how well these algorithms perform, which includes various
levels of supervision (unsupervised versus fully supervised), learning
paradigms (few-shot, continual and noisy label), and efficiency (memory usage
and inference speed). Then, we construct a comprehensive image anomaly
detection benchmark (IM-IAD), which includes 19 algorithms on seven major
datasets with a uniform setting. Extensive experiments (17,017 total) on IM-IAD
provide in-depth insights into IAD algorithm redesign or selection. Moreover,
the proposed IM-IAD benchmark challenges existing algorithms and suggests
future research directions. To foster reproducibility and accessibility, the
source code of IM-IAD is uploaded on the website,
https://github.com/M-3LAB/IM-IAD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1&quot;&gt;Guoyang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinbao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1&quot;&gt;Jiayi Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1&quot;&gt;Feng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yaochu Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12190">
<title>MCWDST: a Minimum-Cost Weighted Directed Spanning Tree Algorithm for Real-Time Fake News Mitigation in Social Media. (arXiv:2302.12190v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12190</link>
<description rdf:parseType="Literal">&lt;p&gt;The widespread availability of internet access and handheld devices confers
to social media a power similar to the one newspapers used to have. People seek
affordable information on social media and can reach it within seconds. Yet
this convenience comes with dangers; any user may freely post whatever they
please and the content can stay online for a long period, regardless of its
truthfulness. A need to detect untruthful information, also known as fake news,
arises. In this paper, we present an end-to-end solution that accurately
detects fake news and immunizes network nodes that spread them in real-time. To
detect fake news, we propose two new stack deep learning architectures that
utilize convolutional and bidirectional LSTM layers. To mitigate the spread of
fake news, we propose a real-time network-aware strategy that (1) constructs a
minimum-cost weighted directed spanning tree for a detected node, and (2)
immunizes nodes in that tree by scoring their harmfulness using a novel ranking
function. We demonstrate the effectiveness of our solution on five real-world
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truica_C/0/1/0/all/0/1&quot;&gt;Ciprian-Octavian Truic&amp;#x103;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apostol_E/0/1/0/all/0/1&quot;&gt;Elena-Simona Apostol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolescu_R/0/1/0/all/0/1&quot;&gt;Radu-C&amp;#x103;t&amp;#x103;lin Nicolescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karras_P/0/1/0/all/0/1&quot;&gt;Panagiotis Karras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.02506">
<title>Prismer: A Vision-Language Model with Multi-Task Experts. (arXiv:2303.02506v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.02506</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent vision-language models have shown impressive multi-modal generation
capabilities. However, typically they require training huge models on massive
datasets. As a more scalable alternative, we introduce Prismer, a data- and
parameter-efficient vision-language model that leverages an ensemble of
task-specific experts. Prismer only requires training of a small number of
components, with the majority of network weights inherited from multiple
readily-available, pre-trained experts, and kept frozen during training. By
leveraging experts from a wide range of domains, we show Prismer can
efficiently pool this expert knowledge and adapt it to various vision-language
reasoning tasks. In our experiments, we show that Prismer achieves fine-tuned
and few-shot learning performance which is competitive with current
state-of-the-arts, whilst requiring up to two orders of magnitude less training
data. Code is available at https://github.com/NVlabs/prismer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shikun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Linxi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1&quot;&gt;Edward Johns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhiding Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1&quot;&gt;Anima Anandkumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15954">
<title>TraffNet: Learning Causality of Traffic Generation for What-if Prediction. (arXiv:2303.15954v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15954</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time what-if traffic prediction is crucial for decision making in
intelligent traffic management and control. Although current deep learning
methods demonstrate significant advantages in traffic prediction, they are
powerless in what-if traffic prediction due to their nature of
correlation-based. Here, we present a simple deep learning framework called
TraffNet that learns the mechanisms of traffic generation for what-if
prediction from vehicle trajectory data. First, we use a heterogeneous graph to
represent the road network, allowing the model to incorporate causal features
of traffic flows, such as Origin-Destination (OD) demands and routes. Next, we
propose a method for learning segment representations, which involves modeling
the process of assigning OD demands onto the road network. The learned segment
representations effectively encapsulate the intricate causes of traffic
generation, facilitating downstream what-if traffic prediction. Finally, we
conduct experiments on synthetic datasets to evaluate the effectiveness of
TraffNet. The code and datasets of TraffNet is available at
https://github.com/mayunyi-1999/TraffNet_code.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Ming Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1&quot;&gt;Qiang Ai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruimin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunyi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1&quot;&gt;Geqi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xiangfu Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Haibo Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17046">
<title>Have it your way: Individualized Privacy Assignment for DP-SGD. (arXiv:2303.17046v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17046</link>
<description rdf:parseType="Literal">&lt;p&gt;When training a machine learning model with differential privacy, one sets a
privacy budget. This budget represents a maximal privacy violation that any
user is willing to face by contributing their data to the training set. We
argue that this approach is limited because different users may have different
privacy expectations. Thus, setting a uniform privacy budget across all points
may be overly conservative for some users or, conversely, not sufficiently
protective for others. In this paper, we capture these preferences through
individualized privacy budgets. To demonstrate their practicality, we introduce
a variant of Differentially Private Stochastic Gradient Descent (DP-SGD) which
supports such individualized budgets. DP-SGD is the canonical approach to
training models with differential privacy. We modify its data sampling and
gradient noising mechanisms to arrive at our approach, which we call
Individualized DP-SGD (IDP-SGD). Because IDP-SGD provides privacy guarantees
tailored to the preferences of individual users and their data points, we find
it empirically improves privacy-utility trade-offs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boenisch_F/0/1/0/all/0/1&quot;&gt;Franziska Boenisch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muhl_C/0/1/0/all/0/1&quot;&gt;Christopher M&amp;#xfc;hl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dziedzic_A/0/1/0/all/0/1&quot;&gt;Adam Dziedzic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rinberg_R/0/1/0/all/0/1&quot;&gt;Roy Rinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1&quot;&gt;Nicolas Papernot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.00746">
<title>OTS: A One-shot Learning Approach for Text Spotting in Historical Manuscripts. (arXiv:2304.00746v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.00746</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of historical manuscript research, scholars frequently encounter
novel symbols in ancient texts, investing considerable effort in their
identification and documentation. Although some object detection methods have
achieved impressive performance, they primarily excel at detecting categories
included in training datasets, often failing to recognize novel symbols without
retraining. To overcome this limitation, we propose a novel One-shot
learning-based Text Spotting (OTS) approach that accurately and reliably spots
novel characters with just one annotated support sample. Drawing inspiration
from cognitive research, we introduce a spatial alignment module that finds,
focuses on, and learns the most discriminative spatial regions in the query
image based on one support image. Especially, since the low-resource spotting
task often faces the problem of example imbalance, we propose a novel loss
function called torus loss which can make the embedding space of distance
metric more discriminative. Our approach is highly efficient and requires only
a few training samples while exhibiting the remarkable ability to handle novel
characters and symbols. To enhance dataset diversity, a new manuscript dataset
that contains the ancient Dongba hieroglyphics (DBH) is created, a script
associated with China and developed by the ancestors of the Naxi minority. We
conduct experiments on publicly available DBH, EGY, VML-HD, TKH, and NC
datasets. The experimental results demonstrate that OTS outperforms the
state-of-the-art methods in one-shot text spotting. Overall, our proposed
method offers promising applications in text spotting in historical
manuscripts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenbo Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1&quot;&gt;Hongjian Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1&quot;&gt;Bing Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yue Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11171">
<title>Granular-ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method. (arXiv:2304.11171v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.11171</link>
<description rdf:parseType="Literal">&lt;p&gt;Human cognition operates on a &quot;Global-first&quot; cognitive mechanism,
prioritizing information processing based on coarse-grained details. This
mechanism inherently possesses an adaptive multi-granularity description
capacity, resulting in computational traits such as efficiency, robustness, and
interpretability. The analysis pattern reliance on the finest granularity and
single-granularity makes most existing computational methods less efficient,
robust, and interpretable, which is an important reason for the current lack of
interpretability in neural networks. Multi-granularity granular-ball computing
employs granular-balls of varying sizes to daptively represent and envelop the
sample space, facilitating learning based on these granular-balls. Given that
the number of coarse-grained &quot;granular-balls&quot; is fewer than sample points,
granular-ball computing proves more efficient. Moreover, the inherent
coarse-grained nature of granular-balls reduces susceptibility to fine-grained
sample disturbances, enhancing robustness. The multi-granularity construct of
granular-balls generates topological structures and coarse-grained
descriptions, naturally augmenting interpretability. Granular-ball computing
has successfully ventured into diverse AI domains, fostering the development of
innovative theoretical methods, including granular-ball classifiers, clustering
techniques, neural networks, rough sets, and evolutionary computing. This has
notably ameliorated the efficiency, noise robustness, and interpretability of
traditional methods. Overall, granular-ball computing is a rare and innovative
theoretical approach in AI that can adaptively and simultaneously enhance
efficiency, robustness, and interpretability. This article delves into the main
application landscapes for granular-ball computing, aiming to equip future
researchers with references and insights to refine and expand this promising
theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shuyin Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guoyin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Lian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.13189">
<title>Onboard Science Instrument Autonomy for the Detection of Microscopy Biosignatures on the Ocean Worlds Life Surveyor. (arXiv:2304.13189v2 [astro-ph.IM] UPDATED)</title>
<link>http://arxiv.org/abs/2304.13189</link>
<description rdf:parseType="Literal">&lt;p&gt;The quest to find extraterrestrial life is a critical scientific endeavor
with civilization-level implications. Icy moons in our solar system are
promising targets for exploration because their liquid oceans make them
potential habitats for microscopic life. However, the lack of a precise
definition of life poses a fundamental challenge to formulating detection
strategies. To increase the chances of unambiguous detection, a suite of
complementary instruments must sample multiple independent biosignatures (e.g.,
composition, motility/behavior, and visible structure). Such an instrument
suite could generate 10,000x more raw data than is possible to transmit from
distant ocean worlds like Enceladus or Europa. To address this bandwidth
limitation, Onboard Science Instrument Autonomy (OSIA) is an emerging
discipline of flight systems capable of evaluating, summarizing, and
prioritizing observational instrument data to maximize science return. We
describe two OSIA implementations developed as part of the Ocean Worlds Life
Surveyor (OWLS) prototype instrument suite at the Jet Propulsion Laboratory.
The first identifies life-like motion in digital holographic microscopy videos,
and the second identifies cellular structure and composition via innate and
dye-induced fluorescence. Flight-like requirements and computational
constraints were used to lower barriers to infusion, similar to those available
on the Mars helicopter, &quot;Ingenuity.&quot; We evaluated the OSIA&apos;s performance using
simulated and laboratory data and conducted a live field test at the
hypersaline Mono Lake planetary analog site. Our study demonstrates the
potential of OSIA for enabling biosignature detection and provides insights and
lessons learned for future mission concepts aimed at exploring the outer solar
system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Wronkiewicz_M/0/1/0/all/0/1&quot;&gt;Mark Wronkiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jake Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Mandrake_L/0/1/0/all/0/1&quot;&gt;Lukas Mandrake&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Lightholder_J/0/1/0/all/0/1&quot;&gt;Jack Lightholder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Doran_G/0/1/0/all/0/1&quot;&gt;Gary Doran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Mauceri_S/0/1/0/all/0/1&quot;&gt;Steffen Mauceri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taewoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Oborny_N/0/1/0/all/0/1&quot;&gt;Nathan Oborny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Schibler_T/0/1/0/all/0/1&quot;&gt;Thomas Schibler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Nadeau_J/0/1/0/all/0/1&quot;&gt;Jay Nadeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Wallace_J/0/1/0/all/0/1&quot;&gt;James K. Wallace&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Moorjani_E/0/1/0/all/0/1&quot;&gt;Eshaan Moorjani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Lindensmith_C/0/1/0/all/0/1&quot;&gt;Chris Lindensmith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07863">
<title>Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control. (arXiv:2306.07863v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07863</link>
<description rdf:parseType="Literal">&lt;p&gt;Building agents with large language models (LLMs) for computer control is a
burgeoning research area, where the agent receives computer states and performs
actions to complete complex tasks. Previous computer agents have demonstrated
the benefits of in-context learning (ICL); however, their performance is
hindered by several issues. First, the limited context length of LLMs and
complex computer states restrict the number of exemplars, as a single webpage
can consume the entire context. Second, the exemplars in current methods, such
as high-level plans and multi-choice questions, cannot represent complete
trajectories, leading to suboptimal performance in long-horizon tasks. Third,
existing computer agents rely on task-specific exemplars and overlook the
similarity among tasks, resulting in poor generalization to novel tasks. To
address these challenges, we introduce Synapse, a computer agent featuring
three key components: i) state abstraction, which filters out task-irrelevant
information from raw states, allowing more exemplars within the limited
context, ii) trajectory-as-exemplar prompting, which prompts the LLM with
complete trajectories of the abstracted states and actions to improve
multi-step decision-making, and iii) exemplar memory, which stores the
embeddings of exemplars and retrieves them via similarity search for
generalization to novel tasks. We evaluate Synapse on MiniWoB++, a standard
task suite, and Mind2Web, a real-world website benchmark. In MiniWoB++, Synapse
achieves a 99.2% average success rate (a 10% relative improvement) across 64
tasks using demonstrations from only 48 tasks. Notably, Synapse is the first
ICL method to solve the book-flight task in MiniWoB++. Synapse also exhibits a
56% relative improvement in average step success rate over the previous
state-of-the-art prompting scheme in Mind2Web.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Longtao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rundong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinrun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1&quot;&gt;Bo An&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08251">
<title>GBSD: Generative Bokeh with Stage Diffusion. (arXiv:2306.08251v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08251</link>
<description rdf:parseType="Literal">&lt;p&gt;The bokeh effect is an artistic technique that blurs out-of-focus areas in a
photograph and has gained interest due to recent developments in text-to-image
synthesis and the ubiquity of smart-phone cameras and photo-sharing apps. Prior
work on rendering bokeh effects have focused on post hoc image manipulation to
produce similar blurring effects in existing photographs using classical
computer graphics or neural rendering techniques, but have either depth
discontinuity artifacts or are restricted to reproducing bokeh effects that are
present in the training data. More recent diffusion based models can synthesize
images with an artistic style, but either require the generation of
high-dimensional masks, expensive fine-tuning, or affect global image
characteristics. In this paper, we present GBSD, the first generative
text-to-image model that synthesizes photorealistic images with a bokeh style.
Motivated by how image synthesis occurs progressively in diffusion models, our
approach combines latent diffusion models with a 2-stage conditioning algorithm
to render bokeh effects on semantically defined objects. Since we can focus the
effect on objects, this semantic bokeh effect is more versatile than classical
rendering techniques. We evaluate GBSD both quantitatively and qualitatively
and demonstrate its ability to be applied in both text-to-image and
image-to-image settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jieren Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1&quot;&gt;Hao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zhihong Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aguiar_D/0/1/0/all/0/1&quot;&gt;Derek Aguiar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08033">
<title>Domain Adaptation for Deep Unit Test Case Generation. (arXiv:2308.08033v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08033</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, deep learning-based test case generation approaches have been
proposed to automate the generation of unit test cases. In this study, we
leverage Transformer-based code models to generate unit tests with the help of
Domain Adaptation (DA) at a project level. Specifically, we use CodeT5, which
is a relatively small language model trained on source code data, and fine-tune
it on the test generation task; then again further fine-tune it on each target
project data to learn the project-specific knowledge (project-level DA). We use
the Methods2test dataset to fine-tune CodeT5 for the test generation task and
the Defects4j dataset for project-level domain adaptation and evaluation. We
compare our approach with (a) CodeT5 fine-tuned on the test generation without
DA, (b) the A3Test tool, and (c) GPT-4, on 5 projects from the Defects4j
dataset. The results show that using DA can increase the line coverage of the
generated tests on average 18.62%, 19.88%, and 18.02% compared to the above
(a), (b), and (c) baselines, respectively. The results also consistently show
improvements using other metrics such as BLEU and CodeBLEU. In addition, we
show that our approach can be seen as a complementary solution alongside
existing search-based test generation tools such as EvoSuite, to increase the
overall coverage and mutation scores with an average of 34.42% and 6.8%, for
line coverage and mutation score, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jiho Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashtroudi_S/0/1/0/all/0/1&quot;&gt;Sepehr Hashtroudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemmati_H/0/1/0/all/0/1&quot;&gt;Hadi Hemmati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Song Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08565">
<title>How Transferable are Attribute Controllers on Pretrained Multilingual Translation Models?. (arXiv:2309.08565v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08565</link>
<description rdf:parseType="Literal">&lt;p&gt;Customizing machine translation models to comply with fine-grained attributes
such as formality has seen tremendous progress recently. However, current
approaches mostly rely on at least some supervised data with attribute
annotation. Data scarcity therefore remains a bottleneck to democratizing such
customization possibilities to a wider range of languages, lower-resource ones
in particular. Given recent progress in pretrained massively multilingual
translation models, we use them as a foundation to transfer the attribute
controlling capabilities to languages without supervised data. In this work, we
present a comprehensive analysis of transferring attribute controllers based on
a pretrained NLLB-200 model. We investigate both training- and inference-time
control techniques under various data scenarios, and uncover their relative
strengths and weaknesses in zero-shot performance and domain robustness. We
show that both paradigms are complementary, as shown by consistent improvements
on 5 zero-shot directions. Moreover, a human evaluation on a real low-resource
language, Bengali, confirms our findings on zero-shot transfer to new target
languages. The code is
$\href{https://github.com/dannigt/attribute-controller-transfer}{\text{here}}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Danni Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1&quot;&gt;Jan Niehues&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.10444">
<title>Exploring Iterative Enhancement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models. (arXiv:2309.10444v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.10444</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models exhibit superior capabilities in processing and
understanding language, yet their applications in educational contexts remain
underexplored. Learnersourcing enhances learning by engaging students in
creating their own educational content. When learnersourcing multiple-choice
questions, creating explanations for the solution of a question is a crucial
step; it helps other students understand the solution and promotes a deeper
understanding of related concepts. However, it is often difficult for students
to craft effective solution explanations, due to limited subject understanding.
To help scaffold the task of automated explanation generation, we present and
evaluate a framework called &quot;ILearner-LLM&quot;, that iteratively enhances the
generated explanations for the given questions with large language models.
Comprising an explanation generation model and an explanation evaluation model,
the framework generates high-quality student-aligned explanations by
iteratively feeding the quality rating score from the evaluation model back
into the instruction prompt of the explanation generation model. Experimental
results demonstrate the effectiveness of our ILearner-LLM on LLaMA2-13B and
GPT-4 to generate higher quality explanations that are closer to those written
by students on five PeerWise datasets. Our findings represent a promising path
to enrich the learnersourcing experience for students and to enhance the
capabilities of large language models for educational applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Q/0/1/0/all/0/1&quot;&gt;Qiming Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leinonen_J/0/1/0/all/0/1&quot;&gt;Juho Leinonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1&quot;&gt;Alex Yuxuan Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1&quot;&gt;Wanjun Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gendron_G/0/1/0/all/0/1&quot;&gt;Ga&amp;#xeb;l Gendron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pistotti_T/0/1/0/all/0/1&quot;&gt;Timothy Pistotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1&quot;&gt;Alice Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1&quot;&gt;Paul Denny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witbrock_M/0/1/0/all/0/1&quot;&gt;Michael Witbrock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiamou Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14393">
<title>LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models. (arXiv:2309.14393v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14393</link>
<description rdf:parseType="Literal">&lt;p&gt;The carbon footprint associated with large language models (LLMs) is a
significant concern, encompassing emissions from their training, inference,
experimentation, and storage processes, including operational and embodied
carbon emissions. An essential aspect is accurately estimating the carbon
impact of emerging LLMs even before their training, which heavily relies on GPU
usage. Existing studies have reported the carbon footprint of LLM training, but
only one tool, mlco2, can predict the carbon footprint of new neural networks
prior to physical training. However, mlco2 has several serious limitations. It
cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs,
disregards critical architectural parameters, focuses solely on GPUs, and
cannot model embodied carbon footprints. Addressing these gaps, we introduce
\textit{\carb}, an end-to-end carbon footprint projection model designed for
both dense and MoE LLMs. Compared to mlco2, \carb~significantly enhances the
accuracy of carbon footprint estimations for various LLMs. The source code is
released at \url{https://github.com/SotaroKaneda/MLCarbon}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faiz_A/0/1/0/all/0/1&quot;&gt;Ahmad Faiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaneda_S/0/1/0/all/0/1&quot;&gt;Sotaro Kaneda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruhan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osi_R/0/1/0/all/0/1&quot;&gt;Rita Osi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1&quot;&gt;Prateek Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Fan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Lei Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04965">
<title>MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks. (arXiv:2310.04965v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04965</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatically generating scripts (i.e. sequences of key steps described in
text) from video demonstrations and reasoning about the subsequent steps are
crucial to the modern AI virtual assistants to guide humans to complete
everyday tasks, especially unfamiliar ones. However, current methods for
generative script learning rely heavily on well-structured preceding steps
described in text and/or images or are limited to a certain domain, resulting
in a disparity with real-world user scenarios. To address these limitations, we
present a new benchmark challenge -- MultiScript, with two new tasks on
task-oriented multimodal script learning: (1) multimodal script generation, and
(2) subsequent step prediction. For both tasks, the input consists of a target
task name and a video illustrating what has been done to complete the target
task, and the expected output is (1) a sequence of structured step descriptions
in text based on the demonstration video, and (2) a single text description for
the subsequent step, respectively. Built from WikiHow, MultiScript covers
multimodal scripts in videos and text descriptions for over 6,655 human
everyday tasks across 19 diverse domains. To establish baseline performance on
MultiScript, we propose two knowledge-guided multimodal generative frameworks
that incorporate the task-related knowledge prompted from large language models
such as Vicuna. Experimental results show that our proposed approaches
significantly improve over the competitive baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1&quot;&gt;Jingyuan Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Minqian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Ying Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lifu Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05492">
<title>How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. (arXiv:2310.05492v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05492</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) with enormous pre-training tokens and parameters
emerge diverse abilities, including math reasoning, code generation, and
instruction following. These abilities are further enhanced by supervised
fine-tuning (SFT). While the open-source community has explored ad-hoc SFT for
enhancing individual capabilities, proprietary LLMs exhibit versatility across
various skills. Therefore, understanding the facilitation of multiple abilities
via SFT is paramount. In this study, we specifically focuses on the interplay
of data composition between mathematical reasoning, code generation, and
general human-aligning abilities during SFT. We propose four intriguing
research questions to explore the association between model performance and
various factors including data amount, composition ratio, model size and SFT
strategies. Our experiments reveal that distinct capabilities scale differently
and larger models generally show superior performance with same amount of data.
Mathematical reasoning and code generation consistently improve with increasing
data amount, whereas general abilities plateau after roughly a thousand
samples. Moreover, we observe data composition appears to enhance various
abilities under limited data conditions, yet can lead to performance conflicts
when data is plentiful. Our findings also suggest the amount of composition
data influences performance more than the composition ratio. In analysis of SFT
strategies, we find that sequentially learning multiple skills risks
catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT)
strategy offers a promising solution to learn multiple abilities with different
scaling patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1&quot;&gt;Guanting Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Hongyi Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1&quot;&gt;Keming Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chengpeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1&quot;&gt;Mingfeng Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dayiheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zheng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12955">
<title>Towards Robust Offline Reinforcement Learning under Diverse Data Corruption. (arXiv:2310.12955v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12955</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline reinforcement learning (RL) presents a promising approach for
learning reinforced policies from offline datasets without the need for costly
or unsafe interactions with the environment. However, datasets collected by
humans in real-world environments are often noisy and may even be maliciously
corrupted, which can significantly degrade the performance of offline RL. In
this work, we first investigate the performance of current offline RL
algorithms under comprehensive data corruption, including states, actions,
rewards, and dynamics. Our extensive experiments reveal that implicit
Q-learning (IQL) demonstrates remarkable resilience to data corruption among
various offline RL algorithms. Furthermore, we conduct both empirical and
theoretical analyses to understand IQL&apos;s robust performance, identifying its
supervised policy learning scheme as the key factor. Despite its relative
robustness, IQL still suffers from heavy-tail targets of Q functions under
dynamics corruption. To tackle this challenge, we draw inspiration from robust
statistics to employ the Huber loss to handle the heavy-tailedness and utilize
quantile estimators to balance penalization for corrupted data and learning
stability. By incorporating these simple yet effective modifications into IQL,
we propose a more robust offline RL approach named Robust IQL (RIQL). Extensive
experiments demonstrate that RIQL exhibits highly robust performance when
subjected to diverse data corruption scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Rui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1&quot;&gt;Han Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiawei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Amy Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chongjie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Lei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00772">
<title>SAGE: Smart home Agent with Grounded Execution. (arXiv:2311.00772v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00772</link>
<description rdf:parseType="Literal">&lt;p&gt;The common sense reasoning abilities and vast general knowledge of Large
Language Models (LLMs) make them a natural fit for interpreting user requests
in a Smart Home assistant context. LLMs, however, lack specific knowledge about
the user and their home limit their potential impact. SAGE (Smart Home Agent
with Grounded Execution), overcomes these and other limitations by using a
scheme in which a user request triggers an LLM-controlled sequence of discrete
actions. These actions can be used to retrieve information, interact with the
user, or manipulate device states. SAGE controls this process through a
dynamically constructed tree of LLM prompts, which help it decide which action
to take next, whether an action was successful, and when to terminate the
process. The SAGE action set augments an LLM&apos;s capabilities to support some of
the most critical requirements for a Smart Home assistant. These include:
flexible and scalable user preference management (&quot;is my team playing
tonight?&quot;), access to any smart device&apos;s full functionality without
device-specific code via API reading &quot;turn down the screen brightness on my
dryer&quot;, persistent device state monitoring (&quot;remind me to throw out the milk
when I open the fridge&quot;), natural device references using only a photo of the
room (&quot;turn on the light on the dresser&quot;), and more. We introduce a benchmark
of 50 new and challenging smart home tasks where SAGE achieves a 75% success
rate, significantly outperforming existing LLM-enabled baselines (30% success
rate).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivkin_D/0/1/0/all/0/1&quot;&gt;Dmitriy Rivkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hogan_F/0/1/0/all/0/1&quot;&gt;Francois Hogan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feriani_A/0/1/0/all/0/1&quot;&gt;Amal Feriani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konar_A/0/1/0/all/0/1&quot;&gt;Abhisek Konar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sigal_A/0/1/0/all/0/1&quot;&gt;Adam Sigal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Steve Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1&quot;&gt;Greg Dudek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03976">
<title>A Foundation Graph Model. (arXiv:2311.03976v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03976</link>
<description rdf:parseType="Literal">&lt;p&gt;The principal benefit of unsupervised graph representation learning is that a
pre-trained model can be fine-tuned where data or labels are scarce. Existing
approaches are domain specific, maintaining consistent node and edge attributes
across the pre-training and target datasets. This precludes transfer to other
domains. A model capable of positive transfer on arbitrary tasks and domains
would represent the first foundation graph model.
&lt;/p&gt;
&lt;p&gt;In this work we use adversarial contrastive learning to present FoToM, a
graph pre-training method based on node and edge feature exclusion. We use
FoToM to pre-train models over multiple graph domains, producing the first
foundation graph models. We demonstrate positive transfer on evaluation
datasets from multiple domains, including domains not present in pre-training
data. On all datasets performance is at worst on-par and on 76% significantly
better than a supervised baseline ($P \leq 0.01$), with an 8 to 40% reduction
in error at 95% confidence. Contrary to other research, pre-training on a
dataset with the target domain excluded leads us to better performance than
pre-training on a dataset from only the target domain. The multi-domain model
at worst, matches, and on 56% of tasks, significantly outperforms single-domain
($P \leq 0.01$). These results include when node labels are used in evaluation,
where performance is consistently superior to single-domain or non-pre-trained
models. Notably, FoToM benefits scenarios in both large or scarce data regimes
for the target domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davies_A/0/1/0/all/0/1&quot;&gt;Alex O. Davies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Green_R/0/1/0/all/0/1&quot;&gt;Riku W. Green&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ajmeri_N/0/1/0/all/0/1&quot;&gt;Nirav S. Ajmeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filho_T/0/1/0/all/0/1&quot;&gt;Telmo M. Silva Filho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09868">
<title>INTERVENOR: Prompt the Coding Ability of Large Language Models with the Interactive Chain of Repairing. (arXiv:2311.09868v3 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09868</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes INTERactiVE chaiN Of Repairing (INTERVENOR), which mimics
human code repairing behavior (iteratively judging, rethinking, and repairing)
and prompts the coding ability of regard Large Language Models (LLMs).
Specifically, INTERVENOR employs two LLM based agents, Code Learner and Code
Teacher, to play different roles in code repairing and work interactively to
repair the generated codes. The Code Learner is asked to generate and repair
code according to the instructions from the Code Teacher. The Code Teacher
rethinks the code errors according to the corresponding feedback from compilers
and iteratively generates the chain-of-repairing (CoR) to guide the code
repairing process for Code Learner. Our experiments show that INTERVENOR
outperforms the state-of-the-art methods and achieves about 13% and 4.5%
improvements over the GPT-3.5 model in code generation and code translation
tasks, respectively. Our further analyses show that CoR can illuminate the bug
reasons and solution plans via natural language. With the feedback of code
compilers, INTERVENOR can accurately identify the syntax errors and assertion
errors in the code and provide precise instructions to repair codes. All data
and codes are available at https://github.com/NEUIR/INTERVENOR
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanbin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhenghao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1&quot;&gt;Ganqu Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1&quot;&gt;Ning Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Ge Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15497">
<title>Adaptive Image Registration: A Hybrid Approach Integrating Deep Learning and Optimization Functions for Enhanced Precision. (arXiv:2311.15497v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15497</link>
<description rdf:parseType="Literal">&lt;p&gt;Image registration has traditionally been done using two distinct approaches:
learning based methods, relying on robust deep neural networks, and
optimization-based methods, applying complex mathematical transformations to
warp images accordingly. Of course, both paradigms offer advantages and
disadvantages, and, in this work, we seek to combine their respective strengths
into a single streamlined framework, using the outputs of the learning based
method as initial parameters for optimization while prioritizing computational
power for the image pairs that offer the greatest loss. Our investigations
showed improvements of up to 1.6% in test data, while maintaining the same
inference time, and a substantial 1.0% points performance gain in deformation
field smoothness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araujo_G/0/1/0/all/0/1&quot;&gt;Gabriel De Araujo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shanlin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohui Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01185">
<title>A ripple in time: a discontinuity in American history. (arXiv:2312.01185v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01185</link>
<description rdf:parseType="Literal">&lt;p&gt;In this note we use the State of the Union Address (SOTU) dataset from Kaggle
to make some surprising (and some not so surprising) observations pertaining to
the general timeline of American history, and the character and nature of the
addresses themselves. Our main approach is using vector embeddings, such as
BERT (DistilBERT) and GPT-2.
&lt;/p&gt;
&lt;p&gt;While it is widely believed that BERT (and its variations) is most suitable
for NLP classification tasks, we find out that GPT-2 in conjunction with
nonlinear dimension reduction methods such as UMAP provide better separation
and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In
our case, no model fine-tuning is required, and the pre-trained out-of-the-box
GPT-2 model is enough.
&lt;/p&gt;
&lt;p&gt;We also used a fine-tuned DistilBERT model for classification detecting which
President delivered which address, with very good results (accuracy 93\% - 95\%
depending on the run). An analogous task was performed to determine the year of
writing, and we were able to pin it down to about 4 years (which is a single
presidential term).
&lt;/p&gt;
&lt;p&gt;It is worth noting that SOTU addresses provide relatively small writing
samples (with about 8000 words on average, and varying widely from under 2000
words to more than 20000), and that the amount of authors is relatively large
(we used SOTU addresses of 42 US presidents). This shows that the techniques
employed turn out to be rather efficient, while all the computations described
in this note can be performed using a single GPU instance of Google Colab.
&lt;/p&gt;
&lt;p&gt;The accompanying code is available on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolpakov_A/0/1/0/all/0/1&quot;&gt;Alexander Kolpakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivin_I/0/1/0/all/0/1&quot;&gt;Igor Rivin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03263">
<title>Weathering Ongoing Uncertainty: Learning and Planning in a Time-Varying Partially Observable Environment. (arXiv:2312.03263v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03263</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimal decision-making presents a significant challenge for autonomous
systems operating in uncertain, stochastic and time-varying environments.
Environmental variability over time can significantly impact the system&apos;s
optimal decision making strategy for mission completion. To model such
environments, our work combines the previous notion of Time-Varying Markov
Decision Processes (TVMDP) with partial observability and introduces
Time-Varying Partially Observable Markov Decision Processes (TV-POMDP). We
propose a two-pronged approach to accurately estimate and plan within the
TV-POMDP: 1) Memory Prioritized State Estimation (MPSE), which leverages
weighted memory to provide more accurate time-varying transition estimates; and
2) an MPSE-integrated planning strategy that optimizes long-term rewards while
accounting for temporal constraint. We validate the proposed framework and
algorithms using simulations and hardware, with robots exploring a partially
observable, time-varying environments. Our results demonstrate superior
performance over standard methods, highlighting the framework&apos;s effectiveness
in stochastic, uncertain, time-varying domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puthumanaillam_G/0/1/0/all/0/1&quot;&gt;Gokul Puthumanaillam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiangyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehr_N/0/1/0/all/0/1&quot;&gt;Negar Mehr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ornik_M/0/1/0/all/0/1&quot;&gt;Melkior Ornik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10401">
<title>Rethinking Dimensional Rationale in Graph Contrastive Learning from Causal Perspective. (arXiv:2312.10401v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10401</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph contrastive learning is a general learning paradigm excelling at
capturing invariant information from diverse perturbations in graphs. Recent
works focus on exploring the structural rationale from graphs, thereby
increasing the discriminability of the invariant information. However, such
methods may incur in the mis-learning of graph models towards the
interpretability of graphs, and thus the learned noisy and task-agnostic
information interferes with the prediction of graphs. To this end, with the
purpose of exploring the intrinsic rationale of graphs, we accordingly propose
to capture the dimensional rationale from graphs, which has not received
sufficient attention in the literature. The conducted exploratory experiments
attest to the feasibility of the aforementioned roadmap. To elucidate the
innate mechanism behind the performance improvement arising from the
dimensional rationale, we rethink the dimensional rationale in graph
contrastive learning from a causal perspective and further formalize the
causality among the variables in the pre-training stage to build the
corresponding structural causal model. On the basis of the understanding of the
structural causal model, we propose the dimensional rationale-aware graph
contrastive learning approach, which introduces a learnable dimensional
rationale acquiring network and a redundancy reduction constraint. The
learnable dimensional rationale acquiring network is updated by leveraging a
bi-level meta-learning technique, and the redundancy reduction constraint
disentangles the redundant features through a decorrelation process during
learning. Empirically, compared with state-of-the-art methods, our method can
yield significant performance boosts on various benchmarks with respect to
discriminability and transferability. The code implementation of our method is
available at https://github.com/ByronJi/DRGCL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Q/0/1/0/all/0/1&quot;&gt;Qirui Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiangmeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jie Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Changwen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1&quot;&gt;Fanjiang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16451">
<title>Domain Generalization with Vital Phase Augmentation. (arXiv:2312.16451v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16451</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have shown remarkable performance in image
classification. However, their performance significantly deteriorates with
corrupted input data. Domain generalization methods have been proposed to train
robust models against out-of-distribution data. Data augmentation in the
frequency domain is one of such approaches that enable a model to learn phase
features to establish domain-invariant representations. This approach changes
the amplitudes of the input data while preserving the phases. However, using
fixed phases leads to susceptibility to phase fluctuations because amplitudes
and phase fluctuations commonly occur in out-of-distribution. In this study, to
address this problem, we introduce an approach using finite variation of the
phases of input data rather than maintaining fixed phases. Based on the
assumption that the degree of domain-invariant features varies for each phase,
we propose a method to distinguish phases based on this degree. In addition, we
propose a method called vital phase augmentation (VIPAug) that applies the
variation to the phases differently according to the degree of domain-invariant
features of given phases. The model depends more on the vital phases that
contain more domain-invariant features for attaining robustness to amplitude
and phase fluctuations. We present experimental evaluations of our proposed
approach, which exhibited improved performance for both clean and corrupted
data. VIPAug achieved SOTA performance on the benchmark CIFAR-10 and CIFAR-100
datasets, as well as near-SOTA performance on the ImageNet-100 and ImageNet
datasets. Our code is available at https://github.com/excitedkid/vipaug.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;Ingyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1&quot;&gt;Wooju Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1&quot;&gt;Hyun Myung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00110">
<title>Diffusion Model with Perceptual Loss. (arXiv:2401.00110v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00110</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models trained with mean squared error loss tend to generate
unrealistic samples. Current state-of-the-art models rely on classifier-free
guidance to improve sample quality, yet its surprising effectiveness is not
fully understood. In this paper, we show that the effectiveness of
classifier-free guidance partly originates from it being a form of implicit
perceptual guidance. As a result, we can directly incorporate perceptual loss
in diffusion training to improve sample quality. Since the score matching
objective used in diffusion training strongly resembles the denoising
autoencoder objective used in unsupervised training of perceptual networks, the
diffusion model itself is a perceptual network and can be used to generate
meaningful perceptual loss. We propose a novel self-perceptual objective that
results in diffusion models capable of generating more realistic samples. For
conditional generation, our method only improves sample quality without
entanglement with the conditional input and therefore does not sacrifice sample
diversity. Our method can also improve sample quality for unconditional
generation, which was not possible with classifier-free guidance before.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Shanchuan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02542">
<title>A Community Detection and Graph Neural Network Based Link Prediction Approach for Scientific Literature. (arXiv:2401.02542v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02542</link>
<description rdf:parseType="Literal">&lt;p&gt;This study presents a novel approach that synergizes community detection
algorithms with various Graph Neural Network (GNN) models to bolster link
prediction in scientific literature networks. By integrating the Louvain
community detection algorithm into our GNN frameworks, we consistently enhance
performance across all models tested. For example, integrating Louvain with the
GAT model resulted in an AUC score increase from 0.777 to 0.823, exemplifying
the typical improvements observed. Similar gains are noted when Louvain is
paired with other GNN architectures, confirming the robustness and
effectiveness of incorporating community-level insights. This consistent uplift
in performance reflected in our extensive experimentation on bipartite graphs
of scientific collaborations and citations highlights the synergistic potential
of combining community detection with GNNs to overcome common link prediction
challenges such as scalability and resolution limits. Our findings advocate for
the integration of community structures as a significant step forward in the
predictive accuracy of network science models, offering a comprehensive
understanding of scientific collaboration patterns through the lens of advanced
machine learning techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chunjiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yikun Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haiyun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shihan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaidi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yongye Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04336">
<title>Deep Efficient Private Neighbor Generation for Subgraph Federated Learning. (arXiv:2401.04336v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04336</link>
<description rdf:parseType="Literal">&lt;p&gt;Behemoth graphs are often fragmented and separately stored by multiple data
owners as distributed subgraphs in many realistic applications. Without harming
data privacy, it is natural to consider the subgraph federated learning
(subgraph FL) scenario, where each local client holds a subgraph of the entire
global graph, to obtain globally generalized graph mining models. To overcome
the unique challenge of incomplete information propagation on local subgraphs
due to missing cross-subgraph neighbors, previous works resort to the
augmentation of local neighborhoods through the joint FL of missing neighbor
generators and GNNs. Yet their technical designs have profound limitations
regarding the utility, efficiency, and privacy goals of FL. In this work, we
propose FedDEP to comprehensively tackle these challenges in subgraph FL.
FedDEP consists of a series of novel technical designs: (1) Deep neighbor
generation through leveraging the GNN embeddings of potential missing
neighbors; (2) Efficient pseudo-FL for neighbor generation through embedding
prototyping; and (3) Privacy protection through noise-less
edge-local-differential-privacy. We analyze the correctness and efficiency of
FedDEP, and provide theoretical guarantees on its privacy. Empirical results on
four real-world datasets justify the clear benefits of proposed techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Ke Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lichao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1&quot;&gt;Bolin Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yiu_S/0/1/0/all/0/1&quot;&gt;Siu Ming Yiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Carl Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05273">
<title>INACIA: Integrating Large Language Models in Brazilian Audit Courts: Opportunities and Challenges. (arXiv:2401.05273v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05273</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces INACIA (Instru\c{c}\~ao Assistida com Intelig\^encia
Artificial), a groundbreaking system designed to integrate Large Language
Models (LLMs) into the operational framework of Brazilian Federal Court of
Accounts (TCU). The system automates various stages of case analysis, including
basic information extraction, admissibility examination, Periculum in mora and
Fumus boni iuris analyses, and recommendations generation. Through a series of
experiments, we demonstrate INACIA&apos;s potential in extracting relevant
information from case documents, evaluating its legal plausibility, and
formulating propositions for judicial decision-making. Utilizing a validation
dataset alongside LLMs, our evaluation methodology presents an innovative
approach to assessing system performance, correlating highly with human
judgment. The results highlight INACIA&apos;s proficiency in handling complex legal
tasks, indicating its suitability for augmenting efficiency and judicial
fairness within legal systems. The paper also discusses potential enhancements
and future applications, positioning INACIA as a model for worldwide AI
integration in legal domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pereira_J/0/1/0/all/0/1&quot;&gt;Jayr Pereira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assumpcao_A/0/1/0/all/0/1&quot;&gt;Andre Assumpcao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trecenti_J/0/1/0/all/0/1&quot;&gt;Julio Trecenti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Airosa_L/0/1/0/all/0/1&quot;&gt;Luiz Airosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lente_C/0/1/0/all/0/1&quot;&gt;Caio Lente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cleto_J/0/1/0/all/0/1&quot;&gt;Jhonatan Cl&amp;#xe9;to&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dobins_G/0/1/0/all/0/1&quot;&gt;Guilherme Dobins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1&quot;&gt;Rodrigo Nogueira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_L/0/1/0/all/0/1&quot;&gt;Luis Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1&quot;&gt;Roberto Lotufo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08326">
<title>RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning. (arXiv:2401.08326v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08326</link>
<description rdf:parseType="Literal">&lt;p&gt;Tool learning has generated widespread interest as a vital means of
interaction between Large Language Models (LLMs) and the physical world.
Current research predominantly emphasizes LLMs&apos; capacity to utilize tools in
well-structured environments while overlooking their stability when confronted
with the inevitable noise of the real world. To bridge this gap, we introduce
RoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool
learning. Specifically, we establish five external environments, each featuring
varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union),
providing an in-depth analysis of the model&apos;s resilience across three critical
phases: tool selection, parameter identification, and content filling.
Experiments involving six widely-used models underscore the urgent necessity
for enhancing the robustness of LLMs in tool learning. For instance, the
performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is
no substantial change in manual accuracy. More surprisingly, the noise
correction capability inherent in the GPT family paradoxically impedes its
adaptability in the face of mild noise. In light of these findings, we propose
RoTTuning, a strategy that enriches the diversity of training environments to
bolster the robustness of LLMs in tool learning. The code and data are
available at https://github.com/Junjie-Ye/RoTBench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Junjie Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yilong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Songyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Caishuang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sixian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xiaoran Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1&quot;&gt;Tao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08897">
<title>CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational AutoEncoder. (arXiv:2401.08897v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08897</link>
<description rdf:parseType="Literal">&lt;p&gt;Symmetries of input and latent vectors have provided valuable insights for
disentanglement learning in VAEs.However, only a few works were proposed as an
unsupervised method, and even these works require known factor information in
training data. We propose a novel method, Composite Factor-Aligned Symmetry
Learning (CFASL), which is integrated into VAEs for learning symmetry-based
disentanglement in unsupervised learning without any knowledge of the dataset
factor information.CFASL incorporates three novel features for learning
symmetry-based disentanglement: 1) Injecting inductive bias to align latent
vector dimensions to factor-aligned symmetries within an explicit learnable
symmetry codebook 2) Learning a composite symmetry to express unknown factors
change between two random samples by learning factor-aligned symmetries within
the codebook 3) Inducing group equivariant encoder and decoder in training VAEs
with the two conditions. In addition, we propose an extended evaluation metric
for multi-factor changes in comparison to disentanglement evaluation in VAEs.
In quantitative and in-depth qualitative analysis, CFASL demonstrates a
significant improvement of disentanglement in single-factor change, and
multi-factor change conditions compared to state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1&quot;&gt;Hee-Jun Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1&quot;&gt;Jaehyoung Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kangil Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09566">
<title>Aligning Large Language Models with Counterfactual DPO. (arXiv:2401.09566v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09566</link>
<description rdf:parseType="Literal">&lt;p&gt;Advancements in large language models (LLMs) have demonstrated remarkable
capabilities across a diverse range of applications. These models excel in
generating text completions that are contextually coherent and cover an
extensive array of subjects. However, the vast datasets required for their
training make aligning response styles during the pretraining and instruction
tuning phases challenging. Consequently, an additional alignment phase is
typically employed, wherein the model is further trained with human preference
data to better align its outputs with human expectations. While this process
doesn&apos;t introduce new capabilities per se, it does accentuate generation styles
innate to the model. This paper explores the utilization of counterfactual
prompting within the framework of Direct Preference Optimization (DPO) to align
the model&apos;s style without relying on human intervention. We demonstrate that
this method effectively instils desirable behaviour, mitigates undesirable
ones, and encourages the model to disregard inappropriate instructions. Our
findings suggest that counterfactual prompting with DPO presents a low-resource
way to fine-tune LLMs to meet the demands for responsible and ethically aligned
AI systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Butcher_B/0/1/0/all/0/1&quot;&gt;Bradley Butcher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09691">
<title>Imitation Learning Inputting Image Feature to Each Layer of Neural Network. (arXiv:2401.09691v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09691</link>
<description rdf:parseType="Literal">&lt;p&gt;Imitation learning enables robots to learn and replicate human behavior from
training data. Recent advances in machine learning enable end-to-end learning
approaches that directly process high-dimensional observation data, such as
images. However, these approaches face a critical challenge when processing
data from multiple modalities, inadvertently ignoring data with a lower
correlation to the desired output, especially when using short sampling
periods. This paper presents a useful method to address this challenge, which
amplifies the influence of data with a relatively low correlation to the output
by inputting the data into each neural network layer. The proposed approach
effectively incorporates diverse data sources into the learning process.
Through experiments using a simple pick-and-place operation with raw images and
joint information as input, significant improvements in success rates are
demonstrated even when dealing with data from short sampling periods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamane_K/0/1/0/all/0/1&quot;&gt;Koki Yamane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakaino_S/0/1/0/all/0/1&quot;&gt;Sho Sakaino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsuji_T/0/1/0/all/0/1&quot;&gt;Toshiaki Tsuji&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>