<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-05T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01201" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01202" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01204" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01210" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01214" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01226" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01229" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01238" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01292" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01301" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01366" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01371" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01394" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01449" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01502" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01504" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01519" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01530" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01532" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01540" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01557" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01558" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01570" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01593" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01616" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01637" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01639" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01644" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01676" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01683" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01684" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01784" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01817" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2009.07888" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2011.14956" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2105.14452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.09973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2108.06161" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.08645" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.09826" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.12883" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.12888" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.14092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.12900" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.10652" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.11922" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.03341" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.12776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.02307" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.06356" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.10656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.15240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.10209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.10329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.14321" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01407" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.08416" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.02090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.05961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.05118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.07832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.07937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.09532" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.13259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.04209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06455" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10557" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02301" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02531" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09840" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09459" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00426" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.09325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00165" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.01201">
<title>Schema-learning and rebinding as mechanisms of in-context learning and emergence. (arXiv:2307.01201v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01201</link>
<description rdf:parseType="Literal">&lt;p&gt;In-context learning (ICL) is one of the most powerful and most unexpected
capabilities to emerge in recent transformer-based large language models
(LLMs). Yet the mechanisms that underlie it are poorly understood. In this
paper, we demonstrate that comparable ICL capabilities can be acquired by an
alternative sequence prediction learning method using clone-structured causal
graphs (CSCGs). Moreover, a key property of CSCGs is that, unlike
transformer-based LLMs, they are {\em interpretable}, which considerably
simplifies the task of explaining how ICL works. Specifically, we show that it
uses a combination of (a) learning template (schema) circuits for pattern
completion, (b) retrieving relevant templates in a context-sensitive manner,
and (c) rebinding of novel tokens to appropriate slots in the templates. We go
on to marshall evidence for the hypothesis that similar mechanisms underlie ICL
in LLMs. For example, we find that, with CSCGs as with LLMs, different
capabilities emerge at different levels of overparameterization, suggesting
that overparameterization helps in learning more complex template (schema)
circuits. By showing how ICL can be achieved with small models and datasets, we
open up a path to novel architectures, and take a vital step towards a more
general understanding of the mechanics behind this important capability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swaminathan_S/0/1/0/all/0/1&quot;&gt;Sivaramakrishnan Swaminathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dedieu_A/0/1/0/all/0/1&quot;&gt;Antoine Dedieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raju_R/0/1/0/all/0/1&quot;&gt;Rajkumar Vasudeva Raju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanahan_M/0/1/0/all/0/1&quot;&gt;Murray Shanahan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazaro_Gredilla_M/0/1/0/all/0/1&quot;&gt;Miguel Lazaro-Gredilla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+George_D/0/1/0/all/0/1&quot;&gt;Dileep George&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01202">
<title>Predictive Patentomics: Forecasting Innovation Success and Valuation with ChatGPT. (arXiv:2307.01202v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01202</link>
<description rdf:parseType="Literal">&lt;p&gt;Analysis of innovation has been fundamentally limited by conventional
approaches to broad, structural variables. This paper pushes the boundaries,
taking an LLM approach to patent analysis with the groundbreaking ChatGPT
technology. OpenAI&apos;s state-of-the-art textual embedding accesses complex
information about the quality and impact of each invention to power deep
learning predictive models. The nuanced embedding drives a 24% incremental
improvement in R-squared predicting patent value and clearly isolates the worst
and best applications. These models enable a revision of the contemporary
Kogan, Papanikolaou, Seru, and Stoffman (2017) valuation of patents by a median
deviation of 1.5 times, accounting for potential institutional predictions.
Furthermore, the market fails to incorporate timely information about
applications; a long-short portfolio based on predicted acceptance rates
achieves significant abnormal returns of 3.3% annually. The models provide an
opportunity to revolutionize startup and small-firm corporate policy vis-a-vis
patenting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Stephen Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01204">
<title>Towards Few-shot Inductive Link Prediction on Knowledge Graphs: A Relational Anonymous Walk-guided Neural Process Approach. (arXiv:2307.01204v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.01204</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot inductive link prediction on knowledge graphs (KGs) aims to predict
missing links for unseen entities with few-shot links observed. Previous
methods are limited to transductive scenarios, where entities exist in the
knowledge graphs, so they are unable to handle unseen entities. Therefore,
recent inductive methods utilize the sub-graphs around unseen entities to
obtain the semantics and predict links inductively. However, in the few-shot
setting, the sub-graphs are often sparse and cannot provide meaningful
inductive patterns. In this paper, we propose a novel relational anonymous
walk-guided neural process for few-shot inductive link prediction on knowledge
graphs, denoted as RawNP. Specifically, we develop a neural process-based
method to model a flexible distribution over link prediction functions. This
enables the model to quickly adapt to new entities and estimate the uncertainty
when making predictions. To capture general inductive patterns, we present a
relational anonymous walk to extract a series of relational motifs from
few-shot observations. These motifs reveal the distinctive semantic patterns on
KGs that support inductive predictions. Extensive experiments on typical
benchmark datasets demonstrate that our model derives new state-of-the-art
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zicheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Linhao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shirui Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1&quot;&gt;Quoc Viet Hung Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1&quot;&gt;Chen Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01210">
<title>AI and Non AI Assessments for Dementia. (arXiv:2307.01210v1 [q-bio.OT])</title>
<link>http://arxiv.org/abs/2307.01210</link>
<description rdf:parseType="Literal">&lt;p&gt;Current progress in the artificial intelligence domain has led to the
development of various types of AI-powered dementia assessments, which can be
employed to identify patients at the early stage of dementia. It can
revolutionize the dementia care settings. It is essential that the medical
community be aware of various AI assessments and choose them considering their
degrees of validity, efficiency, practicality, reliability, and accuracy
concerning the early identification of patients with dementia (PwD). On the
other hand, AI developers should be informed about various non-AI assessments
as well as recently developed AI assessments. Thus, this paper, which can be
readable by both clinicians and AI engineers, fills the gap in the literature
in explaining the existing solutions for the recognition of dementia to
clinicians, as well as the techniques used and the most widespread dementia
datasets to AI engineers. It follows a review of papers on AI and non-AI
assessments for dementia to provide valuable information about various dementia
assessments for both the AI and medical communities. The discussion and
conclusion highlight the most prominent research directions and the maturity of
existing solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mahboobeh/0/1/0/all/0/1&quot;&gt;Mahboobeh&lt;/a&gt; (Mah) &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Parsapoor/0/1/0/all/0/1&quot;&gt;Parsapoor&lt;/a&gt; (Parsa), &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ghodrati_H/0/1/0/all/0/1&quot;&gt;Hamed Ghodrati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dentamaro_V/0/1/0/all/0/1&quot;&gt;Vincenzo Dentamaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Madan_C/0/1/0/all/0/1&quot;&gt;Christopher R. Madan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lazarou_I/0/1/0/all/0/1&quot;&gt;Ioulietta Lazarou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Nikolopoulos_S/0/1/0/all/0/1&quot;&gt;Spiros Nikolopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kompatsiaris_I/0/1/0/all/0/1&quot;&gt;Ioannis Kompatsiaris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01211">
<title>An automated method for the ontological representation of security directives. (arXiv:2307.01211v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.01211</link>
<description rdf:parseType="Literal">&lt;p&gt;Large documents written in juridical language are difficult to interpret,
with long sentences leading to intricate and intertwined relations between the
nouns. The present paper frames this problem in the context of recent European
security directives. The complexity of their language is here thwarted by
automating the extraction of the relevant information, namely of the parts of
speech from each clause, through a specific tailoring of Natural Language
Processing (NLP) techniques. These contribute, in combination with ontology
development principles, to the design of our automated method for the
representation of security directives as ontologies. The method is showcased on
a practical problem, namely to derive an ontology representing the NIS 2
directive, which is the peak of cybersecurity prescripts at the European level.
Although the NLP techniques adopted showed some limitations and had to be
complemented by manual analysis, the overall results provide valid support for
directive compliance in general and for ontology development in particular.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bella_G/0/1/0/all/0/1&quot;&gt;Giampaolo Bella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castiglione_G/0/1/0/all/0/1&quot;&gt;Gianpietro Castiglione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santamaria_D/0/1/0/all/0/1&quot;&gt;Daniele Francesco Santamaria&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01214">
<title>Automatic Counterfactual Augmentation for Robust Text Classification Based on Word-Group Search. (arXiv:2307.01214v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01214</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite large-scale pre-trained language models have achieved striking
results for text classificaion, recent work has raised concerns about the
challenge of shortcut learning. In general, a keyword is regarded as a shortcut
if it creates a superficial association with the label, resulting in a false
prediction. Conversely, shortcut learning can be mitigated if the model relies
on robust causal features that help produce sound predictions. To this end,
many studies have explored post-hoc interpretable methods to mine shortcuts and
causal features for robustness and generalization. However, most existing
methods focus only on single word in a sentence and lack consideration of
word-group, leading to wrong causal features. To solve this problem, we propose
a new Word-Group mining approach, which captures the causal effect of any
keyword combination and orders the combinations that most affect the
prediction. Our approach bases on effective post-hoc analysis and beam search,
which ensures the mining effect and reduces the complexity. Then, we build a
counterfactual augmentation method based on the multiple word-groups, and use
an adaptive voting mechanism to learn the influence of different augmentated
samples on the prediction results, so as to force the model to pay attention to
effective causal features. We demonstrate the effectiveness of the proposed
method by several tasks on 8 affective review datasets and 4 toxic language
datasets, including cross-domain text classificaion, text attack and gender
fairness test.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1&quot;&gt;Rui Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1&quot;&gt;Fausto Giunchiglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingji Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hao Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01217">
<title>FedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy. (arXiv:2307.01217v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01217</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, personalized federated learning (pFL) has attracted increasing
attention in privacy protection, collaborative learning, and tackling
statistical heterogeneity among clients, e.g., hospitals, mobile smartphones,
etc. Most existing pFL methods focus on exploiting the global information and
personalized information in the client-level model parameters while neglecting
that data is the source of these two kinds of information. To address this, we
propose the Federated Conditional Policy (FedCP) method, which generates a
conditional policy for each sample to separate the global information and
personalized information in its features and then processes them by a global
head and a personalized head, respectively. FedCP is more fine-grained to
consider personalization in a sample-specific manner than existing pFL methods.
Extensive experiments in computer vision and natural language processing
domains show that FedCP outperforms eleven state-of-the-art methods by up to
6.69%. Furthermore, FedCP maintains its superiority when some clients
accidentally drop out, which frequently happens in mobile settings. Our code is
public at https://github.com/TsingZ0/FedCP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1&quot;&gt;Yang Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1&quot;&gt;Tao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1&quot;&gt;Zhengui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1&quot;&gt;Ruhui Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1&quot;&gt;Haibing Guan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01221">
<title>Filter Bubbles in Recommender Systems: Fact or Fallacy -- A Systematic Review. (arXiv:2307.01221v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.01221</link>
<description rdf:parseType="Literal">&lt;p&gt;A filter bubble refers to the phenomenon where Internet customization
effectively isolates individuals from diverse opinions or materials, resulting
in their exposure to only a select set of content. This can lead to the
reinforcement of existing attitudes, beliefs, or conditions. In this study, our
primary focus is to investigate the impact of filter bubbles in recommender
systems. This pioneering research aims to uncover the reasons behind this
problem, explore potential solutions, and propose an integrated tool to help
users avoid filter bubbles in recommender systems. To achieve this objective,
we conduct a systematic literature review on the topic of filter bubbles in
recommender systems. The reviewed articles are carefully analyzed and
classified, providing valuable insights that inform the development of an
integrated approach. Notably, our review reveals evidence of filter bubbles in
recommendation systems, highlighting several biases that contribute to their
existence. Moreover, we propose mechanisms to mitigate the impact of filter
bubbles and demonstrate that incorporating diversity into recommendations can
potentially help alleviate this issue. The findings of this timely review will
serve as a benchmark for researchers working in interdisciplinary fields such
as privacy, artificial intelligence ethics, and recommendation systems.
Furthermore, it will open new avenues for future research in related domains,
prompting further exploration and advancement in this critical area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Areeb_Q/0/1/0/all/0/1&quot;&gt;Qazi Mohammad Areeb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nadeem_M/0/1/0/all/0/1&quot;&gt;Mohammad Nadeem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohail_S/0/1/0/all/0/1&quot;&gt;Shahab Saquib Sohail&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imam_R/0/1/0/all/0/1&quot;&gt;Raza Imam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doctor_F/0/1/0/all/0/1&quot;&gt;Faiyaz Doctor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Himeur_Y/0/1/0/all/0/1&quot;&gt;Yassine Himeur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1&quot;&gt;Amir Hussain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amira_A/0/1/0/all/0/1&quot;&gt;Abbes Amira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01225">
<title>Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT). (arXiv:2307.01225v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01225</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based text classifiers like BERT, Roberta, T5, and GPT-3 have
shown impressive performance in NLP. However, their vulnerability to
adversarial examples poses a security risk. Existing defense methods lack
interpretability, making it hard to understand adversarial classifications and
identify model vulnerabilities. To address this, we propose the
Interpretability and Transparency-Driven Detection and Transformation (IT-DT)
framework. It focuses on interpretability and transparency in detecting and
transforming textual adversarial examples. IT-DT utilizes techniques like
attention maps, integrated gradients, and model feedback for interpretability
during detection. This helps identify salient features and perturbed words
contributing to adversarial classifications. In the transformation phase, IT-DT
uses pre-trained embeddings and model feedback to generate optimal replacements
for perturbed words. By finding suitable substitutions, we aim to convert
adversarial examples into non-adversarial counterparts that align with the
model&apos;s intended behavior while preserving the text&apos;s meaning. Transparency is
emphasized through human expert involvement. Experts review and provide
feedback on detection and transformation results, enhancing decision-making,
especially in complex scenarios. The framework generates insights and threat
intelligence empowering analysts to identify vulnerabilities and improve model
robustness. Comprehensive experiments demonstrate the effectiveness of IT-DT in
detecting and transforming adversarial examples. The approach enhances
interpretability, provides transparency, and enables accurate identification
and successful transformation of adversarial inputs. By combining technical
analysis and human expertise, IT-DT significantly improves the resilience and
trustworthiness of transformer-based text classifiers against adversarial
attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabir_B/0/1/0/all/0/1&quot;&gt;Bushra Sabir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babar_M/0/1/0/all/0/1&quot;&gt;M. Ali Babar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abuadbba_S/0/1/0/all/0/1&quot;&gt;Sharif Abuadbba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01226">
<title>vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01226</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Neural Topic Models (NTM), inspired by variational autoencoders,
have attracted a lot of research interest; however, these methods have limited
applications in the real world due to the challenge of incorporating human
knowledge. This work presents a semi-supervised neural topic modeling method,
vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and
optimal transport. When a few keywords per topic are provided, vONTSS in the
semi-supervised setting generates potential topics and optimizes topic-keyword
quality and topic classification. Experiments show that vONTSS outperforms
existing semi-supervised topic modeling methods in classification accuracy and
diversity. vONTSS also supports unsupervised topic modeling. Quantitative and
qualitative experiments show that vONTSS in the unsupervised setting
outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered
and coherent topics on benchmark datasets. It is also much faster than the
state-of-the-art weakly supervised text classification method while achieving
similar classification performance. We further prove the equivalence of optimal
transport loss and cross-entropy loss at the global minimum.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weijie Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengamedu_S/0/1/0/all/0/1&quot;&gt;Srinivasan H. Sengamedu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iannacci_F/0/1/0/all/0/1&quot;&gt;Francis Iannacci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jinjin Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01227">
<title>ESGCN: Edge Squeeze Attention Graph Convolutional Network for Traffic Flow Forecasting. (arXiv:2307.01227v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01227</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic forecasting is a highly challenging task owing to the dynamical
spatio-temporal dependencies of traffic flows. To handle this, we focus on
modeling the spatio-temporal dynamics and propose a network termed Edge Squeeze
Graph Convolutional Network (ESGCN) to forecast traffic flow in multiple
regions. ESGCN consists of two modules: W-module and ES module. W-module is a
fully node-wise convolutional network. It encodes the time-series of each
traffic region separately and decomposes the time-series at various scales to
capture fine and coarse features. The ES module models the spatio-temporal
dynamics using Graph Convolutional Network (GCN) and generates an Adaptive
Adjacency Matrix (AAM) with temporal features. To improve the accuracy of AAM,
we introduce three key concepts. 1) Using edge features to directly capture the
spatiotemporal flow representation among regions. 2) Applying an edge attention
mechanism to GCN to extract the AAM from the edge features. Here, the attention
mechanism can effectively determine important spatio-temporal adjacency
relations. 3) Proposing a novel node contrastive loss to suppress obstructed
connections and emphasize related connections. Experimental results show that
ESGCN achieves state-of-the-art performance by a large margin on four
real-world datasets (PEMS03, 04, 07, and 08) with a low computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sangrok Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01229">
<title>EmoGen: Eliminating Subjective Bias in Emotional Music Generation. (arXiv:2307.01229v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.01229</link>
<description rdf:parseType="Literal">&lt;p&gt;Music is used to convey emotions, and thus generating emotional music is
important in automatic music generation. Previous work on emotional music
generation directly uses annotated emotion labels as control signals, which
suffers from subjective bias: different people may annotate different emotions
on the same music, and one person may feel different emotions under different
situations. Therefore, directly mapping emotion labels to music sequences in an
end-to-end way would confuse the learning process and hinder the model from
generating music with general emotions. In this paper, we propose EmoGen, an
emotional music generation system that leverages a set of emotion-related music
attributes as the bridge between emotion and music, and divides the generation
into two stages: emotion-to-attribute mapping with supervised clustering, and
attribute-to-music generation with self-supervised learning. Both stages are
beneficial: in the first stage, the attribute values around the clustering
center represent the general emotions of these samples, which help eliminate
the impacts of the subjective bias of emotion labels; in the second stage, the
generation is completely disentangled from emotion labels and thus free from
the subjective bias. Both subjective and objective evaluations show that EmoGen
outperforms previous methods on emotion control accuracy and music quality
respectively, which demonstrate our superiority in generating emotional music.
Music samples generated by EmoGen are available via this
link:https://ai-muzic.github.io/emogen/, and the code is available at this
link:https://github.com/microsoft/muzic/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_C/0/1/0/all/0/1&quot;&gt;Chenfei Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Peiling Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Botao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1&quot;&gt;Xu Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wei Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shikun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1&quot;&gt;Jiang Bian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01231">
<title>A Critical Re-evaluation of Benchmark Datasets for (Deep) Learning-Based Matching Algorithms. (arXiv:2307.01231v1 [cs.DB])</title>
<link>http://arxiv.org/abs/2307.01231</link>
<description rdf:parseType="Literal">&lt;p&gt;Entity resolution (ER) is the process of identifying records that refer to
the same entities within one or across multiple databases. Numerous techniques
have been developed to tackle ER challenges over the years, with recent
emphasis placed on machine and deep learning methods for the matching phase.
However, the quality of the benchmark datasets typically used in the
experimental evaluations of learning-based matching algorithms has not been
examined in the literature. To cover this gap, we propose four different
approaches to assessing the difficulty and appropriateness of 13 established
datasets: two theoretical approaches, which involve new measures of linearity
and existing measures of complexity, and two practical approaches: the
difference between the best non-linear and linear matchers, as well as the
difference between the best learning-based matcher and the perfect oracle. Our
analysis demonstrates that most of the popular datasets pose rather easy
classification tasks. As a result, they are not suitable for properly
evaluating learning-based matching algorithms. To address this issue, we
propose a new methodology for yielding benchmark datasets. We put it into
practice by creating four new matching tasks, and we verify that these new
benchmarks are more challenging and therefore more suitable for further
advancements in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadakis_G/0/1/0/all/0/1&quot;&gt;George Papadakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirielle_N/0/1/0/all/0/1&quot;&gt;Nishadi Kirielle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christen_P/0/1/0/all/0/1&quot;&gt;Peter Christen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palpanas_T/0/1/0/all/0/1&quot;&gt;Themis Palpanas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01234">
<title>Internet of Things Fault Detection and Classification via Multitask Learning. (arXiv:2307.01234v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01234</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a comprehensive investigation into developing a fault
detection and classification system for real-world IIoT applications. The study
addresses challenges in data collection, annotation, algorithm development, and
deployment. Using a real-world IIoT system, three phases of data collection
simulate 11 predefined fault categories. We propose SMTCNN for fault detection
and category classification in IIoT, evaluating its performance on real-world
data. SMTCNN achieves superior specificity (3.5%) and shows significant
improvements in precision, recall, and F1 measures compared to existing
techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Mohammad Arif Ul Alam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01238">
<title>Learning Difference Equations with Structured Grammatical Evolution for Postprandial Glycaemia Prediction. (arXiv:2307.01238v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01238</link>
<description rdf:parseType="Literal">&lt;p&gt;People with diabetes must carefully monitor their blood glucose levels,
especially after eating. Blood glucose regulation requires a proper combination
of food intake and insulin boluses. Glucose prediction is vital to avoid
dangerous post-meal complications in treating individuals with diabetes.
Although traditional methods, such as artificial neural networks, have shown
high accuracy rates, sometimes they are not suitable for developing
personalised treatments by physicians due to their lack of interpretability. In
this study, we propose a novel glucose prediction method emphasising
interpretability: Interpretable Sparse Identification by Grammatical Evolution.
Combined with a previous clustering stage, our approach provides finite
difference equations to predict postprandial glucose levels up to two hours
after meals. We divide the dataset into four-hour segments and perform
clustering based on blood glucose values for the twohour window before the
meal. Prediction models are trained for each cluster for the two-hour windows
after meals, allowing predictions in 15-minute steps, yielding up to eight
predictions at different time horizons. Prediction safety was evaluated based
on Parkes Error Grid regions. Our technique produces safe predictions through
explainable expressions, avoiding zones D (0.2% average) and E (0%) and
reducing predictions on zone C (6.2%). In addition, our proposal has slightly
better accuracy than other techniques, including sparse identification of
non-linear dynamics and artificial neural networks. The results demonstrate
that our proposal provides interpretable solutions without sacrificing
prediction accuracy, offering a promising approach to glucose prediction in
diabetes management that balances accuracy, interpretability, and computational
efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parra_D/0/1/0/all/0/1&quot;&gt;Daniel Parra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joedicke_D/0/1/0/all/0/1&quot;&gt;David Joedicke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velasco_J/0/1/0/all/0/1&quot;&gt;J. Manuel Velasco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1&quot;&gt;Gabriel Kronberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hidalgo_J/0/1/0/all/0/1&quot;&gt;J. Ignacio Hidalgo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01240">
<title>MWPRanker: An Expression Similarity Based Math Word Problem Retriever. (arXiv:2307.01240v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.01240</link>
<description rdf:parseType="Literal">&lt;p&gt;Math Word Problems (MWPs) in online assessments help test the ability of the
learner to make critical inferences by interpreting the linguistic information
in them. To test the mathematical reasoning capabilities of the learners,
sometimes the problem is rephrased or the thematic setting of the original MWP
is changed. Since manual identification of MWPs with similar problem models is
cumbersome, we propose a tool in this work for MWP retrieval. We propose a
hybrid approach to retrieve similar MWPs with the same problem model. In our
work, the problem model refers to the sequence of operations to be performed to
arrive at the solution. We demonstrate that our tool is useful for the
mentioned tasks and better than semantic similarity-based approaches, which
fail to capture the arithmetic and logical sequence of the MWPs. A demo of the
tool can be found at https://www.youtube.com/watch?v=gSQWP3chFIs
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_M/0/1/0/all/0/1&quot;&gt;Mayank Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+V_V/0/1/0/all/0/1&quot;&gt;Venktesh V&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1&quot;&gt;Vikram Goyal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01288">
<title>Fighting the disagreement in Explainable Machine Learning with consensus. (arXiv:2307.01288v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01288</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) models are often valued by the accuracy of their
predictions. However, in some areas of science, the inner workings of models
are as relevant as their accuracy. To understand how ML models work internally,
the use of interpretability algorithms is the preferred option. Unfortunately,
despite the diversity of algorithms available, they often disagree in
explaining a model, leading to contradictory explanations. To cope with this
issue, consensus functions can be applied once the models have been explained.
Nevertheless, the problem is not completely solved because the final result
will depend on the selected consensus function and other factors. In this
paper, six consensus functions have been evaluated for the explanation of five
ML models. The models were previously trained on four synthetic datasets whose
internal rules were known in advance. The models were then explained with
model-agnostic local and global interpretability algorithms. Finally, consensus
was calculated with six different functions, including one developed by the
authors. The results demonstrated that the proposed function is fairer than the
others and provides more consistent and accurate explanations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banegas_Luna_A/0/1/0/all/0/1&quot;&gt;Antonio Jesus Banegas-Luna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Cortes_C/0/1/0/all/0/1&quot;&gt;Carlos Mart&amp;#x131;nez-Cortes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_Sanchez_H/0/1/0/all/0/1&quot;&gt;Horacio Perez-Sanchez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01292">
<title>Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems. (arXiv:2307.01292v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.01292</link>
<description rdf:parseType="Literal">&lt;p&gt;With the emergence of large foundational models, model-serving systems are
becoming popular. In such a system, users send the queries to the server and
specify the desired performance metrics (e.g., accuracy, latency, etc.). The
server maintains a set of models (model zoo) in the back-end and serves the
queries based on the specified metrics. This paper examines the security,
specifically robustness against model extraction attacks, of such systems.
Existing black-box attacks cannot be directly applied to extract a victim
model, as models hide among the model zoo behind the inference serving
interface, and attackers cannot identify which model is being used. An
intermediate step is required to ensure that every input query gets the output
from the victim model. To this end, we propose a query-efficient fingerprinting
algorithm to enable the attacker to trigger any desired model consistently. We
show that by using our fingerprinting algorithm, model extraction can have
fidelity and accuracy scores within $1\%$ of the scores obtained if attacking
in a single-model setting and up to $14.6\%$ gain in accuracy and up to $7.7\%$
gain in fidelity compared to the naive attack. Finally, we counter the proposed
attack with a noise-based defense mechanism that thwarts fingerprinting by
adding noise to the specified performance metrics. Our defense strategy reduces
the attack&apos;s accuracy and fidelity by up to $9.8\%$ and $4.8\%$, respectively
(on medium-sized model extraction). We show that the proposed defense induces a
fundamental trade-off between the level of protection and system goodput,
achieving configurable and significant victim model extraction protection while
maintaining acceptable goodput ($&amp;gt;80\%$). We provide anonymous access to our
code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanyal_D/0/1/0/all/0/1&quot;&gt;Debopam Sanyal&lt;/a&gt; (Georgia Institute of Technology), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hung_J/0/1/0/all/0/1&quot;&gt;Jui-Tse Hung&lt;/a&gt; (Georgia Institute of Technology), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_M/0/1/0/all/0/1&quot;&gt;Manav Agrawal&lt;/a&gt; (Georgia Institute of Technology), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jasti_P/0/1/0/all/0/1&quot;&gt;Prahlad Jasti&lt;/a&gt; (Georgia Institute of Technology), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikkhoo_S/0/1/0/all/0/1&quot;&gt;Shahab Nikkhoo&lt;/a&gt; (University of California, Riverside), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1&quot;&gt;Somesh Jha&lt;/a&gt; (University of Wisconsin, Madison), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianhao Wang&lt;/a&gt; (University of Virginia), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1&quot;&gt;Sibin Mohan&lt;/a&gt; (The George Washington University), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tumanov_A/0/1/0/all/0/1&quot;&gt;Alexey Tumanov&lt;/a&gt; (Georgia Institute of Technology)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01301">
<title>Reliable AI: Does the Next Generation Require Quantum Computing?. (arXiv:2307.01301v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.01301</link>
<description rdf:parseType="Literal">&lt;p&gt;In this survey, we aim to explore the fundamental question of whether the
next generation of artificial intelligence requires quantum computing.
Artificial intelligence is increasingly playing a crucial role in many aspects
of our daily lives and is central to the fourth industrial revolution. It is
therefore imperative that artificial intelligence is reliable and trustworthy.
However, there are still many issues with reliability of artificial
intelligence, such as privacy, responsibility, safety, and security, in areas
such as autonomous driving, healthcare, robotics, and others. These problems
can have various causes, including insufficient data, biases, and robustness
problems, as well as fundamental issues such as computability problems on
digital hardware. The cause of these computability problems is rooted in the
fact that digital hardware is based on the computing model of the Turing
machine, which is inherently discrete. Notably, our findings demonstrate that
digital hardware is inherently constrained in solving problems about
optimization, deep learning, or differential equations. Therefore, these
limitations carry substantial implications for the field of artificial
intelligence, in particular for machine learning. Furthermore, although it is
well known that the quantum computer shows a quantum advantage for certain
classes of problems, our findings establish that some of these limitations
persist when employing quantum computing models based on the quantum circuit or
the quantum Turing machine paradigm. In contrast, analog computing models, such
as the Blum-Shub-Smale machine, exhibit the potential to surmount these
limitations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bacho_A/0/1/0/all/0/1&quot;&gt;Aras Bacho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boche_H/0/1/0/all/0/1&quot;&gt;Holger Boche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1&quot;&gt;Gitta Kutyniok&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01312">
<title>Self-Tuning PID Control via a Hybrid Actor-Critic-Based Neural Structure for Quadcopter Control. (arXiv:2307.01312v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2307.01312</link>
<description rdf:parseType="Literal">&lt;p&gt;Proportional-Integrator-Derivative (PID) controller is used in a wide range
of industrial and experimental processes. There are a couple of offline methods
for tuning PID gains. However, due to the uncertainty of model parameters and
external disturbances, real systems such as Quadrotors need more robust and
reliable PID controllers. In this research, a self-tuning PID controller using
a Reinforcement-Learning-based Neural Network for attitude and altitude control
of a Quadrotor has been investigated. An Incremental PID, which contains static
and dynamic gains, has been considered and only the variable gains have been
tuned. To tune dynamic gains, a model-free actor-critic-based hybrid neural
structure was used that was able to properly tune PID gains, and also has done
the best as an identifier. In both tunning and identification tasks, a Neural
Network with two hidden layers and sigmoid activation functions has been
learned using Adaptive Momentum (ADAM) optimizer and Back-Propagation (BP)
algorithm. This method is online, able to tackle disturbance, and fast in
training. In addition to robustness to mass uncertainty and wind gust
disturbance, results showed that the proposed method had a better performance
when compared to a PID controller with constant gains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sharifi_I/0/1/0/all/0/1&quot;&gt;Iman Sharifi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alasty_A/0/1/0/all/0/1&quot;&gt;Aria Alasty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01316">
<title>Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach. (arXiv:2307.01316v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.01316</link>
<description rdf:parseType="Literal">&lt;p&gt;The dynamic nature of driving environments and the presence of diverse road
users pose significant challenges for decision-making in autonomous driving.
Deep reinforcement learning (DRL) has emerged as a popular approach to tackle
this problem. However, the application of existing DRL solutions is mainly
confined to simulated environments due to safety concerns, impeding their
deployment in real-world. To overcome this limitation, this paper introduces a
novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics
(DRLSL) that combines the strengths of DRL (learning from experience) and
symbolic first-order logics knowledge-driven reasoning) to enable safe learning
in real-time interactions of autonomous driving within real environments. This
innovative approach provides a means to learn autonomous driving policies by
actively engaging with the physical environment while ensuring safety. We have
implemented the DRLSL framework in autonomous driving using the highD dataset
and demonstrated that our method successfully avoids unsafe actions during both
the training and testing phases. Furthermore, our results indicate that DRLSL
achieves faster convergence during training and exhibits better
generalizability to new driving scenarios compared to traditional DRL methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharifi_I/0/1/0/all/0/1&quot;&gt;Iman Sharifi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yildirim_M/0/1/0/all/0/1&quot;&gt;Mustafa Yildirim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1&quot;&gt;Saber Fallah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01366">
<title>Minimizing Age of Information for Mobile Edge Computing Systems: A Nested Index Approach. (arXiv:2307.01366v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.01366</link>
<description rdf:parseType="Literal">&lt;p&gt;Exploiting the computational heterogeneity of mobile devices and edge nodes,
mobile edge computation (MEC) provides an efficient approach to achieving
real-time applications that are sensitive to information freshness, by
offloading tasks from mobile devices to edge nodes. We use the metric
Age-of-Information (AoI) to evaluate information freshness. An efficient
solution to minimize the AoI for the MEC system with multiple users is
non-trivial to obtain due to the random computing time. In this paper, we
consider multiple users offloading tasks to heterogeneous edge servers in a MEC
system. We first reformulate the problem as a Restless Multi-Arm-Bandit (RMAB)
problem and establish a hierarchical Markov Decision Process (MDP) to
characterize the updating of AoI for the MEC system. Based on the hierarchical
MDP, we propose a nested index framework and design a nested index policy with
provably asymptotic optimality. Finally, the closed form of the nested index is
obtained, which enables the performance tradeoffs between computation
complexity and accuracy. Our algorithm leads to an optimality gap reduction of
up to 40%, compared to benchmarks. Our algorithm asymptotically approximates
the lower bound as the system scalar gets large enough.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1&quot;&gt;Ning Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Meng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01371">
<title>Efficient Determination of Safety Requirements for Perception Systems. (arXiv:2307.01371v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.01371</link>
<description rdf:parseType="Literal">&lt;p&gt;Perception systems operate as a subcomponent of the general autonomy stack,
and perception system designers often need to optimize performance
characteristics while maintaining safety with respect to the overall
closed-loop system. For this reason, it is useful to distill high-level safety
requirements into component-level requirements on the perception system. In
this work, we focus on efficiently determining sets of safe perception system
performance characteristics given a black-box simulator of the
fully-integrated, closed-loop system. We combine the advantages of common
black-box estimation techniques such as Gaussian processes and threshold
bandits to develop a new estimation method, which we call smoothing bandits. We
demonstrate our method on a vision-based aircraft collision avoidance problem
and show improvements in terms of both accuracy and efficiency over the
Gaussian process and threshold bandit baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katz_S/0/1/0/all/0/1&quot;&gt;Sydney M. Katz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corso_A/0/1/0/all/0/1&quot;&gt;Anthony L. Corso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yel_E/0/1/0/all/0/1&quot;&gt;Esen Yel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1&quot;&gt;Mykel J. Kochenderfer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01378">
<title>A CNN regression model to estimate buildings height maps using Sentinel-1 SAR and Sentinel-2 MSI time series. (arXiv:2307.01378v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01378</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate estimation of building heights is essential for urban planning,
infrastructure management, and environmental analysis. In this study, we
propose a supervised Multimodal Building Height Regression Network (MBHR-Net)
for estimating building heights at 10m spatial resolution using Sentinel-1 (S1)
and Sentinel-2 (S2) satellite time series. S1 provides Synthetic Aperture Radar
(SAR) data that offers valuable information on building structures, while S2
provides multispectral data that is sensitive to different land cover types,
vegetation phenology, and building shadows. Our MBHR-Net aims to extract
meaningful features from the S1 and S2 images to learn complex spatio-temporal
relationships between image patterns and building heights. The model is trained
and tested in 10 cities in the Netherlands. Root Mean Squared Error (RMSE),
Intersection over Union (IOU), and R-squared (R2) score metrics are used to
evaluate the performance of the model. The preliminary results (3.73m RMSE,
0.95 IoU, 0.61 R2) demonstrate the effectiveness of our deep learning model in
accurately estimating building heights, showcasing its potential for urban
planning, environmental impact analysis, and other related applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadav_R/0/1/0/all/0/1&quot;&gt;Ritu Yadav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nascetti_A/0/1/0/all/0/1&quot;&gt;Andrea Nascetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1&quot;&gt;Yifang Ban&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01379">
<title>Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models. (arXiv:2307.01379v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01379</link>
<description rdf:parseType="Literal">&lt;p&gt;Although Large Language Models (LLMs) have shown great potential in Natural
Language Generation, it is still challenging to characterize the uncertainty of
model generations, i.e., when users could trust model outputs. Our research is
derived from the heuristic facts that tokens are created unequally in
reflecting the meaning of generations by auto-regressive LLMs, i.e., some
tokens are more relevant (or representative) than others, yet all the tokens
are equally valued when estimating uncertainty. It is because of the linguistic
redundancy where mostly a few keywords are sufficient to convey the meaning of
a long sentence. We name these inequalities as generative inequalities and
investigate how they affect uncertainty estimation. Our results reveal that
considerable tokens and sentences containing limited semantics are weighted
equally or even heavily when estimating uncertainty. To tackle these biases
posed by generative inequalities, we propose to jointly Shifting Attention to
more Relevant (SAR) components from both the token level and the sentence level
while estimating uncertainty. We conduct experiments over popular
&quot;off-the-shelf&quot; LLMs (e.g., OPT, LLaMA) with model sizes up to 30B and powerful
commercial LLMs (e.g., Davinci from OpenAI), across various free-form
question-answering tasks. Experimental results and detailed demographic
analysis indicate the superior performance of SAR. Code is available at
https://github.com/jinhaoduan/shifting-attention-to-relevance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Jinhao Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chenan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zavalny_A/0/1/0/all/0/1&quot;&gt;Alex Zavalny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Renjing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1&quot;&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kaidi Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01383">
<title>Depth video data-enabled predictions of longitudinal dairy cow body weight using thresholding and Mask R-CNN algorithms. (arXiv:2307.01383v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01383</link>
<description rdf:parseType="Literal">&lt;p&gt;Monitoring cow body weight is crucial to support farm management decisions
due to its direct relationship with the growth, nutritional status, and health
of dairy cows. Cow body weight is a repeated trait, however, the majority of
previous body weight prediction research only used data collected at a single
point in time. Furthermore, the utility of deep learning-based segmentation for
body weight prediction using videos remains unanswered. Therefore, the
objectives of this study were to predict cow body weight from repeatedly
measured video data, to compare the performance of the thresholding and Mask
R-CNN deep learning approaches, to evaluate the predictive ability of body
weight regression models, and to promote open science in the animal science
community by releasing the source code for video-based body weight prediction.
A total of 40,405 depth images and depth map files were obtained from 10
lactating Holstein cows and 2 non-lactating Jersey cows. Three approaches were
investigated to segment the cow&apos;s body from the background, including single
thresholding, adaptive thresholding, and Mask R-CNN. Four image-derived
biometric features, such as dorsal length, abdominal width, height, and volume,
were estimated from the segmented images. On average, the Mask-RCNN approach
combined with a linear mixed model resulted in the best prediction coefficient
of determination and mean absolute percentage error of 0.98 and 2.03%,
respectively, in the forecasting cross-validation. The Mask-RCNN approach was
also the best in the leave-three-cows-out cross-validation. The prediction
coefficients of determination and mean absolute percentage error of the
Mask-RCNN coupled with the linear mixed model were 0.90 and 4.70%,
respectively. Our results suggest that deep learning-based segmentation
improves the prediction performance of cow body weight from longitudinal depth
video data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_Y/0/1/0/all/0/1&quot;&gt;Ye Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campos_L/0/1/0/all/0/1&quot;&gt;Leticia M.Campos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haipeng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanigan_M/0/1/0/all/0/1&quot;&gt;Mark D.Hanigan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morota_G/0/1/0/all/0/1&quot;&gt;Gota Morota&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01394">
<title>In-depth Analysis On Parallel Processing Patterns for High-Performance Dataframes. (arXiv:2307.01394v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2307.01394</link>
<description rdf:parseType="Literal">&lt;p&gt;The Data Science domain has expanded monumentally in both research and
industry communities during the past decade, predominantly owing to the Big
Data revolution. Artificial Intelligence (AI) and Machine Learning (ML) are
bringing more complexities to data engineering applications, which are now
integrated into data processing pipelines to process terabytes of data.
Typically, a significant amount of time is spent on data preprocessing in these
pipelines, and hence improving its e fficiency directly impacts the overall
pipeline performance. The community has recently embraced the concept of
Dataframes as the de-facto data structure for data representation and
manipulation. However, the most widely used serial Dataframes today (R, pandas)
experience performance limitations while working on even moderately large data
sets. We believe that there is plenty of room for improvement by taking a look
at this problem from a high-performance computing point of view. In a prior
publication, we presented a set of parallel processing patterns for distributed
dataframe operators and the reference runtime implementation, Cylon [1]. In
this paper, we are expanding on the initial concept by introducing a cost model
for evaluating the said patterns. Furthermore, we evaluate the performance of
Cylon on the ORNL Summit supercomputer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perera_N/0/1/0/all/0/1&quot;&gt;Niranda Perera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarker_A/0/1/0/all/0/1&quot;&gt;Arup Kumar Sarker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Staylor_M/0/1/0/all/0/1&quot;&gt;Mills Staylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laszewski_G/0/1/0/all/0/1&quot;&gt;Gregor von Laszewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_K/0/1/0/all/0/1&quot;&gt;Kaiying Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamburugamuve_S/0/1/0/all/0/1&quot;&gt;Supun Kamburugamuve&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Widanage_C/0/1/0/all/0/1&quot;&gt;Chathura Widanage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abeykoon_V/0/1/0/all/0/1&quot;&gt;Vibhatha Abeykoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanewela_T/0/1/0/all/0/1&quot;&gt;Thejaka Amila Kanewela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fox_G/0/1/0/all/0/1&quot;&gt;Geoffrey Fox&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01403">
<title>Learning to Communicate using Contrastive Learning. (arXiv:2307.01403v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.01403</link>
<description rdf:parseType="Literal">&lt;p&gt;Communication is a powerful tool for coordination in multi-agent RL. But
inducing an effective, common language is a difficult challenge, particularly
in the decentralized setting. In this work, we introduce an alternative
perspective where communicative messages sent between agents are considered as
different incomplete views of the environment state. By examining the
relationship between messages sent and received, we propose to learn to
communicate using contrastive learning to maximize the mutual information
between messages of a given trajectory. In communication-essential
environments, our method outperforms previous work in both performance and
learning speed. Using qualitative metrics and representation probing, we show
that our method induces more symmetric communication and captures global state
information from the environment. Overall, we show the power of contrastive
learning and the importance of leveraging messages as encodings for effective
communication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_Y/0/1/0/all/0/1&quot;&gt;Yat Long Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_B/0/1/0/all/0/1&quot;&gt;Biswa Sengupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1&quot;&gt;Jakob Foerster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noukhovitch_M/0/1/0/all/0/1&quot;&gt;Michael Noukhovitch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01421">
<title>Unsupervised Feature Learning with Emergent Data-Driven Prototypicality. (arXiv:2307.01421v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01421</link>
<description rdf:parseType="Literal">&lt;p&gt;Given an image set without any labels, our goal is to train a model that maps
each image to a point in a feature space such that, not only proximity
indicates visual similarity, but where it is located directly encodes how
prototypical the image is according to the dataset.
&lt;/p&gt;
&lt;p&gt;Our key insight is to perform unsupervised feature learning in hyperbolic
instead of Euclidean space, where the distance between points still reflect
image similarity, and yet we gain additional capacity for representing
prototypicality with the location of the point: The closer it is to the origin,
the more prototypical it is. The latter property is simply emergent from
optimizing the usual metric learning objective: The image similar to many
training instances is best placed at the center of corresponding points in
Euclidean space, but closer to the origin in hyperbolic space.
&lt;/p&gt;
&lt;p&gt;We propose an unsupervised feature learning algorithm in Hyperbolic space
with sphere pACKing. HACK first generates uniformly packed particles in the
Poincar\&apos;e ball of hyperbolic space and then assigns each image uniquely to
each particle. Images after congealing are regarded more typical of the dataset
it belongs to. With our feature mapper simply trained to spread out training
instances in hyperbolic space, we observe that images move closer to the origin
with congealing, validating our idea of unsupervised prototypicality discovery.
We demonstrate that our data-driven prototypicality provides an easy and
superior unsupervised instance selection to reduce sample complexity, increase
model generalization with atypical instances and robustness with typical ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yunhui Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Youren Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yubei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Stella X. Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01449">
<title>A Double Machine Learning Approach to Combining Experimental and Observational Data. (arXiv:2307.01449v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2307.01449</link>
<description rdf:parseType="Literal">&lt;p&gt;Experimental and observational studies often lack validity due to untestable
assumptions. We propose a double machine learning approach to combine
experimental and observational studies, allowing practitioners to test for
assumption violations and estimate treatment effects consistently. Our
framework tests for violations of external validity and ignorability under
milder assumptions. When only one assumption is violated, we provide
semi-parametrically efficient treatment effect estimators. However, our
no-free-lunch theorem highlights the necessity of accurately identifying the
violated assumption for consistent treatment effect estimation. We demonstrate
the applicability of our approach in three real-world case studies,
highlighting its relevance for practical settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Morucci_M/0/1/0/all/0/1&quot;&gt;Marco Morucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Orlandi_V/0/1/0/all/0/1&quot;&gt;Vittorio Orlandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Parikh_H/0/1/0/all/0/1&quot;&gt;Harsh Parikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Sudeepa Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rudin_C/0/1/0/all/0/1&quot;&gt;Cynthia Rudin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Volfovsky_A/0/1/0/all/0/1&quot;&gt;Alexander Volfovsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01452">
<title>Causal Reinforcement Learning: A Survey. (arXiv:2307.01452v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01452</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning is an essential paradigm for solving sequential
decision problems under uncertainty. Despite many remarkable achievements in
recent decades, applying reinforcement learning methods in the real world
remains challenging. One of the main obstacles is that reinforcement learning
agents lack a fundamental understanding of the world and must therefore learn
from scratch through numerous trial-and-error interactions. They may also face
challenges in providing explanations for their decisions and generalizing the
acquired knowledge. Causality, however, offers a notable advantage as it can
formalize knowledge in a systematic manner and leverage invariance for
effective knowledge transfer. This has led to the emergence of causal
reinforcement learning, a subfield of reinforcement learning that seeks to
enhance existing algorithms by incorporating causal relationships into the
learning process. In this survey, we comprehensively review the literature on
causal reinforcement learning. We first introduce the basic concepts of
causality and reinforcement learning, and then explain how causality can
address core challenges in non-causal reinforcement learning. We categorize and
systematically review existing causal reinforcement learning approaches based
on their target problems and methodologies. Finally, we outline open issues and
future directions in this emerging field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhihong Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1&quot;&gt;Guodong Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chengqi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01472">
<title>Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning. (arXiv:2307.01472v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.01472</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel Diffusion Offline Multi-agent Model (DOM2) for offline
Multi-Agent Reinforcement Learning (MARL). Different from existing algorithms
that rely mainly on conservatism in policy design, DOM2 enhances policy
expressiveness and diversity based on diffusion. Specifically, we incorporate a
diffusion model into the policy network and propose a trajectory-based
data-augmentation scheme in training. These key ingredients make our algorithm
more robust to environment changes and achieve significant improvements in
performance, generalization and data-efficiency. Our extensive experimental
results demonstrate that DOM2 outperforms existing state-of-the-art methods in
multi-agent particle and multi-agent MuJoCo environments, and generalizes
significantly better in shifted environments thanks to its high expressiveness
and diversity. Furthermore, DOM2 shows superior data efficiency and can achieve
state-of-the-art performance with $20+$ times less data compared to existing
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Ling Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Longbo Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01473">
<title>Mitigating Bias: Enhancing Image Classification by Improving Model Explanations. (arXiv:2307.01473v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01473</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models have demonstrated remarkable capabilities in learning
complex patterns and concepts from training data. However, recent findings
indicate that these models tend to rely heavily on simple and easily
discernible features present in the background of images rather than the main
concepts or objects they are intended to classify. This phenomenon poses a
challenge to image classifiers as the crucial elements of interest in images
may be overshadowed. In this paper, we propose a novel approach to address this
issue and improve the learning of main concepts by image classifiers. Our
central idea revolves around concurrently guiding the model&apos;s attention toward
the foreground during the classification task. By emphasizing the foreground,
which encapsulates the primary objects of interest, we aim to shift the focus
of the model away from the dominant influence of the background. To accomplish
this, we introduce a mechanism that encourages the model to allocate sufficient
attention to the foreground. We investigate various strategies, including
modifying the loss function or incorporating additional architectural
components, to enable the classifier to effectively capture the primary concept
within an image. Additionally, we explore the impact of different foreground
attention mechanisms on model performance and provide insights into their
effectiveness. Through extensive experimentation on benchmark datasets, we
demonstrate the efficacy of our proposed approach in improving the
classification accuracy of image classifiers. Our findings highlight the
importance of foreground attention in enhancing model understanding and
representation of the main concepts within images. The results of this study
contribute to advancing the field of image classification and provide valuable
insights for developing more robust and accurate deep-learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmadi_R/0/1/0/all/0/1&quot;&gt;Raha Ahmadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajabi_M/0/1/0/all/0/1&quot;&gt;Mohammad Javad Rajabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabokrou_M/0/1/0/all/0/1&quot;&gt;Mohamamd Khalooiem Mohammad Sabokrou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01502">
<title>HEDI: First-Time Clinical Application and Results of a Biomechanical Evaluation and Visualisation Tool for Incisional Hernia Repair. (arXiv:2307.01502v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01502</link>
<description rdf:parseType="Literal">&lt;p&gt;Abdominal wall defects often lead to pain, discomfort, and recurrence of
incisional hernias, resulting in significant morbidity and repeated surgical
repairs worldwide. Mesh repair for large hernias is usually based on the defect
area with a fixed overlap, without considering biomechanical aspects such as
muscle activation, intra-abdominal pressure, tissue elasticity, and abdominal
wall distention. To address this issue, we present a biomechanical approach to
incisional hernia repair that takes into account the unstable abdominal wall.
Additionally, we introduce HEDI, a tool that uses dynamic computed tomography
with Valsalva maneuver to automatically detect and assess hernia size, volume,
and abdominal wall instability. Our first clinical application of HEDI in the
preoperative evaluation of 31 patients shows significantly improved success
rates compared to reported rates, with all patients remaining pain-free and
showing no hernia recurrence after three years of follow-up.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Relle_J/0/1/0/all/0/1&quot;&gt;Jacob J. Relle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voss_S/0/1/0/all/0/1&quot;&gt;Samuel Vo&amp;#xdf;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raschidi_R/0/1/0/all/0/1&quot;&gt;Ramesch Raschidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nessel_R/0/1/0/all/0/1&quot;&gt;Regine Nessel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorich_J/0/1/0/all/0/1&quot;&gt;Johannes G&amp;#xf6;rich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wielputz_M/0/1/0/all/0/1&quot;&gt;Mark O. Wielp&amp;#xfc;tz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loffler_T/0/1/0/all/0/1&quot;&gt;Thorsten L&amp;#xf6;ffler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heuveline_V/0/1/0/all/0/1&quot;&gt;Vincent Heuveline&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kallinowski_F/0/1/0/all/0/1&quot;&gt;Friedrich Kallinowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Losel_P/0/1/0/all/0/1&quot;&gt;Philipp D. L&amp;#xf6;sel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01504">
<title>All in One: Multi-task Prompting for Graph Neural Networks. (arXiv:2307.01504v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2307.01504</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, &apos;&apos;pre-training and fine-tuning&apos;&apos; has been adopted as a standard
workflow for many graph tasks since it can take general graph knowledge to
relieve the lack of graph annotations from each application. However, graph
tasks with node level, edge level, and graph level are far diversified, making
the pre-training pretext often incompatible with these multiple tasks. This gap
may even cause a &apos;&apos;negative transfer&apos;&apos; to the specific application, leading to
poor results. Inspired by the prompt learning in natural language processing
(NLP), which has presented significant effectiveness in leveraging prior
knowledge for various NLP tasks, we study the prompting topic for graphs with
the motivation of filling the gap between pre-trained models and various graph
tasks. In this paper, we propose a novel multi-task prompting method for graph
models. Specifically, we first unify the format of graph prompts and language
prompts with the prompt token, token structure, and inserting pattern. In this
way, the prompting idea from NLP can be seamlessly introduced to the graph
area. Then, to further narrow the gap between various graph tasks and
state-of-the-art pre-training strategies, we further study the task space of
various graph applications and reformulate downstream problems to the
graph-level task. Afterward, we introduce meta-learning to efficiently learn a
better initialization for the multi-task prompt of graphs so that our prompting
framework can be more reliable and general for different tasks. We conduct
extensive experiments, results from which demonstrate the superiority of our
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiangguo Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hong Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1&quot;&gt;Jihong Guan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01519">
<title>Deep Attention Q-Network for Personalized Treatment Recommendation. (arXiv:2307.01519v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01519</link>
<description rdf:parseType="Literal">&lt;p&gt;Tailoring treatment for individual patients is crucial yet challenging in
order to achieve optimal healthcare outcomes. Recent advances in reinforcement
learning offer promising personalized treatment recommendations; however, they
rely solely on current patient observations (vital signs, demographics) as the
patient&apos;s state, which may not accurately represent the true health status of
the patient. This limitation hampers policy learning and evaluation, ultimately
limiting treatment effectiveness. In this study, we propose the Deep Attention
Q-Network for personalized treatment recommendations, utilizing the Transformer
architecture within a deep reinforcement learning framework to efficiently
incorporate all past patient observations. We evaluated the model on real-world
sepsis and acute hypotension cohorts, demonstrating its superiority to
state-of-the-art models. The source code for our model is available at
https://github.com/stevenmsm/RL-ICU-DAQN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Simin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junghwan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serban_N/0/1/0/all/0/1&quot;&gt;Nicoleta Serban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shihao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01520">
<title>LEAT: Towards Robust Deepfake Disruption in Real-World Scenarios via Latent Ensemble Attack. (arXiv:2307.01520v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01520</link>
<description rdf:parseType="Literal">&lt;p&gt;Deepfakes, malicious visual contents created by generative models, pose an
increasingly harmful threat to society. To proactively mitigate deepfake
damages, recent studies have employed adversarial perturbation to disrupt
deepfake model outputs. However, previous approaches primarily focus on
generating distorted outputs based on only predetermined target attributes,
leading to a lack of robustness in real-world scenarios where target attributes
are unknown. Additionally, the transferability of perturbations between two
prominent generative models, Generative Adversarial Networks (GANs) and
Diffusion Models, remains unexplored. In this paper, we emphasize the
importance of target attribute-transferability and model-transferability for
achieving robust deepfake disruption. To address this challenge, we propose a
simple yet effective disruption method called Latent Ensemble ATtack (LEAT),
which attacks the independent latent encoding process. By disrupting the latent
encoding process, it generates distorted output images in subsequent generation
processes, regardless of the given target attributes. This target
attribute-agnostic attack ensures robust disruption even when the target
attributes are unknown. Additionally, we introduce a Normalized Gradient
Ensemble strategy that effectively aggregates gradients for iterative gradient
attacks, enabling simultaneous attacks on various types of deepfake models,
involving both GAN-based and Diffusion-based models. Moreover, we demonstrate
the insufficiency of evaluating disruption quality solely based on pixel-level
differences. As a result, we propose an alternative protocol for
comprehensively evaluating the success of defense. Extensive experiments
confirm the efficacy of our method in disrupting deepfakes in real-world
scenarios, reporting a higher defense success rate compared to previous
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shim_J/0/1/0/all/0/1&quot;&gt;Joonkyo Shim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1&quot;&gt;Hyunsoo Yoon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01530">
<title>Convolutional Transformer for Autonomous Recognition and Grading of Tomatoes Under Various Lighting, Occlusion, and Ripeness Conditions. (arXiv:2307.01530v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01530</link>
<description rdf:parseType="Literal">&lt;p&gt;Harvesting fully ripe tomatoes with mobile robots presents significant
challenges in real-world scenarios. These challenges arise from factors such as
occlusion caused by leaves and branches, as well as the color similarity
between tomatoes and the surrounding foliage during the fruit development
stage. The natural environment further compounds these issues with varying
light conditions, viewing angles, occlusion factors, and different maturity
levels. To overcome these obstacles, this research introduces a novel framework
that leverages a convolutional transformer architecture to autonomously
recognize and grade tomatoes, irrespective of their occlusion level, lighting
conditions, and ripeness. The proposed model is trained and tested using
carefully annotated images curated specifically for this purpose. The dataset
is prepared under various lighting conditions, viewing perspectives, and
employs different mobile camera sensors, distinguishing it from existing
datasets such as Laboro Tomato and Rob2Pheno Annotated Tomato. The
effectiveness of the proposed framework in handling cluttered and occluded
tomato instances was evaluated using two additional public datasets, Laboro
Tomato and Rob2Pheno Annotated Tomato, as benchmarks. The evaluation results
across these three datasets demonstrate the exceptional performance of our
proposed framework, surpassing the state-of-the-art by 58.14%, 65.42%, and
66.39% in terms of mean average precision scores for KUTomaData, Laboro Tomato,
and Rob2Pheno Annotated Tomato, respectively. The results underscore the
superiority of the proposed model in accurately detecting and delineating
tomatoes compared to baseline methods and previous approaches. Specifically,
the model achieves an F1-score of 80.14%, a Dice coefficient of 73.26%, and a
mean IoU of 66.41% on the KUTomaData image dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Asim Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassan_T/0/1/0/all/0/1&quot;&gt;Taimur Hassan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafay_M/0/1/0/all/0/1&quot;&gt;Muhammad Shafay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fahmy_I/0/1/0/all/0/1&quot;&gt;Israa Fahmy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1&quot;&gt;Naoufel Werghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seneviratne_L/0/1/0/all/0/1&quot;&gt;Lakmal Seneviratne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussain_I/0/1/0/all/0/1&quot;&gt;Irfan Hussain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01532">
<title>Analyzing Intentional Behavior in Autonomous Agents under Uncertainty. (arXiv:2307.01532v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.01532</link>
<description rdf:parseType="Literal">&lt;p&gt;Principled accountability for autonomous decision-making in uncertain
environments requires distinguishing intentional outcomes from negligent
designs from actual accidents. We propose analyzing the behavior of autonomous
agents through a quantitative measure of the evidence of intentional behavior.
We model an uncertain environment as a Markov Decision Process (MDP). For a
given scenario, we rely on probabilistic model checking to compute the ability
of the agent to influence reaching a certain event. We call this the scope of
agency. We say that there is evidence of intentional behavior if the scope of
agency is high and the decisions of the agent are close to being optimal for
reaching the event. Our method applies counterfactual reasoning to
automatically generate relevant scenarios that can be analyzed to increase the
confidence of our assessment. In a case study, we show how our method can
distinguish between &apos;intentional&apos; and &apos;accidental&apos; traffic collisions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cordoba_F/0/1/0/all/0/1&quot;&gt;Filip Cano C&amp;#xf3;rdoba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Judson_S/0/1/0/all/0/1&quot;&gt;Samuel Judson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antonopoulos_T/0/1/0/all/0/1&quot;&gt;Timos Antonopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bjorner_K/0/1/0/all/0/1&quot;&gt;Katrine Bj&amp;#xf8;rner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shoemaker_N/0/1/0/all/0/1&quot;&gt;Nicholas Shoemaker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shapiro_S/0/1/0/all/0/1&quot;&gt;Scott J. Shapiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piskac_R/0/1/0/all/0/1&quot;&gt;Ruzica Piskac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konighofer_B/0/1/0/all/0/1&quot;&gt;Bettina K&amp;#xf6;nighofer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01540">
<title>Learning to Prompt in the Classroom to Understand AI Limits: A pilot study. (arXiv:2307.01540v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.01540</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence&apos;s progress holds great promise in assisting society
in addressing pressing societal issues. In particular Large Language Models
(LLM) and the derived chatbots, like ChatGPT, have highly improved the natural
language processing capabilities of AI systems allowing them to process an
unprecedented amount of unstructured data. The consequent hype has also
backfired, raising negative sentiment even after novel AI methods&apos; surprising
contributions. One of the causes, but also an important issue per se, is the
rising and misleading feeling of being able to access and process any form of
knowledge to solve problems in any domain with no effort or previous expertise
in AI or problem domain, disregarding current LLMs limits, such as
hallucinations and reasoning limits. Acknowledging AI fallibility is crucial to
address the impact of dogmatic overconfidence in possibly erroneous suggestions
generated by LLMs. At the same time, it can reduce fear and other negative
attitudes toward AI. AI literacy interventions are necessary that allow the
public to understand such LLM limits and learn how to use them in a more
effective manner, i.e. learning to &quot;prompt&quot;. With this aim, a pilot educational
intervention was performed in a high school with 30 students. It involved (i)
presenting high-level concepts about intelligence, AI, and LLM, (ii) an initial
naive practice with ChatGPT in a non-trivial task, and finally (iii) applying
currently-accepted prompting strategies. Encouraging preliminary results have
been collected such as students reporting a) high appreciation of the activity,
b) improved quality of the interaction with the LLM during the educational
activity, c) decreased negative sentiments toward AI, d) increased
understanding of limitations and specifically We aim to study factors that
impact AI acceptance and to refine and repeat this activity in more controlled
settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theophilou_E/0/1/0/all/0/1&quot;&gt;Emily Theophilou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyuturk_C/0/1/0/all/0/1&quot;&gt;Cansu Koyuturk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yavari_M/0/1/0/all/0/1&quot;&gt;Mona Yavari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bursic_S/0/1/0/all/0/1&quot;&gt;Sathya Bursic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donabauer_G/0/1/0/all/0/1&quot;&gt;Gregor Donabauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Telari_A/0/1/0/all/0/1&quot;&gt;Alessia Telari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Testa_A/0/1/0/all/0/1&quot;&gt;Alessia Testa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boiano_R/0/1/0/all/0/1&quot;&gt;Raffaele Boiano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Leo_D/0/1/0/all/0/1&quot;&gt;Davinia Hernandez-Leo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruskov_M/0/1/0/all/0/1&quot;&gt;Martin Ruskov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taibi_D/0/1/0/all/0/1&quot;&gt;Davide Taibi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabbiadini_A/0/1/0/all/0/1&quot;&gt;Alessandro Gabbiadini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ognibene_D/0/1/0/all/0/1&quot;&gt;Dimitri Ognibene&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01548">
<title>Knowledge Graph for NLG in the context of conversational agents. (arXiv:2307.01548v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.01548</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of knowledge graphs (KGs) enhances the accuracy and comprehensiveness
of the responses provided by a conversational agent. While generating answers
during conversations consists in generating text from these KGs, it is still
regarded as a challenging task that has gained significant attention in recent
years. In this document, we provide a review of different architectures used
for knowledge graph-to-text generation including: Graph Neural Networks, the
Graph Transformer, and linearization with seq2seq models. We discuss the
advantages and limitations of each architecture and conclude that the choice of
architecture will depend on the specific requirements of the task at hand. We
also highlight the importance of considering constraints such as execution time
and model validity, particularly in the context of conversational agents. Based
on these constraints and the availability of labeled data for the domains of
DAVI, we choose to use seq2seq Transformer-based models (PLMs) for the
Knowledge Graph-to-Text Generation task. We aim to refine benchmark datasets of
kg-to-text generation on PLMs and to explore the emotional and multilingual
dimensions in our future work. Overall, this review provides insights into the
different approaches for knowledge graph-to-text generation and outlines future
directions for research in this area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghanem_H/0/1/0/all/0/1&quot;&gt;Hussam Ghanem&lt;/a&gt; (ICB), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atmani_M/0/1/0/all/0/1&quot;&gt;Massinissa Atmani&lt;/a&gt; (ICB), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_C/0/1/0/all/0/1&quot;&gt;Christophe Cruz&lt;/a&gt; (ICB)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01557">
<title>Separated RoadTopoFormer. (arXiv:2307.01557v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01557</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding driving scenarios is crucial to realizing autonomous driving.
Previous works such as map learning and BEV lane detection neglect the
connection relationship between lane instances, and traffic elements detection
tasks usually neglect the relationship with lane lines. To address these
issues, the task is presented which includes 4 sub-tasks, the detection of
traffic elements, the detection of lane centerlines, reasoning connection
relationships among lanes, and reasoning assignment relationships between lanes
and traffic elements. We present Separated RoadTopoFormer to tackle the issues,
which is an end-to-end framework that detects lane centerline and traffic
elements with reasoning relationships among them. We optimize each module
separately to prevent interaction with each other and aggregate them together
with few finetunes. For two detection heads, we adopted a DETR-like
architecture to detect objects, and for the relationship head, we concat two
instance features from front detectors and feed them to the classifier to
obtain relationship probability. Our final submission achieves 0.445 OLS, which
is competitive in both sub-task and combined scores.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1&quot;&gt;Mingjie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuanxian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Ji Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jinzhang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1&quot;&gt;Lu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sirasao_A/0/1/0/all/0/1&quot;&gt;Ashish Sirasao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01558">
<title>Scalable variable selection for two-view learning tasks with projection operators. (arXiv:2307.01558v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01558</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose a novel variable selection method for two-view
settings, or for vector-valued supervised learning problems. Our framework is
able to handle extremely large scale selection tasks, where number of data
samples could be even millions. In a nutshell, our method performs variable
selection by iteratively selecting variables that are highly correlated with
the output variables, but which are not correlated with the previously chosen
variables. To measure the correlation, our method uses the concept of
projection operators and their algebra. With the projection operators the
relationship, correlation, between sets of input and output variables can also
be expressed by kernel functions, thus nonlinear correlation models can be
exploited as well. We experimentally validate our approach, showing on both
synthetic and real data its scalability and the relevance of the selected
features. Keywords: Supervised variable selection, vector-valued learning,
projection-valued measure, reproducing kernel Hilbert space
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szedmak_S/0/1/0/all/0/1&quot;&gt;Sandor Szedmak&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huusari_R/0/1/0/all/0/1&quot;&gt;Riikka Huusari&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Tat Hong Duong Le&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rousu_J/0/1/0/all/0/1&quot;&gt;Juho Rousu&lt;/a&gt; (1) ((1) Department of Computer Science, Aalto University, Espoo, Finland)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01570">
<title>Machine Learning-Based Intrusion Detection: Feature Selection versus Feature Extraction. (arXiv:2307.01570v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.01570</link>
<description rdf:parseType="Literal">&lt;p&gt;Internet of things (IoT) has been playing an important role in many sectors,
such as smart cities, smart agriculture, smart healthcare, and smart
manufacturing. However, IoT devices are highly vulnerable to cyber-attacks,
which may result in security breaches and data leakages. To effectively prevent
these attacks, a variety of machine learning-based network intrusion detection
methods for IoT networks have been developed, which often rely on either
feature extraction or feature selection techniques for reducing the dimension
of input data before being fed into machine learning models. This aims to make
the detection complexity low enough for real-time operations, which is
particularly vital in any intrusion detection systems. This paper provides a
comprehensive comparison between these two feature reduction methods of
intrusion detection in terms of various performance metrics, namely, precision
rate, recall rate, detection accuracy, as well as runtime complexity, in the
presence of the modern UNSW-NB15 dataset as well as both binary and multiclass
classification. For example, in general, the feature selection method not only
provides better detection performance but also lower training and inference
time compared to its feature extraction counterpart, especially when the number
of reduced features K increases. However, the feature extraction method is much
more reliable than its selection counterpart, particularly when K is very
small, such as K = 4. Additionally, feature extraction is less sensitive to
changing the number of reduced features K than feature selection, and this
holds true for both binary and multiclass classifications. Based on this
comparison, we provide a useful guideline for selecting a suitable intrusion
detection type for each specific scenario, as detailed in Tab. 14 at the end of
Section IV.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ngo_V/0/1/0/all/0/1&quot;&gt;Vu-Duc Ngo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1&quot;&gt;Tuan-Cuong Vuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luong_T/0/1/0/all/0/1&quot;&gt;Thien Van Luong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1&quot;&gt;Hung Tran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01577">
<title>Conceptual Cognitive Maps Formation with Neural Successor Networks and Word Embeddings. (arXiv:2307.01577v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.01577</link>
<description rdf:parseType="Literal">&lt;p&gt;The human brain possesses the extraordinary capability to contextualize the
information it receives from our environment. The entorhinal-hippocampal plays
a critical role in this function, as it is deeply engaged in memory processing
and constructing cognitive maps using place and grid cells. Comprehending and
leveraging this ability could significantly augment the field of artificial
intelligence. The multi-scale successor representation serves as a good model
for the functionality of place and grid cells and has already shown promise in
this role. Here, we introduce a model that employs successor representations
and neural networks, along with word embedding vectors, to construct a
cognitive map of three separate concepts. The network adeptly learns two
different scaled maps and situates new information in proximity to related
pre-existing representations. The dispersion of information across the
cognitive map varies according to its scale - either being heavily
concentrated, resulting in the formation of the three concepts, or spread
evenly throughout the map. We suggest that our model could potentially improve
current AI models by providing multi-modal context information to any input,
based on a similarity metric for the input and pre-existing knowledge
representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoewer_P/0/1/0/all/0/1&quot;&gt;Paul Stoewer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schilling_A/0/1/0/all/0/1&quot;&gt;Achim Schilling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1&quot;&gt;Andreas Maier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krauss_P/0/1/0/all/0/1&quot;&gt;Patrick Krauss&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01578">
<title>Optimal and Efficient Binary Questioning for Human-in-the-Loop Annotation. (arXiv:2307.01578v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01578</link>
<description rdf:parseType="Literal">&lt;p&gt;Even though data annotation is extremely important for interpretability,
research and development of artificial intelligence solutions, most research
efforts such as active learning or few-shot learning focus on the sample
efficiency problem. This paper studies the neglected complementary problem of
getting annotated data given a predictor. For the simple binary classification
setting, we present the spectrum ranging from optimal general solutions to
practical efficient methods. The problem is framed as the full annotation of a
binary classification dataset with the minimal number of yes/no questions when
a predictor is available. For the case of general binary questions the solution
is found in coding theory, where the optimal questioning strategy is given by
the Huffman encoding of the possible labelings. However, this approach is
computationally intractable even for small dataset sizes. We propose an
alternative practical solution based on several heuristics and lookahead
minimization of proxy cost functions. The proposed solution is analysed,
compared with optimal solutions and evaluated on several synthetic and
real-world datasets. On these datasets, the method allows a significant
improvement ($23-86\%$) in annotation efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchesoni_Acland_F/0/1/0/all/0/1&quot;&gt;Franco Marchesoni-Acland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morel_J/0/1/0/all/0/1&quot;&gt;Jean-Michel Morel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kherroubi_J/0/1/0/all/0/1&quot;&gt;Josselin Kherroubi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Facciolo_G/0/1/0/all/0/1&quot;&gt;Gabriele Facciolo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01582">
<title>IAdet: Simplest human-in-the-loop object detection. (arXiv:2307.01582v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01582</link>
<description rdf:parseType="Literal">&lt;p&gt;This work proposes a strategy for training models while annotating data named
Intelligent Annotation (IA). IA involves three modules: (1) assisted data
annotation, (2) background model training, and (3) active selection of the next
datapoints. Under this framework, we open-source the IAdet tool, which is
specific for single-class object detection. Additionally, we devise a method
for automatically evaluating such a human-in-the-loop system. For the PASCAL
VOC dataset, the IAdet tool reduces the database annotation time by $25\%$
while providing a trained model for free. These results are obtained for a
deliberately very simple IAdet design. As a consequence, IAdet is susceptible
to multiple easy improvements, paving the way for powerful human-in-the-loop
object detection systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchesoni_Acland_F/0/1/0/all/0/1&quot;&gt;Franco Marchesoni-Acland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Facciolo_G/0/1/0/all/0/1&quot;&gt;Gabriele Facciolo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01593">
<title>Cross-Element Combinatorial Selection for Multi-Element Creative in Display Advertising. (arXiv:2307.01593v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.01593</link>
<description rdf:parseType="Literal">&lt;p&gt;The effectiveness of ad creatives is greatly influenced by their visual
appearance. Advertising platforms can generate ad creatives with different
appearances by combining creative elements provided by advertisers. However,
with the increasing number of ad creative elements, it becomes challenging to
select a suitable combination from the countless possibilities. The industry&apos;s
mainstream approach is to select individual creative elements independently,
which often overlooks the importance of interaction between creative elements
during the modeling process. In response, this paper proposes a Cross-Element
Combinatorial Selection framework for multiple creative elements, termed CECS.
In the encoder process, a cross-element interaction is adopted to dynamically
adjust the expression of a single creative element based on the current
candidate creatives. In the decoder process, the creative combination problem
is transformed into a cascade selection problem of multiple creative elements.
A pointer mechanism with a cascade design is used to model the associations
among candidates. Comprehensive experiments on real-world datasets show that
CECS achieved the SOTA score on offline metrics. Moreover, the CECS algorithm
has been deployed in our industrial application, resulting in a significant
6.02% CTR and 10.37% GMV lift, which is beneficial to the business.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Ping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jian Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yongkang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pengye Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xingxing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01595">
<title>Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases. (arXiv:2307.01595v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01595</link>
<description rdf:parseType="Literal">&lt;p&gt;As the representation capability of Pre-trained Language Models (PLMs)
improve, there is growing concern that they will inherit social biases from
unprocessed corpora. Most previous debiasing techniques used Counterfactual
Data Augmentation (CDA) to balance the training corpus. However, CDA slightly
modifies the original corpus, limiting the representation distance between
different demographic groups to a narrow range. As a result, the debiasing
model easily fits the differences between counterfactual pairs, which affects
its debiasing performance with limited text resources. In this paper, we
propose an adversarial training-inspired two-stage debiasing model using
Contrastive learning with Continuous Prompt Augmentation (named CCPA) to
mitigate social biases in PLMs&apos; encoding. In the first stage, we propose a data
augmentation method based on continuous prompt tuning to push farther the
representation distance between sample pairs along different demographic
groups. In the second stage, we utilize contrastive learning to pull closer the
representation distance between the augmented sample pairs and then fine-tune
PLMs&apos; parameters to get debiased encoding. Our approach guides the model to
achieve stronger debiasing performance by adding difficulty to the training
process. Extensive experiments show that CCPA outperforms baselines in terms of
debiasing performance. Meanwhile, experimental results on the GLUE benchmark
show that CCPA retains the language modeling capability of PLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingji Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Mengnan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Ying Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01597">
<title>Bridge the Performance Gap in Peak-hour Series Forecasting: The Seq2Peak Framework. (arXiv:2307.01597v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01597</link>
<description rdf:parseType="Literal">&lt;p&gt;Peak-Hour Series Forecasting (PHSF) is a crucial yet underexplored task in
various domains. While state-of-the-art deep learning models excel in regular
Time Series Forecasting (TSF), they struggle to achieve comparable results in
PHSF. This can be attributed to the challenges posed by the high degree of
non-stationarity in peak-hour series, which makes direct forecasting more
difficult than standard TSF. Additionally, manually extracting the maximum
value from regular forecasting results leads to suboptimal performance due to
models minimizing the mean deficit. To address these issues, this paper
presents Seq2Peak, a novel framework designed specifically for PHSF tasks,
bridging the performance gap observed in TSF models. Seq2Peak offers two key
components: the CyclicNorm pipeline to mitigate the non-stationarity issue, and
a simple yet effective trainable-parameter-free peak-hour decoder with a hybrid
loss function that utilizes both the original series and peak-hour series as
supervised signals. Extensive experimentation on publicly available time series
datasets demonstrates the effectiveness of the proposed framework, yielding a
remarkable average relative improvement of 37.7\% across four real-world
datasets for both transformer- and non-transformer-based TSF models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jingyuan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Heling Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yuantao Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01610">
<title>Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction. (arXiv:2307.01610v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.01610</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) models are vulnerable to membership inference attacks
(MIAs), which determine whether a given input is used for training the target
model. While there have been many efforts to mitigate MIAs, they often suffer
from limited privacy protection, large accuracy drop, and/or requiring
additional data that may be difficult to acquire. This work proposes a defense
technique, HAMP that can achieve both strong membership privacy and high
accuracy, without requiring extra data. To mitigate MIAs in different forms, we
observe that they can be unified as they all exploit the ML model&apos;s
overconfidence in predicting training samples through different proxies. This
motivates our design to enforce less confident prediction by the model, hence
forcing the model to behave similarly on the training and testing samples. HAMP
consists of a novel training framework with high-entropy soft labels and an
entropy-based regularizer to constrain the model&apos;s prediction while still
achieving high accuracy. To further reduce privacy risk, HAMP uniformly
modifies all the prediction outputs to become low-confidence outputs while
preserving the accuracy, which effectively obscures the differences between the
prediction on members and non-members. We conduct extensive evaluation on five
benchmark datasets, and show that HAMP provides consistently high accuracy and
strong membership privacy. Our comparison with seven state-of-the-art defenses
shows that HAMP achieves a superior privacy-utility trade off than those
techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zitao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pattabiraman_K/0/1/0/all/0/1&quot;&gt;Karthik Pattabiraman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01616">
<title>SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting. (arXiv:2307.01616v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01616</link>
<description rdf:parseType="Literal">&lt;p&gt;Multivariate time series forecasting plays a critical role in diverse
domains. While recent advancements in deep learning methods, especially
Transformers, have shown promise, there remains a gap in addressing the
significance of inter-series dependencies. This paper introduces SageFormer, a
Series-aware Graph-enhanced Transformer model designed to effectively capture
and model dependencies between series using graph structures. SageFormer
tackles two key challenges: effectively representing diverse temporal patterns
across series and mitigating redundant information among series. Importantly,
the proposed series-aware framework seamlessly integrates with existing
Transformer-based models, augmenting their ability to model inter-series
dependencies. Through extensive experiments on real-world and synthetic
datasets, we showcase the superior performance of SageFormer compared to
previous state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yuantao Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01637">
<title>Random Walk on Multiple Networks. (arXiv:2307.01637v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2307.01637</link>
<description rdf:parseType="Literal">&lt;p&gt;Random Walk is a basic algorithm to explore the structure of networks, which
can be used in many tasks, such as local community detection and network
embedding. Existing random walk methods are based on single networks that
contain limited information. In contrast, real data often contain entities with
different types or/and from different sources, which are comprehensive and can
be better modeled by multiple networks. To take advantage of rich information
in multiple networks and make better inferences on entities, in this study, we
propose random walk on multiple networks, RWM. RWM is flexible and supports
both multiplex networks and general multiple networks, which may form
many-to-many node mappings between networks. RWM sends a random walker on each
network to obtain the local proximity (i.e., node visiting probabilities)
w.r.t. the starting nodes. Walkers with similar visiting probabilities
reinforce each other. We theoretically analyze the convergence properties of
RWM. Two approximation methods with theoretical performance guarantees are
proposed for efficient computation. We apply RWM in link prediction, network
embedding, and local community detection. Comprehensive experiments conducted
on both synthetic and real-world datasets demonstrate the effectiveness and
efficiency of RWM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1&quot;&gt;Dongsheng Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1&quot;&gt;Yuchen Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yaowei Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xiong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huan_J/0/1/0/all/0/1&quot;&gt;Jun Huan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01639">
<title>Heuristic Algorithms for the Approximation of Mutual Coherence. (arXiv:2307.01639v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.01639</link>
<description rdf:parseType="Literal">&lt;p&gt;Mutual coherence is a measure of similarity between two opinions. Although
the notion comes from philosophy, it is essential for a wide range of
technologies, e.g., the Wahl-O-Mat system. In Germany, this system helps voters
to find candidates that are the closest to their political preferences. The
exact computation of mutual coherence is highly time-consuming due to the
iteration over all subsets of an opinion. Moreover, for every subset, an
instance of the SAT model counting problem has to be solved which is known to
be a hard problem in computer science. This work is the first study to
accelerate this computation. We model the distribution of the so-called
confirmation values as a mixture of three Gaussians and present efficient
heuristics to estimate its model parameters. The mutual coherence is then
approximated with the expected value of the distribution. Some of the presented
algorithms are fully polynomial-time, others only require solving a small
number of instances of the SAT model counting problem. The average squared
error of our best algorithm lies below 0.0035 which is insignificant if the
efficiency is taken into account. Furthermore, the accuracy is precise enough
to be used in Wahl-O-Mat-like systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Betz_G/0/1/0/all/0/1&quot;&gt;Gregor Betz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chekan_V/0/1/0/all/0/1&quot;&gt;Vera Chekan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mchedlidze_T/0/1/0/all/0/1&quot;&gt;Tamara Mchedlidze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01644">
<title>Insert-expansions for Tool-enabled Conversational Agents. (arXiv:2307.01644v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.01644</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper delves into an advanced implementation of
Chain-of-Thought-Prompting in Large Language Models, focusing on the use of
tools (or &quot;plug-ins&quot;) within the explicit reasoning paths generated by this
prompting method. We find that tool-enabled conversational agents often become
sidetracked, as additional context from tools like search engines or
calculators diverts from original user intents. To address this, we explore a
concept wherein the user becomes the tool, providing necessary details and
refining their requests. Through Conversation Analysis, we characterize this
interaction as insert-expansion - an intermediary conversation designed to
facilitate the preferred response. We explore possibilities arising from this
&apos;user-as-a-tool&apos; approach in two empirical studies using direct comparison, and
find benefits in the recommendation domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldi_A/0/1/0/all/0/1&quot;&gt;Andreas G&amp;#xf6;ldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rietsche_R/0/1/0/all/0/1&quot;&gt;Roman Rietsche&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01646">
<title>SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. (arXiv:2307.01646v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01646</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models based on permutation-equivariant networks can learn
permutation-invariant distributions for graph data. However, in comparison to
their non-invariant counterparts, we have found that these invariant models
encounter greater learning challenges since 1) their effective target
distributions exhibit more modes; 2) their optimal one-step denoising scores
are the score functions of Gaussian mixtures with more components. Motivated by
this analysis, we propose a non-invariant diffusion model, called
$\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message
passing network and utilizes shifted window based self-attention inspired by
SwinTransformers. Further, through systematic ablations, we identify several
critical training and sampling techniques that significantly improve the sample
quality of graph generation. At last, we introduce a simple post-processing
trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably
converts any graph generative model to a permutation-invariant one. Extensive
experiments on synthetic and real-world protein and molecule datasets show that
our SwinGNN achieves state-of-the-art performances. Our code is released at
https://github.com/qiyan98/SwinGNN .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1&quot;&gt;Qi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1&quot;&gt;Zhengyang Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1&quot;&gt;Renjie Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lele Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01676">
<title>RaidEnv: Exploring New Challenges in Automated Content Balancing for Boss Raid Games. (arXiv:2307.01676v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.01676</link>
<description rdf:parseType="Literal">&lt;p&gt;The balance of game content significantly impacts the gaming experience.
Unbalanced game content diminishes engagement or increases frustration because
of repetitive failure. Although game designers intend to adjust the difficulty
of game content, this is a repetitive, labor-intensive, and challenging
process, especially for commercial-level games with extensive content. To
address this issue, the game research community has explored automated game
balancing using artificial intelligence (AI) techniques. However, previous
studies have focused on limited game content and did not consider the
importance of the generalization ability of playtesting agents when
encountering content changes. In this study, we propose RaidEnv, a new game
simulator that includes diverse and customizable content for the boss raid
scenario in MMORPG games. Additionally, we design two benchmarks for the boss
raid scenario that can aid in the practical application of game AI. These
benchmarks address two open problems in automatic content balancing, and we
introduce two evaluation metrics to provide guidance for AI in automatic
content balancing. This novel game research platform expands the frontiers of
automatic game balancing problems and offers a framework within a realistic
game production pipeline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1&quot;&gt;Hyeon-Chang Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baek_I/0/1/0/all/0/1&quot;&gt;In-Chang Baek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_C/0/1/0/all/0/1&quot;&gt;Cheong-mok Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1&quot;&gt;Taehwa Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_W/0/1/0/all/0/1&quot;&gt;Wonsang You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_T/0/1/0/all/0/1&quot;&gt;Taegwan Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1&quot;&gt;Hoyun Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noh_J/0/1/0/all/0/1&quot;&gt;Jinha Noh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Seungwon Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kyung-Joong Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01683">
<title>Learning Discrete Weights and Activations Using the Local Reparameterization Trick. (arXiv:2307.01683v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01683</link>
<description rdf:parseType="Literal">&lt;p&gt;In computer vision and machine learning, a crucial challenge is to lower the
computation and memory demands for neural network inference. A commonplace
solution to address this challenge is through the use of binarization. By
binarizing the network weights and activations, one can significantly reduce
computational complexity by substituting the computationally expensive floating
operations with faster bitwise operations. This leads to a more efficient
neural network inference that can be deployed on low-resource devices. In this
work, we extend previous approaches that trained networks with discrete weights
using the local reparameterization trick to also allow for discrete
activations. The original approach optimized a distribution over the discrete
weights and uses the central limit theorem to approximate the pre-activation
with a continuous Gaussian distribution. Here we show that the probabilistic
modeling can also allow effective training of networks with discrete activation
as well. This further reduces runtime and memory footprint at inference time
with state-of-the-art results for networks with binary activations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berger_G/0/1/0/all/0/1&quot;&gt;Guy Berger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navon_A/0/1/0/all/0/1&quot;&gt;Aviv Navon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1&quot;&gt;Ethan Fetaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01684">
<title>Serving Graph Neural Networks With Distributed Fog Servers For Smart IoT Services. (arXiv:2307.01684v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2307.01684</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have gained growing interest in miscellaneous
applications owing to their outstanding ability in extracting latent
representation on graph structures. To render GNN-based service for IoT-driven
smart applications, traditional model serving paradigms usually resort to the
cloud by fully uploading geo-distributed input data to remote datacenters.
However, our empirical measurements reveal the significant communication
overhead of such cloud-based serving and highlight the profound potential in
applying the emerging fog computing. To maximize the architectural benefits
brought by fog computing, in this paper, we present Fograph, a novel
distributed real-time GNN inference framework that leverages diverse and
dynamic resources of multiple fog nodes in proximity to IoT data sources. By
introducing heterogeneity-aware execution planning and GNN-specific compression
techniques, Fograph tailors its design to well accommodate the unique
characteristics of GNN serving in fog environments. Prototype-based evaluation
and case study demonstrate that Fograph significantly outperforms the
state-of-the-art cloud serving and fog deployment by up to 5.39x execution
speedup and 6.84x throughput improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1&quot;&gt;Liekang Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Peng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1&quot;&gt;Ke Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoxi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhi Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01689">
<title>Online Learning and Solving Infinite Games with an ERM Oracle. (arXiv:2307.01689v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01689</link>
<description rdf:parseType="Literal">&lt;p&gt;While ERM suffices to attain near-optimal generalization error in the
stochastic learning setting, this is not known to be the case in the online
learning setting, where algorithms for general concept classes rely on
computationally inefficient oracles such as the Standard Optimal Algorithm
(SOA). In this work, we propose an algorithm for online binary classification
setting that relies solely on ERM oracle calls, and show that it has finite
regret in the realizable setting and sublinearly growing regret in the agnostic
setting. We bound the regret in terms of the Littlestone and threshold
dimensions of the underlying concept class.
&lt;/p&gt;
&lt;p&gt;We obtain similar results for nonparametric games, where the ERM oracle can
be interpreted as a best response oracle, finding the best response of a player
to a given history of play of the other players. In this setting, we provide
learning algorithms that only rely on best response oracles and converge to
approximate-minimax equilibria in two-player zero-sum games and approximate
coarse correlated equilibria in multi-player general-sum games, as long as the
game has a bounded fat-threshold dimension. Our algorithms apply to both
binary-valued and real-valued games and can be viewed as providing
justification for the wide use of double oracle and multiple oracle algorithms
in the practice of solving large games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assos_A/0/1/0/all/0/1&quot;&gt;Angelos Assos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Attias_I/0/1/0/all/0/1&quot;&gt;Idan Attias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dagan_Y/0/1/0/all/0/1&quot;&gt;Yuval Dagan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daskalakis_C/0/1/0/all/0/1&quot;&gt;Constantinos Daskalakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fishelson_M/0/1/0/all/0/1&quot;&gt;Maxwell Fishelson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01701">
<title>Synthetic is all you need: removing the auxiliary data assumption for membership inference attacks against synthetic data. (arXiv:2307.01701v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.01701</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthetic data is emerging as the most promising solution to share
individual-level data while safeguarding privacy. Membership inference attacks
(MIAs), based on shadow modeling, have become the standard to evaluate the
privacy of synthetic data. These attacks, however, currently assume the
attacker to have access to an auxiliary dataset sampled from a similar
distribution as the training dataset. This often is a very strong assumption
that would make an attack unlikely to happen in practice. We here show how this
assumption can be removed and how MIAs can be performed using only the
synthetic data. More specifically, in three different attack scenarios using
only synthetic data, our results demonstrate that MIAs are still successful,
across two real-world datasets and two synthetic data generators. These results
show how the strong hypothesis made when auditing synthetic data releases -
access to an auxiliary dataset - can be relaxed to perform an actual attack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guepin_F/0/1/0/all/0/1&quot;&gt;Florent Gu&amp;#xe9;pin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meeus_M/0/1/0/all/0/1&quot;&gt;Matthieu Meeus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cretu_A/0/1/0/all/0/1&quot;&gt;Ana-Maria Cretu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montjoye_Y/0/1/0/all/0/1&quot;&gt;Yves-Alexandre de Montjoye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01708">
<title>Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning. (arXiv:2307.01708v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01708</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of learning models for risk-sensitive reinforcement
learning. We theoretically demonstrate that proper value equivalence, a method
of learning models which can be used to plan optimally in the risk-neutral
setting, is not sufficient to plan optimally in the risk-sensitive setting. We
leverage distributional reinforcement learning to introduce two new notions of
model equivalence, one which is general and can be used to plan for any risk
measure, but is intractable; and a practical variation which allows one to
choose which risk measures they may plan optimally for. We demonstrate how our
framework can be used to augment any model-free risk-sensitive algorithm, and
provide both tabular and large-scale experiments to demonstrate its ability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kastner_T/0/1/0/all/0/1&quot;&gt;Tyler Kastner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erdogdu_M/0/1/0/all/0/1&quot;&gt;Murat A. Erdogdu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farahmand_A/0/1/0/all/0/1&quot;&gt;Amir-massoud Farahmand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01717">
<title>On the Constrained Time-Series Generation Problem. (arXiv:2307.01717v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01717</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthetic time series are often used in practical applications to augment the
historical time series dataset for better performance of machine learning
algorithms, amplify the occurrence of rare events, and also create
counterfactual scenarios described by the time series.
Distributional-similarity (which we refer to as realism) as well as the
satisfaction of certain numerical constraints are common requirements in
counterfactual time series scenario generation requests. For instance, the US
Federal Reserve publishes synthetic market stress scenarios given by the
constrained time series for financial institutions to assess their performance
in hypothetical recessions. Existing approaches for generating constrained time
series usually penalize training loss to enforce constraints, and reject
non-conforming samples. However, these approaches would require re-training if
we change constraints, and rejection sampling can be computationally expensive,
or impractical for complex constraints. In this paper, we propose a novel set
of methods to tackle the constrained time series generation problem and provide
efficient sampling while ensuring the realism of generated time series. In
particular, we frame the problem using a constrained optimization framework and
then we propose a set of generative methods including ``GuidedDiffTime&apos;&apos;, a
guided diffusion model to generate realistic time series. Empirically, we
evaluate our work on several datasets for financial and energy data, where
incorporating constraints is critical. We show that our approaches outperform
existing work both qualitatively and quantitatively. Most importantly, we show
that our ``GuidedDiffTime&apos;&apos; model is the only solution where re-training is not
necessary for new constraints, resulting in a significant carbon footprint
reduction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coletta_A/0/1/0/all/0/1&quot;&gt;Andrea Coletta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalakrishan_S/0/1/0/all/0/1&quot;&gt;Sriram Gopalakrishan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borrajo_D/0/1/0/all/0/1&quot;&gt;Daniel Borrajo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vyetrenko_S/0/1/0/all/0/1&quot;&gt;Svitlana Vyetrenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01719">
<title>MOPO-LSI: A User Guide. (arXiv:2307.01719v1 [q-fin.PM])</title>
<link>http://arxiv.org/abs/2307.01719</link>
<description rdf:parseType="Literal">&lt;p&gt;MOPO-LSI is an open-source Multi-Objective Portfolio Optimization Library for
Sustainable Investments. This document provides a user guide for MOPO-LSI
version 1.0, including problem setup, workflow and the hyper-parameters in
configurations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Shukla_K/0/1/0/all/0/1&quot;&gt;Kumar Neelotpal Shukla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jasmine Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+David/0/1/0/all/0/1&quot;&gt;David&lt;/a&gt; (Xuejun) &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Wang/0/1/0/all/0/1&quot;&gt;Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+OLeary_M/0/1/0/all/0/1&quot;&gt;Michael O&amp;#x27;Leary&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01778">
<title>Physically Realizable Natural-Looking Clothing Textures Evade Person Detectors via 3D Modeling. (arXiv:2307.01778v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01778</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent works have proposed to craft adversarial clothes for evading person
detectors, while they are either only effective at limited viewing angles or
very conspicuous to humans. We aim to craft adversarial texture for clothes
based on 3D modeling, an idea that has been used to craft rigid adversarial
objects such as a 3D-printed turtle. Unlike rigid objects, humans and clothes
are non-rigid, leading to difficulties in physical realization. In order to
craft natural-looking adversarial clothes that can evade person detectors at
multiple viewing angles, we propose adversarial camouflage textures (AdvCaT)
that resemble one kind of the typical textures of daily clothes, camouflage
textures. We leverage the Voronoi diagram and Gumbel-softmax trick to
parameterize the camouflage textures and optimize the parameters via 3D
modeling. Moreover, we propose an efficient augmentation pipeline on 3D meshes
combining topologically plausible projection (TopoProj) and Thin Plate Spline
(TPS) to narrow the gap between digital and real-world objects. We printed the
developed 3D texture pieces on fabric materials and tailored them into T-shirts
and trousers. Experiments show high attack success rates of these clothes
against multiple detectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhanhao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1&quot;&gt;Wenda Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaopei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaolin Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01782">
<title>GHOST: A Graph Neural Network Accelerator using Silicon Photonics. (arXiv:2307.01782v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2307.01782</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) have emerged as a powerful approach for
modelling and learning from graph-structured data. Multiple fields have since
benefitted enormously from the capabilities of GNNs, such as recommendation
systems, social network analysis, drug discovery, and robotics. However,
accelerating and efficiently processing GNNs require a unique approach that
goes beyond conventional artificial neural network accelerators, due to the
substantial computational and memory requirements of GNNs. The slowdown of
scaling in CMOS platforms also motivates a search for alternative
implementation substrates. In this paper, we present GHOST, the first
silicon-photonic hardware accelerator for GNNs. GHOST efficiently alleviates
the costs associated with both vertex-centric and edge-centric operations. It
implements separately the three main stages involved in running GNNs in the
optical domain, allowing it to be used for the inference of various widely used
GNN models and architectures, such as graph convolution networks and graph
attention networks. Our simulation studies indicate that GHOST exhibits at
least 10.2x better throughput and 3.8x better energy efficiency when compared
to GPU, TPU, CPU and multiple state-of-the-art GNN hardware accelerators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afifi_S/0/1/0/all/0/1&quot;&gt;Salma Afifi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunny_F/0/1/0/all/0/1&quot;&gt;Febin Sunny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafiee_A/0/1/0/all/0/1&quot;&gt;Amin Shafiee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikdast_M/0/1/0/all/0/1&quot;&gt;Mahdi Nikdast&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pasricha_S/0/1/0/all/0/1&quot;&gt;Sudeep Pasricha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01784">
<title>The Inner Sentiments of a Thought. (arXiv:2307.01784v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01784</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based large-scale language models (LLMs) are able to generate
highly realistic text. They are duly able to express, and at least implicitly
represent, a wide range of sentiments and color, from the obvious, such as
valence and arousal to the subtle, such as determination and admiration. We
provide a first exploration of these representations and how they can be used
for understanding the inner sentimental workings of single sentences. We train
predictors of the quantiles of the distributions of final sentiments of
sentences from the hidden representations of an LLM applied to prefixes of
increasing lengths. After showing that predictors of distributions of valence,
determination, admiration, anxiety and annoyance are well calibrated, we
provide examples of using these predictors for analyzing sentences,
illustrating, for instance, how even ordinary conjunctions (e.g., &quot;but&quot;) can
dramatically alter the emotional trajectory of an utterance. We then show how
to exploit the distributional predictions to generate sentences with sentiments
in the tails of distributions. We discuss the implications of our results for
the inner workings of thoughts, for instance for psychiatric dysfunction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1&quot;&gt;Chris Gagne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dayan_P/0/1/0/all/0/1&quot;&gt;Peter Dayan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01806">
<title>DeepFlorist: Rethinking Deep Neural Networks and Ensemble Learning as A Meta-Classifier For Object Classification. (arXiv:2307.01806v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01806</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel learning paradigm called &quot;DeepFlorist&quot; for
flower classification using ensemble learning as a meta-classifier. DeepFlorist
combines the power of deep learning with the robustness of ensemble methods to
achieve accurate and reliable flower classification results. The proposed
network architecture leverages a combination of dense convolutional and
convolutional neural networks (DCNNs and CNNs) to extract high-level features
from flower images, followed by a fully connected layer for classification. To
enhance the performance and generalization of DeepFlorist, an ensemble learning
approach is employed, incorporating multiple diverse models to improve the
classification accuracy. Experimental results on benchmark flower datasets
demonstrate the effectiveness of DeepFlorist, outperforming state-of-the-art
methods in terms of accuracy and robustness. The proposed framework holds
significant potential for automated flower recognition systems in real-world
applications, enabling advancements in plant taxonomy, conservation efforts,
and ecological studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khadangi_A/0/1/0/all/0/1&quot;&gt;Afshin Khadangi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01817">
<title>Human Trajectory Forecasting with Explainable Behavioral Uncertainty. (arXiv:2307.01817v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01817</link>
<description rdf:parseType="Literal">&lt;p&gt;Human trajectory forecasting helps to understand and predict human behaviors,
enabling applications from social robots to self-driving cars, and therefore
has been heavily investigated. Most existing methods can be divided into
model-free and model-based methods. Model-free methods offer superior
prediction accuracy but lack explainability, while model-based methods provide
explainability but cannot predict well. Combining both methodologies, we
propose a new Bayesian Neural Stochastic Differential Equation model BNSP-SFM,
where a behavior SDE model is combined with Bayesian neural networks (BNNs).
While the NNs provide superior predictive power, the SDE offers strong
explainability with quantifiable uncertainty in behavior and observation. We
show that BNSP-SFM achieves up to a 50% improvement in prediction accuracy,
compared with 11 state-of-the-art methods. BNSP-SFM also generalizes better to
drastically different scenes with different environments and crowd densities (~
20 times higher than the testing data). Finally, BNSP-SFM can provide
predictions with confidence to better explain potential causes of behaviors.
The code will be released upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_J/0/1/0/all/0/1&quot;&gt;Jiangbei Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1&quot;&gt;Dinesh Manocha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;He Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01831">
<title>DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation. (arXiv:2307.01831v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01831</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent Diffusion Transformers (e.g., DiT) have demonstrated their powerful
effectiveness in generating high-quality 2D images. However, it is still being
determined whether the Transformer architecture performs equally well in 3D
shape generation, as previous 3D diffusion methods mostly adopted the U-Net
architecture. To bridge this gap, we propose a novel Diffusion Transformer for
3D shape generation, namely DiT-3D, which can directly operate the denoising
process on voxelized point clouds using plain Transformers. Compared to
existing U-Net approaches, our DiT-3D is more scalable in model size and
produces much higher quality generations. Specifically, the DiT-3D adopts the
design philosophy of DiT but modifies it by incorporating 3D positional and
patch embeddings to adaptively aggregate input from voxelized point clouds. To
reduce the computational cost of self-attention in 3D shape generation, we
incorporate 3D window attention into Transformer blocks, as the increased 3D
token length resulting from the additional dimension of voxels can lead to high
computation. Finally, linear and devoxelization layers are used to predict the
denoised point clouds. In addition, our transformer architecture supports
efficient fine-tuning from 2D to 3D, where the pre-trained DiT-2D checkpoint on
ImageNet can significantly improve DiT-3D on ShapeNet. Experimental results on
the ShapeNet dataset demonstrate that the proposed DiT-3D achieves
state-of-the-art performance in high-fidelity and diverse 3D point cloud
generation. In particular, our DiT-3D decreases the 1-Nearest Neighbor Accuracy
of the state-of-the-art method by 4.59 and increases the Coverage metric by
3.51 when evaluated on Chamfer Distance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1&quot;&gt;Shentong Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1&quot;&gt;Ruihang Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1&quot;&gt;Lewei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1&quot;&gt;Lanqing Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1&quot;&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2009.07888">
<title>Transfer Learning in Deep Reinforcement Learning: A Survey. (arXiv:2009.07888v7 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2009.07888</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning is a learning paradigm for solving sequential
decision-making problems. Recent years have witnessed remarkable progress in
reinforcement learning upon the fast development of deep neural networks. Along
with the promising prospects of reinforcement learning in numerous domains such
as robotics and game-playing, transfer learning has arisen to tackle various
challenges faced by reinforcement learning, by transferring knowledge from
external expertise to facilitate the efficiency and effectiveness of the
learning process. In this survey, we systematically investigate the recent
progress of transfer learning approaches in the context of deep reinforcement
learning. Specifically, we provide a framework for categorizing the
state-of-the-art transfer learning approaches, under which we analyze their
goals, methodologies, compatible reinforcement learning backbones, and
practical applications. We also draw connections between transfer learning and
other relevant topics from the reinforcement learning perspective and explore
their potential challenges that await future research progress.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhuangdi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kaixiang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Anil K. Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiayu Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2011.14956">
<title>Handling Noisy Labels via One-Step Abductive Multi-Target Learning and Its Application to Helicobacter Pylori Segmentation. (arXiv:2011.14956v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2011.14956</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning from noisy labels is an important concern because of the lack of
accurate ground-truth labels in plenty of real-world scenarios. In practice,
various approaches for this concern first make some corrections corresponding
to potentially noisy-labeled instances, and then update predictive model with
information of the made corrections. However, in specific areas, such as
medical histopathology whole slide image analysis (MHWSIA), it is often
difficult or even impossible for experts to manually achieve the noisy-free
ground-truth labels which leads to labels with complex noise. This situation
raises two more difficult problems: 1) the methodology of approaches making
corrections corresponding to potentially noisy-labeled instances has
limitations due to the complex noise existing in labels; and 2) the appropriate
evaluation strategy for validation/testing is unclear because of the great
difficulty in collecting the noisy-free ground-truth labels. In this paper, we
focus on alleviating these two problems. For the problem 1), we present
one-step abductive multi-target learning (OSAMTL) that imposes a one-step
logical reasoning upon machine learning via a multi-target learning procedure
to constrain the predictions of the learning model to be subject to our prior
knowledge about the true target. For the problem 2), we propose a logical
assessment formula (LAF) that evaluates the logical rationality of the outputs
of an approach by estimating the consistencies between the predictions of the
learning model and the logical facts narrated from the results of the one-step
logical reasoning of OSAMTL. Applying OSAMTL and LAF to the Helicobacter pylori
(H. pylori) segmentation task in MHWSIA, we show that OSAMTL is able to enable
the machine learning model achieving logically more rational predictions, which
is beyond various state-of-the-art approaches in handling complex noisy labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yongquan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yiming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jiayi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zhongxi Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2105.14452">
<title>A unified logical framework for explanations in classifier systems. (arXiv:2105.14452v7 [cs.LO] UPDATED)</title>
<link>http://arxiv.org/abs/2105.14452</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed a renewed interest in Boolean function in
explaining binary classifiers in the field of explainable AI (XAI). The
standard approach of Boolean function is propositional logic. We present a
modal language of a ceteris paribus nature which supports reasoning about
binary input classifiers and their properties. We study a family of classifier
models, axiomatize it as two proof systems regarding the cardinality of the
language and show completeness of our axiomatics. Moreover, we prove that
satisfiability checking problem for our modal language is NEXPTIME-complete in
the infinite-variable case, while it becomes polynomial in the finite-variable
case. We furthermore identify an interesting NP fragment of our language in the
infinite-variable case. We leverage the language to formalize counterfactual
conditional as well as a variety of notions of explanation including abductive,
contrastive and counterfactual explanations, and biases. Finally, we present
two extensions of our language: a dynamic extension by the notion of assignment
enabling classifier change and an epistemic extension in which the classifier&apos;s
uncertainty about the actual input can be represented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinghan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorini_E/0/1/0/all/0/1&quot;&gt;Emiliano Lorini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.09973">
<title>The Curse of Passive Data Collection in Batch Reinforcement Learning. (arXiv:2106.09973v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2106.09973</link>
<description rdf:parseType="Literal">&lt;p&gt;In high stake applications, active experimentation may be considered too
risky and thus data are often collected passively. While in simple cases, such
as in bandits, passive and active data collection are similarly effective, the
price of passive sampling can be much higher when collecting data from a system
with controlled states. The main focus of the current paper is the
characterization of this price. For example, when learning in episodic finite
state-action Markov decision processes (MDPs) with $\mathrm{S}$ states and
$\mathrm{A}$ actions, we show that even with the best (but passively chosen)
logging policy, $\Omega(\mathrm{A}^{\min(\mathrm{S}-1, H)}/\varepsilon^2)$
episodes are necessary (and sufficient) to obtain an $\epsilon$-optimal policy,
where $H$ is the length of episodes. Note that this shows that the sample
complexity blows up exponentially compared to the case of active data
collection, a result which is not unexpected, but, as far as we know, have not
been published beforehand and perhaps the form of the exact expression is a
little surprising. We also extend these results in various directions, such as
other criteria or learning in the presence of function approximation, with
similar conclusions. A remarkable feature of our result is the sharp
characterization of the exponent that appears, which is critical for
understanding what makes passive learning hard.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chenjun Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;Ilbin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1&quot;&gt;Dale Schuurmans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1&quot;&gt;Csaba Szepesvari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2108.06161">
<title>Reinforcement Learning for Robot Navigation with Adaptive Forward Simulation Time (AFST) in a Semi-Markov Model. (arXiv:2108.06161v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2108.06161</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning (DRL) algorithms have proven effective in robot
navigation, especially in unknown environments, by directly mapping perception
inputs into robot control commands. However, most existing methods ignore the
local minimum problem in navigation and thereby cannot handle complex unknown
environments. In this paper, we propose the first DRL-based navigation method
modeled by a semi-Markov decision process (SMDP) with continuous action space,
named Adaptive Forward Simulation Time (AFST), to overcome this problem.
Specifically, we reduce the dimensions of the action space and improve the
distributed proximal policy optimization (DPPO) algorithm for the specified
SMDP problem by modifying its GAE to better estimate the policy gradient in
SMDPs. Experiments in various unknown environments demonstrate the
effectiveness of AFST.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu&amp;#x27;an Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1&quot;&gt;Ruosong Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1&quot;&gt;Ziyang Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongjian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guangda Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jie Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jianmin Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanyong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.08645">
<title>Learning Interpretable Models Through Multi-Objective Neural Architecture Search. (arXiv:2112.08645v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2112.08645</link>
<description rdf:parseType="Literal">&lt;p&gt;Monumental advances in deep learning have led to unprecedented achievements
across various domains. While the performance of deep neural networks is
indubitable, the architectural design and interpretability of such models are
nontrivial. Research has been introduced to automate the design of neural
network architectures through neural architecture search (NAS). Recent progress
has made these methods more pragmatic by exploiting distributed computation and
novel optimization algorithms. However, there is little work in optimizing
architectures for interpretability. To this end, we propose a multi-objective
distributed NAS framework that optimizes for both task performance and
&quot;introspectability,&quot; a surrogate metric for aspects of interpretability. We
leverage the non-dominated sorting genetic algorithm (NSGA-II) and explainable
AI (XAI) techniques to reward architectures that can be better comprehended by
domain experts. The framework is evaluated on several image classification
datasets. We demonstrate that jointly optimizing for task error and
introspectability leads to more disentangled and debuggable architectures that
perform within tolerable error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carmichael_Z/0/1/0/all/0/1&quot;&gt;Zachariah Carmichael&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1&quot;&gt;Tim Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobs_S/0/1/0/all/0/1&quot;&gt;Sam Ade Jacobs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.09826">
<title>Continual Learning Beyond a Single Model. (arXiv:2202.09826v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.09826</link>
<description rdf:parseType="Literal">&lt;p&gt;A growing body of research in continual learning focuses on the catastrophic
forgetting problem. While many attempts have been made to alleviate this
problem, the majority of the methods assume a single model in the continual
learning setup. In this work, we question this assumption and show that
employing ensemble models can be a simple yet effective method to improve
continual performance. However, ensembles&apos; training and inference costs can
increase significantly as the number of models grows. Motivated by this
limitation, we study different ensemble models to understand their benefits and
drawbacks in continual learning scenarios. Finally, to overcome the high
compute cost of ensembles, we leverage recent advances in neural network
subspace to propose a computationally cheap algorithm with similar runtime to a
single model yet enjoying the performance benefits of ensembles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doan_T/0/1/0/all/0/1&quot;&gt;Thang Doan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirzadeh_S/0/1/0/all/0/1&quot;&gt;Seyed Iman Mirzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farajtabar_M/0/1/0/all/0/1&quot;&gt;Mehrdad Farajtabar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.12883">
<title>Human Detection of Political Speech Deepfakes across Transcripts, Audio, and Video. (arXiv:2202.12883v3 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2202.12883</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in technology for hyper-realistic visual effects provoke the
concern that deepfake videos of political speeches will soon be visually
indistinguishable from authentic video recordings. The conventional wisdom in
communication theory predicts people will fall for fake news more often when
the same version of a story is presented as a video versus text. We conduct 4
pre-registered randomized experiments with 2,015 participants to evaluate how
accurately humans distinguish real political speeches from fabrications across
base rates of misinformation, audio sources, and media modalities. We find base
rates of misinformation minimally influence discernment and deepfakes with
audio produced by the state-of-the-art text-to-speech algorithms are harder to
discern than the same deepfakes with voice actor audio. Moreover, we find audio
and visual information enables more accurate discernment than text alone: human
discernment relies more on how something is said, the audio-visual cues, than
what is said, the speech content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groh_M/0/1/0/all/0/1&quot;&gt;Matthew Groh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1&quot;&gt;Aruna Sankaranarayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1&quot;&gt;Nikhil Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dong Young Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lippman_A/0/1/0/all/0/1&quot;&gt;Andrew Lippman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picard_R/0/1/0/all/0/1&quot;&gt;Rosalind Picard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.12888">
<title>Meta-Learning for Simple Regret Minimization. (arXiv:2202.12888v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.12888</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a meta-learning framework for simple regret minimization in
bandits. In this framework, a learning agent interacts with a sequence of
bandit tasks, which are sampled i.i.d.\ from an unknown prior distribution, and
learns its meta-parameters to perform better on future tasks. We propose the
first Bayesian and frequentist meta-learning algorithms for this setting. The
Bayesian algorithm has access to a prior distribution over the meta-parameters
and its meta simple regret over $m$ bandit tasks with horizon $n$ is mere
$\tilde{O}(m / \sqrt{n})$. On the other hand, the meta simple regret of the
frequentist algorithm is $\tilde{O}(\sqrt{m} n + m/ \sqrt{n})$. While its
regret is worse, the frequentist algorithm is more general because it does not
need a prior distribution over the meta-parameters. It can also be analyzed in
more settings. We instantiate our algorithms for several classes of bandit
problems. Our algorithms are general and we complement our theory by evaluating
them empirically in several environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1&quot;&gt;Mohammadjavad Azizi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1&quot;&gt;Branislav Kveton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1&quot;&gt;Mohammad Ghavamzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katariya_S/0/1/0/all/0/1&quot;&gt;Sumeet Katariya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.14092">
<title>Towards Visual Affordance Learning: A Benchmark for Affordance Segmentation and Recognition. (arXiv:2203.14092v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.14092</link>
<description rdf:parseType="Literal">&lt;p&gt;The physical and textural attributes of objects have been widely studied for
recognition, detection and segmentation tasks in computer vision.~A number of
datasets, such as large scale ImageNet, have been proposed for feature learning
using data hungry deep neural networks and for hand-crafted feature extraction.
To intelligently interact with objects, robots and intelligent machines need
the ability to infer beyond the traditional physical/textural attributes, and
understand/learn visual cues, called visual affordances, for affordance
recognition, detection and segmentation. To date there is no publicly available
large dataset for visual affordance understanding and learning. In this paper,
we introduce a large scale multi-view RGBD visual affordance learning dataset,
a benchmark of 47210 RGBD images from 37 object categories, annotated with 15
visual affordance categories. To the best of our knowledge, this is the first
ever and the largest multi-view RGBD visual affordance learning dataset. We
benchmark the proposed dataset for affordance segmentation and recognition
tasks using popular Vision Transformer and Convolutional Neural Networks.
Several state-of-the-art deep learning networks are evaluated each for
affordance recognition and segmentation tasks. Our experimental results
showcase the challenging nature of the dataset and present definite prospects
for new and robust affordance learning algorithms. The dataset is publicly
available at https://sites.google.com/view/afaqshah/dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalifa_Z/0/1/0/all/0/1&quot;&gt;Zeyad Khalifa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1&quot;&gt;Syed Afaq Ali Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.12900">
<title>Cross-Camera Trajectories Help Person Retrieval in a Camera Network. (arXiv:2204.12900v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.12900</link>
<description rdf:parseType="Literal">&lt;p&gt;We are concerned with retrieving a query person from multiple videos captured
by a non-overlapping camera network. Existing methods often rely on purely
visual matching or consider temporal constraints but ignore the spatial
information of the camera network. To address this issue, we propose a
pedestrian retrieval framework based on cross-camera trajectory generation,
which integrates both temporal and spatial information. To obtain pedestrian
trajectories, we propose a novel cross-camera spatio-temporal model that
integrates pedestrians&apos; walking habits and the path layout between cameras to
form a joint probability distribution. Such a spatio-temporal model among a
camera network can be specified using sparsely sampled pedestrian data. Based
on the spatio-temporal model, cross-camera trajectories can be extracted by the
conditional random field model and further optimized by restricted non-negative
matrix factorization. Finally, a trajectory re-ranking technique is proposed to
improve the pedestrian retrieval results. To verify the effectiveness of our
method, we construct the first cross-camera pedestrian trajectory dataset, the
Person Trajectory Dataset, in real surveillance scenarios. Extensive
experiments verify the effectiveness and robustness of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohua Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1&quot;&gt;Jianhuang Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wei-Shi Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.10652">
<title>Are Message Passing Neural Networks Really Helpful for Knowledge Graph Completion?. (arXiv:2205.10652v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2205.10652</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graphs (KGs) facilitate a wide variety of applications. Despite
great efforts in creation and maintenance, even the largest KGs are far from
complete. Hence, KG completion (KGC) has become one of the most crucial tasks
for KG research. Recently, considerable literature in this space has centered
around the use of Message Passing (Graph) Neural Networks (MPNNs), to learn
powerful embeddings. The success of these methods is naturally attributed to
the use of MPNNs over simpler multi-layer perceptron (MLP) models, given their
additional message passing (MP) component. In this work, we find that
surprisingly, simple MLP models are able to achieve comparable performance to
MPNNs, suggesting that MP may not be as crucial as previously believed. With
further exploration, we show careful scoring function and loss function design
has a much stronger influence on KGC model performance. This suggests a
conflation of scoring function design, loss function design, and MP in prior
work, with promising insights regarding the scalability of state-of-the-art KGC
methods today, as well as careful attention to more suitable MP designs for KGC
tasks tomorrow. Our codes are publicly available at:
https://github.com/Juanhui28/Are_MPNNs_helpful.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Juanhui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shomer_H/0/1/0/all/0/1&quot;&gt;Harry Shomer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1&quot;&gt;Jiayuan Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Neil Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiliang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dawei Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.11922">
<title>Worldwide AI Ethics: a review of 200 guidelines and recommendations for AI governance. (arXiv:2206.11922v6 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2206.11922</link>
<description rdf:parseType="Literal">&lt;p&gt;The utilization of artificial intelligence (AI) applications has experienced
tremendous growth in recent years, bringing forth numerous benefits and
conveniences. However, this expansion has also provoked ethical concerns, such
as privacy breaches, algorithmic discrimination, security and reliability
issues, transparency, and other unintended consequences. To determine whether a
global consensus exists regarding the ethical principles that should govern AI
applications and to contribute to the formation of future regulations, this
paper conducts a meta-analysis of 200 governance policies and ethical
guidelines for AI usage published by public bodies, academic institutions,
private companies, and civil society organizations worldwide. We identified at
least 17 resonating principles prevalent in the policies and guidelines of our
dataset, released as an open-source database and tool. We present the
limitations of performing a global scale analysis study paired with a critical
analysis of our findings, presenting areas of consensus that should be
incorporated into future regulatory efforts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Correa_N/0/1/0/all/0/1&quot;&gt;Nicholas Kluge Corr&amp;#xea;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galvao_C/0/1/0/all/0/1&quot;&gt;Camila Galv&amp;#xe3;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1&quot;&gt;James William Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pino_C/0/1/0/all/0/1&quot;&gt;Carolina Del Pino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinto_E/0/1/0/all/0/1&quot;&gt;Edson Pontes Pinto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barbosa_C/0/1/0/all/0/1&quot;&gt;Camila Barbosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Massmann_D/0/1/0/all/0/1&quot;&gt;Diogo Massmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mambrini_R/0/1/0/all/0/1&quot;&gt;Rodrigo Mambrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galvao_L/0/1/0/all/0/1&quot;&gt;Luiza Galv&amp;#xe3;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terem_E/0/1/0/all/0/1&quot;&gt;Edmund Terem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_N/0/1/0/all/0/1&quot;&gt;Nythamar de Oliveira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.03341">
<title>Softmax-free Linear Transformers. (arXiv:2207.03341v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.03341</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers (ViTs) have pushed the state-of-the-art for visual
perception tasks. The self-attention mechanism underpinning the strength of
ViTs has a quadratic complexity in both computation and memory usage. This
motivates the development of approximating the self-attention at linear
complexity. However, an in-depth analysis in this work reveals that existing
methods are either theoretically flawed or empirically ineffective for visual
recognition. We identify that their limitations are rooted in the inheritance
of softmax-based self-attention during approximations, that is, normalizing the
scaled dot-product between token feature vectors using the softmax function. As
preserving the softmax operation challenges any subsequent linearization
efforts. By this insight, a family of Softmax-Free Transformers (SOFT) are
proposed. Specifically, a Gaussian kernel function is adopted to replace the
dot-product similarity, enabling a full self-attention matrix to be
approximated under low-rank matrix decomposition. For computational robustness,
we estimate the Moore-Penrose inverse using an iterative Newton-Raphson method
in the forward process only, while calculating its theoretical gradients only
once in the backward process. To further expand applicability (e.g., dense
prediction tasks), an efficient symmetric normalization technique is
introduced. Extensive experiments on ImageNet, COCO, and ADE20K show that our
SOFT significantly improves the computational efficiency of existing ViT
variants. With linear complexity, much longer token sequences are permitted by
SOFT, resulting in superior trade-off between accuracy and complexity. Code and
models are available at https://github.com/fudan-zvg/SOFT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiachen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jianfeng Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tao Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.12776">
<title>SFusion: Self-attention based N-to-One Multimodal Fusion Block. (arXiv:2208.12776v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.12776</link>
<description rdf:parseType="Literal">&lt;p&gt;People perceive the world with different senses, such as sight, hearing,
smell, and touch. Processing and fusing information from multiple modalities
enables Artificial Intelligence to understand the world around us more easily.
However, when there are missing modalities, the number of available modalities
is different in diverse situations, which leads to an N-to-One fusion problem.
To solve this problem, we propose a self-attention based fusion block called
SFusion. Different from preset formulations or convolution based methods, the
proposed block automatically learns to fuse available modalities without
synthesizing or zero-padding missing ones. Specifically, the feature
representations extracted from upstream processing model are projected as
tokens and fed into self-attention module to generate latent multimodal
correlations. Then, a modal attention mechanism is introduced to build a shared
representation, which can be applied by the downstream decision model. The
proposed SFusion can be easily integrated into existing multimodal analysis
networks. In this work, we apply SFusion to different backbone networks for
human activity recognition and brain tumor segmentation tasks. Extensive
experimental results show that the SFusion block achieves better performance
than the competing fusion strategies. Our code is available at
https://github.com/scut-cszcl/SFusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zecheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jia Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Rui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jianlong Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.02307">
<title>A first-order logic characterization of safety and co-safety languages. (arXiv:2209.02307v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2209.02307</link>
<description rdf:parseType="Literal">&lt;p&gt;Linear Temporal Logic (LTL) is one of the most popular temporal logics, that
comes into play in a variety of branches of computer science. Among the various
reasons of its widespread use there are its strong foundational properties: LTL
is equivalent to counter-free omega-automata, to star-free omega-regular
expressions, and (by Kamp&apos;s theorem) to the First-Order Theory of Linear Orders
(FO-TLO). Safety and co-safety languages, where a finite prefix suffices to
establish whether a word does not belong or belongs to the language,
respectively, play a crucial role in lowering the complexity of problems like
model checking and reactive synthesis for LTL. SafetyLTL (resp., coSafetyLTL)
is a fragment of LTL where only universal (resp., existential) temporal
modalities are allowed, that recognises safety (resp., co-safety) languages
only. The main contribution of this paper is the introduction of a fragment of
FO-TLO, called SafetyFO, and of its dual coSafetyFO, which are expressively
complete with respect to the LTL-definable safety and co-safety languages. We
prove that they exactly characterize SafetyLTL and coSafetyLTL, respectively, a
result that joins Kamp&apos;s theorem, and provides a clearer view of the
characterization of (fragments of) LTL in terms of first-order languages. In
addition, it gives a direct, compact, and self-contained proof that any safety
language definable in LTL is definable in SafetyLTL as well. As a by-product,
we obtain some interesting results on the expressive power of the weak tomorrow
operator of SafetyLTL, interpreted over finite and infinite words. Moreover, we
prove that, when interpreted over finite words, SafetyLTL (resp. coSafetyLTL)
devoid of the tomorrow (resp., weak tomorrow) operator captures the safety
(resp., co-safety) fragment of LTL over finite words.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cimatti_A/0/1/0/all/0/1&quot;&gt;Alessandro Cimatti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geatti_L/0/1/0/all/0/1&quot;&gt;Luca Geatti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gigante_N/0/1/0/all/0/1&quot;&gt;Nicola Gigante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montanari_A/0/1/0/all/0/1&quot;&gt;Angelo Montanari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tonetta_S/0/1/0/all/0/1&quot;&gt;Stefano Tonetta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.06356">
<title>A Simple Approach for State-Action Abstraction using a Learned MDP Homomorphism. (arXiv:2209.06356v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.06356</link>
<description rdf:parseType="Literal">&lt;p&gt;Animals are able to rapidly infer from limited experience when sets of state
action pairs have equivalent reward and transition dynamics. On the other hand,
modern reinforcement learning systems must painstakingly learn through trial
and error that sets of state action pairs are value equivalent -- requiring an
often prohibitively large amount of samples from their environment. MDP
homomorphisms have been proposed that reduce the observed MDP of an environment
to an abstract MDP, which can enable more sample efficient policy learning.
Consequently, impressive improvements in sample efficiency have been achieved
when a suitable MDP homomorphism can be constructed a priori -- usually by
exploiting a practioner&apos;s knowledge of environment symmetries. We propose a
novel approach to constructing a homomorphism in discrete action spaces, which
uses a partial model of environment dynamics to infer which state action pairs
lead to the same state -- reducing the size of the state-action space by a
factor equal to the cardinality of the action space. We call this method
equivalent effect abstraction. In a gridworld setting, we demonstrate
empirically that equivalent effect abstraction can improve sample efficiency in
a model-free setting and planning efficiency for modelbased approaches.
Furthermore, we show on cartpole that our approach outperforms an existing
method for learning homomorphisms, while using 33x less training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mavor_Parker_A/0/1/0/all/0/1&quot;&gt;Augustine N. Mavor-Parker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sargent_M/0/1/0/all/0/1&quot;&gt;Matthew J. Sargent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banino_A/0/1/0/all/0/1&quot;&gt;Andrea Banino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffin_L/0/1/0/all/0/1&quot;&gt;Lewis D. Griffin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barry_C/0/1/0/all/0/1&quot;&gt;Caswell Barry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.10656">
<title>Learning from Symmetry: Meta-Reinforcement Learning with Symmetrical Behaviors and Language Instructions. (arXiv:2209.10656v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2209.10656</link>
<description rdf:parseType="Literal">&lt;p&gt;Meta-reinforcement learning (meta-RL) is a promising approach that enables
the agent to learn new tasks quickly. However, most meta-RL algorithms show
poor generalization in multi-task scenarios due to the insufficient task
information provided only by rewards. Language-conditioned meta-RL improves the
generalization capability by matching language instructions with the agent&apos;s
behaviors. While both behaviors and language instructions have symmetry, which
can speed up human learning of new knowledge. Thus, combining symmetry and
language instructions into meta-RL can help improve the algorithm&apos;s
generalization and learning efficiency. We propose a dual-MDP
meta-reinforcement learning method that enables learning new tasks efficiently
with symmetrical behaviors and language instructions. We evaluate our method in
multiple challenging manipulation tasks, and experimental results show that our
method can greatly improve the generalization and learning efficiency of
meta-reinforcement learning. Videos are available at
https://tumi6robot.wixsite.com/symmetry/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1&quot;&gt;Xiangtong Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bing_Z/0/1/0/all/0/1&quot;&gt;Zhenshan Bing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_G/0/1/0/all/0/1&quot;&gt;Genghang Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kejia Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hongkuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kai Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1&quot;&gt;Alois Knoll&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.15240">
<title>Universal Prompt Tuning for Graph Neural Networks. (arXiv:2209.15240v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.15240</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, prompt tuning has sparked a research surge in adapting
pre-trained models. Unlike the unified pre-training strategy employed in the
language field, the graph field exhibits diverse pre-training strategies,
posing challenges in designing appropriate prompt-based tuning methods for
graph neural networks. While some pioneering work has devised specialized
prompting functions for models that employ edge prediction as their
pre-training tasks, these methods are limited to specific pre-trained GNN
models and lack broader applicability. In this paper, we introduce a universal
prompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained
GNN models under any pre-training strategy. GPF operates on the input graph&apos;s
feature space and can theoretically achieve an equivalent effect to any form of
prompting function. Consequently, we no longer need to illustrate the prompting
function corresponding to each pre-training strategy explicitly. Instead, we
employ GPF to obtain the prompted graph for the downstream task in an adaptive
manner. We provide rigorous derivations to demonstrate the universality of GPF
and make guarantee of its effectiveness. The experimental results under various
pre-training strategies indicate that our method performs better than
fine-tuning, with an average improvement of about 1.4% in full-shot scenarios
and about 3.2% in few-shot scenarios. Moreover, our method significantly
outperforms existing specialized prompt-based tuning methods when applied to
models utilizing the pre-training strategy they specialize in. These numerous
advantages position our method as a compelling alternative to fine-tuning for
downstream adaptations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1&quot;&gt;Taoran Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunchao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chunping Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lei Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.10209">
<title>Exclusive Supermask Subnetwork Training for Continual Learning. (arXiv:2210.10209v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.10209</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual Learning (CL) methods focus on accumulating knowledge over time
while avoiding catastrophic forgetting. Recently, Wortsman et al. (2020)
proposed a CL method, SupSup, which uses a randomly initialized, fixed base
network (model) and finds a supermask for each new task that selectively keeps
or removes each weight to produce a subnetwork. They prevent forgetting as the
network weights are not being updated. Although there is no forgetting, the
performance of SupSup is sub-optimal because fixed weights restrict its
representational power. Furthermore, there is no accumulation or transfer of
knowledge inside the model when new tasks are learned. Hence, we propose
ExSSNeT (Exclusive Supermask SubNEtwork Training), that performs exclusive and
non-overlapping subnetwork weight training. This avoids conflicting updates to
the shared weights by subsequent tasks to improve performance while still
preventing forgetting. Furthermore, we propose a novel KNN-based Knowledge
Transfer (KKT) module that utilizes previously acquired knowledge to learn new
tasks better and faster. We demonstrate that ExSSNeT outperforms strong
previous methods on both NLP and Vision domains while preventing forgetting.
Moreover, ExSSNeT is particularly advantageous for sparse masks that activate
2-10% of the model parameters, resulting in an average improvement of 8.3% over
SupSup. Furthermore, ExSSNeT scales to a large number of tasks (100). Our code
is available at https://github.com/prateeky2806/exessnet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1&quot;&gt;Prateek Yadav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.10329">
<title>Language Detoxification with Attribute-Discriminative Latent Space. (arXiv:2210.10329v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.10329</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based Language Models (LMs) have achieved impressive results on
natural language understanding tasks, but they can also generate toxic text
such as insults, threats, and profanity, limiting their real-world
applications. To overcome this issue, a few text generation approaches aim to
detoxify toxic texts using additional LMs or perturbations. However, previous
methods require excessive memory, computations, and time which are serious
bottlenecks in their real-world application. To address such limitations, we
propose an effective yet efficient method for language detoxification using an
attribute-discriminative latent space. Specifically, we project the latent
space of an original Transformer LM onto a discriminative latent space that
well-separates texts by their attributes using a projection block and an
attribute discriminator. This allows the LM to control the text generation to
be non-toxic with minimal memory and computation overhead. We validate our
model, Attribute-Discriminative Language Model (ADLM) on detoxified language
and dialogue generation tasks, on which our method significantly outperforms
baselines both in performance and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_J/0/1/0/all/0/1&quot;&gt;Jin Myung Kwak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minseon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sung Ju Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.14321">
<title>Artificial ASMR: A Cyber-Psychological Approach. (arXiv:2210.14321v3 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2210.14321</link>
<description rdf:parseType="Literal">&lt;p&gt;The popularity of Autonomous Sensory Meridian Response (ASMR) has skyrockted
over the past decade, but scientific studies on what exactly triggered ASMR
effect remain few and immature, one most commonly acknowledged trigger is that
ASMR clips typically provide rich semantic information. With our attention
caught by the common acoustic patterns in ASMR audios, we investigate the
correlation between the cyclic features of audio signals and their
effectiveness in triggering ASMR effects. A cyber-psychological approach that
combines signal processing, artificial intelligence, and experimental
psychology is taken, with which we are able to quantize ASMR-related acoustic
features, and therewith synthesize ASMR clips with random cyclic patterns but
not delivering identifiably scenarios to the audience, which were proven to be
effective in triggering ASMR effects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zexin Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Bin Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cao_C/0/1/0/all/0/1&quot;&gt;C. Clark Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schotten_H/0/1/0/all/0/1&quot;&gt;Hans. D. Schotten&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01407">
<title>On the Informativeness of Supervision Signals. (arXiv:2211.01407v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.01407</link>
<description rdf:parseType="Literal">&lt;p&gt;Supervised learning typically focuses on learning transferable
representations from training examples annotated by humans. While rich
annotations (like soft labels) carry more information than sparse annotations
(like hard labels), they are also more expensive to collect. For example, while
hard labels only provide information about the closest class an object belongs
to (e.g., &quot;this is a dog&quot;), soft labels provide information about the object&apos;s
relationship with multiple classes (e.g., &quot;this is most likely a dog, but it
could also be a wolf or a coyote&quot;). We use information theory to compare how a
number of commonly-used supervision signals contribute to
representation-learning performance, as well as how their capacity is affected
by factors such as the number of labels, classes, dimensions, and noise. Our
framework provides theoretical justification for using hard labels in the
big-data regime, but richer supervision signals for few-shot learning and
out-of-distribution generalization. We validate these results empirically in a
series of experiments with over 1 million crowdsourced image annotations and
conduct a cost-benefit analysis to establish a tradeoff curve that enables
users to optimize the cost of supervising representation learning on their own
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sucholutsky_I/0/1/0/all/0/1&quot;&gt;Ilia Sucholutsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Battleday_R/0/1/0/all/0/1&quot;&gt;Ruairidh M. Battleday&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_K/0/1/0/all/0/1&quot;&gt;Katherine M. Collins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marjieh_R/0/1/0/all/0/1&quot;&gt;Raja Marjieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peterson_J/0/1/0/all/0/1&quot;&gt;Joshua C. Peterson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1&quot;&gt;Pulkit Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatt_U/0/1/0/all/0/1&quot;&gt;Umang Bhatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1&quot;&gt;Nori Jacoby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1&quot;&gt;Adrian Weller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1&quot;&gt;Thomas L. Griffiths&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.08416">
<title>Robot Learning on the Job: Human-in-the-Loop Autonomy and Learning During Deployment. (arXiv:2211.08416v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2211.08416</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid growth of computing powers and recent advances in deep
learning, we have witnessed impressive demonstrations of novel robot
capabilities in research settings. Nonetheless, these learning systems exhibit
brittle generalization and require excessive training data for practical tasks.
To harness the capabilities of state-of-the-art robot learning models while
embracing their imperfections, we present Sirius, a principled framework for
humans and robots to collaborate through a division of work. In this framework,
partially autonomous robots are tasked with handling a major portion of
decision-making where they work reliably; meanwhile, human operators monitor
the process and intervene in challenging situations. Such a human-robot team
ensures safe deployments in complex tasks. Further, we introduce a new learning
algorithm to improve the policy&apos;s performance on the data collected from the
task executions. The core idea is re-weighing training samples with
approximated human trust and optimizing the policies with weighted behavioral
cloning. We evaluate Sirius in simulation and on real hardware, showing that
Sirius consistently outperforms baselines over a collection of contact-rich
manipulation tasks, achieving an 8% boost in simulation and 27% on real
hardware than the state-of-the-art methods in policy success rate, with twice
faster convergence and 85% memory size reduction. Videos and more details are
available at https://ut-austin-rpl.github.io/sirius/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huihan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasiriany_S/0/1/0/all/0/1&quot;&gt;Soroush Nasiriany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lance Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1&quot;&gt;Zhiyao Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuke Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.02090">
<title>Breaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling. (arXiv:2212.02090v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.02090</link>
<description rdf:parseType="Literal">&lt;p&gt;To capture the relationship between samples and labels, conditional
generative models often inherit spurious correlations from the training
dataset. This can result in label-conditional distributions that are imbalanced
with respect to another latent attribute. To mitigate this issue, which we call
spurious causality of conditional generation, we propose a general two-step
strategy. (a) Fairness Intervention (FI): emphasize the minority samples that
are hard to generate due to the spurious correlation in the training dataset.
(b) Corrective Sampling (CS): explicitly filter the generated samples and
ensure that they follow the desired latent attribute distribution. We have
designed the fairness intervention to work for various degrees of supervision
on the spurious attribute, including unsupervised, weakly-supervised, and
semi-supervised scenarios. Our experimental results demonstrate that FICS can
effectively resolve spurious causality of conditional generation across various
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_J/0/1/0/all/0/1&quot;&gt;Junhyun Nam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1&quot;&gt;Sangwoo Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaeho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jinwoo Shin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.05961">
<title>RPN: A Word Vector Level Data Augmentation Algorithm in Deep Learning for Language Understanding. (arXiv:2212.05961v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.05961</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation is a widely used technique in machine learning to improve
model performance. However, existing data augmentation techniques in natural
language understanding (NLU) may not fully capture the complexity of natural
language variations, and they can be challenging to apply to large datasets.
This paper proposes the Random Position Noise (RPN) algorithm, a novel data
augmentation technique that operates at the word vector level. RPN modifies the
word embeddings of the original text by introducing noise based on the existing
values of selected word vectors, allowing for more fine-grained modifications
and better capturing natural language variations. Unlike traditional data
augmentation methods, RPN does not require gradients in the computational graph
during virtual sample updates, making it simpler to apply to large datasets.
Experimental results demonstrate that RPN consistently outperforms existing
data augmentation techniques across various NLU tasks, including sentiment
analysis, natural language inference, and paraphrase detection. Moreover, RPN
performs well in low-resource settings and is applicable to any model featuring
a word embeddings layer. The proposed RPN algorithm is a promising approach for
enhancing NLU performance and addressing the challenges associated with
traditional data augmentation techniques in large-scale NLU tasks. Our
experimental results demonstrated that the RPN algorithm achieved
state-of-the-art performance in all seven NLU tasks, thereby highlighting its
effectiveness and potential for real-world NLU applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zhengqing Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1&quot;&gt;Xuecong Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1&quot;&gt;Huiwen Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhuanzhe Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.05118">
<title>Beyond In-Domain Scenarios: Robust Density-Aware Calibration. (arXiv:2302.05118v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.05118</link>
<description rdf:parseType="Literal">&lt;p&gt;Calibrating deep learning models to yield uncertainty-aware predictions is
crucial as deep neural networks get increasingly deployed in safety-critical
applications. While existing post-hoc calibration methods achieve impressive
results on in-domain test datasets, they are limited by their inability to
yield reliable uncertainty estimates in domain-shift and out-of-domain (OOD)
scenarios. We aim to bridge this gap by proposing DAC, an accuracy-preserving
as well as Density-Aware Calibration method based on k-nearest-neighbors (KNN).
In contrast to existing post-hoc methods, we utilize hidden layers of
classifiers as a source for uncertainty-related information and study their
importance. We show that DAC is a generic method that can readily be combined
with state-of-the-art post-hoc methods. DAC boosts the robustness of
calibration performance in domain-shift and OOD, while maintaining excellent
in-domain predictive uncertainty estimates. We demonstrate that DAC leads to
consistently better calibration across a large number of model architectures,
datasets, and metrics. Additionally, we show that DAC improves calibration
substantially on recent large-scale neural networks pre-trained on vast amounts
of data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomani_C/0/1/0/all/0/1&quot;&gt;Christian Tomani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waseda_F/0/1/0/all/0/1&quot;&gt;Futa Waseda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yuesong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1&quot;&gt;Daniel Cremers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.07832">
<title>Deep Anomaly Detection under Labeling Budget Constraints. (arXiv:2302.07832v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.07832</link>
<description rdf:parseType="Literal">&lt;p&gt;Selecting informative data points for expert feedback can significantly
improve the performance of anomaly detection (AD) in various contexts, such as
medical diagnostics or fraud detection. In this paper, we determine a set of
theoretical conditions under which anomaly scores generalize from labeled
queries to unlabeled data. Motivated by these results, we propose a data
labeling strategy with optimal data coverage under labeling budget constraints.
In addition, we propose a new learning framework for semi-supervised AD.
Extensive experiments on image, tabular, and video data sets show that our
approach results in state-of-the-art semi-supervised AD performance under
labeling budget constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Aodong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1&quot;&gt;Chen Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kloft_M/0/1/0/all/0/1&quot;&gt;Marius Kloft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smyth_P/0/1/0/all/0/1&quot;&gt;Padhraic Smyth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1&quot;&gt;Stephan Mandt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1&quot;&gt;Maja Rudolph&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.07937">
<title>The Expressive Power of Tuning Only the Normalization Layers. (arXiv:2302.07937v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.07937</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature normalization transforms such as Batch and Layer-Normalization have
become indispensable ingredients of state-of-the-art deep neural networks.
Recent studies on fine-tuning large pretrained models indicate that just tuning
the parameters of these affine transforms can achieve high accuracy for
downstream tasks. These findings open the questions about the expressive power
of tuning the normalization layers of frozen networks. In this work, we take
the first step towards this question and show that for random ReLU networks,
fine-tuning only its normalization layers can reconstruct any target network
that is $O(\sqrt{\text{width}})$ times smaller. We show that this holds even
for randomly sparsified networks, under sufficient overparameterization, in
agreement with prior empirical work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannou_A/0/1/0/all/0/1&quot;&gt;Angeliki Giannou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajput_S/0/1/0/all/0/1&quot;&gt;Shashank Rajput&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1&quot;&gt;Dimitris Papailiopoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.09532">
<title>Pseudo Contrastive Learning for Graph-based Semi-supervised Learning. (arXiv:2302.09532v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.09532</link>
<description rdf:parseType="Literal">&lt;p&gt;Pseudo Labeling is a technique used to improve the performance of
semi-supervised Graph Neural Networks (GNNs) by generating additional
pseudo-labels based on confident predictions. However, the quality of generated
pseudo-labels has been a longstanding concern due to the sensitivity of the
classification objective with respect to the given labels. To avoid the
untrustworthy classification supervision indicating ``a node belongs to a
specific class,&apos;&apos; we favor the fault-tolerant contrasting supervision
demonstrating ``two nodes do not belong to the same class.&apos;&apos; Thus, the problem
of generating high-quality pseudo-labels is then transformed into a relaxed
version, i.e., identifying reliable negative pairs. To achieve this, we propose
a general framework for GNNs, termed Pseudo Contrastive Learning (PCL). It
separates two nodes whose positive and negative pseudo-labels target the same
class. To incorporate topological knowledge into learning, we devise a
topologically weighted contrastive loss that spends more effort separating
negative pairs with smaller topological distances. Experimentally, we apply PCL
to various GNNs, which consistently outperform their counterparts using other
popular general techniques on five real-world graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Weigang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1&quot;&gt;Ziyu Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yaming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1&quot;&gt;Yuanhai Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Baosheng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11325">
<title>Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation. (arXiv:2302.11325v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11325</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a deep learning framework for medical video segmentation.
Convolution neural network (CNN) and transformer-based methods have achieved
great milestones in medical image segmentation tasks due to their incredible
semantic feature encoding and global information comprehension abilities.
However, most existing approaches ignore a salient aspect of medical video data
- the temporal dimension. Our proposed framework explicitly extracts features
from neighbouring frames across the temporal dimension and incorporates them
with a temporal feature blender, which then tokenises the high-level
spatio-temporal feature to form a strong global feature encoded via a Swin
Transformer. The final segmentation results are produced via a UNet-like
encoder-decoder architecture. Our model outperforms other approaches by a
significant margin and improves the segmentation benchmarks on the VFSS2022
dataset, achieving a dice coefficient of 0.8986 and 0.8186 for the two datasets
tested. Our studies also show the efficacy of the temporal feature blending
scheme and cross-dataset transferability of learned capabilities. Code and
models are fully available at https://github.com/SimonZeng7108/Video-SwinUNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1&quot;&gt;Chengxi Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xinyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smithard_D/0/1/0/all/0/1&quot;&gt;David Smithard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirmehdi_M/0/1/0/all/0/1&quot;&gt;Majid Mirmehdi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gambaruto_A/0/1/0/all/0/1&quot;&gt;Alberto M Gambaruto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burghardt_T/0/1/0/all/0/1&quot;&gt;Tilo Burghardt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.13259">
<title>Can we avoid Double Descent in Deep Neural Networks?. (arXiv:2302.13259v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.13259</link>
<description rdf:parseType="Literal">&lt;p&gt;Finding the optimal size of deep learning models is very actual and of broad
impact, especially in energy-saving schemes. Very recently, an unexpected
phenomenon, the ``double descent&apos;&apos;, has caught the attention of the deep
learning community. As the model&apos;s size grows, the performance gets first
worse, and then goes back to improving. It raises serious questions about the
optimal model&apos;s size to maintain high generalization: the model needs to be
sufficiently over-parametrized, but adding too many parameters wastes training
resources. Is it possible to find, in an efficient way, the best trade-off? Our
work shows that the double descent phenomenon is potentially avoidable with
proper conditioning of the learning problem, but a final answer is yet to be
found. We empirically observe that there is hope to dodge the double descent in
complex scenarios with proper regularization, as a simple $\ell_2$
regularization is already positively contributing to such a perspective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quetu_V/0/1/0/all/0/1&quot;&gt;Victor Qu&amp;#xe9;tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tartaglione_E/0/1/0/all/0/1&quot;&gt;Enzo Tartaglione&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.04209">
<title>Causal Dependence Plots. (arXiv:2303.04209v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.04209</link>
<description rdf:parseType="Literal">&lt;p&gt;Explaining artificial intelligence or machine learning models is increasingly
important. To use such data-driven systems wisely we must understand how they
interact with the world, including how they depend causally on data inputs. In
this work we develop Causal Dependence Plots (CDPs) to visualize how one
variable--an outcome--depends on changes in another variable--a
predictor--$\textit{along with any consequent causal changes in other predictor
variables}$. Crucially, CDPs differ from standard methods based on holding
other predictors constant or assuming they are independent. CDPs make use of an
auxiliary causal model because causal conclusions require causal assumptions.
With simulations and real data experiments, we show CDPs can be combined in a
modular way with methods for causal learning or sensitivity analysis. Since
people often think causally about input-output dependence, CDPs can be powerful
tools in the xAI or interpretable machine learning toolkit and contribute to
applications like scientific machine learning and algorithmic fairness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loftus_J/0/1/0/all/0/1&quot;&gt;Joshua R. Loftus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bynum_L/0/1/0/all/0/1&quot;&gt;Lucius E. J. Bynum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hansen_S/0/1/0/all/0/1&quot;&gt;Sakina Hansen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06455">
<title>Graph Neural Network contextual embedding for Deep Learning on Tabular Data. (arXiv:2303.06455v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06455</link>
<description rdf:parseType="Literal">&lt;p&gt;All industries are trying to leverage Artificial Intelligence (AI) based on
their existing big data which is available in so called tabular form, where
each record is composed of a number of heterogeneous continuous and categorical
columns also known as features. Deep Learning (DL) has constituted a major
breakthrough for AI in fields related to human skills like natural language
processing, but its applicability to tabular data has been more challenging.
More classical Machine Learning (ML) models like tree-based ensemble ones
usually perform better. This paper presents a novel DL model using Graph Neural
Network (GNN) more specifically Interaction Network (IN), for contextual
embedding and modelling interactions among tabular features. Its results
outperform those of a recently published survey with DL benchmark based on five
public datasets, also achieving competitive results when compared to
boosted-tree solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villaizan_Vallelado_M/0/1/0/all/0/1&quot;&gt;Mario Villaiz&amp;#xe1;n-Vallelado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salvatori_M/0/1/0/all/0/1&quot;&gt;Matteo Salvatori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_B/0/1/0/all/0/1&quot;&gt;Bel&amp;#xe9;n Carro Martinez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esguevillas_A/0/1/0/all/0/1&quot;&gt;Antonio Javier Sanchez Esguevillas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13988">
<title>Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13988</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are currently at the forefront of intertwining
AI systems with human communication and everyday life. Due to rapid
technological advances and their extreme versatility, LLMs nowadays have
millions of users and are at the cusp of being the main go-to technology for
information retrieval, content generation, problem-solving, etc. Therefore, it
is of great importance to thoroughly assess and scrutinize their capabilities.
Due to increasingly complex and novel behavioral patterns in current LLMs, this
can be done by treating them as participants in psychology experiments that
were originally designed to test humans. For this purpose, the paper introduces
a new field of research called &quot;machine psychology&quot;. The paper outlines how
different subfields of psychology can inform behavioral tests for LLMs. It
defines methodological standards for machine psychology research, especially by
focusing on policies for prompt designs. Additionally, it describes how
behavioral patterns discovered in LLMs are to be interpreted. In sum, machine
psychology aims to discover emergent abilities in LLMs that cannot be detected
by most traditional natural language processing benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hagendorff_T/0/1/0/all/0/1&quot;&gt;Thilo Hagendorff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10557">
<title>An Introduction to Transformers. (arXiv:2304.10557v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10557</link>
<description rdf:parseType="Literal">&lt;p&gt;The transformer is a neural network component that can be used to learn
useful representations of sequences or sets of datapoints. The transformer has
driven recent advances in natural language processing, computer vision, and
spatio-temporal modelling. There are many introductions to transformers, but
most do not contain precise mathematical descriptions of the architecture and
the intuitions behind the design choices are often also missing. Moreover, as
research takes a winding path, the explanations for the components of the
transformer can be idiosyncratic. In this note we aim for a mathematically
precise, intuitive, and clean description of the transformer architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1&quot;&gt;Richard E. Turner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02301">
<title>Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. (arXiv:2305.02301v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02301</link>
<description rdf:parseType="Literal">&lt;p&gt;Deploying large language models (LLMs) is challenging because they are memory
inefficient and compute-intensive for practical applications. In reaction,
researchers train smaller task-specific models by either finetuning with human
labels or distilling using LLM-generated labels. However, finetuning and
distillation require large amounts of training data to achieve comparable
performance to LLMs. We introduce Distilling step-by-step, a new mechanism that
(a) trains smaller models that outperform LLMs, and (b) achieves so by
leveraging less training data needed by finetuning or distillation. Our method
extracts LLM rationales as additional supervision for training small models
within a multi-task framework. We present three findings across 4 NLP
benchmarks: First, compared to both finetuning and distillation, our mechanism
achieves better performance with much fewer labeled/unlabeled training
examples. Second, compared to few-shot prompted LLMs, we achieve better
performance using substantially smaller model sizes. Third, we reduce both the
model size and the amount of data required to outperform LLMs; our finetuned
770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80%
of available data on a benchmark, whereas standard finetuning the same T5 model
struggles to match even by using 100% of the dataset. We release the code at:
https://github.com/google-research/distilling-step-by-step .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cheng-Yu Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chun-Liang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1&quot;&gt;Chih-Kuan Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakhost_H/0/1/0/all/0/1&quot;&gt;Hootan Nakhost&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1&quot;&gt;Yasuhisa Fujii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1&quot;&gt;Alexander Ratner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1&quot;&gt;Ranjay Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chen-Yu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1&quot;&gt;Tomas Pfister&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02531">
<title>A Cross-Linguistic Analysis of Intertemporal Preferences in GPT-3.5. (arXiv:2305.02531v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02531</link>
<description rdf:parseType="Literal">&lt;p&gt;Language has a strong influence on our perceptions of time and rewards. This
raises the question of whether large language models, when asked the same
question in different languages, show different preferences for rewards over
time and if their choices are similar to those of humans. In this study, we
analyze the responses of GPT-3.5 (hereafter referred to as GPT) to prompts in
multiple languages, exploring preferences between smaller, sooner rewards and
larger, later rewards. Our results show that GPT displays greater patience when
prompted in languages with weak future tense references (FTR), such as German
and Mandarin, compared to languages with strong FTR, like English and French.
These findings are consistent with the existing literature and suggest a
correlation between GPT&apos;s choices and the preferences of speakers of these
languages. However, further analysis reveals that the preference for earlier or
later rewards does not systematically change with reward gaps, indicating a
lexicographic preference for earlier payments. While GPT may capture intriguing
variations across languages, our findings indicate that the choices made by
these models do not correspond to those of human decision-makers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goli_A/0/1/0/all/0/1&quot;&gt;Ali Goli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Amandeep Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09840">
<title>Scale-Adaptive Balancing of Exploration and Exploitation in Classical Planning. (arXiv:2305.09840v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09840</link>
<description rdf:parseType="Literal">&lt;p&gt;Balancing exploration and exploitation has been an important problem in both
game tree search and automated planning. However, while the problem has been
extensively analyzed within the Multi-Armed Bandit (MAB) literature, the
planning community has had limited success when attempting to apply those
results. We show that a more detailed theoretical understanding of MAB
literature helps improve existing planning algorithms that are based on Monte
Carlo Tree Search (MCTS) / Trial Based Heuristic Tree Search (THTS). In
particular, THTS uses UCB1 MAB algorithms in an ad hoc manner, as UCB1&apos;s
theoretical requirement of fixed bounded support reward distributions is not
satisfied within heuristic search for classical planning. The core issue lies
in UCB1&apos;s lack of adaptations to the different scales of the rewards. We
propose GreedyUCT-Normal, a MCTS/THTS algorithm with UCB1-Normal bandit for
agile classical planning, which handles distributions with different scales by
taking the reward variance into consideration, and resulted in an improved
algorithmic performance (more plans found with less node expansions) that
outperforms Greedy Best First Search and existing MCTS/THTS-based algorithms
(GreedyUCT,GreedyUCT*).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wissow_S/0/1/0/all/0/1&quot;&gt;Stephen Wissow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asai_M/0/1/0/all/0/1&quot;&gt;Masataro Asai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11019">
<title>Annotation-free Audio-Visual Segmentation. (arXiv:2305.11019v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11019</link>
<description rdf:parseType="Literal">&lt;p&gt;The objective of Audio-Visual Segmentation (AVS) is to localise the sounding
objects within visual scenes by accurately predicting pixel-wise segmentation
masks. To tackle the task, it involves a comprehensive consideration of both
the data and model aspects. In this paper, first, we initiate a novel pipeline
for generating artificial data for the AVS task without human annotating. We
leverage existing image segmentation and audio datasets to match the image-mask
pairs with its corresponding audio samples with the linkage of category labels,
that allows us to effortlessly compose (image, audio, mask) triplets for
training AVS models. The pipeline is annotation-free and scalable to cover a
large number of categories. Additionally, we introduce a lightweight approach
SAMA-AVS to adapt the pre-trained segment anything model~(SAM) to the AVS task.
By introducing only a small number of trainable parameters with adapters, the
proposed model can effectively achieve adequate audio-visual fusion and
interaction in the encoding stage with vast majority of parameters fixed. We
conduct extensive experiments, and the results show our proposed model
remarkably surpasses other competing methods. Moreover, by using the proposed
model pretrained with our synthetic data, the performance on real AVSBench data
is further improved, achieving 83.17 mIoU on S4 subset and 66.95 mIoU on MS3
set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinxiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1&quot;&gt;Chen Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chaofan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weidi Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03361">
<title>WHAT, WHEN, and HOW to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue. (arXiv:2306.03361v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03361</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a method for building a personalized open-domain dialogue
system to address the WWH (WHAT, WHEN, and HOW) problem for natural response
generation in a commercial setting, where personalized dialogue responses are
heavily interleaved with casual response turns. The proposed approach involves
weighted dataset blending, negative persona information augmentation methods,
and the design of personalized conversation datasets to address the challenges
of WWH in personalized, open-domain dialogue systems. Our work effectively
balances dialogue fluency and tendency to ground, while also introducing a
response-type label to improve the controllability and explainability of the
grounded responses. The combination of these methods leads to more fluent
conversations, as evidenced by subjective human evaluations as well as
objective evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_D/0/1/0/all/0/1&quot;&gt;Deuksin Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sunwoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Ki Hyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seojin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taeyoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davis_E/0/1/0/all/0/1&quot;&gt;Eric Davis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04719">
<title>Don&apos;t trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04719</link>
<description rdf:parseType="Literal">&lt;p&gt;How do neural networks extract patterns from pixels? Feature visualizations
attempt to answer this important question by visualizing highly activating
patterns through optimization. Today, visualization methods form the foundation
of our knowledge about the internal workings of neural networks, as a type of
mechanistic interpretability. Here we ask: How reliable are feature
visualizations? We start our investigation by developing network circuits that
trick feature visualizations into showing arbitrary patterns that are
completely disconnected from normal network behavior on natural input. We then
provide evidence for a similar phenomenon occurring in standard, unmanipulated
networks: feature visualizations are processed very differently from standard
input, casting doubt on their ability to &quot;explain&quot; how neural networks process
natural images. We underpin this empirical finding by theory proving that the
set of functions that can be reliably understood by feature visualization is
extremely small and does not include general black-box neural networks.
Therefore, a promising way forward could be the development of networks that
enforce certain structures in order to ensure more reliable feature
visualizations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1&quot;&gt;Robert Geirhos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1&quot;&gt;Roland S. Zimmermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilodeau_B/0/1/0/all/0/1&quot;&gt;Blair Bilodeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1&quot;&gt;Wieland Brendel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Been Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06722">
<title>$E(2)$-Equivariant Vision Transformer. (arXiv:2306.06722v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06722</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformer (ViT) has achieved remarkable performance in computer
vision. However, positional encoding in ViT makes it substantially difficult to
learn the intrinsic equivariance in data. Initial attempts have been made on
designing equivariant ViT but are proved defective in some cases in this paper.
To address this issue, we design a Group Equivariant Vision Transformer
(GE-ViT) via a novel, effective positional encoding operator. We prove that
GE-ViT meets all the theoretical requirements of an equivariant neural network.
Comprehensive experiments are conducted on standard benchmark datasets,
demonstrating that GE-ViT significantly outperforms non-equivariant
self-attention networks. The code is available at
https://github.com/ZJUCDSYangKaifan/GEVit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Renjun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kaifan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Ke Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1&quot;&gt;Fengxiang He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07797">
<title>Monolingual and Cross-Lingual Knowledge Transfer for Topic Classification. (arXiv:2306.07797v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07797</link>
<description rdf:parseType="Literal">&lt;p&gt;This article investigates the knowledge transfer from the RuQTopics dataset.
This Russian topical dataset combines a large sample number (361,560
single-label, 170,930 multi-label) with extensive class coverage (76 classes).
We have prepared this dataset from the &quot;Yandex Que&quot; raw data. By evaluating the
RuQTopics - trained models on the six matching classes of the Russian MASSIVE
subset, we have proved that the RuQTopics dataset is suitable for real-world
conversational tasks, as the Russian-only models trained on this dataset
consistently yield an accuracy around 85\% on this subset. We also have figured
out that for the multilingual BERT, trained on the RuQTopics and evaluated on
the same six classes of MASSIVE (for all MASSIVE languages), the language-wise
accuracy closely correlates (Spearman correlation 0.773 with p-value 2.997e-11)
with the approximate size of the pretraining BERT&apos;s data for the corresponding
language. At the same time, the correlation of the language-wise accuracy with
the linguistical distance from Russian is not statistically significant.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karpov_D/0/1/0/all/0/1&quot;&gt;Dmitry Karpov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1&quot;&gt;Mikhail Burtsev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08149">
<title>Neural Mixed Effects for Nonlinear Personalized Predictions. (arXiv:2306.08149v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08149</link>
<description rdf:parseType="Literal">&lt;p&gt;Personalized prediction is a machine learning approach that predicts a
person&apos;s future observations based on their past labeled observations and is
typically used for sequential tasks, e.g., to predict daily mood ratings. When
making personalized predictions, a model can combine two types of trends: (a)
trends shared across people, i.e., person-generic trends, such as being happier
on weekends, and (b) unique trends for each person, i.e., person-specific
trends, such as a stressful weekly meeting. Mixed effect models are popular
statistical models to study both trends by combining person-generic and
person-specific parameters. Though linear mixed effect models are gaining
popularity in machine learning by integrating them with neural networks, these
integrations are currently limited to linear person-specific parameters: ruling
out nonlinear person-specific trends. In this paper, we propose Neural Mixed
Effect (NME) models to optimize nonlinear person-specific parameters anywhere
in a neural network in a scalable manner. NME combines the efficiency of neural
network optimization with nonlinear mixed effects modeling. Empirically, we
observe that NME improves performance across six unimodal and multimodal
datasets, including a smartphone dataset to predict daily mood and a
mother-adolescent dataset to predict affective state sequences where half the
mothers experience at least moderate symptoms of depression. Furthermore, we
evaluate NME for two model architectures, including for neural conditional
random fields (CRF) to predict affective state sequences where the CRF learns
nonlinear person-specific temporal transitions between affective states.
Analysis of these person-specific transitions on the mother-adolescent dataset
shows interpretable trends related to the mother&apos;s depression symptoms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wortwein_T/0/1/0/all/0/1&quot;&gt;Torsten W&amp;#xf6;rtwein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1&quot;&gt;Nicholas Allen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheeber_L/0/1/0/all/0/1&quot;&gt;Lisa B. Sheeber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1&quot;&gt;Randy P. Auerbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohn_J/0/1/0/all/0/1&quot;&gt;Jeffrey F. Cohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1&quot;&gt;Louis-Philippe Morency&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09459">
<title>Recurrent Memory Decision Transformer. (arXiv:2306.09459v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09459</link>
<description rdf:parseType="Literal">&lt;p&gt;Originally developed for natural language problems, transformer models have
recently been widely used in offline reinforcement learning tasks. This is
because the agent&apos;s history can be represented as a sequence, and the whole
task can be reduced to the sequence modeling task. However, the quadratic
complexity of the transformer operation limits the potential increase in
context. Therefore, different versions of the memory mechanism are used to work
with long sequences in a natural language. This paper proposes the Recurrent
Memory Decision Transformer (RMDT), a model that uses a recurrent memory
mechanism for reinforcement learning problems. We conduct thorough experiments
on Atari games and MuJoCo control problems and show that our proposed model is
significantly superior to its counterparts without the recurrent memory
mechanism on Atari games. We also carefully study the effect of memory on the
performance of the proposed model. These findings shed light on the potential
of incorporating recurrent memory mechanisms to improve the performance of
large-scale transformer models in offline reinforcement learning tasks. The
Recurrent Memory Decision Transformer code is publicly available in the
repository \url{https://anonymous.4open.science/r/RMDT-4FE4}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bessonov_A/0/1/0/all/0/1&quot;&gt;Arkadii Bessonov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Staroverov_A/0/1/0/all/0/1&quot;&gt;Alexey Staroverov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huzhenyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovalev_A/0/1/0/all/0/1&quot;&gt;Alexey K. Kovalev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yudin_D/0/1/0/all/0/1&quot;&gt;Dmitry Yudin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panov_A/0/1/0/all/0/1&quot;&gt;Aleksandr I. Panov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11168">
<title>Learning Models of Adversarial Agent Behavior under Partial Observability. (arXiv:2306.11168v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11168</link>
<description rdf:parseType="Literal">&lt;p&gt;The need for opponent modeling and tracking arises in several real-world
scenarios, such as professional sports, video game design, and drug-trafficking
interdiction. In this work, we present Graph based Adversarial Modeling with
Mutal Information (GrAMMI) for modeling the behavior of an adversarial opponent
agent. GrAMMI is a novel graph neural network (GNN) based approach that uses
mutual information maximization as an auxiliary objective to predict the
current and future states of an adversarial opponent with partial
observability. To evaluate GrAMMI, we design two large-scale, pursuit-evasion
domains inspired by real-world scenarios, where a team of heterogeneous agents
is tasked with tracking and interdicting a single adversarial agent, and the
adversarial agent must evade detection while achieving its own objectives. With
the mutual information formulation, GrAMMI outperforms all baselines in both
domains and achieves 31.68% higher log-likelihood on average for future
adversarial state predictions across both domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1&quot;&gt;Sean Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natarajan_M/0/1/0/all/0/1&quot;&gt;Manisha Natarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zixuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paleja_R/0/1/0/all/0/1&quot;&gt;Rohan Paleja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Letian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gombolay_M/0/1/0/all/0/1&quot;&gt;Matthew C. Gombolay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13695">
<title>Phase Unwrapping of Color Doppler Echocardiography using Deep Learning. (arXiv:2306.13695v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13695</link>
<description rdf:parseType="Literal">&lt;p&gt;Color Doppler echocardiography is a widely used non-invasive imaging modality
that provides real-time information about the intracardiac blood flow. In an
apical long-axis view of the left ventricle, color Doppler is subject to phase
wrapping, or aliasing, especially during cardiac filling and ejection. When
setting up quantitative methods based on color Doppler, it is necessary to
correct this wrapping artifact. We developed an unfolded primal-dual network to
unwrap (dealias) color Doppler echocardiographic images and compared its
effectiveness against two state-of-the-art segmentation approaches based on
nnU-Net and transformer models. We trained and evaluated the performance of
each method on an in-house dataset and found that the nnU-Net-based method
provided the best dealiased results, followed by the primal-dual approach and
the transformer-based technique. Noteworthy, the primal-dual network, which had
significantly fewer trainable parameters, performed competitively with respect
to the other two methods, demonstrating the high potential of deep unfolding
methods. Our results suggest that deep learning-based methods can effectively
remove aliasing artifacts in color Doppler echocardiographic images,
outperforming DeAN, a state-of-the-art semi-automatic technique. Overall, our
results show that deep learning-based methods have the potential to effectively
preprocess color Doppler images for downstream quantitative analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ling_H/0/1/0/all/0/1&quot;&gt;Hang Jung Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bernard_O/0/1/0/all/0/1&quot;&gt;Olivier Bernard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ducros_N/0/1/0/all/0/1&quot;&gt;Nicolas Ducros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Garcia_D/0/1/0/all/0/1&quot;&gt;Damien Garcia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16688">
<title>SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores. (arXiv:2306.16688v2 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16688</link>
<description rdf:parseType="Literal">&lt;p&gt;The ever-growing complexity of reinforcement learning (RL) tasks demands a
distributed RL system to efficiently generate and process a massive amount of
data to train intelligent agents. However, existing open-source libraries
suffer from various limitations, which impede their practical use in
challenging scenarios where large-scale training is necessary. While industrial
systems from OpenAI and DeepMind have achieved successful large-scale RL
training, their system architecture and implementation details remain
undisclosed to the community. In this paper, we present a novel abstraction on
the dataflows of RL training, which unifies practical RL training across
diverse applications into a general framework and enables fine-grained
optimizations. Following this abstraction, we develop a scalable, efficient,
and extensible distributed RL system called ReaLly Scalable RL (SRL). The
system architecture of SRL separates major RL computation components and allows
massively parallelized training. Moreover, SRL offers user-friendly and
extensible interfaces for customized algorithms. Our evaluation shows that SRL
outperforms existing academic libraries in both a single machine and a
medium-sized cluster. In a large-scale cluster, the novel architecture of SRL
leads to up to 3.7x speedup compared to the design choices adopted by the
existing libraries. We also conduct a direct benchmark comparison to OpenAI&apos;s
industrial system, Rapid, in the challenging hide-and-seek environment. SRL
reproduces the same solution as reported by OpenAI with up to 5x speedup in
wall-clock time. Furthermore, we also examine the performance of SRL in a much
harder variant of the hide-and-seek environment and achieve substantial
learning speedup by scaling SRL to over 15k CPU cores and 32 A100 GPUs.
Notably, SRL is the first in the academic community to perform RL experiments
at such a large scale.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_Z/0/1/0/all/0/1&quot;&gt;Zhiyu Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1&quot;&gt;Wei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guangju Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huanchen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yi Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17010">
<title>milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17010</link>
<description rdf:parseType="Literal">&lt;p&gt;Approaching the era of ubiquitous computing, human motion sensing plays a
crucial role in smart systems for decision making, user interaction, and
personalized services. Extensive research has been conducted on human tracking,
pose estimation, gesture recognition, and activity recognition, which are
predominantly based on cameras in traditional methods. However, the intrusive
nature of cameras limits their use in smart home applications. To address this,
mmWave radars have gained popularity due to their privacy-friendly features. In
this work, we propose \textit{milliFlow}, a novel deep learning method for
scene flow estimation as a complementary motion information for mmWave point
cloud, serving as an intermediate level of features and directly benefiting
downstream human motion sensing tasks. Experimental results demonstrate the
superior performance of our method with an average 3D endpoint error of 4.6cm,
significantly surpassing the competing approaches. Furthermore, by
incorporating scene flow information, we achieve remarkable improvements in
human activity recognition, human parsing, and human body part tracking. To
foster further research in this area, we provide our codebase and dataset for
open access.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1&quot;&gt;Fangqiang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Peijun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chris Xiaoxuan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17059">
<title>The mapKurator System: A Complete Pipeline for Extracting and Linking Text from Historical Maps. (arXiv:2306.17059v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17059</link>
<description rdf:parseType="Literal">&lt;p&gt;Scanned historical maps in libraries and archives are valuable repositories
of geographic data that often do not exist elsewhere. Despite the potential of
machine learning tools like the Google Vision APIs for automatically
transcribing text from these maps into machine-readable formats, they do not
work well with large-sized images (e.g., high-resolution scanned documents),
cannot infer the relation between the recognized text and other datasets, and
are challenging to integrate with post-processing tools. This paper introduces
the mapKurator system, an end-to-end system integrating machine learning models
with a comprehensive data processing pipeline. mapKurator empowers automated
extraction, post-processing, and linkage of text labels from large numbers of
large-dimension historical map scans. The output data, comprising bounding
polygons and recognized text, is in the standard GeoJSON format, making it
easily modifiable within Geographic Information Systems (GIS). The proposed
system allows users to quickly generate valuable data from large numbers of
historical maps for in-depth analysis of the map content and, in turn,
encourages map findability, accessibility, interoperability, and reusability
(FAIR principles). We deployed the mapKurator system and enabled the processing
of over 60,000 maps and over 100 million text/place names in the David Rumsey
Historical Map collection. We also demonstrated a seamless integration of
mapKurator with a collaborative web platform to enable accessing automated
approaches for extracting and linking text labels from historical map scans and
collective work to improve the results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jina Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zekun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yijun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namgung_M/0/1/0/all/0/1&quot;&gt;Min Namgung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_L/0/1/0/all/0/1&quot;&gt;Leeje Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_Y/0/1/0/all/0/1&quot;&gt;Yao-Yi Chiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17256">
<title>Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17256</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender systems play a crucial role in helping users discover information
that aligns with their interests based on their past behaviors. However,
developing personalized recommendation systems becomes challenging when
historical records of user-item interactions are unavailable, leading to what
is known as the system cold-start recommendation problem. This issue is
particularly prominent in start-up businesses or platforms with insufficient
user engagement history. Previous studies focus on user or item cold-start
scenarios, where systems could make recommendations for new users or items but
are still trained with historical user-item interactions in the same domain,
which cannot solve our problem. To bridge the gap, our research introduces an
innovative and effective approach, capitalizing on the capabilities of
pre-trained language models. We transform the recommendation process into
sentiment analysis of natural languages containing information of user profiles
and item attributes, where the sentiment polarity is predicted with prompt
learning. By harnessing the extensive knowledge housed within language models,
the prediction can be made without historical user-item interaction records. A
benchmark is also introduced to evaluate the proposed method under the
cold-start setting, and the results demonstrate the effectiveness of our
method. To the best of our knowledge, this is the first study to tackle the
system cold-start recommendation problem. The benchmark and implementation of
the method are available at https://github.com/JacksonWuxs/PromptRec.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xuansheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Huachi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wenlin Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ninghao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17747">
<title>Discriminatory or Samaritan -- which AI is needed for humanity? An Evolutionary Game Theory Analysis of Hybrid Human-AI populations. (arXiv:2306.17747v2 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17747</link>
<description rdf:parseType="Literal">&lt;p&gt;As artificial intelligence (AI) systems are increasingly embedded in our
lives, their presence leads to interactions that shape our behaviour,
decision-making, and social interactions. Existing theoretical research has
primarily focused on human-to-human interactions, overlooking the unique
dynamics triggered by the presence of AI. In this paper, resorting to methods
from evolutionary game theory, we study how different forms of AI influence the
evolution of cooperation in a human population playing the one-shot Prisoner&apos;s
Dilemma game in both well-mixed and structured populations. We found that
Samaritan AI agents that help everyone unconditionally, including defectors,
can promote higher levels of cooperation in humans than Discriminatory AI that
only help those considered worthy/cooperative, especially in slow-moving
societies where change is viewed with caution or resistance (small intensities
of selection). Intuitively, in fast-moving societies (high intensities of
selection), Discriminatory AIs promote higher levels of cooperation than
Samaritan AIs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Booker_T/0/1/0/all/0/1&quot;&gt;Tim Booker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miranda_M/0/1/0/all/0/1&quot;&gt;Manuel Miranda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_J/0/1/0/all/0/1&quot;&gt;Jes&amp;#xfa;s A. Moreno L&amp;#xf3;pez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Mar&amp;#xed;a Ramos Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddel_M/0/1/0/all/0/1&quot;&gt;Max Reddel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Widler_V/0/1/0/all/0/1&quot;&gt;Valeria Widler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmaro_F/0/1/0/all/0/1&quot;&gt;Filippo Zimmaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antonioni_A/0/1/0/all/0/1&quot;&gt;Alberto Antonioni&lt;/a&gt;, The &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_A/0/1/0/all/0/1&quot;&gt;Anh Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00426">
<title>Sparsity-aware generalization theory for deep neural networks. (arXiv:2307.00426v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00426</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep artificial neural networks achieve surprising generalization abilities
that remain poorly understood. In this paper, we present a new approach to
analyzing generalization for deep feed-forward ReLU networks that takes
advantage of the degree of sparsity that is achieved in the hidden layer
activations. By developing a framework that accounts for this reduced effective
model size for each input sample, we are able to show fundamental trade-offs
between sparsity and generalization. Importantly, our results make no strong
assumptions about the degree of sparsity achieved by the model, and it improves
over recent norm-based approaches. We illustrate our results numerically,
demonstrating non-vacuous bounds when coupled with data-dependent priors in
specific settings, even in over-parametrized models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muthukumar_R/0/1/0/all/0/1&quot;&gt;Ramchandran Muthukumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sulam_J/0/1/0/all/0/1&quot;&gt;Jeremias Sulam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00677">
<title>SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption. (arXiv:2307.00677v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00677</link>
<description rdf:parseType="Literal">&lt;p&gt;Density-based clustering could be the most popular clustering algorithm since
it can identify clusters of arbitrary shape as long as different (high-density)
clusters are separated by low-density regions. However, the requirement of the
separateness of clusters by low-density regions is not trivial since a
high-density region might have different structures which should be clustered
into different groups. Such a situation demonstrates the main flaw of all
previous density-based clustering algorithms we have known--structures in a
high-density cluster could not be detected. Therefore, this paper aims to
provide a density-based clustering scheme that not only has the ability
previous ones have but could also detect structures in a high-density region
not separated by low-density ones. The algorithm employs secondary directed
differential, hierarchy, normalized density, as well as the self-adaption
coefficient, and thus is called Structure Detecting Cluster by Hierarchical
Secondary Directed Differential with Normalized Density and Self-Adaption,
dubbed by SDC-HSDD-NDSA for short. To illustrate its effectiveness, we run the
algorithm in several data sets. The results verify its validity in structure
detection, robustness over noises, as well as independence of granularities,
and demonstrate that it could outperform previous ones. The Python code of the
paper could be found on https://github.com/Hao-B-Shu/SDC-HSDD-NDSA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_H/0/1/0/all/0/1&quot;&gt;Hao Shu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.09325">
<title>TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation. (arXiv:2211.09325v2 [cs.RO] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2211.09325</link>
<description rdf:parseType="Literal">&lt;p&gt;How do we imbue robots with the ability to efficiently manipulate unseen
objects and transfer relevant skills based on demonstrations? End-to-end
learning methods often fail to generalize to novel objects or unseen
configurations. Instead, we focus on the task-specific pose relationship
between relevant parts of interacting objects. We conjecture that this
relationship is a generalizable notion of a manipulation task that can transfer
to new objects in the same category; examples include the relationship between
the pose of a pan relative to an oven or the pose of a mug relative to a mug
rack. We call this task-specific pose relationship &quot;cross-pose&quot; and provide a
mathematical definition of this concept. We propose a vision-based system that
learns to estimate the cross-pose between two objects for a given manipulation
task using learned cross-object correspondences. The estimated cross-pose is
then used to guide a downstream motion planner to manipulate the objects into
the desired pose relationship (placing a pan into the oven or the mug onto the
mug rack). We demonstrate our method&apos;s capability to generalize to unseen
objects, in some cases after training on only 10 demonstrations in the real
world. Results show that our system achieves state-of-the-art performance in
both simulated and real-world experiments across a number of tasks.
Supplementary information and videos can be found at
https://sites.google.com/view/tax-pose/home.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1&quot;&gt;Chuer Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okorn_B/0/1/0/all/0/1&quot;&gt;Brian Okorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Harry Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisner_B/0/1/0/all/0/1&quot;&gt;Ben Eisner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1&quot;&gt;David Held&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12680">
<title>Recent Developments in Recommender Systems: A Survey. (arXiv:2306.12680v1 [cs.IR] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2306.12680</link>
<description rdf:parseType="Literal">&lt;p&gt;In this technical survey, we comprehensively summarize the latest
advancements in the field of recommender systems. The objective of this study
is to provide an overview of the current state-of-the-art in the field and
highlight the latest trends in the development of recommender systems. The
study starts with a comprehensive summary of the main taxonomy of recommender
systems, including personalized and group recommender systems, and then delves
into the category of knowledge-based recommender systems. In addition, the
survey analyzes the robustness, data bias, and fairness issues in recommender
systems, summarizing the evaluation metrics used to assess the performance of
these systems. Finally, the study provides insights into the latest trends in
the development of recommender systems and highlights the new directions for
future research in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kangbo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Satapathy_R/0/1/0/all/0/1&quot;&gt;Ranjan Satapathy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Suhang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1&quot;&gt;Erik Cambria&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00019">
<title>Goal quest for an intelligent surfer moving in a chaotic flow. (arXiv:2307.00019v1 [physics.soc-ph] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.00019</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a model of an intelligent surfer moving on the Ulam network
generated by a chaotic dynamics in the Chirikov standard map. This directed
network is obtained by the Ulam method with a division of the phase space in
cells of fixed size forming the nodes of a Markov chain. The goal quest for
this surfer is to determine the network path from an initial node A to a final
node B with minimal resistance given by the sum of inverse transition
probabilities. We develop an algorithm for the intelligent surfer that allows
to perform the quest in a small number of transitions which grows only
logarithmically with the network size. The optimal path search is done on a
fractal intersection set formed by nodes with small Erd\&quot;os numbers of the
forward and inverted networks. The intelligent surfer exponentially outperforms
a naive surfer who tries to minimize its phase space distance to the target B.
We argue that such an algorithm provides new hints for motion control in
chaotic flows.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Frahm_K/0/1/0/all/0/1&quot;&gt;Klaus M. Frahm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shepelyansky_D/0/1/0/all/0/1&quot;&gt;Dima L. Shepelyansky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00165">
<title>Counterfactual Collaborative Reasoning. (arXiv:2307.00165v1 [cs.IR] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.00165</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal reasoning and logical reasoning are two important types of reasoning
abilities for human intelligence. However, their relationship has not been
extensively explored under machine intelligence context. In this paper, we
explore how the two reasoning abilities can be jointly modeled to enhance both
accuracy and explainability of machine learning models. More specifically, by
integrating two important types of reasoning ability -- counterfactual
reasoning and (neural) logical reasoning -- we propose Counterfactual
Collaborative Reasoning (CCR), which conducts counterfactual logic reasoning to
improve the performance. In particular, we use recommender system as an example
to show how CCR alleviate data scarcity, improve accuracy and enhance
transparency. Technically, we leverage counterfactual reasoning to generate
&quot;difficult&quot; counterfactual training examples for data augmentation, which --
together with the original training examples -- can enhance the model
performance. Since the augmented data is model irrelevant, they can be used to
enhance any model, enabling the wide applicability of the technique. Besides,
most of the existing data augmentation methods focus on &quot;implicit data
augmentation&quot; over users&apos; implicit feedback, while our framework conducts
&quot;explicit data augmentation&quot; over users explicit feedback based on
counterfactual logic reasoning. Experiments on three real-world datasets show
that CCR achieves better performance than non-augmented models and implicitly
augmented models, and also improves model transparency by generating
counterfactual explanations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jianchao Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zelong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_M/0/1/0/all/0/1&quot;&gt;Max Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1&quot;&gt;Juntao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>