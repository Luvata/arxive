<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-03T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01375" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01382" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01386" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01391" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01395" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01445" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01454" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01456" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01482" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01505" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01510" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01524" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01543" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01545" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01558" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01569" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01573" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01574" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01575" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01583" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01624" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01642" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01643" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01647" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01650" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01659" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01662" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01674" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01676" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01685" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01693" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01734" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01749" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01752" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01759" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01808" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01822" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01827" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01839" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01885" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.01577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.03359" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.14650" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.00731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.08123" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.02310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05308" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06876" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01186" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09073" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07665" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15752" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11715" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13307" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15927" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16476" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.17240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00028" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01107" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01219" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.01345">
<title>A Synthetic Modal Generation of Additive Manufacturing Roughness Surfaces from Images. (arXiv:2401.01345v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01345</link>
<description rdf:parseType="Literal">&lt;p&gt;A method to infer and synthetically extrapolate roughness fields from
electron microscope scans of additively manufactured surfaces using an
adaptation of Rogallo&apos;s synthetic turbulence method [R. S. Rogallo, NASA
Technical Memorandum 81315, 1981] based on Fourier modes is presented. The
resulting synthetic roughness fields are smooth and are compatible with grid
generators in computational fluid dynamics or other numerical simulations.
Unlike machine learning methods, which can require over twenty scans of surface
roughness for training, the Fourier mode based method can extrapolate
homogeneous synthetic roughness fields using a single physical roughness scan
to any desired size and range. Five types of synthetic roughness fields are
generated using an electron microscope roughness image from literature. A
comparison of their spectral energy and two-point correlation spectra show that
the synthetic fields closely approximate the roughness structures and spectral
energy of the scan.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keesom_T/0/1/0/all/0/1&quot;&gt;T.B. Keesom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popov_P/0/1/0/all/0/1&quot;&gt;P.P. Popov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhyani_P/0/1/0/all/0/1&quot;&gt;P. Dhyani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobs_G/0/1/0/all/0/1&quot;&gt;G.B. Jacobs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01361">
<title>Optimizing Convolutional Neural Network Architecture. (arXiv:2401.01361v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01361</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNN) are widely used to face challenging tasks
like speech recognition, natural language processing or computer vision. As CNN
architectures get larger and more complex, their computational requirements
increase, incurring significant energetic costs and challenging their
deployment on resource-restricted devices. In this paper, we propose Optimizing
Convolutional Neural Network Architecture (OCNNA), a novel CNN optimization and
construction method based on pruning and knowledge distillation designed to
establish the importance of convolutional layers. The proposal has been
evaluated though a thorough empirical study including the best known datasets
(CIFAR-10, CIFAR-100 and Imagenet) and CNN architectures (VGG-16, ResNet-50,
DenseNet-40 and MobileNet), setting Accuracy Drop and Remaining Parameters
Ratio as objective metrics to compare the performance of OCNNA against the
other state-of-art approaches. Our method has been compared with more than 20
convolutional neural network simplification algorithms obtaining outstanding
results. As a result, OCNNA is a competitive CNN constructing method which
could ease the deployment of neural networks into IoT or resource-limited
devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balderas_L/0/1/0/all/0/1&quot;&gt;Luis Balderas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lastra_M/0/1/0/all/0/1&quot;&gt;Miguel Lastra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benitez_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; M. Ben&amp;#xed;tez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01362">
<title>Assisting Blind People Using Object Detection with Vocal Feedback. (arXiv:2401.01362v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01362</link>
<description rdf:parseType="Literal">&lt;p&gt;For visually impaired people, it is highly difficult to make independent
movement and safely move in both indoors and outdoors environment. Furthermore,
these physically and visually challenges prevent them from in day-today live
activities. Similarly, they have problem perceiving objects of surrounding
environment that may pose a risk to them. The proposed approach suggests
detection of objects in real-time video by using a web camera, for the object
identification, process. You Look Only Once (YOLO) model is utilized which is
CNN-based real-time object detection technique. Additionally, The OpenCV
libraries of Python is used to implement the software program as well as deep
learning process is performed. Image recognition results are transferred to the
visually impaired users in audible form by means of Google text-to-speech
library and determine object location relative to its position in the screen.
The obtaining result was evaluated by using the mean Average Precision (mAP),
and it was found that the proposed approach achieves excellent results when it
compared to previous approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najm_H/0/1/0/all/0/1&quot;&gt;Heba Najm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elferjani_K/0/1/0/all/0/1&quot;&gt;Khirallah Elferjani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alariyibi_A/0/1/0/all/0/1&quot;&gt;Alhaam Alariyibi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01370">
<title>Fast Quantum Convolutional Neural Networks for Low-Complexity Object Detection in Autonomous Driving Applications. (arXiv:2401.01370v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01370</link>
<description rdf:parseType="Literal">&lt;p&gt;Spurred by consistent advances and innovation in deep learning, object
detection applications have become prevalent, particularly in autonomous
driving that leverages various visual data. As convolutional neural networks
(CNNs) are being optimized, the performances and computation speeds of object
detection in autonomous driving have been significantly improved. However, due
to the exponentially rapid growth in the complexity and scale of data used in
object detection, there are limitations in terms of computation speeds while
conducting object detection solely with classical computing. Motivated by this,
quantum convolution-based object detection (QCOD) is proposed to adopt quantum
computing to perform object detection at high speed. The QCOD utilizes our
proposed fast quantum convolution that uploads input channel information and
re-constructs output channels for achieving reduced computational complexity
and thus improving performances. Lastly, the extensive experiments with KITTI
autonomous driving object detection dataset verify that the proposed fast
quantum convolution and QCOD are successfully operated in real object detection
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baek_H/0/1/0/all/0/1&quot;&gt;Hankyul Baek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Donghyeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Joongheon Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01373">
<title>Boosting Defect Detection in Manufacturing using Tensor Convolutional Neural Networks. (arXiv:2401.01373v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01373</link>
<description rdf:parseType="Literal">&lt;p&gt;Defect detection is one of the most important yet challenging tasks in the
quality control stage in the manufacturing sector. In this work, we introduce a
Tensor Convolutional Neural Network (T-CNN) and examine its performance on a
real defect detection application in one of the components of the ultrasonic
sensors produced at Robert Bosch&apos;s manufacturing plants. Our quantum-inspired
T-CNN operates on a reduced model parameter space to substantially improve the
training speed and performance of an equivalent CNN model without sacrificing
accuracy. More specifically, we demonstrate how T-CNNs are able to reach the
same performance as classical CNNs as measured by quality metrics, with up to
fifteen times fewer parameters and 4% to 19% faster training times. Our results
demonstrate that the T-CNN greatly outperforms the results of traditional human
visual inspection, providing value in a current real application in
manufacturing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_Ramiro_P/0/1/0/all/0/1&quot;&gt;Pablo Martin-Ramiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maza_U/0/1/0/all/0/1&quot;&gt;Unai Sainz de la Maza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orus_R/0/1/0/all/0/1&quot;&gt;Roman Orus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mugel_S/0/1/0/all/0/1&quot;&gt;Samuel Mugel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01375">
<title>Mapping Walnut water Stress with High Resolution Multispectral UAV Imagery and Machine Learning. (arXiv:2401.01375v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01375</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective monitoring of walnut water status and stress level across the whole
orchard is an essential step towards precision irrigation management of
walnuts, a significant crop in California. This study presents a machine
learning approach using Random Forest (RF) models to map stem water potential
(SWP) by integrating high-resolution multispectral remote sensing imagery from
Unmanned Aerial Vehicle (UAV) flights with weather data. From 2017 to 2018,
five flights of an UAV equipped with a seven-band multispectral camera were
conducted over a commercial walnut orchard, paired with concurrent ground
measurements of sampled walnut plants. The RF regression model, utilizing
vegetation indices derived from orthomosaiced UAV imagery and weather data,
effectively estimated ground-measured SWPs, achieving an $R^2$ of 0.63 and a
mean absolute error (MAE) of 0.80 bars. The integration of weather data was
particularly crucial for consolidating data across various flight dates.
Significant variables for SWP estimation included wind speed and vegetation
indices such as NDVI, NDRE, and PSRI.A reduced RF model excluding red-edge
indices of NDRE and PSRI, demonstrated slightly reduced accuracy ($R^2$ =
0.54). Additionally, the RF classification model predicted water stress levels
in walnut trees with 85% accuracy, surpassing the 80% accuracy of the reduced
classification model. The results affirm the efficacy of UAV-based
multispectral imaging combined with machine learning, incorporating thermal
data, NDVI, red-edge indices, and weather data, in walnut water stress
estimation and assessment. This methodology offers a scalable, cost-effective
tool for data-driven precision irrigation management at an individual plant
level in walnut orchards.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaitlyn Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yufang Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01382">
<title>Exploring Multi-Modal Control in Music-Driven Dance Generation. (arXiv:2401.01382v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.01382</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing music-driven 3D dance generation methods mainly concentrate on
high-quality dance generation, but lack sufficient control during the
generation process. To address these issues, we propose a unified framework
capable of generating high-quality dance movements and supporting multi-modal
control, including genre control, semantic control, and spatial control. First,
we decouple the dance generation network from the dance control network,
thereby avoiding the degradation in dance quality when adding additional
control information. Second, we design specific control strategies for
different control information and integrate them into a unified framework.
Experimental results show that the proposed dance generation framework
outperforms state-of-the-art methods in terms of motion quality and
controllability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ronghui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yuqin Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yachao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jie Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01383">
<title>Predicting Infant Brain Connectivity with Federated Multi-Trajectory GNNs using Scarce Data. (arXiv:2401.01383v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2401.01383</link>
<description rdf:parseType="Literal">&lt;p&gt;The understanding of the convoluted evolution of infant brain networks during
the first postnatal year is pivotal for identifying the dynamics of early brain
connectivity development. Existing deep learning solutions suffer from three
major limitations. First, they cannot generalize to multi-trajectory prediction
tasks, where each graph trajectory corresponds to a particular imaging modality
or connectivity type (e.g., T1-w MRI). Second, existing models require
extensive training datasets to achieve satisfactory performance which are often
challenging to obtain. Third, they do not efficiently utilize incomplete time
series data. To address these limitations, we introduce FedGmTE-Net++, a
federated graph-based multi-trajectory evolution network. Using the power of
federation, we aggregate local learnings among diverse hospitals with limited
datasets. As a result, we enhance the performance of each hospital&apos;s local
generative model, while preserving data privacy. The three key innovations of
FedGmTE-Net++ are: (i) presenting the first federated learning framework
specifically designed for brain multi-trajectory evolution prediction in a
data-scarce environment, (ii) incorporating an auxiliary regularizer in the
local objective function to exploit all the longitudinal brain connectivity
within the evolution trajectory and maximize data utilization, (iii)
introducing a two-step imputation process, comprising a preliminary KNN-based
precompletion followed by an imputation refinement step that employs regressors
to improve similarity scores and refine imputations. Our comprehensive
experimental results showed the outperformance of FedGmTE-Net++ in brain
multi-trajectory prediction from a single baseline graph in comparison with
benchmark methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Pistos_M/0/1/0/all/0/1&quot;&gt;Michalis Pistos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rekik_I/0/1/0/all/0/1&quot;&gt;Islem Rekik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01386">
<title>Tissue Artifact Segmentation and Severity Analysis for Automated Diagnosis Using Whole Slide Images. (arXiv:2401.01386v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.01386</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditionally, pathological analysis and diagnosis are performed by manually
eyeballing glass slide specimens under a microscope by an expert. The whole
slide image is the digital specimen produced from the glass slide. Whole slide
image enabled specimens to be observed on a computer screen and led to
computational pathology where computer vision and artificial intelligence are
utilized for automated analysis and diagnosis. With the current computational
advancement, the entire whole slide image can be analyzed autonomously without
human supervision. However, the analysis could fail or lead to wrong diagnosis
if the whole slide image is affected by tissue artifacts such as tissue fold or
air bubbles depending on the severity. Existing artifact detection methods rely
on experts for severity assessment to eliminate artifact affected regions from
the analysis. This process is time consuming, exhausting and undermines the
goal of automated analysis or removal of artifacts without evaluating their
severity, which could result in the loss of diagnostically important data.
Therefore, it is necessary to detect artifacts and then assess their severity
automatically. In this paper, we propose a system that incorporates severity
evaluation with artifact detection utilizing convolutional neural networks. The
proposed system uses DoubleUNet to segment artifacts and an ensemble network of
six fine tuned convolutional neural network models to determine severity. This
method outperformed current state of the art in accuracy by 9 percent for
artifact segmentation and achieved a strong correlation of 97 percent with the
evaluation of pathologists for severity assessment. The robustness of the
system was demonstrated using our proposed heterogeneous dataset and practical
usability was ensured by integrating it with an automated analysis system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Himel_G/0/1/0/all/0/1&quot;&gt;Galib Muhammad Shahriar Himel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01387">
<title>DiffAugment: Diffusion based Long-Tailed Visual Relationship Recognition. (arXiv:2401.01387v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01387</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of Visual Relationship Recognition (VRR) aims to identify
relationships between two interacting objects in an image and is particularly
challenging due to the widely-spread and highly imbalanced distribution of
&amp;lt;subject, relation, object&amp;gt; triplets. To overcome the resultant performance
bias in existing VRR approaches, we introduce DiffAugment -- a method which
first augments the tail classes in the linguistic space by making use of
WordNet and then utilizes the generative prowess of Diffusion Models to expand
the visual space for minority classes. We propose a novel hardness-aware
component in diffusion which is based upon the hardness of each &amp;lt;S,R,O&amp;gt; triplet
and demonstrate the effectiveness of hardness-aware diffusion in generating
visual embeddings for the tail classes. We also propose a novel subject and
object based seeding strategy for diffusion sampling which improves the
discriminative capability of the generated visual embeddings. Extensive
experimentation on the GQA-LT dataset shows favorable gains in the
subject/object and relation average per-class accuracy using Diffusion
augmented samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1&quot;&gt;Parul Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tuan Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhall_A/0/1/0/all/0/1&quot;&gt;Abhinav Dhall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1&quot;&gt;Munawar Hayat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Trung Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1&quot;&gt;Thanh-Toan Do&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01388">
<title>Directional Antenna Systems for Long-Range Through-Wall Human Activity Recognition. (arXiv:2401.01388v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01388</link>
<description rdf:parseType="Literal">&lt;p&gt;WiFi Channel State Information (CSI)-based human activity recognition (HAR)
enables contactless, long-range sensing in spatially constrained environments
while preserving visual privacy. However, despite the presence of numerous
WiFi-enabled devices around us, few expose CSI to users, resulting in a lack of
sensing hardware options. Variants of the Espressif ESP32 have emerged as
potential low-cost and easy-to-deploy solutions for WiFi CSI-based HAR. In this
work, four ESP32-S3-based 2.4GHz directional antenna systems are evaluated for
their ability to facilitate long-range through-wall HAR. Two promising systems
are proposed, one of which combines the ESP32-S3 with a directional biquad
antenna. This combination represents, to the best of our knowledge, the first
demonstration of such a system in WiFi-based HAR. The second system relies on
the built-in printed inverted-F antenna (PIFA) of the ESP32-S3 and achieves
directionality through a plane reflector. In a comprehensive evaluation of
line-of-sight (LOS) and non-line-of-sight (NLOS) HAR performance, both systems
are deployed in an office environment spanning a distance of 18 meters across
five rooms. In this experimental setup, the Wallhack1.8k dataset, comprising
1806 CSI amplitude spectrograms of human activities, is collected and made
publicly available. Based on Wallhack1.8k, we train activity recognition models
using the EfficientNetV2 architecture to assess system performance in LOS and
NLOS scenarios. For the core NLOS activity recognition problem, the biquad
antenna and PIFA-based systems achieve accuracies of 92.0$\pm$3.5 and
86.8$\pm$4.7, respectively, demonstrating the feasibility of long-range
through-wall HAR with the proposed systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strohmayer_J/0/1/0/all/0/1&quot;&gt;Julian Strohmayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kampel_M/0/1/0/all/0/1&quot;&gt;Martin Kampel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01391">
<title>On Optimal Sampling for Learning SDF Using MLPs Equipped with Positional Encoding. (arXiv:2401.01391v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01391</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural implicit fields, such as the neural signed distance field (SDF) of a
shape, have emerged as a powerful representation for many applications, e.g.,
encoding a 3D shape and performing collision detection. Typically, implicit
fields are encoded by Multi-layer Perceptrons (MLP) with positional encoding
(PE) to capture high-frequency geometric details. However, a notable side
effect of such PE-equipped MLPs is the noisy artifacts present in the learned
implicit fields. While increasing the sampling rate could in general mitigate
these artifacts, in this paper we aim to explain this adverse phenomenon
through the lens of Fourier analysis. We devise a tool to determine the
appropriate sampling rate for learning an accurate neural implicit field
without undesirable side effects. Specifically, we propose a simple yet
effective method to estimate the intrinsic frequency of a given network with
randomized weights based on the Fourier analysis of the network&apos;s responses. It
is observed that a PE-equipped MLP has an intrinsic frequency much higher than
the highest frequency component in the PE layer. Sampling against this
intrinsic frequency following the Nyquist-Sannon sampling theorem allows us to
determine an appropriate training sampling rate. We empirically show in the
setting of SDF fitting that this recommended sampling rate is sufficient to
secure accurate fitting results, while further increasing the sampling rate
would not further noticeably reduce the fitting error. Training PE-equipped
MLPs simply with our sampling strategy leads to performances superior to the
existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guying Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Congyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Junhui Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xiaogang Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1&quot;&gt;Taku Komura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keyser_J/0/1/0/all/0/1&quot;&gt;John Keyser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01395">
<title>Deep autoregressive modeling for land use land cover. (arXiv:2401.01395v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01395</link>
<description rdf:parseType="Literal">&lt;p&gt;Land use / land cover (LULC) modeling is a challenging task due to long-range
dependencies between geographic features and distinct spatial patterns related
to topography, ecology, and human development. We identify a close connection
between modeling of spatial patterns of land use and the task of image
inpainting from computer vision and conduct a study of a modified PixelCNN
architecture with approximately 19 million parameters for modeling LULC. In
comparison with a benchmark spatial statistical model, we find that the former
is capable of capturing much richer spatial correlation patterns such as roads
and water bodies but does not produce a calibrated predictive distribution,
suggesting the need for additional tuning. We find evidence of predictive
underdispersion with regard to important ecologically-relevant land use
statistics such as patch count and adjacency which can be ameliorated to some
extent by manipulating sampling variability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krapu_C/0/1/0/all/0/1&quot;&gt;Christopher Krapu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borsuk_M/0/1/0/all/0/1&quot;&gt;Mark Borsuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calder_R/0/1/0/all/0/1&quot;&gt;Ryan Calder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01425">
<title>SwapTransformer: highway overtaking tactical planner model via imitation learning on OSHA dataset. (arXiv:2401.01425v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.01425</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates the high-level decision-making problem in highway
scenarios regarding lane changing and over-taking other slower vehicles. In
particular, this paper aims to improve the Travel Assist feature for automatic
overtaking and lane changes on highways. About 9 million samples including lane
images and other dynamic objects are collected in simulation. This data;
Overtaking on Simulated HighwAys (OSHA) dataset is released to tackle this
challenge. To solve this problem, an architecture called SwapTransformer is
designed and implemented as an imitation learning approach on the OSHA dataset.
Moreover, auxiliary tasks such as future points and car distance network
predictions are proposed to aid the model in better understanding the
surrounding environment. The performance of the proposed solution is compared
with a multi-layer perceptron (MLP) and multi-head self-attention networks as
baselines in a simulation environment. We also demonstrate the performance of
the model with and without auxiliary tasks. All models are evaluated based on
different metrics such as time to finish each lap, number of overtakes, and
speed difference with speed limit. The evaluation shows that the
SwapTransformer model outperforms other models in different traffic densities
in the inference phase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shamsoshoara_A/0/1/0/all/0/1&quot;&gt;Alireza Shamsoshoara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salih_S/0/1/0/all/0/1&quot;&gt;Safin B Salih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aghazadeh_P/0/1/0/all/0/1&quot;&gt;Pedram Aghazadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01439">
<title>Off-Road LiDAR Intensity Based Semantic Segmentation. (arXiv:2401.01439v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01439</link>
<description rdf:parseType="Literal">&lt;p&gt;LiDAR is used in autonomous driving to provide 3D spatial information and
enable accurate perception in off-road environments, aiding in obstacle
detection, mapping, and path planning. Learning-based LiDAR semantic
segmentation utilizes machine learning techniques to automatically classify
objects and regions in LiDAR point clouds. Learning-based models struggle in
off-road environments due to the presence of diverse objects with varying
colors, textures, and undefined boundaries, which can lead to difficulties in
accurately classifying and segmenting objects using traditional geometric-based
features. In this paper, we address this problem by harnessing the LiDAR
intensity parameter to enhance object segmentation in off-road environments.
Our approach was evaluated in the RELLIS-3D data set and yielded promising
results as a preliminary analysis with improved mIoU for classes &quot;puddle&quot; and
&quot;grass&quot; compared to more complex deep learning-based benchmarks. The
methodology was evaluated for compatibility across both Velodyne and Ouster
LiDAR systems, assuring its cross-platform applicability. This analysis
advocates for the incorporation of calibrated intensity as a supplementary
input, aiming to enhance the prediction accuracy of learning based semantic
segmentation frameworks.
https://github.com/MOONLABIISERB/lidar-intensity-predictor/tree/main
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viswanath_K/0/1/0/all/0/1&quot;&gt;Kasi Viswanath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1&quot;&gt;Peng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+PB_S/0/1/0/all/0/1&quot;&gt;Sujit PB&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saripalli_S/0/1/0/all/0/1&quot;&gt;Srikanth Saripalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01445">
<title>Indoor Obstacle Discovery on Reflective Ground via Monocular Camera. (arXiv:2401.01445v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01445</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual obstacle discovery is a key step towards autonomous navigation of
indoor mobile robots. Successful solutions have many applications in multiple
scenes. One of the exceptions is the reflective ground. In this case, the
reflections on the floor resemble the true world, which confuses the obstacle
discovery and leaves navigation unsuccessful. We argue that the key to this
problem lies in obtaining discriminative features for reflections and
obstacles. Note that obstacle and reflection can be separated by the ground
plane in 3D space. With this observation, we firstly introduce a
pre-calibration based ground detection scheme that uses robot motion to predict
the ground plane. Due to the immunity of robot motion to reflection, this
scheme avoids failed ground detection caused by reflection. Given the detected
ground, we design a ground-pixel parallax to describe the location of a pixel
relative to the ground. Based on this, a unified appearance-geometry feature
representation is proposed to describe objects inside rectangular boxes.
Eventually, based on segmenting by detection framework, an appearance-geometry
fusion regressor is designed to utilize the proposed feature to discover the
obstacles. It also prevents our model from concentrating too much on parts of
obstacles instead of whole obstacles. For evaluation, we introduce a new
dataset for Obstacle on Reflective Ground (ORG), which comprises 15 scenes with
various ground reflections, a total of more than 200 image sequences and 3400
RGB images. The pixel-wise annotations of ground and obstacle provide a
comparison to our method and other methods. By reducing the misdetection of the
reflection, the proposed approach outperforms others. The source code and the
dataset will be available at
https://github.com/XuefengBUPT/IndoorObstacleDiscovery-RG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1&quot;&gt;Feng Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yicong Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianxi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ming_A/0/1/0/all/0/1&quot;&gt;Anlong Ming&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01448">
<title>ProbMCL: Simple Probabilistic Contrastive Learning for Multi-label Visual Classification. (arXiv:2401.01448v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01448</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-label image classification presents a challenging task in many domains,
including computer vision and medical imaging. Recent advancements have
introduced graph-based and transformer-based methods to improve performance and
capture label dependencies. However, these methods often include complex
modules that entail heavy computation and lack interpretability. In this paper,
we propose Probabilistic Multi-label Contrastive Learning (ProbMCL), a novel
framework to address these challenges in multi-label image classification
tasks. Our simple yet effective approach employs supervised contrastive
learning, in which samples that share enough labels with an anchor image based
on a decision threshold are introduced as a positive set. This structure
captures label dependencies by pulling positive pair embeddings together and
pushing away negative samples that fall below the threshold. We enhance
representation learning by incorporating a mixture density network into
contrastive learning and generating Gaussian mixture distributions to explore
the epistemic uncertainty of the feature encoder. We validate the effectiveness
of our framework through experimentation with datasets from the computer vision
and medical imaging domains. Our method outperforms the existing
state-of-the-art methods while achieving a low computational footprint on both
datasets. Visualization analyses also demonstrate that ProbMCL-learned
classifiers maintain a meaningful semantic topology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajedi_A/0/1/0/all/0/1&quot;&gt;Ahmad Sajedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khaki_S/0/1/0/all/0/1&quot;&gt;Samir Khaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lawryshyn_Y/0/1/0/all/0/1&quot;&gt;Yuri A. Lawryshyn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1&quot;&gt;Konstantinos N. Plataniotis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01454">
<title>A Survey on Autonomous Driving Datasets: Data Statistic, Annotation, and Outlook. (arXiv:2401.01454v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01454</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous driving has rapidly developed and shown promising performance with
recent advances in hardware and deep learning methods. High-quality datasets
are fundamental for developing reliable autonomous driving algorithms. Previous
dataset surveys tried to review the datasets but either focused on a limited
number or lacked detailed investigation of the characters of datasets. To this
end, we present an exhaustive study of over 200 autonomous driving datasets
from multiple perspectives, including sensor modalities, data size, tasks, and
contextual conditions. We introduce a novel metric to evaluate the impact of
each dataset, which can also be a guide for establishing new datasets. We
further analyze the annotation process and quality of datasets. Additionally,
we conduct an in-depth analysis of the data distribution of several vital
datasets. Finally, we discuss the development trend of the future autonomous
driving datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mingyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yurtsever_E/0/1/0/all/0/1&quot;&gt;Ekim Yurtsever&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xingcheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fossaert_J/0/1/0/all/0/1&quot;&gt;Jonathan Fossaert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yuning Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zagar_B/0/1/0/all/0/1&quot;&gt;Bare Luka Zagar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1&quot;&gt;Alois C. Knoll&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01456">
<title>ColorizeDiffusion: Adjustable Sketch Colorization with Reference Image and Text. (arXiv:2401.01456v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01456</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, diffusion models have demonstrated their effectiveness in
generating extremely high-quality images and have found wide-ranging
applications, including automatic sketch colorization. However, most existing
models use text to guide the conditional generation, with fewer attempts
exploring the potential advantages of using image tokens as conditional inputs
for networks. As such, this paper exhaustively investigates image-guided
models, specifically targeting reference-based sketch colorization, which aims
to colorize sketch images using reference color images. We investigate three
critical aspects of reference-based diffusion models: the shortcomings compared
to text-based counterparts, the training strategies, and the capability in
zero-shot, sequential text-based manipulation. We introduce two variations of
an image-guided latent diffusion model using different image tokens from the
pre-trained CLIP image encoder, and we propose corresponding manipulation
methods to adjust their results sequentially using weighted text inputs. We
conduct comprehensive evaluations of our models through qualitative and
quantitative experiments, as well as a user study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1&quot;&gt;Dingkun Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Liang Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishioka_Y/0/1/0/all/0/1&quot;&gt;Yuma Nishioka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujishiro_I/0/1/0/all/0/1&quot;&gt;Issei Fujishiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1&quot;&gt;Suguru Saito&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01461">
<title>Efficient Hybrid Zoom using Camera Fusion on Mobile Phones. (arXiv:2401.01461v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01461</link>
<description rdf:parseType="Literal">&lt;p&gt;DSLR cameras can achieve multiple zoom levels via shifting lens distances or
swapping lens types. However, these techniques are not possible on smartphone
devices due to space constraints. Most smartphone manufacturers adopt a hybrid
zoom system: commonly a Wide (W) camera at a low zoom level and a Telephoto (T)
camera at a high zoom level. To simulate zoom levels between W and T, these
systems crop and digitally upsample images from W, leading to significant
detail loss. In this paper, we propose an efficient system for hybrid zoom
super-resolution on mobile devices, which captures a synchronous pair of W and
T shots and leverages machine learning models to align and transfer details
from T to W. We further develop an adaptive blending method that accounts for
depth-of-field mismatches, scene occlusion, flow uncertainty, and alignment
errors. To minimize the domain gap, we design a dual-phone camera rig to
capture real-world inputs and ground-truths for supervised training. Our method
generates a 12-megapixel image in 500ms on a mobile platform and compares
favorably against state-of-the-art methods under extensive evaluation on
real-world scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaotong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1&quot;&gt;Wei-Sheng Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shih_Y/0/1/0/all/0/1&quot;&gt;YiChang Shih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herrmann_C/0/1/0/all/0/1&quot;&gt;Charles Herrmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krainin_M/0/1/0/all/0/1&quot;&gt;Michael Krainin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1&quot;&gt;Deqing Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Chia-Kai Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01470">
<title>Token Propagation Controller for Efficient Vision Transformer. (arXiv:2401.01470v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01470</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers (ViTs) have achieved promising results on a variety of
Computer Vision tasks, however their quadratic complexity in the number of
input tokens has limited their application specially in resource-constrained
settings. Previous approaches that employ gradual token reduction to address
this challenge assume that token redundancy in one layer implies redundancy in
all the following layers. We empirically demonstrate that this assumption is
often not correct, i.e., tokens that are redundant in one layer can be useful
in later layers. We employ this key insight to propose a novel token
propagation controller (TPC) that incorporates two different
token-distributions, i.e., pause probability and restart probability to control
the reduction and reuse of tokens respectively, which results in more efficient
token utilization. To improve the estimates of token distributions, we propose
a smoothing mechanism that acts as a regularizer and helps remove noisy
outliers. Furthermore, to improve the training-stability of our proposed TPC,
we introduce a model stabilizer that is able to implicitly encode local image
structures and minimize accuracy fluctuations during model training. We present
extensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT
and Swin models to demonstrate the effectiveness of our proposed method. For
example, compared to baseline models, our proposed method improves the
inference speed of the DeiT-S by 250% while increasing the classification
accuracy by 1.0%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wentao Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01482">
<title>Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition. (arXiv:2401.01482v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01482</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing object recognition models have been shown to lack robustness in
diverse geographical scenarios due to significant domain shifts in design and
context. Class representations need to be adapted to more accurately reflect an
object concept under these shifts. In the absence of training data from target
geographies, we hypothesize that geography-specific descriptive knowledge of
object categories can be leveraged to enhance robustness. For this purpose, we
explore the feasibility of probing a large-language model for
geography-specific object knowledge, and we investigate integrating knowledge
in zero-shot and learnable soft prompting with the CLIP vision-language model.
In particular, we propose a geography knowledge regularization method to ensure
that soft prompts trained on a source set of geographies generalize to an
unseen target set of geographies. Our gains on DollarStreet when generalizing
from a model trained only on data from Europe are as large as +2.8 on countries
from Africa, and +4.6 on the hardest classes. We further show competitive
performance vs. few-shot target training, and provide insights into how
descriptive knowledge captures geographical differences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buettner_K/0/1/0/all/0/1&quot;&gt;Kyle Buettner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malakouti_S/0/1/0/all/0/1&quot;&gt;Sina Malakouti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Lorraine Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1&quot;&gt;Adriana Kovashka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01496">
<title>From Pixel to Slide image: Polarization Modality-based Pathological Diagnosis Using Representation Learning. (arXiv:2401.01496v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.01496</link>
<description rdf:parseType="Literal">&lt;p&gt;Thyroid cancer is the most common endocrine malignancy, and accurately
distinguishing between benign and malignant thyroid tumors is crucial for
developing effective treatment plans in clinical practice. Pathologically,
thyroid tumors pose diagnostic challenges due to improper specimen sampling. In
this study, we have designed a three-stage model using representation learning
to integrate pixel-level and slice-level annotations for distinguishing thyroid
tumors. This structure includes a pathology structure recognition method to
predict structures related to thyroid tumors, an encoder-decoder network to
extract pixel-level annotation information by learning the feature
representations of image blocks, and an attention-based learning mechanism for
the final classification task. This mechanism learns the importance of
different image blocks in a pathological region, globally considering the
information from each block. In the third stage, all information from the image
blocks in a region is aggregated using attention mechanisms, followed by
classification to determine the category of the region. Experimental results
demonstrate that our proposed method can predict microscopic structures more
accurately. After color-coding, the method achieves results on unstained
pathology slides that approximate the quality of Hematoxylin and eosin
staining, reducing the need for stained pathology slides. Furthermore, by
leveraging the concept of indirect measurement and extracting polarized
features from structures correlated with lesions, the proposed method can also
classify samples where membrane structures cannot be obtained through sampling,
providing a potential objective and highly accurate indirect diagnostic
technique for thyroid tumors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jia Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yao Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Hui Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01505">
<title>Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports. (arXiv:2401.01505v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01505</link>
<description rdf:parseType="Literal">&lt;p&gt;Reasoning over sports videos for question answering is an important task with
numerous applications, such as player training and information retrieval.
However, this task has not been explored due to the lack of relevant datasets
and the challenging nature it presents. Most datasets for video question
answering (VideoQA) focus mainly on general and coarse-grained understanding of
daily-life videos, which is not applicable to sports scenarios requiring
professional action understanding and fine-grained motion analysis. In this
paper, we introduce the first dataset, named Sports-QA, specifically designed
for the sports VideoQA task. The Sports-QA dataset includes various types of
questions, such as descriptions, chronologies, causalities, and counterfactual
conditions, covering multiple sports. Furthermore, to address the
characteristics of the sports VideoQA task, we propose a new Auto-Focus
Transformer (AFT) capable of automatically focusing on particular scales of
temporal information for question answering. We conduct extensive experiments
on Sports-QA, including baseline studies and the evaluation of different
methods. The results demonstrate that our AFT achieves state-of-the-art
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haopeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_A/0/1/0/all/0/1&quot;&gt;Andong Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1&quot;&gt;Qiuhong Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1&quot;&gt;Hossein Rahmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yulan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1&quot;&gt;Bernt Schiele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01510">
<title>Answering from Sure to Uncertain: Uncertainty-Aware Curriculum Learning for Video Question Answering. (arXiv:2401.01510v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01510</link>
<description rdf:parseType="Literal">&lt;p&gt;While significant advancements have been made in video question answering
(VideoQA), the potential benefits of enhancing model generalization through
tailored difficulty scheduling have been largely overlooked in existing
research. This paper seeks to bridge that gap by incorporating VideoQA into a
curriculum learning (CL) framework that progressively trains models from
simpler to more complex data. Recognizing that conventional self-paced CL
methods rely on training loss for difficulty measurement, which might not
accurately reflect the intricacies of video-question pairs, we introduce the
concept of uncertainty-aware CL. Here, uncertainty serves as the guiding
principle for dynamically adjusting the difficulty. Furthermore, we address the
challenge posed by uncertainty by presenting a probabilistic modeling approach
for VideoQA. Specifically, we conceptualize VideoQA as a stochastic computation
graph, where the hidden representations are treated as stochastic variables.
This yields two distinct types of uncertainty: one related to the inherent
uncertainty in the data and another pertaining to the model&apos;s confidence. In
practice, we seamlessly integrate the VideoQA model into our framework and
conduct comprehensive experiments. The findings affirm that our approach not
only achieves enhanced performance but also effectively quantifies uncertainty
in the context of VideoQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haopeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1&quot;&gt;Qiuhong Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1&quot;&gt;Mingming Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drummond_T/0/1/0/all/0/1&quot;&gt;Tom Drummond&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01520">
<title>S$^{2}$-DMs:Skip-Step Diffusion Models. (arXiv:2401.01520v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01520</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have emerged as powerful generative tools, rivaling GANs in
sample quality and mirroring the likelihood scores of autoregressive models. A
subset of these models, exemplified by DDIMs, exhibit an inherent asymmetry:
they are trained over $T$ steps but only sample from a subset of $T$ during
generation. This selective sampling approach, though optimized for speed,
inadvertently misses out on vital information from the unsampled steps, leading
to potential compromises in sample quality. To address this issue, we present
the S$^{2}$-DMs, which is a new training method by using an innovative
$L_{skip}$, meticulously designed to reintegrate the information omitted during
the selective sampling phase. The benefits of this approach are manifold: it
notably enhances sample quality, is exceptionally simple to implement, requires
minimal code modifications, and is flexible enough to be compatible with
various sampling algorithms. On the CIFAR10 dataset, models trained using our
algorithm showed an improvement of 3.27% to 14.06% over models trained with
traditional methods across various sampling algorithms (DDIMs, PNDMs, DEIS) and
different numbers of sampling steps (10, 20, ..., 1000). On the CELEBA dataset,
the improvement ranged from 8.97% to 27.08%. Access to the code and additional
resources is provided in the github.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yixuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuangyin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01522">
<title>LORE++: Logical Location Regression Network for Table Structure Recognition with Pre-training. (arXiv:2401.01522v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01522</link>
<description rdf:parseType="Literal">&lt;p&gt;Table structure recognition (TSR) aims at extracting tables in images into
machine-understandable formats. Recent methods solve this problem by predicting
the adjacency relations of detected cell boxes or learning to directly generate
the corresponding markup sequences from the table images. However, existing
approaches either count on additional heuristic rules to recover the table
structures, or face challenges in capturing long-range dependencies within
tables, resulting in increased complexity. In this paper, we propose an
alternative paradigm. We model TSR as a logical location regression problem and
propose a new TSR framework called LORE, standing for LOgical location
REgression network, which for the first time regresses logical location as well
as spatial location of table cells in a unified network. Our proposed LORE is
conceptually simpler, easier to train, and more accurate than other paradigms
of TSR. Moreover, inspired by the persuasive success of pre-trained models on a
number of computer vision and natural language processing tasks, we propose two
pre-training tasks to enrich the spatial and logical representations at the
feature level of LORE, resulting in an upgraded version called LORE++. The
incorporation of pre-training in LORE++ has proven to enjoy significant
advantages, leading to a substantial enhancement in terms of accuracy,
generalization, and few-shot capability compared to its predecessor.
Experiments on standard benchmarks against methods of previous paradigms
demonstrate the superiority of LORE++, which highlights the potential and
promising prospect of the logical location regression paradigm for TSR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_R/0/1/0/all/0/1&quot;&gt;Rujiao Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_H/0/1/0/all/0/1&quot;&gt;Hangdi Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhibo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1&quot;&gt;Cong Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01524">
<title>Multimodal self-supervised learning for lesion localization. (arXiv:2401.01524v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01524</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal deep learning utilizing imaging and diagnostic reports has made
impressive progress in the field of medical imaging diagnostics, demonstrating
a particularly strong capability for auxiliary diagnosis in cases where
sufficient annotation information is lacking. Nonetheless, localizing diseases
accurately without detailed positional annotations remains a challenge.
Although existing methods have attempted to utilize local information to
achieve fine-grained semantic alignment, their capability in extracting the
fine-grained semantics of the comprehensive contextual within reports is
limited. To solve this problem, we introduce a new method that takes full
sentences from textual reports as the basic units for local semantic alignment.
Our approach combines chest X-ray images with their corresponding textual
reports, performing contrastive learning at both global and local levels. The
leading results obtained by our method on multiple datasets confirm its
efficacy in the task of lesion localization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hong-Yu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Cheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weijian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiarun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shanshan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01529">
<title>Glance and Focus: Memory Prompting for Multi-Event Video Question Answering. (arXiv:2401.01529v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01529</link>
<description rdf:parseType="Literal">&lt;p&gt;Video Question Answering (VideoQA) has emerged as a vital tool to evaluate
agents&apos; ability to understand human daily behaviors. Despite the recent success
of large vision language models in many multi-modal tasks, complex situation
reasoning over videos involving multiple human-object interaction events still
remains challenging. In contrast, humans can easily tackle it by using a series
of episode memories as anchors to quickly locate question-related key moments
for reasoning. To mimic this effective reasoning strategy, we propose the
Glance-Focus model. One simple way is to apply an action detection model to
predict a set of actions as key memories. However, these actions within a
closed set vocabulary are hard to generalize to various video domains. Instead
of that, we train an Encoder-Decoder to generate a set of dynamic event
memories at the glancing stage. Apart from using supervised bipartite matching
to obtain the event memories, we further design an unsupervised memory
generation method to get rid of dependence on event annotations. Next, at the
focusing stage, these event memories act as a bridge to establish the
correlation between the questions with high-level event concepts and low-level
lengthy video content. Given the question, the model first focuses on the
generated key event memory, then focuses on the most relevant moment for
reasoning through our designed multi-level cross-attention mechanism. We
conduct extensive experiments on four Multi-Event VideoQA benchmarks including
STAR, EgoTaskQA, AGQA, and NExT-QA. Our proposed model achieves
state-of-the-art results, surpassing current large models in various
challenging reasoning tasks. The code and models are available at
https://github.com/ByZ0e/Glance-Focus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1&quot;&gt;Ziyi Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruiping Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xilin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01539">
<title>DDPM based X-ray Image Synthesizer. (arXiv:2401.01539v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.01539</link>
<description rdf:parseType="Literal">&lt;p&gt;Access to high-quality datasets in the medical industry limits machine
learning model performance. To address this issue, we propose a Denoising
Diffusion Probabilistic Model (DDPM) combined with a UNet architecture for
X-ray image synthesis. Focused on pneumonia medical condition, our methodology
employs over 3000 pneumonia X-ray images obtained from Kaggle for training.
Results demonstrate the effectiveness of our approach, as the model
successfully generated realistic images with low Mean Squared Error (MSE). The
synthesized images showed distinct differences from non-pneumonia images,
highlighting the model&apos;s ability to capture key features of positive cases.
Beyond pneumonia, the applications of this synthesizer extend to various
medical conditions, provided an ample dataset is available. The capability to
produce high-quality images can potentially enhance machine learning models&apos;
performance, aiding in more accurate and efficient medical diagnoses. This
innovative DDPM-based X-ray photo synthesizer presents a promising avenue for
addressing the scarcity of positive medical image datasets, paving the way for
improved medical image analysis and diagnosis in the healthcare industry.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mahaulpatha_P/0/1/0/all/0/1&quot;&gt;Praveen Mahaulpatha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Abeywardane_T/0/1/0/all/0/1&quot;&gt;Thulana Abeywardane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+George_T/0/1/0/all/0/1&quot;&gt;Tomson George&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01543">
<title>Retraining-free Model Quantization via One-Shot Weight-Coupling Learning. (arXiv:2401.01543v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01543</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantization is of significance for compressing the over-parameterized deep
neural models and deploying them on resource-limited devices. Fixed-precision
quantization suffers from performance drop due to the limited numerical
representation ability. Conversely, mixed-precision quantization (MPQ) is
advocated to compress the model effectively by allocating heterogeneous
bit-width for layers. MPQ is typically organized into a searching-retraining
two-stage process. Previous works only focus on determining the optimal
bit-width configuration in the first stage efficiently, while ignoring the
considerable time costs in the second stage. However, retraining always
consumes hundreds of GPU-hours on the cutting-edge GPUs, thus hindering
deployment efficiency significantly. In this paper, we devise a one-shot
training-searching paradigm for mixed-precision model compression.
Specifically, in the first stage, all potential bit-width configurations are
coupled and thus optimized simultaneously within a set of shared weights.
However, our observations reveal a previously unseen and severe bit-width
interference phenomenon among highly coupled weights during optimization,
leading to considerable performance degradation under a high compression ratio.
To tackle this problem, we first design a bit-width scheduler to dynamically
freeze the most turbulent bit-width of layers during training, to ensure the
rest bit-widths converged properly. Then, taking inspiration from information
theory, we present an information distortion mitigation technique to align the
behaviour of the bad-performing bit-widths to the well-performing ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chen Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1&quot;&gt;Yuan Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jiacheng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Shuzhao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1&quot;&gt;Rongwei Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xinzhu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wenwu Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01544">
<title>Collaborative Perception for Connected and Autonomous Driving: Challenges, Possible Solutions and Opportunities. (arXiv:2401.01544v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01544</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous driving has attracted significant attention from both academia and
industries, which is expected to offer a safer and more efficient driving
system. However, current autonomous driving systems are mostly based on a
single vehicle, which has significant limitations which still poses threats to
driving safety. Collaborative perception with connected and autonomous vehicles
(CAVs) shows a promising solution to overcoming these limitations. In this
article, we first identify the challenges of collaborative perception, such as
data sharing asynchrony, data volume, and pose errors. Then, we discuss the
possible solutions to address these challenges with various technologies, where
the research opportunities are also elaborated. Furthermore, we propose a
scheme to deal with communication efficiency and latency problems, which is a
channel-aware collaborative perception framework to dynamically adjust the
communication graph and minimize latency, thereby improving perception
performance while increasing communication efficiency. Finally, we conduct
experiments to demonstrate the effectiveness of our proposed scheme.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Senkang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zhengru Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yiqin Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xianhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yuguang Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01545">
<title>DDN-SLAM: Real-time Dense Dynamic Neural Implicit SLAM with Joint Semantic Encoding. (arXiv:2401.01545v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01545</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose DDN-SLAM, a real-time dense neural implicit semantic SLAM system
designed for dynamic scenes. While existing neural implicit SLAM systems
perform well in static scenes, they often encounter challenges in real-world
environments with dynamic interferences, leading to ineffective tracking and
mapping. DDN-SLAM utilizes the priors provided by the deep semantic system,
combined with conditional probability fields, for segmentation.By constructing
depth-guided static masks and employing joint multi-resolution hashing
encoding, we ensure fast hole filling and high-quality mapping while mitigating
the effects of dynamic information interference. To enhance tracking
robustness, we utilize sparse feature points validated with optical flow and
keyframes, enabling loop closure detection and global bundle optimization.
Furthermore, DDN-SLAM supports monocular, stereo, and RGB-D inputs, operating
robustly at a frequency of 20-30Hz. Extensive experiments on 6 virtual/real
datasets demonstrate that our method outperforms state-of-the-art approaches in
both dynamic and static scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mingrui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jiaming He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1&quot;&gt;Guangan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongyu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01548">
<title>Boosting of Implicit Neural Representation-based Image Denoiser. (arXiv:2401.01548v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01548</link>
<description rdf:parseType="Literal">&lt;p&gt;Implicit Neural Representation (INR) has emerged as an effective method for
unsupervised image denoising. However, INR models are typically
overparameterized; consequently, these models are prone to overfitting during
learning, resulting in suboptimal results, even noisy ones. To tackle this
problem, we propose a general recipe for regularizing INR models in image
denoising. In detail, we propose to iteratively substitute the supervision
signal with the mean value derived from both the prediction and supervision
signal during the learning process. We theoretically prove that such a simple
iterative substitute can gradually enhance the signal-to-noise ratio of the
supervision signal, thereby benefiting INR models during the learning process.
Our experimental results demonstrate that INR models can be effectively
regularized by the proposed approach, relieving overfitting and boosting image
denoising performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zipei Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengji Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jizhou Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01552">
<title>CRA-PCN: Point Cloud Completion with Intra- and Inter-level Cross-Resolution Transformers. (arXiv:2401.01552v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01552</link>
<description rdf:parseType="Literal">&lt;p&gt;Point cloud completion is an indispensable task for recovering complete point
clouds due to incompleteness caused by occlusion, limited sensor resolution,
etc. The family of coarse-to-fine generation architectures has recently
exhibited great success in point cloud completion and gradually became
mainstream. In this work, we unveil one of the key ingredients behind these
methods: meticulously devised feature extraction operations with explicit
cross-resolution aggregation. We present Cross-Resolution Transformer that
efficiently performs cross-resolution aggregation with local attention
mechanisms. With the help of our recursive designs, the proposed operation can
capture more scales of features than common aggregation operations, which is
beneficial for capturing fine geometric characteristics. While prior
methodologies have ventured into various manifestations of inter-level
cross-resolution aggregation, the effectiveness of intra-level one and their
combination has not been analyzed. With unified designs, Cross-Resolution
Transformer can perform intra- or inter-level cross-resolution aggregation by
switching inputs. We integrate two forms of Cross-Resolution Transformers into
one up-sampling block for point generation, and following the coarse-to-fine
manner, we construct CRA-PCN to incrementally predict complete shapes with
stacked up-sampling blocks. Extensive experiments demonstrate that our method
outperforms state-of-the-art methods by a large margin on several widely used
benchmarks. Codes are available at https://github.com/EasyRy/CRA-PCN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1&quot;&gt;Yi Rong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Haoran Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Lixin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_C/0/1/0/all/0/1&quot;&gt;Cheng Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiahao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Tong Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01553">
<title>Multi-modal Learning with Missing Modality in Predicting Axillary Lymph Node Metastasis. (arXiv:2401.01553v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.01553</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal Learning has attracted widespread attention in medical image
analysis. Using multi-modal data, whole slide images (WSIs) and clinical
information, can improve the performance of deep learning models in the
diagnosis of axillary lymph node metastasis. However, clinical information is
not easy to collect in clinical practice due to privacy concerns, limited
resources, lack of interoperability, etc. Although patient selection can ensure
the training set to have multi-modal data for model development, missing
modality of clinical information can appear during test. This normally leads to
performance degradation, which limits the use of multi-modal models in the
clinic. To alleviate this problem, we propose a bidirectional distillation
framework consisting of a multi-modal branch and a single-modal branch. The
single-modal branch acquires the complete multi-modal knowledge from the
multi-modal branch, while the multi-modal learns the robust features of WSI
from the single-modal. We conduct experiments on a public dataset of Lymph Node
Metastasis in Early Breast Cancer to validate the method. Our approach not only
achieves state-of-the-art performance with an AUC of 0.861 on the test set
without missing data, but also yields an AUC of 0.842 when the rate of missing
modality is 80\%. This shows the effectiveness of the approach in dealing with
multi-modal data and missing modality. Such a model has the potential to
improve treatment decision-making for early breast cancer patients who have
axillary lymph node metastatic status.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shichuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Sunyi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shui_Z/0/1/0/all/0/1&quot;&gt;Zhongyi Shui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Honglin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01558">
<title>One-Step Late Fusion Multi-view Clustering with Compressed Subspace. (arXiv:2401.01558v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01558</link>
<description rdf:parseType="Literal">&lt;p&gt;Late fusion multi-view clustering (LFMVC) has become a rapidly growing class
of methods in the multi-view clustering (MVC) field, owing to its excellent
computational speed and clustering performance. One bottleneck faced by
existing late fusion methods is that they are usually aligned to the average
kernel function, which makes the clustering performance highly dependent on the
quality of datasets. Another problem is that they require subsequent k-means
clustering after obtaining the consensus partition matrix to get the final
discrete labels, and the resulting separation of the label learning and cluster
structure optimization processes limits the integrity of these models. To
address the above issues, we propose an integrated framework named One-Step
Late Fusion Multi-view Clustering with Compressed Subspace (OS-LFMVC-CS).
Specifically, we use the consensus subspace to align the partition matrix while
optimizing the partition fusion, and utilize the fused partition matrix to
guide the learning of discrete labels. A six-step iterative optimization
approach with verified convergence is proposed. Sufficient experiments on
multiple datasets validate the effectiveness and efficiency of our proposed
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ou_Q/0/1/0/all/0/1&quot;&gt;Qiyuan Ou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Sihang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1&quot;&gt;En Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01569">
<title>AttentionLut: Attention Fusion-based Canonical Polyadic LUT for Real-time Image Enhancement. (arXiv:2401.01569v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01569</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, many algorithms have employed image-adaptive lookup tables (LUTs)
to achieve real-time image enhancement. Nonetheless, a prevailing trend among
existing methods has been the employment of linear combinations of basic LUTs
to formulate image-adaptive LUTs, which limits the generalization ability of
these methods. To address this limitation, we propose a novel framework named
AttentionLut for real-time image enhancement, which utilizes the attention
mechanism to generate image-adaptive LUTs. Our proposed framework consists of
three lightweight modules. We begin by employing the global image context
feature module to extract image-adaptive features. Subsequently, the attention
fusion module integrates the image feature with the priori attention feature
obtained during training to generate image-adaptive canonical polyadic tensors.
Finally, the canonical polyadic reconstruction module is deployed to
reconstruct image-adaptive residual 3DLUT, which is subsequently utilized for
enhancing input images. Experiments on the benchmark MIT-Adobe FiveK dataset
demonstrate that the proposed method achieves better enhancement performance
quantitatively and qualitatively than the state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1&quot;&gt;Kang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yicong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zicheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qihang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaohong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1&quot;&gt;Guangtao Zhai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01573">
<title>View Distribution Alignment with Progressive Adversarial Learning for UAV Visual Geo-Localization. (arXiv:2401.01573v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01573</link>
<description rdf:parseType="Literal">&lt;p&gt;Unmanned Aerial Vehicle (UAV) visual geo-localization aims to match images of
the same geographic target captured from different views, i.e., the UAV view
and the satellite view. It is very challenging due to the large appearance
differences in UAV-satellite image pairs. Previous works map images captured by
UAVs and satellites to a shared feature space and employ a classification
framework to learn location-dependent features while neglecting the overall
distribution shift between the UAV view and the satellite view. In this paper,
we address these limitations by introducing distribution alignment of the two
views to shorten their distance in a common space. Specifically, we propose an
end-to-end network, called PVDA (Progressive View Distribution Alignment).
During training, feature encoder, location classifier, and view discriminator
are jointly optimized by a novel progressive adversarial learning strategy.
Competition between feature encoder and view discriminator prompts both of them
to be stronger. It turns out that the adversarial learning is progressively
emphasized until UAV-view images are indistinguishable from satellite-view
images. As a result, the proposed PVDA becomes powerful in learning
location-dependent yet view-invariant features with good scalability towards
unseen images of new locations. Compared to the state-of-the-art methods, the
proposed PVDA requires less inference time but has achieved superior
performance on the University-1652 dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cuiwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiahao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1&quot;&gt;Huaijun Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhaokui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xiangbin Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01574">
<title>A Transformer-Based Adaptive Semantic Aggregation Method for UAV Visual Geo-Localization. (arXiv:2401.01574v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01574</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the task of Unmanned Aerial Vehicles (UAV) visual
geo-localization, which aims to match images of the same geographic target
taken by different platforms, i.e., UAVs and satellites. In general, the key to
achieving accurate UAV-satellite image matching lies in extracting visual
features that are robust against viewpoint changes, scale variations, and
rotations. Current works have shown that part matching is crucial for UAV
visual geo-localization since part-level representations can capture image
details and help to understand the semantic information of scenes. However, the
importance of preserving semantic characteristics in part-level representations
is not well discussed. In this paper, we introduce a transformer-based adaptive
semantic aggregation method that regards parts as the most representative
semantics in an image. Correlations of image patches to different parts are
learned in terms of the transformer&apos;s feature map. Then our method decomposes
part-level features into an adaptive sum of all patch features. By doing this,
the learned parts are encouraged to focus on patches with typical semantics.
Extensive experiments on the University-1652 dataset have shown the superiority
of our method over the current works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shishen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cuiwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1&quot;&gt;Huaijun Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhaokui Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01575">
<title>Enhancing Generalization of Invisible Facial Privacy Cloak via Gradient Accumulation. (arXiv:2401.01575v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01575</link>
<description rdf:parseType="Literal">&lt;p&gt;The blooming of social media and face recognition (FR) systems has increased
people&apos;s concern about privacy and security. A new type of adversarial privacy
cloak (class-universal) can be applied to all the images of regular users, to
prevent malicious FR systems from acquiring their identity information. In this
work, we discover the optimization dilemma in the existing methods -- the local
optima problem in large-batch optimization and the gradient information
elimination problem in small-batch optimization. To solve these problems, we
propose Gradient Accumulation (GA) to aggregate multiple small-batch gradients
into a one-step iterative gradient to enhance the gradient stability and reduce
the usage of quantization operations. Experiments show that our proposed method
achieves high performance on the Privacy-Commons dataset against black-box face
recognition models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuannan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yaoyao Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1&quot;&gt;Weihong Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Hongzhi Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1&quot;&gt;Xingchen Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1&quot;&gt;Yunfeng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_D/0/1/0/all/0/1&quot;&gt;Dongchao Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01577">
<title>Test-Time Personalization with Meta Prompt for Gaze Estimation. (arXiv:2401.01577v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01577</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the recent remarkable achievement in gaze estimation, efficient and
accurate personalization of gaze estimation without labels is a practical
problem but rarely touched on in the literature. To achieve efficient
personalization, we take inspiration from the recent advances in Natural
Language Processing (NLP) by updating a negligible number of parameters,
&quot;prompts&quot;, at the test time. Specifically, the prompt is additionally attached
without perturbing original network and can contain less than 1% of a
ResNet-18&apos;s parameters. Our experiments show high efficiency of the prompt
tuning approach. The proposed one can be 10 times faster in terms of adaptation
speed than the methods compared. However, it is non-trivial to update the
prompt for personalized gaze estimation without labels. At the test time, it is
essential to ensure that the minimizing of particular unsupervised loss leads
to the goals of minimizing gaze estimation error. To address this difficulty,
we propose to meta-learn the prompt to ensure that its updates align with the
goal. Our experiments show that the meta-learned prompt can be effectively
adapted even with a simple symmetry loss. In addition, we experiment on four
cross-dataset validations to show the remarkable advantages of the proposed
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1&quot;&gt;Julia Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassanpour_M/0/1/0/all/0/1&quot;&gt;Mohammad Hassanpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1&quot;&gt;Konstantinos Plataniotis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yuanhao Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01578">
<title>Context-Guided Spatio-Temporal Video Grounding. (arXiv:2401.01578v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01578</link>
<description rdf:parseType="Literal">&lt;p&gt;Spatio-temporal video grounding (or STVG) task aims at locating a
spatio-temporal tube for a specific instance given a text query. Despite
advancements, current methods easily suffer the distractors or heavy object
appearance variations in videos due to insufficient object information from the
text, leading to degradation. Addressing this, we propose a novel framework,
context-guided STVG (CG-STVG), which mines discriminative instance context for
object in videos and applies it as a supplementary guidance for target
localization. The key of CG-STVG lies in two specially designed modules,
including instance context generation (ICG), which focuses on discovering
visual context information (in both appearance and motion) of the instance, and
instance context refinement (ICR), which aims to improve the instance context
from ICG by eliminating irrelevant or even harmful information from the
context. During grounding, ICG, together with ICR, are deployed at each
decoding stage of a Transformer architecture for instance context learning.
Particularly, instance context learned from one decoding stage is fed to the
next stage, and leveraged as a guidance containing rich and discriminative
object feature to enhance the target-awareness in decoding feature, which
conversely benefits generating better new instance context for improving
localization finally. Compared to existing methods, CG-STVG enjoys object
information in text query and guidance from mined instance visual context for
more accurate target localization. In our experiments on three benchmarks,
including HCSTVG-v1/-v2 and VidSTG, CG-STVG sets new state-of-the-arts in
m_tIoU and m_vIoU on all of them, showing its efficacy. The code will be
released at https://github.com/HengLan/CGSTVG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Heng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1&quot;&gt;Tiejian Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Libo Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01583">
<title>Enhancing the medical foundation model with multi-scale and cross-modality feature learning. (arXiv:2401.01583v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01583</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of multi-modal medical foundation models has attracted
significant attention in the field of medicine and healthcare due to their
promising prospects in various clinical applications. One area of focus in this
research direction is the extractions of features at different scales. While
previous studies have explored feature learning at individual scales,
investigation on integrating the diverse scales and modalities of information
is lacking, which may hinder the potential for mutual reinforcement among these
features. This paper aims to bridge this gap by proposing a method that
effectively exploits multi-scale and cross-modality information to enhance the
performance of medical foundation models. The proposed method simultaneously
exploit features at the local, instance, modality and global aspects,
facilitating comprehensive representation learning within the models. We
evaluate the effectiveness of the proposed method on six open-source datasets
across different clinical tasks, demonstrating its ability to enhance the
performance of medical foundation models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weijian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Cheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hong-Yu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiarun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shanshan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01587">
<title>Real-Time Human Fall Detection using a Lightweight Pose Estimation Technique. (arXiv:2401.01587v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01587</link>
<description rdf:parseType="Literal">&lt;p&gt;The elderly population is increasing rapidly around the world. There are no
enough caretakers for them. Use of AI-based in-home medical care systems is
gaining momentum due to this. Human fall detection is one of the most important
tasks of medical care system for the aged people. Human fall is a common
problem among elderly people. Detection of a fall and providing medical help as
early as possible is very important to reduce any further complexity. The
chances of death and other medical complications can be reduced by detecting
and providing medical help as early as possible after the fall. There are many
state-of-the-art fall detection techniques available these days, but the
majority of them need very high computing power. In this paper, we proposed a
lightweight and fast human fall detection system using pose estimation. We used
`Movenet&apos; for human joins key-points extraction. Our proposed method can work
in real-time on any low-computing device with any basic camera. All computation
can be processed locally, so there is no problem of privacy of the subject. We
used two datasets `GMDCSA&apos; and `URFD&apos; for the experiment. We got the
sensitivity value of 0.9375 and 0.9167 for the dataset `GMDCSA&apos; and `URFD&apos;
respectively. The source code and the dataset GMDCSA of our work are available
online to access.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_E/0/1/0/all/0/1&quot;&gt;Ekram Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sufian_A/0/1/0/all/0/1&quot;&gt;Abu Sufian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_P/0/1/0/all/0/1&quot;&gt;Paramartha Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leo_M/0/1/0/all/0/1&quot;&gt;Marco Leo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01591">
<title>MLIP: Medical Language-Image Pre-training with Masked Local Representation Learning. (arXiv:2401.01591v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01591</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing contrastive language-image pre-training aims to learn a joint
representation by matching abundant image-text pairs. However, the number of
image-text pairs in medical datasets is usually orders of magnitude smaller
than that in natural datasets. Besides, medical image-text pairs often involve
numerous complex fine-grained correspondences. This paper aims to enhance the
data efficiency by introducing multiple-to-multiple local relationship modeling
to capture denser supervisions. More specifically, we propose a Medical
Language-Image Pre-training (MLIP) framework, which exploits the limited
image-text medical data more efficiently through patch-sentence matching.
Furthermore, we introduce a masked contrastive learning strategy with semantic
integrity estimation to reduce redundancy in images while preserving the
underlying semantics. Our evaluation results show that MLIP outperforms
previous work in zero/few-shot classification and few-shot segmentation tasks
by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiarun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hong-Yu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Cheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weijian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shanshan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01598">
<title>Learning Prompt with Distribution-Based Feature Replay for Few-Shot Class-Incremental Learning. (arXiv:2401.01598v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01598</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot Class-Incremental Learning (FSCIL) aims to continuously learn new
classes based on very limited training data without forgetting the old ones
encountered. Existing studies solely relied on pure visual networks, while in
this paper we solved FSCIL by leveraging the Vision-Language model (e.g., CLIP)
and propose a simple yet effective framework, named Learning Prompt with
Distribution-based Feature Replay (LP-DiF). We observe that simply using CLIP
for zero-shot evaluation can substantially outperform the most influential
methods. Then, prompt tuning technique is involved to further improve its
adaptation ability, allowing the model to continually capture specific
knowledge from each session. To prevent the learnable prompt from forgetting
old knowledge in the new session, we propose a pseudo-feature replay approach.
Specifically, we preserve the old knowledge of each class by maintaining a
feature-level Gaussian distribution with a diagonal covariance matrix, which is
estimated by the image features of training images and synthesized features
generated from a VAE. When progressing to a new session, pseudo-features are
sampled from old-class distributions combined with training images of the
current session to optimize the prompt, thus enabling the model to learn new
knowledge while retaining old knowledge. Experiments on three prevalent
benchmarks, i.e., CIFAR100, mini-ImageNet, CUB-200, and two more challenging
benchmarks, i.e., SUN-397 and CUB-200$^*$ proposed in this paper showcase the
superiority of LP-DiF, achieving new state-of-the-art (SOTA) in FSCIL. Code is
publicly available at https://github.&lt;a href=&quot;/abs/com/1170300&quot;&gt;com/1170300&lt;/a&gt;714/LP-DiF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zitong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Ze Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhixing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1&quot;&gt;Erjin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xinxing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1&quot;&gt;Rick Siow Mong Goh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1&quot;&gt;Chunmei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wangmeng Zuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01614">
<title>GPT-4V(ision) is a Generalist Web Agent, if Grounded. (arXiv:2401.01614v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2401.01614</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent development on large multimodal models (LMMs), especially
GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries
of multimodal models beyond traditional tasks like image captioning and visual
question answering. In this work, we explore the potential of LMMs like GPT-4V
as a generalist web agent that can follow natural language instructions to
complete tasks on any given website. We propose SEEACT, a generalist web agent
that harnesses the power of LMMs for integrated visual understanding and acting
on the web. We evaluate on the recent MIND2WEB benchmark. In addition to
standard offline evaluation on cached websites, we enable a new online
evaluation setting by developing a tool that allows running web agents on live
websites. We show that GPT-4V presents a great potential for web agents - it
can successfully complete 50% of the tasks on live websites if we manually
ground its textual plans into actions on the websites. This substantially
outperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)
specifically fine-tuned for web agents. However, grounding still remains a
major challenge. Existing LMM grounding strategies like set-of-mark prompting
turns out not effective for web agents, and the best grounding strategy we
develop in this paper leverages both the HTML text and visuals. Yet, there is
still a substantial gap with oracle grounding, leaving ample room for further
improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1&quot;&gt;Boyuan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gou_B/0/1/0/all/0/1&quot;&gt;Boyu Gou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kil_J/0/1/0/all/0/1&quot;&gt;Jihyung Kil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Huan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yu Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01624">
<title>Context-Aware Interaction Network for RGB-T Semantic Segmentation. (arXiv:2401.01624v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01624</link>
<description rdf:parseType="Literal">&lt;p&gt;RGB-T semantic segmentation is a key technique for autonomous driving scenes
understanding. For the existing RGB-T semantic segmentation methods, however,
the effective exploration of the complementary relationship between different
modalities is not implemented in the information interaction between multiple
levels. To address such an issue, the Context-Aware Interaction Network
(CAINet) is proposed for RGB-T semantic segmentation, which constructs
interaction space to exploit auxiliary tasks and global context for explicitly
guided learning. Specifically, we propose a Context-Aware Complementary
Reasoning (CACR) module aimed at establishing the complementary relationship
between multimodal features with the long-term context in both spatial and
channel dimensions. Further, considering the importance of global contextual
and detailed information, we propose the Global Context Modeling (GCM) module
and Detail Aggregation (DA) module, and we introduce specific auxiliary
supervision to explicitly guide the context interaction and refine the
segmentation map. Extensive experiments on two benchmark datasets of MFNet and
PST900 demonstrate that the proposed CAINet achieves state-of-the-art
performance. The code is available at https://github.com/YingLv1106/CAINet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1&quot;&gt;Ying Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gongyang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01642">
<title>BLADE: Box-Level Supervised Amodal Segmentation through Directed Expansion. (arXiv:2401.01642v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01642</link>
<description rdf:parseType="Literal">&lt;p&gt;Perceiving the complete shape of occluded objects is essential for human and
machine intelligence. While the amodal segmentation task is to predict the
complete mask of partially occluded objects, it is time-consuming and
labor-intensive to annotate the pixel-level ground truth amodal masks.
Box-level supervised amodal segmentation addresses this challenge by relying
solely on ground truth bounding boxes and instance classes as supervision,
thereby alleviating the need for exhaustive pixel-level annotations.
Nevertheless, current box-level methodologies encounter limitations in
generating low-resolution masks and imprecise boundaries, failing to meet the
demands of practical real-world applications. We present a novel solution to
tackle this problem by introducing a directed expansion approach from visible
masks to corresponding amodal masks. Our approach involves a hybrid end-to-end
network based on the overlapping region - the area where different instances
intersect. Diverse segmentation strategies are applied for overlapping regions
and non-overlapping regions according to distinct characteristics. To guide the
expansion of visible masks, we introduce an elaborately-designed connectivity
loss for overlapping regions, which leverages correlations with visible masks
and facilitates accurate amodal segmentation. Experiments are conducted on
several challenging datasets and the results show that our proposed method can
outperform existing state-of-the-art methods with large margins.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhaochen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhixuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1&quot;&gt;Tingting Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01643">
<title>S3Net: Innovating Stereo Matching and Semantic Segmentation with a Single-Branch Semantic Stereo Network in Satellite Epipolar Imagery. (arXiv:2401.01643v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01643</link>
<description rdf:parseType="Literal">&lt;p&gt;Stereo matching and semantic segmentation are significant tasks in binocular
satellite 3D reconstruction. However, previous studies primarily view these as
independent parallel tasks, lacking an integrated multitask learning framework.
This work introduces a solution, the Single-branch Semantic Stereo Network
(S3Net), which innovatively combines semantic segmentation and stereo matching
using Self-Fuse and Mutual-Fuse modules. Unlike preceding methods that utilize
semantic or disparity information independently, our method dentifies and
leverages the intrinsic link between these two tasks, leading to a more
accurate understanding of semantic information and disparity estimation.
Comparative testing on the US3D dataset proves the effectiveness of our S3Net.
Our model improves the mIoU in semantic segmentation from 61.38 to 67.39, and
reduces the D1-Error and average endpoint error (EPE) in disparity estimation
from 10.051 to 9.579 and 1.439 to 1.403 respectively, surpassing existing
competitive methods. Our codes are available at:https://github.com/CVEO/S3Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qingyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guanzhou Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1&quot;&gt;Xiaoliang Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaodong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01646">
<title>Prototypical Information Bottlenecking and Disentangling for Multimodal Cancer Survival Prediction. (arXiv:2401.01646v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01646</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal learning significantly benefits cancer survival prediction,
especially the integration of pathological images and genomic data. Despite
advantages of multimodal learning for cancer survival prediction, massive
redundancy in multimodal data prevents it from extracting discriminative and
compact information: (1) An extensive amount of intra-modal task-unrelated
information blurs discriminability, especially for gigapixel whole slide images
(WSIs) with many patches in pathology and thousands of pathways in genomic
data, leading to an ``intra-modal redundancy&quot; issue. (2) Duplicated information
among modalities dominates the representation of multimodal data, which makes
modality-specific information prone to being ignored, resulting in an
``inter-modal redundancy&quot; issue. To address these, we propose a new framework,
Prototypical Information Bottlenecking and Disentangling (PIBD), consisting of
Prototypical Information Bottleneck (PIB) module for intra-modal redundancy and
Prototypical Information Disentanglement (PID) module for inter-modal
redundancy. Specifically, a variant of information bottleneck, PIB, is proposed
to model prototypes approximating a bunch of instances for different risk
levels, which can be used for selection of discriminative instances within
modality. PID module decouples entangled multimodal data into compact distinct
components: modality-common and modality-specific knowledge, under the guidance
of the joint prototypical distribution. Extensive experiments on five cancer
benchmark datasets demonstrated our superiority over other methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yilan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yingxue Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1&quot;&gt;Fengying Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01647">
<title>SIGNeRF: Scene Integrated Generation for Neural Radiance Fields. (arXiv:2401.01647v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01647</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in image diffusion models have recently led to notable improvements
in the generation of high-quality images. In combination with Neural Radiance
Fields (NeRFs), they enabled new opportunities in 3D generation. However, most
generative 3D approaches are object-centric and applying them to editing
existing photorealistic scenes is not trivial. We propose SIGNeRF, a novel
approach for fast and controllable NeRF scene editing and scene-integrated
object generation. A new generative update strategy ensures 3D consistency
across the edited images, without requiring iterative optimization. We find
that depth-conditioned diffusion models inherently possess the capability to
generate 3D consistent views by requesting a grid of images instead of single
views. Based on these insights, we introduce a multi-view reference sheet of
modified images. Our method updates an image collection consistently based on
the reference sheet and refines the original NeRF with the newly generated
image set in one go. By exploiting the depth conditioning mechanism of the
image diffusion model, we gain fine control over the spatial location of the
edit and enforce shape guidance by a selected region or an external mesh.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dihlmann_J/0/1/0/all/0/1&quot;&gt;Jan-Niklas Dihlmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engelhardt_A/0/1/0/all/0/1&quot;&gt;Andreas Engelhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1&quot;&gt;Hendrik Lensch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01650">
<title>De-Confusing Pseudo-Labels in Source-Free Domain Adaptation. (arXiv:2401.01650v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01650</link>
<description rdf:parseType="Literal">&lt;p&gt;Source-free domain adaptation (SFDA) aims to transfer knowledge learned from
a source domain to an unlabeled target domain, where the source data is
unavailable during adaptation. Existing approaches for SFDA focus on
self-training usually including well-established entropy minimization and
pseudo-labeling techniques. Recent work suggested a co-learning strategy to
improve the quality of the generated target pseudo-labels using robust
pretrained networks such as Swin-B. However, since the generated pseudo-labels
depend on the source model, they may be noisy due to domain shift. In this
paper, we view SFDA from the perspective of label noise learning and learn to
de-confuse the pseudo-labels. More specifically, we learn a noise transition
matrix of the pseudo-labels to capture the label corruption of each class and
learn the underlying true label distribution. Estimating the noise transition
matrix enables a better true class-posterior estimation results with better
prediction accuracy. We demonstrate the effectiveness of our approach applied
with several SFDA methods: SHOT, SHOT++, and AaD. We obtain state-of-the-art
results on three domain adaptation datasets: VisDA, DomainNet, and OfficeHome.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diamant_I/0/1/0/all/0/1&quot;&gt;Idit Diamant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achituve_I/0/1/0/all/0/1&quot;&gt;Idan Achituve&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Netzer_A/0/1/0/all/0/1&quot;&gt;Arnon Netzer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01651">
<title>AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated by AI. (arXiv:2401.01651v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01651</link>
<description rdf:parseType="Literal">&lt;p&gt;The burgeoning field of Artificial Intelligence Generated Content (AIGC) is
witnessing rapid advancements, particularly in video generation. This paper
introduces AIGCBench, a pioneering comprehensive and scalable benchmark
designed to evaluate a variety of video generation tasks, with a primary focus
on Image-to-Video (I2V) generation. AIGCBench tackles the limitations of
existing benchmarks, which suffer from a lack of diverse datasets, by including
a varied and open-domain image-text dataset that evaluates different
state-of-the-art algorithms under equivalent conditions. We employ a novel text
combiner and GPT-4 to create rich text prompts, which are then used to generate
images via advanced Text-to-Image models. To establish a unified evaluation
framework for video generation tasks, our benchmark includes 11 metrics
spanning four dimensions to assess algorithm performance. These dimensions are
control-video alignment, motion effects, temporal consistency, and video
quality. These metrics are both reference video-dependent and video-free,
ensuring a comprehensive evaluation strategy. The evaluation standard proposed
correlates well with human judgment, providing insights into the strengths and
weaknesses of current I2V algorithms. The findings from our extensive
experiments aim to stimulate further research and development in the I2V field.
AIGCBench represents a significant step toward creating standardized benchmarks
for the broader AIGC landscape, proposing an adaptable and equitable framework
for future assessments of video generation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1&quot;&gt;Fanda Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chunjie Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1&quot;&gt;Jianfeng Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wanling Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01659">
<title>DiffYOLO: Object Detection for Anti-Noise via YOLO and Diffusion Models. (arXiv:2401.01659v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01659</link>
<description rdf:parseType="Literal">&lt;p&gt;Object detection models represented by YOLO series have been widely used and
have achieved great results on the high quality datasets, but not all the
working conditions are ideal. To settle down the problem of locating targets on
low quality datasets, the existing methods either train a new object detection
network, or need a large collection of low-quality datasets to train. However,
we propose a framework in this paper and apply it on the YOLO models called
DiffYOLO. Specifically, we extract feature maps from the denoising diffusion
probabilistic models to enhance the well-trained models, which allows us
fine-tune YOLO on high-quality datasets and test on low-quality datasets. The
results proved this framework can not only prove the performance on noisy
datasets, but also prove the detection results on high-quality test datasets.
We will supplement more experiments later (with various datasets and network
architectures).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yichen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huajian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1&quot;&gt;Daqing Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01662">
<title>Simultaneous q-Space Sampling Optimization and Reconstruction for Fast and High-fidelity Diffusion Magnetic Resonance Imaging. (arXiv:2401.01662v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01662</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion Magnetic Resonance Imaging (dMRI) plays a crucial role in the
noninvasive investigation of tissue microstructural properties and structural
connectivity in the \textit{in vivo} human brain. However, to effectively
capture the intricate characteristics of water diffusion at various directions
and scales, it is important to employ comprehensive q-space sampling.
Unfortunately, this requirement leads to long scan times, limiting the clinical
applicability of dMRI. To address this challenge, we propose SSOR, a
Simultaneous q-Space sampling Optimization and Reconstruction framework. We
jointly optimize a subset of q-space samples using a continuous representation
of spherical harmonic functions and a reconstruction network. Additionally, we
integrate the unique properties of diffusion magnetic resonance imaging (dMRI)
in both the q-space and image domains by applying $l1$-norm and total-variation
regularization. The experiments conducted on HCP data demonstrate that SSOR has
promising strengths both quantitatively and qualitatively and exhibits
robustness to noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jian Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Cheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1&quot;&gt;Wenxin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;Juan Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruoyou Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shanshan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01674">
<title>Transformer RGBT Tracking with Spatio-Temporal Multimodal Tokens. (arXiv:2401.01674v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01674</link>
<description rdf:parseType="Literal">&lt;p&gt;Many RGBT tracking researches primarily focus on modal fusion design, while
overlooking the effective handling of target appearance changes. While some
approaches have introduced historical frames or fuse and replace initial
templates to incorporate temporal information, they have the risk of disrupting
the original target appearance and accumulating errors over time. To alleviate
these limitations, we propose a novel Transformer RGBT tracking approach, which
mixes spatio-temporal multimodal tokens from the static multimodal templates
and multimodal search regions in Transformer to handle target appearance
changes, for robust RGBT tracking. We introduce independent dynamic template
tokens to interact with the search region, embedding temporal information to
address appearance changes, while also retaining the involvement of the initial
static template tokens in the joint feature extraction process to ensure the
preservation of the original reliable target appearance information that
prevent deviations from the target appearance caused by traditional temporal
updates. We also use attention mechanisms to enhance the target features of
multimodal template tokens by incorporating supplementary modal cues, and make
the multimodal search region tokens interact with multimodal dynamic template
tokens via attention mechanisms, which facilitates the conveyance of
multimodal-enhanced target change information. Our module is inserted into the
transformer backbone network and inherits joint feature extraction,
search-template matching, and cross-modal interaction. Extensive experiments on
three RGBT benchmark datasets show that the proposed approach maintains
competitive performance compared to other state-of-the-art tracking algorithms
while running at 39.1 FPS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1&quot;&gt;Dengdi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yajie Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1&quot;&gt;Andong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenglong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Bin Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01676">
<title>Performance Evaluation of GPS Trajectory Rasterization Methods. (arXiv:2401.01676v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2401.01676</link>
<description rdf:parseType="Literal">&lt;p&gt;The availability of the Global Positioning System (GPS) trajectory data is
increasing along with the availability of different GPS receivers and with the
increasing use of various mobility services. GPS trajectory is an important
data source which is used in traffic density detection, transport mode
detection, mapping data inferences with the use of different methods such as
image processing and machine learning methods. While the data size increases,
efficient representation of this type of data is becoming difficult to be used
in these methods. A common approach is the representation of GPS trajectory
information such as average speed, bearing, etc. in raster image form and
applying analysis methods. In this study, we evaluate GPS trajectory data
rasterization using the spatial join functions of QGIS, PostGIS+QGIS, and our
iterative spatial structured grid aggregation implementation coded in the
Python programming language. Our implementation is also parallelizable, and
this parallelization is also included as the fourth method. According to the
results of experiment carried out with an example GPS trajectory dataset, QGIS
method and PostGIS+QGIS method showed relatively low performance with respect
to our method using the metric of total processing time. PostGIS+QGIS method
achieved the best results for spatial join though its total performance
decreased quickly while test area size increases. On the other hand, both of
our methods&apos; performances decrease directly proportional to GPS point. And our
methods&apos; performance can be increased proportional to the increase with the
number of processor cores and/or with multiple computing clusters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gengec_N/0/1/0/all/0/1&quot;&gt;Necip Enes Gengec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tari_E/0/1/0/all/0/1&quot;&gt;Ergin Tari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01685">
<title>Modality Exchange Network for Retinogeniculate Visual Pathway Segmentation. (arXiv:2401.01685v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.01685</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate segmentation of the retinogeniculate visual pathway (RGVP) aids in
the diagnosis and treatment of visual disorders by identifying disruptions or
abnormalities within the pathway. However, the complex anatomical structure and
connectivity of RGVP make it challenging to achieve accurate segmentation. In
this study, we propose a novel Modality Exchange Network (ME-Net) that
effectively utilizes multi-modal magnetic resonance (MR) imaging information to
enhance RGVP segmentation. Our ME-Net has two main contributions. Firstly, we
introduce an effective multi-modal soft-exchange technique. Specifically, we
design a channel and spatially mixed attention module to exchange modality
information between T1-weighted and fractional anisotropy MR images. Secondly,
we propose a cross-fusion module that further enhances the fusion of
information between the two modalities. Experimental results demonstrate that
our method outperforms existing state-of-the-art approaches in terms of RGVP
segmentation performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_H/0/1/0/all/0/1&quot;&gt;Hua Han&lt;/a&gt; (1 and 2), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Cheng Li&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Lei Xie&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yuanjing Feng&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Diakite_A/0/1/0/all/0/1&quot;&gt;Alou Diakite&lt;/a&gt; (1 and 2), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shanshan Wang&lt;/a&gt; (1 and 4) ((1) Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China, (2) University of Chinese Academy of Sciences, Beijing, China, (3) College of Information Engineering, Zhejiang University of Technology, Hangzhou, China, (4) Peng Cheng Laboratory, Shenzhen, China)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01686">
<title>ODTrack: Online Dense Temporal Token Learning for Visual Tracking. (arXiv:2401.01686v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01686</link>
<description rdf:parseType="Literal">&lt;p&gt;Online contextual reasoning and association across consecutive video frames
are critical to perceive instances in visual tracking. However, most current
top-performing trackers persistently lean on sparse temporal relationships
between reference and search frames via an offline mode. Consequently, they can
only interact independently within each image-pair and establish limited
temporal correlations. To alleviate the above problem, we propose a simple,
flexible and effective video-level tracking pipeline, named \textbf{ODTrack},
which densely associates the contextual relationships of video frames in an
online token propagation manner. ODTrack receives video frames of arbitrary
length to capture the spatio-temporal trajectory relationships of an instance,
and compresses the discrimination features (localization information) of a
target into a token sequence to achieve frame-to-frame association. This new
solution brings the following benefits: 1) the purified token sequences can
serve as prompts for the inference in the next video frame, whereby past
information is leveraged to guide future inference; 2) the complex online
update strategies are effectively avoided by the iterative propagation of token
sequences, and thus we can achieve more efficient model representation and
computation. ODTrack achieves a new \textit{SOTA} performance on seven
benchmarks, while running at real-time speed. Code and models are available at
\url{https://github.com/GXNU-ZhongLab/ODTrack}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yaozong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_B/0/1/0/all/0/1&quot;&gt;Bineng Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Q/0/1/0/all/0/1&quot;&gt;Qihua Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_Z/0/1/0/all/0/1&quot;&gt;Zhiyi Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shengping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xianxian Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01693">
<title>AID-DTI: Accelerating High-fidelity Diffusion Tensor Imaging with Detail-Preserving Model-based Deep Learning. (arXiv:2401.01693v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01693</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has shown great potential in accelerating diffusion tensor
imaging (DTI). Nevertheless, existing methods tend to suffer from Rician noise
and detail loss in reconstructing the DTI-derived parametric maps especially
when sparsely sampled q-space data are used. This paper proposes a novel
method, AID-DTI (Accelerating hIgh fiDelity Diffusion Tensor Imaging), to
facilitate fast and accurate DTI with only six measurements. AID-DTI is
equipped with a newly designed Singular Value Decomposition (SVD)-based
regularizer, which can effectively capture fine details while suppressing noise
during network training. Experimental results on Human Connectome Project (HCP)
data consistently demonstrate that the proposed method estimates DTI parameter
maps with fine-grained details and outperforms three state-of-the-art methods
both quantitatively and qualitatively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1&quot;&gt;Wenxin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jian Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Cheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xinrui Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;Juan Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruoyou Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiegen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shanshan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01699">
<title>WordArt Designer API: User-Driven Artistic Typography Synthesis with Large Language Models on ModelScope. (arXiv:2401.01699v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01699</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces the WordArt Designer API, a novel framework for
user-driven artistic typography synthesis utilizing Large Language Models
(LLMs) on ModelScope. We address the challenge of simplifying artistic
typography for non-professionals by offering a dynamic, adaptive, and
computationally efficient alternative to traditional rigid templates. Our
approach leverages the power of LLMs to understand and interpret user input,
facilitating a more intuitive design process. We demonstrate through various
case studies how users can articulate their aesthetic preferences and
functional requirements, which the system then translates into unique and
creative typographic designs. Our evaluations indicate significant improvements
in user satisfaction, design flexibility, and creative expression over existing
systems. The WordArt Designer API not only democratizes the art of typography
but also opens up new possibilities for personalized digital communication and
design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jun-Yan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Zhi-Qi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jingdong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1&quot;&gt;Wangmeng Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yusen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xianhui Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zengke Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Bin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1&quot;&gt;Yifeng Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xuansong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01702">
<title>Image Sculpting: Precise Object Editing with 3D Geometry Control. (arXiv:2401.01702v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2401.01702</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Image Sculpting, a new framework for editing 2D images by
incorporating tools from 3D geometry and graphics. This approach differs
markedly from existing methods, which are confined to 2D spaces and typically
rely on textual instructions, leading to ambiguity and limited control. Image
Sculpting converts 2D objects into 3D, enabling direct interaction with their
3D geometry. Post-editing, these objects are re-rendered into 2D, merging into
the original image to produce high-fidelity results through a coarse-to-fine
enhancement process. The framework supports precise, quantifiable, and
physically-plausible editing options such as pose editing, rotation,
translation, 3D composition, carving, and serial addition. It marks an initial
step towards combining the creative freedom of generative models with the
precision of graphics pipelines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yenphraphai_J/0/1/0/all/0/1&quot;&gt;Jiraphon Yenphraphai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xichen Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sainan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panozzo_D/0/1/0/all/0/1&quot;&gt;Daniele Panozzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Saining Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01717">
<title>Fact-checking based fake news detection: a review. (arXiv:2401.01717v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01717</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper reviews and summarizes the research results on fact-based fake
news from the perspectives of tasks and problems, algorithm strategies, and
datasets. First, the paper systematically explains the task definition and core
problems of fact-based fake news detection. Second, the paper summarizes the
existing detection methods based on the algorithm principles. Third, the paper
analyzes the classic and newly proposed datasets in the field, and summarizes
the experimental results on each dataset. Finally, the paper summarizes the
advantages and disadvantages of existing methods, proposes several challenges
that methods in this field may face, and looks forward to the next stage of
research. It is hoped that this paper will provide reference for subsequent
work in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuzhou Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yangming Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1&quot;&gt;Qichao Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1&quot;&gt;Zhenxing Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1&quot;&gt;Dan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01720">
<title>Local Adaptive Clustering Based Image Matching for Automatic Visual Identification. (arXiv:2401.01720v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01720</link>
<description rdf:parseType="Literal">&lt;p&gt;Monitoring cameras are extensively utilized in industrial production to
monitor equipment running. With advancements in computer vision, device
recognition using image features is viable. This paper presents a
vision-assisted identification system that implements real-time automatic
equipment labeling through image matching in surveillance videos. The system
deploys the ORB algorithm to extract image features and the GMS algorithm to
remove incorrect matching points. According to the principles of clustering and
template locality, a method known as Local Adaptive Clustering (LAC) has been
established to enhance label positioning. This method segments matching
templates using the cluster center, which improves the efficiency and stability
of labels. The experimental results demonstrate that LAC effectively curtails
the label drift.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhizhen Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01724">
<title>Lightweight Adaptive Feature De-drifting for Compressed Image Classification. (arXiv:2401.01724v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01724</link>
<description rdf:parseType="Literal">&lt;p&gt;JPEG is a widely used compression scheme to efficiently reduce the volume of
transmitted images. The artifacts appear among blocks due to the information
loss, which not only affects the quality of images but also harms the
subsequent high-level tasks in terms of feature drifting. High-level vision
models trained on high-quality images will suffer performance degradation when
dealing with compressed images, especially on mobile devices. Numerous
learning-based JPEG artifact removal methods have been proposed to handle
visual artifacts. However, it is not an ideal choice to use these JPEG artifact
removal methods as a pre-processing for compressed image classification for the
following reasons: 1. These methods are designed for human vision rather than
high-level vision models; 2. These methods are not efficient enough to serve as
pre-processing on resource-constrained devices. To address these issues, this
paper proposes a novel lightweight AFD module to boost the performance of
pre-trained image classification models when facing compressed images. First, a
FDE-Net is devised to generate the spatial-wise FDM in the DCT domain. Next,
the estimated FDM is transmitted to the FE-Net to generate the mapping
relationship between degraded features and corresponding high-quality features.
A simple but effective RepConv block equipped with structural
re-parameterization is utilized in FE-Net, which enriches feature
representation in the training phase while maintaining efficiency in the
deployment phase. After training on limited compressed images, the AFD-Module
can serve as a &quot;plug-and-play&quot; model for pre-trained classification models to
improve their performance on compressed images. Experiments demonstrate that
our proposed AFD module can comprehensively improve the accuracy of the
pre-trained classification models and significantly outperform the existing
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1&quot;&gt;Long Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuejin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01730">
<title>STAF: 3D Human Mesh Recovery from Video with Spatio-Temporal Alignment Fusion. (arXiv:2401.01730v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01730</link>
<description rdf:parseType="Literal">&lt;p&gt;The recovery of 3D human mesh from monocular images has significantly been
developed in recent years. However, existing models usually ignore spatial and
temporal information, which might lead to mesh and image misalignment and
temporal discontinuity. For this reason, we propose a novel Spatio-Temporal
Alignment Fusion (STAF) model. As a video-based model, it leverages coherence
clues from human motion by an attention-based Temporal Coherence Fusion Module
(TCFM). As for spatial mesh-alignment evidence, we extract fine-grained local
information through predicted mesh projection on the feature maps. Based on the
spatial features, we further introduce a multi-stage adjacent Spatial Alignment
Fusion Module (SAFM) to enhance the feature representation of the target frame.
In addition to the above, we propose an Average Pooling Module (APM) to allow
the model to focus on the entire input sequence rather than just the target
frame. This method can remarkably improve the smoothness of recovery results
from video. Extensive experiments on 3DPW, MPII3D, and H36M demonstrate the
superiority of STAF. We achieve a state-of-the-art trade-off between precision
and smoothness. Our code and more video results are on the project page
https://yw0208.github.io/staf/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongwen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yunlian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jinhui Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01734">
<title>Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data. (arXiv:2401.01734v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01734</link>
<description rdf:parseType="Literal">&lt;p&gt;Assistive robots should be able to wash, fold or iron clothes. However, due
to the variety, deformability and self-occlusions of clothes, creating
general-purpose robot systems for cloth manipulation is challenging. Synthetic
data is a promising direction to improve generalization, though its usability
is often limited by the sim-to-real gap. To advance the use of synthetic data
for cloth manipulation and to enable tasks such as robotic folding, we present
a synthetic data pipeline to train keypoint detectors for almost flattened
cloth items. To test its performance, we have also collected a real-world
dataset. We train detectors for both T-shirts, towels and shorts and obtain an
average precision of 64.3%. Fine-tuning on real-world data improves performance
to 74.2%. Additional insight is provided by discussing various failure modes of
the keypoint detectors and by comparing different approaches to obtain cloth
meshes and materials. We also quantify the remaining sim-to-real gap and argue
that further improvements to the fidelity of cloth assets will be required to
further reduce this gap. The code, dataset and trained models are available
online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lips_T/0/1/0/all/0/1&quot;&gt;Thomas Lips&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gusseme_V/0/1/0/all/0/1&quot;&gt;Victor-Louis De Gusseme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+wyffels_F/0/1/0/all/0/1&quot;&gt;Francis wyffels&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01736">
<title>Few-shot Adaptation of Multi-modal Foundation Models: A Survey. (arXiv:2401.01736v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01736</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal (vision-language) models, such as CLIP, are replacing traditional
supervised pre-training models (e.g., ImageNet-based pre-training) as the new
generation of visual foundation models. These models with robust and aligned
semantic representations learned from billions of internet image-text pairs and
can be applied to various downstream tasks in a zero-shot manner. However, in
some fine-grained domains like medical imaging and remote sensing, the
performance of multi-modal foundation models often leaves much to be desired.
Consequently, many researchers have begun to explore few-shot adaptation
methods for these models, gradually deriving three main technical approaches:
1) prompt-based methods, 2) adapter-based methods, and 3) external
knowledge-based methods. Nevertheless, this rapidly developing field has
produced numerous results without a comprehensive survey to systematically
organize the research progress. Therefore, in this survey, we introduce and
analyze the research advancements in few-shot adaptation methods for
multi-modal models, summarizing commonly used datasets and experimental setups,
and comparing the results of different methods. In addition, due to the lack of
reliable theoretical support for existing methods, we derive the few-shot
adaptation generalization error bound for multi-modal models. The theorem
reveals that the generalization error of multi-modal foundation models is
constrained by three factors: domain gap, model capacity, and sample size.
Based on this, we propose three possible solutions from the following aspects:
1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive
knowledge utilization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianshu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Wenwen Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wenwen Cai Xiaocong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Delong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01749">
<title>Few-shot Image Generation via Information Transfer from the Built Geodesic Surface. (arXiv:2401.01749v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01749</link>
<description rdf:parseType="Literal">&lt;p&gt;Images generated by most of generative models trained with limited data often
exhibit deficiencies in either fidelity, diversity, or both. One effective
solution to address the limitation is few-shot generative model adaption.
However, the type of approaches typically rely on a large-scale pre-trained
model, serving as a source domain, to facilitate information transfer to the
target domain. In this paper, we propose a method called Information Transfer
from the Built Geodesic Surface (ITBGS), which contains two module: Feature
Augmentation on Geodesic Surface (FAGS); Interpolation and Regularization
(I\&amp;amp;R). With the FAGS module, a pseudo-source domain is created by projecting
image features from the training dataset into the Pre-Shape Space, subsequently
generating new features on the Geodesic surface. Thus, no pre-trained models is
needed for the adaption process during the training of generative models with
FAGS. I\&amp;amp;R module are introduced for supervising the interpolated images and
regularizing their relative distances, respectively, to further enhance the
quality of generated images. Through qualitative and quantitative experiments,
we demonstrate that the proposed method consistently achieves optimal or
comparable results across a diverse range of semantically distinct datasets,
even in extremely few-shot scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yuexing Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_L/0/1/0/all/0/1&quot;&gt;Liheng Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bing Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01750">
<title>Towards Robust Semantic Segmentation against Patch-based Attack via Attention Refinement. (arXiv:2401.01750v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01750</link>
<description rdf:parseType="Literal">&lt;p&gt;The attention mechanism has been proven effective on various visual tasks in
recent years. In the semantic segmentation task, the attention mechanism is
applied in various methods, including the case of both Convolution Neural
Networks (CNN) and Vision Transformer (ViT) as backbones. However, we observe
that the attention mechanism is vulnerable to patch-based adversarial attacks.
Through the analysis of the effective receptive field, we attribute it to the
fact that the wide receptive field brought by global attention may lead to the
spread of the adversarial patch. To address this issue, in this paper, we
propose a Robust Attention Mechanism (RAM) to improve the robustness of the
semantic segmentation model, which can notably relieve the vulnerability
against patch-based attacks. Compared to the vallina attention mechanism, RAM
introduces two novel modules called Max Attention Suppression and Random
Attention Dropout, both of which aim to refine the attention matrix and limit
the influence of a single adversarial patch on the semantic segmentation
results of other positions. Extensive experiments demonstrate the effectiveness
of our RAM to improve the robustness of semantic segmentation models against
various patch-based attack methods under different attack settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zheng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yude Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1&quot;&gt;Shiguang Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xilin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01752">
<title>FullLoRA-AT: Efficiently Boosting the Robustness of Pretrained Vision Transformers. (arXiv:2401.01752v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01752</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the Vision Transformer (ViT) model has gradually become
mainstream in various computer vision tasks, and the robustness of the model
has received increasing attention. However, existing large models tend to
prioritize performance during training, potentially neglecting the robustness,
which may lead to serious security concerns. In this paper, we establish a new
challenge: exploring how to use a small number of additional parameters for
adversarial finetuning to quickly and effectively enhance the adversarial
robustness of a standardly trained model. To address this challenge, we develop
the novel LNLoRA module, incorporating a learnable layer normalization before
the conventional LoRA module, which helps mitigate magnitude differences in
parameters between the adversarial and standard training paradigms.
&lt;/p&gt;
&lt;p&gt;Furthermore, we propose the FullLoRA-AT framework by integrating the
learnable LNLoRA modules into all key components of ViT-based models while
keeping the pretrained model frozen, which can significantly improve the model
robustness via adversarial finetuning in a parameter-efficient manner.
&lt;/p&gt;
&lt;p&gt;Extensive experiments on CIFAR-10, CIFAR-100, and Imagenette demonstrate the
superiority of our proposed FullLoRA-AT framework. It achieves comparable
robustness with full finetuning while only requiring about 5% of the learnable
parameters. This also effectively addresses concerns regarding extra model
storage space and enormous training time caused by adversarial finetuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zheng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1&quot;&gt;Shiguang Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01759">
<title>VGA: Vision and Graph Fused Attention Network for Rumor Detection. (arXiv:2401.01759v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2401.01759</link>
<description rdf:parseType="Literal">&lt;p&gt;With the development of social media, rumors have been spread broadly on
social media platforms, causing great harm to society. Beside textual
information, many rumors also use manipulated images or conceal textual
information within images to deceive people and avoid being detected, making
multimodal rumor detection be a critical problem. The majority of multimodal
rumor detection methods mainly concentrate on extracting features of source
claims and their corresponding images, while ignoring the comments of rumors
and their propagation structures. These comments and structures imply the
wisdom of crowds and are proved to be crucial to debunk rumors. Moreover, these
methods usually only extract visual features in a basic manner, seldom consider
tampering or textual information in images. Therefore, in this study, we
propose a novel Vision and Graph Fused Attention Network (VGA) for rumor
detection to utilize propagation structures among posts so as to obtain the
crowd opinions and further explore visual tampering features, as well as the
textual information hidden in images. We conduct extensive experiments on three
datasets, demonstrating that VGA can effectively detect multimodal rumors and
outperform state-of-the-art methods significantly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Lin Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1&quot;&gt;Caiyan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Ziying Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1&quot;&gt;Chaoqun Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01764">
<title>Understanding the Detrimental Class-level Effects of Data Augmentation. (arXiv:2401.01764v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01764</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation (DA) encodes invariance and provides implicit
regularization critical to a model&apos;s performance in image classification tasks.
However, while DA improves average accuracy, recent studies have shown that its
impact can be highly class dependent: achieving optimal average accuracy comes
at the cost of significantly hurting individual class accuracy by as much as
20% on ImageNet. There has been little progress in resolving class-level
accuracy drops due to a limited understanding of these effects. In this work,
we present a framework for understanding how DA interacts with class-level
learning dynamics. Using higher-quality multi-label annotations on ImageNet, we
systematically categorize the affected classes and find that the majority are
inherently ambiguous, co-occur, or involve fine-grained distinctions, while DA
controls the model&apos;s bias towards one of the closely related classes. While
many of the previously reported performance drops are explained by multi-label
annotations, our analysis of class confusions reveals other sources of accuracy
degradation. We show that simple class-conditional augmentation strategies
informed by our framework improve performance on the negatively affected
classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirichenko_P/0/1/0/all/0/1&quot;&gt;Polina Kirichenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1&quot;&gt;Mark Ibrahim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1&quot;&gt;Randall Balestriero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouchacourt_D/0/1/0/all/0/1&quot;&gt;Diane Bouchacourt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vedantam_R/0/1/0/all/0/1&quot;&gt;Ramakrishna Vedantam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1&quot;&gt;Hamed Firooz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1&quot;&gt;Andrew Gordon Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01808">
<title>aMUSEd: An Open MUSE Reproduction. (arXiv:2401.01808v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01808</link>
<description rdf:parseType="Literal">&lt;p&gt;We present aMUSEd, an open-source, lightweight masked image model (MIM) for
text-to-image generation based on MUSE. With 10 percent of MUSE&apos;s parameters,
aMUSEd is focused on fast image generation. We believe MIM is under-explored
compared to latent diffusion, the prevailing approach for text-to-image
generation. Compared to latent diffusion, MIM requires fewer inference steps
and is more interpretable. Additionally, MIM can be fine-tuned to learn
additional styles with only a single image. We hope to encourage further
exploration of MIM by demonstrating its effectiveness on large-scale
text-to-image generation and releasing reproducible training code. We also
release checkpoints for two models which directly produce images at 256x256 and
512x512 resolutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patil_S/0/1/0/all/0/1&quot;&gt;Suraj Patil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berman_W/0/1/0/all/0/1&quot;&gt;William Berman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rombach_R/0/1/0/all/0/1&quot;&gt;Robin Rombach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Platen_P/0/1/0/all/0/1&quot;&gt;Patrick von Platen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01822">
<title>HawkRover: An Autonomous mmWave Vehicular Communication Testbed with Multi-sensor Fusion and Deep Learning. (arXiv:2401.01822v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2401.01822</link>
<description rdf:parseType="Literal">&lt;p&gt;Connected and automated vehicles (CAVs) have become a transformative
technology that can change our daily life. Currently, millimeter-wave (mmWave)
bands are identified as the promising CAV connectivity solution. While it can
provide high data rate, their realization faces many challenges such as high
attenuation during mmWave signal propagation and mobility management. Existing
solution has to initiate pilot signal to measure channel information, then
apply signal processing to calculate the best narrow beam towards the receiver
end to guarantee sufficient signal power. This process takes significant
overhead and time, hence not suitable for vehicles. In this study, we propose
an autonomous and low-cost testbed to collect extensive co-located mmWave
signal and other sensors data such as LiDAR (Light Detection and Ranging),
cameras, ultrasonic, etc, traditionally for ``automated&apos;&apos;, to facilitate mmWave
vehicular communications. Intuitively, these sensors can build a 3D map around
the vehicle and signal propagation path can be estimated, eliminating iterative
the process via pilot signals. This multimodal data fusion, together with AI,
is expected to bring significant advances in ``connected&apos;&apos; research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1&quot;&gt;Ethan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Haijian Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01823">
<title>Detours for Navigating Instructional Videos. (arXiv:2401.01823v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01823</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the video detours problem for navigating instructional videos.
Given a source video and a natural language query asking to alter the how-to
video&apos;s current path of execution in a certain way, the goal is to find a
related &apos;&apos;detour video&apos;&apos; that satisfies the requested alteration. To address
this challenge, we propose VidDetours, a novel video-language approach that
learns to retrieve the targeted temporal segments from a large repository of
how-to&apos;s using video-and-text conditioned queries. Furthermore, we devise a
language-based pipeline that exploits how-to video narration text to create
weakly supervised training data. We demonstrate our idea applied to the domain
of how-to cooking videos, where a user can detour from their current recipe to
find steps with alternate ingredients, tools, and techniques. Validating on a
ground truth annotated dataset of 16K samples, we show our model&apos;s significant
improvements over best available methods for video retrieval and question
answering, with recall rates exceeding the state of the art by 35%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashutosh_K/0/1/0/all/0/1&quot;&gt;Kumar Ashutosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1&quot;&gt;Zihui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagarajan_T/0/1/0/all/0/1&quot;&gt;Tushar Nagarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1&quot;&gt;Kristen Grauman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01827">
<title>Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions. (arXiv:2401.01827v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01827</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing video diffusion models (VDMs) are limited to mere text
conditions. Thereby, they are usually lacking in control over visual appearance
and geometry structure of the generated videos. This work presents Moonshot, a
new video generation model that conditions simultaneously on multimodal inputs
of image and text. The model builts upon a core module, called multimodal video
block (MVB), which consists of conventional spatialtemporal layers for
representing video features, and a decoupled cross-attention layer to address
image and text inputs for appearance conditioning. In addition, we carefully
design the model architecture such that it can optionally integrate with
pre-trained image ControlNet modules for geometry visual conditions, without
needing of extra training overhead as opposed to prior methods. Experiments
show that with versatile multimodal conditioning mechanisms, Moonshot
demonstrates significant improvement on visual quality and temporal consistency
compared to existing models. In addition, the model can be easily repurposed
for a variety of generative applications, such as personalized video
generation, image animation and video editing, unveiling its potential to serve
as a fundamental architecture for controllable video generation. Models will be
made public on https://github.com/salesforce/LAVIS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;David Junhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dongxu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1&quot;&gt;Hung Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahoo_D/0/1/0/all/0/1&quot;&gt;Doyen Sahoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01839">
<title>Frequency Domain Modality-invariant Feature Learning for Visible-infrared Person Re-Identification. (arXiv:2401.01839v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01839</link>
<description rdf:parseType="Literal">&lt;p&gt;Visible-infrared person re-identification (VI-ReID) is challenging due to the
significant cross-modality discrepancies between visible and infrared images.
While existing methods have focused on designing complex network architectures
or using metric learning constraints to learn modality-invariant features, they
often overlook which specific component of the image causes the modality
discrepancy problem. In this paper, we first reveal that the difference in the
amplitude component of visible and infrared images is the primary factor that
causes the modality discrepancy and further propose a novel Frequency Domain
modality-invariant feature learning framework (FDMNet) to reduce modality
discrepancy from the frequency domain perspective. Our framework introduces two
novel modules, namely the Instance-Adaptive Amplitude Filter (IAF) module and
the Phrase-Preserving Normalization (PPNorm) module, to enhance the
modality-invariant amplitude component and suppress the modality-specific
component at both the image- and feature-levels. Extensive experimental results
on two standard benchmarks, SYSU-MM01 and RegDB, demonstrate the superior
performance of our FDMNet against state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yulin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianzhu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongdong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01858">
<title>Synthetic dataset of ID and Travel Document. (arXiv:2401.01858v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01858</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new synthetic dataset of ID and travel documents,
called SIDTD. The SIDTD dataset is created to help training and evaluating
forged ID documents detection systems. Such a dataset has become a necessity as
ID documents contain personal information and a public dataset of real
documents can not be released. Moreover, forged documents are scarce, compared
to legit ones, and the way they are generated varies from one fraudster to
another resulting in a class of high intra-variability. In this paper we
trained state-of-the-art models on this dataset and we compare them to the
performance achieved in larger, but private, datasets. The creation of this
dataset will help to document image analysis community to progress in the task
of ID document verification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boned_C/0/1/0/all/0/1&quot;&gt;Carlos Boned&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talarmain_M/0/1/0/all/0/1&quot;&gt;Maxime Talarmain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghanmi_N/0/1/0/all/0/1&quot;&gt;Nabil Ghanmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiron_G/0/1/0/all/0/1&quot;&gt;Guillaume Chiron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1&quot;&gt;Sanket Biswas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awal_A/0/1/0/all/0/1&quot;&gt;Ahmad Montaser Awal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terrades_O/0/1/0/all/0/1&quot;&gt;Oriol Ramos Terrades&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01862">
<title>A Vision Check-up for Language Models. (arXiv:2401.01862v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01862</link>
<description rdf:parseType="Literal">&lt;p&gt;What does learning to model relationships between strings teach large
language models (LLMs) about the visual world? We systematically evaluate LLMs&apos;
abilities to generate and recognize an assortment of visual concepts of
increasing complexity and then demonstrate how a preliminary visual
representation learning system can be trained using models of text. As language
models lack the ability to consume or output visual information as pixels, we
use code to represent images in our study. Although LLM-generated images do not
look like natural images, results on image generation and the ability of models
to correct these generated images indicate that precise modeling of strings can
teach language models about numerous aspects of the visual world. Furthermore,
experiments on self-supervised visual representation learning, utilizing images
generated with text models, highlight the potential to train vision models
capable of making semantic assessments of natural images using just LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1&quot;&gt;Pratyusha Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaham_T/0/1/0/all/0/1&quot;&gt;Tamar Rott Shaham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baradad_M/0/1/0/all/0/1&quot;&gt;Manel Baradad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1&quot;&gt;Stephanie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_Munoz_A/0/1/0/all/0/1&quot;&gt;Adrian Rodriguez-Munoz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duggal_S/0/1/0/all/0/1&quot;&gt;Shivam Duggal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1&quot;&gt;Phillip Isola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1&quot;&gt;Antonio Torralba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01868">
<title>Step length measurement in the wild using FMCW radar. (arXiv:2401.01868v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01868</link>
<description rdf:parseType="Literal">&lt;p&gt;With an aging population, numerous assistive and monitoring technologies are
under development to enable older adults to age in place. To facilitate aging
in place predicting risk factors such as falls, and hospitalization and
providing early interventions are important. Much of the work on ambient
monitoring for risk prediction has centered on gait speed analysis, utilizing
privacy-preserving sensors like radar. Despite compelling evidence that
monitoring step length, in addition to gait speed, is crucial for predicting
risk, radar-based methods have not explored step length measurement in the
home. Furthermore, laboratory experiments on step length measurement using
radars are limited to proof of concept studies with few healthy subjects. To
address this gap, a radar-based step length measurement system for the home is
proposed based on detection and tracking using radar point cloud, followed by
Doppler speed profiling of the torso to obtain step lengths in the home. The
proposed method was evaluated in a clinical environment, involving 35 frail
older adults, to establish its validity. Additionally, the method was assessed
in people&apos;s homes, with 21 frail older adults who had participated in the
clinical assessment. The proposed radar-based step length measurement method
was compared to the gold standard Zeno Walkway Gait Analysis System, revealing
a 4.5cm/8.3% error in a clinical setting. Furthermore, it exhibited excellent
reliability (ICC(2,k)=0.91, 95% CI 0.82 to 0.96) in uncontrolled home settings.
The method also proved accurate in uncontrolled home settings, as indicated by
a strong agreement (ICC(3,k)=0.81 (95% CI 0.53 to 0.92)) between home
measurements and in-clinic assessments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siva_P/0/1/0/all/0/1&quot;&gt;Parthipan Siva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alexander Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hewston_P/0/1/0/all/0/1&quot;&gt;Patricia Hewston&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ioannidis_G/0/1/0/all/0/1&quot;&gt;George Ioannidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adachi_D/0/1/0/all/0/1&quot;&gt;Dr. Jonathan Adachi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabinovich_D/0/1/0/all/0/1&quot;&gt;Dr. Alexander Rabinovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1&quot;&gt;Andrea Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papaioannou_A/0/1/0/all/0/1&quot;&gt;Alexandra Papaioannou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01885">
<title>From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations. (arXiv:2401.01885v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01885</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a framework for generating full-bodied photorealistic avatars that
gesture according to the conversational dynamics of a dyadic interaction. Given
speech audio, we output multiple possibilities of gestural motion for an
individual, including face, body, and hands. The key behind our method is in
combining the benefits of sample diversity from vector quantization with the
high-frequency details obtained through diffusion to generate more dynamic,
expressive motion. We visualize the generated motion using highly
photorealistic avatars that can express crucial nuances in gestures (e.g.
sneers and smirks). To facilitate this line of research, we introduce a
first-of-its-kind multi-view conversational dataset that allows for
photorealistic reconstruction. Experiments show our model generates appropriate
and diverse gestures, outperforming both diffusion- and VQ-only methods.
Furthermore, our perceptual evaluation highlights the importance of
photorealism (vs. meshes) in accurately assessing subtle motion details in
conversational gestures. Code and dataset available online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_E/0/1/0/all/0/1&quot;&gt;Evonne Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_J/0/1/0/all/0/1&quot;&gt;Javier Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1&quot;&gt;Timur Bagautdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1&quot;&gt;Shaojie Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1&quot;&gt;Angjoo Kanazawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richard_A/0/1/0/all/0/1&quot;&gt;Alexander Richard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01887">
<title>LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry. (arXiv:2401.01887v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01887</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual odometry estimates the motion of a moving camera based on visual
input. Existing methods, mostly focusing on two-view point tracking, often
ignore the rich temporal context in the image sequence, thereby overlooking the
global motion patterns and providing no assessment of the full trajectory
reliability. These shortcomings hinder performance in scenarios with occlusion,
dynamic objects, and low-texture areas. To address these challenges, we present
the Long-term Effective Any Point Tracking (LEAP) module. LEAP innovatively
combines visual, inter-track, and temporal cues with mindfully selected anchors
for dynamic track estimation. Moreover, LEAP&apos;s temporal probabilistic
formulation integrates distribution updates into a learnable iterative
refinement module to reason about point-wise uncertainty. Based on these
traits, we develop LEAP-VO, a robust visual odometry system adept at handling
occlusions and dynamic scenes. Our mindful integration showcases a novel
practice by employing long-term point tracking as the front-end. Extensive
experiments demonstrate that the proposed pipeline significantly outperforms
existing baselines across various visual odometry benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weirong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Le Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1&quot;&gt;Marc Pollefeys&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.01577">
<title>HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction. (arXiv:2203.01577v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.01577</link>
<description rdf:parseType="Literal">&lt;p&gt;We present HOI4D, a large-scale 4D egocentric dataset with rich annotations,
to catalyze the research of category-level human-object interaction. HOI4D
consists of 2.4M RGB-D egocentric video frames over 4000 sequences collected by
4 participants interacting with 800 different object instances from 16
categories over 610 different indoor rooms. Frame-wise annotations for panoptic
segmentation, motion segmentation, 3D hand pose, category-level object pose and
hand action have also been provided, together with reconstructed object meshes
and scene point clouds. With HOI4D, we establish three benchmarking tasks to
promote category-level HOI from 4D visual signals including semantic
segmentation of 4D dynamic point cloud sequences, category-level object pose
tracking, and egocentric action segmentation with diverse interaction targets.
In-depth analysis shows HOI4D poses great challenges to existing methods and
produces great research opportunities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yunze Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Che Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_K/0/1/0/all/0/1&quot;&gt;Kangbo Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1&quot;&gt;Weikang Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Hao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1&quot;&gt;Boqiang Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1&quot;&gt;Zhoujie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;He Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1&quot;&gt;Li Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.03359">
<title>ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO. (arXiv:2204.03359v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.03359</link>
<description rdf:parseType="Literal">&lt;p&gt;Image-Text matching (ITM) is a common task for evaluating the quality of
Vision and Language (VL) models. However, existing ITM benchmarks have a
significant limitation. They have many missing correspondences, originating
from the data construction process itself. For example, a caption is only
matched with one image although the caption can be matched with other similar
images and vice versa. To correct the massive false negatives, we construct the
Extended COCO Validation (ECCV) Caption dataset by supplying the missing
associations with machine and human annotators. We employ five state-of-the-art
ITM models with diverse properties for our annotation process. Our dataset
provides x3.6 positive image-to-caption associations and x8.5 caption-to-image
associations compared to the original MS-COCO. We also propose to use an
informative ranking-based metric mAP@R, rather than the popular Recall@K (R@K).
We re-evaluate the existing 25 VL models on existing and proposed benchmarks.
Our findings are that the existing benchmarks, such as COCO 1K R@K, COCO 5K
R@K, CxC R@1 are highly correlated with each other, while the rankings change
when we shift to the ECCV mAP@R. Lastly, we delve into the effect of the bias
introduced by the choice of machine annotator. Source code and dataset are
available at https://github.com/naver-ai/eccv-caption
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1&quot;&gt;Sanghyuk Chun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1&quot;&gt;Wonjae Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Song Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1&quot;&gt;Minsuk Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Seong Joon Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.14650">
<title>SYNTA: A novel approach for deep learning-based image analysis in muscle histopathology using photo-realistic synthetic data. (arXiv:2207.14650v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.14650</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI), machine learning, and deep learning (DL)
methods are becoming increasingly important in the field of biomedical image
analysis. However, to exploit the full potential of such methods, a
representative number of experimentally acquired images containing a
significant number of manually annotated objects is needed as training data.
Here we introduce SYNTA (synthetic data) as a novel approach for the generation
of synthetic, photo-realistic, and highly complex biomedical images as training
data for DL systems. We show the versatility of our approach in the context of
muscle fiber and connective tissue analysis in histological sections. We
demonstrate that it is possible to perform robust and expert-level segmentation
tasks on previously unseen real-world data, without the need for manual
annotations using synthetic training data alone. Being a fully parametric
technique, our approach poses an interpretable and controllable alternative to
Generative Adversarial Networks (GANs) and has the potential to significantly
accelerate quantitative image analysis in a variety of biomedical applications
in microscopy and beyond.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mill_L/0/1/0/all/0/1&quot;&gt;Leonid Mill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aust_O/0/1/0/all/0/1&quot;&gt;Oliver Aust&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ackermann_J/0/1/0/all/0/1&quot;&gt;Jochen A. Ackermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Burger_P/0/1/0/all/0/1&quot;&gt;Philipp Burger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pascual_M/0/1/0/all/0/1&quot;&gt;Monica Pascual&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Palumbo_Zerr_K/0/1/0/all/0/1&quot;&gt;Katrin Palumbo-Zerr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kronke_G/0/1/0/all/0/1&quot;&gt;Gerhard Kr&amp;#xf6;nke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Uderhardt_S/0/1/0/all/0/1&quot;&gt;Stefan Uderhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schett_G/0/1/0/all/0/1&quot;&gt;Georg Schett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Clemen_C/0/1/0/all/0/1&quot;&gt;Christoph S. Clemen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schroder_R/0/1/0/all/0/1&quot;&gt;Rolf Schr&amp;#xf6;der&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Holtzhausen_C/0/1/0/all/0/1&quot;&gt;Christian Holtzhausen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jabari_S/0/1/0/all/0/1&quot;&gt;Samir Jabari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1&quot;&gt;Andreas Maier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gruneboom_A/0/1/0/all/0/1&quot;&gt;Anika Gr&amp;#xfc;neboom&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.00731">
<title>FuRPE: Learning Full-body Reconstruction from Part Experts. (arXiv:2212.00731v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.00731</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of full-body reconstruction, the scarcity of annotated data
often impedes the efficacy of prevailing methods. To address this issue, we
introduce FuRPE, a novel framework that employs part-experts and an ingenious
pseudo ground-truth selection scheme to derive high-quality pseudo labels.
These labels, central to our approach, equip our network with the capability to
efficiently learn from the available data. Integral to FuRPE is a unique
exponential moving average training strategy and expert-derived feature
distillation strategy. These novel elements of FuRPE not only serve to further
refine the model but also to reduce potential biases that may arise from
inaccuracies in pseudo labels, thereby optimizing the network&apos;s training
process and enhancing the robustness of the model. We apply FuRPE to train both
two-stage and fully convolutional single-stage full-body reconstruction
networks. Our exhaustive experiments on numerous benchmark datasets illustrate
a substantial performance boost over existing methods, underscoring FuRPE&apos;s
potential to reshape the state-of-the-art in full-body reconstruction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zhaoxin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yuqing Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhenbo Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1&quot;&gt;Kejian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongyan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jun He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.08123">
<title>Bayesian posterior approximation with stochastic ensembles. (arXiv:2212.08123v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.08123</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce ensembles of stochastic neural networks to approximate the
Bayesian posterior, combining stochastic methods such as dropout with deep
ensembles. The stochastic ensembles are formulated as families of distributions
and trained to approximate the Bayesian posterior with variational inference.
We implement stochastic ensembles based on Monte Carlo dropout, DropConnect and
a novel non-parametric version of dropout and evaluate them on a toy problem
and CIFAR image classification. For both tasks, we test the quality of the
posteriors directly against Hamiltonian Monte Carlo simulations. Our results
show that stochastic ensembles provide more accurate posterior estimates than
other popular baselines for Bayesian inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balabanov_O/0/1/0/all/0/1&quot;&gt;Oleksandr Balabanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehlig_B/0/1/0/all/0/1&quot;&gt;Bernhard Mehlig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linander_H/0/1/0/all/0/1&quot;&gt;Hampus Linander&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.02310">
<title>PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images. (arXiv:2301.02310v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.02310</link>
<description rdf:parseType="Literal">&lt;p&gt;Touch plays a fundamental role in manipulation for humans; however, machine
perception of contact and pressure typically requires invasive sensors. Recent
research has shown that deep models can estimate hand pressure based on a
single RGB image. However, evaluations have been limited to controlled settings
since collecting diverse data with ground-truth pressure measurements is
difficult. We present a novel approach that enables diverse data to be captured
with only an RGB camera and a cooperative participant. Our key insight is that
people can be prompted to apply pressure in a certain way, and this prompt can
serve as a weak label to supervise models to perform well under varied
conditions. We collect a novel dataset with 51 participants making fingertip
contact with diverse objects. Our network, PressureVision++, outperforms human
annotators and prior work. We also demonstrate an application of
PressureVision++ to mixed reality where pressure estimation allows everyday
surfaces to be used as arbitrary touch-sensitive interfaces. Code, data, and
models are available online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grady_P/0/1/0/all/0/1&quot;&gt;Patrick Grady&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_J/0/1/0/all/0/1&quot;&gt;Jeremy A. Collins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chengcheng Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Twigg_C/0/1/0/all/0/1&quot;&gt;Christopher D. Twigg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aneja_K/0/1/0/all/0/1&quot;&gt;Kunal Aneja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hays_J/0/1/0/all/0/1&quot;&gt;James Hays&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kemp_C/0/1/0/all/0/1&quot;&gt;Charles C. Kemp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05308">
<title>SpyroPose: SE(3) Pyramids for Object Pose Distribution Estimation. (arXiv:2303.05308v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05308</link>
<description rdf:parseType="Literal">&lt;p&gt;Object pose estimation is a core computer vision problem and often an
essential component in robotics. Pose estimation is usually approached by
seeking the single best estimate of an object&apos;s pose, but this approach is
ill-suited for tasks involving visual ambiguity. In such cases it is desirable
to estimate the uncertainty as a pose distribution to allow downstream tasks to
make informed decisions. Pose distributions can have arbitrary complexity which
motivates estimating unparameterized distributions, however, until now they
have only been used for orientation estimation on SO(3) due to the difficulty
in training on and normalizing over SE(3). We propose a novel method for pose
distribution estimation on SE(3). We use a hierarchical grid, a pyramid, which
enables efficient importance sampling during training and sparse evaluation of
the pyramid at inference, allowing real time 6D pose distribution estimation.
Our method outperforms state-of-the-art methods on SO(3), and to the best of
our knowledge, we provide the first quantitative results on pose distribution
estimation on SE(3). Code will be available at spyropose.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haugaard_R/0/1/0/all/0/1&quot;&gt;Rasmus Laurvig Haugaard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hagelskjaer_F/0/1/0/all/0/1&quot;&gt;Frederik Hagelskj&amp;#xe6;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iversen_T/0/1/0/all/0/1&quot;&gt;Thorbj&amp;#xf8;rn Mosekj&amp;#xe6;r Iversen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06876">
<title>A Test Statistic Estimation-based Approach for Establishing Self-interpretable CNN-based Binary Classifiers. (arXiv:2303.06876v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06876</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretability is highly desired for deep neural network-based classifiers,
especially when addressing high-stake decisions in medical imaging. Commonly
used post-hoc interpretability methods have the limitation that they can
produce plausible but different interpretations of a given model, leading to
ambiguity about which one to choose. To address this problem, a novel
decision-theory-inspired approach is investigated to establish a
self-interpretable model, given a pre-trained deep binary black-box medical
image classifier. This approach involves utilizing a self-interpretable
encoder-decoder model in conjunction with a single-layer fully connected
network with unity weights. The model is trained to estimate the test statistic
of the given trained black-box deep binary classifier to maintain a similar
accuracy. The decoder output image, referred to as an equivalency map, is an
image that represents a transformed version of the to-be-classified image that,
when processed by the fixed fully connected layer, produces the same test
statistic value as the original classifier. The equivalency map provides a
visualization of the transformed image features that directly contribute to the
test statistic value and, moreover, permits quantification of their relative
contributions. Unlike the traditional post-hoc interpretability methods, the
proposed method is self-interpretable, quantitative. Detailed quantitative and
qualitative analyses have been performed with three different medical image
binary classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sengupta_S/0/1/0/all/0/1&quot;&gt;Sourya Sengupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1&quot;&gt;Mark A. Anastasio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01186">
<title>Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos. (arXiv:2304.01186v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01186</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating text-editable and pose-controllable character videos have an
imperious demand in creating various digital human. Nevertheless, this task has
been restricted by the absence of a comprehensive dataset featuring paired
video-pose captions and the generative prior models for videos. In this work,
we design a novel two-stage training scheme that can utilize easily obtained
datasets (i.e.,image pose pair and pose-free video) and the pre-trained
text-to-image (T2I) model to obtain the pose-controllable character videos.
Specifically, in the first stage, only the keypoint-image pairs are used only
for a controllable text-to-image generation. We learn a zero-initialized
convolutional encoder to encode the pose information. In the second stage, we
finetune the motion of the above network via a pose-free video dataset by
adding the learnable temporal self-attention and reformed cross-frame
self-attention blocks. Powered by our new designs, our method successfully
generates continuously pose-controllable character videos while keeps the
editing and concept composition ability of the pre-trained T2I model. The code
and models will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yue Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yingqing He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cun_X/0/1/0/all/0/1&quot;&gt;Xiaodong Cun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Siran Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qifeng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14484">
<title>OriCon3D: Effective 3D Object Detection using Orientation and Confidence. (arXiv:2304.14484v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14484</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose an advanced methodology for the detection of 3D
objects and precise estimation of their spatial positions from a single image.
Unlike conventional frameworks that rely solely on center-point and dimension
predictions, our research leverages a deep convolutional neural network-based
3D object weighted orientation regression paradigm. These estimates are then
seamlessly integrated with geometric constraints obtained from a 2D bounding
box, resulting in derivation of a comprehensive 3D bounding box. Our novel
network design encompasses two key outputs. The first output involves the
estimation of 3D object orientation through the utilization of a
discrete-continuous loss function. Simultaneously, the second output predicts
objectivity-based confidence scores with minimal variance. Additionally, we
also introduce enhancements to our methodology through the incorporation of
lightweight residual feature extractors. By combining the derived estimates
with the geometric constraints inherent in the 2D bounding box, our approach
significantly improves the accuracy of 3D object pose determination, surpassing
baseline methodologies. Our method is rigorously evaluated on the KITTI 3D
object detection benchmark, demonstrating superior performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajani_D/0/1/0/all/0/1&quot;&gt;Dhyey Manish Rajani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Surya Pratap Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swayampakula_R/0/1/0/all/0/1&quot;&gt;Rahul Kashyap Swayampakula&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09073">
<title>Consensus and Subjectivity of Skin Tone Annotation for ML Fairness. (arXiv:2305.09073v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09073</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding different human attributes and how they affect model behavior
may become a standard need for all model creation and usage, from traditional
computer vision tasks to the newest multimodal generative AI systems. In
computer vision specifically, we have relied on datasets augmented with
perceived attribute signals (e.g., gender presentation, skin tone, and age) and
benchmarks enabled by these datasets. Typically labels for these tasks come
from human annotators. However, annotating attribute signals, especially skin
tone, is a difficult and subjective task. Perceived skin tone is affected by
technical factors, like lighting conditions, and social factors that shape an
annotator&apos;s lived experience. This paper examines the subjectivity of skin tone
annotation through a series of annotation experiments using the Monk Skin Tone
(MST) scale, a small pool of professional photographers, and a much larger pool
of trained crowdsourced annotators. Along with this study we release the Monk
Skin Tone Examples (MST-E) dataset, containing 1515 images and 31 videos spread
across the full MST scale. MST-E is designed to help train human annotators to
annotate MST effectively. Our study shows that annotators can reliably annotate
skin tone in a way that aligns with an expert in the MST scale, even under
challenging environmental conditions. We also find evidence that annotators
from different geographic regions rely on different mental models of MST
categories resulting in annotations that systematically vary across regions.
Given this, we advise practitioners to use a diverse set of annotators and a
higher replication count for each image when annotating skin tone for fairness
research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schumann_C/0/1/0/all/0/1&quot;&gt;Candice Schumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olanubi_G/0/1/0/all/0/1&quot;&gt;Gbolahan O. Olanubi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wright_A/0/1/0/all/0/1&quot;&gt;Auriel Wright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monk_E/0/1/0/all/0/1&quot;&gt;Ellis Monk Jr.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heldreth_C/0/1/0/all/0/1&quot;&gt;Courtney Heldreth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ricco_S/0/1/0/all/0/1&quot;&gt;Susanna Ricco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17033">
<title>The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs). (arXiv:2305.17033v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17033</link>
<description rdf:parseType="Literal">&lt;p&gt;Pediatric tumors of the central nervous system are the most common cause of
cancer-related death in children. The five-year survival rate for high-grade
gliomas in children is less than 20\%. Due to their rarity, the diagnosis of
these entities is often delayed, their treatment is mainly based on historic
treatment concepts, and clinical trials require multi-institutional
collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a
landmark community benchmark event with a successful history of 12 years of
resource creation for the segmentation and analysis of adult glioma. Here we
present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which
represents the first BraTS challenge focused on pediatric brain tumors with
data acquired across multiple international consortia dedicated to pediatric
neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on
benchmarking the development of volumentric segmentation algorithms for
pediatric brain glioma through standardized quantitative performance evaluation
metrics utilized across the BraTS 2023 cluster of challenges. Models gaining
knowledge from the BraTS-PEDs multi-parametric structural MRI (mpMRI) training
data will be evaluated on separate validation and unseen test mpMRI dataof
high-grade pediatric glioma. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023
challenge brings together clinicians and AI/imaging scientists to lead to
faster development of automated segmentation techniques that could benefit
clinical trials, and ultimately the care of children with brain tumors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kazerooni_A/0/1/0/all/0/1&quot;&gt;Anahita Fathi Kazerooni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khalili_N/0/1/0/all/0/1&quot;&gt;Nastaran Khalili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Haldar_D/0/1/0/all/0/1&quot;&gt;Debanjan Haldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhifan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anwar_S/0/1/0/all/0/1&quot;&gt;Syed Muhammed Anwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Albrecht_J/0/1/0/all/0/1&quot;&gt;Jake Albrecht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Adewole_M/0/1/0/all/0/1&quot;&gt;Maruf Adewole&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anazodo_U/0/1/0/all/0/1&quot;&gt;Udunna Anazodo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anderson_H/0/1/0/all/0/1&quot;&gt;Hannah Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bagheri_S/0/1/0/all/0/1&quot;&gt;Sina Bagheri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baid_U/0/1/0/all/0/1&quot;&gt;Ujjwal Baid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bergquist_T/0/1/0/all/0/1&quot;&gt;Timothy Bergquist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Borja_A/0/1/0/all/0/1&quot;&gt;Austin J. Borja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Calabrese_E/0/1/0/all/0/1&quot;&gt;Evan Calabrese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chung_V/0/1/0/all/0/1&quot;&gt;Verena Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Conte_G/0/1/0/all/0/1&quot;&gt;Gian-Marco Conte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dako_F/0/1/0/all/0/1&quot;&gt;Farouk Dako&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eddy_J/0/1/0/all/0/1&quot;&gt;James Eddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ezhov_I/0/1/0/all/0/1&quot;&gt;Ivan Ezhov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Familiar_A/0/1/0/all/0/1&quot;&gt;Ariana Familiar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Farahani_K/0/1/0/all/0/1&quot;&gt;Keyvan Farahani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Haldar_S/0/1/0/all/0/1&quot;&gt;Shuvanjan Haldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1&quot;&gt;Juan Eugenio Iglesias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Janas_A/0/1/0/all/0/1&quot;&gt;Anastasia Janas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Johansen_E/0/1/0/all/0/1&quot;&gt;Elaine Johansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jones_B/0/1/0/all/0/1&quot;&gt;Blaise V Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kofler_F/0/1/0/all/0/1&quot;&gt;Florian Kofler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+LaBella_D/0/1/0/all/0/1&quot;&gt;Dominic LaBella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lai_H/0/1/0/all/0/1&quot;&gt;Hollie Anne Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Leemput_K/0/1/0/all/0/1&quot;&gt;Koen Van Leemput&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongwei Bran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maleki_N/0/1/0/all/0/1&quot;&gt;Nazanin Maleki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+McAllister_A/0/1/0/all/0/1&quot;&gt;Aaron S McAllister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meier_Z/0/1/0/all/0/1&quot;&gt;Zeke Meier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1&quot;&gt;Bjoern Menze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moawad_A/0/1/0/all/0/1&quot;&gt;Ahmed W Moawad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nandolia_K/0/1/0/all/0/1&quot;&gt;Khanak K Nandolia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pavaine_J/0/1/0/all/0/1&quot;&gt;Julija Pavaine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Piraud_M/0/1/0/all/0/1&quot;&gt;Marie Piraud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Poussaint_T/0/1/0/all/0/1&quot;&gt;Tina Poussaint&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prabhu_S/0/1/0/all/0/1&quot;&gt;Sanjay P Prabhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Reitman_Z/0/1/0/all/0/1&quot;&gt;Zachary Reitman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rodriguez_A/0/1/0/all/0/1&quot;&gt;Andres Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rudie_J/0/1/0/all/0/1&quot;&gt;Jeffrey D Rudie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shaikh_I/0/1/0/all/0/1&quot;&gt;Ibraheem Salman Shaikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shah_L/0/1/0/all/0/1&quot;&gt;Lubdha M. Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sheth_N/0/1/0/all/0/1&quot;&gt;Nakul Sheth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shinohara_R/0/1/0/all/0/1&quot;&gt;Russel Taki Shinohara&lt;/a&gt;, et al. (23 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18891">
<title>EmotionGesture: Audio-Driven Diverse Emotional Co-Speech 3D Gesture Generation. (arXiv:2305.18891v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18891</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating vivid and diverse 3D co-speech gestures is crucial for various
applications in animating virtual avatars. While most existing methods can
generate gestures from audio directly, they usually overlook that emotion is
one of the key factors of authentic co-speech gesture generation. In this work,
we propose EmotionGesture, a novel framework for synthesizing vivid and diverse
emotional co-speech 3D gestures from audio. Considering emotion is often
entangled with the rhythmic beat in speech audio, we first develop an
Emotion-Beat Mining module (EBM) to extract the emotion and audio beat features
as well as model their correlation via a transcript-based visual-rhythm
alignment. Then, we propose an initial pose based Spatial-Temporal Prompter
(STP) to generate future gestures from the given initial poses. STP effectively
models the spatial-temporal correlations between the initial poses and the
future gestures, thus producing the spatial-temporal coherent pose prompt. Once
we obtain pose prompts, emotion, and audio beat features, we will generate 3D
co-speech gestures through a transformer architecture. However, considering the
poses of existing datasets often contain jittering effects, this would lead to
generating unstable gestures. To address this issue, we propose an effective
objective function, dubbed Motion-Smooth Loss. Specifically, we model motion
offset to compensate for jittering ground-truth by forcing gestures to be
smooth. Last, we present an emotion-conditioned VAE to sample emotion features,
enabling us to generate diverse emotional results. Extensive experiments
demonstrate that our framework outperforms the state-of-the-art, achieving
vivid and diverse emotional co-speech 3D gestures. Our code and dataset will be
released at the project page:
https://xingqunqi-lab.github.io/Emotion-Gesture-Web/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xingqun Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lincheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Jie Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_H/0/1/0/all/0/1&quot;&gt;Haoran Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05134">
<title>TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation. (arXiv:2307.05134v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05134</link>
<description rdf:parseType="Literal">&lt;p&gt;The progress in the generation of synthetic images has made it crucial to
assess their quality. While several metrics have been proposed to assess the
rendering of images, it is crucial for Text-to-Image (T2I) models, which
generate images based on a prompt, to consider additional aspects such as to
which extent the generated image matches the important content of the prompt.
Moreover, although the generated images usually result from a random starting
point, the influence of this one is generally not considered. In this article,
we propose a new metric based on prompt templates to study the alignment
between the content specified in the prompt and the corresponding generated
images. It allows us to better characterize the alignment in terms of the type
of the specified objects, their number, and their color. We conducted a study
on several recent T2I models about various aspects. An additional interesting
result we obtained with our approach is that image quality can vary drastically
depending on the noise used as a seed for the images. We also quantify the
influence of the number of concepts in the prompt, their order as well as their
(color) attributes. Finally, our method allows us to identify some seeds that
produce better images than others, opening novel directions of research on this
understudied topic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grimal_P/0/1/0/all/0/1&quot;&gt;Paul Grimal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borgne_H/0/1/0/all/0/1&quot;&gt;Herv&amp;#xe9; Le Borgne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferret_O/0/1/0/all/0/1&quot;&gt;Olivier Ferret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tourille_J/0/1/0/all/0/1&quot;&gt;Julien Tourille&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14823">
<title>Fading memory as inductive bias in residual recurrent networks. (arXiv:2307.14823v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14823</link>
<description rdf:parseType="Literal">&lt;p&gt;Residual connections have been proposed as an architecture-based inductive
bias to mitigate the problem of exploding and vanishing gradients and increased
task performance in both feed-forward and recurrent networks (RNNs) when
trained with the backpropagation algorithm. Yet, little is known about how
residual connections in RNNs influence their dynamics and fading memory
properties. Here, we introduce weakly coupled residual recurrent networks
(WCRNNs) in which residual connections result in well-defined Lyapunov
exponents and allow for studying properties of fading memory. We investigate
how the residual connections of WCRNNs influence their performance, network
dynamics, and memory properties on a set of benchmark tasks. We show that
several distinct forms of residual connections yield effective inductive biases
that result in increased network expressivity. In particular, those are
residual connections that (i) result in network dynamics at the proximity of
the edge of chaos, (ii) allow networks to capitalize on characteristic spectral
properties of the data, and (iii) result in heterogeneous memory properties. In
addition, we demonstrate how our results can be extended to non-linear
residuals and introduce a weakly coupled residual initialization scheme that
can be used for Elman RNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubinin_I/0/1/0/all/0/1&quot;&gt;Igor Dubinin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Effenberger_F/0/1/0/all/0/1&quot;&gt;Felix Effenberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07665">
<title>Inversion-by-Inversion: Exemplar-based Sketch-to-Photo Synthesis via Stochastic Differential Equations without Training. (arXiv:2308.07665v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07665</link>
<description rdf:parseType="Literal">&lt;p&gt;Exemplar-based sketch-to-photo synthesis allows users to generate
photo-realistic images based on sketches. Recently, diffusion-based methods
have achieved impressive performance on image generation tasks, enabling
highly-flexible control through text-driven generation or energy functions.
However, generating photo-realistic images with color and texture from sketch
images remains challenging for diffusion models. Sketches typically consist of
only a few strokes, with most regions left blank, making it difficult for
diffusion-based methods to produce photo-realistic images. In this work, we
propose a two-stage method named ``Inversion-by-Inversion&quot; for exemplar-based
sketch-to-photo synthesis. This approach includes shape-enhancing inversion and
full-control inversion. During the shape-enhancing inversion process, an
uncolored photo is generated with the guidance of a shape-energy function. This
step is essential to ensure control over the shape of the generated photo. In
the full-control inversion process, we propose an appearance-energy function to
control the color and texture of the final generated photo.Importantly, our
Inversion-by-Inversion pipeline is training-free and can accept different types
of exemplars for color and texture control. We conducted extensive experiments
to evaluate our proposed method, and the results demonstrate its effectiveness.
The code and project can be found at
https://ximinng.github.io/inversion-by-inversion-project/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1&quot;&gt;Ximing Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chuang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Haitao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhihao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qian Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15752">
<title>Large-scale data extraction from the UNOS organ donor documents. (arXiv:2308.15752v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15752</link>
<description rdf:parseType="Literal">&lt;p&gt;The scope of our study is all UNOS data of the USA organ donors since 2008.
The data is not analyzable in a large scale in the past because it was captured
in PDF documents known as &quot;Attachments&quot;, whereby every donor is represented by
dozens of PDF documents in heterogenous formats. To make the data analyzable,
one needs to convert the content inside these PDFs to an analyzable data
format, such as a standard SQL database. In this paper we will focus on 2022
UNOS data comprised of $\approx 400,000$ PDF documents spanning millions of
pages. The totality of UNOS data covers 15 years (2008--20022) and our results
will be quickly extended to the entire data. Our method captures a portion of
the data in DCD flowsheets, kidney perfusion data, and data captured during
patient hospital stay (e.g. vital signs, ventilator settings, etc.). The
current paper assumes that the reader is familiar with the content of the UNOS
data. The overview of the types of data and challenges they present is a
subject of another paper. Here we focus on demonstrating that the goal of
building a comprehensive, analyzable database from UNOS documents is an
attainable task, and we provide an overview of our methodology. The project
resulted in datasets by far larger than previously available even in this
preliminary phase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rychlik_M/0/1/0/all/0/1&quot;&gt;Marek Rychlik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanriover_B/0/1/0/all/0/1&quot;&gt;Bekir Tanriover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yan Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11715">
<title>Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow removal. (arXiv:2309.11715v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.11715</link>
<description rdf:parseType="Literal">&lt;p&gt;Segment Anything (SAM), an advanced universal image segmentation model
trained on an expansive visual dataset, has set a new benchmark in image
segmentation and computer vision. However, it faced challenges when it came to
distinguishing between shadows and their backgrounds. To address this, we
developed Deshadow-Anything, considering the generalization of large-scale
datasets, and we performed Fine-tuning on large-scale datasets to achieve image
shadow removal. The diffusion model can diffuse along the edges and textures of
an image, helping to remove shadows while preserving the details of the image.
Furthermore, we design Multi-Self-Attention Guidance (MSAG) and adaptive input
perturbation (DDPM-AIP) to accelerate the iterative training speed of
diffusion. Experiments on shadow removal tasks demonstrate that these methods
can effectively improve image restoration performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiao Feng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1&quot;&gt;Tian Yi Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jia Wei Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12867">
<title>Accurate and Fast Compressed Video Captioning. (arXiv:2309.12867v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12867</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing video captioning approaches typically require to first sample video
frames from a decoded video and then conduct a subsequent process (e.g.,
feature extraction and/or captioning model learning). In this pipeline, manual
frame sampling may ignore key information in videos and thus degrade
performance. Additionally, redundant information in the sampled frames may
result in low efficiency in the inference of video captioning. Addressing this,
we study video captioning from a different perspective in compressed domain,
which brings multi-fold advantages over the existing pipeline: 1) Compared to
raw images from the decoded video, the compressed video, consisting of
I-frames, motion vectors and residuals, is highly distinguishable, which allows
us to leverage the entire video for learning without manual sampling through a
specialized model design; 2) The captioning model is more efficient in
inference as smaller and less redundant information is processed. We propose a
simple yet effective end-to-end transformer in the compressed domain for video
captioning that enables learning from the compressed video for captioning. We
show that even with a simple design, our method can achieve state-of-the-art
performance on different benchmarks while running almost 2x faster than
existing approaches. Code is available at https://github.com/acherstyx/CoCap.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yaojie Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Heng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1&quot;&gt;Longyin Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Libo Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05195">
<title>GMMFormer: Gaussian-Mixture-Model Based Transformer for Efficient Partially Relevant Video Retrieval. (arXiv:2310.05195v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05195</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a text query, partially relevant video retrieval (PRVR) seeks to find
untrimmed videos containing pertinent moments in a database. For PRVR, clip
modeling is essential to capture the partial relationship between texts and
videos. Current PRVR methods adopt scanning-based clip construction to achieve
explicit clip modeling, which is information-redundant and requires a large
storage overhead. To solve the efficiency problem of PRVR methods, this paper
proposes GMMFormer, a Gaussian-Mixture-Model based Transformer which models
clip representations implicitly. During frame interactions, we incorporate
Gaussian-Mixture-Model constraints to focus each frame on its adjacent frames
instead of the whole video. Then generated representations will contain
multi-scale clip information, achieving implicit clip modeling. In addition,
PRVR methods ignore semantic differences between text queries relevant to the
same video, leading to a sparse embedding space. We propose a query diverse
loss to distinguish these text queries, making the embedding space more
intensive and contain more semantic information. Extensive experiments on three
large-scale video datasets (i.e., TVR, ActivityNet Captions, and Charades-STA)
demonstrate the superiority and efficiency of GMMFormer. Code is available at
\url{https://github.com/huangmozhi9527/GMMFormer}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuting Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Ziyun Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shu-Tao Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16033">
<title>ViCrop: Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal Large Language Models. (arXiv:2310.16033v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16033</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal Large Language Models (MLLMs) have recently achieved promising
zero-shot accuracy on visual question answering (VQA) -- a fundamental task
affecting various downstream applications and domains. Given the great
potential for the broad use of these models, it is important to investigate
their limitations in dealing with different image and question properties. In
this work, we investigate whether MLLMs can perceive details as well as larger
components in images. In particular, we show that their zero-shot accuracy in
answering visual questions is very sensitive to the size of the visual subject
related to the question, declining up to $45.91\%$ with size. Furthermore, we
show that this effect is causal by observing that human visual cropping can
significantly mitigate their sensitivity to size. To scale up the usefulness of
human cropping, we propose ViCrop, a general framework that utilizes automatic
visual cropping to enhance zero-shot VQA of MLLMs. We construct five variants
of ViCrop leveraging either external localization models or the decision
process of the given MLLM itself. Our results show that ViCrop improves MLLMs&apos;
zero-shot accuracy across different VQA datasets, for example, enhances
BLIP2-T5&apos;s performance by $32.23\%$ on the TextVQA test set. To facilitate
further investigation of MLLMs&apos; behaviors, our code is publicly released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiarui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khayatkhoei_M/0/1/0/all/0/1&quot;&gt;Mahyar Khayatkhoei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chhikara_P/0/1/0/all/0/1&quot;&gt;Prateek Chhikara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1&quot;&gt;Filip Ilievski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17097">
<title>Navigating Data Heterogeneity in Federated Learning A Semi-Supervised Federated Object Detection. (arXiv:2310.17097v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17097</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) has emerged as a potent framework for training models
across distributed data sources while maintaining data privacy. Nevertheless,
it faces challenges with limited high-quality labels and non-IID client data,
particularly in applications like autonomous driving. To address these hurdles,
we navigate the uncharted waters of Semi-Supervised Federated Object Detection
(SSFOD). We present a pioneering SSFOD framework, designed for scenarios where
labeled data reside only at the server while clients possess unlabeled data.
Notably, our method represents the inaugural implementation of SSFOD for
clients with 0% labeled non-IID data, a stark contrast to previous studies that
maintain some subset of labels at each client. We propose FedSTO, a two-stage
strategy encompassing Selective Training followed by Orthogonally enhanced
full-parameter training, to effectively address data shift (e.g. weather
conditions) between server and clients. Our contributions include selectively
refining the backbone of the detector to avert overfitting, orthogonality
regularization to boost representation divergence, and local EMA-driven pseudo
label assignment to yield high-quality pseudo labels. Extensive validation on
prominent autonomous driving datasets (BDD100K, Cityscapes, and SODA10M)
attests to the efficacy of our approach, demonstrating state-of-the-art
results. Remarkably, FedSTO, using just 20-30% of labels, performs nearly as
well as fully-supervised centralized training methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taehyeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1&quot;&gt;Eric Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lau_C/0/1/0/all/0/1&quot;&gt;Christian Lau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mugunthan_V/0/1/0/all/0/1&quot;&gt;Vaikkunth Mugunthan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17190">
<title>Lookup Table meets Local Laplacian Filter: Pyramid Reconstruction Network for Tone Mapping. (arXiv:2310.17190v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17190</link>
<description rdf:parseType="Literal">&lt;p&gt;Tone mapping aims to convert high dynamic range (HDR) images to low dynamic
range (LDR) representations, a critical task in the camera imaging pipeline. In
recent years, 3-Dimensional LookUp Table (3D LUT) based methods have gained
attention due to their ability to strike a favorable balance between
enhancement performance and computational efficiency. However, these methods
often fail to deliver satisfactory results in local areas since the look-up
table is a global operator for tone mapping, which works based on pixel values
and fails to incorporate crucial local information. To this end, this paper
aims to address this issue by exploring a novel strategy that integrates global
and local operators by utilizing closed-form Laplacian pyramid decomposition
and reconstruction. Specifically, we employ image-adaptive 3D LUTs to
manipulate the tone in the low-frequency image by leveraging the specific
characteristics of the frequency information. Furthermore, we utilize local
Laplacian filters to refine the edge details in the high-frequency components
in an adaptive manner. Local Laplacian filters are widely used to preserve edge
details in photographs, but their conventional usage involves manual tuning and
fixed implementation within camera imaging pipelines or photo editing tools. We
propose to learn parameter value maps progressively for local Laplacian filters
from annotated data using a lightweight network. Our model achieves
simultaneous global tone manipulation and local edge detail preservation in an
end-to-end manner. Extensive experimental results on two benchmark datasets
demonstrate that the proposed method performs favorably against
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Feng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_M/0/1/0/all/0/1&quot;&gt;Ming Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Bin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1&quot;&gt;Qingbo Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Changxin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1&quot;&gt;Nong Sang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11467">
<title>SkateboardAI: The Coolest Video Action Recognition for Skateboarding. (arXiv:2311.11467v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11467</link>
<description rdf:parseType="Literal">&lt;p&gt;Impressed by the coolest skateboarding sports program from 2021 Tokyo Olympic
Games, we are the first to curate the original real-world video datasets
&quot;SkateboardAI&quot; in the wild, even self-design and implement diverse uni-modal
and multi-modal video action recognition approaches to recognize different
tricks accurately. For uni-modal methods, we separately apply (1) CNN and LSTM;
(2) CNN and BiLSTM; (3) CNN and BiLSTM with effective attention mechanisms; (4)
Transformer-based action recognition pipeline. Transferred to the multi-modal
conditions, we investigated the two-stream Inflated-3D architecture on
&quot;SkateboardAI&quot; datasets to compare its performance with uni-modal cases. In
sum, our objective is developing an excellent AI sport referee for the coolest
skateboarding competitions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanxiao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01597">
<title>SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference. (arXiv:2312.01597v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01597</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in contrastive language-image pretraining (CLIP) have
demonstrated strong capabilities in zero-shot classification by aligning visual
representations with target text embeddings in an image level. However, in
dense prediction tasks, CLIP often struggles to localize visual features within
an image and fails to give accurate pixel-level predictions, which prevents it
from functioning as a generalized visual foundation model. In this work, we aim
to enhance CLIP&apos;s potential for semantic segmentation with minimal
modifications to its pretrained models. By rethinking self-attention, we
surprisingly find that CLIP can adapt to dense prediction tasks by simply
introducing a novel Correlative Self-Attention (CSA) mechanism. Specifically,
we replace the traditional self-attention block of CLIP vision encoder&apos;s last
layer by our CSA module and reuse its pretrained projection matrices of query,
key, and value, leading to a training-free adaptation approach for CLIP&apos;s
zero-shot semantic segmentation. Extensive experiments show the advantage of
CSA: we obtain a 38.2% average zero-shot mIoU across eight semantic
segmentation benchmarks highlighted in this paper, significantly outperforming
the existing SoTA&apos;s 33.9% and the vanilla CLIP&apos;s 14.1%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Feng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1&quot;&gt;Jieru Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1&quot;&gt;Alan Yuille&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01711">
<title>Regressor-Segmenter Mutual Prompt Learning for Crowd Counting. (arXiv:2312.01711v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01711</link>
<description rdf:parseType="Literal">&lt;p&gt;Crowd counting has achieved significant progress by training regressors to
predict instance positions. In heavily crowded scenarios, however, regressors
are challenged by uncontrollable annotation variance, which causes density map
bias and context information inaccuracy. In this study, we propose mutual
prompt learning (mPrompt), which leverages a regressor and a segmenter as
guidance for each other, solving bias and inaccuracy caused by annotation
variance while distinguishing foreground from background. In specific, mPrompt
leverages point annotations to tune the segmenter and predict pseudo head masks
in a way of point prompt learning. It then uses the predicted segmentation
masks, which serve as spatial constraint, to rectify biased point annotations
as context prompt learning. mPrompt defines a way of mutual information
maximization from prompt learning, mitigating the impact of annotation variance
while improving model accuracy. Experiments show that mPrompt significantly
reduces the Mean Average Error (MAE), demonstrating the potential to be general
framework for down-stream vision tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1&quot;&gt;Mingyue Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Li Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhaoyi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Binghui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qixiang Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05288">
<title>MotionCrafter: One-Shot Motion Customization of Diffusion Models. (arXiv:2312.05288v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05288</link>
<description rdf:parseType="Literal">&lt;p&gt;The essence of a video lies in its dynamic motions, including character
actions, object movements, and camera movements. While text-to-video generative
diffusion models have recently advanced in creating diverse contents,
controlling specific motions through text prompts remains a significant
challenge. A primary issue is the coupling of appearance and motion, often
leading to overfitting on appearance. To tackle this challenge, we introduce
MotionCrafter, a novel one-shot instance-guided motion customization method.
MotionCrafter employs a parallel spatial-temporal architecture that injects the
reference motion into the temporal component of the base model, while the
spatial module is independently adjusted for character or style control. To
enhance the disentanglement of motion and appearance, we propose an innovative
dual-branch motion disentanglement approach, comprising a motion
disentanglement loss and an appearance prior enhancement strategy. During
training, a frozen base model provides appearance normalization, effectively
separating appearance from motion and thereby preserving diversity.
Comprehensive quantitative and qualitative experiments, along with user
preference tests, demonstrate that MotionCrafter can successfully integrate
dynamic motions while preserving the coherence and quality of the base model
with a wide range of appearance generation capabilities. Project page:
https://zyxelsa.github.io/homepage-motioncrafter. Codes are available at
https://github.com/zyxElsa/MotionCrafter.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1&quot;&gt;Fan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_N/0/1/0/all/0/1&quot;&gt;Nisha Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haibin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chongyang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1&quot;&gt;Weiming Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Changsheng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06892">
<title>VitalLens: Take A Vital Selfie. (arXiv:2312.06892v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06892</link>
<description rdf:parseType="Literal">&lt;p&gt;This report introduces VitalLens, an app that estimates vital signs such as
heart rate and respiration rate from selfie video in real time. VitalLens uses
a computer vision model trained on a diverse dataset of video and physiological
sensor data. We benchmark performance on several diverse datasets, including
VV-Medium, which consists of 289 unique participants. VitalLens outperforms
several existing methods including POS and MTTS-CAN on all datasets while
maintaining a fast inference speed. On VV-Medium, VitalLens achieves mean
absolute errors of 0.71 bpm for heart rate estimation, and 0.76 bpm for
respiratory rate estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rouast_P/0/1/0/all/0/1&quot;&gt;Philipp V. Rouast&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11195">
<title>Cross-Age Contrastive Learning for Age-Invariant Face Recognition. (arXiv:2312.11195v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11195</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-age facial images are typically challenging and expensive to collect,
making noise-free age-oriented datasets relatively small compared to
widely-used large-scale facial datasets. Additionally, in real scenarios,
images of the same subject at different ages are usually hard or even
impossible to obtain. Both of these factors lead to a lack of supervised data,
which limits the versatility of supervised methods for age-invariant face
recognition, a critical task in applications such as security and biometrics.
To address this issue, we propose a novel semi-supervised learning approach
named Cross-Age Contrastive Learning (CACon). Thanks to the identity-preserving
power of recent face synthesis models, CACon introduces a new contrastive
learning method that leverages an additional synthesized sample from the input
image. We also propose a new loss function in association with CACon to perform
contrastive learning on a triplet of samples. We demonstrate that our method
not only achieves state-of-the-art performance in homogeneous-dataset
experiments on several age-invariant face recognition benchmarks but also
outperforms other methods by a large margin in cross-dataset experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1&quot;&gt;Victor Sanchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chang-Tsun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13307">
<title>Not All Steps are Equal: Efficient Generation with Progressive Diffusion Models. (arXiv:2312.13307v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13307</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have demonstrated remarkable efficacy in various generative
tasks with the predictive prowess of denoising model. Currently, these models
employ a uniform denoising approach across all timesteps. However, the inherent
variations in noisy latents at each timestep lead to conflicts during training,
constraining the potential of diffusion models. To address this challenge, we
propose a novel two-stage training strategy termed Step-Adaptive Training. In
the initial stage, a base denoising model is trained to encompass all
timesteps. Subsequently, we partition the timesteps into distinct groups,
fine-tuning the model within each group to achieve specialized denoising
capabilities. Recognizing that the difficulties of predicting noise at
different timesteps vary, we introduce a diverse model size requirement. We
dynamically adjust the model size for each timestep by estimating task
difficulty based on its signal-to-noise ratio before fine-tuning. This
adjustment is facilitated by a proxy-based structural importance assessment
mechanism, enabling precise and efficient pruning of the base denoising model.
Our experiments validate the effectiveness of the proposed training strategy,
demonstrating an improvement in the FID score on CIFAR10 by over 0.3 while
utilizing only 80\% of the computational resources. This innovative approach
not only enhances model performance but also significantly reduces
computational costs, opening new avenues for the development and application of
diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1&quot;&gt;Xiu Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1&quot;&gt;Shan You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13763">
<title>Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models. (arXiv:2312.13763v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13763</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-guided diffusion models have revolutionized image and video generation
and have also been successfully used for optimization-based 3D object
synthesis. Here, we instead focus on the underexplored text-to-4D setting and
synthesize dynamic, animated 3D objects using score distillation methods with
an additional temporal dimension. Compared to previous work, we pursue a novel
compositional generation-based approach, and combine text-to-image,
text-to-video, and 3D-aware multiview diffusion models to provide feedback
during 4D object optimization, thereby simultaneously enforcing temporal
consistency, high-quality visual appearance and realistic geometry. Our method,
called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with
deformation fields as 4D representation. Crucial to AYG is a novel method to
regularize the distribution of the moving 3D Gaussians and thereby stabilize
the optimization and induce motion. We also propose a motion amplification
mechanism as well as a new autoregressive synthesis scheme to generate and
combine multiple 4D sequences for longer generation. These techniques allow us
to synthesize vivid dynamic scenes, outperform previous work qualitatively and
quantitatively and achieve state-of-the-art text-to-4D performance. Due to the
Gaussian 4D representation, different 4D animations can be seamlessly combined,
as we demonstrate. AYG opens up promising avenues for animation, simulation and
digital content creation as well as synthetic data generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1&quot;&gt;Huan Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seung Wook Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1&quot;&gt;Antonio Torralba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1&quot;&gt;Sanja Fidler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreis_K/0/1/0/all/0/1&quot;&gt;Karsten Kreis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14705">
<title>SCUNet++: Swin-UNet and CNN Bottleneck Hybrid Architecture with Multi-Fusion Dense Skip Connection for Pulmonary Embolism CT Image Segmentation. (arXiv:2312.14705v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14705</link>
<description rdf:parseType="Literal">&lt;p&gt;Pulmonary embolism (PE) is a prevalent lung disease that can lead to right
ventricular hypertrophy and failure in severe cases, ranking second in severity
only to myocardial infarction and sudden death. Pulmonary artery CT angiography
(CTPA) is a widely used diagnostic method for PE. However, PE detection
presents challenges in clinical practice due to limitations in imaging
technology. CTPA can produce noises similar to PE, making confirmation of its
presence time-consuming and prone to overdiagnosis. Nevertheless, the
traditional segmentation method of PE can not fully consider the hierarchical
structure of features, local and global spatial features of PE CT images. In
this paper, we propose an automatic PE segmentation method called SCUNet++
(Swin Conv UNet++). This method incorporates multiple fusion dense skip
connections between the encoder and decoder, utilizing the Swin Transformer as
the encoder. And fuses features of different scales in the decoder subnetwork
to compensate for spatial information loss caused by the inevitable
downsampling in Swin-UNet or other state-of-the-art methods, effectively
solving the above problem. We provide a theoretical analysis of this method in
detail and validate it on publicly available PE CT image datasets FUMPE and
CAD-PE. The experimental results indicate that our proposed method achieved a
Dice similarity coefficient (DSC) of 83.47% and a Hausdorff distance 95th
percentile (HD95) of 3.83 on the FUMPE dataset, as well as a DSC of 83.42% and
an HD95 of 5.10 on the CAD-PE dataset. These findings demonstrate that our
method exhibits strong performance in PE segmentation tasks, potentially
enhancing the accuracy of automatic segmentation of PE and providing a powerful
diagnostic tool for clinical physicians. Our source code and new FUMPE dataset
are available at https://github.com/JustlfC03/SCUNet-plusplus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zou_B/0/1/0/all/0/1&quot;&gt;Binfeng Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhaoxin Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yifan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qin_F/0/1/0/all/0/1&quot;&gt;Feiwei Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qinhai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changmiao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15927">
<title>M3D: Dataset Condensation by Minimizing Maximum Mean Discrepancy. (arXiv:2312.15927v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15927</link>
<description rdf:parseType="Literal">&lt;p&gt;Training state-of-the-art (SOTA) deep models often requires extensive data,
resulting in substantial training and storage costs. To address these
challenges, dataset condensation has been developed to learn a small synthetic
set that preserves essential information from the original large-scale dataset.
Nowadays, optimization-oriented methods have been the primary method in the
field of dataset condensation for achieving SOTA results. However, the bi-level
optimization process hinders the practical application of such methods to
realistic and larger datasets. To enhance condensation efficiency, previous
works proposed Distribution-Matching (DM) as an alternative, which
significantly reduces the condensation cost. Nonetheless, current DM-based
methods have yielded less comparable results to optimization-oriented methods
due to their focus on aligning only the first moment of the distributions. In
this paper, we present a novel DM-based method named M3D for dataset
condensation by Minimizing the Maximum Mean Discrepancy between feature
representations of the synthetic and real images. By embedding their
distributions in a reproducing kernel Hilbert space, we align all orders of
moments of the distributions of real and synthetic images, resulting in a more
generalized condensed set. Notably, our method even surpasses the SOTA
optimization-oriented method IDC on the high-resolution ImageNet dataset.
Extensive analysis is conducted to verify the effectiveness of the proposed
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hansong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shikun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pengju Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1&quot;&gt;Dan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1&quot;&gt;Shiming Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16250">
<title>A Comprehensive Study of Object Tracking in Low-Light Environments. (arXiv:2312.16250v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16250</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate object tracking in low-light environments is crucial, particularly
in surveillance and ethology applications. However, achieving this is
significantly challenging due to the poor quality of captured sequences.
Factors such as noise, color imbalance, and low contrast contribute to these
challenges. This paper presents a comprehensive study examining the impact of
these distortions on automatic object trackers. Additionally, we propose a
solution to enhance tracking performance by integrating denoising and low-light
enhancement methods into the transformer-based object tracking system.
Experimental results show that the proposed tracker, trained with low-light
synthetic datasets, outperforms both the vanilla MixFormer and Siam R-CNN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_A/0/1/0/all/0/1&quot;&gt;Anqi Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1&quot;&gt;Nantheera Anantrasirichai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16476">
<title>SVGDreamer: Text Guided SVG Generation with Diffusion Model. (arXiv:2312.16476v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16476</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, text-guided scalable vector graphics (SVGs) synthesis has shown
promise in domains such as iconography and sketch. However, existing
text-to-SVG generation methods lack editability and struggle with visual
quality and result diversity. To address these limitations, we propose a novel
text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer
incorporates a semantic-driven image vectorization (SIVE) process that enables
the decomposition of synthesis into foreground objects and background, thereby
enhancing editability. Specifically, the SIVE process introduce attention-based
primitive control and an attention-mask loss function for effective control and
manipulation of individual elements. Additionally, we propose a Vectorized
Particle-based Score Distillation (VPSD) approach to tackle the challenges of
color over-saturation, vector primitives over-smoothing, and limited result
diversity in existing text-to-SVG generation methods. Furthermore, on the basis
of VPSD, we introduce Reward Feedback Learning (ReFL) to accelerate VPSD
convergence and improve aesthetic appeal. Extensive experiments have been
conducted to validate the effectiveness of SVGDreamer, demonstrating its
superiority over baseline methods in terms of editability, visual quality, and
diversity. The code and demo of SVGDreamer can be found at
\href{https://ximinng.github.io/SVGDreamer-project/}{https://ximinng.github.io/SVGDreamer-project/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1&quot;&gt;Ximing Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Haitao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chuang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qian Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.17240">
<title>An Improved Baseline for Reasoning Segmentation with Large Language Model. (arXiv:2312.17240v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.17240</link>
<description rdf:parseType="Literal">&lt;p&gt;While LISA effectively bridges the gap between segmentation and large
language models to enable reasoning segmentation, it poses certain limitations:
unable to distinguish different instances of the target region, and constrained
by the pre-defined textual response formats. In this work, we introduce LISA++,
an update to the existing LISA model, focusing on improving core
functionalities while keeping the base architecture intact. The main
enhancements in LISA++ include: \textbf{1) Enhanced Segmentation}: The instance
segmentation ability has been added, providing a more detailed scene analysis
along with the existing multi-region semantic segmentation. \textbf{2) More
Natural Conversation}: Improved capability for multi-turn dialogue, with the
ability to incorporate segmentation results directly into text responses, i.e.,
Segmentation in Dialogue (SiD). These improvements are achieved by curating the
existing samples of generic segmentation datasets, aimed specifically at
enhancing the segmentation and conversational skills without structural change
and additional data sources. Comparative analysis with the original LISA model
shows significant advancements in these areas, positioning LISA++ as a notable
upgrade in visual understanding and interaction. LISA++&apos;s adaptability and
improved features highlight the versatility of the mask-as-embedding paradigm
proposed by LISA, and the potential as a foundational model for diverse
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Senqiao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_T/0/1/0/all/0/1&quot;&gt;Tianyuan Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1&quot;&gt;Xin Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1&quot;&gt;Zhuotao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1&quot;&gt;Bohao Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jiaya Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00028">
<title>Large OCR Model:An Empirical Study of Scaling Law for OCR. (arXiv:2401.00028v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00028</link>
<description rdf:parseType="Literal">&lt;p&gt;The laws of model size, data volume, computation and model performance have
been extensively studied in the field of Natural Language Processing (NLP).
However, the scaling laws in Optical Character Recognition (OCR) have not yet
been investigated. To address this, we conducted comprehensive studies that
involved examining the correlation between performance and the scale of models,
data volume and computation in the field of text recognition.Conclusively, the
study demonstrates smooth power laws between performance and model size, as
well as training data volume, when other influencing factors are held constant.
Additionally, we have constructed a large-scale dataset called REBU-Syn, which
comprises 6 million real samples and 18 million synthetic samples. Based on our
scaling law and new dataset, we have successfully trained a scene text
recognition model, achieving a new state-ofthe-art on 6 common test benchmarks
with a top-1 average accuracy of 97.42%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rang_M/0/1/0/all/0/1&quot;&gt;Miao Rang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1&quot;&gt;Zhenni Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chuanjian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1&quot;&gt;Kai Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00110">
<title>Diffusion Model with Perceptual Loss. (arXiv:2401.00110v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00110</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models trained with mean squared error loss tend to generate
unrealistic samples. Current state-of-the-art models rely on classifier-free
guidance to improve sample quality, yet its surprising effectiveness is not
fully understood. In this paper, We show that the effectiveness of
classifier-free guidance partly originates from it being a form of implicit
perceptual guidance. As a result, we can directly incorporate perceptual loss
in diffusion training to improve sample quality. Since the score matching
objective used in diffusion training strongly resembles the denoising
autoencoder objective used in unsupervised training of perceptual networks, the
diffusion model itself is a perceptual network and can be used to generate
meaningful perceptual loss. We propose a novel self-perceptual objective that
results in diffusion models capable of generating more realistic samples. For
conditional generation, our method only improves sample quality without
entanglement with the conditional input and therefore does not sacrifice sample
diversity. Our method can also improve sample quality for unconditional
generation, which was not possible with classifier-free guidance before.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Shanchuan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00436">
<title>Diff-PCR: Diffusion-Based Correspondence Searching in Doubly Stochastic Matrix Space for Point Cloud Registration. (arXiv:2401.00436v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00436</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiently finding optimal correspondences between point clouds is crucial
for solving both rigid and non-rigid point cloud registration problems.
Existing methods often rely on geometric or semantic feature embedding to
establish correspondences and estimate transformations or flow fields.
Recently, state-of-the-art methods have employed RAFT-like iterative updates to
refine the solution. However, these methods have certain limitations. Firstly,
their iterative refinement design lacks transparency, and their iterative
updates follow a fixed path during the refinement process, which can lead to
suboptimal results. Secondly, these methods overlook the importance of refining
or optimizing correspondences (or matching matrices) as a precursor to solving
transformations or flow fields. They typically compute candidate
correspondences based on distances in the point feature space. However, they
only project the candidate matching matrix into some matrix space once with
Sinkhorn or dual softmax operations to obtain final correspondences. This
one-shot projected matching matrix may be far from the globally optimal one,
and these approaches do not consider the distribution of the target matching
matrix. In this paper, we propose a novel approach that exploits the Denoising
Diffusion Model to predict a searching gradient for the optimal matching matrix
within the Doubly Stochastic Matrix Space. During the reverse denoising
process, our method iteratively searches for better solutions along this
denoising gradient, which points towards the maximum likelihood direction of
the target matching matrix. Our method offers flexibility by allowing the
search to start from any initial matching matrix provided by the online
backbone or white noise. Experimental evaluations on the 3DMatch/3DLoMatch and
4DMatch/4DLoMatch datasets demonstrate the effectiveness of our newly designed
framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qianliang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Haobo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yaqing Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Lei Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00695">
<title>Credible Teacher for Semi-Supervised Object Detection in Open Scene. (arXiv:2401.00695v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00695</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-Supervised Object Detection (SSOD) has achieved resounding success by
leveraging unlabeled data to improve detection performance. However, in Open
Scene Semi-Supervised Object Detection (O-SSOD), unlabeled data may contains
unknown objects not observed in the labeled data, which will increase
uncertainty in the model&apos;s predictions for known objects. It is detrimental to
the current methods that mainly rely on self-training, as more uncertainty
leads to the lower localization and classification precision of pseudo labels.
To this end, we propose Credible Teacher, an end-to-end framework. Credible
Teacher adopts an interactive teaching mechanism using flexible labels to
prevent uncertain pseudo labels from misleading the model and gradually reduces
its uncertainty through the guidance of other credible pseudo labels. Empirical
results have demonstrated our method effectively restrains the adverse effect
caused by O-SSOD and significantly outperforms existing counterparts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1&quot;&gt;Jingyu Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanbin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00789">
<title>Retrieval-Augmented Egocentric Video Captioning. (arXiv:2401.00789v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00789</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding human actions from videos of first-person view poses
significant challenges. Most prior approaches explore representation learning
on egocentric videos only, while overlooking the potential benefit of
exploiting existing large-scale third-person videos. In this paper, (1) we
develop EgoInstructor, a retrieval-augmented multimodal captioning model that
automatically retrieves semantically relevant third-person instructional videos
to enhance the video captioning of egocentric videos. (2) For training the
cross-view retrieval module, we devise an automatic pipeline to discover
ego-exo video pairs from distinct large-scale egocentric and exocentric
datasets. (3) We train the cross-view retrieval module with a novel EgoExoNCE
loss that pulls egocentric and exocentric video features closer by aligning
them to shared text features that describe similar actions. (4) Through
extensive experiments, our cross-view retrieval module demonstrates superior
performance across seven benchmarks. Regarding egocentric video captioning,
EgoInstructor exhibits significant improvements by leveraging third-person
videos as references.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jilan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yifei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Junlin Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuejie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1&quot;&gt;Rui Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weidi Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01107">
<title>CityPulse: Fine-Grained Assessment of Urban Change with Street View Time Series. (arXiv:2401.01107v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01107</link>
<description rdf:parseType="Literal">&lt;p&gt;Urban transformations have profound societal impact on both individuals and
communities at large. Accurately assessing these shifts is essential for
understanding their underlying causes and ensuring sustainable urban planning.
Traditional measurements often encounter constraints in spatial and temporal
granularity, failing to capture real-time physical changes. While street view
imagery, capturing the heartbeat of urban spaces from a pedestrian point of
view, can add as a high-definition, up-to-date, and on-the-ground visual proxy
of urban change. We curate the largest street view time series dataset to date,
and propose an end-to-end change detection model to effectively capture
physical alterations in the built environment at scale. We demonstrate the
effectiveness of our proposed method by benchmark comparisons with previous
literature and implementing it at the city-wide level. Our approach has the
potential to supplement existing dataset and serve as a fine-grained and
accurate assessment of urban change.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tianyuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zejia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jackelyn Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajagopal_R/0/1/0/all/0/1&quot;&gt;Ram Rajagopal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01219">
<title>Distribution Matching for Multi-Task Learning of Classification Tasks: a Large-Scale Study on Faces &amp; Beyond. (arXiv:2401.01219v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01219</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Task Learning (MTL) is a framework, where multiple related tasks are
learned jointly and benefit from a shared representation space, or parameter
transfer. To provide sufficient learning support, modern MTL uses annotated
data with full, or sufficiently large overlap across tasks, i.e., each input
sample is annotated for all, or most of the tasks. However, collecting such
annotations is prohibitive in many real applications, and cannot benefit from
datasets available for individual tasks. In this work, we challenge this setup
and show that MTL can be successful with classification tasks with little, or
non-overlapping annotations, or when there is big discrepancy in the size of
labeled data per task. We explore task-relatedness for co-annotation and
co-training, and propose a novel approach, where knowledge exchange is enabled
between the tasks via distribution matching. To demonstrate the general
applicability of our method, we conducted diverse case studies in the domains
of affective computing, face recognition, species recognition, and shopping
item classification using nine datasets. Our large-scale study of affective
tasks for basic expression recognition and facial action unit detection
illustrates that our approach is network agnostic and brings large performance
improvements compared to the state-of-the-art in both tasks and across all
studied databases. In all case studies, we show that co-training via
task-relatedness is advantageous and prevents negative transfer (which occurs
when MT model&apos;s performance is worse than that of at least one single-task
model).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kollias_D/0/1/0/all/0/1&quot;&gt;Dimitrios Kollias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharmanska_V/0/1/0/all/0/1&quot;&gt;Viktoriia Sharmanska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1&quot;&gt;Stefanos Zafeiriou&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>