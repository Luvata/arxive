<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-09T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04910" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04911" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04921" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04922" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04928" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04933" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04938" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04939" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04943" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04951" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04954" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04965" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05042" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05054" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05075" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05088" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05089" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05106" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05155" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05160" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05185" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05201" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05263" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05265" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05297" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05371" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05418" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05419" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05450" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05481" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05490" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05511" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05513" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05546" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05580" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05584" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05590" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05599" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05607" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05608" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2010.13380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2012.04132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.01736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.16463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.07028" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.04798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.00445" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.08067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.09633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.04023" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.08789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.13000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10564" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15001" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16841" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01665" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04027" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08013" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11167" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06933" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06294" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11100" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14991" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.04069" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13121" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18590" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00447" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02271" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03839" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04879" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.04905">
<title>Detecting Relevant Information in High-Volume Chat Logs: Keyphrase Extraction for Grooming and Drug Dealing Forensic Analysis. (arXiv:2311.04905v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04905</link>
<description rdf:parseType="Literal">&lt;p&gt;The growing use of digital communication platforms has given rise to various
criminal activities, such as grooming and drug dealing, which pose significant
challenges to law enforcement and forensic experts. This paper presents a
supervised keyphrase extraction approach to detect relevant information in
high-volume chat logs involving grooming and drug dealing for forensic
analysis. The proposed method, JointKPE++, builds upon the JointKPE keyphrase
extractor by employing improvements to handle longer texts effectively. We
evaluate JointKPE++ using BERT-based pre-trained models on grooming and drug
dealing datasets, including BERT, RoBERTa, SpanBERT, and BERTimbau. The results
show significant improvements over traditional approaches and demonstrate the
potential for JointKPE++ to aid forensic experts in efficiently detecting
keyphrases related to criminal activities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alves_J/0/1/0/all/0/1&quot;&gt;Jeovane Hon&amp;#xf3;rio Alves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedroso_H/0/1/0/all/0/1&quot;&gt;Hor&amp;#xe1;cio A. C. G. Pedroso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venetikides_R/0/1/0/all/0/1&quot;&gt;Rafael Honorio Venetikides&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koster_J/0/1/0/all/0/1&quot;&gt;Joel E. M. K&amp;#xf6;ster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grochocki_L/0/1/0/all/0/1&quot;&gt;Luiz Rodrigo Grochocki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freitas_C/0/1/0/all/0/1&quot;&gt;Cinthia O. A. Freitas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barddal_J/0/1/0/all/0/1&quot;&gt;Jean Paul Barddal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04906">
<title>FlaCGEC: A Chinese Grammatical Error Correction Dataset with Fine-grained Linguistic Annotation. (arXiv:2311.04906v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04906</link>
<description rdf:parseType="Literal">&lt;p&gt;Chinese Grammatical Error Correction (CGEC) has been attracting growing
attention from researchers recently. In spite of the fact that multiple CGEC
datasets have been developed to support the research, these datasets lack the
ability to provide a deep linguistic topology of grammar errors, which is
critical for interpreting and diagnosing CGEC approaches. To address this
limitation, we introduce FlaCGEC, which is a new CGEC dataset featured with
fine-grained linguistic annotation. Specifically, we collect raw corpus from
the linguistic schema defined by Chinese language experts, conduct edits on
sentences via rules, and refine generated samples manually, which results in
10k sentences with 78 instantiated grammar points and 3 types of edits. We
evaluate various cutting-edge CGEC methods on the proposed FlaCGEC dataset and
their unremarkable results indicate that this dataset is challenging in
covering a large range of grammatical errors. In addition, we also treat
FlaCGEC as a diagnostic dataset for testing generalization skills and conduct a
thorough evaluation of existing CGEC models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1&quot;&gt;Hanyue Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yike Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qingyuan Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiani Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yunshi Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xuesong Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04910">
<title>Ontology-Driven Processing of Transdisciplinary Domain Knowledge. (arXiv:2311.04910v1 [cs.DL])</title>
<link>http://arxiv.org/abs/2311.04910</link>
<description rdf:parseType="Literal">&lt;p&gt;The monograph discusses certain aspects of modern real-world problems facing
humanity, which are much more challenging than scientific ones. Modern science
is unable to solve them in a fundamental way. Vernadsky&apos;s noosphere thesis, in
fact, appeals to the scientific worldview that needs to be built in a way that
overcomes the interdisciplinary barriers and increases the effectiveness of
interdisciplinary interaction and modern science overall. We are talking about
the general transdisciplinary knowledge. In world practice, there is still no
systematic methodology and a specific form of generally accepted valid
scientific theory that would provide transdisciplinary knowledge. Non-linear
interdisciplinary interaction is the standard of evolution of modern science.
At the same time, a new transdisciplinary theory (domain of scientific
research) is being de facto created and the process is repeated many times:
from an individual or group of disciplines, through interdisciplinary
interaction, in a direction that brings us closer to creating a holistic
general scientific worldview.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palagin_O/0/1/0/all/0/1&quot;&gt;Oleksandr Palagin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrenko_M/0/1/0/all/0/1&quot;&gt;Mykola Petrenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kryvyi_S/0/1/0/all/0/1&quot;&gt;Sergii Kryvyi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boyko_M/0/1/0/all/0/1&quot;&gt;Mykola Boyko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malakhov_K/0/1/0/all/0/1&quot;&gt;Kyrylo Malakhov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04911">
<title>From Text to Structure: Using Large Language Models to Support the Development of Legal Expert Systems. (arXiv:2311.04911v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04911</link>
<description rdf:parseType="Literal">&lt;p&gt;Encoding legislative text in a formal representation is an important
prerequisite to different tasks in the field of AI &amp;amp; Law. For example,
rule-based expert systems focused on legislation can support laypeople in
understanding how legislation applies to them and provide them with helpful
context and information. However, the process of analyzing legislation and
other sources to encode it in the desired formal representation can be
time-consuming and represents a bottleneck in the development of such systems.
Here, we investigate to what degree large language models (LLMs), such as
GPT-4, are able to automatically extract structured representations from
legislation. We use LLMs to create pathways from legislation, according to the
JusticeBot methodology for legal decision support systems, evaluate the
pathways and compare them to manually created pathways. The results are
promising, with 60% of generated pathways being rated as equivalent or better
than manually created ones in a blind comparison. The approach suggests a
promising path to leverage the capabilities of LLMs to ease the costly
development of systems based on symbolic approaches that are transparent and
explainable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janatian_S/0/1/0/all/0/1&quot;&gt;Samyar Janatian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Westermann_H/0/1/0/all/0/1&quot;&gt;Hannes Westermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1&quot;&gt;Jinzhe Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savelka_J/0/1/0/all/0/1&quot;&gt;Jaromir Savelka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benyekhlef_K/0/1/0/all/0/1&quot;&gt;Karim Benyekhlef&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04913">
<title>An Improved Transformer-based Model for Detecting Phishing, Spam, and Ham: A Large Language Model Approach. (arXiv:2311.04913v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04913</link>
<description rdf:parseType="Literal">&lt;p&gt;Phishing and spam detection is long standing challenge that has been the
subject of much academic research. Large Language Models (LLM) have vast
potential to transform society and provide new and innovative approaches to
solve well-established challenges. Phishing and spam have caused financial
hardships and lost time and resources to email users all over the world and
frequently serve as an entry point for ransomware threat actors. While
detection approaches exist, especially heuristic-based approaches, LLMs offer
the potential to venture into a new unexplored area for understanding and
solving this challenge. LLMs have rapidly altered the landscape from business,
consumers, and throughout academia and demonstrate transformational potential
for the potential of society. Based on this, applying these new and innovative
approaches to email detection is a rational next step in academic research. In
this work, we present IPSDM, our model based on fine-tuning the BERT family of
models to specifically detect phishing and spam email. We demonstrate our
fine-tuned version, IPSDM, is able to better classify emails in both unbalanced
and balanced datasets. This work serves as an important first step towards
employing LLMs to improve the security of our information systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamal_S/0/1/0/all/0/1&quot;&gt;Suhaima Jamal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wimmer_H/0/1/0/all/0/1&quot;&gt;Hayden Wimmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04915">
<title>Chain of Empathy: Enhancing Empathetic Response of Large Language Models Based on Psychotherapy Models. (arXiv:2311.04915v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04915</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel method, the Chain of Empathy (CoE) prompting, that
utilizes insights from psychotherapy to induce Large Language Models (LLMs) to
reason about human emotional states. This method is inspired by various
psychotherapy approaches including Cognitive Behavioral Therapy (CBT),
Dialectical Behavior Therapy (DBT), Person Centered Therapy (PCT), and Reality
Therapy (RT), each leading to different patterns of interpreting clients&apos;
mental states. LLMs without reasoning generated predominantly exploratory
responses. However, when LLMs used CoE reasoning, we found a more comprehensive
range of empathetic responses aligned with the different reasoning patterns of
each psychotherapy model. The CBT based CoE resulted in the most balanced
generation of empathetic responses. The findings underscore the importance of
understanding the emotional context and how it affects human and AI
communication. Our research contributes to understanding how psychotherapeutic
models can be incorporated into LLMs, facilitating the development of
context-specific, safer, and empathetic AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yoon Kyung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;Inju Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1&quot;&gt;Minjung Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1&quot;&gt;Seoyeon Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hahn_S/0/1/0/all/0/1&quot;&gt;Sowon Hahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04917">
<title>Adapting Fake News Detection to the Era of Large Language Models. (arXiv:2311.04917v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04917</link>
<description rdf:parseType="Literal">&lt;p&gt;In the age of large language models (LLMs) and the widespread adoption of
AI-driven content creation, the landscape of information dissemination has
witnessed a paradigm shift. With the proliferation of both human-written and
machine-generated real and fake news, robustly and effectively discerning the
veracity of news articles has become an intricate challenge. While substantial
research has been dedicated to fake news detection, this either assumes that
all news articles are human-written or abruptly assumes that all
machine-generated news are fake. Thus, a significant gap exists in
understanding the interplay between machine-(paraphrased) real news,
machine-generated fake news, human-written fake news, and human-written real
news. In this paper, we study this gap by conducting a comprehensive evaluation
of fake news detectors trained in various scenarios. Our primary objectives
revolve around the following pivotal question: How to adapt fake news detectors
to the era of LLMs? Our experiments reveal an interesting pattern that
detectors trained exclusively on human-written articles can indeed perform well
at detecting machine-generated fake news, but not vice versa. Moreover, due to
the bias of detectors against machine-generated texts \cite{su2023fake}, they
should be trained on datasets with a lower machine-generated news ratio than
the test set. Building on our findings, we provide a practical strategy for the
development of robust fake news detectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jinyan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1&quot;&gt;Claire Cardie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1&quot;&gt;Preslav Nakov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04919">
<title>The Impact of Preference Agreement in Reinforcement Learning from Human Feedback: A Case Study in Summarization. (arXiv:2311.04919v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04919</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement Learning from Human Feedback (RLHF) can be used to capture
complex and nuanced properties of text generation quality. As a result, the
task of text summarization has been identified as a good candidate for this
process. In this paper, we explore how preference agreement impacts the
efficacy of RLHF for summarization. We show that sampling human preferences to
include a range of annotator agreement results in (1) higher accuracy reward
models and (2) alters the characteristics of quality captured. We additionally
show improvements in downstream generation when using a reward model trained
with a range of preference agreements. Our contributions have implications for
the design of synthetic datasets as well as the importance of considering
quality differentials in comparison-based data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gooding_S/0/1/0/all/0/1&quot;&gt;Sian Gooding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansoor_H/0/1/0/all/0/1&quot;&gt;Hassan Mansoor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04921">
<title>Successor Features for Efficient Multisubject Controlled Text Generation. (arXiv:2311.04921v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04921</link>
<description rdf:parseType="Literal">&lt;p&gt;While large language models (LLMs) have achieved impressive performance in
generating fluent and realistic text, controlling the generated text so that it
exhibits properties such as safety, factuality, and non-toxicity remains
challenging. % such as DExperts, GeDi, and rectification Existing
decoding-based methods are static in terms of the dimension of control; if the
target subject is changed, they require new training. Moreover, it can quickly
become prohibitive to concurrently control multiple subjects. In this work, we
introduce SF-GEN, which is grounded in two primary concepts: successor features
(SFs) to decouple the LLM&apos;s dynamics from task-specific rewards, and language
model rectification to proportionally adjust the probability of selecting a
token based on the likelihood that the finished text becomes undesired. SF-GEN
seamlessly integrates the two to enable dynamic steering of text generation
with no need to alter the LLM&apos;s parameters. Thanks to the decoupling effect
induced by successor features, our method proves to be memory-wise and
computationally efficient for training as well as decoding, especially when
dealing with multiple target subjects. To the best of our knowledge, our
research represents the first application of successor features in text
generation. In addition to its computational efficiency, the resultant language
produced by our method is comparable to the SOTA (and outperforms baselines) in
both control measures as well as language quality, which we demonstrate through
a series of experiments in various controllable text generation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1&quot;&gt;Meng Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fatemi_M/0/1/0/all/0/1&quot;&gt;Mehdi Fatemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1&quot;&gt;Jackie Chi Kit Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shabanian_S/0/1/0/all/0/1&quot;&gt;Samira Shabanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04922">
<title>Are cascade dialogue state tracking models speaking out of turn in spoken dialogues?. (arXiv:2311.04922v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04922</link>
<description rdf:parseType="Literal">&lt;p&gt;In Task-Oriented Dialogue (TOD) systems, correctly updating the system&apos;s
understanding of the user&apos;s needs is key to a smooth interaction. Traditionally
TOD systems are composed of several modules that interact with one another.
While each of these components is the focus of active research communities,
their behavior in interaction can be overlooked. This paper proposes a
comprehensive analysis of the errors of state of the art systems in complex
settings such as Dialogue State Tracking which highly depends on the dialogue
context. Based on spoken MultiWoz, we identify that errors on non-categorical
slots&apos; values are essential to address in order to bridge the gap between
spoken and chat-based dialogue systems. We explore potential solutions to
improve transcriptions and help dialogue state tracking generative models
correct such errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Druart_L/0/1/0/all/0/1&quot;&gt;Lucas Druart&lt;/a&gt; (LIA), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacqmin_L/0/1/0/all/0/1&quot;&gt;L&amp;#xe9;o Jacqmin&lt;/a&gt; (LIS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Favre_B/0/1/0/all/0/1&quot;&gt;Beno&amp;#xee;t Favre&lt;/a&gt; (LIS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rojas_Barahona_L/0/1/0/all/0/1&quot;&gt;Lina Maria Rojas-Barahona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vielzeuf_V/0/1/0/all/0/1&quot;&gt;Valentin Vielzeuf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04923">
<title>Is one brick enough to break the wall of spoken dialogue state tracking?. (arXiv:2311.04923v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04923</link>
<description rdf:parseType="Literal">&lt;p&gt;In Task-Oriented Dialogue (TOD) systems, correctly updating the system&apos;s
understanding of the user&apos;s needs (a.k.a dialogue state tracking) is key to a
smooth interaction. Traditionally, TOD systems perform this update in three
steps: transcription of the user&apos;s utterance, semantic extraction of the key
concepts, and contextualization with the previously identified concepts. Such
cascade approaches suffer from cascading errors and separate optimization.
End-to-End approaches have been proved helpful up to the semantic extraction
step. This paper goes one step further paving the path towards completely
neural spoken dialogue state tracking by comparing three approaches: (1) a
state of the art cascade approach, (2) a locally E2E approach with rule-based
contextualization and (3) a completely neural approach. Our study highlights
that although they all outperform the recent DSTC11 best model, especially with
a filtering post-processing step, (1) remains the most accurate approach.
Indeed, both (2) and (3) have trouble propagating context as dialogues unfold
showing that context propagation in completely neural approaches is an open
challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Druart_L/0/1/0/all/0/1&quot;&gt;Lucas Druart&lt;/a&gt; (LIA), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vielzeuf_V/0/1/0/all/0/1&quot;&gt;Valentin Vielzeuf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1&quot;&gt;Yannick Est&amp;#xe8;ve&lt;/a&gt; (LIA)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04924">
<title>Tuning-less Object Naming with a Foundation Model. (arXiv:2311.04924v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04924</link>
<description rdf:parseType="Literal">&lt;p&gt;We implement a real-time object naming system that enables learning a set of
named entities never seen. Our approach employs an existing foundation model
that we consider ready to see anything before starting. It turns seen images
into relatively small feature vectors that we associate with index to a
gradually built vocabulary without any training of fine-tuning of the model.
Our contribution is using the association mechanism known from transformers as
attention. It has features that support generalization from irrelevant
information for distinguishing the entities and potentially enable associating
with much more than indices to vocabulary. As a result, the system can work in
a one-shot manner and correctly name objects named in different contents. We
also outline implementation details of the system modules integrated by a
blackboard architecture. Finally, we investigate the system&apos;s quality, mainly
how many objects it can handle in this way.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucny_A/0/1/0/all/0/1&quot;&gt;Andrej Lucny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrovic_P/0/1/0/all/0/1&quot;&gt;Pavel Petrovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04925">
<title>Investigating Deep-Learning NLP for Automating the Extraction of Oncology Efficacy Endpoints from Scientific Literature. (arXiv:2311.04925v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04925</link>
<description rdf:parseType="Literal">&lt;p&gt;Benchmarking drug efficacy is a critical step in clinical trial design and
planning. The challenge is that much of the data on efficacy endpoints is
stored in scientific papers in free text form, so extraction of such data is
currently a largely manual task. Our objective is to automate this task as much
as possible. In this study we have developed and optimised a framework to
extract efficacy endpoints from text in scientific papers, using a machine
learning approach. Our machine learning model predicts 25 classes associated
with efficacy endpoints and leads to high F1 scores (harmonic mean of precision
and recall) of 96.4% on the test set, and 93.9% and 93.7% on two case studies.
These methods were evaluated against - and showed strong agreement with -
subject matter experts and show significant promise in the future of automating
the extraction of clinical endpoints from free text. Clinical information
extraction from text data is currently a laborious manual task which scales
poorly and is prone to human error. Demonstrating the ability to extract
efficacy endpoints automatically shows great promise for accelerating clinical
trial design moving forwards.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gendrin_Brokmann_A/0/1/0/all/0/1&quot;&gt;Aline Gendrin-Brokmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harrison_E/0/1/0/all/0/1&quot;&gt;Eden Harrison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noveras_J/0/1/0/all/0/1&quot;&gt;Julianne Noveras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souliotis_L/0/1/0/all/0/1&quot;&gt;Leonidas Souliotis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vince_H/0/1/0/all/0/1&quot;&gt;Harris Vince&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smit_I/0/1/0/all/0/1&quot;&gt;Ines Smit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_F/0/1/0/all/0/1&quot;&gt;Francisco Costa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milward_D/0/1/0/all/0/1&quot;&gt;David Milward&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimitrievska_S/0/1/0/all/0/1&quot;&gt;Sashka Dimitrievska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metcalfe_P/0/1/0/all/0/1&quot;&gt;Paul Metcalfe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Louvet_E/0/1/0/all/0/1&quot;&gt;Emilie Louvet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04926">
<title>More Robots are Coming: Large Multimodal Models (ChatGPT) can Solve Visually Diverse Images of Parsons Problems. (arXiv:2311.04926v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04926</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of large language models is reshaping computing education. Recent
research has demonstrated that these models can produce better explanations
than students, answer multiple-choice questions at or above the class average,
and generate code that can pass automated tests in introductory courses. These
capabilities have prompted instructors to rapidly adapt their courses and
assessment methods to accommodate changes in learning objectives and the
potential for academic integrity violations. While some scholars have advocated
for the integration of visual problems as a safeguard against the capabilities
of language models, new multimodal language models now have vision and language
capabilities that may allow them to analyze and solve visual problems. In this
paper, we evaluate the performance of two large multimodal models on visual
assignments, with a specific focus on Parsons problems presented across diverse
visual representations. Our results show that GPT-4V solved 96.7\% of these
visual problems, struggling minimally with a single Parsons problem.
Conversely, Bard performed poorly by only solving 69.2\% of problems,
struggling with common issues like hallucinations and refusals. These findings
suggest that merely transitioning to visual programming problems might not be a
panacea to issues of academic integrity in the generative AI era.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_I/0/1/0/all/0/1&quot;&gt;Irene Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Man_O/0/1/0/all/0/1&quot;&gt;Owen Man&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mettille_S/0/1/0/all/0/1&quot;&gt;Sophie Mettille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gutierrez_S/0/1/0/all/0/1&quot;&gt;Sebastian Gutierrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Angelikas_K/0/1/0/all/0/1&quot;&gt;Kenneth Angelikas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacNeil_S/0/1/0/all/0/1&quot;&gt;Stephen MacNeil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04928">
<title>Leveraging Large Language Models for Collective Decision-Making. (arXiv:2311.04928v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04928</link>
<description rdf:parseType="Literal">&lt;p&gt;In various work contexts, such as meeting scheduling, collaborating, and
project planning, collective decision-making is essential but often challenging
due to diverse individual preferences, varying work focuses, and power dynamics
among members. To address this, we propose a system leveraging Large Language
Models (LLMs) to facilitate group decision-making by managing conversations and
balancing preferences among individuals. Our system extracts individual
preferences and suggests options that satisfy a significant portion of the
members. We apply this system to corporate meeting scheduling. We create
synthetic employee profiles and simulate conversations at scale, leveraging
LLMs to evaluate the system. Our results indicate efficient coordination with
reduced interactions between members and the LLM-based system. The system also
effectively refines proposed options over time, ensuring their quality and
equity. Finally, we conduct a survey study involving human participants to
assess our system&apos;s ability to aggregate preferences and reasoning. Our
findings show that the system exhibits strong performance in both dimensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papachristou_M/0/1/0/all/0/1&quot;&gt;Marios Papachristou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Longqi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1&quot;&gt;Chin-Chia Hsu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04929">
<title>An Interdisciplinary Outlook on Large Language Models for Scientific Research. (arXiv:2311.04929v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04929</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we describe the capabilities and constraints of Large Language
Models (LLMs) within disparate academic disciplines, aiming to delineate their
strengths and limitations with precision. We examine how LLMs augment
scientific inquiry, offering concrete examples such as accelerating literature
review by summarizing vast numbers of publications, enhancing code development
through automated syntax correction, and refining the scientific writing
process. Simultaneously, we articulate the challenges LLMs face, including
their reliance on extensive and sometimes biased datasets, and the potential
ethical dilemmas stemming from their use. Our critical discussion extends to
the varying impacts of LLMs across fields, from the natural sciences, where
they help model complex biological sequences, to the social sciences, where
they can parse large-scale qualitative data. We conclude by offering a nuanced
perspective on how LLMs can be both a boon and a boundary to scientific
progress.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boyko_J/0/1/0/all/0/1&quot;&gt;James Boyko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1&quot;&gt;Joseph Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fox_N/0/1/0/all/0/1&quot;&gt;Nathan Fox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veiga_M/0/1/0/all/0/1&quot;&gt;Maria Han Veiga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jennifer I-Hsiu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Modenesi_B/0/1/0/all/0/1&quot;&gt;Bernardo Modenesi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rauch_A/0/1/0/all/0/1&quot;&gt;Andreas H. Rauch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reid_K/0/1/0/all/0/1&quot;&gt;Kenneth N. Reid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tribedi_S/0/1/0/all/0/1&quot;&gt;Soumi Tribedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Visheratina_A/0/1/0/all/0/1&quot;&gt;Anastasia Visheratina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xin Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04930">
<title>Large language models implicitly learn to straighten neural sentence trajectories to construct a predictive representation of natural language. (arXiv:2311.04930v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04930</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting upcoming events is critical to our ability to interact with our
environment. Transformer models, trained on next-word prediction, appear to
construct representations of linguistic input that can support diverse
downstream tasks. But how does a predictive objective shape such
representations? Inspired by recent work in vision (Henaff et al., 2019), we
test a hypothesis about predictive representations of autoregressive
transformers. In particular, we test whether the neural trajectory of a
sentence becomes progressively straighter as it passes through the network
layers. The key insight is that straighter trajectories should facilitate
prediction via linear extrapolation. We quantify straightness using a
1-dimensional curvature metric, and present four findings in support of the
trajectory straightening hypothesis: i) In trained models, the curvature
decreases from the early to the deeper layers of the network. ii) Models that
perform better on the next-word prediction objective exhibit greater decreases
in curvature, suggesting that this improved ability to straighten sentence
trajectories may be the driver of better language modeling performance. iii)
Given the same linguistic context, the sequences that are generated by the
model have lower curvature than the actual continuations observed in a language
corpus, suggesting that the model favors straighter trajectories for making
predictions. iv) A consistent relationship holds between the average curvature
and the average surprisal of sentences in the deep model layers, such that
sentences with straighter trajectories also have lower surprisal. Importantly,
untrained models do not exhibit these behaviors. In tandem, these results
support the trajectory straightening hypothesis and provide a possible
mechanism for how the geometry of the internal representations of
autoregressive models supports next word prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosseini_E/0/1/0/all/0/1&quot;&gt;Eghbal A. Hosseini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fedorenko_E/0/1/0/all/0/1&quot;&gt;Evelina Fedorenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04931">
<title>GPT4All: An Ecosystem of Open Source Compressed Language Models. (arXiv:2311.04931v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04931</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have recently achieved human-level performance
on a range of professional and academic benchmarks. The accessibility of these
models has lagged behind their performance. State-of-the-art LLMs require
costly infrastructure; are only accessible via rate-limited, geo-locked, and
censored web interfaces; and lack publicly available code and technical
reports. In this paper, we tell the story of GPT4All, a popular open source
repository that aims to democratize access to LLMs. We outline the technical
details of the original GPT4All model family, as well as the evolution of the
GPT4All project from a single model into a fully fledged open source ecosystem.
It is our hope that this paper acts as both a technical overview of the
original GPT4All models as well as a case study on the subsequent growth of the
GPT4All open source ecosystem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anand_Y/0/1/0/all/0/1&quot;&gt;Yuvanesh Anand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nussbaum_Z/0/1/0/all/0/1&quot;&gt;Zach Nussbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Treat_A/0/1/0/all/0/1&quot;&gt;Adam Treat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_A/0/1/0/all/0/1&quot;&gt;Aaron Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1&quot;&gt;Richard Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_B/0/1/0/all/0/1&quot;&gt;Ben Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Community_G/0/1/0/all/0/1&quot;&gt;GPT4All Community&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duderstadt_B/0/1/0/all/0/1&quot;&gt;Brandon Duderstadt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mulyar_A/0/1/0/all/0/1&quot;&gt;Andriy Mulyar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04933">
<title>Evaluating Large Language Models in Ophthalmology. (arXiv:2311.04933v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04933</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: The performance of three different large language models (LLMS)
(GPT-3.5, GPT-4, and PaLM2) in answering ophthalmology professional questions
was evaluated and compared with that of three different professional
populations (medical undergraduates, medical masters, and attending
physicians). Methods: A 100-item ophthalmology single-choice test was
administered to three different LLMs (GPT-3.5, GPT-4, and PaLM2) and three
different professional levels (medical undergraduates, medical masters, and
attending physicians), respectively. The performance of LLM was comprehensively
evaluated and compared with the human group in terms of average score,
stability, and confidence. Results: Each LLM outperformed undergraduates in
general, with GPT-3.5 and PaLM2 being slightly below the master&apos;s level, while
GPT-4 showed a level comparable to that of attending physicians. In addition,
GPT-4 showed significantly higher answer stability and confidence than GPT-3.5
and PaLM2. Conclusion: Our study shows that LLM represented by GPT-4 performs
better in the field of ophthalmology. With further improvements, LLM will bring
unexpected benefits in medical education and clinical decision making in the
near future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holmes_J/0/1/0/all/0/1&quot;&gt;Jason Holmes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1&quot;&gt;Shuyuan Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shi-Nan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zihao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jinyu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Huan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hong Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;Jie Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1&quot;&gt;Yi Shao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04934">
<title>Prompt Cache: Modular Attention Reuse for Low-Latency Inference. (arXiv:2311.04934v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04934</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Prompt Cache, an approach for accelerating inference for large
language models (LLM) by reusing attention states across different LLM prompts.
Many input prompts have overlapping text segments, such as system messages,
prompt templates, and documents provided for context. Our key insight is that
by precomputing and storing the attention states of these frequently occurring
text segments on the inference server, we can efficiently reuse them when these
segments appear in user prompts. Prompt Cache employs a schema to explicitly
define such reusable text segments, called prompt modules. The schema ensures
positional accuracy during attention state reuse and provides users with an
interface to access cached states in their prompt. Using a prototype
implementation, we evaluate Prompt Cache across several LLMs. We show that
Prompt Cache significantly reduce latency in time-to-first-token, especially
for longer prompts such as document-based question answering and
recommendations. The improvements range from 8x for GPU-based inference to 60x
for CPU-based inference, all while maintaining output accuracy and without the
need for model parameter modifications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gim_I/0/1/0/all/0/1&quot;&gt;In Gim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guojun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seung-seob Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarda_N/0/1/0/all/0/1&quot;&gt;Nikhil Sarda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1&quot;&gt;Anurag Khandelwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1&quot;&gt;Lin Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04936">
<title>A comparative analysis between Conformer-Transducer, Whisper, and wav2vec2 for improving the child speech recognition. (arXiv:2311.04936v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04936</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic Speech Recognition (ASR) systems have progressed significantly in
their performance on adult speech data; however, transcribing child speech
remains challenging due to the acoustic differences in the characteristics of
child and adult voices. This work aims to explore the potential of adapting
state-of-the-art Conformer-transducer models to child speech to improve child
speech recognition performance. Furthermore, the results are compared with
those of self-supervised wav2vec2 models and semi-supervised multi-domain
Whisper models that were previously finetuned on the same data. We demonstrate
that finetuning Conformer-transducer models on child speech yields significant
improvements in ASR performance on child speech, compared to the non-finetuned
models. We also show Whisper and wav2vec2 adaptation on different child speech
datasets. Our detailed comparative analysis shows that wav2vec2 provides the
most consistent performance improvements among the three methods studied.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barcovschi_A/0/1/0/all/0/1&quot;&gt;Andrei Barcovschi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1&quot;&gt;Rishabh Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corcoran_P/0/1/0/all/0/1&quot;&gt;Peter Corcoran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04937">
<title>Multimodal Clinical Benchmark for Emergency Care (MC-BEC): A Comprehensive Benchmark for Evaluating Foundation Models in Emergency Medicine. (arXiv:2311.04937v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04937</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the Multimodal Clinical Benchmark for Emergency Care (MC-BEC), a
comprehensive benchmark for evaluating foundation models in Emergency Medicine
using a dataset of 100K+ continuously monitored Emergency Department visits
from 2020-2022. MC-BEC focuses on clinically relevant prediction tasks at
timescales from minutes to days, including predicting patient decompensation,
disposition, and emergency department (ED) revisit, and includes a standardized
evaluation framework with train-test splits and evaluation metrics. The
multimodal dataset includes a wide range of detailed clinical data, including
triage information, prior diagnoses and medications, continuously measured
vital signs, electrocardiogram and photoplethysmograph waveforms, orders placed
and medications administered throughout the visit, free-text reports of imaging
studies, and information on ED diagnosis, disposition, and subsequent revisits.
We provide performance baselines for each prediction task to enable the
evaluation of multimodal, multitask models. We believe that MC-BEC will
encourage researchers to develop more effective, generalizable, and accessible
foundation models for multimodal clinical data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1&quot;&gt;Emma Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kansal_A/0/1/0/all/0/1&quot;&gt;Aman Kansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Julie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1&quot;&gt;Boyang Tom Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reisler_J/0/1/0/all/0/1&quot;&gt;Julia Rachel Reisler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;David A Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1&quot;&gt;Pranav Rajpurkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04938">
<title>Improved DDIM Sampling with Moment Matching Gaussian Mixtures. (arXiv:2311.04938v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04938</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose using a Gaussian Mixture Model (GMM) as reverse transition
operator (kernel) within the Denoising Diffusion Implicit Models (DDIM)
framework, which is one of the most widely used approaches for accelerated
sampling from pre-trained Denoising Diffusion Probabilistic Models (DDPM).
Specifically we match the first and second order central moments of the DDPM
forward marginals by constraining the parameters of the GMM. We see that moment
matching is sufficient to obtain samples with equal or better quality than the
original DDIM with Gaussian kernels. We provide experimental results with
unconditional models trained on CelebAHQ and FFHQ and class-conditional models
trained on ImageNet datasets respectively. Our results suggest that using the
GMM kernel leads to significant improvements in the quality of the generated
samples when the number of sampling steps is small, as measured by FID and IS
metrics. For example on ImageNet 256x256, using 10 sampling steps, we achieve a
FID of 6.94 and IS of 207.85 with a GMM kernel compared to 10.15 and 196.73
respectively with a Gaussian kernel.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabbur_P/0/1/0/all/0/1&quot;&gt;Prasad Gabbur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04939">
<title>LooGLE: Can Long-Context Language Models Understand Long Contexts?. (arXiv:2311.04939v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04939</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs), despite their impressive performance in various
language tasks, are typically limited to processing texts within context-window
size. This limitation has spurred significant research efforts to enhance LLMs&apos;
long-context understanding with high-quality long-sequence benchmarks. However,
prior datasets in this regard suffer from shortcomings, such as short context
length compared to the context window of modern LLMs; outdated documents that
have data leakage problems; and an emphasis on short dependency tasks rather
than long dependency tasks. In this paper, we present LooGLE, a Long Context
Generic Language Evaluation benchmark for LLMs&apos; long context understanding.
LooGLE features relatively new documents post-2022, with over 24,000 tokens per
document and 6,000 newly generated questions spanning diverse domains. Human
annotators meticulously crafted more than 1,100 high-quality question-answer
pairs to meet the long dependency requirements. These pairs underwent thorough
cross-validation, yielding the most precise assessment of LLMs&apos; long dependency
capabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed
key findings: (i) commercial models outperformed open-sourced models; (ii) LLMs
excelled in short dependency tasks like short question-answering and cloze
tasks but struggled with more intricate long dependency tasks; (iii) in-context
learning and chaining thoughts offered only marginal improvements; (iv)
retrieval-based techniques demonstrated substantial benefits for short
question-answering, while strategies for extending context window length had
limited impact on long context understanding. As such, LooGLE not only provides
a systematic and comprehensive evaluation schema on long-context LLMs, but also
sheds light on future development of enhanced models towards &quot;true long-context
understanding&quot;.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiaqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengmeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zilong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Muhan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04940">
<title>Interpretable Geoscience Artificial Intelligence (XGeoS-AI): Application to Demystify Image Recognition. (arXiv:2311.04940v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04940</link>
<description rdf:parseType="Literal">&lt;p&gt;As Earth science enters the era of big data, artificial intelligence (AI) not
only offers great potential for solving geoscience problems, but also plays a
critical role in accelerating the understanding of the complex, interactive,
and multiscale processes of Earth&apos;s behavior. As geoscience AI models are
progressively utilized for significant predictions in crucial situations,
geoscience researchers are increasingly demanding their interpretability and
versatility. This study proposes an interpretable geoscience artificial
intelligence (XGeoS-AI) framework to unravel the mystery of image recognition
in the Earth sciences, and its effectiveness and versatility is demonstrated by
taking computed tomography (CT) image recognition as an example. Inspired by
the mechanism of human vision, the proposed XGeoS-AI framework generates a
threshold value from a local region within the whole image to complete the
recognition. Different kinds of artificial intelligence (AI) methods, such as
Support Vector Regression (SVR), Multilayer Perceptron (MLP), Convolutional
Neural Network (CNN), can be adopted as the AI engines of the proposed XGeoS-AI
framework to efficiently complete geoscience image recognition tasks.
Experimental results demonstrate that the effectiveness, versatility, and
heuristics of the proposed framework have great potential in solving geoscience
image recognition problems. Interpretable AI should receive more and more
attention in the field of the Earth sciences, which is the key to promoting
more rational and wider applications of AI in the field of Earth sciences. In
addition, the proposed interpretable framework may be the forerunner of
technological innovation in the Earth sciences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jin-Jian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chao-Sheng Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bin Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04943">
<title>MathNAS: If Blocks Have a Role in Mathematical Architecture Design. (arXiv:2311.04943v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04943</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Architecture Search (NAS) has emerged as a favoured method for
unearthing effective neural architectures. Recent development of large models
has intensified the demand for faster search speeds and more accurate search
results. However, designing large models by NAS is challenging due to the
dramatical increase of search space and the associated huge performance
evaluation cost. Consider a typical modular search space widely used in NAS, in
which a neural architecture consists of $m$ block nodes and a block node has
$n$ alternative blocks. Facing the space containing $n^m$ candidate networks,
existing NAS methods attempt to find the best one by searching and evaluating
candidate networks directly.Different from the general strategy that takes
architecture search as a whole problem, we propose a novel divide-and-conquer
strategy by making use of the modular nature of the search space.Here, we
introduce MathNAS, a general NAS framework based on mathematical programming.In
MathNAS, the performances of the $m*n$ possible building blocks in the search
space are calculated first, and then the performance of a network is directly
predicted based on the performances of its building blocks. Although estimating
block performances involves network training, just as what happens for network
performance evaluation in existing NAS methods, predicting network performance
is completely training-free and thus extremely fast. In contrast to the $n^m$
candidate networks to evaluate in existing NAS methods, which require training
and a formidable computational burden, there are only $m*n$ possible blocks to
handle in MathNAS. Therefore, our approach effectively reduces the complexity
of network performance evaluation.Our code is available at
https://github.com/wangqinsi1/MathNAS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qinsi_W/0/1/0/all/0/1&quot;&gt;Wang Qinsi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jinhan_K/0/1/0/all/0/1&quot;&gt;Ke Jinhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhi_L/0/1/0/all/0/1&quot;&gt;Liang Zhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sihai_Z/0/1/0/all/0/1&quot;&gt;Zhang Sihai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04944">
<title>Edge-assisted U-Shaped Split Federated Learning with Privacy-preserving for Internet of Things. (arXiv:2311.04944v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04944</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of the Internet of Things (IoT), deploying deep learning models
to process data generated or collected by IoT devices is a critical challenge.
However, direct data transmission can cause network congestion and inefficient
execution, given that IoT devices typically lack computation and communication
capabilities. Centralized data processing in data centers is also no longer
feasible due to concerns over data privacy and security. To address these
challenges, we present an innovative Edge-assisted U-Shaped Split Federated
Learning (EUSFL) framework, which harnesses the high-performance capabilities
of edge servers to assist IoT devices in model training and optimization
process. In this framework, we leverage Federated Learning (FL) to enable data
holders to collaboratively train models without sharing their data, thereby
enhancing data privacy protection by transmitting only model parameters.
Additionally, inspired by Split Learning (SL), we split the neural network into
three parts using U-shaped splitting for local training on IoT devices. By
exploiting the greater computation capability of edge servers, our framework
effectively reduces overall training time and allows IoT devices with varying
capabilities to perform training tasks efficiently. Furthermore, we proposed a
novel noise mechanism called LabelDP to ensure that data features and labels
can securely resist reconstruction attacks, eliminating the risk of privacy
leakage. Our theoretical analysis and experimental results demonstrate that
EUSFL can be integrated with various aggregation algorithms, maintaining good
performance across different computing capabilities of IoT devices, and
significantly reducing training time and local computation overhead.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hengliang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zihang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Detian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shiqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1&quot;&gt;Siqing You&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04945">
<title>Auto deep learning for bioacoustic signals. (arXiv:2311.04945v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04945</link>
<description rdf:parseType="Literal">&lt;p&gt;This study investigates the potential of automated deep learning to enhance
the accuracy and efficiency of multi-class classification of bird
vocalizations, compared against traditional manually-designed deep learning
models. Using the Western Mediterranean Wetland Birds dataset, we investigated
the use of AutoKeras, an automated machine learning framework, to automate
neural architecture search and hyperparameter tuning. Comparative analysis
validates our hypothesis that the AutoKeras-derived model consistently
outperforms traditional models like MobileNet, ResNet50 and VGG16. Our approach
and findings underscore the transformative potential of automated deep learning
for advancing bioacoustics research and models. In fact, the automated
techniques eliminate the need for manual feature engineering and model design
while improving performance. This study illuminates best practices in sampling,
evaluation and reporting to enhance reproducibility in this nascent field. All
the code used is available at https:
//github.com/giuliotosato/AutoKeras-bioacustic
&lt;/p&gt;
&lt;p&gt;Keywords: AutoKeras; automated deep learning; audio classification; Wetlands
Bird dataset; comparative analysis; bioacoustics; validation dataset;
multi-class classification; spectrograms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tosato_G/0/1/0/all/0/1&quot;&gt;Giulio Tosato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shehata_A/0/1/0/all/0/1&quot;&gt;Abdelrahman Shehata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janssen_J/0/1/0/all/0/1&quot;&gt;Joshua Janssen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamp_K/0/1/0/all/0/1&quot;&gt;Kees Kamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jati_P/0/1/0/all/0/1&quot;&gt;Pramatya Jati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stowell_D/0/1/0/all/0/1&quot;&gt;Dan Stowell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04946">
<title>Causal Inference on Investment Constraints and Non-stationarity in Dynamic Portfolio Optimization through Reinforcement Learning. (arXiv:2311.04946v1 [q-fin.PM])</title>
<link>http://arxiv.org/abs/2311.04946</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we have developed a dynamic asset allocation investment
strategy using reinforcement learning techniques. To begin with, we have
addressed the crucial issue of incorporating non-stationarity of financial time
series data into reinforcement learning algorithms, which is a significant
implementation in the application of reinforcement learning in investment
strategies. Our findings highlight the significance of introducing certain
variables such as regime change in the environment setting to enhance the
prediction accuracy. Furthermore, the application of reinforcement learning in
investment strategies provides a remarkable advantage of setting the
optimization problem flexibly. This enables the integration of practical
constraints faced by investors into the algorithm, resulting in efficient
optimization. Our study has categorized the investment strategy formulation
conditions into three main categories, including performance measurement
indicators, portfolio management rules, and other constraints. We have
evaluated the impact of incorporating these conditions into the environment and
rewards in a reinforcement learning framework and examined how they influence
investment behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Nakayama_Y/0/1/0/all/0/1&quot;&gt;Yasuhiro Nakayama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Sawaki_T/0/1/0/all/0/1&quot;&gt;Tomochika Sawaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04948">
<title>Explained anomaly detection in text reviews: Can subjective scenarios be correctly evaluated?. (arXiv:2311.04948v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04948</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a pipeline to detect and explain anomalous reviews in
online platforms. The pipeline is made up of three modules and allows the
detection of reviews that do not generate value for users due to either
worthless or malicious composition. The classifications are accompanied by a
normality score and an explanation that justifies the decision made. The
pipeline&apos;s ability to solve the anomaly detection task was evaluated using
different datasets created from a large Amazon database. Additionally, a study
comparing three explainability techniques involving 241 participants was
conducted to assess the explainability module. The study aimed to measure the
impact of explanations on the respondents&apos; ability to reproduce the
classification model and their perceived usefulness. This work can be useful to
automate tasks in review online platforms, such as those for electronic
commerce, and offers inspiration for addressing similar problems in the field
of anomaly detection in textual data. We also consider it interesting to have
carried out a human evaluation of the capacity of different explainability
techniques in a real and infrequent scenario such as the detection of anomalous
reviews, as well as to reflect on whether it is possible to explain tasks as
humanly subjective as this one.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Novoa_Paradela_D/0/1/0/all/0/1&quot;&gt;David Novoa-Paradela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fontenla_Romero_O/0/1/0/all/0/1&quot;&gt;Oscar Fontenla-Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guijarro_Berdinas_B/0/1/0/all/0/1&quot;&gt;Bertha Guijarro-Berdi&amp;#xf1;as&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04951">
<title>Leveraging Speculative Sampling and KV-Cache Optimizations Together for Generative AI using OpenVINO. (arXiv:2311.04951v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04951</link>
<description rdf:parseType="Literal">&lt;p&gt;Inference optimizations are critical for improving user experience and
reducing infrastructure costs and power consumption. In this article, we
illustrate a form of dynamic execution known as speculative sampling to reduce
the overall latency of text generation and compare it with standard
autoregressive sampling. This can be used together with model-based
optimizations (e.g. quantization) to provide an optimized solution. Both
sampling methods make use of KV caching. A Jupyter notebook and some sample
executions are provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barad_H/0/1/0/all/0/1&quot;&gt;Haim Barad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aidova_E/0/1/0/all/0/1&quot;&gt;Ekaterina Aidova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorbachev_Y/0/1/0/all/0/1&quot;&gt;Yury Gorbachev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04954">
<title>Prompt Sketching for Large Language Models. (arXiv:2311.04954v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04954</link>
<description rdf:parseType="Literal">&lt;p&gt;Many recent prompting strategies for large language models (LLMs) query the
model multiple times sequentially -- first to produce intermediate results and
then the final answer. However, using these methods, both decoder and model are
unaware of potential follow-up prompts, leading to disconnected and undesirably
wordy intermediate responses. In this work, we address this issue by proposing
prompt sketching, a new prompting paradigm in which an LLM does not only
respond by completing a prompt, but by predicting values for multiple variables
in a template. This way, sketching grants users more control over the
generation process, e.g., by providing a reasoning framework via intermediate
instructions, leading to better overall results. The key idea enabling
sketching with existing, autoregressive models is to adapt the decoding
procedure to also score follow-up instructions during text generation, thus
optimizing overall template likelihood in inference. Our experiments show that
in a zero-shot setting, prompt sketching outperforms existing, sequential
prompting schemes such as direct asking or chain-of-thought on 7 out of 8 LLM
benchmarking tasks, including state tracking, arithmetic reasoning, and general
question answering. To facilitate future use, we release a number of generic,
yet effective sketches applicable to many tasks, and an open source library
called dclib, powering our sketch-aware decoders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beurer_Kellner_L/0/1/0/all/0/1&quot;&gt;Luca Beurer-Kellner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1&quot;&gt;Mark Niklas M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1&quot;&gt;Marc Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1&quot;&gt;Martin Vechev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04965">
<title>Expressibility-induced Concentration of Quantum Neural Tangent Kernels. (arXiv:2311.04965v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2311.04965</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantum tangent kernel methods provide an efficient approach to analyzing the
performance of quantum machine learning models in the infinite-width limit,
which is of crucial importance in designing appropriate circuit architectures
for certain learning tasks. Recently, they have been adapted to describe the
convergence rate of training errors in quantum neural networks in an analytical
manner. Here, we study the connections between the trainability and
expressibility of quantum tangent kernel models. In particular, for global loss
functions, we rigorously prove that high expressibility of both the global and
local quantum encodings can lead to exponential concentration of quantum
tangent kernel values to zero. Whereas for local loss functions, such issue of
exponential concentration persists owing to the high expressibility, but can be
partially mitigated. We further carry out extensive numerical simulations to
support our analytical theories. Our discoveries unveil a pivotal
characteristic of quantum neural tangent kernels, offering valuable insights
for the design of wide quantum variational circuit models in practical
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Li-Wei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weikang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qi Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhide Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zizhao Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Deng_D/0/1/0/all/0/1&quot;&gt;Dong-Ling Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05014">
<title>Interpreting Pretrained Language Models via Concept Bottlenecks. (arXiv:2311.05014v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.05014</link>
<description rdf:parseType="Literal">&lt;p&gt;Pretrained language models (PLMs) have made significant strides in various
natural language processing tasks. However, the lack of interpretability due to
their ``black-box&apos;&apos; nature poses challenges for responsible implementation.
Although previous studies have attempted to improve interpretability by using,
e.g., attention weights in self-attention layers, these weights often lack
clarity, readability, and intuitiveness. In this research, we propose a novel
approach to interpreting PLMs by employing high-level, meaningful concepts that
are easily understandable for humans. For example, we learn the concept of
``Food&apos;&apos; and investigate how it influences the prediction of a model&apos;s
sentiment towards a restaurant review. We introduce C$^3$M, which combines
human-annotated and machine-generated concepts to extract hidden neurons
designed to encapsulate semantically meaningful and task-specific concepts.
Through empirical evaluations on real-world datasets, we manifest that our
approach offers valuable insights to interpret PLM behavior, helps diagnose
model failures, and enhances model robustness amidst noisy concept labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zhen Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Song Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bo_Y/0/1/0/all/0/1&quot;&gt;Yuan Bo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jundong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05017">
<title>Joint Sensing and Semantic Communications with Multi-Task Deep Learning. (arXiv:2311.05017v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2311.05017</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the integration of deep learning techniques for joint
sensing and communications, with an extension to semantic communications. The
integrated system comprises a transmitter and receiver operating over a
wireless channel, subject to noise and fading effects. The transmitter employs
a deep neural network, namely an encoder, for joint operations of source
coding, channel coding, and modulation, while the receiver utilizes another
deep neural network, namely a decoder, for joint operations of demodulation,
channel decoding, and source decoding to reconstruct the data samples. The
transmitted signal serves a dual purpose, supporting communication with the
receiver and enabling sensing. When a target is present, the reflected signal
is received, and another deep neural network decoder is utilized for sensing.
This decoder is responsible for detecting the target&apos;s presence and determining
its range. All these deep neural networks, including one encoder and two
decoders, undergo joint training through multi-task learning, considering data
and channel characteristics. This paper extends to incorporate semantic
communications by introducing an additional deep neural network, another
decoder at the receiver, operating as a task classifier. This decoder evaluates
the fidelity of label classification for received signals, enhancing the
integration of semantics within the communication process. The study presents
results based on using the CIFAR-10 as the input data and accounting for
channel effects like Additive White Gaussian Noise (AWGN) and Rayleigh fading.
The results underscore the effectiveness of multi-task deep learning in
achieving high-fidelity joint sensing and semantic communications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sagduyu_Y/0/1/0/all/0/1&quot;&gt;Yalin E. Sagduyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erpek_T/0/1/0/all/0/1&quot;&gt;Tugba Erpek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yener_A/0/1/0/all/0/1&quot;&gt;Aylin Yener&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulukus_S/0/1/0/all/0/1&quot;&gt;Sennur Ulukus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05018">
<title>Towards Effective Paraphrasing for Information Disguise. (arXiv:2311.05018v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2311.05018</link>
<description rdf:parseType="Literal">&lt;p&gt;Information Disguise (ID), a part of computational ethics in Natural Language
Processing (NLP), is concerned with best practices of textual paraphrasing to
prevent the non-consensual use of authors&apos; posts on the Internet. Research on
ID becomes important when authors&apos; written online communication pertains to
sensitive domains, e.g., mental health. Over time, researchers have utilized
AI-based automated word spinners (e.g., SpinRewriter, WordAI) for paraphrasing
content. However, these tools fail to satisfy the purpose of ID as their
paraphrased content still leads to the source when queried on search engines.
There is limited prior work on judging the effectiveness of paraphrasing
methods for ID on search engines or their proxies, neural retriever (NeurIR)
models. We propose a framework where, for a given sentence from an author&apos;s
post, we perform iterative perturbation on the sentence in the direction of
paraphrasing with an attempt to confuse the search mechanism of a NeurIR system
when the sentence is queried on it. Our experiments involve the subreddit
&apos;r/AmItheAsshole&apos; as the source of public content and Dense Passage Retriever
as a NeurIR system-based proxy for search engines. Our work introduces a novel
method of phrase-importance rankings using perplexity scores and involves
multi-level phrase substitutions via beam search. Our multi-phrase substitution
scheme succeeds in disguising sentences 82% of the time and hence takes an
essential step towards enabling researchers to disguise sensitive content
effectively before making it public. We also release the code of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Anmol Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Shrey Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonagiri_V/0/1/0/all/0/1&quot;&gt;Vamshi Bonagiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1&quot;&gt;Manas Gaur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reagle_J/0/1/0/all/0/1&quot;&gt;Joseph Reagle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1&quot;&gt;Ponnurangam Kumaraguru&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05032">
<title>Transfer learning from a sparsely annotated dataset of 3D medical images. (arXiv:2311.05032v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.05032</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning leverages pre-trained model features from a large dataset
to save time and resources when training new models for various tasks,
potentially enhancing performance. Due to the lack of large datasets in the
medical imaging domain, transfer learning from one medical imaging model to
other medical imaging models has not been widely explored. This study explores
the use of transfer learning to improve the performance of deep convolutional
neural networks for organ segmentation in medical imaging. A base segmentation
model (3D U-Net) was trained on a large and sparsely annotated dataset; its
weights were used for transfer learning on four new down-stream segmentation
tasks for which a fully annotated dataset was available. We analyzed the
training set size&apos;s influence to simulate scarce data. The results showed that
transfer learning from the base model was beneficial when small datasets were
available, providing significant performance improvements; where fine-tuning
the base model is more beneficial than updating all the network weights with
vanilla transfer learning. Transfer learning with fine-tuning increased the
performance by up to 0.129 (+28\%) Dice score than experiments trained from
scratch, and on average 23 experiments increased the performance by 0.029 Dice
score in the new segmentation tasks. The study also showed that cross-modality
transfer learning using CT scans was beneficial. The findings of this study
demonstrate the potential of transfer learning to improve the efficiency of
annotation and increase the accessibility of accurate organ segmentation in
medical imaging, ultimately leading to improved patient care. We made the
network definition and weights publicly available to benefit other users and
researchers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Humpire_Mamani_G/0/1/0/all/0/1&quot;&gt;Gabriel Efrain Humpire-Mamani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jacobs_C/0/1/0/all/0/1&quot;&gt;Colin Jacobs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prokop_M/0/1/0/all/0/1&quot;&gt;Mathias Prokop&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ginneken_B/0/1/0/all/0/1&quot;&gt;Bram van Ginneken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lessmann_N/0/1/0/all/0/1&quot;&gt;Nikolas Lessmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05042">
<title>Automated Annotation of Scientific Texts for ML-based Keyphrase Extraction and Validation. (arXiv:2311.05042v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2311.05042</link>
<description rdf:parseType="Literal">&lt;p&gt;Advanced omics technologies and facilities generate a wealth of valuable data
daily; however, the data often lacks the essential metadata required for
researchers to find and search them effectively. The lack of metadata poses a
significant challenge in the utilization of these datasets. Machine
learning-based metadata extraction techniques have emerged as a potentially
viable approach to automatically annotating scientific datasets with the
metadata necessary for enabling effective search. Text labeling, usually
performed manually, plays a crucial role in validating machine-extracted
metadata. However, manual labeling is time-consuming; thus, there is an need to
develop automated text labeling techniques in order to accelerate the process
of scientific innovation. This need is particularly urgent in fields such as
environmental genomics and microbiome science, which have historically received
less attention in terms of metadata curation and creation of gold-standard text
mining datasets.
&lt;/p&gt;
&lt;p&gt;In this paper, we present two novel automated text labeling approaches for
the validation of ML-generated metadata for unlabeled texts, with specific
applications in environmental genomics. Our techniques show the potential of
two new ways to leverage existing information about the unlabeled texts and the
scientific domain. The first technique exploits relationships between different
types of data sources related to the same research study, such as publications
and proposals. The second technique takes advantage of domain-specific
controlled vocabularies or ontologies. In this paper, we detail applying these
approaches for ML-generated metadata validation. Our results show that the
proposed label assignment approaches can generate both generic and
highly-specific text labels for the unlabeled texts, with up to 44% of the
labels matching with those suggested by a ML keyword extraction algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amusat_O/0/1/0/all/0/1&quot;&gt;Oluwamayowa O. Amusat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hegde_H/0/1/0/all/0/1&quot;&gt;Harshad Hegde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mungall_C/0/1/0/all/0/1&quot;&gt;Christopher J. Mungall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannakou_A/0/1/0/all/0/1&quot;&gt;Anna Giannakou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byers_N/0/1/0/all/0/1&quot;&gt;Neil P. Byers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunter_D/0/1/0/all/0/1&quot;&gt;Dan Gunter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fagnan_K/0/1/0/all/0/1&quot;&gt;Kjiersten Fagnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramakrishnan_L/0/1/0/all/0/1&quot;&gt;Lavanya Ramakrishnan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05043">
<title>Zero-shot Translation of Attention Patterns in VQA Models to Natural Language. (arXiv:2311.05043v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05043</link>
<description rdf:parseType="Literal">&lt;p&gt;Converting a model&apos;s internals to text can yield human-understandable
insights about the model. Inspired by the recent success of training-free
approaches for image captioning, we propose ZS-A2T, a zero-shot framework that
translates the transformer attention of a given model into natural language
without requiring any training. We consider this in the context of Visual
Question Answering (VQA). ZS-A2T builds on a pre-trained large language model
(LLM), which receives a task prompt, question, and predicted answer, as inputs.
The LLM is guided to select tokens which describe the regions in the input
image that the VQA model attended to. Crucially, we determine this similarity
by exploiting the text-image matching capabilities of the underlying VQA model.
Our framework does not require any training and allows the drop-in replacement
of different guiding sources (e.g. attribution instead of attention maps), or
language models. We evaluate this novel task on textual explanation datasets
for VQA, giving state-of-the-art performances for the zero-shot setting on
GQA-REX and VQA-X. Our code is available at:
https://github.com/ExplainableML/ZS-A2T.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salewski_L/0/1/0/all/0/1&quot;&gt;Leonard Salewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koepke_A/0/1/0/all/0/1&quot;&gt;A. Sophia Koepke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1&quot;&gt;Hendrik P. A. Lensch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1&quot;&gt;Zeynep Akata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05054">
<title>Geometry-Calibrated DRO: Combating Over-Pessimism with Free Energy Implications. (arXiv:2311.05054v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.05054</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning algorithms minimizing average risk are susceptible to
distributional shifts. Distributionally Robust Optimization (DRO) addresses
this issue by optimizing the worst-case risk within an uncertainty set.
However, DRO suffers from over-pessimism, leading to low-confidence
predictions, poor parameter estimations as well as poor generalization. In this
work, we conduct a theoretical analysis of a probable root cause of
over-pessimism: excessive focus on noisy samples. To alleviate the impact of
noise, we incorporate data geometry into calibration terms in DRO, resulting in
our novel Geometry-Calibrated DRO (GCDRO) for regression. We establish the
connection between our risk objective and the Helmholtz free energy in
statistical physics, and this free-energy-based risk can extend to standard DRO
methods. Leveraging gradient flow in Wasserstein space, we develop an
approximate minimax optimization algorithm with a bounded error ratio and
elucidate how our approach mitigates noisy sample effects. Comprehensive
experiments confirm GCDRO&apos;s superiority over conventional DRO methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiashuo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiayun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1&quot;&gt;Hao Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1&quot;&gt;Peng Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05067">
<title>Accelerating Exploration with Unlabeled Prior Data. (arXiv:2311.05067v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.05067</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning to solve tasks from a sparse reward signal is a major challenge for
standard reinforcement learning (RL) algorithms. However, in the real world,
agents rarely need to solve sparse reward tasks entirely from scratch. More
often, we might possess prior experience to draw on that provides considerable
guidance about which actions and outcomes are possible in the world, which we
can use to explore more effectively for new tasks. In this work, we study how
prior data without reward labels may be used to guide and accelerate
exploration for an agent solving a new sparse reward task. We propose a simple
approach that learns a reward model from online experience, labels the
unlabeled prior data with optimistic rewards, and then uses it concurrently
alongside the online data for downstream policy and critic optimization. This
general formula leads to rapid exploration in several challenging sparse-reward
domains where tabula rasa exploration is insufficient, including the AntMaze
domain, Adroit hand manipulation domain, and a visual simulated robotic
manipulation domain. Our results highlight the ease of incorporating unlabeled
prior data into existing online RL algorithms, and the (perhaps surprising)
effectiveness of doing so.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qiyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jason Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1&quot;&gt;Dibya Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Amy Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05074">
<title>A Framework to Assess (Dis)agreement Among Diverse Rater Groups. (arXiv:2311.05074v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.05074</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in conversational AI have created an urgent need for
safety guardrails that prevent users from being exposed to offensive and
dangerous content. Much of this work relies on human ratings and feedback, but
does not account for the fact that perceptions of offense and safety are
inherently subjective and that there may be systematic disagreements between
raters that align with their socio-demographic identities. Instead, current
machine learning approaches largely ignore rater subjectivity and use gold
standards that obscure disagreements (e.g., through majority voting). In order
to better understand the socio-cultural leanings of such tasks, we propose a
comprehensive disagreement analysis framework to measure systematic diversity
in perspectives among different rater subgroups. We then demonstrate its
utility by applying this framework to a dataset of human-chatbot conversations
rated by a demographically diverse pool of raters. Our analysis reveals
specific rater groups that have more diverse perspectives than the rest, and
informs demographic axes that are crucial to consider for safety annotations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1&quot;&gt;Vinodkumar Prabhakaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Homan_C/0/1/0/all/0/1&quot;&gt;Christopher Homan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aroyo_L/0/1/0/all/0/1&quot;&gt;Lora Aroyo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parrish_A/0/1/0/all/0/1&quot;&gt;Alicia Parrish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_A/0/1/0/all/0/1&quot;&gt;Alex Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1&quot;&gt;Mark D&amp;#xed;az&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Ding Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05075">
<title>Mental Health Diagnosis in the Digital Age: Harnessing Sentiment Analysis on Social Media Platforms upon Ultra-Sparse Feature Content. (arXiv:2311.05075v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.05075</link>
<description rdf:parseType="Literal">&lt;p&gt;Amid growing global mental health concerns, particularly among vulnerable
groups, natural language processing offers a tremendous potential for early
detection and intervention of people&apos;s mental disorders via analyzing their
postings and discussions on social media platforms. However, ultra-sparse
training data, often due to vast vocabularies and low-frequency words, hinders
the analysis accuracy. Multi-labeling and Co-occurrences of symptoms may also
blur the boundaries in distinguishing similar/co-related disorders. To address
these issues, we propose a novel semantic feature preprocessing technique with
a three-folded structure: 1) mitigating the feature sparsity with a weak
classifier, 2) adaptive feature dimension with modulus loops, and 3)
deep-mining and extending features among the contexts. With enhanced semantic
features, we train a machine learning model to predict and classify mental
disorders. We utilize the Reddit Mental Health Dataset 2022 to examine
conditions such as Anxiety, Borderline Personality Disorder (BPD), and
Bipolar-Disorder (BD) and present solutions to the data sparsity challenge,
highlighted by 99.81% non-zero elements. After applying our preprocessing
technique, the feature sparsity decreases to 85.4%. Overall, our methods, when
compared to seven benchmark models, demonstrate significant performance
improvements: 8.0% in accuracy, 0.069 in precision, 0.093 in recall, 0.102 in
F1 score, and 0.059 in AUC. This research provides foundational insights for
mental health prediction and monitoring, providing innovative solutions to
navigate challenges associated with ultra-sparse data feature and intricate
multi-label classification in the domain of mental health analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1&quot;&gt;Haijian Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Ming Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1&quot;&gt;Shengjie Zhai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05084">
<title>Signal Temporal Logic-Guided Apprenticeship Learning. (arXiv:2311.05084v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.05084</link>
<description rdf:parseType="Literal">&lt;p&gt;Apprenticeship learning crucially depends on effectively learning rewards,
and hence control policies from user demonstrations. Of particular difficulty
is the setting where the desired task consists of a number of sub-goals with
temporal dependencies. The quality of inferred rewards and hence policies are
typically limited by the quality of demonstrations, and poor inference of these
can lead to undesirable outcomes. In this letter, we show how temporal logic
specifications that describe high level task objectives, are encoded in a graph
to define a temporal-based metric that reasons about behaviors of demonstrators
and the learner agent to improve the quality of inferred rewards and policies.
Through experiments on a diverse set of robot manipulator simulations, we show
how our framework overcomes the drawbacks of prior literature by drastically
improving the number of demonstrations required to learn a control policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puranic_A/0/1/0/all/0/1&quot;&gt;Aniruddh G. Puranic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshmukh_J/0/1/0/all/0/1&quot;&gt;Jyotirmoy V. Deshmukh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolaidis_S/0/1/0/all/0/1&quot;&gt;Stefanos Nikolaidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05085">
<title>Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks. (arXiv:2311.05085v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.05085</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are proficient at generating fluent text with
minimal task-specific supervision. Yet, their ability to provide well-grounded
rationalizations for knowledge-intensive tasks remains under-explored. Such
tasks, like commonsense multiple-choice questions, require rationales based on
world knowledge to support predictions and refute alternate options. We
consider the task of generating knowledge-guided rationalization in natural
language by using expert-written examples in a few-shot manner. Surprisingly,
crowd-workers preferred knowledge-grounded rationales over crowdsourced
rationalizations, citing their factuality, sufficiency, and comprehensive
refutations. Although LLMs-generated rationales were preferable, further
improvements in conciseness and novelty are required. In another study, we show
how rationalization of incorrect model predictions erodes humans&apos; trust in
LLM-generated rationales. Motivated by these observations, we create a
two-stage pipeline to review task predictions and eliminate potential incorrect
decisions before rationalization, enabling trustworthy rationale generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1&quot;&gt;Aditi Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1&quot;&gt;Sajjadur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hannah Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_K/0/1/0/all/0/1&quot;&gt;Kushan Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1&quot;&gt;Estevam Hruschka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05088">
<title>Meta-learning of semi-supervised learning from tasks with heterogeneous attribute spaces. (arXiv:2311.05088v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.05088</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a meta-learning method for semi-supervised learning that learns
from multiple tasks with heterogeneous attribute spaces. The existing
semi-supervised meta-learning methods assume that all tasks share the same
attribute space, which prevents us from learning with a wide variety of tasks.
With the proposed method, the expected test performance on tasks with a small
amount of labeled data is improved with unlabeled data as well as data in
various tasks, where the attribute spaces are different among tasks. The
proposed method embeds labeled and unlabeled data simultaneously in a
task-specific space using a neural network, and the unlabeled data&apos;s labels are
estimated by adapting classification or regression models in the embedding
space. For the neural network, we develop variable-feature self-attention
layers, which enable us to find embeddings of data with different attribute
spaces with a single neural network by considering interactions among examples,
attributes, and labels. Our experiments on classification and regression
datasets with heterogeneous attribute spaces demonstrate that our proposed
method outperforms the existing meta-learning and semi-supervised learning
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iwata_T/0/1/0/all/0/1&quot;&gt;Tomoharu Iwata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumagai_A/0/1/0/all/0/1&quot;&gt;Atsutoshi Kumagai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05089">
<title>Legal-HNet: Mixing Legal Long-Context Tokens with Hartley Transform. (arXiv:2311.05089v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.05089</link>
<description rdf:parseType="Literal">&lt;p&gt;Since its introduction, the transformers architecture has seen great adoption
in NLP applications, but it also has limitations. Although the self-attention
mechanism allows for generating very rich representations of the input text,
its effectiveness may be limited in specialized domains such as legal, where,
for example, language models often have to process very long texts. In this
paper, we explore alternatives to replace the attention-based layers with
simpler token-mixing mechanisms: Hartley and Fourier transforms. Using these
non-parametric techniques, we train models with long input documents from
scratch in the legal domain setting. We also introduce a new hybrid Seq2Seq
architecture, a no-attention-based encoder connected with an attention-based
decoder, which performs quite well on existing summarization tasks with much
less compute and memory requirements. We believe that similar, if not better
performance, as in the case of long correlations of abstractive text
summarization tasks, can be achieved by adopting these simpler infrastructures.
This not only makes training models from scratch accessible to more people, but
also contributes to the reduction of the carbon footprint during training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giofre_D/0/1/0/all/0/1&quot;&gt;Daniele Giofr&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghantasala_S/0/1/0/all/0/1&quot;&gt;Sneha Ghantasala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05106">
<title>A differentiable brain simulator bridging brain simulation and brain-inspired computing. (arXiv:2311.05106v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2311.05106</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain simulation builds dynamical models to mimic the structure and functions
of the brain, while brain-inspired computing (BIC) develops intelligent systems
by learning from the structure and functions of the brain. The two fields are
intertwined and should share a common programming framework to facilitate each
other&apos;s development. However, none of the existing software in the fields can
achieve this goal, because traditional brain simulators lack differentiability
for training, while existing deep learning (DL) frameworks fail to capture the
biophysical realism and complexity of brain dynamics. In this paper, we
introduce BrainPy, a differentiable brain simulator developed using JAX and
XLA, with the aim of bridging the gap between brain simulation and BIC. BrainPy
expands upon the functionalities of JAX, a powerful AI framework, by
introducing complete capabilities for flexible, efficient, and scalable brain
simulation. It offers a range of sparse and event-driven operators for
efficient and scalable brain simulation, an abstraction for managing the
intricacies of synaptic computations, a modular and flexible interface for
constructing multi-scale brain models, and an object-oriented just-in-time
compilation approach to handle the memory-intensive nature of brain dynamics.
We showcase the efficiency and scalability of BrainPy on benchmark tasks,
highlight its differentiable simulation for biologically plausible spiking
models, and discuss its potential to support research at the intersection of
brain simulation and BIC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaoming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianqiu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Sichao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yifeng Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1&quot;&gt;Hongyaoxing Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shangyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Si Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05112">
<title>A Survey of Large Language Models in Medicine: Progress, Application, and Challenge. (arXiv:2311.05112v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.05112</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs), such as ChatGPT, have achieved substantial
attention due to their impressive human language understanding and generation
capabilities. Therefore, the application of LLMs in medicine to assist
physicians and patient care emerges as a promising research direction in both
artificial intelligence and clinical medicine. To this end, this survey
provides a comprehensive overview of the current progress, applications, and
challenges faced by LLMs in medicine. Specifically, we aim to address the
following questions: 1) What are LLMs and how can medical LLMs be built? 2)
What are the downstream performances of medical LLMs? 3) How can medical LLMs
be utilized in real-world clinical practice? 4) What challenges arise from the
use of medical LLMs? 5) How can we better construct and utilize medical LLMs?
As a result, this survey aims to provide insights into the opportunities and
challenges of LLMs in medicine and serve as a valuable resource for
constructing practical and effective medical LLMs. A regularly updated list of
practical guide resources of medical LLMs can be found at
https://github.com/AI-in-Health/MedLLMsPracticalGuide.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hongjian Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1&quot;&gt;Boyang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xinyu Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiru Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sam S. Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Peilin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Junling Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1&quot;&gt;Yining Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1&quot;&gt;Chengfeng Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fenglin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05152">
<title>Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks. (arXiv:2311.05152v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.05152</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the deployment of large-scale pre-trained models in
audio-visual downstream tasks has yielded remarkable outcomes. However, these
models, primarily trained on single-modality unconstrained datasets, still
encounter challenges in feature extraction for multi-modal tasks, leading to
suboptimal performance. This limitation arises due to the introduction of
irrelevant modality-specific information during encoding, which adversely
affects the performance of downstream tasks. To address this challenge, this
paper proposes a novel Dual-Guided Spatial-Channel-Temporal (DG-SCT) attention
mechanism. This mechanism leverages audio and visual modalities as soft prompts
to dynamically adjust the parameters of pre-trained models based on the current
multi-modal input features. Specifically, the DG-SCT module incorporates
trainable cross-modal interaction layers into pre-trained audio-visual
encoders, allowing adaptive extraction of crucial information from the current
modality across spatial, channel, and temporal dimensions, while preserving the
frozen parameters of large-scale pre-trained models. Experimental evaluations
demonstrate that our proposed model achieves state-of-the-art results across
multiple downstream tasks, including AVE, AVVP, AVS, and AVQA. Furthermore, our
model exhibits promising performance in challenging few-shot and zero-shot
scenarios. The source code and pre-trained models are available at
https://github.com/haoyi-duan/DG-SCT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1&quot;&gt;Haoyi Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingze Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Li Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jieming Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhou Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05155">
<title>Weakly-supervised Deep Cognate Detection Framework for Low-Resourced Languages Using Morphological Knowledge of Closely-Related Languages. (arXiv:2311.05155v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.05155</link>
<description rdf:parseType="Literal">&lt;p&gt;Exploiting cognates for transfer learning in under-resourced languages is an
exciting opportunity for language understanding tasks, including unsupervised
machine translation, named entity recognition and information retrieval.
Previous approaches mainly focused on supervised cognate detection tasks based
on orthographic, phonetic or state-of-the-art contextual language models, which
under-perform for most under-resourced languages. This paper proposes a novel
language-agnostic weakly-supervised deep cognate detection framework for
under-resourced languages using morphological knowledge from closely related
languages. We train an encoder to gain morphological knowledge of a language
and transfer the knowledge to perform unsupervised and weakly-supervised
cognate detection tasks with and without the pivot language for the
closely-related languages. While unsupervised, it overcomes the need for
hand-crafted annotation of cognates. We performed experiments on different
published cognate detection datasets across language families and observed not
only significant improvement over the state-of-the-art but also our method
outperformed the state-of-the-art supervised and unsupervised methods. Our
model can be extended to a wide range of languages from any language family as
it overcomes the requirement of the annotation of the cognate pairs for
training. The code and dataset building scripts can be found at
https://github.com/koustavagoswami/Weakly_supervised-Cognate_Detection
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goswami_K/0/1/0/all/0/1&quot;&gt;Koustava Goswami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rani_P/0/1/0/all/0/1&quot;&gt;Priya Rani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fransen_T/0/1/0/all/0/1&quot;&gt;Theodorus Fransen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCrae_J/0/1/0/all/0/1&quot;&gt;John P. McCrae&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05160">
<title>RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information. (arXiv:2311.05160v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.05160</link>
<description rdf:parseType="Literal">&lt;p&gt;As the IT industry advances, system log data becomes increasingly crucial.
Many computer systems rely on log texts for management due to restricted access
to source code. The need for log anomaly detection is growing, especially in
real-world applications, but identifying anomalies in rapidly accumulating logs
remains a challenging task. Traditional deep learning-based anomaly detection
models require dataset-specific training, leading to corresponding delays.
Notably, most methods only focus on sequence-level log information, which makes
the detection of subtle anomalies harder, and often involve inference processes
that are difficult to utilize in real-time. We introduce RAPID, a model that
capitalizes on the inherent features of log data to enable anomaly detection
without training delays, ensuring real-time capability. RAPID treats logs as
natural language, extracting representations using pre-trained language models.
Given that logs can be categorized based on system context, we implement a
retrieval-based technique to contrast test logs with the most similar normal
logs. This strategy not only obviates the need for log-specific training but
also adeptly incorporates token-level information, ensuring refined and robust
detection, particularly for unseen logs. We also propose the core set
technique, which can reduce the computational cost needed for comparison.
Experimental results show that even without training on log data, RAPID
demonstrates competitive performance compared to prior models and achieves the
best performance on certain datasets. Through various research questions, we
verified its capability for real-time detection without delay.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+No_G/0/1/0/all/0/1&quot;&gt;Gunho No&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yukyung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1&quot;&gt;Hyeongwon Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_P/0/1/0/all/0/1&quot;&gt;Pilsung Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05168">
<title>FireMatch: A Semi-Supervised Video Fire Detection Network Based on Consistency and Distribution Alignment. (arXiv:2311.05168v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05168</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning techniques have greatly enhanced the performance of fire
detection in videos. However, video-based fire detection models heavily rely on
labeled data, and the process of data labeling is particularly costly and
time-consuming, especially when dealing with videos. Considering the limited
quantity of labeled video data, we propose a semi-supervised fire detection
model called FireMatch, which is based on consistency regularization and
adversarial distribution alignment. Specifically, we first combine consistency
regularization with pseudo-label. For unlabeled data, we design video data
augmentation to obtain corresponding weakly augmented and strongly augmented
samples. The proposed model predicts weakly augmented samples and retains
pseudo-label above a threshold, while training on strongly augmented samples to
predict these pseudo-labels for learning more robust feature representations.
Secondly, we generate video cross-set augmented samples by adversarial
distribution alignment to expand the training data and alleviate the decline in
classification performance caused by insufficient labeled data. Finally, we
introduce a fairness loss to help the model produce diverse predictions for
input samples, thereby addressing the issue of high confidence with the
non-fire class in fire classification scenarios. The FireMatch achieved an
accuracy of 76.92% and 91.81% on two real-world fire datasets, respectively.
The experimental results demonstrate that the proposed method outperforms the
current state-of-the-art semi-supervised classification methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Qinghua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zuoyong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1&quot;&gt;Kun Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Haoyi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiaoguang Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05185">
<title>Mixture of Weak &amp; Strong Experts on Graphs. (arXiv:2311.05185v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.05185</link>
<description rdf:parseType="Literal">&lt;p&gt;Realistic graphs contain both rich self-features of nodes and informative
structures of neighborhoods, jointly handled by a GNN in the typical setup. We
propose to decouple the two modalities by mixture of weak and strong experts
(Mowst), where the weak expert is a light-weight Multi-layer Perceptron (MLP),
and the strong expert is an off-the-shelf Graph Neural Network (GNN). To adapt
the experts&apos; collaboration to different target nodes, we propose a &quot;confidence&quot;
mechanism based on the dispersion of the weak expert&apos;s prediction logits. The
strong expert is conditionally activated when either the node&apos;s classification
relies on neighborhood information, or the weak expert has low model quality.
We reveal interesting training dynamics by analyzing the influence of the
confidence function on loss: our training algorithm encourages the
specialization of each expert by effectively generating soft splitting of the
graph. In addition, our &quot;confidence&quot; design imposes a desirable bias toward the
strong expert to benefit from GNN&apos;s better generalization capability. Mowst is
easy to optimize and achieves strong expressive power, with a computation cost
comparable to a single GNN. Empirically, Mowst shows significant accuracy
improvement on 6 standard node classification benchmarks (including both
homophilous and heterophilous graphs).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1&quot;&gt;Hanqing Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1&quot;&gt;Hanjia Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Diyi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yinglong Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jiebo Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05197">
<title>Deep Learning in Computed Tomography Pulmonary Angiography Imaging: A Dual-Pronged Approach for Pulmonary Embolism Detection. (arXiv:2311.05197v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05197</link>
<description rdf:parseType="Literal">&lt;p&gt;Pulmonary Embolism (PE) is a critical medical condition characterized by
obstructions in the pulmonary arteries. Despite being a major health concern,
it often goes underdiagnosed leading to detrimental clinical outcomes. The
increasing reliance on Computed Tomography Pulmonary Angiography for diagnosis
presents challenges and a pressing need for enhanced diagnostic solutions. The
primary objective of this study is to leverage deep learning techniques to
enhance the Computer Assisted Diagnosis of PE. This study presents a
comprehensive dual-pronged approach combining classification and detection for
PE diagnosis. We introduce an Attention-Guided Convolutional Neural Network
(AG-CNN) for classification, addressing both global and local lesion region.
For detection, state-of-the-art models are employed to pinpoint potential PE
regions. Different ensembling techniques further improve detection accuracy by
combining predictions from different models. Finally, a heuristic strategy
integrates classifier outputs with detection results, ensuring robust and
accurate PE identification. Our attention-guided classification approach,
tested on the Ferdowsi University of Mashhad&apos;s Pulmonary Embolism (FUMPE)
dataset, outperformed the baseline model DenseNet-121 by achieving an 8.1%
increase in the Area Under the Receiver Operating Characteristic. By employing
ensemble techniques with detection models, the mean average precision (mAP) was
considerably enhanced by a 4.7% increase. The classifier-guided framework
further refined the mAP and F1 scores over the ensemble models. Our research
offers a comprehensive approach to PE diagnostics using deep learning,
addressing the prevalent issues of underdiagnosis and misdiagnosis. We aim to
improve PE patient care by integrating AI solutions into clinical workflows,
highlighting the potential of human-AI collaboration in medical diagnostics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bushra_F/0/1/0/all/0/1&quot;&gt;Fabiha Bushra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1&quot;&gt;Muhammad E. H. Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarmun_R/0/1/0/all/0/1&quot;&gt;Rusab Sarmun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kabir_S/0/1/0/all/0/1&quot;&gt;Saidul Kabir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Said_M/0/1/0/all/0/1&quot;&gt;Menatalla Said&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoghoul_S/0/1/0/all/0/1&quot;&gt;Sohaib Bassam Zoghoul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mushtak_A/0/1/0/all/0/1&quot;&gt;Adam Mushtak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Hashimi_I/0/1/0/all/0/1&quot;&gt;Israa Al-Hashimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alqahtani_A/0/1/0/all/0/1&quot;&gt;Abdulrahman Alqahtani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1&quot;&gt;Anwarul Hasan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05201">
<title>Green Resilience of Cyber-Physical Systems. (arXiv:2311.05201v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2311.05201</link>
<description rdf:parseType="Literal">&lt;p&gt;Cyber-Physical System (CPS) represents systems that join both hardware and
software components to perform real-time services. Maintaining the system&apos;s
reliability is critical to the continuous delivery of these services. However,
the CPS running environment is full of uncertainties and can easily lead to
performance degradation. As a result, the need for a recovery technique is
highly needed to achieve resilience in the system, with keeping in mind that
this technique should be as green as possible. This early doctorate proposal,
suggests a game theory solution to achieve resilience and green in CPS. Game
theory has been known for its fast performance in decision-making, helping the
system to choose what maximizes its payoffs. The proposed game model is
described over a real-life collaborative artificial intelligence system (CAIS),
that involves robots with humans to achieve a common goal. It shows how the
expected results of the system will achieve the resilience of CAIS with
minimized CO2 footprint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rimawi_D/0/1/0/all/0/1&quot;&gt;Diaeddin Rimawi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05227">
<title>Kantian Deontology Meets AI Alignment: Towards Morally Robust Fairness Metrics. (arXiv:2311.05227v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.05227</link>
<description rdf:parseType="Literal">&lt;p&gt;Deontological ethics, specifically understood through Immanuel Kant, provides
a moral framework that emphasizes the importance of duties and principles,
rather than the consequences of action. Understanding that despite the
prominence of deontology, it is currently an overlooked approach in fairness
metrics, this paper explores the compatibility of a Kantian deontological
framework in fairness metrics, part of the AI alignment field. We revisit
Kant&apos;s critique of utilitarianism, which is the primary approach in AI fairness
metrics and argue that fairness principles should align with the Kantian
deontological framework. By integrating Kantian ethics into AI alignment, we
not only bring in a widely-accepted prominent moral theory but also strive for
a more morally grounded AI landscape that better balances outcomes and
procedures in pursuit of fairness and justice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mougan_C/0/1/0/all/0/1&quot;&gt;Carlos Mougan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brand_J/0/1/0/all/0/1&quot;&gt;Joshua Brand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05245">
<title>Uncertainty Wrapper in the medical domain: Establishing transparent uncertainty quantification for opaque machine learning models in practice. (arXiv:2311.05245v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.05245</link>
<description rdf:parseType="Literal">&lt;p&gt;When systems use data-based models that are based on machine learning (ML),
errors in their results cannot be ruled out. This is particularly critical if
it remains unclear to the user how these models arrived at their decisions and
if errors can have safety-relevant consequences, as is often the case in the
medical field. In such cases, the use of dependable methods to quantify the
uncertainty remaining in a result allows the user to make an informed decision
about further usage and draw possible conclusions based on a given result. This
paper demonstrates the applicability and practical utility of the Uncertainty
Wrapper using flow cytometry as an application from the medical field that can
benefit from the use of ML models in conjunction with dependable and
transparent uncertainty quantification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jockel_L/0/1/0/all/0/1&quot;&gt;Lisa J&amp;#xf6;ckel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klas_M/0/1/0/all/0/1&quot;&gt;Michael Kl&amp;#xe4;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popp_G/0/1/0/all/0/1&quot;&gt;Georg Popp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilger_N/0/1/0/all/0/1&quot;&gt;Nadja Hilger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fricke_S/0/1/0/all/0/1&quot;&gt;Stephan Fricke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05263">
<title>Model-Based Minimum Bayes Risk Decoding. (arXiv:2311.05263v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.05263</link>
<description rdf:parseType="Literal">&lt;p&gt;Minimum Bayes Risk (MBR) decoding has been shown to be a powerful alternative
to beam search decoding in a variety of text generation tasks. MBR decoding
selects a hypothesis from a pool of hypotheses that has the least expected risk
under a probability model according to a given utility function. Since it is
impractical to compute the expected risk exactly over all possible hypotheses,
two approximations are commonly used in MBR. First, it integrates over a
sampled set of hypotheses rather than over all possible hypotheses. Second, it
estimates the probability of each hypothesis using a Monte Carlo estimator.
While the first approximation is necessary to make it computationally feasible,
the second is not essential since we typically have access to the model
probability at inference time. We propose Model-Based MBR (MBMBR), a variant of
MBR that uses the model probability itself as the estimate of the probability
distribution instead of the Monte Carlo estimate. We show analytically and
empirically that the model-based estimate is more promising than the Monte
Carlo estimate in text generation tasks. Our experiments show that MBMBR
outperforms MBR in several text generation tasks, both with encoder-decoder
models and with large language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jinnai_Y/0/1/0/all/0/1&quot;&gt;Yuu Jinnai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morimura_T/0/1/0/all/0/1&quot;&gt;Tetsuro Morimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honda_U/0/1/0/all/0/1&quot;&gt;Ukyo Honda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ariu_K/0/1/0/all/0/1&quot;&gt;Kaito Ariu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abe_K/0/1/0/all/0/1&quot;&gt;Kenshi Abe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05265">
<title>Don&apos;t Waste a Single Annotation: Improving Single-Label Classifiers Through Soft Labels. (arXiv:2311.05265v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.05265</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we address the limitations of the common data annotation and
training methods for objective single-label classification tasks. Typically,
when annotating such tasks annotators are only asked to provide a single label
for each sample and annotator disagreement is discarded when a final hard label
is decided through majority voting. We challenge this traditional approach,
acknowledging that determining the appropriate label can be difficult due to
the ambiguity and lack of context in the data samples. Rather than discarding
the information from such ambiguous annotations, our soft label method makes
use of them for training. Our findings indicate that additional annotator
information, such as confidence, secondary label and disagreement, can be used
to effectively generate soft labels. Training classifiers with these soft
labels then leads to improved performance and calibration on the hard label
test set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Ben Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1&quot;&gt;Yida Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1&quot;&gt;Carolina Scarton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1&quot;&gt;Kalina Bontcheva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xingyi Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05297">
<title>Do personality tests generalize to Large Language Models?. (arXiv:2311.05297v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.05297</link>
<description rdf:parseType="Literal">&lt;p&gt;With large language models (LLMs) appearing to behave increasingly human-like
in text-based interactions, it has become popular to attempt to evaluate
various properties of these models using tests originally designed for humans.
While re-using existing tests is a resource-efficient way to evaluate LLMs,
careful adjustments are usually required to ensure that test results are even
valid across human sub-populations. Thus, it is not clear to what extent
different tests&apos; validity generalizes to LLMs. In this work, we provide
evidence that LLMs&apos; responses to personality tests systematically deviate from
typical human responses, implying that these results cannot be interpreted in
the same way as human test results. Concretely, reverse-coded items (e.g. &quot;I am
introverted&quot; vs &quot;I am extraverted&quot;) are often both answered affirmatively by
LLMs. In addition, variation across different prompts designed to &quot;steer&quot; LLMs
to simulate particular personality types does not follow the clear separation
into five independent personality factors from human samples. In light of these
results, we believe it is important to pay more attention to tests&apos; validity
for LLMs before drawing strong conclusions about potentially ill-defined
concepts like LLMs&apos; &quot;personality&quot;.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dorner_F/0/1/0/all/0/1&quot;&gt;Florian E. Dorner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suhr_T/0/1/0/all/0/1&quot;&gt;Tom S&amp;#xfc;hr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samadi_S/0/1/0/all/0/1&quot;&gt;Samira Samadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kelava_A/0/1/0/all/0/1&quot;&gt;Augustin Kelava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05304">
<title>Data Valuation and Detections in Federated Learning. (arXiv:2311.05304v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.05304</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) enables collaborative model training without sharing
raw data, demanding abundant, high-quality data for optimal model performance.
Fair and efficient data evaluation is a fundamental issue for incentivizing
clients to provide more high-quality data. Meanwhile, it is likely that only a
subset of clients and datasets are relevant for a learning task while the rest
of them may have a negative impact on the model training. This paper introduces
a novel privacy-preserving method for evaluating client contributions and
selecting relevant data samples without a pre-specified training algorithm. Our
proposed approach, FedBary, utilizes Wasserstein distance within the federated
context, offering a new pioneering solution for data valuation, which provides
transparent data evaluation and efficient computation of Wasserstein barycenter
to mitigate reliance on validation data. We conduct extensive empirical
experiments and theoretical analysis, showing the promising research of this
valuation metric.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenqian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1&quot;&gt;Shuran Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fengrui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1&quot;&gt;Yan Pang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05316">
<title>ABIGX: A Unified Framework for eXplainable Fault Detection and Classification. (arXiv:2311.05316v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.05316</link>
<description rdf:parseType="Literal">&lt;p&gt;For explainable fault detection and classification (FDC), this paper proposes
a unified framework, ABIGX (Adversarial fault reconstruction-Based Integrated
Gradient eXplanation). ABIGX is derived from the essentials of previous
successful fault diagnosis methods, contribution plots (CP) and
reconstruction-based contribution (RBC). It is the first explanation framework
that provides variable contributions for the general FDC models. The core part
of ABIGX is the adversarial fault reconstruction (AFR) method, which rethinks
the FR from the perspective of adversarial attack and generalizes to fault
classification models with a new fault index. For fault classification, we put
forward a new problem of fault class smearing, which intrinsically hinders the
correct explanation. We prove that ABIGX effectively mitigates this problem and
outperforms the existing gradient-based explanation methods. For fault
detection, we theoretically bridge ABIGX with conventional fault diagnosis
methods by proving that CP and RBC are the linear specifications of ABIGX. The
experiments evaluate the explanations of FDC by quantitative metrics and
intuitive illustrations, the results of which show the general superiority of
ABIGX to other advanced explanation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_Y/0/1/0/all/0/1&quot;&gt;Yue Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1&quot;&gt;Jinchuan Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhihuan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05332">
<title>On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving. (arXiv:2311.05332v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05332</link>
<description rdf:parseType="Literal">&lt;p&gt;The pursuit of autonomous driving technology hinges on the sophisticated
integration of perception, decision-making, and control systems. Traditional
approaches, both data-driven and rule-based, have been hindered by their
inability to grasp the nuance of complex driving environments and the
intentions of other road users. This has been a significant bottleneck,
particularly in the development of common sense reasoning and nuanced scene
understanding necessary for safe and reliable autonomous driving. The advent of
Visual Language Models (VLM) represents a novel frontier in realizing fully
autonomous vehicle driving. This report provides an exhaustive evaluation of
the latest state-of-the-art VLM, \modelnamefull, and its application in
autonomous driving scenarios. We explore the model&apos;s abilities to understand
and reason about driving scenes, make decisions, and ultimately act in the
capacity of a driver. Our comprehensive tests span from basic scene recognition
to complex causal reasoning and real-time decision-making under varying
conditions. Our findings reveal that \modelname demonstrates superior
performance in scene understanding and causal reasoning compared to existing
autonomous systems. It showcases the potential to handle out-of-distribution
scenarios, recognize intentions, and make informed decisions in real driving
contexts. However, challenges remain, particularly in direction discernment,
traffic light recognition, vision grounding, and spatial reasoning tasks. These
limitations underscore the need for further research and development. Project
is now available on GitHub for interested parties to access and utilize:
\url{https://github.com/PJLab-ADG/GPT4V-AD-Exploration}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1&quot;&gt;Licheng Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xuemeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1&quot;&gt;Daocheng Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1&quot;&gt;Pinlong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Linran Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_D/0/1/0/all/0/1&quot;&gt;Dengke Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shaoyan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yeqi Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1&quot;&gt;Xinyu Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_M/0/1/0/all/0/1&quot;&gt;Min Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shuanglu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Botian Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05371">
<title>Training Robust Deep Physiological Measurement Models with Synthetic Video-based Data. (arXiv:2311.05371v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05371</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in supervised deep learning techniques have demonstrated the
possibility to remotely measure human physiological vital signs (e.g.,
photoplethysmograph, heart rate) just from facial videos. However, the
performance of these methods heavily relies on the availability and diversity
of real labeled data. Yet, collecting large-scale real-world data with
high-quality labels is typically challenging and resource intensive, which also
raises privacy concerns when storing personal bio-metric data. Synthetic
video-based datasets (e.g., SCAMPS~\cite{mcduff2022scamps}) with
photo-realistic synthesized avatars are introduced to alleviate the issues
while providing high-quality synthetic data. However, there exists a
significant gap between synthetic and real-world data, which hinders the
generalization of neural models trained on these synthetic datasets. In this
paper, we proposed several measures to add real-world noise to synthetic
physiological signals and corresponding facial videos. We experimented with
individual and combined augmentation methods and evaluated our framework on
three public real-world datasets. Our results show that we were able to reduce
the average MAE from 6.9 to 2.0.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Ou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuntang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1&quot;&gt;Shwetak Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDuf_D/0/1/0/all/0/1&quot;&gt;Daniel McDuf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05374">
<title>TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for Human-Aligned LLMs. (arXiv:2311.05374v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.05374</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have shown impressive capabilities across
various natural language tasks. However, evaluating their alignment with human
preferences remains a challenge. To this end, we propose a comprehensive human
evaluation framework to assess LLMs&apos; proficiency in following instructions on
diverse real-world tasks. We construct a hierarchical task tree encompassing 7
major areas covering over 200 categories and over 800 tasks, which covers
diverse capabilities such as question answering, reasoning, multiturn dialogue,
and text generation, to evaluate LLMs in a comprehensive and in-depth manner.
We also design detailed evaluation standards and processes to facilitate
consistent, unbiased judgments from human evaluators. A test set of over 3,000
instances is released, spanning different difficulty levels and knowledge
domains. Our work provides a standardized methodology to evaluate human
alignment in LLMs for both English and Chinese. We also analyze the feasibility
of automating parts of evaluation with a strong LLM (GPT-4). Our framework
supports a thorough assessment of LLMs as they are integrated into real-world
applications. We have made publicly available the task tree, TencentLLMEval
dataset, and evaluation methodology which have been demonstrated as effective
in assessing the performance of Tencent Hunyuan LLMs. By doing so, we aim to
facilitate the benchmarking of advances in the development of safe and
human-aligned LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Shuyi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wenlin Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yong Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shaobo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Donlin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1&quot;&gt;Lifeng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1&quot;&gt;Xinhua Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1&quot;&gt;Pengzhi Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yujie Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhichao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhengyou Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1&quot;&gt;Jing Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuhong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05418">
<title>Generalization in medical AI: a perspective on developing scalable models. (arXiv:2311.05418v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.05418</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past few years, research has witnessed the advancement of deep
learning models trained on large datasets, some even encompassing millions of
examples. While these impressive performance on their hidden test sets, they
often underperform when assessed on external datasets. Recognizing the critical
role of generalization in medical AI development, many prestigious journals now
require reporting results both on the local hidden test set as well as on
external datasets before considering a study for publication. Effectively, the
field of medical AI has transitioned from the traditional usage of a single
dataset that is split into train and test to a more comprehensive framework
using multiple datasets, some of which are used for model development (source
domain) and others for testing (target domains). However, this new experimental
setting does not necessarily resolve the challenge of generalization. This is
because of the variability encountered in intended use and specificities across
hospital cultures making the idea of universally generalizable systems a myth.
On the other hand, the systematic, and a fortiori recurrent re-calibration, of
models at the individual hospital level, although ideal, may be overoptimistic
given the legal, regulatory and technical challenges that are involved.
Re-calibration using transfer learning may not even be possible in some
instances where reference labels of target domains are not available. In this
perspective we establish a hierarchical three-level scale system reflecting the
generalization level of a medical AI algorithm. This scale better reflects the
diversity of real-world medical scenarios per which target domain data for
re-calibration of models may or not be available and if it is, may or not have
reference labels systematically available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behar_J/0/1/0/all/0/1&quot;&gt;Joachim A. Behar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_J/0/1/0/all/0/1&quot;&gt;Jeremy Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1&quot;&gt;Leo Anthony Celi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05419">
<title>Mirror: A Universal Framework for Various Information Extraction Tasks. (arXiv:2311.05419v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.05419</link>
<description rdf:parseType="Literal">&lt;p&gt;Sharing knowledge between information extraction tasks has always been a
challenge due to the diverse data formats and task variations. Meanwhile, this
divergence leads to information waste and increases difficulties in building
complex applications in real scenarios. Recent studies often formulate IE tasks
as a triplet extraction problem. However, such a paradigm does not support
multi-span and n-ary extraction, leading to weak versatility. To this end, we
reorganize IE problems into unified multi-slot tuples and propose a universal
framework for various IE tasks, namely Mirror. Specifically, we recast existing
IE tasks as a multi-span cyclic graph extraction problem and devise a
non-autoregressive graph decoding algorithm to extract all spans in a single
step. It is worth noting that this graph structure is incredibly versatile, and
it supports not only complex IE tasks, but also machine reading comprehension
and classification tasks. We manually construct a corpus containing 57 datasets
for model pretraining, and conduct experiments on 30 datasets across 8
downstream tasks. The experimental results demonstrate that our model has
decent compatibility and outperforms or reaches competitive performance with
SOTA systems under few-shot and zero-shot settings. The code, model weights,
and pretraining corpus are available at https://github.com/Spico197/Mirror .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1&quot;&gt;Tong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Junfei Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zijian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Mengsong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guoliang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xiaoye Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenliang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhefeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huai_B/0/1/0/all/0/1&quot;&gt;Baoxing Huai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05437">
<title>LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents. (arXiv:2311.05437v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05437</link>
<description rdf:parseType="Literal">&lt;p&gt;LLaVA-Plus is a general-purpose multimodal assistant that expands the
capabilities of large multimodal models. It maintains a skill repository of
pre-trained vision and vision-language models and can activate relevant tools
based on users&apos; inputs to fulfill real-world tasks. LLaVA-Plus is trained on
multimodal instruction-following data to acquire the ability to use tools,
covering visual understanding, generation, external knowledge retrieval, and
compositions. Empirical results show that LLaVA-Plus outperforms LLaVA in
existing capabilities and exhibits new ones. It is distinct in that the image
query is directly grounded and actively engaged throughout the entire human-AI
interaction sessions, significantly improving tool use performance and enabling
new scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shilong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haotian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1&quot;&gt;Tianhe Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xueyan Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyuan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05450">
<title>Cognitively Inspired Components for Social Conversational Agents. (arXiv:2311.05450v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.05450</link>
<description rdf:parseType="Literal">&lt;p&gt;Current conversational agents (CA) have seen improvement in conversational
quality in recent years due to the influence of large language models (LLMs)
like GPT3. However, two key categories of problem remain. Firstly there are the
unique technical problems resulting from the approach taken in creating the CA,
such as scope with retrieval agents and the often nonsensical answers of former
generative agents. Secondly, humans perceive CAs as social actors, and as a
result expect the CA to adhere to social convention. Failure on the part of the
CA in this respect can lead to a poor interaction and even the perception of
threat by the user. As such, this paper presents a survey highlighting a
potential solution to both categories of problem through the introduction of
cognitively inspired additions to the CA. Through computational facsimiles of
semantic and episodic memory, emotion, working memory, and the ability to
learn, it is possible to address both the technical and social problems
encountered by CAs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clay_A/0/1/0/all/0/1&quot;&gt;Alex Clay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_E/0/1/0/all/0/1&quot;&gt;Eduardo Alonso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondragon_E/0/1/0/all/0/1&quot;&gt;Esther Mondrag&amp;#xf3;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05472">
<title>Text Representation Distillation via Information Bottleneck Principle. (arXiv:2311.05472v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.05472</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained language models (PLMs) have recently shown great success in text
representation field. However, the high computational cost and high-dimensional
representation of PLMs pose significant challenges for practical applications.
To make models more accessible, an effective method is to distill large models
into smaller representation models. In order to relieve the issue of
performance degradation after distillation, we propose a novel Knowledge
Distillation method called IBKD. This approach is motivated by the Information
Bottleneck principle and aims to maximize the mutual information between the
final representation of the teacher and student model, while simultaneously
reducing the mutual information between the student model&apos;s representation and
the input data. This enables the student model to preserve important learned
information while avoiding unnecessary information, thus reducing the risk of
over-fitting. Empirical studies on two main downstream applications of text
representation (Semantic Textual Similarity and Dense Retrieval tasks)
demonstrate the effectiveness of our proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanzhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_D/0/1/0/all/0/1&quot;&gt;Dingkun Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zehan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1&quot;&gt;Pengjun Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05481">
<title>meta4: semantically-aligned generation of metaphoric gestures using self-supervised text and speech representation. (arXiv:2311.05481v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.05481</link>
<description rdf:parseType="Literal">&lt;p&gt;Image Schemas are repetitive cognitive patterns that influence the way we
conceptualize and reason about various concepts present in speech. These
patterns are deeply embedded within our cognitive processes and are reflected
in our bodily expressions including gestures. Particularly, metaphoric gestures
possess essential characteristics and semantic meanings that align with Image
Schemas, to visually represent abstract concepts. The shape and form of
gestures can convey abstract concepts, such as extending the forearm and hand
or tracing a line with hand movements to visually represent the image schema of
PATH. Previous behavior generation models have primarily focused on utilizing
speech (acoustic features and text) to drive the generation model of virtual
agents. They have not considered key semantic information as those carried by
Image Schemas to effectively generate metaphoric gestures. To address this
limitation, we introduce META4, a deep learning approach that generates
metaphoric gestures from both speech and Image Schemas. Our approach has two
primary goals: computing Image Schemas from input text to capture the
underlying semantic and metaphorical meaning, and generating metaphoric
gestures driven by speech and the computed image schemas. Our approach is the
first method for generating speech driven metaphoric gestures while leveraging
the potential of Image Schemas. We demonstrate the effectiveness of our
approach and highlight the importance of both speech and image schemas in
modeling metaphoric gestures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fares_M/0/1/0/all/0/1&quot;&gt;Mireille Fares&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelachaud_C/0/1/0/all/0/1&quot;&gt;Catherine Pelachaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obin_N/0/1/0/all/0/1&quot;&gt;Nicolas Obin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05490">
<title>General Policies, Subgoal Structure, and Planning Width. (arXiv:2311.05490v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.05490</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been observed that many classical planning domains with atomic goals
can be solved by means of a simple polynomial exploration procedure, called IW,
that runs in time exponential in the problem width, which in these cases is
bounded and small. Yet, while the notion of width has become part of
state-of-the-art planning algorithms such as BFWS, there is no good explanation
for why so many benchmark domains have bounded width when atomic goals are
considered. In this work, we address this question by relating bounded width
with the existence of general optimal policies that in each planning instance
are represented by tuples of atoms of bounded size. We also define the notions
of (explicit) serializations and serialized width that have a broader scope as
many domains have a bounded serialized width but no bounded width. Such
problems are solved non-optimally in polynomial time by a suitable variant of
the Serialized IW algorithm. Finally, the language of general policies and the
semantics of serializations are combined to yield a simple, meaningful, and
expressive language for specifying serializations in compact form in the form
of sketches, which can be used for encoding domain control knowledge by hand or
for learning it from small examples. Sketches express general problem
decompositions in terms of subgoals, and sketches of bounded width express
problem decompositions that can be solved in polynomial time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonet_B/0/1/0/all/0/1&quot;&gt;Blai Bonet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geffner_H/0/1/0/all/0/1&quot;&gt;Hector Geffner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05511">
<title>Anytime-Constrained Reinforcement Learning. (arXiv:2311.05511v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.05511</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce and study constrained Markov Decision Processes (cMDPs) with
anytime constraints. An anytime constraint requires the agent to never violate
its budget at any point in time, almost surely. Although Markovian policies are
no longer sufficient, we show that there exist optimal deterministic policies
augmented with cumulative costs. In fact, we present a fixed-parameter
tractable reduction from anytime-constrained cMDPs to unconstrained MDPs. Our
reduction yields planning and learning algorithms that are time and
sample-efficient for tabular cMDPs so long as the precision of the costs is
logarithmic in the size of the cMDP. However, we also show that computing
non-trivial approximately optimal policies is NP-hard in general. To circumvent
this bottleneck, we design provable approximation algorithms that efficiently
compute or learn an approximately feasible policy with optimal value so long as
the maximum supported cost is bounded by a polynomial in the cMDP or by the
absolute budget. Given our hardness results, our approximation guarantees are
the best possible in terms of tractability under worst-case analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McMahan_J/0/1/0/all/0/1&quot;&gt;Jeremy McMahan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaojin Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05513">
<title>From Learning Management System to Affective Tutoring system: a preliminary study. (arXiv:2311.05513v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.05513</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we investigate the combination of indicators, including
performance, behavioral engagement, and emotional engagement, to identify
students experiencing difficulties. We analyzed data from two primary sources:
digital traces extracted from th e Learning Management System (LMS) and images
captured by students&apos; webcams. The digital traces provided insights into
students&apos; interactions with the educational content, while the images were
utilized to analyze their emotional expressions during learnin g activities. By
utilizing real data collected from students at a French engineering school,
recorded during the 2022 2023 academic year, we observed a correlation between
positive emotional states and improved academic outcomes. These preliminary
findings support the notion that emotions play a crucial role in
differentiating between high achieving and low achieving students.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edouard_N/0/1/0/all/0/1&quot;&gt;Nadaud Edouard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thibault_G/0/1/0/all/0/1&quot;&gt;Geoffroy Thibault&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tesnim_K/0/1/0/all/0/1&quot;&gt;Khelifi Tesnim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antoun_Y/0/1/0/all/0/1&quot;&gt;Yaacoub Antoun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siba_H/0/1/0/all/0/1&quot;&gt;Haidar Siba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+NourhEne_B/0/1/0/all/0/1&quot;&gt;Ben Rabah Nourh&amp;#xc8;ne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pierre_A/0/1/0/all/0/1&quot;&gt;Aubin Jean Pierre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lionel_P/0/1/0/all/0/1&quot;&gt;Prevost Lionel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benedicte_L/0/1/0/all/0/1&quot;&gt;Le Grand Benedicte&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05546">
<title>Multi-Agent Quantum Reinforcement Learning using Evolutionary Optimization. (arXiv:2311.05546v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2311.05546</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Agent Reinforcement Learning is becoming increasingly more important in
times of autonomous driving and other smart industrial applications.
Simultaneously a promising new approach to Reinforcement Learning arises using
the inherent properties of quantum mechanics, reducing the trainable parameters
of a model significantly. However, gradient-based Multi-Agent Quantum
Reinforcement Learning methods often have to struggle with barren plateaus,
holding them back from matching the performance of classical approaches. We
build upon a existing approach for gradient free Quantum Reinforcement Learning
and propose tree approaches with Variational Quantum Circuits for Multi-Agent
Reinforcement Learning using evolutionary optimization. We evaluate our
approach in the Coin Game environment and compare them to classical approaches.
We showed that our Variational Quantum Circuit approaches perform significantly
better compared to a neural network with a similar amount of trainable
parameters. Compared to the larger neural network, our approaches archive
similar results using $97.88\%$ less parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kolle_M/0/1/0/all/0/1&quot;&gt;Michael K&amp;#xf6;lle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Topp_F/0/1/0/all/0/1&quot;&gt;Felix Topp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Phan_T/0/1/0/all/0/1&quot;&gt;Thomy Phan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Altmann_P/0/1/0/all/0/1&quot;&gt;Philipp Altmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Nusslein_J/0/1/0/all/0/1&quot;&gt;Jonas N&amp;#xfc;&amp;#xdf;lein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Linnhoff_Popien_C/0/1/0/all/0/1&quot;&gt;Claudia Linnhoff-Popien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05553">
<title>Removing RLHF Protections in GPT-4 via Fine-Tuning. (arXiv:2311.05553v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.05553</link>
<description rdf:parseType="Literal">&lt;p&gt;As large language models (LLMs) have increased in their capabilities, so does
their potential for dual use. To reduce harmful outputs, produces and vendors
of LLMs have used reinforcement learning with human feedback (RLHF). In tandem,
LLM vendors have been increasingly enabling fine-tuning of their most powerful
models. However, concurrent work has shown that fine-tuning can remove RLHF
protections. We may expect that the most powerful models currently available
(GPT-4) are less susceptible to fine-tuning attacks.
&lt;/p&gt;
&lt;p&gt;In this work, we show the contrary: fine-tuning allows attackers to remove
RLHF protections with as few as 340 examples and a 95% success rate. These
training examples can be automatically generated with weaker models. We further
show that removing RLHF protections does not decrease usefulness on
non-censored outputs, providing evidence that our fine-tuning strategy does not
decrease usefulness despite using weaker models to generate training data. Our
results show the need for further research on protections on LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_Q/0/1/0/all/0/1&quot;&gt;Qiusi Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1&quot;&gt;Richard Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bindu_R/0/1/0/all/0/1&quot;&gt;Rohan Bindu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Akul Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1&quot;&gt;Tatsunori Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1&quot;&gt;Daniel Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05580">
<title>Inference for Probabilistic Dependency Graphs. (arXiv:2311.05580v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2311.05580</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic dependency graphs (PDGs) are a flexible class of probabilistic
graphical models, subsuming Bayesian Networks and Factor Graphs. They can also
capture inconsistent beliefs, and provide a way of measuring the degree of this
inconsistency. We present the first tractable inference algorithm for PDGs with
discrete variables, making the asymptotic complexity of PDG inference similar
that of the graphical models they generalize. The key components are: (1) the
observation that, in many cases, the distribution a PDG specifies can be
formulated as a convex optimization problem (with exponential cone
constraints), (2) a construction that allows us to express these problems
compactly for PDGs of boundeed treewidth, (3) contributions to the theory of
PDGs that justify the construction, and (4) an appeal to interior point methods
that can solve such problems in polynomial time. We verify the correctness and
complexity of our approach, and provide an implementation of it. We then
evaluate our implementation, and demonstrate that it outperforms baseline
approaches. Our code is available at
&lt;a href=&quot;http://github.com/orichardson/pdg-infer-uai.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richardson_O/0/1/0/all/0/1&quot;&gt;Oliver E. Richardson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halpern_J/0/1/0/all/0/1&quot;&gt;Joseph Y. Halpern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1&quot;&gt;Christopher De Sa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05584">
<title>Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations. (arXiv:2311.05584v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.05584</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have emerged as powerful and general solutions
to many natural language tasks. However, many of the most important
applications of language generation are interactive, where an agent has to talk
to a person to reach a desired outcome. For example, a teacher might try to
understand their student&apos;s current comprehension level to tailor their
instruction accordingly, and a travel agent might ask questions of their
customer to understand their preferences in order to recommend activities they
might enjoy. LLMs trained with supervised fine-tuning or &quot;single-step&quot; RL, as
with standard RLHF, might struggle which tasks that require such goal-directed
behavior, since they are not trained to optimize for overall conversational
outcomes after multiple turns of interaction. In this work, we explore a new
method for adapting LLMs with RL for such goal-directed dialogue. Our key
insight is that, though LLMs might not effectively solve goal-directed dialogue
tasks out of the box, they can provide useful data for solving such tasks by
simulating suboptimal but human-like behaviors. Given a textual description of
a goal-directed dialogue task, we leverage LLMs to sample diverse synthetic
rollouts of hypothetical in-domain human-human interactions. Our algorithm then
utilizes this dataset with offline reinforcement learning to train an
interactive conversational agent that can optimize goal-directed objectives
over multiple turns. In effect, the LLM produces examples of possible
interactions, and RL then processes these examples to learn to perform more
optimal interactions. Empirically, we show that our proposed approach achieves
state-of-the-art performance in various goal-directed dialogue tasks that
include teaching and preference elicitation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Joey Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1&quot;&gt;Anca Dragan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05590">
<title>Conversational AI Threads for Visualizing Multidimensional Datasets. (arXiv:2311.05590v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2311.05590</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Large Language Models (LLMs) show potential in data analysis, yet
their full capabilities remain uncharted. Our work explores the capabilities of
LLMs for creating and refining visualizations via conversational interfaces. We
used an LLM to conduct a re-analysis of a prior Wizard-of-Oz study examining
the use of chatbots for conducting visual analysis. We surfaced the strengths
and weaknesses of LLM-driven analytic chatbots, finding that they fell short in
supporting progressive visualization refinements. From these findings, we
developed AI Threads, a multi-threaded analytic chatbot that enables analysts
to proactively manage conversational context and improve the efficacy of its
outputs. We evaluate its usability through a crowdsourced study (n=40) and
in-depth interviews with expert analysts (n=10). We further demonstrate the
capabilities of AI Threads on a dataset outside the LLM&apos;s training corpus. Our
findings show the potential of LLMs while also surfacing challenges and
fruitful avenues for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1&quot;&gt;Matt-Heun Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crisan_A/0/1/0/all/0/1&quot;&gt;Anamaria Crisan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05591">
<title>Accuracy of a Vision-Language Model on Challenging Medical Cases. (arXiv:2311.05591v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05591</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: General-purpose large language models that utilize both text and
images have not been evaluated on a diverse array of challenging medical cases.
&lt;/p&gt;
&lt;p&gt;Methods: Using 934 cases from the NEJM Image Challenge published between 2005
and 2023, we evaluated the accuracy of the recently released Generative
Pre-trained Transformer 4 with Vision model (GPT-4V) compared to human
respondents overall and stratified by question difficulty, image type, and skin
tone. We further conducted a physician evaluation of GPT-4V on 69 NEJM
clinicopathological conferences (CPCs). Analyses were conducted for models
utilizing text alone, images alone, and both text and images.
&lt;/p&gt;
&lt;p&gt;Results: GPT-4V achieved an overall accuracy of 61% (95% CI, 58 to 64%)
compared to 49% (95% CI, 49 to 50%) for humans. GPT-4V outperformed humans at
all levels of difficulty and disagreement, skin tones, and image types; the
exception was radiographic images, where performance was equivalent between
GPT-4V and human respondents. Longer, more informative captions were associated
with improved performance for GPT-4V but similar performance for human
respondents. GPT-4V included the correct diagnosis in its differential for 80%
(95% CI, 68 to 88%) of CPCs when using text alone, compared to 58% (95% CI, 45
to 70%) of CPCs when using both images and text.
&lt;/p&gt;
&lt;p&gt;Conclusions: GPT-4V outperformed human respondents on challenging medical
cases and was able to synthesize information from both images and text, but
performance deteriorated when images were added to highly informative text.
Overall, our results suggest that multimodal AI models may be useful in medical
diagnostic reasoning but that their accuracy may depend heavily on context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buckley_T/0/1/0/all/0/1&quot;&gt;Thomas Buckley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diao_J/0/1/0/all/0/1&quot;&gt;James A. Diao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodman_A/0/1/0/all/0/1&quot;&gt;Adam Rodman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manrai_A/0/1/0/all/0/1&quot;&gt;Arjun K. Manrai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05596">
<title>LLM Augmented Hierarchical Agents. (arXiv:2311.05596v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.05596</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving long-horizon, temporally-extended tasks using Reinforcement Learning
(RL) is challenging, compounded by the common practice of learning without
prior knowledge (or tabula rasa learning). Humans can generate and execute
plans with temporally-extended actions and quickly learn to perform new tasks
because we almost never solve problems from scratch. We want autonomous agents
to have this same ability. Recently, LLMs have been shown to encode a
tremendous amount of knowledge about the world and to perform impressive
in-context learning and reasoning. However, using LLMs to solve real world
problems is hard because they are not grounded in the current task. In this
paper we exploit the planning capabilities of LLMs while using RL to provide
learning from the environment, resulting in a hierarchical agent that uses LLMs
to solve long-horizon tasks. Instead of completely relying on LLMs, they guide
a high-level policy, making learning significantly more sample efficient. This
approach is evaluated in simulation environments such as MiniGrid, SkillHack,
and Crafter, and on a real robot arm in block manipulation tasks. We show that
agents trained using our approach outperform other baselines methods and, once
trained, don&apos;t need access to LLMs during deployment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakash_B/0/1/0/all/0/1&quot;&gt;Bharat Prakash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oates_T/0/1/0/all/0/1&quot;&gt;Tim Oates&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohsenin_T/0/1/0/all/0/1&quot;&gt;Tinoosh Mohsenin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05599">
<title>SynH2R: Synthesizing Hand-Object Motions for Learning Human-to-Robot Handovers. (arXiv:2311.05599v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.05599</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-based human-to-robot handover is an important and challenging task in
human-robot interaction. Recent work has attempted to train robot policies by
interacting with dynamic virtual humans in simulated environments, where the
policies can later be transferred to the real world. However, a major
bottleneck is the reliance on human motion capture data, which is expensive to
acquire and difficult to scale to arbitrary objects and human grasping motions.
In this paper, we introduce a framework that can generate plausible human
grasping motions suitable for training the robot. To achieve this, we propose a
hand-object synthesis method that is designed to generate handover-friendly
motions similar to humans. This allows us to generate synthetic training and
testing data with 100x more objects than previous work. In our experiments, we
show that our method trained purely with synthetic data is competitive with
state-of-the-art methods that rely on real human motion data both in simulation
and on a real system. In addition, we can perform evaluations on a larger scale
compared to prior work. With our newly introduced test set, we show that our
model can better scale to a large variety of unseen objects and human motions
compared to the baselines. Project page:
https://eth-ait.github.io/synthetic-handovers/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christen_S/0/1/0/all/0/1&quot;&gt;Sammy Christen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1&quot;&gt;Lan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_Y/0/1/0/all/0/1&quot;&gt;Yu-Wei Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1&quot;&gt;Otmar Hilliges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jie Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05607">
<title>Real-Time Neural Rasterization for Large Scenes. (arXiv:2311.05607v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05607</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new method for realistic real-time novel-view synthesis (NVS) of
large scenes. Existing neural rendering methods generate realistic results, but
primarily work for small scale scenes (&amp;lt;50 square meters) and have difficulty
at large scale (&amp;gt;10000 square meters). Traditional graphics-based rasterization
rendering is fast for large scenes but lacks realism and requires expensive
manually created assets. Our approach combines the best of both worlds by
taking a moderate-quality scaffold mesh as input and learning a neural texture
field and shader to model view-dependant effects to enhance realism, while
still using the standard graphics pipeline for real-time rendering. Our method
outperforms existing neural rendering methods, providing at least 30x faster
rendering with comparable or better realism for large self-driving and drone
scenes. Our work is the first to enable real-time rendering of large real-world
scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jeffrey Yunfan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ze Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingkang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manivasagam_S/0/1/0/all/0/1&quot;&gt;Sivabalan Manivasagam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05608">
<title>FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts. (arXiv:2311.05608v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2311.05608</link>
<description rdf:parseType="Literal">&lt;p&gt;Large vision-language models (VLMs) like GPT-4V represent an unprecedented
revolution in the field of artificial intelligence (AI). Compared to
single-modal large language models (LLMs), VLMs possess more versatile
capabilities by incorporating additional modalities (e.g., images). Meanwhile,
there&apos;s a rising enthusiasm in the AI community to develop open-source VLMs,
such as LLaVA and MiniGPT4, which, however, have not undergone rigorous safety
assessment. In this paper, to demonstrate that more modalities lead to
unforeseen AI safety issues, we propose FigStep, a novel jailbreaking framework
against VLMs. FigStep feeds harmful instructions into VLMs through the image
channel and then uses benign text prompts to induce VLMs to output contents
that violate common AI safety policies. Our experimental results show that
FigStep can achieve an average attack success rate of 94.8% across 2 families
of popular open-source VLMs, LLaVA and MiniGPT4 (a total of 5 VLMs). Moreover,
we demonstrate that the methodology of FigStep can even jailbreak GPT-4V, which
already leverages several system-level mechanisms to filter harmful queries.
Above all, our experimental results reveal that VLMs are vulnerable to
jailbreaking attacks, which highlights the necessity of novel safety alignments
between visual and textual modalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yichen Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ran_D/0/1/0/all/0/1&quot;&gt;Delong Ran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Conglei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_T/0/1/0/all/0/1&quot;&gt;Tianshuo Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Anyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1&quot;&gt;Sisi Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoyun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2010.13380">
<title>The training accuracy of two-layer neural networks: its estimation and understanding using random datasets. (arXiv:2010.13380v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2010.13380</link>
<description rdf:parseType="Literal">&lt;p&gt;Although the neural network (NN) technique plays an important role in machine
learning, understanding the mechanism of NN models and the transparency of deep
learning still require more basic research. In this study, we propose a novel
theory based on space partitioning to estimate the approximate training
accuracy for two-layer neural networks on random datasets without training.
There appear to be no other studies that have proposed a method to estimate
training accuracy without using input data and/or trained models. Our method
estimates the training accuracy for two-layer fully-connected neural networks
on two-class random datasets using only three arguments: the dimensionality of
inputs (d), the number of inputs (N), and the number of neurons in the hidden
layer (L). We have verified our method using real training accuracies in our
experiments. The results indicate that the method will work for any dimension,
and the proposed theory could extend also to estimate deeper NN models. The
main purpose of this paper is to understand the mechanism of NN models by the
approach of estimating training accuracy but not to analyze their
generalization nor their performance in real-world applications. This study may
provide a starting point for a new way for researchers to make progress on the
difficult problem of understanding deep learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1&quot;&gt;Shuyue Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loew_M/0/1/0/all/0/1&quot;&gt;Murray Loew&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2012.04132">
<title>A Number Sense as an Emergent Property of the Manipulating Brain. (arXiv:2012.04132v3 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2012.04132</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) systems struggle to generalize beyond their
training data and abstract general properties from the specifics of the
training examples. We propose a model that reproduces the apparent human
ability to come up with a number sense through unsupervised everyday
experience. The ability to understand and manipulate numbers and quantities
emerges during childhood, but the mechanism through which humans acquire and
develop this ability is still poorly understood. In particular, it is not known
whether acquiring such a number sense is possible without supervision from a
teacher. We explore this question through a model, assuming that the learner is
able to pick and place small objects and will spontaneously engage in
undirected manipulation. We assume that the learner&apos;s visual system will
monitor the changing arrangements of objects in the scene and will learn to
predict the effects of each action by comparing perception with the efferent
signal of the motor system. We model perception using standard deep networks
for feature extraction and classification. We find that, from learning the
unrelated task of action prediction, an unexpected image representation emerges
exhibiting regularities that foreshadow the perception and representation of
numbers. These include distinct categories for the first few natural numbers, a
strict ordering of the numbers, and a one-dimensional signal that correlates
with numerical quantity. As a result, our model acquires the ability to
estimate numerosity and subitize. Remarkably, subitization and numerosity
estimation extrapolate to scenes containing many objects, far beyond the three
objects used during training. We conclude that important aspects of a facility
with numbers and quantities may be learned without teacher supervision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kondapaneni_N/0/1/0/all/0/1&quot;&gt;Neehar Kondapaneni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Perona_P/0/1/0/all/0/1&quot;&gt;Pietro Perona&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.01736">
<title>AdjointBackMapV2: Precise Reconstruction of Arbitrary CNN Unit&apos;s Activation via Adjoint Operators. (arXiv:2110.01736v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2110.01736</link>
<description rdf:parseType="Literal">&lt;p&gt;Adjoint operators have been found to be effective in the exploration of CNN&apos;s
inner workings [1]. However, the previous no-bias assumption restricted its
generalization. We overcome the restriction via embedding input images into an
extended normed space that includes bias in all CNN layers as part of the
extended space and propose an adjoint-operator-based algorithm that maps
high-level weights back to the extended input space for reconstructing an
effective hypersurface. Such hypersurface can be computed for an arbitrary unit
in the CNN, and we prove that this reconstructed hypersurface, when multiplied
by the original input (through an inner product), will precisely replicate the
output value of each unit. We show experimental results based on the CIFAR-10
and CIFAR-100 data sets where the proposed approach achieves near 0 activation
value reconstruction error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Q/0/1/0/all/0/1&quot;&gt;Qing Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_S/0/1/0/all/0/1&quot;&gt;Siu Wun Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choe_Y/0/1/0/all/0/1&quot;&gt;Yoonsuck Choe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.16463">
<title>Perfectly Accurate Membership Inference by a Dishonest Central Server in Federated Learning. (arXiv:2203.16463v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2203.16463</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning is expected to provide strong privacy guarantees, as only
gradients or model parameters but no plain text training data is ever exchanged
either between the clients or between the clients and the central server. In
this paper, we challenge this claim by introducing a simple but still very
effective membership inference attack algorithm, which relies only on a single
training step. In contrast to the popular honest-but-curious model, we
investigate a framework with a dishonest central server. Our strategy is
applicable to models with ReLU activations and uses the properties of this
activation function to achieve perfect accuracy. Empirical evaluation on visual
classification tasks with MNIST, CIFAR10, CIFAR100 and CelebA datasets show
that our method provides perfect accuracy in identifying one sample in a
training set with thousands of samples. Occasional failures of our method lead
us to discover duplicate images in the CIFAR100 and CelebA datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pichler_G/0/1/0/all/0/1&quot;&gt;Georg Pichler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romanelli_M/0/1/0/all/0/1&quot;&gt;Marco Romanelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vega_L/0/1/0/all/0/1&quot;&gt;Leonardo Rey Vega&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1&quot;&gt;Pablo Piantanida&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.07028">
<title>Exploring the Distributed Knowledge Congruence in Proxy-data-free Federated Distillation. (arXiv:2204.07028v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2204.07028</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) is a privacy-preserving machine learning paradigm in
which the server periodically aggregates local model parameters from clients
without assembling their private data.
&lt;/p&gt;
&lt;p&gt;Constrained communication and personalization requirements pose severe
challenges to FL. Federated distillation (FD) is proposed to simultaneously
address the above two problems, which exchanges knowledge between the server
and clients, supporting heterogeneous local models while significantly reducing
communication overhead. However, most existing FD methods require a proxy
dataset, which is often unavailable in reality.
&lt;/p&gt;
&lt;p&gt;A few recent proxy-data-free FD approaches can eliminate the need for
additional public data, but suffer from remarkable discrepancy among local
knowledge due to client-side model heterogeneity, leading to ambiguous
representation on the server and inevitable accuracy degradation.
&lt;/p&gt;
&lt;p&gt;To tackle this issue, we propose a proxy-data-free FD algorithm based on
distributed knowledge congruence (FedDKC). FedDKC leverages well-designed
refinement strategies to narrow local knowledge differences into an acceptable
upper bound, so as to mitigate the negative effects of knowledge incongruence.
&lt;/p&gt;
&lt;p&gt;Specifically, from perspectives of peak probability and Shannon entropy of
local knowledge, we design kernel-based knowledge refinement (KKR) and
searching-based knowledge refinement (SKR) respectively, and theoretically
guarantee that the refined-local knowledge can satisfy an approximately-similar
distribution and be regarded as congruent.
&lt;/p&gt;
&lt;p&gt;Extensive experiments conducted on three common datasets demonstrate that our
proposed FedDKC significantly outperforms the state-of-the-art on various
heterogeneous settings while evidently improving the convergence speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Sheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Min Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Q/0/1/0/all/0/1&quot;&gt;Quyang Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junbo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zeju Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qingxiang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.04798">
<title>A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs. (arXiv:2206.04798v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2206.04798</link>
<description rdf:parseType="Literal">&lt;p&gt;Reasoning on large-scale knowledge graphs has been long dominated by
embedding methods. While path-based methods possess the inductive capacity that
embeddings lack, their scalability is limited by the exponential number of
paths. Here we present A*Net, a scalable path-based method for knowledge graph
reasoning. Inspired by the A* algorithm for shortest path problems, our A*Net
learns a priority function to select important nodes and edges at each
iteration, to reduce time and memory footprint for both training and inference.
The ratio of selected nodes and edges can be specified to trade off between
performance and efficiency. Experiments on both transductive and inductive
knowledge graph reasoning benchmarks show that A*Net achieves competitive
performance with existing state-of-the-art path-based methods, while merely
visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset
ogbl-wikikg2, A*Net not only achieves a new state-of-the-art result, but also
converges faster than embedding methods. A*Net is the first path-based method
for knowledge graph reasoning at such scale.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhaocheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xinyu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1&quot;&gt;Mikhail Galkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xhonneux_S/0/1/0/all/0/1&quot;&gt;Sophie Xhonneux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gazeau_M/0/1/0/all/0/1&quot;&gt;Maxime Gazeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.00445">
<title>Interpreting Embedding Spaces by Conceptualization. (arXiv:2209.00445v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2209.00445</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the main methods for computational interpretation of a text is mapping
it into a vector in some embedding space. Such vectors can then be used for a
variety of textual processing tasks. Recently, most embedding spaces are a
product of training large language models (LLMs). One major drawback of this
type of representation is their incomprehensibility to humans. Understanding
the embedding space is crucial for several important needs, including the need
to debug the embedding method and compare it to alternatives, and the need to
detect biases hidden in the model. In this paper, we present a novel method of
understanding embeddings by transforming a latent embedding space into a
comprehensible conceptual space. We present an algorithm for deriving a
conceptual space with dynamic on-demand granularity. We devise a new evaluation
method, using either human rater or LLM-based raters, to show that the
conceptualized vectors indeed represent the semantics of the original latent
ones. We show the use of our method for various tasks, including comparing the
semantics of alternative models and tracing the layers of the LLM. The code is
available online
https://github.com/adiSimhi/Interpreting-Embedding-Spaces-by-Conceptualization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simhi_A/0/1/0/all/0/1&quot;&gt;Adi Simhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markovitch_S/0/1/0/all/0/1&quot;&gt;Shaul Markovitch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.08067">
<title>Interpreting CNN Predictions using Conditional Generative Adversarial Networks. (arXiv:2301.08067v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.08067</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel method that trains a conditional Generative Adversarial
Network (GAN) to generate visual interpretations of a Convolutional Neural
Network (CNN). To comprehend a CNN, the GAN is trained with information on how
the CNN processes an image when making predictions. Supplying that information
has two main challenges: how to represent this information in a form that is
feedable to the GANs and how to effectively feed the representation to the GAN.
To address these issues, we developed a suitable representation of CNN
architectures by cumulatively averaging intermediate interpretation maps. We
also propose two alternative approaches to feed the representations to the GAN
and to choose an effective training strategy. Our approach learned the general
aspects of CNNs and was agnostic to datasets and CNN architectures. The study
includes both qualitative and quantitative evaluations and compares the
proposed GANs with state-of-the-art approaches. We found that the initial
layers of CNNs and final layers are equally crucial for interpreting CNNs upon
interpreting the proposed GAN. We believe training a GAN to interpret CNNs
would open doors for improved interpretations by leveraging fast-paced deep
learning advancements. The code used for experimentation is publicly available
at https://github.com/Akash-guna/Explain-CNN-With-GANS
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guna_R/0/1/0/all/0/1&quot;&gt;R T Akash Guna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benitez_R/0/1/0/all/0/1&quot;&gt;Raul Benitez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sikha_O/0/1/0/all/0/1&quot;&gt;O K Sikha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.09633">
<title>Prediction-Powered Inference. (arXiv:2301.09633v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2301.09633</link>
<description rdf:parseType="Literal">&lt;p&gt;Prediction-powered inference is a framework for performing valid statistical
inference when an experimental dataset is supplemented with predictions from a
machine-learning system. The framework yields simple algorithms for computing
provably valid confidence intervals for quantities such as means, quantiles,
and linear and logistic regression coefficients, without making any assumptions
on the machine-learning algorithm that supplies the predictions. Furthermore,
more accurate predictions translate to smaller confidence intervals.
Prediction-powered inference could enable researchers to draw valid and more
data-efficient conclusions using machine learning. The benefits of
prediction-powered inference are demonstrated with datasets from proteomics,
astronomy, genomics, remote sensing, census analysis, and ecology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Angelopoulos_A/0/1/0/all/0/1&quot;&gt;Anastasios N. Angelopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bates_S/0/1/0/all/0/1&quot;&gt;Stephen Bates&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fannjiang_C/0/1/0/all/0/1&quot;&gt;Clara Fannjiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zrnic_T/0/1/0/all/0/1&quot;&gt;Tijana Zrnic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10171">
<title>Spectral Cross-Domain Neural Network with Soft-adaptive Threshold Spectral Enhancement. (arXiv:2301.10171v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10171</link>
<description rdf:parseType="Literal">&lt;p&gt;Electrocardiography (ECG) signals can be considered as multi-variable
time-series. The state-of-the-art ECG data classification approaches, based on
either feature engineering or deep learning techniques, treat separately
spectral and time domains in machine learning systems. No spectral-time domain
communication mechanism inside the classifier model can be found in current
approaches, leading to difficulties in identifying complex ECG forms. In this
paper, we proposed a novel deep learning model named Spectral Cross-domain
neural network (SCDNN) with a new block called Soft-adaptive threshold spectral
enhancement (SATSE), to simultaneously reveal the key information embedded in
spectral and time domains inside the neural network. More precisely, the
domain-cross information is captured by a general Convolutional neural network
(CNN) backbone, and different information sources are merged by a self-adaptive
mechanism to mine the connection between time and spectral domains. In SATSE,
the knowledge from time and spectral domains is extracted via the Fast Fourier
Transformation (FFT) with soft trainable thresholds in modified Sigmoid
functions. The proposed SCDNN is tested with several classification tasks
implemented on the public ECG databases \textit{PTB-XL} and \textit{MIT-BIH}.
SCDNN outperforms the state-of-the-art approaches with a low computational cost
regarding a variety of metrics in all classification tasks on both databases,
by finding appropriate domains from the infinite spectral mapping. The
convergence of the trainable thresholds in the spectral domain is also
numerically investigated in this paper. The robust performance of SCDNN
provides a new perspective to exploit knowledge across deep learning models
from time and spectral domains. The repository can be found:
https://github.com/DL-WG/SCDNN-TS
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Che Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Sibo Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1&quot;&gt;Weiping Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arcucci_R/0/1/0/all/0/1&quot;&gt;Rossella Arcucci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01713">
<title>Towards Avoiding the Data Mess: Industry Insights from Data Mesh Implementations. (arXiv:2302.01713v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01713</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing importance of data and artificial intelligence,
organizations strive to become more data-driven. However, current data
architectures are not necessarily designed to keep up with the scale and scope
of data and analytics use cases. In fact, existing architectures often fail to
deliver the promised value associated with them. Data mesh is a
socio-technical, decentralized, distributed concept for enterprise data
management. As the concept of data mesh is still novel, it lacks empirical
insights from the field. Specifically, an understanding of the motivational
factors for introducing data mesh, the associated challenges, implementation
strategies, its business impact, and potential archetypes is missing. To
address this gap, we conduct 15 semi-structured interviews with industry
experts. Our results show, among other insights, that organizations have
difficulties with the transition toward federated governance associated with
the data mesh concept, the shift of responsibility for the development,
provision, and maintenance of data products, and the comprehension of the
overall concept. In our work, we derive multiple implementation strategies and
suggest organizations introduce a cross-domain steering unit, observe the data
product usage, create quick wins in the early phases, and favor small dedicated
teams that prioritize data products. While we acknowledge that organizations
need to apply implementation strategies according to their individual needs, we
also deduct two archetypes that provide suggestions in more detail. Our
findings synthesize insights from industry experts and provide researchers and
professionals with preliminary guidelines for the successful adoption of data
mesh.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bode_J/0/1/0/all/0/1&quot;&gt;Jan Bode&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhl_N/0/1/0/all/0/1&quot;&gt;Niklas K&amp;#xfc;hl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreuzberger_D/0/1/0/all/0/1&quot;&gt;Dominik Kreuzberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirschl_S/0/1/0/all/0/1&quot;&gt;Sebastian Hirschl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holtmann_C/0/1/0/all/0/1&quot;&gt;Carsten Holtmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.04023">
<title>A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. (arXiv:2302.04023v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2302.04023</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a framework for quantitatively evaluating interactive
LLMs such as ChatGPT using publicly available data sets. We carry out an
extensive technical evaluation of ChatGPT using 23 data sets covering 8
different common NLP application tasks. We evaluate the multitask, multilingual
and multi-modal aspects of ChatGPT based on these data sets and a newly
designed multimodal dataset. We find that ChatGPT outperforms LLMs with
zero-shot learning on most tasks and even outperforms fine-tuned models on some
tasks. We find that it is better at understanding non-Latin script languages
than generating them. It is able to generate multimodal content from textual
prompts, via an intermediate code generation step. Moreover, we find that
ChatGPT is 63.41% accurate on average in 10 different reasoning categories
under logical reasoning, non-textual reasoning, and commonsense reasoning,
hence making it an unreliable reasoner. It is, for example, better at deductive
than inductive reasoning. ChatGPT suffers from hallucination problems like
other LLMs and it generates more extrinsic hallucinations from its parametric
memory as it does not have access to an external knowledge base. Finally, the
interactive feature of ChatGPT enables human collaboration with the underlying
LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++
on machine translation, in a multi-turn &quot;prompt engineering&quot; fashion. We also
release codebase for evaluation set extraction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1&quot;&gt;Yejin Bang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1&quot;&gt;Samuel Cahyawijaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1&quot;&gt;Nayeon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Wenliang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1&quot;&gt;Dan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1&quot;&gt;Bryan Wilie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1&quot;&gt;Holy Lovenia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1&quot;&gt;Ziwei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tiezheng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_W/0/1/0/all/0/1&quot;&gt;Willy Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Do_Q/0/1/0/all/0/1&quot;&gt;Quyet V. Do&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1&quot;&gt;Pascale Fung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00986">
<title>Pay Less But Get More: A Dual-Attention-based Channel Estimation Network for Massive MIMO Systems with Low-Density Pilots. (arXiv:2303.00986v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00986</link>
<description rdf:parseType="Literal">&lt;p&gt;To reap the promising benefits of massive multiple-input multiple-output
(MIMO) systems, accurate channel state information (CSI) is required through
channel estimation. However, due to the complicated wireless propagation
environment and large-scale antenna arrays, precise channel estimation for
massive MIMO systems is significantly challenging and costs an enormous
training overhead. Considerable time-frequency resources are consumed to
acquire sufficient accuracy of CSI, which thus severely degrades systems&apos;
spectral and energy efficiencies. In this paper, we propose a
dual-attention-based channel estimation network (DACEN) to realize accurate
channel estimation via low-density pilots, by jointly learning the
spatial-temporal domain features of massive MIMO channels with the temporal
attention module and the spatial attention module. To further improve the
estimation accuracy, we propose a parameter-instance transfer learning approach
to transfer the channel knowledge learned from the high-density pilots
pre-acquired during the training dataset collection period. Experimental
results reveal that the proposed DACEN-based method achieves better channel
estimation performance than the existing methods under various pilot-density
settings and signal-to-noise ratios. Additionally, with the proposed
parameter-instance transfer learning approach, the DACEN-based method achieves
additional performance gain, thereby further demonstrating the effectiveness
and superiority of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Binggui Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shaodan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Feifei Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guanghua Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05828">
<title>Adapting Contrastive Language-Image Pretrained (CLIP) Models for Out-of-Distribution Detection. (arXiv:2303.05828v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05828</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a comprehensive experimental study on pretrained feature
extractors for visual out-of-distribution (OOD) detection, focusing on adapting
contrastive language-image pretrained (CLIP) models. Without fine-tuning on the
training data, we are able to establish a positive correlation ($R^2\geq0.92$)
between in-distribution classification and unsupervised OOD detection for CLIP
models in $4$ benchmarks. We further propose a new simple and scalable method
called \textit{pseudo-label probing} (PLP) that adapts vision-language models
for OOD detection. Given a set of label names of the training set, PLP trains a
linear layer using the pseudo-labels derived from the text encoder of CLIP. To
test the OOD detection robustness of pretrained models, we develop a novel
feature-based adversarial OOD data manipulation approach to create adversarial
samples. Intriguingly, we show that (i) PLP outperforms the previous
state-of-the-art \citep{ming2022mcm} on all $5$ large-scale benchmarks based on
ImageNet, specifically by an average AUROC gain of 3.4\% using the largest CLIP
model (ViT-G), (ii) we show that linear probing outperforms fine-tuning by
large margins for CLIP architectures (i.e. CLIP ViT-H achieves a mean gain of
7.3\% AUROC on average on all ImageNet-based benchmarks), and (iii)
billion-parameter CLIP models still fail at detecting adversarially manipulated
OOD images. The code and adversarially created datasets will be made publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adaloglou_N/0/1/0/all/0/1&quot;&gt;Nikolas Adaloglou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michels_F/0/1/0/all/0/1&quot;&gt;Felix Michels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaiser_T/0/1/0/all/0/1&quot;&gt;Tim Kaiser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kollmann_M/0/1/0/all/0/1&quot;&gt;Markus Kollmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.08789">
<title>PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining. (arXiv:2303.08789v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2303.08789</link>
<description rdf:parseType="Literal">&lt;p&gt;A rich representation is key to general robotic manipulation, but existing
approaches to representation learning require large amounts of multimodal
demonstrations. In this work we propose PLEX, a transformer-based architecture
that learns from a small amount of task-agnostic visuomotor trajectories and a
much larger amount of task-conditioned object manipulation videos -- a type of
data available in quantity. PLEX uses visuomotor trajectories to induce a
latent feature space and to learn task-agnostic manipulation routines, while
diverse video-only demonstrations teach PLEX how to plan in the induced latent
feature space for a wide variety of tasks. Experiments showcase PLEX&apos;s
generalization on Meta-World and SOTA performance in challenging Robosuite
environments. In particular, using relative positional encoding in PLEX&apos;s
transformers greatly helps in low-data regimes of learning from human-collected
demonstrations. The paper&apos;s accompanying code and data are available at
https://microsoft.github.io/PLEX.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomas_G/0/1/0/all/0/1&quot;&gt;Garrett Thomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Ching-An Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loynd_R/0/1/0/all/0/1&quot;&gt;Ricky Loynd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frujeri_F/0/1/0/all/0/1&quot;&gt;Felipe Vieira Frujeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1&quot;&gt;Vibhav Vineet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jalobeanu_M/0/1/0/all/0/1&quot;&gt;Mihai Jalobeanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolobov_A/0/1/0/all/0/1&quot;&gt;Andrey Kolobov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07880">
<title>Sabi\&apos;a: Portuguese Large Language Models. (arXiv:2304.07880v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07880</link>
<description rdf:parseType="Literal">&lt;p&gt;As the capabilities of language models continue to advance, it is conceivable
that &quot;one-size-fits-all&quot; model will remain as the main paradigm. For instance,
given the vast number of languages worldwide, many of which are low-resource,
the prevalent practice is to pretrain a single model on multiple languages. In
this paper, we add to the growing body of evidence that challenges this
practice, demonstrating that monolingual pretraining on the target language
significantly improves models already extensively trained on diverse corpora.
More specifically, we further pretrain GPT-J and LLaMA models on Portuguese
texts using 3% or less of their original pretraining budget. Few-shot
evaluations on Poeta, a suite of 14 Portuguese datasets, reveal that our models
outperform English-centric and multilingual counterparts by a significant
margin. Our best model, Sabi\&apos;a-65B, performs on par with GPT-3.5-turbo. By
evaluating on datasets originally conceived in the target language as well as
translated ones, we study the contributions of language-specific pretraining in
terms of 1) capturing linguistic nuances and structures inherent to the target
language, and 2) enriching the model&apos;s knowledge about a domain or culture. Our
results indicate that the majority of the benefits stem from the
domain-specific knowledge acquired through monolingual pretraining.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pires_R/0/1/0/all/0/1&quot;&gt;Ramon Pires&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abonizio_H/0/1/0/all/0/1&quot;&gt;Hugo Abonizio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almeida_T/0/1/0/all/0/1&quot;&gt;Thales Sales Almeida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1&quot;&gt;Rodrigo Nogueira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.13000">
<title>Segment anything, from space?. (arXiv:2304.13000v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.13000</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the first foundation model developed specifically for image
segmentation tasks was developed, termed the &quot;Segment Anything Model&quot; (SAM).
SAM can segment objects in input imagery based on cheap input prompts, such as
one (or more) points, a bounding box, or a mask. The authors examined the
\textit{zero-shot} image segmentation accuracy of SAM on a large number of
vision benchmark tasks and found that SAM usually achieved recognition accuracy
similar to, or sometimes exceeding, vision models that had been trained on the
target tasks. The impressive generalization of SAM for segmentation has major
implications for vision researchers working on natural imagery. In this work,
we examine whether SAM&apos;s performance extends to overhead imagery problems and
help guide the community&apos;s response to its development. We examine SAM&apos;s
performance on a set of diverse and widely studied benchmark tasks. We find
that SAM does often generalize well to overhead imagery, although it fails in
some cases due to the unique characteristics of overhead imagery and its common
target objects. We report on these unique systematic failure cases for remote
sensing imagery that may comprise useful future research for the community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Simiao Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luzi_F/0/1/0/all/0/1&quot;&gt;Francesco Luzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lahrichi_S/0/1/0/all/0/1&quot;&gt;Saad Lahrichi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kassaw_K/0/1/0/all/0/1&quot;&gt;Kaleb Kassaw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_L/0/1/0/all/0/1&quot;&gt;Leslie M. Collins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bradbury_K/0/1/0/all/0/1&quot;&gt;Kyle Bradbury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malof_J/0/1/0/all/0/1&quot;&gt;Jordan M. Malof&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10564">
<title>Counterfactually Comparing Abstaining Classifiers. (arXiv:2305.10564v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10564</link>
<description rdf:parseType="Literal">&lt;p&gt;Abstaining classifiers have the option to abstain from making predictions on
inputs that they are unsure about. These classifiers are becoming increasingly
popular in high-stakes decision-making problems, as they can withhold uncertain
predictions to improve their reliability and safety. When evaluating black-box
abstaining classifier(s), however, we lack a principled approach that accounts
for what the classifier would have predicted on its abstentions. These missing
predictions matter when they can eventually be utilized, either directly or as
a backup option in a failure mode. In this paper, we introduce a novel approach
and perspective to the problem of evaluating and comparing abstaining
classifiers by treating abstentions as missing data. Our evaluation approach is
centered around defining the counterfactual score of an abstaining classifier,
defined as the expected performance of the classifier had it not been allowed
to abstain. We specify the conditions under which the counterfactual score is
identifiable: if the abstentions are stochastic, and if the evaluation data is
independent of the training data (ensuring that the predictions are missing at
random), then the score is identifiable. Note that, if abstentions are
deterministic, then the score is unidentifiable because the classifier can
perform arbitrarily poorly on its abstentions. Leveraging tools from
observational causal inference, we then develop nonparametric and doubly robust
methods to efficiently estimate this quantity under identification. Our
approach is examined in both simulated and real data experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Choe_Y/0/1/0/all/0/1&quot;&gt;Yo Joong Choe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gangrade_A/0/1/0/all/0/1&quot;&gt;Aditya Gangrade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1&quot;&gt;Aaditya Ramdas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14979">
<title>Assessment of the Reliablity of a Model&apos;s Decision by Generalizing Attribution to the Wavelet Domain. (arXiv:2305.14979v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14979</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks have shown remarkable performance in computer vision, but
their deployment in numerous scientific and technical fields is challenging due
to their black-box nature. Scientists and practitioners need to evaluate the
reliability of a decision, i.e., to know simultaneously if a model relies on
the relevant features and whether these features are robust to image
corruptions. Existing attribution methods aim to provide human-understandable
explanations by highlighting important regions in the image domain, but fail to
fully characterize a decision process&apos;s reliability. To bridge this gap, we
introduce the Wavelet sCale Attribution Method (WCAM), a generalization of
attribution from the pixel domain to the space-scale domain using wavelet
transforms. Attribution in the wavelet domain reveals where and on what scales
the model focuses, thus enabling us to assess whether a decision is reliable.
Our code is accessible here:
\url{https://github.com/gabrielkasmi/spectral-attribution}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasmi_G/0/1/0/all/0/1&quot;&gt;Gabriel Kasmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubus_L/0/1/0/all/0/1&quot;&gt;Laurent Dubus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drenan_Y/0/1/0/all/0/1&quot;&gt;Yves-Marie Saint Drenan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blanc_P/0/1/0/all/0/1&quot;&gt;Philippe Blanc&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15001">
<title>Contrastive Training of Complex-Valued Autoencoders for Object Discovery. (arXiv:2305.15001v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15001</link>
<description rdf:parseType="Literal">&lt;p&gt;Current state-of-the-art object-centric models use slots and attention-based
routing for binding. However, this class of models has several conceptual
limitations: the number of slots is hardwired; all slots have equal capacity;
training has high computational cost; there are no object-level relational
factors within slots. Synchrony-based models in principle can address these
limitations by using complex-valued activations which store binding information
in their phase components. However, working examples of such synchrony-based
models have been developed only very recently, and are still limited to toy
grayscale datasets and simultaneous storage of less than three objects in
practice. Here we introduce architectural modifications and a novel contrastive
learning method that greatly improve the state-of-the-art synchrony-based
model. For the first time, we obtain a class of synchrony-based models capable
of discovering objects in an unsupervised manner in multi-object color datasets
and simultaneously representing more than three objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanic_A/0/1/0/all/0/1&quot;&gt;Aleksandar Stani&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalakrishnan_A/0/1/0/all/0/1&quot;&gt;Anand Gopalakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irie_K/0/1/0/all/0/1&quot;&gt;Kazuki Irie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1&quot;&gt;J&amp;#xfc;rgen Schmidhuber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16841">
<title>Differentiable Random Partition Models. (arXiv:2305.16841v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16841</link>
<description rdf:parseType="Literal">&lt;p&gt;Partitioning a set of elements into an unknown number of mutually exclusive
subsets is essential in many machine learning problems. However, assigning
elements, such as samples in a dataset or neurons in a network layer, to an
unknown and discrete number of subsets is inherently non-differentiable,
prohibiting end-to-end gradient-based optimization of parameters. We overcome
this limitation by proposing a novel two-step method for inferring partitions,
which allows its usage in variational inference tasks. This new approach
enables reparameterized gradients with respect to the parameters of the new
random partition model. Our method works by inferring the number of elements
per subset and, second, by filling these subsets in a learned order. We
highlight the versatility of our general-purpose approach on three different
challenging experiments: variational clustering, inference of shared and
independent generative factors under weak supervision, and multitask learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutter_T/0/1/0/all/0/1&quot;&gt;Thomas M. Sutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryser_A/0/1/0/all/0/1&quot;&gt;Alain Ryser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liebeskind_J/0/1/0/all/0/1&quot;&gt;Joram Liebeskind&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1&quot;&gt;Julia E. Vogt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01665">
<title>SourceP: Detecting Ponzi Schemes on Ethereum with Source Code. (arXiv:2306.01665v4 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01665</link>
<description rdf:parseType="Literal">&lt;p&gt;As blockchain technology becomes more and more popular, a typical financial
scam, the Ponzi scheme, has also emerged in the blockchain platform Ethereum.
This Ponzi scheme deployed through smart contracts, also known as the smart
Ponzi scheme, has caused a lot of economic losses and negative impacts.
Existing methods for detecting smart Ponzi schemes on Ethereum mainly rely on
bytecode features, opcode features, account features, and transaction behavior
features of smart contracts, and the performance of identifying schemes is
insufficient. In this paper, we propose SourceP, a method to detect smart Ponzi
schemes on the Ethereum platform using pre-trained models and data flow, which
only requires using the source code of smart contracts as features to explore
the possibility of detecting smart Ponzi schemes from another direction.
SourceP reduces the difficulty of data acquisition and feature extraction of
existing detection methods while increasing the interpretability of the model.
Specifically, we first convert the source code of a smart contract into a data
flow graph and then introduce a pre-trained model based on learning code
representations to build a classification model to identify Ponzi schemes in
smart contracts. The experimental results show that SourceP achieves 87.2\%
recall and 90.7\% F-score for detecting smart Ponzi schemes within Ethereum&apos;s
smart contract dataset, outperforming state-of-the-art methods in terms of
performance and sustainability. We also demonstrate through additional
experiments that pre-trained models and data flow play an important
contribution to SourceP, as well as proving that SourceP has a good
generalization ability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Pengcheng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1&quot;&gt;Liang Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1&quot;&gt;Keting Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04027">
<title>Intervention Generalization: A View from Factor Graph Models. (arXiv:2306.04027v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04027</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the goals of causal inference is to generalize from past experiments
and observational data to novel conditions. While it is in principle possible
to eventually learn a mapping from a novel experimental condition to an outcome
of interest, provided a sufficient variety of experiments is available in the
training data, coping with a large combinatorial space of possible
interventions is hard. Under a typical sparse experimental design, this mapping
is ill-posed without relying on heavy regularization or prior distributions.
Such assumptions may or may not be reliable, and can be hard to defend or test.
In this paper, we take a close look at how to warrant a leap from past
experiments to novel conditions based on minimal assumptions about the
factorization of the distribution of the manipulated system, communicated in
the well-understood language of factor graph models. A postulated
$\textit{interventional factor model}$ (IFM) may not always be informative, but
it conveniently abstracts away a need for explicitly modeling unmeasured
confounding and feedback mechanisms, leading to directly testable claims. Given
an IFM and datasets from a collection of experimental regimes, we derive
conditions for identifiability of the expected outcomes of new regimes never
observed in these training data. We implement our framework using several
efficient algorithms, and apply them on a range of semi-synthetic experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bravo_Hermsdorff_G/0/1/0/all/0/1&quot;&gt;Gecia Bravo-Hermsdorff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Watson_D/0/1/0/all/0/1&quot;&gt;David S. Watson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jialin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zeitler_J/0/1/0/all/0/1&quot;&gt;Jakob Zeitler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Silva_R/0/1/0/all/0/1&quot;&gt;Ricardo Silva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08013">
<title>TopP&amp;R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08013</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a robust and reliable evaluation metric for generative models by
introducing topological and statistical treatments for rigorous support
estimation. Existing metrics, such as Inception Score (IS), Frechet Inception
Distance (FID), and the variants of Precision and Recall (P&amp;amp;R), heavily rely on
supports that are estimated from sample features. However, the reliability of
their estimation has not been seriously discussed (and overlooked) even though
the quality of the evaluation entirely depends on it. In this paper, we propose
Topological Precision and Recall (TopP&amp;amp;R, pronounced &apos;topper&apos;), which provides
a systematic approach to estimating supports, retaining only topologically and
statistically important features with a certain level of confidence. This not
only makes TopP&amp;amp;R strong for noisy features, but also provides statistical
consistency. Our theoretical and experimental results show that TopP&amp;amp;R is
robust to outliers and non-independent and identically distributed (Non-IID)
perturbations, while accurately capturing the true trend of change in samples.
To the best of our knowledge, this is the first evaluation metric focused on
the robust estimation of the support and provides its statistical consistency
under noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_P/0/1/0/all/0/1&quot;&gt;Pum Jun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1&quot;&gt;Yoojin Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jisu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1&quot;&gt;Jaejun Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11167">
<title>Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset. (arXiv:2306.11167v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11167</link>
<description rdf:parseType="Literal">&lt;p&gt;The quest for human imitative AI has been an enduring topic in AI research
since its inception. The technical evolution and emerging capabilities of the
latest cohort of large language models (LLMs) have reinvigorated the subject
beyond academia to the cultural zeitgeist. While recent NLP evaluation
benchmark tasks test some aspects of human-imitative behaviour (e.g.,
BIG-bench&apos;s &apos;human-like behavior&apos; tasks), few, if not none, examine creative
problem solving abilities. Creative problem solving in humans is a well-studied
topic in cognitive neuroscience with standardized tests that predominantly use
the ability to associate (heterogeneous) connections among clue words as a
metric for creativity. Exposure to misleading stimuli - distractors dubbed red
herrings - impede human performance in such tasks via the fixation effect and
Einstellung paradigm. In cognitive neuroscience studies, such fixations are
experimentally induced by pre-exposing participants to orthographically similar
incorrect words to subsequent word-fragments or clues. The popular British quiz
show Only Connect&apos;s Connecting Wall segment essentially mimics Mednick&apos;s Remote
Associates Test (RAT) formulation with built-in, deliberate red herrings, which
makes it an ideal proxy dataset to explore and study fixation effect and
Einstellung paradigm from cognitive neuroscience in LLMs. In this paper we
present the novel Only Connect Wall (OCW) dataset and report results from our
evaluation of selected pre-trained language models and LLMs on creative problem
solving tasks like grouping clue words by heterogeneous connections, and
identifying correct open knowledge domain connections in respective groups. We
synthetically generate two additional datasets: OCW-Randomized, OCW-WordNet to
further analyze our red-herrings hypothesis in language models. The code and
link to the dataset are available at https://github.com/TaatiTeam/OCW.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naeini_S/0/1/0/all/0/1&quot;&gt;Saeid Naeini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saqur_R/0/1/0/all/0/1&quot;&gt;Raeid Saqur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saeidi_M/0/1/0/all/0/1&quot;&gt;Mozhgan Saeidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giorgi_J/0/1/0/all/0/1&quot;&gt;John Giorgi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taati_B/0/1/0/all/0/1&quot;&gt;Babak Taati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12929">
<title>Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing. (arXiv:2306.12929v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12929</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer models have been widely adopted in various domains over the last
years, and especially large language models have advanced the field of AI
significantly. Due to their size, the capability of these networks has
increased tremendously, but this has come at the cost of a significant increase
in necessary compute. Quantization is one of the most effective ways to reduce
the computational time and memory consumption of neural networks. Many studies
have shown, however, that modern transformer models tend to learn strong
outliers in their activations, making them difficult to quantize. To retain
acceptable performance, the existence of these outliers requires activations to
be in higher bitwidth or the use of different numeric formats, extra
fine-tuning, or other workarounds. We show that strong outliers are related to
very specific behavior of attention heads that try to learn a &quot;no-op&quot; or just a
partial update of the residual. To achieve the exact zeros needed in the
attention matrix for a no-update, the input to the softmax is pushed to be
larger and larger during training, causing outliers in other parts of the
network. Based on these observations, we propose two simple (independent)
modifications to the attention mechanism - clipped softmax and gated attention.
We empirically show that models pre-trained using our methods learn
significantly smaller outliers while maintaining and sometimes even improving
the floating-point task performance. This enables us to quantize transformers
to full INT8 quantization of the activations without any additional effort. We
demonstrate the effectiveness of our methods on both language models (BERT,
OPT) and vision transformers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bondarenko_Y/0/1/0/all/0/1&quot;&gt;Yelysei Bondarenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagel_M/0/1/0/all/0/1&quot;&gt;Markus Nagel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blankevoort_T/0/1/0/all/0/1&quot;&gt;Tijmen Blankevoort&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06933">
<title>FDAPT: Federated Domain-adaptive Pre-training for Language Models. (arXiv:2307.06933v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06933</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation models (FMs) have shown prominent success in a wide range of
tasks. Their applicability to specific domain-task pairings relies on the
availability of, both, high-quality data and significant computational
resources. These challenges are not new to the field and, indeed, Federated
Learning (FL) has been shown to be a promising solution in similar setups. This
paper tackles the specific case of Domain-Adaptive Pre-Training (DAPT), a key
step in the application of FMs. We conduct the first comprehensive empirical
study to evaluate the performance of Federated Domain-Adaptive Pre-Training
(FDAPT). We demonstrate that FDAPT can maintain competitive downstream task
performance to the centralized baseline in both IID and non-IID situations.
Finally, we propose a novel algorithm, Frozen Federated Domain-Adaptive
Pre-Training (FFDAPT). FFDAPT improves the computational efficiency by 12.1% on
average and exhibits similar downstream task performance to vanilla FDAPT, with
general performance fluctuations remaining less than 1%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Lekang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Svoboda_F/0/1/0/all/0/1&quot;&gt;Filip Svoboda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1&quot;&gt;Nicholas D. Lane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06294">
<title>Enhancing Phenotype Recognition in Clinical Notes Using Large Language Models: PhenoBCBERT and PhenoGPT. (arXiv:2308.06294v2 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06294</link>
<description rdf:parseType="Literal">&lt;p&gt;We hypothesize that large language models (LLMs) based on the transformer
architecture can enable automated detection of clinical phenotype terms,
including terms not documented in the HPO. In this study, we developed two
types of models: PhenoBCBERT, a BERT-based model, utilizing Bio+Clinical BERT
as its pre-trained model, and PhenoGPT, a GPT-based model that can be
initialized from diverse GPT models, including open-source versions such as
GPT-J, Falcon, and LLaMA, as well as closed-source versions such as GPT-3 and
GPT-3.5. We compared our methods with PhenoTagger, a recently developed HPO
recognition tool that combines rule-based and deep learning methods. We found
that our methods can extract more phenotype concepts, including novel ones not
characterized by HPO. We also performed case studies on biomedical literature
to illustrate how new phenotype information can be recognized and extracted. We
compared current BERT-based versus GPT-based models for phenotype tagging, in
multiple aspects including model architecture, memory usage, speed, accuracy,
and privacy protection. We also discussed the addition of a negation step and
an HPO normalization layer to the transformer models for improved HPO term
tagging. In conclusion, PhenoBCBERT and PhenoGPT enable the automated discovery
of phenotype terms from clinical notes and biomedical literature, facilitating
automated downstream tasks to derive new biological insights on human diseases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jingye Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Deng_W/0/1/0/all/0/1&quot;&gt;Wendy Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Da Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Weng_C/0/1/0/all/0/1&quot;&gt;Chunhua Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yunyun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11100">
<title>Using Early Exits for Fast Inference in Automatic Modulation Classification. (arXiv:2308.11100v2 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11100</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic modulation classification (AMC) plays a critical role in wireless
communications by autonomously classifying signals transmitted over the radio
spectrum. Deep learning (DL) techniques are increasingly being used for AMC due
to their ability to extract complex wireless signal features. However, DL
models are computationally intensive and incur high inference latencies. This
paper proposes the application of early exiting (EE) techniques for DL models
used for AMC to accelerate inference. We present and analyze four early exiting
architectures and a customized multi-branch training algorithm for this
problem. Through extensive experimentation, we show that signals with moderate
to high signal-to-noise ratios (SNRs) are easier to classify, do not require
deep architectures, and can therefore leverage the proposed EE architectures.
Our experimental results demonstrate that EE techniques can significantly
reduce the inference speed of deep neural networks without sacrificing
classification accuracy. We also thoroughly study the trade-off between
classification accuracy and inference time when using these architectures. To
the best of our knowledge, this work represents the first attempt to apply
early exiting methods to AMC, providing a foundation for future research in
this area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammed_E/0/1/0/all/0/1&quot;&gt;Elsayed Mohammed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mashaal_O/0/1/0/all/0/1&quot;&gt;Omar Mashaal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abou_Zeid_H/0/1/0/all/0/1&quot;&gt;Hatem Abou-Zeid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14991">
<title>Incorporating Neuro-Inspired Adaptability for Continual Learning in Artificial Intelligence. (arXiv:2308.14991v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14991</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning aims to empower artificial intelligence (AI) with strong
adaptability to the real world. For this purpose, a desirable solution should
properly balance memory stability with learning plasticity, and acquire
sufficient compatibility to capture the observed distributions. Existing
advances mainly focus on preserving memory stability to overcome catastrophic
forgetting, but remain difficult to flexibly accommodate incremental changes as
biological intelligence (BI) does. By modeling a robust Drosophila learning
system that actively regulates forgetting with multiple learning modules, here
we propose a generic approach that appropriately attenuates old memories in
parameter distributions to improve learning plasticity, and accordingly
coordinates a multi-learner architecture to ensure solution compatibility.
Through extensive theoretical and empirical validation, our approach not only
clearly enhances the performance of continual learning, especially over
synaptic regularization methods in task-incremental settings, but also
potentially advances the understanding of neurological adaptive mechanisms,
serving as a novel paradigm to progress AI and BI together.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xingxing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingtian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yi Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.04069">
<title>Inferring physical laws by artificial intelligence based causal models. (arXiv:2309.04069v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.04069</link>
<description rdf:parseType="Literal">&lt;p&gt;The advances in Artificial Intelligence (AI) and Machine Learning (ML) have
opened up many avenues for scientific research, and are adding new dimensions
to the process of knowledge creation. However, even the most powerful and
versatile of ML applications till date are primarily in the domain of analysis
of associations and boil down to complex data fitting. Judea Pearl has pointed
out that Artificial General Intelligence must involve interventions involving
the acts of doing and imagining. Any machine assisted scientific discovery thus
must include casual analysis and interventions. In this context, we propose a
causal learning model of physical principles, which not only recognizes
correlations but also brings out casual relationships. We use the principles of
causal inference and interventions to study the cause-and-effect relationships
in the context of some well-known physical phenomena. We show that this
technique can not only figure out associations among data, but is also able to
correctly ascertain the cause-and-effect relations amongst the variables,
thereby strengthening (or weakening) our confidence in the proposed model of
the underlying physical process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1&quot;&gt;Jorawar Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bharti_K/0/1/0/all/0/1&quot;&gt;Kishor Bharti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arvind/0/1/0/all/0/1&quot;&gt;Arvind&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07867">
<title>Cheap Talking Algorithms. (arXiv:2310.07867v2 [econ.TH] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07867</link>
<description rdf:parseType="Literal">&lt;p&gt;We simulate behavior of independent reinforcement learning algorithms playing
the Crawford and Sobel (1982) game of strategic information transmission. We
show that a sender and a receiver training together converge to strategies
close to the ex-ante optimal equilibrium of the game. Hence, communication
takes place to the largest extent predicted by Nash equilibrium. The conclusion
is robust to alternative specifications of the learning hyperparameters and of
the game. We discuss implications for theories of equilibrium selection in
information transmission games, for work on emerging communication among
algorithms in computer science, and for the economics of collusions in markets
populated by artificially intelligent agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Condorelli_D/0/1/0/all/0/1&quot;&gt;Daniele Condorelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Furlan_M/0/1/0/all/0/1&quot;&gt;Massimiliano Furlan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09949">
<title>Chameleon: a heterogeneous and disaggregated accelerator system for retrieval-augmented language models. (arXiv:2310.09949v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09949</link>
<description rdf:parseType="Literal">&lt;p&gt;A Retrieval-Augmented Language Model (RALM) augments a generative language
model by retrieving context-specific knowledge from an external database. This
strategy facilitates impressive text generation quality even with smaller
models, thus reducing orders of magnitude of computational demands. However,
RALMs introduce unique system design challenges due to (a) the diverse workload
characteristics between LM inference and retrieval and (b) the various system
requirements and bottlenecks for different RALM configurations such as model
sizes, database sizes, and retrieval frequencies. We propose Chameleon, a
heterogeneous accelerator system that integrates both LM and retrieval
accelerators in a disaggregated architecture. The heterogeneity ensures
efficient acceleration of both LM inference and retrieval, while the
accelerator disaggregation enables the system to independently scale both types
of accelerators to fulfill diverse RALM requirements. Our Chameleon prototype
implements retrieval accelerators on FPGAs and assigns LM inference to GPUs,
with a CPU server orchestrating these accelerators over the network. Compared
to CPU-based and CPU-GPU vector search systems, Chameleon achieves up to 23.72x
speedup and 26.2x energy efficiency. Evaluated on various RALMs, Chameleon
exhibits up to 2.16x reduction in latency and 3.18x speedup in throughput
compared to the hybrid CPU-GPU architecture. These promising results pave the
way for bringing accelerator heterogeneity and disaggregation into future RALM
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wenqi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeller_M/0/1/0/all/0/1&quot;&gt;Marco Zeller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waleffe_R/0/1/0/all/0/1&quot;&gt;Roger Waleffe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1&quot;&gt;Torsten Hoefler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_G/0/1/0/all/0/1&quot;&gt;Gustavo Alonso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10378">
<title>Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models. (arXiv:2310.10378v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10378</link>
<description rdf:parseType="Literal">&lt;p&gt;Multilingual large-scale Pretrained Language Models (PLMs) have been shown to
store considerable amounts of factual knowledge, but large variations are
observed across languages. With the ultimate goal of ensuring that users with
different language backgrounds obtain consistent feedback from the same model,
we study the cross-lingual consistency (CLC) of factual knowledge in various
multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC)
metric to evaluate knowledge consistency across languages independently from
accuracy. Using this metric, we conduct an in-depth analysis of the determining
factors for CLC, both at model level and at language-pair level. Among other
results, we find that increasing model size leads to higher factual probing
accuracy in most languages, but does not improve cross-lingual consistency.
Finally, we conduct a case study on CLC when new factual associations are
inserted in the PLMs via model editing. Results on a small sample of facts
inserted in English reveal a clear pattern whereby the new piece of knowledge
transfers only to languages with which English has a high RankC score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1&quot;&gt;Jirui Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_R/0/1/0/all/0/1&quot;&gt;Raquel Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisazza_A/0/1/0/all/0/1&quot;&gt;Arianna Bisazza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12802">
<title>An effective theory of collective deep learning. (arXiv:2310.12802v2 [physics.soc-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12802</link>
<description rdf:parseType="Literal">&lt;p&gt;Unraveling the emergence of collective learning in systems of coupled
artificial neural networks points to broader implications for machine learning,
neuroscience, and society. Here we introduce a minimal model that condenses
several recent decentralized algorithms by considering a competition between
two terms: the local learning dynamics in the parameters of each neural network
unit, and a diffusive coupling among units that tends to homogenize the
parameters of the ensemble. We derive an effective theory for linear networks
to show that the coarse-grained behavior of our system is equivalent to a
deformed Ginzburg-Landau model with quenched disorder. This framework predicts
depth-dependent disorder-order-disorder phase transitions in the parameters&apos;
solutions that reveal a depth-delayed onset of a collective learning phase and
a low-rank microscopic learning path. We validate the theory in coupled
ensembles of realistic neural networks trained on the MNIST dataset under
privacy constraints. Interestingly, experiments confirm that individual
networks -- trained on private data -- can fully generalize to unseen data
classes when the collective learning phase emerges. Our work establishes the
physics of collective learning and contributes to the mechanistic
interpretability of deep learning in decentralized settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Arola_Fernandez_L/0/1/0/all/0/1&quot;&gt;Llu&amp;#xed;s Arola-Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lacasa_L/0/1/0/all/0/1&quot;&gt;Lucas Lacasa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13121">
<title>Understanding Addition in Transformers. (arXiv:2310.13121v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13121</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the inner workings of machine learning models like Transformers
is vital for their safe and ethical use. This paper presents an in-depth
analysis of a one-layer Transformer model trained for n-digit integer addition.
We reveal that the model divides the task into parallel, digit-specific streams
and employs distinct algorithms for different digit positions. Our study also
finds that the model starts calculations late but executes them rapidly. A rare
use case with high loss is identified and explained. Overall, the model&apos;s
algorithm is explained in detail. These findings are validated through rigorous
testing and mathematical modeling, contributing to the broader works in
Mechanistic Interpretability, AI safety, and alignment. Our approach opens the
door for analyzing more complex tasks and multi-layer Transformer models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quirke_P/0/1/0/all/0/1&quot;&gt;Philip Quirke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1&quot;&gt;Fazl Barez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16772">
<title>AI Agent as Urban Planner: Steering Stakeholder Dynamics in Urban Planning via Consensus-based Multi-Agent Reinforcement Learning. (arXiv:2310.16772v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16772</link>
<description rdf:parseType="Literal">&lt;p&gt;In urban planning, land use readjustment plays a pivotal role in aligning
land use configurations with the current demands for sustainable urban
development. However, present-day urban planning practices face two main
issues. Firstly, land use decisions are predominantly dependent on human
experts. Besides, while resident engagement in urban planning can promote urban
sustainability and livability, it is challenging to reconcile the diverse
interests of stakeholders. To address these challenges, we introduce a
Consensus-based Multi-Agent Reinforcement Learning framework for real-world
land use readjustment. This framework serves participatory urban planning,
allowing diverse intelligent agents as stakeholder representatives to vote for
preferred land use types. Within this framework, we propose a novel consensus
mechanism in reward design to optimize land utilization through collective
decision making. To abstract the structure of the complex urban system, the
geographic information of cities is transformed into a spatial graph structure
and then processed by graph neural networks. Comprehensive experiments on both
traditional top-down planning and participatory planning methods from
real-world communities indicate that our computational framework enhances
global benefits and accommodates diverse interests, leading to improved
satisfaction across different demographic groups. By integrating Multi-Agent
Reinforcement Learning, our framework ensures that participatory urban planning
decisions are more dynamic and adaptive to evolving community needs and
provides a robust platform for automating complex real-world urban planning
processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1&quot;&gt;Kejiang Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_L/0/1/0/all/0/1&quot;&gt;Lingjun Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yimin Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xinran Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Ziyi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiajie Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18590">
<title>Using Early Readouts to Mediate Featural Bias in Distillation. (arXiv:2310.18590v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18590</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep networks tend to learn spurious feature-label correlations in real-world
supervised learning tasks. This vulnerability is aggravated in distillation,
where a student model may have lesser representational capacity than the
corresponding teacher model. Often, knowledge of specific spurious correlations
is used to reweight instances &amp;amp; rebalance the learning process. We propose a
novel early readout mechanism whereby we attempt to predict the label using
representations from earlier network layers. We show that these early readouts
automatically identify problem instances or groups in the form of confident,
incorrect predictions. Leveraging these signals to modulate the distillation
loss on an instance level allows us to substantially improve not only group
fairness measures across benchmark datasets, but also overall accuracy of the
student model. We also provide secondary analyses that bring insight into the
role of feature learning in supervision and distillation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiwari_R/0/1/0/all/0/1&quot;&gt;Rishabh Tiwari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sivasubramanian_D/0/1/0/all/0/1&quot;&gt;Durga Sivasubramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mekala_A/0/1/0/all/0/1&quot;&gt;Anmol Mekala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1&quot;&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shenoy_P/0/1/0/all/0/1&quot;&gt;Pradeep Shenoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00447">
<title>On the Opportunities of Green Computing: A Survey. (arXiv:2311.00447v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00447</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence (AI) has achieved significant advancements in
technology and research with the development over several decades, and is
widely used in many areas including computing vision, natural language
processing, time-series analysis, speech synthesis, etc. During the age of deep
learning, especially with the arise of Large Language Models, a large majority
of researchers&apos; attention is paid on pursuing new state-of-the-art (SOTA)
results, resulting in ever increasing of model size and computational
complexity. The needs for high computing power brings higher carbon emission
and undermines research fairness by preventing small or medium-sized research
institutions and companies with limited funding in participating in research.
To tackle the challenges of computing resources and environmental impact of AI,
Green Computing has become a hot research topic. In this survey, we give a
systematic overview of the technologies used in Green Computing. We propose the
framework of Green Computing and devide it into four key components: (1)
Measures of Greenness, (2) Energy-Efficient AI, (3) Energy-Efficient Computing
Systems and (4) AI Use Cases for Sustainability. For each components, we
discuss the research progress made and the commonly used techniques to optimize
the AI efficiency. We conclude that this new research direction has the
potential to address the conflicts between resource constraints and AI
development. We encourage more researchers to put attention on this direction
and make AI more environmental friendly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;You Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xiujing Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Maolin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1&quot;&gt;Gangwei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Huakang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yupeng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhe Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kehang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1&quot;&gt;Yongduo Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1&quot;&gt;Fengwei Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zuoli Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongxuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tiannuo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weibo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yunong Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_D/0/1/0/all/0/1&quot;&gt;De Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1&quot;&gt;Hongrui Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingwen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jinchi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+WEI_Y/0/1/0/all/0/1&quot;&gt;Ying WEI&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1&quot;&gt;Hong Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kin_W/0/1/0/all/0/1&quot;&gt;Wai Kin&lt;/a&gt; (Victor) &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan/0/1/0/all/0/1&quot;&gt;Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yusen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shiyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Jining Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mou_C/0/1/0/all/0/1&quot;&gt;Chao Mou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Shuai Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1&quot;&gt;Wuxia Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guannan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xiaodong Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02271">
<title>FaMeSumm: Investigating and Improving Faithfulness of Medical Summarization. (arXiv:2311.02271v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02271</link>
<description rdf:parseType="Literal">&lt;p&gt;Summaries of medical text shall be faithful by being consistent and factual
with source inputs, which is an important but understudied topic for safety and
efficiency in healthcare. In this paper, we investigate and improve
faithfulness in summarization on a broad range of medical summarization tasks.
Our investigation reveals that current summarization models often produce
unfaithful outputs for medical input text. We then introduce FaMeSumm, a
framework to improve faithfulness by fine-tuning pre-trained language models
based on medical knowledge. FaMeSumm performs contrastive learning on designed
sets of faithful and unfaithful summaries, and it incorporates medical terms
and their contexts to encourage faithful generation of medical terms. We
conduct comprehensive experiments on three datasets in two languages: health
question and radiology report summarization datasets in English, and a
patient-doctor dialogue dataset in Chinese. Results demonstrate that FaMeSumm
is flexible and effective by delivering consistent improvements over mainstream
language models such as BART, T5, mT5, and PEGASUS, yielding state-of-the-art
performances on metrics for faithfulness and general quality. Human evaluation
by doctors also shows that FaMeSumm generates more faithful outputs. Our code
is available at https://github.com/psunlpgroup/FaMeSumm .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Nan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yusen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1&quot;&gt;Wu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1&quot;&gt;Prasenjit Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rui Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03405">
<title>Communication Efficient and Privacy-Preserving Federated Learning Based on Evolution Strategies. (arXiv:2311.03405v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03405</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) is an emerging paradigm for training deep neural
networks (DNNs) in distributed manners. Current FL approaches all suffer from
high communication overhead and information leakage. In this work, we present a
federated learning algorithm based on evolution strategies (FedES), a
zeroth-order training method. Instead of transmitting model parameters, FedES
only communicates loss values, and thus has very low communication overhead.
Moreover, a third party is unable to estimate gradients without knowing the
pre-shared seed, which protects data privacy. Experimental results demonstrate
FedES can achieve the above benefits while keeping convergence performance the
same as that with back propagation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_G/0/1/0/all/0/1&quot;&gt;Guangchen Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03839">
<title>Aspects of human memory and Large Language Models. (arXiv:2311.03839v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03839</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) are huge artificial neural networks which
primarily serve to generate text, but also provide a very sophisticated
probabilistic model of language use. Since generating a semantically consistent
text requires a form of effective memory, we investigate the memory properties
of LLMs and find surprising similarities with key characteristics of human
memory. We argue that the human-like memory properties of the Large Language
Model do not follow automatically from the LLM architecture but are rather
learned from the statistics of the training textual data. These results
strongly suggest that the biological features of human memory leave an imprint
on the way that we structure our textual narratives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janik_R/0/1/0/all/0/1&quot;&gt;Romuald A. Janik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04879">
<title>LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models. (arXiv:2311.04879v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04879</link>
<description rdf:parseType="Literal">&lt;p&gt;We present LongQLoRA, an efficient and effective method to extend context
length of large language models with less training resources. LongQLoRA
combines the advantages of Position Interpolation, QLoRA and Shift Short
Attention of LongLoRA. With a single 32GB V100 GPU, LongQLoRA can extend the
context length of LLaMA2 7B and 13B from 4096 to 8192 and even to 12k within
1000 finetuning steps. LongQLoRA achieves competitive perplexity performance on
PG19 and Proof-pile datasets, our model outperforms LongLoRA and is very close
to MPT-7B-8K within the evaluation context length of 8192. We collect and build
39k long instruction data to extend context length of Vicuna-13B from 4096 to
8192 and achieve good performance both in long and short context generation
task. We also do some ablation experiments to study the effect of LoRA rank,
finetuning steps and attention patterns in inference.The model weights,
training data and code are avaliable at
https://github.com/yangjianxin1/LongQLoRA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianxin Yang&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>