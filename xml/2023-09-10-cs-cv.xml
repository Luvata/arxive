<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-09-07T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03216" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03295" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03351" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03353" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03367" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03390" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03440" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03445" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03453" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03483" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03493" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03499" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03504" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03509" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03530" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03531" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03558" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03569" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03575" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03576" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03590" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03599" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03640" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03641" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03652" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03659" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03671" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03696" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03734" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03744" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03759" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03774" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03809" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03827" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03837" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03879" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03895" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03900" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03903" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03904" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2007.05059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.03624" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.05893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.09957" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.15201" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.03367" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.01708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.13008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.07590" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.15341" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.03744" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08272" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.14051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00601" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07853" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13606" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.05163" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07803" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09479" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10226" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.01905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01188" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06145" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12760" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13455" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15955" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02321" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12900" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15536" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00398" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00938" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01627" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01728" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02556" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02855" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02843" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2309.03215">
<title>Explainable and Trustworthy Traffic Sign Detection for Safe Autonomous Driving: An Inductive Logic Programming Approach. (arXiv:2309.03215v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.03215</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic sign detection is a critical task in the operation of Autonomous
Vehicles (AV), as it ensures the safety of all road users. Current DNN-based
sign classification systems rely on pixel-level features to detect traffic
signs and can be susceptible to adversarial attacks. These attacks involve
small, imperceptible changes to a sign that can cause traditional classifiers
to misidentify the sign. We propose an Inductive Logic Programming (ILP) based
approach for stop sign detection in AVs to address this issue. This method
utilises high-level features of a sign, such as its shape, colour, and text, to
detect categories of traffic signs. This approach is more robust against
adversarial attacks, as it mimics human-like perception and is less susceptible
to the limitations of current DNN classifiers. We consider two adversarial
attacking methods to evaluate our approach: Robust Physical Perturbation (PR2)
and Adversarial Camouflage (AdvCam). These attacks are able to deceive DNN
classifiers, causing them to misidentify stop signs as other signs with high
confidence. The results show that the proposed ILP-based technique is able to
correctly identify all targeted stop signs, even in the presence of PR2 and
ADvCam attacks. The proposed learning method is also efficient as it requires
minimal training data. Moreover, it is fully explainable, making it possible to
debug AVs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaghazardi_Z/0/1/0/all/0/1&quot;&gt;Zahra Chaghazardi&lt;/a&gt; (University of Surrey), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1&quot;&gt;Saber Fallah&lt;/a&gt; (University of Surrey), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamaddoni_Nezhad_A/0/1/0/all/0/1&quot;&gt;Alireza Tamaddoni-Nezhad&lt;/a&gt; (University of Surrey)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03216">
<title>A Multisensor Hyperspectral Benchmark Dataset For Unmixing of Intimate Mixtures. (arXiv:2309.03216v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03216</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical hyperspectral cameras capture the spectral reflectance of materials.
Since many materials behave as heterogeneous intimate mixtures with which each
photon interacts differently, the relationship between spectral reflectance and
material composition is very complex. Quantitative validation of spectral
unmixing algorithms requires high-quality ground truth fractional abundance
data, which are very difficult to obtain. In this work, we generated a
comprehensive laboratory ground truth dataset of intimately mixed mineral
powders. For this, five clay powders (Kaolin, Roof clay, Red clay, mixed clay,
and Calcium hydroxide) were mixed homogeneously to prepare 325 samples of 60
binary, 150 ternary, 100 quaternary, and 15 quinary mixtures. Thirteen
different hyperspectral sensors have been used to acquire the reflectance
spectra of these mixtures in the visible, near, short, mid, and long-wavelength
infrared regions (350-15385) nm. {\color{black} Overlaps in wavelength regions
due to the operational ranges of each sensor} and variations in acquisition
conditions {\color{black} resulted in} a large amount of spectral variability.
Ground truth composition is given by construction, but to verify that the
generated samples are sufficiently homogeneous, XRD and XRF elemental analysis
is performed. We believe these data will be beneficial for validating advanced
methods for nonlinear unmixing and material composition estimation, including
studying spectral variability and training supervised unmixing approaches. The
datasets can be downloaded from the following link:
https://github.com/VisionlabUA/Multisensor_datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koirala_B/0/1/0/all/0/1&quot;&gt;Bikram Koirala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasti_B/0/1/0/all/0/1&quot;&gt;Behnood Rasti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bnoulkacem_Z/0/1/0/all/0/1&quot;&gt;Zakaria Bnoulkacem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1&quot;&gt;Andrea de Lima Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madriz_Y/0/1/0/all/0/1&quot;&gt;Yuleika Madriz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herrmann_E/0/1/0/all/0/1&quot;&gt;Erik Herrmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gestels_A/0/1/0/all/0/1&quot;&gt;Arthur Gestels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerf_T/0/1/0/all/0/1&quot;&gt;Thomas De Kerf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorenz_S/0/1/0/all/0/1&quot;&gt;Sandra Lorenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuchs_M/0/1/0/all/0/1&quot;&gt;Margret Fuchs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janssens_K/0/1/0/all/0/1&quot;&gt;Koen Janssens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steenackers_G/0/1/0/all/0/1&quot;&gt;Gunther Steenackers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gloaguen_R/0/1/0/all/0/1&quot;&gt;Richard Gloaguen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scheunders_P/0/1/0/all/0/1&quot;&gt;Paul Scheunders&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03232">
<title>Retail store customer behavior analysis system: Design and Implementation. (arXiv:2309.03232v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03232</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding customer behavior in retail stores plays a crucial role in
improving customer satisfaction by adding personalized value to services.
Behavior analysis reveals both general and detailed patterns in the interaction
of customers with a store items and other people, providing store managers with
insight into customer preferences. Several solutions aim to utilize this data
by recognizing specific behaviors through statistical visualization. However,
current approaches are limited to the analysis of small customer behavior sets,
utilizing conventional methods to detect behaviors. They do not use deep
learning techniques such as deep neural networks, which are powerful methods in
the field of computer vision. Furthermore, these methods provide limited
figures when visualizing the behavioral data acquired by the system. In this
study, we propose a framework that includes three primary parts: mathematical
modeling of customer behaviors, behavior analysis using an efficient deep
learning based system, and individual and group behavior visualization. Each
module and the entire system were validated using data from actual situations
in a retail store.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tuan Dinh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hihara_K/0/1/0/all/0/1&quot;&gt;Keisuke Hihara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1&quot;&gt;Tung Cao Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Utada_Y/0/1/0/all/0/1&quot;&gt;Yumeka Utada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torii_A/0/1/0/all/0/1&quot;&gt;Akihiko Torii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izumi_N/0/1/0/all/0/1&quot;&gt;Naoki Izumi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thuy_N/0/1/0/all/0/1&quot;&gt;Nguyen Thanh Thuy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_L/0/1/0/all/0/1&quot;&gt;Long Quoc Tran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03240">
<title>RepSGG: Novel Representations of Entities and Relationships for Scene Graph Generation. (arXiv:2309.03240v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03240</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene Graph Generation (SGG) has achieved significant progress recently.
However, most previous works rely heavily on fixed-size entity representations
based on bounding box proposals, anchors, or learnable queries. As each
representation&apos;s cardinality has different trade-offs between performance and
computation overhead, extracting highly representative features efficiently and
dynamically is both challenging and crucial for SGG. In this work, a novel
architecture called RepSGG is proposed to address the aforementioned
challenges, formulating a subject as queries, an object as keys, and their
relationship as the maximum attention weight between pairwise queries and keys.
With more fine-grained and flexible representation power for entities and
relationships, RepSGG learns to sample semantically discriminative and
representative points for relationship inference. Moreover, the long-tailed
distribution also poses a significant challenge for generalization of SGG. A
run-time performance-guided logit adjustment (PGLA) strategy is proposed such
that the relationship logits are modified via affine transformations based on
run-time performance during training. This strategy encourages a more balanced
performance between dominant and rare classes. Experimental results show that
RepSGG achieves the state-of-the-art or comparable performance on the Visual
Genome and Open Images V6 datasets with fast inference speed, demonstrating the
efficacy and efficiency of the proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hengyue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1&quot;&gt;Bir Bhanu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03244">
<title>EGIC: Enhanced Low-Bit-Rate Generative Image Compression Guided by Semantic Segmentation. (arXiv:2309.03244v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.03244</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce EGIC, a novel generative image compression method that allows
traversing the distortion-perception curve efficiently from a single model.
Specifically, we propose an implicitly encoded variant of image interpolation
that predicts the residual between a MSE-optimized and GAN-optimized decoder
output. On the receiver side, the user can then control the impact of the
residual on the GAN-based reconstruction. Together with improved GAN-based
building blocks, EGIC outperforms a wide-variety of perception-oriented and
distortion-oriented baselines, including HiFiC, MRIC and DIRAC, while
performing almost on par with VTM-20.0 on the distortion end. EGIC is simple to
implement, very lightweight (e.g. 0.18x model parameters compared to HiFiC) and
provides excellent interpolation characteristics, which makes it a promising
candidate for practical applications targeting the low bit range.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Korber_N/0/1/0/all/0/1&quot;&gt;Nikolai K&amp;#xf6;rber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kromer_E/0/1/0/all/0/1&quot;&gt;Eduard Kromer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Siebert_A/0/1/0/all/0/1&quot;&gt;Andreas Siebert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hauke_S/0/1/0/all/0/1&quot;&gt;Sascha Hauke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mueller_Gritschneder_D/0/1/0/all/0/1&quot;&gt;Daniel Mueller-Gritschneder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03247">
<title>Robust Visual Tracking by Motion Analyzing. (arXiv:2309.03247v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03247</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, Video Object Segmentation (VOS) has emerged as a
complementary method to Video Object Tracking (VOT). VOS focuses on classifying
all the pixels around the target, allowing for precise shape labeling, while
VOT primarily focuses on the approximate region where the target might be.
However, traditional segmentation modules usually classify pixels frame by
frame, disregarding information between adjacent frames.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a new algorithm that addresses this limitation by
analyzing the motion pattern using the inherent tensor structure. The tensor
structure, obtained through Tucker2 tensor decomposition, proves to be
effective in describing the target&apos;s motion. By incorporating this information,
we achieved competitive results on Four benchmarks LaSOT\cite{fan2019lasot},
AVisT\cite{noman2022avist}, OTB100\cite{7001050}, and
GOT-10k\cite{huang2019got} LaSOT\cite{fan2019lasot} with SOTA. Furthermore, the
proposed tracker is capable of real-time operation, adding value to its
practical application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leo_M/0/1/0/all/0/1&quot;&gt;Mohammed Leo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ubul_K/0/1/0/all/0/1&quot;&gt;Kurban Ubul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;ShengJie Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Michael Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03295">
<title>Comparative Analysis of Deep-Fake Algorithms. (arXiv:2309.03295v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03295</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the widespread use of smartphones with high-quality digital cameras
and easy access to a wide range of software apps for recording, editing, and
sharing videos and images, as well as the deep learning AI platforms, a new
phenomenon of &apos;faking&apos; videos has emerged. Deepfake algorithms can create fake
images and videos that are virtually indistinguishable from authentic ones.
Therefore, technologies that can detect and assess the integrity of digital
visual media are crucial. Deepfakes, also known as deep learning-based fake
videos, have become a major concern in recent years due to their ability to
manipulate and alter images and videos in a way that is virtually
indistinguishable from the original. These deepfake videos can be used for
malicious purposes such as spreading misinformation, impersonating individuals,
and creating fake news. Deepfake detection technologies use various approaches
such as facial recognition, motion analysis, and audio-visual synchronization
to identify and flag fake videos. However, the rapid advancement of deepfake
technologies has made it increasingly difficult to detect these videos with
high accuracy. In this paper, we aim to provide a comprehensive review of the
current state of deepfake creation and detection technologies. We examine the
various deep learning-based approaches used for creating deepfakes, as well as
the techniques used for detecting them. Additionally, we analyze the
limitations and challenges of current deepfake detection methods and discuss
future research directions in this field. Overall, the paper highlights the
importance of continued research and development in deepfake detection
technologies in order to combat the negative impact of deepfakes on society and
ensure the integrity of digital visual media.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sontakke_N/0/1/0/all/0/1&quot;&gt;Nikhil Sontakke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Utekar_S/0/1/0/all/0/1&quot;&gt;Sejal Utekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastogi_S/0/1/0/all/0/1&quot;&gt;Shivansh Rastogi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonawane_S/0/1/0/all/0/1&quot;&gt;Shriraj Sonawane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03320">
<title>CoNeS: Conditional neural fields with shift modulation for multi-sequence MRI translation. (arXiv:2309.03320v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.03320</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-sequence magnetic resonance imaging (MRI) has found wide applications
in both modern clinical studies and deep learning research. However, in
clinical practice, it frequently occurs that one or more of the MRI sequences
are missing due to different image acquisition protocols or contrast agent
contraindications of patients, limiting the utilization of deep learning models
trained on multi-sequence data. One promising approach is to leverage
generative models to synthesize the missing sequences, which can serve as a
surrogate acquisition. State-of-the-art methods tackling this problem are based
on convolutional neural networks (CNN) which usually suffer from spectral
biases, resulting in poor reconstruction of high-frequency fine details. In
this paper, we propose Conditional Neural fields with Shift modulation (CoNeS),
a model that takes voxel coordinates as input and learns a representation of
the target images for multi-sequence MRI translation. The proposed model uses a
multi-layer perceptron (MLP) instead of a CNN as the decoder for pixel-to-pixel
mapping. Hence, each target image is represented as a neural field that is
conditioned on the source image via shift modulation with a learned latent
code. Experiments on BraTS 2018 and an in-house clinical dataset of vestibular
schwannoma patients showed that the proposed method outperformed
state-of-the-art methods for multi-sequence MRI translation both visually and
quantitatively. Moreover, we conducted spectral analysis, showing that CoNeS
was able to overcome the spectral bias issue common in conventional CNN models.
To further evaluate the usage of synthesized images in clinical downstream
tasks, we tested a segmentation network using the synthesized images at
inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yunjie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Staring_M/0/1/0/all/0/1&quot;&gt;Marius Staring&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Neve_O/0/1/0/all/0/1&quot;&gt;Olaf M. Neve&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Romeijn_S/0/1/0/all/0/1&quot;&gt;Stephan R. Romeijn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hensen_E/0/1/0/all/0/1&quot;&gt;Erik F. Hensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Verbist_B/0/1/0/all/0/1&quot;&gt;Berit M. Verbist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wolterink_J/0/1/0/all/0/1&quot;&gt;Jelmer M. Wolterink&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tao_Q/0/1/0/all/0/1&quot;&gt;Qian Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03329">
<title>MEGANet: Multi-Scale Edge-Guided Attention Network for Weak Boundary Polyp Segmentation. (arXiv:2309.03329v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03329</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient polyp segmentation in healthcare plays a critical role in enabling
early diagnosis of colorectal cancer. However, the segmentation of polyps
presents numerous challenges, including the intricate distribution of
backgrounds, variations in polyp sizes and shapes, and indistinct boundaries.
Defining the boundary between the foreground (i.e. polyp itself) and the
background (surrounding tissue) is difficult. To mitigate these challenges, we
propose Multi-Scale Edge-Guided Attention Network (MEGANet) tailored
specifically for polyp segmentation within colonoscopy images. This network
draws inspiration from the fusion of a classical edge detection technique with
an attention mechanism. By combining these techniques, MEGANet effectively
preserves high-frequency information, notably edges and boundaries, which tend
to erode as neural networks deepen. MEGANet is designed as an end-to-end
framework, encompassing three key modules: an encoder, which is responsible for
capturing and abstracting the features from the input image, a decoder, which
focuses on salient features, and the Edge-Guided Attention module (EGA) that
employs the Laplacian Operator to accentuate polyp boundaries. Extensive
experiments, both qualitative and quantitative, on five benchmark datasets,
demonstrate that our EGANet outperforms other existing SOTA methods under six
evaluation metrics. Our code is available at
\url{https://github.com/DinhHieuHoang/MEGANet}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bui_N/0/1/0/all/0/1&quot;&gt;Nhat-Tan Bui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_D/0/1/0/all/0/1&quot;&gt;Dinh-Hieu Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1&quot;&gt;Quang-Thuc Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1&quot;&gt;Minh-Triet Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngan Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03331">
<title>Expert Uncertainty and Severity Aware Chest X-Ray Classification by Multi-Relationship Graph Learning. (arXiv:2309.03331v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03331</link>
<description rdf:parseType="Literal">&lt;p&gt;Patients undergoing chest X-rays (CXR) often endure multiple lung diseases.
When evaluating a patient&apos;s condition, due to the complex pathologies, subtle
texture changes of different lung lesions in images, and patient condition
differences, radiologists may make uncertain even when they have experienced
long-term clinical training and professional guidance, which makes much noise
in extracting disease labels based on CXR reports. In this paper, we re-extract
disease labels from CXR reports to make them more realistic by considering
disease severity and uncertainty in classification. Our contributions are as
follows: 1. We re-extracted the disease labels with severity and uncertainty by
a rule-based approach with keywords discussed with clinical experts. 2. To
further improve the explainability of chest X-ray diagnosis, we designed a
multi-relationship graph learning method with an expert uncertainty-aware loss
function. 3. Our multi-relationship graph learning method can also interpret
the disease classification results. Our experimental results show that models
considering disease severity and uncertainty outperform previous
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengliang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xinyue Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1&quot;&gt;Lin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liangchen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobayashi_K/0/1/0/all/0/1&quot;&gt;Kazuma Kobayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1&quot;&gt;Tatsuya Harada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Summers_R/0/1/0/all/0/1&quot;&gt;Ronald M. Summers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yingying Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03335">
<title>SADIR: Shape-Aware Diffusion Models for 3D Image Reconstruction. (arXiv:2309.03335v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03335</link>
<description rdf:parseType="Literal">&lt;p&gt;3D image reconstruction from a limited number of 2D images has been a
long-standing challenge in computer vision and image analysis. While deep
learning-based approaches have achieved impressive performance in this area,
existing deep networks often fail to effectively utilize the shape structures
of objects presented in images. As a result, the topology of reconstructed
objects may not be well preserved, leading to the presence of artifacts such as
discontinuities, holes, or mismatched connections between different parts. In
this paper, we propose a shape-aware network based on diffusion models for 3D
image reconstruction, named SADIR, to address these issues. In contrast to
previous methods that primarily rely on spatial correlations of image
intensities for 3D reconstruction, our model leverages shape priors learned
from the training data to guide the reconstruction process. To achieve this, we
develop a joint learning network that simultaneously learns a mean shape under
deformation models. Each reconstructed image is then considered as a deformed
variant of the mean shape. We validate our model, SADIR, on both brain and
cardiac magnetic resonance images (MRIs). Experimental results show that our
method outperforms the baselines with lower reconstruction error and better
preservation of the shape structure of objects within the images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayakumar_N/0/1/0/all/0/1&quot;&gt;Nivetha Jayakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hossain_T/0/1/0/all/0/1&quot;&gt;Tonmoy Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Miaomiao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03350">
<title>Relay Diffusion: Unifying diffusion process across resolutions for image synthesis. (arXiv:2309.03350v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03350</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models achieved great success in image synthesis, but still face
challenges in high-resolution generation. Through the lens of discrete cosine
transformation, we find the main reason is that \emph{the same noise level on a
higher resolution results in a higher Signal-to-Noise Ratio in the frequency
domain}. In this work, we present Relay Diffusion Model (RDM), which transfers
a low-resolution image or noise into an equivalent high-resolution one for
diffusion model via blurring diffusion and block noise. Therefore, the
diffusion process can continue seamlessly in any new resolution or model
without restarting from pure noise or low-resolution conditioning. RDM achieves
state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256$\times$256,
surpassing previous works such as ADM, LDM and DiT by a large margin. All the
codes and checkpoints are open-sourced at
\url{https://github.com/THUDM/RelayDiffusion}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_J/0/1/0/all/0/1&quot;&gt;Jiayan Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wendi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Ming Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1&quot;&gt;Wenyi Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wangni_J/0/1/0/all/0/1&quot;&gt;Jianqiao Wangni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhuoyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jie Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03351">
<title>Using Neural Networks for Fast SAR Roughness Estimation of High Resolution Images. (arXiv:2309.03351v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03351</link>
<description rdf:parseType="Literal">&lt;p&gt;The analysis of Synthetic Aperture Radar (SAR) imagery is an important step
in remote sensing applications, and it is a challenging problem due to its
inherent speckle noise. One typical solution is to model the data using the
$G_I^0$ distribution and extract its roughness information, which in turn can
be used in posterior imaging tasks, such as segmentation, classification and
interpretation. This leads to the need of quick and reliable estimation of the
roughness parameter from SAR data, especially with high resolution images.
Unfortunately, traditional parameter estimation procedures are slow and prone
to estimation failures. In this work, we proposed a neural network-based
estimation framework that first learns how to predict underlying parameters of
$G_I^0$ samples and then can be used to estimate the roughness of unseen data.
We show that this approach leads to an estimator that is quicker, yields less
estimation error and is less prone to failures than the traditional estimation
procedures for this problem, even when we use a simple network. More
importantly, we show that this same methodology can be generalized to handle
image inputs and, even if trained on purely synthetic data for a few seconds,
is able to perform real time pixel-wise roughness estimation for high
resolution real SAR imagery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Li Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neto_J/0/1/0/all/0/1&quot;&gt;Jeova Farias Sales Rocha Neto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03353">
<title>Source Camera Identification and Detection in Digital Videos through Blind Forensics. (arXiv:2309.03353v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03353</link>
<description rdf:parseType="Literal">&lt;p&gt;Source camera identification in digital videos is the problem of associating
an unknown digital video with its source device, within a closed set of
possible devices. The existing techniques in source detection of digital videos
try to find a fingerprint of the actual source in the video in form of PRNU
(Photo Response Non--Uniformity), and match it against the SPN (Sensor Pattern
Noise) of each possible device. The highest correlation indicates the correct
source. We investigate the problem of identifying a video source through a
feature based approach using machine learning. In this paper, we present a
blind forensic technique of video source authentication and identification,
based on feature extraction, feature selection and subsequent source
classification. The main aim is to determine whether a claimed source for a
video is actually its original source. If not, we identify its original source.
Our experimental results prove the efficiency of the proposed method compared
to traditional fingerprint based technique.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sameer_V/0/1/0/all/0/1&quot;&gt;Venkata Udaya Sameer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukhopadhyay_S/0/1/0/all/0/1&quot;&gt;Shilpa Mukhopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naskar_R/0/1/0/all/0/1&quot;&gt;Ruchira Naskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dali_I/0/1/0/all/0/1&quot;&gt;Ishaan Dali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03360">
<title>ViewMix: Augmentation for Robust Representation in Self-Supervised Learning. (arXiv:2309.03360v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03360</link>
<description rdf:parseType="Literal">&lt;p&gt;Joint Embedding Architecture-based self-supervised learning methods have
attributed the composition of data augmentations as a crucial factor for their
strong representation learning capabilities. While regional dropout strategies
have proven to guide models to focus on lesser indicative parts of the objects
in supervised methods, it hasn&apos;t been adopted by self-supervised methods for
generating positive pairs. This is because the regional dropout methods are not
suitable for the input sampling process of the self-supervised methodology.
Whereas dropping informative pixels from the positive pairs can result in
inefficient training, replacing patches of a specific object with a different
one can steer the model from maximizing the agreement between different
positive pairs. Moreover, joint embedding representation learning methods have
not made robustness their primary training outcome. To this end, we propose the
ViewMix augmentation policy, specially designed for self-supervised learning,
upon generating different views of the same image, patches are cut and pasted
from one view to another. By leveraging the different views created by this
augmentation strategy, multiple joint embedding-based self-supervised
methodologies obtained better localization capability and consistently
outperformed their corresponding baseline methods. It is also demonstrated that
incorporating ViewMix augmentation policy promotes robustness of the
representations in the state-of-the-art methods. Furthermore, our
experimentation and analysis of compute times suggest that ViewMix augmentation
doesn&apos;t introduce any additional overhead compared to other counterparts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Arjon Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1&quot;&gt;Xin Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03367">
<title>Self-Supervised Masked Digital Elevation Models Encoding for Low-Resource Downstream Tasks. (arXiv:2309.03367v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03367</link>
<description rdf:parseType="Literal">&lt;p&gt;The lack of quality labeled data is one of the main bottlenecks for training
Deep Learning models. As the task increases in complexity, there is a higher
penalty for overfitting and unstable learning. The typical paradigm employed
today is Self-Supervised learning, where the model attempts to learn from a
large corpus of unstructured and unlabeled data and then transfer that
knowledge to the required task. Some notable examples of self-supervision in
other modalities are BERT for Large Language Models, Wav2Vec for Speech
Recognition, and the Masked AutoEncoder for Vision, which all utilize
Transformers to solve a masked prediction task. GeoAI is uniquely poised to
take advantage of the self-supervised methodology due to the decades of data
collected, little of which is precisely and dependably annotated. Our goal is
to extract building and road segmentations from Digital Elevation Models (DEM)
that provide a detailed topography of the earths surface. The proposed
architecture is the Masked Autoencoder pre-trained on ImageNet (with the
limitation that there is a large domain discrepancy between ImageNet and DEM)
with an UperNet Head for decoding segmentations. We tested this model with 450
and 50 training images only, utilizing roughly 5% and 0.5% of the original data
respectively. On the building segmentation task, this model obtains an 82.1%
Intersection over Union (IoU) with 450 Images and 69.1% IoU with only 50
images. On the more challenging road detection task the model obtains an 82.7%
IoU with 450 images and 73.2% IoU with only 50 images. Any hand-labeled dataset
made today about the earths surface will be immediately obsolete due to the
constantly changing nature of the landscape. This motivates the clear necessity
for data-efficient learners that can be used for a wide variety of downstream
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazumdar_P/0/1/0/all/0/1&quot;&gt;Priyam Mazumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soliman_A/0/1/0/all/0/1&quot;&gt;Aiman Soliman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kindratenko_V/0/1/0/all/0/1&quot;&gt;Volodymyr Kindratenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marini_L/0/1/0/all/0/1&quot;&gt;Luigi Marini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McHenry_K/0/1/0/all/0/1&quot;&gt;Kenton McHenry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03381">
<title>Active shooter detection and robust tracking utilizing supplemental synthetic data. (arXiv:2309.03381v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03381</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing concern surrounding gun violence in the United States has led
to a focus on developing systems to improve public safety. One approach to
developing such a system is to detect and track shooters, which would help
prevent or mitigate the impact of violent incidents. In this paper, we proposed
detecting shooters as a whole, rather than just guns, which would allow for
improved tracking robustness, as obscuring the gun would no longer cause the
system to lose sight of the threat. However, publicly available data on
shooters is much more limited and challenging to create than a gun dataset
alone. Therefore, we explore the use of domain randomization and transfer
learning to improve the effectiveness of training with synthetic data obtained
from Unreal Engine environments. This enables the model to be trained on a
wider range of data, increasing its ability to generalize to different
situations. Using these techniques with YOLOv8 and Deep OC-SORT, we implemented
an initial version of a shooter tracking system capable of running on edge
hardware, including both a Raspberry Pi and a Jetson Nano.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waite_J/0/1/0/all/0/1&quot;&gt;Joshua R. Waite&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiale Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tavassoli_R/0/1/0/all/0/1&quot;&gt;Riley Tavassoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harris_L/0/1/0/all/0/1&quot;&gt;Laura Harris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1&quot;&gt;Sin Yong Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1&quot;&gt;Subhadeep Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Soumik Sarkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03383">
<title>Kidney abnormality segmentation in thorax-abdomen CT scans. (arXiv:2309.03383v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.03383</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we introduce a deep learning approach for segmenting kidney
parenchyma and kidney abnormalities to support clinicians in identifying and
quantifying renal abnormalities such as cysts, lesions, masses, metastases, and
primary tumors. Our end-to-end segmentation method was trained on 215
contrast-enhanced thoracic-abdominal CT scans, with half of these scans
containing one or more abnormalities.
&lt;/p&gt;
&lt;p&gt;We began by implementing our own version of the original 3D U-Net network and
incorporated four additional components: an end-to-end multi-resolution
approach, a set of task-specific data augmentations, a modified loss function
using top-$k$, and spatial dropout. Furthermore, we devised a tailored
post-processing strategy. Ablation studies demonstrated that each of the four
modifications enhanced kidney abnormality segmentation performance, while three
out of four improved kidney parenchyma segmentation. Subsequently, we trained
the nnUNet framework on our dataset. By ensembling the optimized 3D U-Net and
the nnUNet with our specialized post-processing, we achieved marginally
superior results.
&lt;/p&gt;
&lt;p&gt;Our best-performing model attained Dice scores of 0.965 and 0.947 for
segmenting kidney parenchyma in two test sets (20 scans without abnormalities
and 30 with abnormalities), outperforming an independent human observer who
scored 0.944 and 0.925, respectively. In segmenting kidney abnormalities within
the 30 test scans containing them, the top-performing method achieved a Dice
score of 0.585, while an independent second human observer reached a score of
0.664, suggesting potential for further improvement in computerized methods.
&lt;/p&gt;
&lt;p&gt;All training data is available to the research community under a CC-BY 4.0
license on https://doi.org/10.5281/zenodo.8014289
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mamani_G/0/1/0/all/0/1&quot;&gt;Gabriel Efrain Humpire Mamani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lessmann_N/0/1/0/all/0/1&quot;&gt;Nikolas Lessmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Scholten_E/0/1/0/all/0/1&quot;&gt;Ernst Th. Scholten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prokop_M/0/1/0/all/0/1&quot;&gt;Mathias Prokop&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jacobs_C/0/1/0/all/0/1&quot;&gt;Colin Jacobs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ginneken_B/0/1/0/all/0/1&quot;&gt;Bram van Ginneken&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03390">
<title>A novel method for iris recognition using BP neural network and parallel computing by the aid of GPUs (Graphics Processing Units). (arXiv:2309.03390v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03390</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we seek a new method in designing an iris recognition system.
In this method, first the Haar wavelet features are extracted from iris images.
The advantage of using these features is the high-speed extraction, as well as
being unique to each iris. Then the back propagation neural network (BPNN) is
used as a classifier. In this system, the BPNN parallel algorithms and their
implementation on GPUs have been used by the aid of CUDA in order to speed up
the learning process. Finally, the system performance and the speeding outcomes
in a way that this algorithm is done in series are presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosseini_F/0/1/0/all/0/1&quot;&gt;Farahnaz Hosseini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebrahimpour_H/0/1/0/all/0/1&quot;&gt;Hossein Ebrahimpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Askari_S/0/1/0/all/0/1&quot;&gt;Samaneh Askari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03401">
<title>Reasonable Anomaly Detection in Long Sequences. (arXiv:2309.03401v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03401</link>
<description rdf:parseType="Literal">&lt;p&gt;Video anomaly detection is a challenging task due to the lack in approaches
for representing samples. The visual representations of most existing
approaches are limited by short-term sequences of observations which cannot
provide enough clues for achieving reasonable detections. In this paper, we
propose to completely represent the motion patterns of objects by learning from
long-term sequences. Firstly, a Stacked State Machine (SSM) model is proposed
to represent the temporal dependencies which are consistent across long-range
observations. Then SSM model functions in predicting future states based on
past ones, the divergence between the predictions with inherent normal patterns
and observed ones determines anomalies which violate normal motion patterns.
Extensive experiments are carried out to evaluate the proposed approach on the
dataset and existing ones. Improvements over state-of-the-art methods can be
observed. Our code is available at
https://github.com/AllenYLJiang/Anomaly-Detection-in-Sequences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yalong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Changkang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03406">
<title>Distribution-Aware Prompt Tuning for Vision-Language Models. (arXiv:2309.03406v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03406</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained vision-language models (VLMs) have shown impressive performance
on various downstream tasks by utilizing knowledge learned from large data. In
general, the performance of VLMs on target tasks can be further improved by
prompt tuning, which adds context to the input image or text. By leveraging
data from target tasks, various prompt-tuning methods have been studied in the
literature. A key to prompt tuning is the feature space alignment between two
modalities via learnable vectors with model parameters fixed. We observed that
the alignment becomes more effective when embeddings of each modality are
`well-arranged&apos; in the latent space. Inspired by this observation, we proposed
distribution-aware prompt tuning (DAPT) for vision-language models, which is
simple yet effective. Specifically, the prompts are learned by maximizing
inter-dispersion, the distance between classes, as well as minimizing the
intra-dispersion measured by the distance between embeddings from the same
class. Our extensive experiments on 11 benchmark datasets demonstrate that our
method significantly improves generalizability. The code is available at
https://github.com/mlvlab/DAPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_E/0/1/0/all/0/1&quot;&gt;Eulrang Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jooyeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyunwoo J. Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03440">
<title>Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning. (arXiv:2309.03440v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.03440</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate segmentation of punctate white matter lesions (PWMLs) are
fundamental for the timely diagnosis and treatment of related developmental
disorders. Automated PWMLs segmentation from infant brain MR images is
challenging, considering that the lesions are typically small and low-contrast,
and the number of lesions may dramatically change across subjects. Existing
learning-based methods directly apply general network architectures to this
challenging task, which may fail to capture detailed positional information of
PWMLs, potentially leading to severe under-segmentations. In this paper, we
propose to leverage the idea of counterfactual reasoning coupled with the
auxiliary task of brain tissue segmentation to learn fine-grained positional
and morphological representations of PWMLs for accurate localization and
segmentation. A simple and easy-to-implement deep-learning framework (i.e.,
DeepPWML) is accordingly designed. It combines the lesion counterfactual map
with the tissue probability map to train a lightweight PWML segmentation
network, demonstrating state-of-the-art performance on a real-clinical dataset
of infant T1w MR images. The code is available at
\href{https://github.com/ladderlab-xjtu/DeepPWML}{https://github.com/ladderlab-xjtu/DeepPWML}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zehua Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yongheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Miaomiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yuying Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xianjun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Chao Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lian_C/0/1/0/all/0/1&quot;&gt;Chunfeng Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03445">
<title>Underwater Image Enhancement by Transformer-based Diffusion Model with Non-uniform Sampling for Skip Strategy. (arXiv:2309.03445v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03445</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present an approach to image enhancement with diffusion
model in underwater scenes. Our method adapts conditional denoising diffusion
probabilistic models to generate the corresponding enhanced images by using the
underwater images and the Gaussian noise as the inputs. Additionally, in order
to improve the efficiency of the reverse process in the diffusion model, we
adopt two different ways. We firstly propose a lightweight transformer-based
denoising network, which can effectively promote the time of network forward
per iteration. On the other hand, we introduce a skip sampling strategy to
reduce the number of iterations. Besides, based on the skip sampling strategy,
we propose two different non-uniform sampling methods for the sequence of the
time step, namely piecewise sampling and searching with the evolutionary
algorithm. Both of them are effective and can further improve performance by
using the same steps against the previous uniform sampling. In the end, we
conduct a relative evaluation of the widely used underwater enhancement
datasets between the recent state-of-the-art methods and the proposed approach.
The experimental results prove that our approach can achieve both competitive
performance and high efficiency. Our code is available at
\href{mailto:https://github.com/piggy2009/DM_underwater}{\color{blue}{https://github.com/piggy2009/DM\_underwater}}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iwaguchi_T/0/1/0/all/0/1&quot;&gt;Takafumi Iwaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawasaki_H/0/1/0/all/0/1&quot;&gt;Hiroshi Kawasaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03452">
<title>Multi-Modality Guidance Network For Missing Modality Inference. (arXiv:2309.03452v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03452</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal models have gained significant success in recent years. Standard
multimodal approaches often assume unchanged modalities from training stage to
inference stage. In practice, however, many scenarios fail to satisfy such
assumptions with missing modalities during inference, leading to limitations on
where multimodal models can be applied. While existing methods mitigate the
problem through reconstructing the missing modalities, it increases unnecessary
computational cost, which could be just as critical, especially for large,
deployed systems. To solve the problem from both sides, we propose a novel
guidance network that promotes knowledge sharing during training, taking
advantage of the multimodal representations to train better single-modality
models for inference. Real-life experiment in violence detection shows that our
proposed framework trains single-modality models that significantly outperform
its traditionally trained counterparts while maintaining the same inference
cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhuokai Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palani_H/0/1/0/all/0/1&quot;&gt;Harish Palani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_L/0/1/0/all/0/1&quot;&gt;Lena Evans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toner_R/0/1/0/all/0/1&quot;&gt;Ruth Toner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03453">
<title>SyncDreamer: Generating Multiview-consistent Images from a Single-view Image. (arXiv:2309.03453v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03453</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a novel diffusion model called that generates
multiview-consistent images from a single-view image. Using pretrained
large-scale 2D diffusion models, recent work Zero123 demonstrates the ability
to generate plausible novel views from a single-view image of an object.
However, maintaining consistency in geometry and colors for the generated
images remains a challenge. To address this issue, we propose a synchronized
multiview diffusion model that models the joint probability distribution of
multiview images, enabling the generation of multiview-consistent images in a
single reverse process. SyncDreamer synchronizes the intermediate states of all
the generated images at every step of the reverse process through a 3D-aware
feature attention mechanism that correlates the corresponding features across
different views. Experiments show that SyncDreamer generates images with high
consistency across different views, thus making it well-suited for various 3D
generation tasks such as novel-view-synthesis, text-to-3D, and image-to-3D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Cheng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zijiao Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1&quot;&gt;Taku Komura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03467">
<title>Autoregressive Omni-Aware Outpainting for Open-Vocabulary 360-Degree Image Generation. (arXiv:2309.03467v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03467</link>
<description rdf:parseType="Literal">&lt;p&gt;A 360-degree (omni-directional) image provides an all-encompassing spherical
view of a scene. Recently, there has been an increasing interest in
synthesising 360-degree images from conventional narrow field of view (NFoV)
images captured by digital cameras and smartphones, for providing immersive
experiences in various scenarios such as virtual reality. Yet, existing methods
typically fall short in synthesizing intricate visual details or ensure the
generated images align consistently with user-provided prompts. In this study,
autoregressive omni-aware generative network (AOG-Net) is proposed for
360-degree image generation by out-painting an incomplete 360-degree image
progressively with NFoV and text guidances joinly or individually. This
autoregressive scheme not only allows for deriving finer-grained and
text-consistent patterns by dynamically generating and adjusting the process
but also offers users greater flexibility to edit their conditions throughout
the generation process. A global-local conditioning mechanism is devised to
comprehensively formulate the outpainting guidance in each autoregressive step.
Text guidances, omni-visual cues, NFoV inputs and omni-geometry are encoded and
further formulated with cross-attention based transformers into a global stream
and a local stream into a conditioned generative backbone model. As AOG-Net is
compatible to leverage large-scale models for the conditional encoder and the
generative prior, it enables the generation to use extensive open-vocabulary
text guidances. Comprehensive experiments on two commonly used 360-degree image
datasets for both indoor and outdoor settings demonstrate the state-of-the-art
performance of our proposed method. Our code will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhuqiang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1&quot;&gt;Kun Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaoyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Lei Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03468">
<title>Cross-Image Context Matters for Bongard Problems. (arXiv:2309.03468v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03468</link>
<description rdf:parseType="Literal">&lt;p&gt;Current machine learning methods struggle to solve Bongard problems, which
are a type of IQ test that requires deriving an abstract &quot;concept&quot; from a set
of positive and negative &quot;support&quot; images, and then classifying whether or not
a new query image depicts the key concept. On Bongard-HOI, a benchmark for
natural-image Bongard problems, existing methods have only reached 66% accuracy
(where chance is 50%). Low accuracy is often attributed to neural nets&apos; lack of
ability to find human-like symbolic rules. In this work, we point out that many
existing methods are forfeiting accuracy due to a much simpler problem: they do
not incorporate information contained in the support set as a whole, and rely
instead on information extracted from individual supports. This is a critical
issue, because unlike in few-shot learning tasks concerning object
classification, the &quot;key concept&quot; in a typical Bongard problem can only be
distinguished using multiple positives and multiple negatives. We explore a
variety of simple methods to take this cross-image context into account, and
demonstrate substantial gains over prior methods, leading to new
state-of-the-art performance on Bongard-LOGO (75.3%) and Bongard-HOI (72.45%)
and strong performance on the original Bongard problem set (60.84%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghuraman_N/0/1/0/all/0/1&quot;&gt;Nikhil Raghuraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1&quot;&gt;Adam W. Harley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1&quot;&gt;Leonidas Guibas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03469">
<title>Fast FixMatch: Faster Semi-Supervised Learning with Curriculum Batch Size. (arXiv:2309.03469v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03469</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in Semi-Supervised Learning (SSL) have almost entirely closed the
gap between SSL and Supervised Learning at a fraction of the number of labels.
However, recent performance improvements have often come \textit{at the cost of
significantly increased training computation}. To address this, we propose
Curriculum Batch Size (CBS), \textit{an unlabeled batch size curriculum which
exploits the natural training dynamics of deep neural networks.} A small
unlabeled batch size is used in the beginning of training and is gradually
increased to the end of training. A fixed curriculum is used regardless of
dataset, model or number of epochs, and reduced training computations is
demonstrated on all settings. We apply CBS, strong labeled augmentation,
Curriculum Pseudo Labeling (CPL) \citep{FlexMatch} to FixMatch \citep{FixMatch}
and term the new SSL algorithm Fast FixMatch. We perform an ablation study to
show that strong labeled augmentation and/or CPL do not significantly reduce
training computations, but, in synergy with CBS, they achieve optimal
performance. Fast FixMatch also achieves substantially higher data utilization
compared to previous state-of-the-art. Fast FixMatch achieves between
$2.1\times$ - $3.4\times$ reduced training computations on CIFAR-10 with all
but 40, 250 and 4000 labels removed, compared to vanilla FixMatch, while
attaining the same cited state-of-the-art error rate \citep{FixMatch}. Similar
results are achieved for CIFAR-100, SVHN and STL-10. Finally, Fast MixMatch
achieves between $2.6\times$ - $3.3\times$ reduced training computations in
federated SSL tasks and online/streaming learning SSL tasks, which further
demonstrate the generializbility of Fast MixMatch to different scenarios and
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;John Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dun_C/0/1/0/all/0/1&quot;&gt;Chen Dun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1&quot;&gt;Anastasios Kyrillidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03472">
<title>Perceptual Quality Assessment of 360$^\circ$ Images Based on Generative Scanpath Representation. (arXiv:2309.03472v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03472</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite substantial efforts dedicated to the design of heuristic models for
omnidirectional (i.e., 360$^\circ$) image quality assessment (OIQA), a
conspicuous gap remains due to the lack of consideration for the diversity of
viewing behaviors that leads to the varying perceptual quality of 360$^\circ$
images. Two critical aspects underline this oversight: the neglect of viewing
conditions that significantly sway user gaze patterns and the overreliance on a
single viewport sequence from the 360$^\circ$ image for quality inference. To
address these issues, we introduce a unique generative scanpath representation
(GSR) for effective quality inference of 360$^\circ$ images, which aggregates
varied perceptual experiences of multi-hypothesis users under a predefined
viewing condition. More specifically, given a viewing condition characterized
by the starting point of viewing and exploration time, a set of scanpaths
consisting of dynamic visual fixations can be produced using an apt scanpath
generator. Following this vein, we use the scanpaths to convert the 360$^\circ$
image into the unique GSR, which provides a global overview of gazed-focused
contents derived from scanpaths. As such, the quality inference of the
360$^\circ$ image is swiftly transformed to that of GSR. We then propose an
efficient OIQA computational framework by learning the quality maps of GSR.
Comprehensive experimental results validate that the predictions of the
proposed framework are highly consistent with human perception in the
spatiotemporal domain, especially in the challenging context of locally
distorted 360$^\circ$ images under varied viewing conditions. The code will be
released at https://github.com/xiangjieSui/GSR
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_X/0/1/0/all/0/1&quot;&gt;Xiangjie Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hanwei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuelin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yuming Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhou Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03473">
<title>Temporal Collection and Distribution for Referring Video Object Segmentation. (arXiv:2309.03473v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03473</link>
<description rdf:parseType="Literal">&lt;p&gt;Referring video object segmentation aims to segment a referent throughout a
video sequence according to a natural language expression. It requires aligning
the natural language expression with the objects&apos; motions and their dynamic
associations at the global video level but segmenting objects at the frame
level. To achieve this goal, we propose to simultaneously maintain a global
referent token and a sequence of object queries, where the former is
responsible for capturing video-level referent according to the language
expression, while the latter serves to better locate and segment objects with
each frame. Furthermore, to explicitly capture object motions and
spatial-temporal cross-modal reasoning over objects, we propose a novel
temporal collection-distribution mechanism for interacting between the global
referent token and object queries. Specifically, the temporal collection
mechanism collects global information for the referent token from object
queries to the temporal motions to the language expression. In turn, the
temporal distribution first distributes the referent token to the referent
sequence across all frames and then performs efficient cross-frame reasoning
between the referent sequence and object queries in every frame. Experimental
results show that our method outperforms state-of-the-art methods on all
benchmarks consistently and significantly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiajin Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1&quot;&gt;Ge Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Sibei Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03477">
<title>TSI-Net: A Timing Sequence Image Segmentation Network for Intracranial Artery Segmentation in Digital Subtraction Angiography. (arXiv:2309.03477v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.03477</link>
<description rdf:parseType="Literal">&lt;p&gt;Cerebrovascular disease is one of the major diseases facing the world today.
Automatic segmentation of intracranial artery (IA) in digital subtraction
angiography (DSA) sequences is an important step in the diagnosis of vascular
related diseases and in guiding neurointerventional procedures. While, a single
image can only show part of the IA within the contrast medium according to the
imaging principle of DSA technology. Therefore, 2D DSA segmentation methods are
unable to capture the complete IA information and treatment of cerebrovascular
diseases. We propose A timing sequence image segmentation network with U-shape,
called TSI-Net, which incorporates a bi-directional ConvGRU module (BCM) in the
encoder. The network incorporates a bi-directional ConvGRU module (BCM) in the
encoder, which can input variable-length DSA sequences, retain past and future
information, segment them into 2D images. In addition, we introduce a sensitive
detail branch (SDB) at the end for supervising fine vessels. Experimented on
the DSA sequence dataset DIAS, the method performs significantly better than
state-of-the-art networks in recent years. In particular, it achieves a Sen
evaluation metric of 0.797, which is a 3% improvement compared to other
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lemeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wentao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weijin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Huihua Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Feng Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03483">
<title>DetermiNet: A Large-Scale Diagnostic Dataset for Complex Visually-Grounded Referencing using Determiners. (arXiv:2309.03483v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03483</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art visual grounding models can achieve high detection accuracy,
but they are not designed to distinguish between all objects versus only
certain objects of interest. In natural language, in order to specify a
particular object or set of objects of interest, humans use determiners such as
&quot;my&quot;, &quot;either&quot; and &quot;those&quot;. Determiners, as an important word class, are a type
of schema in natural language about the reference or quantity of the noun.
Existing grounded referencing datasets place much less emphasis on determiners,
compared to other word classes such as nouns, verbs and adjectives. This makes
it difficult to develop models that understand the full variety and complexity
of object referencing. Thus, we have developed and released the DetermiNet
dataset , which comprises 250,000 synthetically generated images and captions
based on 25 determiners. The task is to predict bounding boxes to identify
objects of interest, constrained by the semantics of the given determiner. We
find that current state-of-the-art visual grounding models do not perform well
on the dataset, highlighting the limitations of existing models on reference
and quantification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Clarence Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1&quot;&gt;M Ganesh Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Cheston Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03493">
<title>SAM3D: Segment Anything Model in Volumetric Medical Images. (arXiv:2309.03493v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.03493</link>
<description rdf:parseType="Literal">&lt;p&gt;Image segmentation is a critical task in medical image analysis, providing
valuable information that helps to make an accurate diagnosis. In recent years,
deep learning-based automatic image segmentation methods have achieved
outstanding results in medical images. In this paper, inspired by the Segment
Anything Model (SAM), a foundation model that has received much attention for
its impressive accuracy and powerful generalization ability in 2D still image
segmentation, we propose a SAM3D that targets at 3D volumetric medical images
and utilizes the pre-trained features from the SAM encoder to capture
meaningful representations of input images. Different from other existing
SAM-based volumetric segmentation methods that perform the segmentation by
dividing the volume into a set of 2D slices, our model takes the whole 3D
volume image as input and processes it simply and effectively that avoids
training a significant number of parameters. Extensive experiments are
conducted on multiple medical image datasets to demonstrate that our network
attains competitive results compared with other state-of-the-art methods in 3D
medical segmentation tasks while being significantly efficient in terms of
parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bui_N/0/1/0/all/0/1&quot;&gt;Nhat-Tan Bui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hoang_D/0/1/0/all/0/1&quot;&gt;Dinh-Hieu Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tran_M/0/1/0/all/0/1&quot;&gt;Minh-Triet Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngan Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03494">
<title>Evaluating Deep Learning-based Melanoma Classification using Immunohistochemistry and Routine Histology: A Three Center Study. (arXiv:2309.03494v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.03494</link>
<description rdf:parseType="Literal">&lt;p&gt;Pathologists routinely use immunohistochemical (IHC)-stained tissue slides
against MelanA in addition to hematoxylin and eosin (H&amp;amp;E)-stained slides to
improve their accuracy in diagnosing melanomas. The use of diagnostic Deep
Learning (DL)-based support systems for automated examination of tissue
morphology and cellular composition has been well studied in standard
H&amp;amp;E-stained tissue slides. In contrast, there are few studies that analyze IHC
slides using DL. Therefore, we investigated the separate and joint performance
of ResNets trained on MelanA and corresponding H&amp;amp;E-stained slides. The MelanA
classifier achieved an area under receiver operating characteristics curve
(AUROC) of 0.82 and 0.74 on out of distribution (OOD)-datasets, similar to the
H&amp;amp;E-based benchmark classification of 0.81 and 0.75, respectively. A combined
classifier using MelanA and H&amp;amp;E achieved AUROCs of 0.85 and 0.81 on the OOD
datasets. DL MelanA-based assistance systems show the same performance as the
benchmark H&amp;amp;E classification and may be improved by multi stain classification
to assist pathologists in their clinical routine.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wies_C/0/1/0/all/0/1&quot;&gt;Christoph Wies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schneide_L/0/1/0/all/0/1&quot;&gt;Lucas Schneide&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Haggenmueller_S/0/1/0/all/0/1&quot;&gt;Sarah Haggenmueller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bucher_T/0/1/0/all/0/1&quot;&gt;Tabea-Clara Bucher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hobelsberger_S/0/1/0/all/0/1&quot;&gt;Sarah Hobelsberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Heppt_M/0/1/0/all/0/1&quot;&gt;Markus V. Heppt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ferrara_G/0/1/0/all/0/1&quot;&gt;Gerardo Ferrara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Krieghoff_Henning_E/0/1/0/all/0/1&quot;&gt;Eva I. Krieghoff-Henning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brinker_T/0/1/0/all/0/1&quot;&gt;Titus J. Brinker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03499">
<title>Instance Segmentation of Dislocations in TEM Images. (arXiv:2309.03499v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03499</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantitative Transmission Electron Microscopy (TEM) during in-situ straining
experiment is able to reveal the motion of dislocations -- linear defects in
the crystal lattice of metals. In the domain of materials science, the
knowledge about the location and movement of dislocations is important for
creating novel materials with superior properties. A long-standing problem,
however, is to identify the position and extract the shape of dislocations,
which would ultimately help to create a digital twin of such materials. In this
work, we quantitatively compare state-of-the-art instance segmentation methods,
including Mask R-CNN and YOLOv8. The dislocation masks as the results of the
instance segmentation are converted to mathematical lines, enabling
quantitative analysis of dislocation length and geometry -- important
information for the domain scientist, which we then propose to include as a
novel length-aware quality metric for estimating the network performance. Our
segmentation pipeline shows a high accuracy suitable for all domain-specific,
further post-processing. Additionally, our physics-based metric turns out to
perform much more consistently than typically used pixel-wise metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruzaeva_K/0/1/0/all/0/1&quot;&gt;Karina Ruzaeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Govind_K/0/1/0/all/0/1&quot;&gt;Kishan Govind&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Legros_M/0/1/0/all/0/1&quot;&gt;Marc Legros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sandfeld_S/0/1/0/all/0/1&quot;&gt;Stefan Sandfeld&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03504">
<title>Stroke-based Neural Painting and Stylization with Dynamically Predicted Painting Region. (arXiv:2309.03504v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03504</link>
<description rdf:parseType="Literal">&lt;p&gt;Stroke-based rendering aims to recreate an image with a set of strokes. Most
existing methods render complex images using an uniform-block-dividing
strategy, which leads to boundary inconsistency artifacts. To solve the
problem, we propose Compositional Neural Painter, a novel stroke-based
rendering framework which dynamically predicts the next painting region based
on the current canvas, instead of dividing the image plane uniformly into
painting regions. We start from an empty canvas and divide the painting process
into several steps. At each step, a compositor network trained with a phasic RL
strategy first predicts the next painting region, then a painter network
trained with a WGAN discriminator predicts stroke parameters, and a stroke
renderer paints the strokes onto the painting region of the current canvas.
Moreover, we extend our method to stroke-based style transfer with a novel
differentiable distance transform loss, which helps preserve the structure of
the input image during stroke-based stylization. Extensive experiments show our
model outperforms the existing models in both stroke-based neural painting and
stroke-based stylization. Code is available at
https://github.com/sjtuplayer/Compositional_Neural_Painter
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1&quot;&gt;Teng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1&quot;&gt;Ran Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Haokun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jinlong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yabiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lizhuang Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03506">
<title>Towards Robust Natural-Looking Mammography Lesion Synthesis on Ipsilateral Dual-Views Breast Cancer Analysis. (arXiv:2309.03506v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03506</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, many mammographic image analysis methods have been
introduced for improving cancer classification tasks. Two major issues of
mammogram classification tasks are leveraging multi-view mammographic
information and class-imbalance handling. In the first problem, many multi-view
methods have been released for concatenating features of two or more views for
the training and inference stage. Having said that, most multi-view existing
methods are not explainable in the meaning of feature fusion, and treat many
views equally for diagnosing. Our work aims to propose a simple but novel
method for enhancing examined view (main view) by leveraging low-level feature
information from the auxiliary view (ipsilateral view) before learning the
high-level feature that contains the cancerous features. For the second issue,
we also propose a simple but novel malignant mammogram synthesis framework for
upsampling minor class samples. Our easy-to-implement and no-training framework
has eliminated the current limitation of the CutMix algorithm which is
unreliable synthesized images with random pasted patches, hard-contour
problems, and domain shift problems. Our results on VinDr-Mammo and CMMD
datasets show the effectiveness of our two new frameworks for both multi-view
training and synthesizing mammographic images, outperforming the previous
conventional methods in our experimental settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thanh-Huy Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kha_Q/0/1/0/all/0/1&quot;&gt;Quang Hien Kha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1&quot;&gt;Thai Ngoc Toan Truong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_B/0/1/0/all/0/1&quot;&gt;Ba Thinh Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ngo_B/0/1/0/all/0/1&quot;&gt;Ba Hung Ngo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dinh_Q/0/1/0/all/0/1&quot;&gt;Quang Vinh Dinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Nguyen Quoc Khanh Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03508">
<title>Dynamic Frame Interpolation in Wavelet Domain. (arXiv:2309.03508v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03508</link>
<description rdf:parseType="Literal">&lt;p&gt;Video frame interpolation is an important low-level vision task, which can
increase frame rate for more fluent visual experience. Existing methods have
achieved great success by employing advanced motion models and synthesis
networks. However, the spatial redundancy when synthesizing the target frame
has not been fully explored, that can result in lots of inefficient
computation. On the other hand, the computation compression degree in frame
interpolation is highly dependent on both texture distribution and scene
motion, which demands to understand the spatial-temporal information of each
input frame pair for a better compression degree selection. In this work, we
propose a novel two-stage frame interpolation framework termed WaveletVFI to
address above problems. It first estimates intermediate optical flow with a
lightweight motion perception network, and then a wavelet synthesis network
uses flow aligned context features to predict multi-scale wavelet coefficients
with sparse convolution for efficient target frame reconstruction, where the
sparse valid masks that control computation in each scale are determined by a
crucial threshold ratio. Instead of setting a fixed value like previous
methods, we find that embedding a classifier in the motion perception network
to learn a dynamic threshold for each sample can achieve more computation
reduction with almost no loss of accuracy. On the common high resolution and
animation frame interpolation benchmarks, proposed WaveletVFI can reduce
computation up to 40% while maintaining similar accuracy, making it perform
more efficiently against other state-of-the-arts. Code is available at
https://github.com/ltkong218/WaveletVFI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1&quot;&gt;Lingtong Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Boyuan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1&quot;&gt;Donghao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1&quot;&gt;Wenqing Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1&quot;&gt;Ying Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03509">
<title>BroadCAM: Outcome-agnostic Class Activation Mapping for Small-scale Weakly Supervised Applications. (arXiv:2309.03509v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03509</link>
<description rdf:parseType="Literal">&lt;p&gt;Class activation mapping~(CAM), a visualization technique for interpreting
deep learning models, is now commonly used for weakly supervised semantic
segmentation~(WSSS) and object localization~(WSOL). It is the weighted
aggregation of the feature maps by activating the high class-relevance ones.
Current CAM methods achieve it relying on the training outcomes, such as
predicted scores~(forward information), gradients~(backward information), etc.
However, when with small-scale data, unstable training may lead to less
effective model outcomes and generate unreliable weights, finally resulting in
incorrect activation and noisy CAM seeds. In this paper, we propose an
outcome-agnostic CAM approach, called BroadCAM, for small-scale weakly
supervised applications. Since broad learning system (BLS) is independent to
the model learning, BroadCAM can avoid the weights being affected by the
unreliable model outcomes when with small-scale data. By evaluating BroadCAM on
VOC2012 (natural images) and BCSS-WSSS (medical images) for WSSS and
OpenImages30k for WSOL, BroadCAM demonstrates superior performance than
existing CAM methods with small-scale data (less than 5\%) in different CNN
architectures. It also achieves SOTA performance with large-scale training
data. Extensive qualitative comparisons are conducted to demonstrate how
BroadCAM activates the high class-relevance feature maps and generates reliable
CAMs when with small-scale training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jiatai Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1&quot;&gt;Guoqiang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xuemiao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Changhong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1&quot;&gt;Tien-Tsin Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;C. L. Philip Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zaiyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Chu Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03530">
<title>Efficient Single Object Detection on Image Patches with Early Exit Enhanced High-Precision CNNs. (arXiv:2309.03530v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03530</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel approach for detecting objects using mobile
robots in the context of the RoboCup Standard Platform League, with a primary
focus on detecting the ball. The challenge lies in detecting a dynamic object
in varying lighting conditions and blurred images caused by fast movements. To
address this challenge, the paper presents a convolutional neural network
architecture designed specifically for computationally constrained robotic
platforms. The proposed CNN is trained to achieve high precision classification
of single objects in image patches and to determine their precise spatial
positions. The paper further integrates Early Exits into the existing
high-precision CNN architecture to reduce the computational cost of easily
rejectable cases in the background class. The training process involves a
composite loss function based on confidence and positional losses with dynamic
weighting and data augmentation. The proposed approach achieves a precision of
100% on the validation dataset and a recall of almost 87%, while maintaining an
execution time of around 170 $\mu$s per hypotheses. By combining the proposed
approach with an Early Exit, a runtime optimization of more than 28%, on
average, can be achieved compared to the original CNN. Overall, this paper
provides an efficient solution for an enhanced detection of objects, especially
the ball, in computationally constrained robotic platforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moos_A/0/1/0/all/0/1&quot;&gt;Arne Moos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03531">
<title>A Robust Negative Learning Approach to Partial Domain Adaptation Using Source Prototypes. (arXiv:2309.03531v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03531</link>
<description rdf:parseType="Literal">&lt;p&gt;This work proposes a robust Partial Domain Adaptation (PDA) framework that
mitigates the negative transfer problem by incorporating a robust
target-supervision strategy. It leverages ensemble learning and includes
diverse, complementary label feedback, alleviating the effect of incorrect
feedback and promoting pseudo-label refinement. Rather than relying exclusively
on first-order moments for distribution alignment, our approach offers explicit
objectives to optimize intra-class compactness and inter-class separation with
the inferred source prototypes and highly-confident target samples in a
domain-invariant fashion. Notably, we ensure source data privacy by eliminating
the need to access the source data during the adaptation phase through a priori
inference of source prototypes. We conducted a series of comprehensive
experiments, including an ablation analysis, covering a range of partial domain
adaptation tasks. Comprehensive evaluations on benchmark datasets corroborate
our framework&apos;s enhanced robustness and generalization, demonstrating its
superiority over existing state-of-the-art PDA approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choudhuri_S/0/1/0/all/0/1&quot;&gt;Sandipan Choudhuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adeniye_S/0/1/0/all/0/1&quot;&gt;Suli Adeniye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_A/0/1/0/all/0/1&quot;&gt;Arunabha Sen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03535">
<title>Feature Enhancer Segmentation Network (FES-Net) for Vessel Segmentation. (arXiv:2309.03535v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.03535</link>
<description rdf:parseType="Literal">&lt;p&gt;Diseases such as diabetic retinopathy and age-related macular degeneration
pose a significant risk to vision, highlighting the importance of precise
segmentation of retinal vessels for the tracking and diagnosis of progression.
However, existing vessel segmentation methods that heavily rely on
encoder-decoder structures struggle to capture contextual information about
retinal vessel configurations, leading to challenges in reconciling semantic
disparities between encoder and decoder features. To address this, we propose a
novel feature enhancement segmentation network (FES-Net) that achieves accurate
pixel-wise segmentation without requiring additional image enhancement steps.
FES-Net directly processes the input image and utilizes four prompt
convolutional blocks (PCBs) during downsampling, complemented by a shallow
upsampling approach to generate a binary mask for each class. We evaluate the
performance of FES-Net on four publicly available state-of-the-art datasets:
DRIVE, STARE, CHASE, and HRF. The evaluation results clearly demonstrate the
superior performance of FES-Net compared to other competitive approaches
documented in the existing literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_T/0/1/0/all/0/1&quot;&gt;Tariq M. Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Arsalan_M/0/1/0/all/0/1&quot;&gt;Muhammad Arsalan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Iqbal_S/0/1/0/all/0/1&quot;&gt;Shahzaib Iqbal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Razzak_I/0/1/0/all/0/1&quot;&gt;Imran Razzak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meijering_E/0/1/0/all/0/1&quot;&gt;Erik Meijering&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03539">
<title>YOLO series target detection algorithms for underwater environments. (arXiv:2309.03539v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03539</link>
<description rdf:parseType="Literal">&lt;p&gt;You Only Look Once (YOLO) algorithm is a representative target detection
algorithm emerging in 2016, which is known for its balance of computing speed
and accuracy, and now plays an important role in various fields of human
production and life. However, there are still many limitations in the
application of YOLO algorithm in underwater environments due to problems such
as dim light and turbid water. With limited land area resources, the ocean must
have great potential for future human development. In this paper, starting from
the actual needs of marine engineering applications, taking underwater
structural health monitoring (SHM) and underwater biological detection as
examples, we propose improved methods for the application of underwater YOLO
algorithms, and point out the problems that still exist.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenjie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_P/0/1/0/all/0/1&quot;&gt;Pengcheng Jiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03542">
<title>Zero-Shot Scene Graph Generation via Triplet Calibration and Reduction. (arXiv:2309.03542v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03542</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene Graph Generation (SGG) plays a pivotal role in downstream
vision-language tasks. Existing SGG methods typically suffer from poor
compositional generalizations on unseen triplets. They are generally trained on
incompletely annotated scene graphs that contain dominant triplets and tend to
bias toward these seen triplets during inference. To address this issue, we
propose a Triplet Calibration and Reduction (T-CAR) framework in this paper. In
our framework, a triplet calibration loss is first presented to regularize the
representations of diverse triplets and to simultaneously excavate the unseen
triplets in incompletely annotated training scene graphs. Moreover, the unseen
space of scene graphs is usually several times larger than the seen space since
it contains a huge number of unrealistic compositions. Thus, we propose an
unseen space reduction loss to shift the attention of excavation to reasonable
unseen compositions to facilitate the model training. Finally, we propose a
contextual encoder to improve the compositional generalizations of unseen
triplets by explicitly modeling the relative spatial relations between subjects
and objects. Extensive experiments show that our approach achieves consistent
improvements for zero-shot SGG over state-of-the-art methods. The code is
available at https://github.com/jkli1998/T-CAR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiankai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weixin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03548">
<title>Trash to Treasure: Low-Light Object Detection via Decomposition-and-Aggregation. (arXiv:2309.03548v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03548</link>
<description rdf:parseType="Literal">&lt;p&gt;Object detection in low-light scenarios has attracted much attention in the
past few years. A mainstream and representative scheme introduces enhancers as
the pre-processing for regular detectors. However, because of the disparity in
task objectives between the enhancer and detector, this paradigm cannot shine
at its best ability. In this work, we try to arouse the potential of enhancer +
detector. Different from existing works, we extend the illumination-based
enhancers (our newly designed or existing) as a scene decomposition module,
whose removed illumination is exploited as the auxiliary in the detector for
extracting detection-friendly features. A semantic aggregation module is
further established for integrating multi-scale scene-related semantic
information in the context space. Actually, our built scheme successfully
transforms the &quot;trash&quot; (i.e., the ignored illumination in the detector) into
the &quot;treasure&quot; for the detector. Plenty of experiments are conducted to reveal
our superiority against other state-of-the-art methods. The code will be public
if it is accepted.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1&quot;&gt;Xiaohan Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Long Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tengyu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Risheng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03549">
<title>Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation. (arXiv:2309.03549v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03549</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by the remarkable success of Latent Diffusion Models (LDMs) for
image synthesis, we study LDM for text-to-video generation, which is a
formidable challenge due to the computational and memory constraints during
both model training and inference. A single LDM is usually only capable of
generating a very limited number of video frames. Some existing works focus on
separate prediction models for generating more video frames, which suffer from
additional training cost and frame-level jittering, however. In this paper, we
propose a framework called &quot;Reuse and Diffuse&quot; dubbed $\textit{VidRD}$ to
produce more frames following the frames already generated by an LDM.
Conditioned on an initial video clip with a small number of frames, additional
frames are iteratively generated by reusing the original latent features and
following the previous diffusion process. Besides, for the autoencoder used for
translation between pixel space and latent space, we inject temporal layers
into its decoder and fine-tune these layers for higher temporal consistency. We
also propose a set of strategies for composing video-text data that involve
diverse content from multiple existing datasets including video datasets for
action recognition and image-text datasets. Extensive experiments show that our
method achieves good results in both quantitative and qualitative evaluations.
Our project page is available
$\href{https://anonymous0x233.github.io/ReuseAndDiffuse/}{here}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiaxi Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shicong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haoyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Tianyi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zuxuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Songcen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Gang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03550">
<title>Text2Control3D: Controllable 3D Avatar Generation in Neural Radiance Fields using Geometry-Guided Text-to-Image Diffusion Model. (arXiv:2309.03550v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03550</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in diffusion models such as ControlNet have enabled
geometrically controllable, high-fidelity text-to-image generation. However,
none of them addresses the question of adding such controllability to
text-to-3D generation. In response, we propose Text2Control3D, a controllable
text-to-3D avatar generation method whose facial expression is controllable
given a monocular video casually captured with hand-held camera. Our main
strategy is to construct the 3D avatar in Neural Radiance Fields (NeRF)
optimized with a set of controlled viewpoint-aware images that we generate from
ControlNet, whose condition input is the depth map extracted from the input
video. When generating the viewpoint-aware images, we utilize cross-reference
attention to inject well-controlled, referential facial expression and
appearance via cross attention. We also conduct low-pass filtering of Gaussian
latent of the diffusion model in order to ameliorate the viewpoint-agnostic
texture problem we observed from our empirical analysis, where the
viewpoint-aware images contain identical textures on identical pixel positions
that are incomprehensible in 3D. Finally, to train NeRF with the images that
are viewpoint-aware yet are not strictly consistent in geometry, our approach
considers per-image geometric variation as a view of deformation from a shared
3D canonical space. Consequently, we construct the 3D avatar in a canonical
space of deformable NeRF by learning a set of per-image deformation via
deformation field table. We demonstrate the empirical results and discuss the
effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sungwon Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hyung_J/0/1/0/all/0/1&quot;&gt;Junha Hyung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1&quot;&gt;Jaegul Choo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03558">
<title>Region Generation and Assessment Network for Occluded Person Re-Identification. (arXiv:2309.03558v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03558</link>
<description rdf:parseType="Literal">&lt;p&gt;Person Re-identification (ReID) plays a more and more crucial role in recent
years with a wide range of applications. Existing ReID methods are suffering
from the challenges of misalignment and occlusions, which degrade the
performance dramatically. Most methods tackle such challenges by utilizing
external tools to locate body parts or exploiting matching strategies.
Nevertheless, the inevitable domain gap between the datasets utilized for
external tools and the ReID datasets and the complicated matching process make
these methods unreliable and sensitive to noises. In this paper, we propose a
Region Generation and Assessment Network (RGANet) to effectively and
efficiently detect the human body regions and highlight the important regions.
In the proposed RGANet, we first devise a Region Generation Module (RGM) which
utilizes the pre-trained CLIP to locate the human body regions using semantic
prototypes extracted from text descriptions. Learnable prompt is designed to
eliminate domain gap between CLIP datasets and ReID datasets. Then, to measure
the importance of each generated region, we introduce a Region Assessment
Module (RAM) that assigns confidence scores to different regions and reduces
the negative impact of the occlusion regions by lower scores. The RAM consists
of a discrimination-aware indicator and an invariance-aware indicator, where
the former indicates the capability to distinguish from different identities
and the latter represents consistency among the images of the same class of
human body regions. Extensive experimental results for six widely-used
benchmarks including three tasks (occluded, partial, and holistic) demonstrate
the superiority of RGANet against state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shuting He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weihua Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1&quot;&gt;Hao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Henghui Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03569">
<title>Sparse Federated Training of Object Detection in the Internet of Vehicles. (arXiv:2309.03569v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03569</link>
<description rdf:parseType="Literal">&lt;p&gt;As an essential component part of the Intelligent Transportation System
(ITS), the Internet of Vehicles (IoV) plays a vital role in alleviating traffic
issues. Object detection is one of the key technologies in the IoV, which has
been widely used to provide traffic management services by analyzing timely and
sensitive vehicle-related information. However, the current object detection
methods are mostly based on centralized deep training, that is, the sensitive
data obtained by edge devices need to be uploaded to the server, which raises
privacy concerns. To mitigate such privacy leakage, we first propose a
federated learning-based framework, where well-trained local models are shared
in the central server. However, since edge devices usually have limited
computing power, plus a strict requirement of low latency in IoVs, we further
propose a sparse training process on edge devices, which can effectively
lighten the model, and ensure its training efficiency on edge devices, thereby
reducing communication overheads. In addition, due to the diverse computing
capabilities and dynamic environment, different sparsity rates are applied to
edge devices. To further guarantee the performance, we propose, FedWeg, an
improved aggregation scheme based on FedAvg, which is designed by the inverse
ratio of sparsity rates. Experiments on the real-life dataset using YOLO show
that the proposed scheme can achieve the required object detection rate while
saving considerable communication costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_L/0/1/0/all/0/1&quot;&gt;Luping Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Ming Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yuwen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Lu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhe Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03575">
<title>Toward High Quality Facial Representation Learning. (arXiv:2309.03575v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03575</link>
<description rdf:parseType="Literal">&lt;p&gt;Face analysis tasks have a wide range of applications, but the universal
facial representation has only been explored in a few works. In this paper, we
explore high-performance pre-training methods to boost the face analysis tasks
such as face alignment and face parsing. We propose a self-supervised
pre-training framework, called \textbf{\it Mask Contrastive Face (MCF)}, with
mask image modeling and a contrastive strategy specially adjusted for face
domain tasks. To improve the facial representation quality, we use feature map
of a pre-trained visual backbone as a supervision item and use a partially
pre-trained decoder for mask image modeling. To handle the face identity during
the pre-training stage, we further use random masks to build contrastive
learning pairs. We conduct the pre-training on the LAION-FACE-cropped dataset,
a variants of LAION-FACE 20M, which contains more than 20 million face images
from Internet websites. For efficiency pre-training, we explore our framework
pre-training performance on a small part of LAION-FACE-cropped and verify the
superiority with different pre-training settings. Our model pre-trained with
the full pre-training dataset outperforms the state-of-the-art methods on
multiple downstream tasks. Our model achieves 0.932 NME$_{diag}$ for AFLW-19
face alignment and 93.96 F1 score for LaPa face parsing. Code is available at
https://github.com/nomewang/MCF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jinlong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiangning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1&quot;&gt;Ran Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yabiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengjie Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03576">
<title>DropPos: Pre-Training Vision Transformers by Reconstructing Dropped Positions. (arXiv:2309.03576v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03576</link>
<description rdf:parseType="Literal">&lt;p&gt;As it is empirically observed that Vision Transformers (ViTs) are quite
insensitive to the order of input tokens, the need for an appropriate
self-supervised pretext task that enhances the location awareness of ViTs is
becoming evident. To address this, we present DropPos, a novel pretext task
designed to reconstruct Dropped Positions. The formulation of DropPos is
simple: we first drop a large random subset of positional embeddings and then
the model classifies the actual position for each non-overlapping patch among
all possible positions solely based on their visual appearance. To avoid
trivial solutions, we increase the difficulty of this task by keeping only a
subset of patches visible. Additionally, considering there may be different
patches with similar visual appearances, we propose position smoothing and
attentive reconstruction strategies to relax this classification problem, since
it is not necessary to reconstruct their exact positions in these cases.
Empirical evaluations of DropPos show strong capabilities. DropPos outperforms
supervised pre-training and achieves competitive results compared with
state-of-the-art self-supervised alternatives on a wide range of downstream
benchmarks. This suggests that explicitly encouraging spatial reasoning
abilities, as DropPos does, indeed contributes to the improved location
awareness of ViTs. The code is publicly available at
https://github.com/Haochen-Wang409/DropPos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haochen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Junsong Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuxi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1&quot;&gt;Kaiyou Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhaoxiang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03590">
<title>Spatial encoding of BOLD fMRI time series for categorizing static images across visual datasets: A pilot study on human vision. (arXiv:2309.03590v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.03590</link>
<description rdf:parseType="Literal">&lt;p&gt;Functional MRI (fMRI) is widely used to examine brain functionality by
detecting alteration in oxygenated blood flow that arises with brain activity.
In this study, complexity specific image categorization across different visual
datasets is performed using fMRI time series (TS) to understand differences in
neuronal activities related to vision. Publicly available BOLD5000 dataset is
used for this purpose, containing fMRI scans while viewing 5254 images of
diverse categories, drawn from three standard computer vision datasets: COCO,
ImageNet and SUN. To understand vision, it is important to study how brain
functions while looking at different images. To achieve this, spatial encoding
of fMRI BOLD TS has been performed that uses classical Gramian Angular Field
(GAF) and Markov Transition Field (MTF) to obtain 2D BOLD TS, representing
images of COCO, Imagenet and SUN. For classification, individual GAF and MTF
features are fed into regular CNN. Subsequently, parallel CNN model is employed
that uses combined 2D features for classifying images across COCO, Imagenet and
SUN. The result of 2D CNN models is also compared with 1D LSTM and Bi-LSTM that
utilizes raw fMRI BOLD signal for classification. It is seen that parallel CNN
model outperforms other network models with an improvement of 7% for
multi-class classification. Clinical relevance- The obtained result of this
analysis establishes a baseline in studying how differently human brain
functions while looking at images of diverse complexities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kancharala_V/0/1/0/all/0/1&quot;&gt;Vamshi K. Kancharala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhattacharya_D/0/1/0/all/0/1&quot;&gt;Debanjali Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sinha_N/0/1/0/all/0/1&quot;&gt;Neelam Sinha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03598">
<title>Enhancing Sample Utilization through Sample Adaptive Augmentation in Semi-Supervised Learning. (arXiv:2309.03598v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03598</link>
<description rdf:parseType="Literal">&lt;p&gt;In semi-supervised learning, unlabeled samples can be utilized through
augmentation and consistency regularization. However, we observed certain
samples, even undergoing strong augmentation, are still correctly classified
with high confidence, resulting in a loss close to zero. It indicates that
these samples have been already learned well and do not provide any additional
optimization benefits to the model. We refer to these samples as ``naive
samples&quot;. Unfortunately, existing SSL models overlook the characteristics of
naive samples, and they just apply the same learning strategy to all samples.
To further optimize the SSL model, we emphasize the importance of giving
attention to naive samples and augmenting them in a more diverse manner. Sample
adaptive augmentation (SAA) is proposed for this stated purpose and consists of
two modules: 1) sample selection module; 2) sample augmentation module.
Specifically, the sample selection module picks out {naive samples} based on
historical training information at each epoch, then the naive samples will be
augmented in a more diverse manner in the sample augmentation module. Thanks to
the extreme ease of implementation of the above modules, SAA is advantageous
for being simple and lightweight. We add SAA on top of FixMatch and FlexMatch
respectively, and experiments demonstrate SAA can significantly improve the
models. For example, SAA helped improve the accuracy of FixMatch from 92.50% to
94.76% and that of FlexMatch from 95.01% to 95.31% on CIFAR-10 with 40 labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_G/0/1/0/all/0/1&quot;&gt;Guan Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1&quot;&gt;Lei Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Luping Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yinghuan Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03599">
<title>Chasing Consistency in Text-to-3D Generation from a Single Image. (arXiv:2309.03599v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03599</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-3D generation from a single-view image is a popular but challenging
task in 3D vision. Although numerous methods have been proposed, existing works
still suffer from the inconsistency issues, including 1) semantic
inconsistency, 2) geometric inconsistency, and 3) saturation inconsistency,
resulting in distorted, overfitted, and over-saturated generations. In light of
the above issues, we present Consist3D, a three-stage framework Chasing for
semantic-, geometric-, and saturation-Consistent Text-to-3D generation from a
single image, in which the first two stages aim to learn parameterized
consistency tokens, and the last stage is for optimization. Specifically, the
semantic encoding stage learns a token independent of views and estimations,
promoting semantic consistency and robustness. Meanwhile, the geometric
encoding stage learns another token with comprehensive geometry and
reconstruction constraints under novel-view estimations, reducing overfitting
and encouraging geometric consistency. Finally, the optimization stage benefits
from the semantic and geometric tokens, allowing a low classifier-free guidance
scale and therefore preventing oversaturation. Experimental results demonstrate
that Consist3D produces more consistent, faithful, and photo-realistic 3D
assets compared to previous state-of-the-art methods. Furthermore, Consist3D
also allows background and object editing through text prompts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1&quot;&gt;Yichen Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_W/0/1/0/all/0/1&quot;&gt;Wenhao Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jiayi Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dapeng Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1&quot;&gt;Yibing Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Gaoang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03640">
<title>Context-Aware 3D Object Localization from Single Calibrated Images: A Study of Basketballs. (arXiv:2309.03640v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03640</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately localizing objects in three dimensions (3D) is crucial for various
computer vision applications, such as robotics, autonomous driving, and
augmented reality. This task finds another important application in sports
analytics and, in this work, we present a novel method for 3D basketball
localization from a single calibrated image. Our approach predicts the object&apos;s
height in pixels in image space by estimating its projection onto the ground
plane within the image, leveraging the image itself and the object&apos;s location
as inputs. The 3D coordinates of the ball are then reconstructed by exploiting
the known projection matrix. Extensive experiments on the public DeepSport
dataset, which provides ground truth annotations for 3D ball location alongside
camera calibration information for each image, demonstrate the effectiveness of
our method, offering substantial accuracy improvements compared to recent work.
Our work opens up new possibilities for enhanced ball tracking and
understanding, advancing computer vision in diverse domains. The source code of
this work is made publicly available at
\url{https://github.com/gabriel-vanzandycke/deepsport}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caio_M/0/1/0/all/0/1&quot;&gt;Marcello Davide Caio&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zandycke_G/0/1/0/all/0/1&quot;&gt;Gabriel Van Zandycke&lt;/a&gt; (1 and 2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vleeschouwer_C/0/1/0/all/0/1&quot;&gt;Christophe De Vleeschouwer&lt;/a&gt; (2) ((1) Sportradar AG, (2) UCLouvain)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03641">
<title>Spiking Structured State Space Model for Monaural Speech Enhancement. (arXiv:2309.03641v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2309.03641</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech enhancement seeks to extract clean speech from noisy signals.
Traditional deep learning methods face two challenges: efficiently using
information in long speech sequences and high computational costs. To address
these, we introduce the Spiking Structured State Space Model (Spiking-S4). This
approach merges the energy efficiency of Spiking Neural Networks (SNN) with the
long-range sequence modeling capabilities of Structured State Space Models
(S4), offering a compelling solution. Evaluation on the DNS Challenge and
VoiceBank+Demand Datasets confirms that Spiking-S4 rivals existing Artificial
Neural Network (ANN) methods but with fewer computational resources, as
evidenced by reduced parameters and Floating Point Operations (FLOPs).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yu Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_Y/0/1/0/all/0/1&quot;&gt;Yansong Chua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03652">
<title>Anatomy-informed Data Augmentation for Enhanced Prostate Cancer Detection. (arXiv:2309.03652v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.03652</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation (DA) is a key factor in medical image analysis, such as in
prostate cancer (PCa) detection on magnetic resonance images. State-of-the-art
computer-aided diagnosis systems still rely on simplistic spatial
transformations to preserve the pathological label post transformation.
However, such augmentations do not substantially increase the organ as well as
tumor shape variability in the training set, limiting the model&apos;s ability to
generalize to unseen cases with more diverse localized soft-tissue
deformations. We propose a new anatomy-informed transformation that leverages
information from adjacent organs to simulate typical physiological deformations
of the prostate and generates unique lesion shapes without altering their
label. Due to its lightweight computational requirements, it can be easily
integrated into common DA frameworks. We demonstrate the effectiveness of our
augmentation on a dataset of 774 biopsy-confirmed examinations, by evaluating a
state-of-the-art method for PCa detection with different augmentation settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kovacs_B/0/1/0/all/0/1&quot;&gt;Balint Kovacs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Netzer_N/0/1/0/all/0/1&quot;&gt;Nils Netzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baumgartner_M/0/1/0/all/0/1&quot;&gt;Michael Baumgartner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eith_C/0/1/0/all/0/1&quot;&gt;Carolin Eith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bounias_D/0/1/0/all/0/1&quot;&gt;Dimitrios Bounias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meinzer_C/0/1/0/all/0/1&quot;&gt;Clara Meinzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jaeger_P/0/1/0/all/0/1&quot;&gt;Paul F. Jaeger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kevin S. Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Floca_R/0/1/0/all/0/1&quot;&gt;Ralf Floca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schrader_A/0/1/0/all/0/1&quot;&gt;Adrian Schrader&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Isensee_F/0/1/0/all/0/1&quot;&gt;Fabian Isensee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gnirs_R/0/1/0/all/0/1&quot;&gt;Regula Gnirs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Goertz_M/0/1/0/all/0/1&quot;&gt;Magdalena Goertz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schuetz_V/0/1/0/all/0/1&quot;&gt;Viktoria Schuetz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Stenzinger_A/0/1/0/all/0/1&quot;&gt;Albrecht Stenzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hohenfellner_M/0/1/0/all/0/1&quot;&gt;Markus Hohenfellner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schlemmer_H/0/1/0/all/0/1&quot;&gt;Heinz-Peter Schlemmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wolf_I/0/1/0/all/0/1&quot;&gt;Ivo Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bonekamp_D/0/1/0/all/0/1&quot;&gt;David Bonekamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maier_Hein_K/0/1/0/all/0/1&quot;&gt;Klaus H. Maier-Hein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03659">
<title>Towards Comparable Knowledge Distillation in Semantic Image Segmentation. (arXiv:2309.03659v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03659</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge Distillation (KD) is one proposed solution to large model sizes and
slow inference speed in semantic segmentation. In our research we identify 25
proposed distillation loss terms from 14 publications in the last 4 years.
Unfortunately, a comparison of terms based on published results is often
impossible, because of differences in training configurations. A good
illustration of this problem is the comparison of two publications from 2022.
Using the same models and dataset, Structural and Statistical Texture
Distillation (SSTKD) reports an increase of student mIoU of 4.54 and a final
performance of 29.19, while Adaptive Perspective Distillation (APD) only
improves student performance by 2.06 percentage points, but achieves a final
performance of 39.25. The reason for such extreme differences is often a
suboptimal choice of hyperparameters and a resulting underperformance of the
student model used as reference point. In our work, we reveal problems of
insufficient hyperparameter tuning by showing that distillation improvements of
two widely accepted frameworks, SKD and IFVD, vanish when hyperparameters are
optimized sufficiently. To improve comparability of future research in the
field, we establish a solid baseline for three datasets and two student models
and provide extensive information on hyperparameter tuning. We find that only
two out of eight techniques can compete with our simple baseline on the ADE20K
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niemann_O/0/1/0/all/0/1&quot;&gt;Onno Niemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vox_C/0/1/0/all/0/1&quot;&gt;Christopher Vox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werner_T/0/1/0/all/0/1&quot;&gt;Thorben Werner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03661">
<title>Prompt-based Context- and Domain-aware Pretraining for Vision and Language Navigation. (arXiv:2309.03661v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03661</link>
<description rdf:parseType="Literal">&lt;p&gt;With strong representation capabilities, pretrained vision-language models
are widely used in vision and language navigation (VLN). However, most of them
are trained on web-crawled general-purpose datasets, which incurs a
considerable domain gap when used for VLN tasks. Another challenge for VLN is
how the agent understands the contextual relations between actions on a
trajectory and performs cross-modal alignment sequentially. In this paper, we
propose a novel Prompt-bAsed coNtext- and Domain-Aware (PANDA) pretraining
framework to address these problems. It performs prompting in two stages. In
the domain-aware stage, we apply a low-cost prompt tuning paradigm to learn
soft visual prompts from an in-domain dataset for equipping the pretrained
models with object-level and scene-level cross-modal alignment in VLN tasks.
Furthermore, in the context-aware stage, we design a set of hard context
prompts to capture the sequence-level semantics and instill both out-of-context
and contextual knowledge in the instruction into cross-modal representations.
They enable further tuning of the pretrained models via contrastive learning.
Experimental results on both R2R and REVERIE show the superiority of PANDA
compared to previous state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wansen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yue Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Youkai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1&quot;&gt;Quanjun Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03671">
<title>Dataset Generation and Bonobo Classification from Weakly Labelled Videos. (arXiv:2309.03671v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03671</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a bonobo detection and classification pipeline built from
the commonly used machine learning methods. Such application is motivated by
the need to test bonobos in their enclosure using touch screen devices without
human assistance. This work introduces a newly acquired dataset based on bonobo
recordings generated semi-automatically. The recordings are weakly labelled and
fed to a macaque detector in order to spatially detect the individual present
in the video. Handcrafted features coupled with different classification
algorithms and deep-learning methods using a ResNet architecture are
investigated for bonobo identification. Performance is compared in terms of
classification accuracy on the splits of the database using different data
separation methods. We demonstrate the importance of data preparation and how a
wrong data separation can lead to false good results. Finally, after a
meaningful separation of the data, the best classification performance is
obtained using a fine-tuned ResNet model and reaches 75% of accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_P/0/1/0/all/0/1&quot;&gt;Pierre-Etienne Martin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03686">
<title>MS-UNet-v2: Adaptive Denoising Method and Training Strategy for Medical Image Segmentation with Small Training Data. (arXiv:2309.03686v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.03686</link>
<description rdf:parseType="Literal">&lt;p&gt;Models based on U-like structures have improved the performance of medical
image segmentation. However, the single-layer decoder structure of U-Net is too
&quot;thin&quot; to exploit enough information, resulting in large semantic differences
between the encoder and decoder parts. Things get worse if the number of
training sets of data is not sufficiently large, which is common in medical
image processing tasks where annotated data are more difficult to obtain than
other tasks. Based on this observation, we propose a novel U-Net model named
MS-UNet for the medical image segmentation task in this study. Instead of the
single-layer U-Net decoder structure used in Swin-UNet and TransUnet, we
specifically design a multi-scale nested decoder based on the Swin Transformer
for U-Net. The proposed multi-scale nested decoder structure allows the feature
mapping between the decoder and encoder to be semantically closer, thus
enabling the network to learn more detailed features. In addition, we propose a
novel edge loss and a plug-and-play fine-tuning Denoising module, which not
only effectively improves the segmentation performance of MS-UNet, but could
also be applied to other models individually. Experimental results show that
MS-UNet could effectively improve the network performance with more efficient
feature learning capability and exhibit more advanced performance, especially
in the extreme case with a small amount of training data, and the proposed Edge
loss and Denoising module could significantly enhance the segmentation
performance of MS-UNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haoyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yufei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Pin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jianping Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03696">
<title>Efficient Adaptive Human-Object Interaction Detection with Concept-guided Memory. (arXiv:2309.03696v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03696</link>
<description rdf:parseType="Literal">&lt;p&gt;Human Object Interaction (HOI) detection aims to localize and infer the
relationships between a human and an object. Arguably, training supervised
models for this task from scratch presents challenges due to the performance
drop over rare classes and the high computational cost and time required to
handle long-tailed distributions of HOIs in complex HOI scenes in realistic
settings. This observation motivates us to design an HOI detector that can be
trained even with long-tailed labeled data and can leverage existing knowledge
from pre-trained models. Inspired by the powerful generalization ability of the
large Vision-Language Models (VLM) on classification and retrieval tasks, we
propose an efficient Adaptive HOI Detector with Concept-guided Memory (ADA-CM).
ADA-CM has two operating modes. The first mode makes it tunable without
learning new parameters in a training-free paradigm. Its second mode
incorporates an instance-aware adapter mechanism that can further efficiently
boost performance if updating a lightweight set of parameters can be afforded.
Our proposed method achieves competitive results with state-of-the-art on the
HICO-DET and V-COCO datasets with much less training time. Code can be found at
https://github.com/ltttpku/ADA-CM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1&quot;&gt;Ting Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caba_F/0/1/0/all/0/1&quot;&gt;Fabian Caba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qingchao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Hailin Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yuxin Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03702">
<title>DiffDefense: Defending against Adversarial Attacks via Diffusion Models. (arXiv:2309.03702v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03702</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel reconstruction method that leverages Diffusion
Models to protect machine learning classifiers against adversarial attacks, all
without requiring any modifications to the classifiers themselves. The
susceptibility of machine learning models to minor input perturbations renders
them vulnerable to adversarial attacks. While diffusion-based methods are
typically disregarded for adversarial defense due to their slow reverse
process, this paper demonstrates that our proposed method offers robustness
against adversarial threats while preserving clean accuracy, speed, and
plug-and-play compatibility. Code at:
https://github.com/HondamunigePrasannaSilva/DiffDefence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_H/0/1/0/all/0/1&quot;&gt;Hondamunige Prasanna Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seidenari_L/0/1/0/all/0/1&quot;&gt;Lorenzo Seidenari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1&quot;&gt;Alberto Del Bimbo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03722">
<title>A boundary-aware point clustering approach in Euclidean and embedding spaces for roof plane segmentation. (arXiv:2309.03722v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03722</link>
<description rdf:parseType="Literal">&lt;p&gt;Roof plane segmentation from airborne LiDAR point clouds is an important
technology for 3D building model reconstruction. One of the key issues of plane
segmentation is how to design powerful features that can exactly distinguish
adjacent planar patches. The quality of point feature directly determines the
accuracy of roof plane segmentation. Most of existing approaches use
handcrafted features to extract roof planes. However, the abilities of these
features are relatively low, especially in boundary area. To solve this
problem, we propose a boundary-aware point clustering approach in Euclidean and
embedding spaces constructed by a multi-task deep network for roof plane
segmentation. We design a three-branch network to predict semantic labels,
point offsets and extract deep embedding features. In the first branch, we
classify the input data as non-roof, boundary and plane points. In the second
branch, we predict point offsets for shifting each point toward its respective
instance center. In the third branch, we constrain that points of the same
plane instance should have the similar embeddings. We aim to ensure that points
of the same plane instance are close as much as possible in both Euclidean and
embedding spaces. However, although deep network has strong feature
representative ability, it is still hard to accurately distinguish points near
plane instance boundary. Therefore, we first group plane points into many
clusters in the two spaces, and then we assign the rest boundary points to
their closest clusters to generate final complete roof planes. In this way, we
can effectively reduce the influence of unreliable boundary points. In
addition, we construct a synthetic dataset and a real dataset to train and
evaluate our approach. The experiments results show that the proposed approach
significantly outperforms the existing state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qingqing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1&quot;&gt;Guozheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pengwei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1&quot;&gt;Jingmin Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jian Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03726">
<title>Interpretable Visual Question Answering via Reasoning Supervision. (arXiv:2309.03726v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03726</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based architectures have recently demonstrated remarkable
performance in the Visual Question Answering (VQA) task. However, such models
are likely to disregard crucial visual cues and often rely on multimodal
shortcuts and inherent biases of the language modality to predict the correct
answer, a phenomenon commonly referred to as lack of visual grounding. In this
work, we alleviate this shortcoming through a novel architecture for visual
question answering that leverages common sense reasoning as a supervisory
signal. Reasoning supervision takes the form of a textual justification of the
correct answer, with such annotations being already available on large-scale
Visual Common Sense Reasoning (VCR) datasets. The model&apos;s visual attention is
guided toward important elements of the scene through a similarity loss that
aligns the learned attention distributions guided by the question and the
correct reasoning. We demonstrate both quantitatively and qualitatively that
the proposed approach can boost the model&apos;s visual perception capability and
lead to performance increase, without requiring training on explicit grounding
annotations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parelli_M/0/1/0/all/0/1&quot;&gt;Maria Parelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mallis_D/0/1/0/all/0/1&quot;&gt;Dimitrios Mallis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diomataris_M/0/1/0/all/0/1&quot;&gt;Markos Diomataris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pitsikalis_V/0/1/0/all/0/1&quot;&gt;Vassilis Pitsikalis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03729">
<title>Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption. (arXiv:2309.03729v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03729</link>
<description rdf:parseType="Literal">&lt;p&gt;Training a generative model with limited number of samples is a challenging
task. Current methods primarily rely on few-shot model adaption to train the
network. However, in scenarios where data is extremely limited (less than 10),
the generative network tends to overfit and suffers from content degradation.
To address these problems, we propose a novel phasic content fusing few-shot
diffusion model with directional distribution consistency loss, which targets
different learning objectives at distinct training stages of the diffusion
model. Specifically, we design a phasic training strategy with phasic content
fusion to help our model learn content and style information when t is large,
and learn local details of target domain when t is small, leading to an
improvement in the capture of content, style and local details. Furthermore, we
introduce a novel directional distribution consistency loss that ensures the
consistency between the generated and source distributions more efficiently and
stably than the prior methods, preventing our model from overfitting. Finally,
we propose a cross-domain structure guidance strategy that enhances structure
consistency during domain adaptation. Theoretical analysis, qualitative and
quantitative experiments demonstrate the superiority of our approach in
few-shot generative model adaption tasks compared to state-of-the-art methods.
The source code is available at:
https://github.com/sjtuplayer/few-shot-diffusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1&quot;&gt;Teng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiangning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1&quot;&gt;Ran Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kou_S/0/1/0/all/0/1&quot;&gt;Siqi Kou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Haokun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yabiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lizhuang Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03734">
<title>ClusterFusion: Leveraging Radar Spatial Features for Radar-Camera 3D Object Detection in Autonomous Vehicles. (arXiv:2309.03734v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03734</link>
<description rdf:parseType="Literal">&lt;p&gt;Thanks to the complementary nature of millimeter wave radar and camera, deep
learning-based radar-camera 3D object detection methods may reliably produce
accurate detections even in low-visibility conditions. This makes them
preferable to use in autonomous vehicles&apos; perception systems, especially as the
combined cost of both sensors is cheaper than the cost of a lidar. Recent
radar-camera methods commonly perform feature-level fusion which often involves
projecting the radar points onto the same plane as the image features and
fusing the extracted features from both modalities. While performing fusion on
the image plane is generally simpler and faster, projecting radar points onto
the image plane flattens the depth dimension of the point cloud which might
lead to information loss and makes extracting the spatial features of the point
cloud harder. We proposed ClusterFusion, an architecture that leverages the
local spatial features of the radar point cloud by clustering the point cloud
and performing feature extraction directly on the point cloud clusters before
projecting the features onto the image plane. ClusterFusion achieved the
state-of-the-art performance among all radar-monocular camera methods on the
test slice of the nuScenes dataset with 48.7% nuScenes detection score (NDS).
We also investigated the performance of different radar feature extraction
strategies on point cloud clusters: a handcrafted strategy, a learning-based
strategy, and a combination of both, and found that the handcrafted strategy
yielded the best performance. The main goal of this work is to explore the use
of radar&apos;s local spatial and point-wise features by extracting them directly
from radar point cloud clusters for a radar-monocular camera 3D object
detection method that performs cross-modal feature fusion on the image plane.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurniawan_I/0/1/0/all/0/1&quot;&gt;Irfan Tito Kurniawan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trilaksono_B/0/1/0/all/0/1&quot;&gt;Bambang Riyanto Trilaksono&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03744">
<title>Label-efficient Contrastive Learning-based model for nuclei detection and classification in 3D Cardiovascular Immunofluorescent Images. (arXiv:2309.03744v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.03744</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, deep learning-based methods achieved promising performance in
nuclei detection and classification applications. However, training deep
learning-based methods requires a large amount of pixel-wise annotated data,
which is time-consuming and labor-intensive, especially in 3D images. An
alternative approach is to adapt weak-annotation methods, such as labeling each
nucleus with a point, but this method does not extend from 2D histopathology
images (for which it was originally developed) to 3D immunofluorescent images.
The reason is that 3D images contain multiple channels (z-axis) for nuclei and
different markers separately, which makes training using point annotations
difficult. To address this challenge, we propose the Label-efficient
Contrastive learning-based (LECL) model to detect and classify various types of
nuclei in 3D immunofluorescent images. Previous methods use Maximum Intensity
Projection (MIP) to convert immunofluorescent images with multiple slices to 2D
images, which can cause signals from different z-stacks to falsely appear
associated with each other. To overcome this, we devised an Extended Maximum
Intensity Projection (EMIP) approach that addresses issues using MIP.
Furthermore, we performed a Supervised Contrastive Learning (SCL) approach for
weakly supervised settings. We conducted experiments on cardiovascular datasets
and found that our proposed framework is effective and efficient in detecting
and classifying various types of nuclei in 3D immunofluorescent images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moradinasab_N/0/1/0/all/0/1&quot;&gt;Nazanin Moradinasab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Deaton_R/0/1/0/all/0/1&quot;&gt;Rebecca A. Deaton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shankman_L/0/1/0/all/0/1&quot;&gt;Laura S. Shankman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Owens_G/0/1/0/all/0/1&quot;&gt;Gary K. Owens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brown_D/0/1/0/all/0/1&quot;&gt;Donald E. Brown&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03750">
<title>PBP: Path-based Trajectory Prediction for Autonomous Driving. (arXiv:2309.03750v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03750</link>
<description rdf:parseType="Literal">&lt;p&gt;Trajectory prediction plays a crucial role in the autonomous driving stack by
enabling autonomous vehicles to anticipate the motion of surrounding agents.
Goal-based prediction models have gained traction in recent years for
addressing the multimodal nature of future trajectories. Goal-based prediction
models simplify multimodal prediction by first predicting 2D goal locations of
agents and then predicting trajectories conditioned on each goal. However, a
single 2D goal location serves as a weak inductive bias for predicting the
whole trajectory, often leading to poor map compliance, i.e., part of the
trajectory going off-road or breaking traffic rules. In this paper, we improve
upon goal-based prediction by proposing the Path-based prediction (PBP)
approach. PBP predicts a discrete probability distribution over reference paths
in the HD map using the path features and predicts trajectories in the
path-relative Frenet frame. We applied the PBP trajectory decoder on top of the
HiVT scene encoder and report results on the Argoverse dataset. Our experiments
show that PBP achieves competitive performance on the standard trajectory
prediction metrics, while significantly outperforming state-of-the-art
baselines in terms of map compliance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afshar_S/0/1/0/all/0/1&quot;&gt;Sepideh Afshar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deo_N/0/1/0/all/0/1&quot;&gt;Nachiket Deo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhagat_A/0/1/0/all/0/1&quot;&gt;Akshay Bhagat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1&quot;&gt;Titas Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1&quot;&gt;Yunming Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buddharaju_B/0/1/0/all/0/1&quot;&gt;Balarama Raju Buddharaju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1&quot;&gt;Adwait Deshpande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1&quot;&gt;Henggang Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03759">
<title>M(otion)-mode Based Prediction of Ejection Fraction using Echocardiograms. (arXiv:2309.03759v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.03759</link>
<description rdf:parseType="Literal">&lt;p&gt;Early detection of cardiac dysfunction through routine screening is vital for
diagnosing cardiovascular diseases. An important metric of cardiac function is
the left ventricular ejection fraction (EF), where lower EF is associated with
cardiomyopathy. Echocardiography is a popular diagnostic tool in cardiology,
with ultrasound being a low-cost, real-time, and non-ionizing technology.
However, human assessment of echocardiograms for calculating EF is
time-consuming and expertise-demanding, raising the need for an automated
approach. In this work, we propose using the M(otion)-mode of echocardiograms
for estimating the EF and classifying cardiomyopathy. We generate multiple
artificial M-mode images from a single echocardiogram and combine them using
off-the-shelf model architectures. Additionally, we extend contrastive learning
(CL) to cardiac imaging to learn meaningful representations from exploiting
structures in unlabeled data allowing the model to achieve high accuracy, even
with limited annotations. Our experiments show that the supervised setting
converges with only ten modes and is comparable to the baseline method while
bypassing its cumbersome training process and being computationally much more
efficient. Furthermore, CL using M-mode images is helpful for limited data
scenarios, such as having labels for only 200 patients, which is common in
medical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ozkan_E/0/1/0/all/0/1&quot;&gt;Ece Ozkan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sutter_T/0/1/0/all/0/1&quot;&gt;Thomas M. Sutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yurong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Balzer_S/0/1/0/all/0/1&quot;&gt;Sebastian Balzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vogt_J/0/1/0/all/0/1&quot;&gt;Julia E. Vogt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03763">
<title>dacl1k: Real-World Bridge Damage Dataset Putting Open-Source Data to the Test. (arXiv:2309.03763v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03763</link>
<description rdf:parseType="Literal">&lt;p&gt;Recognising reinforced concrete defects (RCDs) is a crucial element for
determining the structural integrity, traffic safety and durability of bridges.
However, most of the existing datasets in the RCD domain are derived from a
small number of bridges acquired in specific camera poses, lighting conditions
and with fixed hardware. These limitations question the usability of models
trained on such open-source data in real-world scenarios. We address this
problem by testing such models on our &quot;dacl1k&quot; dataset, a highly diverse RCD
dataset for multi-label classification based on building inspections including
1,474 images. Thereby, we trained the models on different combinations of
open-source data (meta datasets) which were subsequently evaluated both
extrinsically and intrinsically. During extrinsic evaluation, we report metrics
on dacl1k and the meta datasets. The performance analysis on dacl1k shows
practical usability of the meta data, where the best model shows an Exact Match
Ratio of 32%. Additionally, we conduct an intrinsic evaluation by clustering
the bottleneck features of the best model derived from the extrinsic evaluation
in order to find out, if the model has learned distinguishing datasets or the
classes (RCDs) which is the aspired goal. The dacl1k dataset and our trained
models will be made publicly available, enabling researchers and practitioners
to put their models to the real-world test.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flotzinger_J/0/1/0/all/0/1&quot;&gt;Johannes Flotzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosch_P/0/1/0/all/0/1&quot;&gt;Philipp J. R&amp;#xf6;sch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oswald_N/0/1/0/all/0/1&quot;&gt;Norbert Oswald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braml_T/0/1/0/all/0/1&quot;&gt;Thomas Braml&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03764">
<title>$L_{2,1}$-Norm Regularized Quaternion Matrix Completion Using Sparse Representation and Quaternion QR Decomposition. (arXiv:2309.03764v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03764</link>
<description rdf:parseType="Literal">&lt;p&gt;Color image completion is a challenging problem in computer vision, but
recent research has shown that quaternion representations of color images
perform well in many areas. These representations consider the entire color
image and effectively utilize coupling information between the three color
channels. Consequently, low-rank quaternion matrix completion (LRQMC)
algorithms have gained significant attention. We propose a method based on
quaternion Qatar Riyal decomposition (QQR) and quaternion $L_{2,1}$-norm called
QLNM-QQR. This new approach reduces computational complexity by avoiding the
need to calculate the QSVD of large quaternion matrices. We also present two
improvements to the QLNM-QQR method: an enhanced version called IRQLNM-QQR that
uses iteratively reweighted quaternion $L_{2,1}$-norm minimization and a method
called QLNM-QQR-SR that integrates sparse regularization. Our experiments on
natural color images and color medical images show that IRQLNM-QQR outperforms
QLNM-QQR and that the proposed QLNM-QQR-SR method is superior to several
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Juan Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kou_K/0/1/0/all/0/1&quot;&gt;Kit Ian Kou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1&quot;&gt;Jifei Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lizhi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haojiang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03774">
<title>Deep Learning Safety Concerns in Automated Driving Perception. (arXiv:2309.03774v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03774</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in the field of deep learning and impressive performance of
deep neural networks (DNNs) for perception have resulted in an increased demand
for their use in automated driving (AD) systems. The safety of such systems is
of utmost importance and thus requires to consider the unique properties of
DNNs.
&lt;/p&gt;
&lt;p&gt;In order to achieve safety of AD systems with DNN-based perception components
in a systematic and comprehensive approach, so-called safety concerns have been
introduced as a suitable structuring element. On the one hand, the concept of
safety concerns is -- by design -- well aligned to existing standards relevant
for safety of AD systems such as ISO 21448 (SOTIF). On the other hand, it has
already inspired several academic publications and upcoming standards on AI
safety such as ISO PAS 8800.
&lt;/p&gt;
&lt;p&gt;While the concept of safety concerns has been previously introduced, this
paper extends and refines it, leveraging feedback from various domain and
safety experts in the field. In particular, this paper introduces an additional
categorization for a better understanding as well as enabling cross-functional
teams to jointly address the concerns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abrecht_S/0/1/0/all/0/1&quot;&gt;Stephanie Abrecht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirsch_A/0/1/0/all/0/1&quot;&gt;Alexander Hirsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raafatnia_S/0/1/0/all/0/1&quot;&gt;Shervin Raafatnia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woehrle_M/0/1/0/all/0/1&quot;&gt;Matthias Woehrle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03799">
<title>FisheyePP4AV: A privacy-preserving method for autonomous vehicles on fisheye camera images. (arXiv:2309.03799v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03799</link>
<description rdf:parseType="Literal">&lt;p&gt;In many parts of the world, the use of vast amounts of data collected on
public roadways for autonomous driving has increased. In order to detect and
anonymize pedestrian faces and nearby car license plates in actual road-driving
scenarios, there is an urgent need for effective solutions. As more data is
collected, privacy concerns regarding it increase, including but not limited to
pedestrian faces and surrounding vehicle license plates. Normal and fisheye
cameras are the two common camera types that are typically mounted on
collection vehicles. With complex camera distortion models, fisheye camera
images were deformed in contrast to regular images. It causes computer vision
tasks to perform poorly when using numerous deep learning models. In this work,
we pay particular attention to protecting privacy while yet adhering to several
laws for fisheye camera photos taken by driverless vehicles. First, we suggest
a framework for extracting face and plate identification knowledge from several
teacher models. Our second suggestion is to transform both the image and the
label from a regular image to fisheye-like data using a varied and realistic
fisheye transformation. Finally, we run a test using the open-source PP4AV
dataset. The experimental findings demonstrated that our model outperformed
baseline methods when trained on data from autonomous vehicles, even when the
data were softly labeled. The implementation code is available at our github:
https://github.com/khaclinh/FisheyePP4AV.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trinh_L/0/1/0/all/0/1&quot;&gt;Linh Trinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_B/0/1/0/all/0/1&quot;&gt;Bach Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Tu Tran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03809">
<title>SimNP: Learning Self-Similarity Priors Between Neural Points. (arXiv:2309.03809v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03809</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing neural field representations for 3D object reconstruction either (1)
utilize object-level representations, but suffer from low-quality details due
to conditioning on a global latent code, or (2) are able to perfectly
reconstruct the observations, but fail to utilize object-level prior knowledge
to infer unobserved regions. We present SimNP, a method to learn category-level
self-similarities, which combines the advantages of both worlds by connecting
neural point radiance fields with a category-level self-similarity
representation. Our contribution is two-fold. (1) We design the first neural
point representation on a category level by utilizing the concept of coherent
point clouds. The resulting neural point radiance fields store a high level of
detail for locally supported object regions. (2) We learn how information is
shared between neural points in an unconstrained and unsupervised fashion,
which allows to derive unobserved regions of an object during the
reconstruction process from given observations. We show that SimNP is able to
outperform previous methods in reconstructing symmetric unseen object regions,
surpassing methods that build upon category-level or pixel-aligned radiance
fields, while providing semantic correspondences between instances
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wewer_C/0/1/0/all/0/1&quot;&gt;Christopher Wewer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilg_E/0/1/0/all/0/1&quot;&gt;Eddy Ilg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1&quot;&gt;Bernt Schiele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lenssen_J/0/1/0/all/0/1&quot;&gt;Jan Eric Lenssen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03811">
<title>Panoramas from Photons. (arXiv:2309.03811v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03811</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene reconstruction in the presence of high-speed motion and low
illumination is important in many applications such as augmented and virtual
reality, drone navigation, and autonomous robotics. Traditional motion
estimation techniques fail in such conditions, suffering from too much blur in
the presence of high-speed motion and strong noise in low-light conditions.
Single-photon cameras have recently emerged as a promising technology capable
of capturing hundreds of thousands of photon frames per second thanks to their
high speed and extreme sensitivity. Unfortunately, traditional computer vision
techniques are not well suited for dealing with the binary-valued photon data
captured by these cameras because these are corrupted by extreme Poisson noise.
Here we present a method capable of estimating extreme scene motion under
challenging conditions, such as low light or high dynamic range, from a
sequence of high-speed image frames such as those captured by a single-photon
camera. Our method relies on iteratively improving a motion estimate by
grouping and aggregating frames after-the-fact, in a stratified manner. We
demonstrate the creation of high-quality panoramas under fast motion and
extremely low light, and super-resolution results using a custom single-photon
camera prototype. For code and supplemental material see our
$\href{https://wisionlab.com/project/panoramas-from-photons/}{\text{project
webpage}}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jungerman_S/0/1/0/all/0/1&quot;&gt;Sacha Jungerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ingle_A/0/1/0/all/0/1&quot;&gt;Atul Ingle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1&quot;&gt;Mohit Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03812">
<title>AnthroNet: Conditional Generation of Humans via Anthropometrics. (arXiv:2309.03812v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03812</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel human body model formulated by an extensive set of
anthropocentric measurements, which is capable of generating a wide range of
human body shapes and poses. The proposed model enables direct modeling of
specific human identities through a deep generative architecture, which can
produce humans in any arbitrary pose. It is the first of its kind to have been
trained end-to-end using only synthetically generated data, which not only
provides highly accurate human mesh representations but also allows for precise
anthropometry of the body. Moreover, using a highly diverse animation library,
we articulated our synthetic humans&apos; body and hands to maximize the diversity
of the learnable priors for model training. Our model was trained on a dataset
of $100k$ procedurally-generated posed human meshes and their corresponding
anthropometric measurements. Our synthetic data generator can be used to
generate millions of unique human identities and poses for non-commercial
academic research purposes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picetti_F/0/1/0/all/0/1&quot;&gt;Francesco Picetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshpande_S/0/1/0/all/0/1&quot;&gt;Shrinath Deshpande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leban_J/0/1/0/all/0/1&quot;&gt;Jonathan Leban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahtalebi_S/0/1/0/all/0/1&quot;&gt;Soroosh Shahtalebi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_J/0/1/0/all/0/1&quot;&gt;Jay Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_P/0/1/0/all/0/1&quot;&gt;Peifeng Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chunpu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metze_C/0/1/0/all/0/1&quot;&gt;Charles Metze III&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Cameron Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laidlaw_C/0/1/0/all/0/1&quot;&gt;Cera Laidlaw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warren_J/0/1/0/all/0/1&quot;&gt;James Warren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huynh_K/0/1/0/all/0/1&quot;&gt;Kathy Huynh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Page_R/0/1/0/all/0/1&quot;&gt;River Page&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hogins_J/0/1/0/all/0/1&quot;&gt;Jonathan Hogins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crespi_A/0/1/0/all/0/1&quot;&gt;Adam Crespi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganguly_S/0/1/0/all/0/1&quot;&gt;Sujoy Ganguly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebadi_S/0/1/0/all/0/1&quot;&gt;Salehe Erfanian Ebadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03815">
<title>T2IW: Joint Text to Image &amp; Watermark Generation. (arXiv:2309.03815v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03815</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent developments in text-conditioned image generative models have
revolutionized the production of realistic results. Unfortunately, this has
also led to an increase in privacy violations and the spread of false
information, which requires the need for traceability, privacy protection, and
other security measures. However, existing text-to-image paradigms lack the
technical capabilities to link traceable messages with image generation. In
this study, we introduce a novel task for the joint generation of text to image
and watermark (T2IW). This T2IW scheme ensures minimal damage to image quality
when generating a compound image by forcing the semantic feature and the
watermark signal to be compatible in pixels. Additionally, by utilizing
principles from Shannon information theory and non-cooperative game theory, we
are able to separate the revealed image and the revealed watermark from the
compound image. Furthermore, we strengthen the watermark robustness of our
approach by subjecting the compound image to various post-processing attacks,
with minimal pixel distortion observed in the revealed watermark. Extensive
experiments have demonstrated remarkable achievements in image quality,
watermark invisibility, and watermark robustness, supported by our proposed set
of evaluation metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;An-An Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guokai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yuting Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1&quot;&gt;Ning Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongdong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lanjun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03827">
<title>ArtHDR-Net: Perceptually Realistic and Accurate HDR Content Creation. (arXiv:2309.03827v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03827</link>
<description rdf:parseType="Literal">&lt;p&gt;High Dynamic Range (HDR) content creation has become an important topic for
modern media and entertainment sectors, gaming and Augmented/Virtual Reality
industries. Many methods have been proposed to recreate the HDR counterparts of
input Low Dynamic Range (LDR) images/videos given a single exposure or
multi-exposure LDRs. The state-of-the-art methods focus primarily on the
preservation of the reconstruction&apos;s structural similarity and the pixel-wise
accuracy. However, these conventional approaches do not emphasize preserving
the artistic intent of the images in terms of human visual perception, which is
an essential element in media, entertainment and gaming. In this paper, we
attempt to study and fill this gap. We propose an architecture called
ArtHDR-Net based on a Convolutional Neural Network that uses multi-exposed LDR
features as input. Experimental results show that ArtHDR-Net can achieve
state-of-the-art performance in terms of the HDR-VDP-2 score (i.e., mean
opinion score index) while reaching competitive performance in terms of PSNR
and SSIM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barua_H/0/1/0/all/0/1&quot;&gt;Hrishav Bakul Barua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnasamy_G/0/1/0/all/0/1&quot;&gt;Ganesh Krishnasamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1&quot;&gt;KokSheik Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stefanov_K/0/1/0/all/0/1&quot;&gt;Kalin Stefanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhall_A/0/1/0/all/0/1&quot;&gt;Abhinav Dhall&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03837">
<title>Cross-Task Attention Network: Improving Multi-Task Learning for Medical Imaging Applications. (arXiv:2309.03837v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03837</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-task learning (MTL) is a powerful approach in deep learning that
leverages the information from multiple tasks during training to improve model
performance. In medical imaging, MTL has shown great potential to solve various
tasks. However, existing MTL architectures in medical imaging are limited in
sharing information across tasks, reducing the potential performance
improvements of MTL. In this study, we introduce a novel attention-based MTL
framework to better leverage inter-task interactions for various tasks from
pixel-level to image-level predictions. Specifically, we propose a Cross-Task
Attention Network (CTAN) which utilizes cross-task attention mechanisms to
incorporate information by interacting across tasks. We validated CTAN on four
medical imaging datasets that span different domains and tasks including:
radiation treatment planning prediction using planning CT images of two
different target cancers (Prostate, OpenKBP); pigmented skin lesion
segmentation and diagnosis using dermatoscopic images (HAM10000); and COVID-19
diagnosis and severity prediction using chest CT scans (STOIC). Our study
demonstrates the effectiveness of CTAN in improving the accuracy of medical
imaging tasks. Compared to standard single-task learning (STL), CTAN
demonstrated a 4.67% improvement in performance and outperformed both widely
used MTL baselines: hard parameter sharing (HPS) with an average performance
improvement of 3.22%; and multi-task attention network (MTAN) with a relative
decrease of 5.38%. These findings highlight the significance of our proposed
MTL framework in solving medical imaging tasks and its potential to improve
their accuracy across domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sangwook Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purdie_T/0/1/0/all/0/1&quot;&gt;Thomas G. Purdie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McIntosh_C/0/1/0/all/0/1&quot;&gt;Chris McIntosh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03851">
<title>CenTime: Event-Conditional Modelling of Censoring in Survival Analysis. (arXiv:2309.03851v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03851</link>
<description rdf:parseType="Literal">&lt;p&gt;Survival analysis is a valuable tool for estimating the time until specific
events, such as death or cancer recurrence, based on baseline observations.
This is particularly useful in healthcare to prognostically predict clinically
important events based on patient data. However, existing approaches often have
limitations; some focus only on ranking patients by survivability, neglecting
to estimate the actual event time, while others treat the problem as a
classification task, ignoring the inherent time-ordered structure of the
events. Furthermore, the effective utilization of censored samples - training
data points where the exact event time is unknown - is essential for improving
the predictive accuracy of the model. In this paper, we introduce CenTime, a
novel approach to survival analysis that directly estimates the time to event.
Our method features an innovative event-conditional censoring mechanism that
performs robustly even when uncensored data is scarce. We demonstrate that our
approach forms a consistent estimator for the event model parameters, even in
the absence of uncensored data. Furthermore, CenTime is easily integrated with
deep learning models with no restrictions on batch size or the number of
uncensored samples. We compare our approach with standard survival analysis
methods, including the Cox proportional-hazard model and DeepHit. Our results
indicate that CenTime offers state-of-the-art performance in predicting
time-to-death while maintaining comparable ranking performance. Our
implementation is publicly available at
https://github.com/ahmedhshahin/CenTime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahin_A/0/1/0/all/0/1&quot;&gt;Ahmed H. Shahin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_A/0/1/0/all/0/1&quot;&gt;An Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whitehead_A/0/1/0/all/0/1&quot;&gt;Alexander C. Whitehead&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexander_D/0/1/0/all/0/1&quot;&gt;Daniel C. Alexander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacob_J/0/1/0/all/0/1&quot;&gt;Joseph Jacob&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barber_D/0/1/0/all/0/1&quot;&gt;David Barber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03869">
<title>Text-to-feature diffusion for audio-visual few-shot learning. (arXiv:2309.03869v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03869</link>
<description rdf:parseType="Literal">&lt;p&gt;Training deep learning models for video classification from audio-visual data
commonly requires immense amounts of labeled training data collected via a
costly process. A challenging and underexplored, yet much cheaper, setup is
few-shot learning from video data. In particular, the inherently multi-modal
nature of video data with sound and visual information has not been leveraged
extensively for the few-shot video classification task. Therefore, we introduce
a unified audio-visual few-shot video classification benchmark on three
datasets, i.e. the VGGSound-FSL, UCF-FSL, ActivityNet-FSL datasets, where we
adapt and compare ten methods. In addition, we propose AV-DIFF, a
text-to-feature diffusion framework, which first fuses the temporal and
audio-visual features via cross-modal attention and then generates multi-modal
features for the novel classes. We show that AV-DIFF obtains state-of-the-art
performance on our proposed benchmark for audio-visual (generalised) few-shot
learning. Our benchmark paves the way for effective audio-visual classification
when only limited labeled data is available. Code and data are available at
https://github.com/ExplainableML/AVDIFF-GFSL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mercea_O/0/1/0/all/0/1&quot;&gt;Otniel-Bogdan Mercea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hummel_T/0/1/0/all/0/1&quot;&gt;Thomas Hummel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koepke_A/0/1/0/all/0/1&quot;&gt;A. Sophia Koepke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1&quot;&gt;Zeynep Akata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03874">
<title>Box-based Refinement for Weakly Supervised and Unsupervised Localization Tasks. (arXiv:2309.03874v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03874</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been established that training a box-based detector network can
enhance the localization performance of weakly supervised and unsupervised
methods. Moreover, we extend this understanding by demonstrating that these
detectors can be utilized to improve the original network, paving the way for
further advancements. To accomplish this, we train the detectors on top of the
network output instead of the image data and apply suitable loss
backpropagation. Our findings reveal a significant improvement in phrase
grounding for the ``what is where by looking&apos;&apos; task, as well as various methods
of unsupervised object discovery. Our code is available at
https://github.com/eyalgomel/box-based-refinement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomel_E/0/1/0/all/0/1&quot;&gt;Eyal Gomel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaharabany_T/0/1/0/all/0/1&quot;&gt;Tal Shaharabany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1&quot;&gt;Lior Wolf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03879">
<title>Better Practices for Domain Adaptation. (arXiv:2309.03879v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03879</link>
<description rdf:parseType="Literal">&lt;p&gt;Distribution shifts are all too common in real-world applications of machine
learning. Domain adaptation (DA) aims to address this by providing various
frameworks for adapting models to the deployment data without using labels.
However, the domain shift scenario raises a second more subtle challenge: the
difficulty of performing hyperparameter optimisation (HPO) for these adaptation
algorithms without access to a labelled validation set. The unclear validation
protocol for DA has led to bad practices in the literature, such as performing
HPO using the target test labels when, in real-world scenarios, they are not
available. This has resulted in over-optimism about DA research progress
compared to reality. In this paper, we analyse the state of DA when using good
evaluation practice, by benchmarking a suite of candidate validation criteria
and using them to assess popular adaptation algorithms. We show that there are
challenges across all three branches of domain adaptation methodology including
Unsupervised Domain Adaptation (UDA), Source-Free Domain Adaptation (SFDA), and
Test Time Adaptation (TTA). While the results show that realistically
achievable performance is often worse than expected, they also show that using
proper validation splits is beneficial, as well as showing that some previously
unexplored validation metrics provide the best options to date. Altogether, our
improved practices covering data, training, validation and hyperparameter
optimisation form a new rigorous pipeline to improve benchmarking, and hence
research progress, within this important field going forward.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ericsson_L/0/1/0/all/0/1&quot;&gt;Linus Ericsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Da Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1&quot;&gt;Timothy M. Hospedales&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03891">
<title>ArtiGrasp: Physically Plausible Synthesis of Bi-Manual Dexterous Grasping and Articulation. (arXiv:2309.03891v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2309.03891</link>
<description rdf:parseType="Literal">&lt;p&gt;We present ArtiGrasp, a novel method to synthesize bi-manual hand-object
interactions that include grasping and articulation. This task is challenging
due to the diversity of the global wrist motions and the precise finger control
that are necessary to articulate objects. ArtiGrasp leverages reinforcement
learning and physics simulations to train a policy that controls the global and
local hand pose. Our framework unifies grasping and articulation within a
single policy guided by a single hand pose reference. Moreover, to facilitate
the training of the precise finger control required for articulation, we
present a learning curriculum with increasing difficulty. It starts with
single-hand manipulation of stationary objects and continues with multi-agent
training including both hands and non-stationary objects. To evaluate our
method, we introduce Dynamic Object Grasping and Articulation, a task that
involves bringing an object into a target articulated pose. This task requires
grasping, relocation, and articulation. We show our method&apos;s efficacy towards
this task. We further demonstrate that our method can generate motions with
noisy hand-object pose estimates from an off-the-shelf image-based regressor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christen_S/0/1/0/all/0/1&quot;&gt;Sammy Christen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zicong Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Luocheng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwangbo_J/0/1/0/all/0/1&quot;&gt;Jemin Hwangbo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jie Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1&quot;&gt;Otmar Hilliges&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03893">
<title>DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection. (arXiv:2309.03893v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03893</link>
<description rdf:parseType="Literal">&lt;p&gt;Data is the cornerstone of deep learning. This paper reveals that the
recently developed Diffusion Model is a scalable data engine for object
detection. Existing methods for scaling up detection-oriented data often
require manual collection or generative models to obtain target images,
followed by data augmentation and labeling to produce training pairs, which are
costly, complex, or lacking diversity. To address these issues, we
presentDiffusionEngine (DE), a data scaling-up engine that provides
high-quality detection-oriented training pairs in a single stage. DE consists
of a pre-trained diffusion model and an effective Detection-Adapter,
contributing to generating scalable, diverse and generalizable detection data
in a plug-and-play manner. Detection-Adapter is learned to align the implicit
semantic and location knowledge in off-the-shelf diffusion models with
detection-aware signals to make better bounding-box predictions. Additionally,
we contribute two datasets, i.e., COCO-DE and VOC-DE, to scale up existing
detection benchmarks for facilitating follow-up research. Extensive experiments
demonstrate that data scaling-up via DE can achieve significant improvements in
diverse scenarios, such as various detection algorithms, self-supervised
pre-training, data-sparse, label-scarce, cross-domain, and semi-supervised
learning. For example, when using DE with a DINO-based adapter to scale up
data, mAP is improved by 3.1% on COCO, 7.6% on VOC, and 11.5% on Clipart.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Manlin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yuxi Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Ming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1&quot;&gt;Jie Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xuefeng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1&quot;&gt;Min Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1&quot;&gt;Andy J. Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03895">
<title>InstructDiffusion: A Generalist Modeling Interface for Vision Tasks. (arXiv:2309.03895v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03895</link>
<description rdf:parseType="Literal">&lt;p&gt;We present InstructDiffusion, a unifying and generic framework for aligning
computer vision tasks with human instructions. Unlike existing approaches that
integrate prior knowledge and pre-define the output space (e.g., categories and
coordinates) for each vision task, we cast diverse vision tasks into a
human-intuitive image-manipulating process whose output space is a flexible and
interactive pixel space. Concretely, the model is built upon the diffusion
process and is trained to predict pixels according to user instructions, such
as encircling the man&apos;s left shoulder in red or applying a blue mask to the
left car. InstructDiffusion could handle a variety of vision tasks, including
understanding tasks (such as segmentation and keypoint detection) and
generative tasks (such as editing and enhancement). It even exhibits the
ability to handle unseen tasks and outperforms prior methods on novel datasets.
This represents a significant step towards a generalist modeling interface for
vision tasks, advancing artificial general intelligence in the field of
computer vision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1&quot;&gt;Zigang Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Binxin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hang_T/0/1/0/all/0/1&quot;&gt;Tiankai Hang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1&quot;&gt;Shuyang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Ting Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1&quot;&gt;Jianmin Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Han Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1&quot;&gt;Baining Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03897">
<title>ProPainter: Improving Propagation and Transformer for Video Inpainting. (arXiv:2309.03897v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03897</link>
<description rdf:parseType="Literal">&lt;p&gt;Flow-based propagation and spatiotemporal Transformer are two mainstream
mechanisms in video inpainting (VI). Despite the effectiveness of these
components, they still suffer from some limitations that affect their
performance. Previous propagation-based approaches are performed separately
either in the image or feature domain. Global image propagation isolated from
learning may cause spatial misalignment due to inaccurate optical flow.
Moreover, memory or computational constraints limit the temporal range of
feature propagation and video Transformer, preventing exploration of
correspondence information from distant frames. To address these issues, we
propose an improved framework, called ProPainter, which involves enhanced
ProPagation and an efficient Transformer. Specifically, we introduce
dual-domain propagation that combines the advantages of image and feature
warping, exploiting global correspondences reliably. We also propose a
mask-guided sparse video Transformer, which achieves high efficiency by
discarding unnecessary and redundant tokens. With these components, ProPainter
outperforms prior arts by a large margin of 1.46 dB in PSNR while maintaining
appealing efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shangchen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1&quot;&gt;Kelvin C.K. Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1&quot;&gt;Chen Change Loy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03899">
<title>The Making and Breaking of Camouflage. (arXiv:2309.03899v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03899</link>
<description rdf:parseType="Literal">&lt;p&gt;Not all camouflages are equally effective, as even a partially visible
contour or a slight color difference can make the animal stand out and break
its camouflage. In this paper, we address the question of what makes a
camouflage successful, by proposing three scores for automatically assessing
its effectiveness. In particular, we show that camouflage can be measured by
the similarity between background and foreground features and boundary
visibility. We use these camouflage scores to assess and compare all available
camouflage datasets. We also incorporate the proposed camouflage score into a
generative model as an auxiliary loss and show that effective camouflage images
or videos can be synthesised in a scalable manner. The generated synthetic
dataset is used to train a transformer-based model for segmenting camouflaged
animals in videos. Experimentally, we demonstrate state-of-the-art camouflage
breaking performance on the public MoCA-Mask benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamdouar_H/0/1/0/all/0/1&quot;&gt;Hala Lamdouar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weidi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1&quot;&gt;Andrew Zisserman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03900">
<title>Learning Continuous Exposure Value Representations for Single-Image HDR Reconstruction. (arXiv:2309.03900v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.03900</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning is commonly used to reconstruct HDR images from LDR images. LDR
stack-based methods are used for single-image HDR reconstruction, generating an
HDR image from a deep learning-generated LDR stack. However, current methods
generate the stack with predetermined exposure values (EVs), which may limit
the quality of HDR reconstruction. To address this, we propose the continuous
exposure value representation (CEVR), which uses an implicit function to
generate LDR images with arbitrary EVs, including those unseen during training.
Our approach generates a continuous stack with more images containing diverse
EVs, significantly improving HDR reconstruction. We use a cycle training
strategy to supervise the model in generating continuous EV LDR images without
corresponding ground truths. Our CEVR model outperforms existing methods, as
demonstrated by experimental results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Su-Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yen_H/0/1/0/all/0/1&quot;&gt;Hung-Lin Yen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu-Lun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Min-Hung Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hou-Ning Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Peng_W/0/1/0/all/0/1&quot;&gt;Wen-Hsiao Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yen-Yu Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03903">
<title>Tracking Anything with Decoupled Video Segmentation. (arXiv:2309.03903v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03903</link>
<description rdf:parseType="Literal">&lt;p&gt;Training data for video segmentation are expensive to annotate. This impedes
extensions of end-to-end algorithms to new video segmentation tasks, especially
in large-vocabulary settings. To &apos;track anything&apos; without training on video
data for every individual task, we develop a decoupled video segmentation
approach (DEVA), composed of task-specific image-level segmentation and
class/task-agnostic bi-directional temporal propagation. Due to this design, we
only need an image-level model for the target task (which is cheaper to train)
and a universal temporal propagation model which is trained once and
generalizes across tasks. To effectively combine these two modules, we use
bi-directional propagation for (semi-)online fusion of segmentation hypotheses
from different frames to generate a coherent segmentation. We show that this
decoupled formulation compares favorably to end-to-end approaches in several
data-scarce tasks including large-vocabulary video panoptic segmentation,
open-world video segmentation, referring video segmentation, and unsupervised
video object segmentation. Code is available at:
https://hkchengrex.github.io/Tracking-Anything-with-DEVA
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Ho Kei Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Seoung Wug Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1&quot;&gt;Brian Price&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1&quot;&gt;Alexander Schwing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Joon-Young Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03904">
<title>Exploring Sparse MoE in GANs for Text-conditioned Image Synthesis. (arXiv:2309.03904v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03904</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the difficulty in scaling up, generative adversarial networks (GANs)
seem to be falling from grace on the task of text-conditioned image synthesis.
Sparsely-activated mixture-of-experts (MoE) has recently been demonstrated as a
valid solution to training large-scale models with limited computational
resources. Inspired by such a philosophy, we present Aurora, a GAN-based
text-to-image generator that employs a collection of experts to learn feature
processing, together with a sparse router to help select the most suitable
expert for each feature point. To faithfully decode the sampling stochasticity
and the text condition to the final synthesis, our router adaptively makes its
decision by taking into account the text-integrated global latent code. At
64x64 image resolution, our model trained on LAION2B-en and COYO-700M achieves
6.2 zero-shot FID on MS COCO. We release the code and checkpoints to facilitate
the community for further development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jiapeng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Ceyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1&quot;&gt;Kecheng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yinghao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zifan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yujun Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03905">
<title>ImageBind-LLM: Multi-modality Instruction Tuning. (arXiv:2309.03905v1 [cs.MM])</title>
<link>http://arxiv.org/abs/2309.03905</link>
<description rdf:parseType="Literal">&lt;p&gt;We present ImageBind-LLM, a multi-modality instruction tuning method of large
language models (LLMs) via ImageBind. Existing works mainly focus on language
and image instruction tuning, different from which, our ImageBind-LLM can
respond to multi-modality conditions, including audio, 3D point clouds, video,
and their embedding-space arithmetic by only image-text alignment training.
During training, we adopt a learnable bind network to align the embedding space
between LLaMA and ImageBind&apos;s image encoder. Then, the image features
transformed by the bind network are added to word tokens of all layers in
LLaMA, which progressively injects visual instructions via an attention-free
and zero-initialized gating mechanism. Aided by the joint embedding of
ImageBind, the simple image-text training enables our model to exhibit superior
multi-modality instruction-following capabilities. During inference, the
multi-modality inputs are fed into the corresponding ImageBind encoders, and
processed by a proposed visual cache model for further cross-modal embedding
enhancement. The training-free cache model retrieves from three million image
features extracted by ImageBind, which effectively mitigates the
training-inference modality discrepancy. Notably, with our approach,
ImageBind-LLM can respond to instructions of diverse modalities and demonstrate
significant language generation quality. Code is released at
https://github.com/OpenGVLab/LLaMA-Adapter.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiaming Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Renrui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1&quot;&gt;Wenqi Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1&quot;&gt;Peng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1&quot;&gt;Han Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chris Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1&quot;&gt;Song Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Ziyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xudong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Shuai Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yafei Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xiangyu Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2007.05059">
<title>Learning Representations that Support Extrapolation. (arXiv:2007.05059v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2007.05059</link>
<description rdf:parseType="Literal">&lt;p&gt;Extrapolation -- the ability to make inferences that go beyond the scope of
one&apos;s experiences -- is a hallmark of human intelligence. By contrast, the
generalization exhibited by contemporary neural network algorithms is largely
limited to interpolation between data points in their training corpora. In this
paper, we consider the challenge of learning representations that support
extrapolation. We introduce a novel visual analogy benchmark that allows the
graded evaluation of extrapolation as a function of distance from the convex
domain defined by the training data. We also introduce a simple technique,
temporal context normalization, that encourages representations that emphasize
the relations between objects. We find that this technique enables a
significant improvement in the ability to extrapolate, considerably
outperforming a number of competitive techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webb_T/0/1/0/all/0/1&quot;&gt;Taylor W. Webb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dulberg_Z/0/1/0/all/0/1&quot;&gt;Zachary Dulberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frankland_S/0/1/0/all/0/1&quot;&gt;Steven M. Frankland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrov_A/0/1/0/all/0/1&quot;&gt;Alexander A. Petrov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OReilly_R/0/1/0/all/0/1&quot;&gt;Randall C. O&amp;#x27;Reilly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1&quot;&gt;Jonathan D. Cohen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.03624">
<title>FCNet: A Convolutional Neural Network for Arbitrary-Length Exposure Estimation. (arXiv:2203.03624v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.03624</link>
<description rdf:parseType="Literal">&lt;p&gt;The photographs captured by digital cameras usually suffer from over or under
exposure problems. For image exposure enhancement, the tasks of Single-Exposure
Correction (SEC) and Multi-Exposure Fusion (MEF) are widely studied in the
image processing community. However, current SEC or MEF methods are developed
under different motivations and thus ignore the internal correlation between
SEC and MEF, making it difficult to process arbitrary-length sequences with
improper exposures. Besides, the MEF methods usually fail at estimating the
exposure of a sequence containing only under-exposed or over-exposed images. To
alleviate these problems, in this paper, we develop a novel Fusion-Correction
Network (FCNet) to tackle an arbitrary-length (including one) image sequence
with improper exposures. This is achieved by fusing and correcting an image
sequence by Laplacian Pyramid (LP) image decomposition. In each LP level, the
low-frequency base component of the input image sequence is fed into a Fusion
block and a Correction block sequentially for consecutive exposure estimation,
implemented by alternative exposure fusion and correction. The
exposure-corrected image in current LP level is upsampled and fused with the
high-frequency detail components of the input image sequence in the next LP
level, to output the base component for the Fusion and Correction blocks in
next LP level. Experiments on the benchmark dataset demonstrate that our FCNet
is effective on arbitrary-length exposure estimation, including both SEC and
MEF. The code is publicly released at https://github.com/NKUJinLiang/FCNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuchen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Anran Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhen_X/0/1/0/all/0/1&quot;&gt;Xiantong Zhen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.05893">
<title>DRTAM: Dual Rank-1 Tensor Attention Module. (arXiv:2203.05893v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.05893</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, attention mechanisms have been extensively investigated in computer
vision, but few of them show excellent performance on both large and mobile
networks. This paper proposes Dual Rank-1 Tensor Attention Module (DRTAM), a
novel residual-attention-learning-guided attention module for feed-forward
convolutional neural networks. Given a 3D feature tensor map, DRTAM firstly
generates three 2D feature descriptors along three axes. Then, using three
descriptors, DRTAM sequentially infers two rank-1 tensor attention maps, the
initial attention map and the complement attention map, combines and multiplied
them to the input feature map for adaptive feature refinement(see Fig.1(c)). To
generate two attention maps, DRTAM introduces rank-1 tensor attention module
(RTAM) and residual descriptors extraction module (RDEM): RTAM divides each 2D
feature descriptors into several chunks, and generate three factor vectors of a
rank-1 tensor attention map by employing strip pooling on each chunk so that
local and long-range contextual information can be captured along three
dimension respectively; RDEM generates three 2D feature descriptors of the
residual feature to produce the complement attention map, using three factor
vectors of the initial attention map and three descriptors of the input
feature. Extensive experimental results on ImageNet-1K, MS COCO and PASCAL VOC
demonstrate that DRTAM achieves competitive performance on both large and
mobile networks compare with other state-of-the-art attention modules.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1&quot;&gt;Hanxing Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Baihong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jun Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.09957">
<title>Enhancement of Novel View Synthesis Using Omnidirectional Image Completion. (arXiv:2203.09957v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.09957</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we present a method for synthesizing novel views from a single
360-degree RGB-D image based on the neural radiance field (NeRF) . Prior
studies relied on the neighborhood interpolation capability of multi-layer
perceptrons to complete missing regions caused by occlusion and zooming, which
leads to artifacts. In the method proposed in this study, the input image is
reprojected to 360-degree RGB images at other camera positions, the missing
regions of the reprojected images are completed by a 2D image generative model,
and the completed images are utilized to train the NeRF. Because multiple
completed images contain inconsistencies in 3D, we introduce a method to learn
the NeRF model using a subset of completed images that cover the target scene
with less overlap of completed regions. The selection of such a subset of
images can be attributed to the maximum weight independent set problem, which
is solved through simulated annealing. Experiments demonstrated that the
proposed method can synthesize plausible novel views while preserving the
features of the scene for both artificial and real-world data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hara_T/0/1/0/all/0/1&quot;&gt;Takayuki Hara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1&quot;&gt;Tatsuya Harada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.15201">
<title>Light Field Depth Estimation via Stitched Epipolar Plane Images. (arXiv:2203.15201v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.15201</link>
<description rdf:parseType="Literal">&lt;p&gt;Depth estimation is a fundamental problem in light field processing.
Epipolar-plane image (EPI)-based methods often encounter challenges such as low
accuracy in slope computation due to discretization errors and limited angular
resolution. Besides, existing methods perform well in most regions but struggle
to produce sharp edges in occluded regions and resolve ambiguities in
texture-less regions. To address these issues, we propose the concept of
stitched-EPI (SEPI) to enhance slope computation. SEPI achieves this by
shifting and concatenating lines from different EPIs that correspond to the
same 3D point. Moreover, we introduce the half-SEPI algorithm, which focuses
exclusively on the non-occluded portion of lines to handle occlusion.
Additionally, we present a depth propagation strategy aimed at improving depth
estimation in texture-less regions. This strategy involves determining the
depth of such regions by progressing from the edges towards the interior,
prioritizing accurate regions over coarse regions. Through extensive
experimental evaluations and ablation studies, we validate the effectiveness of
our proposed method. The results demonstrate its superior ability to generate
more accurate and robust depth maps across all regions compared to
state-of-the-art methods. The source code will be publicly available at
https://github.com/PingZhou-LF/Light-Field-Depth-Estimation-Based-on-Stitched-EPIs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Ping Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1&quot;&gt;Langqing Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jing Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuting Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Junhui Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.03367">
<title>Joint Super-Resolution and Inverse Tone-Mapping: A Feature Decomposition Aggregation Network and A New Benchmark. (arXiv:2207.03367v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.03367</link>
<description rdf:parseType="Literal">&lt;p&gt;Joint Super-Resolution and Inverse Tone-Mapping (joint SR-ITM) aims to
increase the resolution and dynamic range of low-resolution and standard
dynamic range images. Recent networks mainly resort to image decomposition
techniques with complex multi-branch architectures. However, the fixed
decomposition techniques would largely restricts their power on versatile
images. To exploit the potential power of decomposition mechanism, in this
paper, we generalize it from the image domain to the broader feature domain. To
this end, we propose a lightweight Feature Decomposition Aggregation Network
(FDAN). In particular, we design a Feature Decomposition Block (FDB) to achieve
learnable separation of detail and base feature maps, and develop a
Hierarchical Feature Decomposition Group by cascading FDBs for powerful
multi-level feature decomposition. Moreover, to better evaluate the comparison
methods, we collect a large-scale dataset for joint SR-ITM, i.e., SRITM-4K,
which provides versatile scenarios for robust model training and evaluation.
Experimental results on two benchmark datasets demonstrate that our FDAN is
efficient and outperforms state-of-the-art methods on joint SR-ITM. The code of
our FDAN and the SRITM-4K dataset are available at
https://github.com/CS-GangXu/FDAN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1&quot;&gt;Gang Xu&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yu-chen Yang&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1&quot;&gt;Xian-Tong Zhen&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jun Xu&lt;/a&gt; (1) ((1) Nankai University, (2) Institute of Automation, CAS, (3) Guangdong University of Petrochemical Technology)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.01708">
<title>Autonomous Agriculture Robot for Smart Farming. (arXiv:2208.01708v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2208.01708</link>
<description rdf:parseType="Literal">&lt;p&gt;This project aims to develop and demonstrate a ground robot with intelligence
capable of conducting semi-autonomous farm operations for different low-heights
vegetable crops referred as Agriculture Application Robot(AAR). AAR is a
lightweight, solar-electric powered robot that uses intelligent perception for
conducting detection and classification of plants and their characteristics.
The system also has a robotic arm for the autonomous weed cutting process. The
robot can deliver fertilizer spraying, insecticide, herbicide, and other fluids
to the targets such as crops, weeds, and other pests. Besides, it provides
information for future research into higher-level tasks such as yield
estimation, crop, and soil health monitoring. We present the design of robot
and the associated experiments which show the promising results in real world
environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ummadi_V/0/1/0/all/0/1&quot;&gt;Vinay Ummadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gundlapalle_A/0/1/0/all/0/1&quot;&gt;Aravind Gundlapalle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaik_A/0/1/0/all/0/1&quot;&gt;Althaf Shaik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+B_S/0/1/0/all/0/1&quot;&gt;Shaik Mohammad Rafi B&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.13008">
<title>USE-Evaluator: Performance Metrics for Medical Image Segmentation Models with Uncertain, Small or Empty Reference Annotations. (arXiv:2209.13008v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.13008</link>
<description rdf:parseType="Literal">&lt;p&gt;Performance metrics for medical image segmentation models are used to measure
the agreement between the reference annotation and the predicted segmentation.
Usually, overlap metrics, such as the Dice, are used as a metric to evaluate
the performance of these models in order for results to be comparable. However,
there is a mismatch between the distributions of cases and difficulty level of
segmentation tasks in public data sets compared to clinical practice. Common
metrics fail to measure the impact of this mismatch, especially for clinical
data sets that include low signal pathologies, a difficult segmentation task,
and uncertain, small, or empty reference annotations. This limitation may
result in ineffective research of machine learning practitioners in designing
and optimizing models. Dimensions of evaluating clinical value include
consideration of the uncertainty of reference annotations, independence from
reference annotation volume size, and evaluation of classification of empty
reference annotations. We study how uncertain, small, and empty reference
annotations influence the value of metrics for medical image segmentation on an
in-house data set regardless of the model. We examine metrics behavior on the
predictions of a standard deep learning framework in order to identify metrics
with clinical value. We compare to a public benchmark data set (BraTS 2019)
with a high-signal pathology and certain, larger, and no empty reference
annotations. We may show machine learning practitioners, how uncertain, small,
or empty reference annotations require a rethinking of the evaluation and
optimizing procedures. The evaluation code was released to encourage further
analysis of this topic.
https://github.com/SophieOstmeier/UncertainSmallEmpty.git
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ostmeier_S/0/1/0/all/0/1&quot;&gt;Sophie Ostmeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Axelrod_B/0/1/0/all/0/1&quot;&gt;Brian Axelrod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bertels_J/0/1/0/all/0/1&quot;&gt;Jeroen Bertels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Isensee_F/0/1/0/all/0/1&quot;&gt;Fabian Isensee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lansberg_M/0/1/0/all/0/1&quot;&gt;Maarten G.Lansberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Christensen_S/0/1/0/all/0/1&quot;&gt;Soren Christensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Albers_G/0/1/0/all/0/1&quot;&gt;Gregory W. Albers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li-Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Heit_J/0/1/0/all/0/1&quot;&gt;Jeremy J. Heit&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.07590">
<title>Stain-invariant self supervised learning for histopathology image analysis. (arXiv:2211.07590v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.07590</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a self-supervised algorithm for several classification tasks
within hematoxylin and eosin (H&amp;amp;E) stained images of breast cancer. Our method
is robust to stain variations inherent to the histology images acquisition
process, which has limited the applicability of automated analysis tools. We
address this problem by imposing constraints a learnt latent space which
leverages stain normalization techniques during training. At every iteration,
we select an image as a normalization target and generate a version of every
image in the batch normalized to that target. We minimize the distance between
the embeddings that correspond to the same image under different staining
variations while maximizing the distance between other samples. We show that
our method not only improves robustness to stain variations across multi-center
data, but also classification performance through extensive experiments on
various normalization targets and methods. Our method achieves the
state-of-the-art performance on several publicly available breast cancer
datasets ranging from tumor classification (CAMELYON17) and subtyping (BRACS)
to HER2 status classification and treatment response prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiard_A/0/1/0/all/0/1&quot;&gt;Alexandre Tiard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alex Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1&quot;&gt;David Joon Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yangchao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nof_E/0/1/0/all/0/1&quot;&gt;Eliram Nof&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goh_A/0/1/0/all/0/1&quot;&gt;Alvin C. Goh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1&quot;&gt;Stefano Soatto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nadeem_S/0/1/0/all/0/1&quot;&gt;Saad Nadeem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.15341">
<title>Non-inferiority of Deep Learning Acute Ischemic Stroke Segmentation on Non-Contrast CT Compared to Expert Neuroradiologists. (arXiv:2211.15341v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.15341</link>
<description rdf:parseType="Literal">&lt;p&gt;To determine if a convolutional neural network (CNN) deep learning model can
accurately segment acute ischemic changes on non-contrast CT compared to
neuroradiologists. Non-contrast CT (NCCT) examinations from 232 acute ischemic
stroke patients who were enrolled in the DEFUSE 3 trial were included in this
study. Three experienced neuroradiologists independently segmented hypodensity
that reflected the ischemic core on each scan. The neuroradiologist with the
most experience (expert A) served as the ground truth for deep learning model
training. Two additional neuroradiologists (experts B and C) segmentations were
used for data testing. The 232 studies were randomly split into training and
test sets. The training set was further randomly divided into 5 folds with
training and validation sets. A 3-dimensional CNN architecture was trained and
optimized to predict the segmentations of expert A from NCCT. The performance
of the model was assessed using a set of volume, overlap, and distance metrics
using non-inferiority thresholds of 20%, 3ml, and 3mm. The optimized model
trained on expert A was compared to test experts B and C. We used a one-sided
Wilcoxon signed-rank test to test for the non-inferiority of the model-expert
compared to the inter-expert agreement. The final model performance for the
ischemic core segmentation task reached a performance of 0.46+-0.09 Surface
Dice at Tolerance 5mm and 0.47+-0.13 Dice when trained on expert A. Compared to
the two test neuroradiologists the model-expert agreement was non-inferior to
the inter-expert agreement, p &amp;lt; 0.05. The CNN accurately delineates the
hypodense ischemic core on NCCT in acute ischemic stroke patients with an
accuracy comparable to neuroradiologists.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ostmeier_S/0/1/0/all/0/1&quot;&gt;Sophie Ostmeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Axelrod_B/0/1/0/all/0/1&quot;&gt;Brian Axelrod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Verhaaren_B/0/1/0/all/0/1&quot;&gt;Benjamin F.J. Verhaaren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Christensen_S/0/1/0/all/0/1&quot;&gt;Soren Christensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mahammedi_A/0/1/0/all/0/1&quot;&gt;Abdelkader Mahammedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongkai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pulli_B/0/1/0/all/0/1&quot;&gt;Benjamin Pulli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li-Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zaharchuk_G/0/1/0/all/0/1&quot;&gt;Greg Zaharchuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Heit_J/0/1/0/all/0/1&quot;&gt;Jeremy J. Heit&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01448">
<title>PGFed: Personalize Each Client&apos;s Global Objective for Federated Learning. (arXiv:2212.01448v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01448</link>
<description rdf:parseType="Literal">&lt;p&gt;Personalized federated learning has received an upsurge of attention due to
the mediocre performance of conventional federated learning (FL) over
heterogeneous data. Unlike conventional FL which trains a single global
consensus model, personalized FL allows different models for different clients.
However, existing personalized FL algorithms only implicitly transfer the
collaborative knowledge across the federation by embedding the knowledge into
the aggregated model or regularization. We observed that this implicit
knowledge transfer fails to maximize the potential of each client&apos;s empirical
risk toward other clients. Based on our observation, in this work, we propose
Personalized Global Federated Learning (PGFed), a novel personalized FL
framework that enables each client to personalize its own global objective by
explicitly and adaptively aggregating the empirical risks of itself and other
clients. To avoid massive (O(N^2)) communication overhead and potential privacy
leakage while achieving this, each client&apos;s risk is estimated through a
first-order approximation for other clients&apos; adaptive risk aggregation. On top
of PGFed, we develop a momentum upgrade, dubbed PGFedMo, to more efficiently
utilize clients&apos; empirical risks. Our extensive experiments on four datasets
under different federated settings show consistent improvements of PGFed over
previous state-of-the-art methods. The code is publicly available at
https://github.com/ljaiverson/pgfed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jun Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendieta_M/0/1/0/all/0/1&quot;&gt;Matias Mendieta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shandong Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.03744">
<title>3D Neural Embedding Likelihood: Probabilistic Inverse Graphics for Robust 6D Pose Estimation. (arXiv:2302.03744v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.03744</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to perceive and understand 3D scenes is crucial for many
applications in computer vision and robotics. Inverse graphics is an appealing
approach to 3D scene understanding that aims to infer the 3D scene structure
from 2D images. In this paper, we introduce probabilistic modeling to the
inverse graphics framework to quantify uncertainty and achieve robustness in 6D
pose estimation tasks. Specifically, we propose 3D Neural Embedding Likelihood
(3DNEL) as a unified probabilistic model over RGB-D images, and develop
efficient inference procedures on 3D scene descriptions. 3DNEL effectively
combines learned neural embeddings from RGB with depth information to improve
robustness in sim-to-real 6D object pose estimation from RGB-D images.
Performance on the YCB-Video dataset is on par with state-of-the-art yet is
much more robust in challenging regimes. In contrast to discriminative
approaches, 3DNEL&apos;s probabilistic generative formulation jointly models
multiple objects in a scene, quantifies uncertainty in a principled way, and
handles object pose tracking under heavy occlusion. Finally, 3DNEL provides a
principled framework for incorporating prior knowledge about the scene and
objects, which allows natural extension to additional tasks like camera pose
tracking from video.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Guangyao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gothoskar_N/0/1/0/all/0/1&quot;&gt;Nishad Gothoskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lirui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gutfreund_D/0/1/0/all/0/1&quot;&gt;Dan Gutfreund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazaro_Gredilla_M/0/1/0/all/0/1&quot;&gt;Miguel L&amp;#xe1;zaro-Gredilla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+George_D/0/1/0/all/0/1&quot;&gt;Dileep George&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansinghka_V/0/1/0/all/0/1&quot;&gt;Vikash K. Mansinghka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08272">
<title>Revisiting Hidden Representations in Transfer Learning for Medical Imaging. (arXiv:2302.08272v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08272</link>
<description rdf:parseType="Literal">&lt;p&gt;While a key component to the success of deep learning is the availability of
massive amounts of training data, medical image datasets are often limited in
diversity and size. Transfer learning has the potential to bridge the gap
between related yet different domains. For medical applications, however, it
remains unclear whether it is more beneficial to pre-train on natural or
medical images. We aim to shed light on this problem by comparing
initialization on ImageNet and RadImageNet on seven medical classification
tasks. Our work includes a replication study, which yields results contrary to
previously published findings. In our experiments, ResNet50 models pre-trained
on ImageNet tend to outperform those trained on RadImageNet. To gain further
insights, we investigate the learned representations using Canonical
Correlation Analysis (CCA) and compare the predictions of the different models.
Our results indicate that, contrary to intuition, ImageNet and RadImageNet may
converge to distinct intermediate representations, which appear to diverge
further during fine-tuning. Despite these distinct representations, the
predictions of the models remain similar. Our findings show that the similarity
between networks before and after fine-tuning does not correlate with
performance gains, suggesting that the advantages of transfer learning might
not solely originate from the reuse of features in the early layers of a
convolutional neural network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juodelyte_D/0/1/0/all/0/1&quot;&gt;Dovile Juodelyte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jimenez_Sanchez_A/0/1/0/all/0/1&quot;&gt;Amelia Jim&amp;#xe9;nez-S&amp;#xe1;nchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheplygina_V/0/1/0/all/0/1&quot;&gt;Veronika Cheplygina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.14051">
<title>Internet Explorer: Targeted Representation Learning on the Open Web. (arXiv:2302.14051v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.14051</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern vision models typically rely on fine-tuning general-purpose models
pre-trained on large, static datasets. These general-purpose models only
capture the knowledge within their pre-training datasets, which are tiny,
out-of-date snapshots of the Internet -- where billions of images are uploaded
each day. We suggest an alternate approach: rather than hoping our static
datasets transfer to our desired tasks after large-scale pre-training, we
propose dynamically utilizing the Internet to quickly train a small-scale model
that does extremely well on the task at hand. Our approach, called Internet
Explorer, explores the web in a self-supervised manner to progressively find
relevant examples that improve performance on a desired target dataset. It
cycles between searching for images on the Internet with text queries,
self-supervised training on downloaded images, determining which images were
useful, and prioritizing what to search for next. We evaluate Internet Explorer
across several datasets and show that it outperforms or matches CLIP oracle
performance by using just a single GPU desktop to actively query the Internet
for 30--40 hours. Results, visualizations, and videos at
https://internet-explorer-ssl.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Alexander C. Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_E/0/1/0/all/0/1&quot;&gt;Ellis Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1&quot;&gt;Alexei A. Efros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1&quot;&gt;Deepak Pathak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00601">
<title>Multimodal Industrial Anomaly Detection via Hybrid Fusion. (arXiv:2303.00601v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00601</link>
<description rdf:parseType="Literal">&lt;p&gt;2D-based Industrial Anomaly Detection has been widely discussed, however,
multimodal industrial anomaly detection based on 3D point clouds and RGB images
still has many untouched fields. Existing multimodal industrial anomaly
detection methods directly concatenate the multimodal features, which leads to
a strong disturbance between features and harms the detection performance. In
this paper, we propose Multi-3D-Memory (M3DM), a novel multimodal anomaly
detection method with hybrid fusion scheme: firstly, we design an unsupervised
feature fusion with patch-wise contrastive learning to encourage the
interaction of different modal features; secondly, we use a decision layer
fusion with multiple memory banks to avoid loss of information and additional
novelty classifiers to make the final decision. We further propose a point
feature alignment operation to better align the point cloud and RGB features.
Extensive experiments show that our multimodal industrial anomaly detection
model outperforms the state-of-the-art (SOTA) methods on both detection and
segmentation precision on MVTec-3D AD dataset. Code is available at
https://github.com/nomewang/M3DM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jinlong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiangning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1&quot;&gt;Ran Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yabiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengjie Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07853">
<title>ReFit: A Framework for Refinement of Weakly Supervised Semantic Segmentation using Object Border Fitting for Medical Images. (arXiv:2303.07853v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07853</link>
<description rdf:parseType="Literal">&lt;p&gt;Weakly Supervised Semantic Segmentation (WSSS) relying only on image-level
supervision is a promising approach to deal with the need for Segmentation
networks, especially for generating a large number of pixel-wise masks in a
given dataset. However, most state-of-the-art image-level WSSS techniques lack
an understanding of the geometric features embedded in the images since the
network cannot derive any object boundary information from just image-level
labels. We define a boundary here as the line separating an object and its
background, or two different objects. To address this drawback, we are
proposing our novel ReFit framework, which deploys state-of-the-art class
activation maps combined with various post-processing techniques in order to
achieve fine-grained higher-accuracy segmentation masks. To achieve this, we
investigate a state-of-the-art unsupervised segmentation network that can be
used to construct a boundary map, which enables ReFit to predict object
locations with sharper boundaries. By applying our method to WSSS predictions,
we achieved up to 10% improvement over the current state-of-the-art WSSS
methods for medical imaging. The framework is open-source, to ensure that our
results are reproducible, and accessible online at
https://github.com/bharathprabakaran/ReFit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabakaran_B/0/1/0/all/0/1&quot;&gt;Bharath Srinivas Prabakaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ostrowski_E/0/1/0/all/0/1&quot;&gt;Erik Ostrowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1&quot;&gt;Muhammad Shafique&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07898">
<title>ISLE: A Framework for Image Level Semantic Segmentation Ensemble. (arXiv:2303.07898v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07898</link>
<description rdf:parseType="Literal">&lt;p&gt;One key bottleneck of employing state-of-the-art semantic segmentation
networks in the real world is the availability of training labels. Conventional
semantic segmentation networks require massive pixel-wise annotated labels to
reach state-of-the-art prediction quality. Hence, several works focus on
semantic segmentation networks trained with only image-level annotations.
However, when scrutinizing the results of state-of-the-art in more detail, we
notice that they are remarkably close to each other on average prediction
quality, different approaches perform better in different classes while
providing low quality in others. To address this problem, we propose a novel
framework, ISLE, which employs an ensemble of the &quot;pseudo-labels&quot; for a given
set of different semantic segmentation techniques on a class-wise level.
Pseudo-labels are the pixel-wise predictions of the image-level semantic
segmentation frameworks used to train the final segmentation model. Our
pseudo-labels seamlessly combine the strong points of multiple segmentation
techniques approaches to reach superior prediction quality. We reach up to 2.4%
improvement over ISLE&apos;s individual components. An exhaustive analysis was
performed to demonstrate ISLE&apos;s effectiveness over state-of-the-art frameworks
for image-level semantic segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ostrowski_E/0/1/0/all/0/1&quot;&gt;Erik Ostrowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1&quot;&gt;Muhammad Shafique&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09681">
<title>Event-based Human Pose Tracking by Spiking Spatiotemporal Transformer. (arXiv:2303.09681v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09681</link>
<description rdf:parseType="Literal">&lt;p&gt;Event camera, as an emerging biologically-inspired vision sensor for
capturing motion dynamics, presents new potential for 3D human pose tracking,
or video-based 3D human pose estimation. However, existing works in pose
tracking either require the presence of additional gray-scale images to
establish a solid starting pose, or ignore the temporal dependencies all
together by collapsing segments of event streams to form static event frames.
Meanwhile, although the effectiveness of Artificial Neural Networks (ANNs,
a.k.a. dense deep learning) has been showcased in many event-based tasks, the
use of ANNs tends to neglect the fact that compared to the dense frame-based
image sequences, the occurrence of events from an event camera is
spatiotemporally much sparser. Motivated by the above mentioned issues, we
present in this paper a dedicated end-to-end sparse deep learning approach for
event-based pose tracking: 1) to our knowledge this is the first time that 3D
human pose tracking is obtained from events only, thus eliminating the need of
accessing to any frame-based images as part of input; 2) our approach is based
entirely upon the framework of Spiking Neural Networks (SNNs), which consists
of Spike-Element-Wise (SEW) ResNet and a novel Spiking Spatiotemporal
Transformer; 3) a large-scale synthetic dataset is constructed that features a
broad and diverse set of annotated 3D human motions, as well as longer hours of
event stream data, named SynEventHPD. Empirical experiments demonstrate that,
with superior performance over the state-of-the-art (SOTA) ANNs counterparts,
our approach also achieves a significant computation reduction of 80% in FLOPS.
Furthermore, our proposed method also outperforms SOTA SNNs in the regression
task of human pose tracking. Our implementation is available at
https://github.com/JimmyZou/HumanPoseTracking_SNN and dataset will be released
upon paper acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1&quot;&gt;Shihao Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1&quot;&gt;Xinxin Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Li Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13606">
<title>Adaptive Similarity Bootstrapping for Self-Distillation based Representation Learning. (arXiv:2303.13606v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13606</link>
<description rdf:parseType="Literal">&lt;p&gt;Most self-supervised methods for representation learning leverage a
cross-view consistency objective i.e., they maximize the representation
similarity of a given image&apos;s augmented views. Recent work NNCLR goes beyond
the cross-view paradigm and uses positive pairs from different images obtained
via nearest neighbor bootstrapping in a contrastive setting. We empirically
show that as opposed to the contrastive learning setting which relies on
negative samples, incorporating nearest neighbor bootstrapping in a
self-distillation scheme can lead to a performance drop or even collapse. We
scrutinize the reason for this unexpected behavior and provide a solution. We
propose to adaptively bootstrap neighbors based on the estimated quality of the
latent space. We report consistent improvements compared to the naive
bootstrapping approach and the original baselines. Our approach leads to
performance improvements for various self-distillation method/backbone
combinations and standard downstream tasks. Our code is publicly available at
https://github.com/tileb1/AdaSim.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lebailly_T/0/1/0/all/0/1&quot;&gt;Tim Lebailly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stegmuller_T/0/1/0/all/0/1&quot;&gt;Thomas Stegm&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bozorgtabar_B/0/1/0/all/0/1&quot;&gt;Behzad Bozorgtabar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiran_J/0/1/0/all/0/1&quot;&gt;Jean-Philippe Thiran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1&quot;&gt;Tinne Tuytelaars&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.05163">
<title>Self-supervision for medical image classification: state-of-the-art performance with ~100 labeled training samples per class. (arXiv:2304.05163v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.05163</link>
<description rdf:parseType="Literal">&lt;p&gt;Is self-supervised deep learning (DL) for medical image analysis already a
serious alternative to the de facto standard of end-to-end trained supervised
DL? We tackle this question for medical image classification, with a particular
focus on one of the currently most limiting factors of the field: the
(non-)availability of labeled data. Based on three common medical imaging
modalities (bone marrow microscopy, gastrointestinal endoscopy, dermoscopy) and
publicly available data sets, we analyze the performance of self-supervised DL
within the self-distillation with no labels (DINO) framework. After learning an
image representation without use of image labels, conventional machine learning
classifiers are applied. The classifiers are fit using a systematically varied
number of labeled data (1-1000 samples per class). Exploiting the learned image
representation, we achieve state-of-the-art classification performance for all
three imaging modalities and data sets with only a fraction of between 1% and
10% of the available labeled data and about 100 labeled samples per class.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nielsen_M/0/1/0/all/0/1&quot;&gt;Maximilian Nielsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wenderoth_L/0/1/0/all/0/1&quot;&gt;Laura Wenderoth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sentker_T/0/1/0/all/0/1&quot;&gt;Thilo Sentker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werner_R/0/1/0/all/0/1&quot;&gt;Ren&amp;#xe9; Werner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07803">
<title>EGformer: Equirectangular Geometry-biased Transformer for 360 Depth Estimation. (arXiv:2304.07803v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07803</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating the depths of equirectangular (i.e., 360) images (EIs) is
challenging given the distorted 180 x 360 field-of-view, which is hard to be
addressed via convolutional neural network (CNN). Although a transformer with
global attention achieves significant improvements over CNN for EI depth
estimation task, it is computationally inefficient, which raises the need for
transformer with local attention. However, to apply local attention
successfully for EIs, a specific strategy, which addresses distorted
equirectangular geometry and limited receptive field simultaneously, is
required. Prior works have only cared either of them, resulting in
unsatisfactory depths occasionally. In this paper, we propose an
equirectangular geometry-biased transformer termed EGformer. While limiting the
computational cost and the number of network parameters, EGformer enables the
extraction of the equirectangular geometry-aware local attention with a large
receptive field. To achieve this, we actively utilize the equirectangular
geometry as the bias for the local attention instead of struggling to reduce
the distortion of EIs. As compared to the most recent EI depth estimation
studies, the proposed approach yields the best depth outcomes overall with the
lowest computational cost and the fewest parameters, demonstrating the
effectiveness of the proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_I/0/1/0/all/0/1&quot;&gt;Ilwi Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_C/0/1/0/all/0/1&quot;&gt;Chanyong Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyunku Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyuk-Jae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rhee_C/0/1/0/all/0/1&quot;&gt;Chae Eun Rhee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09479">
<title>DiFaReli: Diffusion Face Relighting. (arXiv:2304.09479v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09479</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel approach to single-view face relighting in the wild.
Handling non-diffuse effects, such as global illumination or cast shadows, has
long been a challenge in face relighting. Prior work often assumes Lambertian
surfaces, simplified lighting models or involves estimating 3D shape, albedo,
or a shadow map. This estimation, however, is error-prone and requires many
training examples with lighting ground truth to generalize well. Our work
bypasses the need for accurate estimation of intrinsic components and can be
trained solely on 2D images without any light stage data, multi-view images, or
lighting ground truth. Our key idea is to leverage a conditional diffusion
implicit model (DDIM) for decoding a disentangled light encoding along with
other encodings related to 3D shape and facial identity inferred from
off-the-shelf estimators. We also propose a novel conditioning technique that
eases the modeling of the complex interaction between light and geometry by
using a rendered shading reference to spatially modulate the DDIM. We achieve
state-of-the-art performance on standard benchmark Multi-PIE and can
photorealistically relight in-the-wild images. Please visit our page:
https://diffusion-face-relighting.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ponglertnapakorn_P/0/1/0/all/0/1&quot;&gt;Puntawat Ponglertnapakorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tritrong_N/0/1/0/all/0/1&quot;&gt;Nontawat Tritrong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suwajanakorn_S/0/1/0/all/0/1&quot;&gt;Supasorn Suwajanakorn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10226">
<title>Domain Generalization for Mammographic Image Analysis with Contrastive Learning. (arXiv:2304.10226v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10226</link>
<description rdf:parseType="Literal">&lt;p&gt;The deep learning technique has been shown to be effectively addressed
several image analysis tasks in the computer-aided diagnosis scheme for
mammography. The training of an efficacious deep learning model requires large
data with diverse styles and qualities. The diversity of data often comes from
the use of various scanners of vendors. But, in practice, it is impractical to
collect a sufficient amount of diverse data for training. To this end, a novel
contrastive learning is developed to equip the deep learning models with better
style generalization capability. Specifically, the multi-style and multi-view
unsupervised self-learning scheme is carried out to seek robust feature
embedding against style diversity as a pretrained model. Afterward, the
pretrained network is further fine-tuned to the downstream tasks, e.g., mass
detection, matching, BI-RADS rating, and breast density classification. The
proposed method has been evaluated extensively and rigorously with mammograms
from various vendor style domains and several public datasets. The experimental
results suggest that the proposed domain generalization method can effectively
improve performance of four mammographic image tasks on the data from both seen
and unseen domains, and outperform many state-of-the-art (SOTA) generalization
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zheren Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1&quot;&gt;Zhiming Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lichi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1&quot;&gt;Chenjin Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_X/0/1/0/all/0/1&quot;&gt;Xi Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dongdong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yajia Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zaiyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chunling Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dinggang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jie-Zhi Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.01905">
<title>Localization using Multi-Focal Spatial Attention for Masked Face Recognition. (arXiv:2305.01905v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.01905</link>
<description rdf:parseType="Literal">&lt;p&gt;Since the beginning of world-wide COVID-19 pandemic, facial masks have been
recommended to limit the spread of the disease. However, these masks hide
certain facial attributes. Hence, it has become difficult for existing face
recognition systems to perform identity verification on masked faces. In this
context, it is necessary to develop masked Face Recognition (MFR) for
contactless biometric recognition systems. Thus, in this paper, we propose
Complementary Attention Learning and Multi-Focal Spatial Attention that
precisely removes masked region by training complementary spatial attention to
focus on two distinct regions: masked regions and backgrounds. In our method,
standard spatial attention and networks focus on unmasked regions, and extract
mask-invariant features while minimizing the loss of the conventional Face
Recognition (FR) performance. For conventional FR, we evaluate the performance
on the IJB-C, Age-DB, CALFW, and CPLFW datasets. We evaluate the MFR
performance on the ICCV2021-MFR/Insightface track, and demonstrate the improved
performance on the both MFR and FR datasets. Additionally, we empirically
verify that spatial attention of proposed method is more precisely activated in
unmasked regions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1&quot;&gt;Yooshin Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1&quot;&gt;Hanbyel Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1&quot;&gt;Hyeong Gwon Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1&quot;&gt;Jaesung Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_D/0/1/0/all/0/1&quot;&gt;Dongmin Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1&quot;&gt;JungWoo Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junmo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09241">
<title>Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples. (arXiv:2305.09241v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09241</link>
<description rdf:parseType="Literal">&lt;p&gt;Safeguarding data from unauthorized exploitation is vital for privacy and
security, especially in recent rampant research in security breach such as
adversarial/membership attacks. To this end, \textit{unlearnable examples}
(UEs) have been recently proposed as a compelling protection, by adding
imperceptible perturbation to data so that models trained on them cannot
classify them accurately on original clean distribution. Unfortunately, we find
UEs provide a false sense of security, because they cannot stop unauthorized
users from utilizing other unprotected data to remove the protection, by
turning unlearnable data into learnable again. Motivated by this observation,
we formally define a new threat by introducing \textit{learnable unauthorized
examples} (LEs) which are UEs with their protection removed. The core of this
approach is a novel purification process that projects UEs onto the manifold of
LEs. This is realized by a new joint-conditional diffusion model which denoises
UEs conditioned on the pixel and perceptual similarity between UEs and LEs.
Extensive experiments demonstrate that LE delivers state-of-the-art countering
performance against both supervised UEs and unsupervised UEs in various
scenarios, which is the first generalizable countermeasure to UEs across
supervised learning and unsupervised learning. Our code is available at
\url{https://github.com/jiangw-0/LE_JCDP}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diao_Y/0/1/0/all/0/1&quot;&gt;Yunfeng Diao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;He Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jianxin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1&quot;&gt;Richang Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00526">
<title>Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering. (arXiv:2306.00526v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00526</link>
<description rdf:parseType="Literal">&lt;p&gt;Layout-aware pre-trained models has achieved significant progress on document
image question answering. They introduce extra learnable modules into existing
language models to capture layout information within document images from text
bounding box coordinates obtained by OCR tools. However, extra modules
necessitate pre-training on extensive document images. This prevents these
methods from directly utilizing off-the-shelf instruction-tuning language
foundation models, which have recently shown promising potential in zero-shot
learning. Instead, in this paper, we find that instruction-tuning language
models like Claude and ChatGPT can understand layout by spaces and line breaks.
Based on this observation, we propose the LAyout and Task aware Instruction
Prompt (LATIN-Prompt), which consists of layout-aware document content and
task-aware instruction. Specifically, the former uses appropriate spaces and
line breaks to recover the layout information among text segments obtained by
OCR tools, and the latter ensures that generated answers adhere to formatting
requirements. Moreover, we propose the LAyout and Task aware Instruction Tuning
(LATIN-Tuning) to improve the performance of small instruction-tuning models
like Alpaca. Experimental results show that LATIN-Prompt enables zero-shot
performance of Claude and ChatGPT to be comparable to the fine-tuning
performance of SOTAs on document image question answering, and LATIN-Tuning
enhances the zero-shot performance of Alpaca significantly. For example,
LATIN-Prompt improves the performance of Claude and ChatGPT on DocVQA by 263%
and 20% respectively. LATIN-Tuning improves the performance of Alpaca on DocVQA
by 87.7%. Quantitative and qualitative analyses demonstrate the effectiveness
of LATIN-Prompt and LATIN-Tuning. We provide the code in supplementary and will
release it to facilitate future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenjin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1&quot;&gt;Yixin Ou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yin Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01188">
<title>Event-based Stereo Visual Odometry with Native Temporal Resolution via Continuous-time Gaussian Process Regression. (arXiv:2306.01188v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01188</link>
<description rdf:parseType="Literal">&lt;p&gt;Event-based cameras asynchronously capture individual visual changes in a
scene. This makes them more robust than traditional frame-based cameras to
highly dynamic motions and poor illumination. It also means that every
measurement in a scene can occur at a unique time.
&lt;/p&gt;
&lt;p&gt;Handling these different measurement times is a major challenge of using
event-based cameras. It is often addressed in visual odometry (VO) pipelines by
approximating temporally close measurements as occurring at one common time.
This grouping simplifies the estimation problem but, absent additional sensors,
sacrifices the inherent temporal resolution of event-based cameras.
&lt;/p&gt;
&lt;p&gt;This paper instead presents a complete stereo VO pipeline that estimates
directly with individual event-measurement times without requiring any grouping
or approximation in the estimation state. It uses continuous-time trajectory
estimation to maintain the temporal fidelity and asynchronous nature of
event-based cameras through Gaussian process regression with a physically
motivated prior. Its performance is evaluated on the MVSEC dataset, where it
achieves 7.9e-3 and 5.9e-3 RMS relative error on two independent sequences,
outperforming the existing publicly available event-based stereo VO pipeline by
two and four times, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gammell_J/0/1/0/all/0/1&quot;&gt;Jonathan D. Gammell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06145">
<title>LDMRes-Net: Enabling Efficient Medical Image Segmentation on IoT and Edge Platforms. (arXiv:2306.06145v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06145</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we propose LDMRes-Net, a lightweight dual-multiscale residual
block-based computational neural network tailored for medical image
segmentation on IoT and edge platforms. Conventional U-Net-based models face
challenges in meeting the speed and efficiency demands of real-time clinical
applications, such as disease monitoring, radiation therapy, and image-guided
surgery. LDMRes-Net overcomes these limitations with its remarkably low number
of learnable parameters (0.072M), making it highly suitable for
resource-constrained devices. The model&apos;s key innovation lies in its dual
multi-residual block architecture, which enables the extraction of refined
features on multiple scales, enhancing overall segmentation performance. To
further optimize efficiency, the number of filters is carefully selected to
prevent overlap, reduce training time, and improve computational efficiency.
The study includes comprehensive evaluations, focusing on segmentation of the
retinal image of vessels and hard exudates crucial for the diagnosis and
treatment of ophthalmology. The results demonstrate the robustness,
generalizability, and high segmentation accuracy of LDMRes-Net, positioning it
as an efficient tool for accurate and rapid medical image segmentation in
diverse clinical applications, particularly on IoT and edge platforms. Such
advances hold significant promise for improving healthcare outcomes and
enabling real-time medical image analysis in resource-limited settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Iqbal_S/0/1/0/all/0/1&quot;&gt;Shahzaib Iqbal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_T/0/1/0/all/0/1&quot;&gt;Tariq M. Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Naqvi_S/0/1/0/all/0/1&quot;&gt;Syed S. Naqvi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Usman_M/0/1/0/all/0/1&quot;&gt;Muhammad Usman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Razzak_I/0/1/0/all/0/1&quot;&gt;Imran Razzak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07050">
<title>Revisiting Token Pruning for Object Detection and Instance Segmentation. (arXiv:2306.07050v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07050</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) have shown impressive performance in computer
vision, but their high computational cost, quadratic in the number of tokens,
limits their adoption in computation-constrained applications. However, this
large number of tokens may not be necessary, as not all tokens are equally
important. In this paper, we investigate token pruning to accelerate inference
for object detection and instance segmentation, extending prior works from
image classification. Through extensive experiments, we offer four insights for
dense tasks: (i) tokens should not be completely pruned and discarded, but
rather preserved in the feature maps for later use. (ii) reactivating
previously pruned tokens can further enhance model performance. (iii) a dynamic
pruning rate based on images is better than a fixed pruning rate. (iv) a
lightweight, 2-layer MLP can effectively prune tokens, achieving accuracy
comparable with complex gating networks with a simpler design. We evaluate the
impact of these design choices on COCO dataset and present a method integrating
these insights that outperforms prior art token pruning models, significantly
reducing performance drop from ~1.5 mAP to ~0.3 mAP for both boxes and masks.
Compared to the dense counterpart that uses all tokens, our method achieves up
to 34% faster inference speed for the whole network and 46% for the backbone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yifei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gehrig_M/0/1/0/all/0/1&quot;&gt;Mathias Gehrig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Messikommer_N/0/1/0/all/0/1&quot;&gt;Nico Messikommer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cannici_M/0/1/0/all/0/1&quot;&gt;Marco Cannici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1&quot;&gt;Davide Scaramuzza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12760">
<title>Blended-NeRF: Zero-Shot Object Generation and Blending in Existing Neural Radiance Fields. (arXiv:2306.12760v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12760</link>
<description rdf:parseType="Literal">&lt;p&gt;Editing a local region or a specific object in a 3D scene represented by a
NeRF or consistently blending a new realistic object into the scene is
challenging, mainly due to the implicit nature of the scene representation. We
present Blended-NeRF, a robust and flexible framework for editing a specific
region of interest in an existing NeRF scene, based on text prompts, along with
a 3D ROI box. Our method leverages a pretrained language-image model to steer
the synthesis towards a user-provided text prompt, along with a 3D MLP model
initialized on an existing NeRF scene to generate the object and blend it into
a specified region in the original scene. We allow local editing by localizing
a 3D ROI box in the input scene, and blend the content synthesized inside the
ROI with the existing scene using a novel volumetric blending technique. To
obtain natural looking and view-consistent results, we leverage existing and
new geometric priors and 3D augmentations for improving the visual fidelity of
the final result. We test our framework both qualitatively and quantitatively
on a variety of real 3D scenes and text prompts, demonstrating realistic
multi-view consistent results with much flexibility and diversity compared to
the baselines. Finally, we show the applicability of our framework for several
3D editing applications, including adding new objects to a scene,
removing/replacing/altering existing objects, and texture conversion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gordon_O/0/1/0/all/0/1&quot;&gt;Ori Gordon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avrahami_O/0/1/0/all/0/1&quot;&gt;Omri Avrahami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lischinski_D/0/1/0/all/0/1&quot;&gt;Dani Lischinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13166">
<title>A Sparse Graph Formulation for Efficient Spectral Image Segmentation. (arXiv:2306.13166v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13166</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral Clustering is one of the most traditional methods to solve
segmentation problems. Based on Normalized Cuts, it aims at partitioning an
image using an objective function defined by a graph. Despite their
mathematical attractiveness, spectral approaches are traditionally neglected by
the scientific community due to their practical issues and underperformance. In
this paper, we adopt a sparse graph formulation based on the inclusion of extra
nodes to a simple grid graph. While the grid encodes the pixel spatial
disposition, the extra nodes account for the pixel color data. Applying the
original Normalized Cuts algorithm to this graph leads to a simple and scalable
method for spectral image segmentation, with an interpretable solution. Our
experiments also demonstrate that our proposed methodology over performs both
traditional and modern unsupervised algorithms for segmentation in both real
and synthetic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palnitkar_R/0/1/0/all/0/1&quot;&gt;Rahul Palnitkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neto_J/0/1/0/all/0/1&quot;&gt;Jeova Farias Sales Rocha Neto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13455">
<title>DreamEditor: Text-Driven 3D Scene Editing with Neural Fields. (arXiv:2306.13455v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13455</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural fields have achieved impressive advancements in view synthesis and
scene reconstruction. However, editing these neural fields remains challenging
due to the implicit encoding of geometry and texture information. In this
paper, we propose DreamEditor, a novel framework that enables users to perform
controlled editing of neural fields using text prompts. By representing scenes
as mesh-based neural fields, DreamEditor allows localized editing within
specific regions. DreamEditor utilizes the text encoder of a pretrained
text-to-Image diffusion model to automatically identify the regions to be
edited based on the semantics of the text prompts. Subsequently, DreamEditor
optimizes the editing region and aligns its geometry and texture with the text
prompts through score distillation sampling [29]. Extensive experiments have
demonstrated that DreamEditor can accurately edit neural fields of real-world
scenes according to the given text prompts while ensuring consistency in
irrelevant areas. DreamEditor generates highly realistic textures and geometry,
significantly surpassing previous works in both quantitative and qualitative
evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1&quot;&gt;Jingyu Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanbin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15955">
<title>Understanding Prompt Tuning for V-L Models Through the Lens of Neural Collapse. (arXiv:2306.15955v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15955</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale vision-language (V-L) models have demonstrated remarkable
generalization capabilities for downstream tasks through prompt tuning.
However, the mechanisms behind the learned text representations are unknown,
limiting further generalization gains, especially under class imbalance
scenarios. Recent advances in the neural collapse (NC) phenomenon of
vision-only models suggest that the optimal representation structure is the
simplex ETF, which paves the way to study representations in V-L models. In
this paper, we make the first attempt to use NC for examining the
representations in V-L models via prompt tuning. It is found that NC optimality
of text-to-image representations shows a positive correlation with downstream
generalizability, which is more severe under class imbalance settings. To
improve the representations, we propose Neural-collapse-anchored Prompt Tuning
(NPT), a novel method that learns prompts with text and image representations
that satisfy the same simplex ETF. NPT incorporates two regularization terms:
language-modality collapse and multi-modality isomorphism; and it is compatible
with other prompt tuning methods. Extensive experiments show that NPT can
consistently help to improve existing prompt tuning techniques across 11
datasets for both balanced and imbalanced settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Didi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zexi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Junkun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1&quot;&gt;Yunfeng Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiashuo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1&quot;&gt;Kun Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yinchuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chao Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17206">
<title>FarSight: A Physics-Driven Whole-Body Biometric System at Large Distance and Altitude. (arXiv:2306.17206v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17206</link>
<description rdf:parseType="Literal">&lt;p&gt;Whole-body biometric recognition is an important area of research due to its
vast applications in law enforcement, border security, and surveillance. This
paper presents the end-to-end design, development and evaluation of FarSight,
an innovative software system designed for whole-body (fusion of face, gait and
body shape) biometric recognition. FarSight accepts videos from elevated
platforms and drones as input and outputs a candidate list of identities from a
gallery. The system is designed to address several challenges, including (i)
low-quality imagery, (ii) large yaw and pitch angles, (iii) robust feature
extraction to accommodate large intra-person variabilities and large
inter-person similarities, and (iv) the large domain gap between training and
test sets. FarSight combines the physics of imaging and deep learning models to
enhance image restoration and biometric feature encoding. We test FarSight&apos;s
effectiveness using the newly acquired IARPA Biometric Recognition and
Identification at Altitude and Range (BRIAR) dataset. Notably, FarSight
demonstrated a substantial performance increase on the BRIAR dataset, with
gains of +11.82% Rank-20 identification and +11.3% TAR@1% FAR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Feng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashbaugh_R/0/1/0/all/0/1&quot;&gt;Ryan Ashbaugh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chimitt_N/0/1/0/all/0/1&quot;&gt;Nicholas Chimitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassan_N/0/1/0/all/0/1&quot;&gt;Najmul Hassan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassani_A/0/1/0/all/0/1&quot;&gt;Ali Hassani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1&quot;&gt;Ajay Jaiswal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minchul Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perry_C/0/1/0/all/0/1&quot;&gt;Christopher Perry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yiyang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varghaei_P/0/1/0/all/0/1&quot;&gt;Pegah Varghaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xingguang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1&quot;&gt;Stanley Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1&quot;&gt;Arun Ross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Humphrey Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Anil Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00586">
<title>ClipSitu: Effectively Leveraging CLIP for Conditional Predictions in Situation Recognition. (arXiv:2307.00586v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00586</link>
<description rdf:parseType="Literal">&lt;p&gt;Situation Recognition is the task of generating a structured summary of what
is happening in an image using an activity verb and the semantic roles played
by actors and objects. In this task, the same activity verb can describe a
diverse set of situations as well as the same actor or object category can play
a diverse set of semantic roles depending on the situation depicted in the
image. Hence a situation recognition model needs to understand the context of
the image and the visual-linguistic meaning of semantic roles. Therefore, we
leverage the CLIP foundational model that has learned the context of images via
language descriptions. We show that deeper-and-wider multi-layer perceptron
(MLP) blocks obtain noteworthy results for the situation recognition task by
using CLIP image and text embedding features and it even outperforms the
state-of-the-art CoFormer, a Transformer-based model, thanks to the external
implicit visual-linguistic knowledge encapsulated by CLIP and the expressive
power of modern MLP block designs. Motivated by this, we design a
cross-attention-based Transformer using CLIP visual tokens that model the
relation between textual roles and visual entities. Our cross-attention-based
Transformer known as ClipSitu XTF outperforms existing state-of-the-art by a
large margin of 14.1\% on semantic role labelling (value) for top-1 accuracy
using imSitu dataset. {Similarly, our ClipSitu XTF obtains state-of-the-art
situation localization performance.} We will make the code publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1&quot;&gt;Debaditya Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_D/0/1/0/all/0/1&quot;&gt;Dhruv Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1&quot;&gt;Basura Fernando&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02321">
<title>MSViT: Dynamic Mixed-Scale Tokenization for Vision Transformers. (arXiv:2307.02321v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02321</link>
<description rdf:parseType="Literal">&lt;p&gt;The input tokens to Vision Transformers carry little semantic meaning as they
are defined as regular equal-sized patches of the input image, regardless of
its content. However, processing uniform background areas of an image should
not necessitate as much compute as dense, cluttered areas. To address this
issue, we propose a dynamic mixed-scale tokenization scheme for ViT, MSViT. Our
method introduces a conditional gating mechanism that selects the optimal token
scale for every image region, such that the number of tokens is dynamically
determined per input. In addition, to enhance the conditional behavior of the
gate during training, we introduce a novel generalization of the batch-shaping
loss. We show that our gating module is able to learn meaningful semantics
despite operating locally at the coarse patch-level. The proposed gating module
is lightweight, agnostic to the choice of transformer backbone, and trained
within a few epochs with little training overhead. Furthermore, in contrast to
token pruning, MSViT does not lose information about the input, thus can be
readily applied for dense tasks. We validate MSViT on the tasks of
classification and segmentation where it leads to improved accuracy-complexity
trade-off.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Havtorn_J/0/1/0/all/0/1&quot;&gt;Jakob Drachmann Havtorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Royer_A/0/1/0/all/0/1&quot;&gt;Amelie Royer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blankevoort_T/0/1/0/all/0/1&quot;&gt;Tijmen Blankevoort&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bejnordi_B/0/1/0/all/0/1&quot;&gt;Babak Ehteshami Bejnordi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05766">
<title>Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting. (arXiv:2307.05766v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05766</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiology reporting is a crucial part of the communication between
radiologists and other medical professionals, but it can be time-consuming and
error-prone. One approach to alleviate this is structured reporting, which
saves time and enables a more accurate evaluation than free-text reports.
However, there is limited research on automating structured reporting, and no
public benchmark is available for evaluating and comparing different methods.
To close this gap, we introduce Rad-ReStruct, a new benchmark dataset that
provides fine-grained, hierarchically ordered annotations in the form of
structured reports for X-Ray images. We model the structured reporting task as
hierarchical visual question answering (VQA) and propose hi-VQA, a novel method
that considers prior context in the form of previously asked questions and
answers for populating a structured radiology report. Our experiments show that
hi-VQA achieves competitive performance to the state-of-the-art on the medical
VQA benchmark VQARad while performing best among methods without
domain-specific vision-language pretraining and provides a strong baseline on
Rad-ReStruct. Our work represents a significant step towards the automated
population of structured radiology reports and provides a valuable first
benchmark for future research in this area. Our dataset and code is available
at https://github.com/ChantalMP/Rad-ReStruct.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pellegrini_C/0/1/0/all/0/1&quot;&gt;Chantal Pellegrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keicher_M/0/1/0/all/0/1&quot;&gt;Matthias Keicher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozsoy_E/0/1/0/all/0/1&quot;&gt;Ege &amp;#xd6;zsoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10829">
<title>Exact Diffusion Inversion via Bi-directional Integration Approximation. (arXiv:2307.10829v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10829</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, various methods have been proposed to address the inconsistency
issue of DDIM inversion to enable image editing, such as EDICT [36] and
Null-text inversion [22]. However, the above methods introduce considerable
computational overhead. In this paper, we propose a new technique, named
\emph{bi-directional integration approximation} (BDIA), to perform exact
diffusion inversion with neglible computational overhead. Suppose we would like
to estimate the next diffusion state $\boldsymbol{z}_{i-1}$ at timestep $t_i$
with the historical information $(i,\boldsymbol{z}_i)$ and
$(i+1,\boldsymbol{z}_{i+1})$. We first obtain the estimated Gaussian noise
$\hat{\boldsymbol{\epsilon}}(\boldsymbol{z}_i,i)$, and then apply the DDIM
update procedure twice for approximating the ODE integration over the next
time-slot $[t_i, t_{i-1}]$ in the forward manner and the previous time-slot
$[t_i, t_{t+1}]$ in the backward manner. The DDIM step for the previous
time-slot is used to refine the integration approximation made earlier when
computing $\boldsymbol{z}_i$. A nice property of BDIA-DDIM is that the update
expression for $\boldsymbol{z}_{i-1}$ is a linear combination of
$(\boldsymbol{z}_{i+1}, \boldsymbol{z}_i,
\hat{\boldsymbol{\epsilon}}(\boldsymbol{z}_i,i))$. This allows for exact
backward computation of $\boldsymbol{z}_{i+1}$ given $(\boldsymbol{z}_i,
\boldsymbol{z}_{i-1})$, thus leading to exact diffusion inversion. It is
demonstrated with experiments that (round-trip) BDIA-DDIM is particularly
effective for image editing. Our experiments further show that BDIA-DDIM
produces markedly better image sampling qualities than DDIM for text-to-image
generation.
&lt;/p&gt;
&lt;p&gt;BDIA can also be applied to improve the performance of other ODE solvers in
addition to DDIM. In our work, it is found that applying BDIA to the EDM
sampling procedure produces new SOTA performance over CIFAR10.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guoqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_J/0/1/0/all/0/1&quot;&gt;J. P. Lewis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleijn_W/0/1/0/all/0/1&quot;&gt;W. Bastiaan Kleijn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12900">
<title>Automotive Object Detection via Learning Sparse Events by Spiking Neurons. (arXiv:2307.12900v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12900</link>
<description rdf:parseType="Literal">&lt;p&gt;Event-based sensors, distinguished by their high temporal resolution of 1
{\mu}s and a dynamic range of 120 dB, stand out as ideal tools for deployment
in fast-paced settings like vehicles and drones. Traditional object detection
techniques that utilize Artificial Neural Networks (ANNs) face challenges due
to the sparse and asynchronous nature of the events these sensors capture. In
contrast, Spiking Neural Networks (SNNs) offer a promising alternative,
providing a temporal representation that is inherently aligned with event-based
data. This paper explores the unique membrane potential dynamics of SNNs and
their ability to modulate sparse events. We introduce an innovative
spike-triggered adaptive threshold mechanism designed for stable training.
Building on these insights, we present a specialized spiking feature pyramid
network (SpikeFPN) optimized for automotive event based object detection.
Comprehensive evaluations demonstrate that SpikeFPN surpasses both traditional
SNNs and advanced ANNs enhanced with attention mechanisms. Evidently, SpikeFPN
achieves a mean Average Precision (mAP) of 0.477 on the GEN1 Automotive
Detection (GAD) benchmark dataset, marking a significant increase of 9.7% over
the previous best SNN. Moreover, the efficient design of SpikeFPN ensures
robust performance while optimizing computational resources, attributed to its
innate sparse computation capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanchen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leng_L/0/1/0/all/0/1&quot;&gt;Luziwei Leng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_K/0/1/0/all/0/1&quot;&gt;Kaiwei Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qinghai Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1&quot;&gt;Jianxing Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1&quot;&gt;Ran Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14971">
<title>Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models. (arXiv:2307.14971v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14971</link>
<description rdf:parseType="Literal">&lt;p&gt;With the overwhelming trend of mask image modeling led by MAE, generative
pre-training has shown a remarkable potential to boost the performance of
fundamental models in 2D vision. However, in 3D vision, the over-reliance on
Transformer-based backbones and the unordered nature of point clouds have
restricted the further development of generative pre-training. In this paper,
we propose a novel 3D-to-2D generative pre-training method that is adaptable to
any point cloud model. We propose to generate view images from different
instructed poses via the cross-attention mechanism as the pre-training scheme.
Generating view images has more precise supervision than its point cloud
counterpart, thus assisting 3D backbones to have a finer comprehension of the
geometrical structure and stereoscopic relations of the point cloud.
Experimental results have proved the superiority of our proposed 3D-to-2D
generative pre-training over previous pre-training methods. Our method is also
effective in boosting the performance of architecture-oriented approaches,
achieving state-of-the-art performance when fine-tuning on ScanObjectNN
classification and ShapeNetPart segmentation tasks. Code is available at
https://github.com/wangzy22/TAP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xumin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1&quot;&gt;Yongming Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00452">
<title>A Majority Invariant Approach to Patch Robustness Certification for Deep Learning Models. (arXiv:2308.00452v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00452</link>
<description rdf:parseType="Literal">&lt;p&gt;Patch robustness certification ensures no patch within a given bound on a
sample can manipulate a deep learning model to predict a different label.
However, existing techniques cannot certify samples that cannot meet their
strict bars at the classifier or patch region levels. This paper proposes
MajorCert. MajorCert firstly finds all possible label sets manipulatable by the
same patch region on the same sample across the underlying classifiers, then
enumerates their combinations element-wise, and finally checks whether the
majority invariant of all these combinations is intact to certify samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qilin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhengyuan Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haipeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1&quot;&gt;W.K. Chan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02335">
<title>RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification. (arXiv:2308.02335v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02335</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph classification is a crucial task in many real-world multimedia
applications, where graphs can represent various multimedia data types such as
images, videos, and social networks. Previous efforts have applied graph neural
networks (GNNs) in balanced situations where the class distribution is
balanced. However, real-world data typically exhibit long-tailed class
distributions, resulting in a bias towards the head classes when using GNNs and
limited generalization ability over the tail classes. Recent approaches mainly
focus on re-balancing different classes during model training, which fails to
explicitly introduce new knowledge and sacrifices the performance of the head
classes. To address these drawbacks, we propose a novel framework called
Retrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust feature
extractor and an unbiased classifier in a decoupled manner. In the feature
extractor training stage, we develop a graph retrieval module to search for
relevant graphs that directly enrich the intra-class diversity for the tail
classes. Moreover, we innovatively optimize a category-centered supervised
contrastive loss to obtain discriminative representations, which is more
suitable for long-tailed scenarios. In the classifier fine-tuning stage, we
balance the classifier weights with two weight regularization techniques, i.e.,
Max-norm and weight decay. Experiments on various popular benchmarks verify the
superiority of the proposed method against state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1&quot;&gt;Zhengyang Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_W/0/1/0/all/0/1&quot;&gt;Wei Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yifang Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08197">
<title>Self-Reference Deep Adaptive Curve Estimation for Low-Light Image Enhancement. (arXiv:2308.08197v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08197</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a 2-stage low-light image enhancement method called
Self-Reference Deep Adaptive Curve Estimation (Self-DACE). In the first stage,
we present an intuitive, lightweight, fast, and unsupervised luminance
enhancement algorithm. The algorithm is based on a novel low-light enhancement
curve that can be used to locally boost image brightness. We also propose a new
loss function with a simplified physical model designed to preserve natural
images&apos; color, structure, and fidelity. We use a vanilla CNN to map each pixel
through deep Adaptive Adjustment Curves (AAC) while preserving the local image
structure. Secondly, we introduce the corresponding denoising scheme to remove
the latent noise in the darkness. We approximately model the noise in the dark
and deploy a Denoising-Net to estimate and remove the noise after the first
stage. Exhaustive qualitative and quantitative analysis shows that our method
outperforms existing state-of-the-art algorithms on multiple real-world
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Jianyu Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chenhao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yixuan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Swierczynski_P/0/1/0/all/0/1&quot;&gt;Piotr Swierczynski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14469">
<title>Pixel-Aware Stable Diffusion for Realistic Image Super-resolution and Personalized Stylization. (arXiv:2308.14469v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14469</link>
<description rdf:parseType="Literal">&lt;p&gt;Realistic image super-resolution (Real-ISR) aims to reproduce perceptually
realistic image details from a low-quality input. The commonly used adversarial
training based Real-ISR methods often introduce unnatural visual artifacts and
fail to generate realistic textures for natural scene images. The recently
developed generative stable diffusion models provide a potential solution to
Real-ISR with pre-learned strong image priors. However, the existing methods
along this line either fail to keep faithful pixel-wise image structures or
resort to extra skipped connections to reproduce details, which requires
additional training in image space and limits their extension to other related
tasks in latent space such as image stylization. In this work, we propose a
pixel-aware stable diffusion (PASD) network to achieve robust Real-ISR as well
as personalized stylization. In specific, a pixel-aware cross attention module
is introduced to enable diffusion models perceiving image local structures in
pixel-wise level, while a degradation removal module is used to extract
degradation insensitive features to guide the diffusion process together with
image high level information. By simply replacing the base diffusion model with
a personalized one, our method can generate diverse stylized images without the
need to collect pairwise training data. PASD can be easily integrated into
existing diffusion models such as Stable Diffusion. Experiments on Real-ISR and
personalized stylization demonstrate the effectiveness of our proposed
approach. The source code and models can be found at
\url{https://github.com/yangxy/PASD}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1&quot;&gt;Peiran Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xuansong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15536">
<title>DebSDF: Delving into the Details and Bias of Neural Indoor Scene Reconstruction. (arXiv:2308.15536v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15536</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the neural implicit surface has emerged as a powerful
representation for multi-view surface reconstruction due to its simplicity and
state-of-the-art performance. However, reconstructing smooth and detailed
surfaces in indoor scenes from multi-view images presents unique challenges.
Indoor scenes typically contain large texture-less regions, making the
photometric loss unreliable for optimizing the implicit surface. Previous work
utilizes monocular geometry priors to improve the reconstruction in indoor
scenes. However, monocular priors often contain substantial errors in thin
structure regions due to domain gaps and the inherent inconsistencies when
derived independently from different views. This paper presents \textbf{DebSDF}
to address these challenges, focusing on the utilization of uncertainty in
monocular priors and the bias in SDF-based volume rendering. We propose an
uncertainty modeling technique that associates larger uncertainties with larger
errors in the monocular priors. High-uncertainty priors are then excluded from
optimization to prevent bias. This uncertainty measure also informs an
importance-guided ray sampling and adaptive smoothness regularization,
enhancing the learning of fine structures. We further introduce a bias-aware
signed distance function to density transformation that takes into account the
curvature and the angle between the view direction and the SDF normals to
reconstruct fine details better. Our approach has been validated through
extensive experiments on several challenging datasets, demonstrating improved
qualitative and quantitative results in reconstructing thin structures in
indoor scenes, thereby outperforming previous work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yuting Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jingwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zehao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shenghua Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16215">
<title>Deep Video Codec Control. (arXiv:2308.16215v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16215</link>
<description rdf:parseType="Literal">&lt;p&gt;Lossy video compression is commonly used when transmitting and storing video
data. Unified video codecs (e.g., H.264 or H.265) remain the de facto standard,
despite the availability of advanced (neural) compression approaches.
Transmitting videos in the face of dynamic network bandwidth conditions
requires video codecs to adapt to vastly different compression strengths. Rate
control modules augment the codec&apos;s compression such that bandwidth constraints
are satisfied and video distortion is minimized. While, both standard video
codes and their rate control modules are developed to minimize video distortion
w.r.t. human quality assessment, preserving the downstream performance of deep
vision models is not considered. In this paper, we present the first end-to-end
learnable deep video codec control considering both bandwidth constraints and
downstream vision performance, while not breaking existing standardization. We
demonstrate for two common vision tasks (semantic segmentation and optical flow
estimation) and on two different datasets that our deep codec control better
preserves downstream performance than using 2-pass average bit rate control
while meeting dynamic bandwidth constraints and adhering to standardizations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Reich_C/0/1/0/all/0/1&quot;&gt;Christoph Reich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Debnath_B/0/1/0/all/0/1&quot;&gt;Biplob Debnath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Patel_D/0/1/0/all/0/1&quot;&gt;Deep Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prangemeier_T/0/1/0/all/0/1&quot;&gt;Tim Prangemeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chakradhar_S/0/1/0/all/0/1&quot;&gt;Srimat Chakradhar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00398">
<title>VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation. (arXiv:2309.00398v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00398</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present VideoGen, a text-to-video generation approach,
which can generate a high-definition video with high frame fidelity and strong
temporal consistency using reference-guided latent diffusion. We leverage an
off-the-shelf text-to-image generation model, e.g., Stable Diffusion, to
generate an image with high content quality from the text prompt, as a
reference image to guide video generation. Then, we introduce an efficient
cascaded latent diffusion module conditioned on both the reference image and
the text prompt, for generating latent video representations, followed by a
flow-based temporal upsampling step to improve the temporal resolution.
Finally, we map latent video representations into a high-definition video
through an enhanced video decoder. During training, we use the first frame of a
ground-truth video as the reference image for training the cascaded latent
diffusion module. The main characterises of our approach include: the reference
image generated by the text-to-image model improves the visual fidelity; using
it as the condition makes the diffusion model focus more on learning the video
dynamics; and the video decoder is trained over unlabeled video data, thus
benefiting from high-quality easily-available videos. VideoGen sets a new
state-of-the-art in text-to-video generation in terms of both qualitative and
quantitative evaluation. See \url{https://videogen.github.io/VideoGen/} for
more samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1&quot;&gt;Wenqing Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Ye Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Weihang Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fanglong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Haocheng Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1&quot;&gt;Errui Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingdong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00938">
<title>Exploring the Robustness of Human Parsers Towards Common Corruptions. (arXiv:2309.00938v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00938</link>
<description rdf:parseType="Literal">&lt;p&gt;Human parsing aims to segment each pixel of the human image with fine-grained
semantic categories. However, current human parsers trained with clean data are
easily confused by numerous image corruptions such as blur and noise. To
improve the robustness of human parsers, in this paper, we construct three
corruption robustness benchmarks, termed LIP-C, ATR-C, and
Pascal-Person-Part-C, to assist us in evaluating the risk tolerance of human
parsing models. Inspired by the data augmentation strategy, we propose a novel
heterogeneous augmentation-enhanced mechanism to bolster robustness under
commonly corrupted conditions. Specifically, two types of data augmentations
from different views, i.e., image-aware augmentation and model-aware
image-to-image transformation, are integrated in a sequential manner for
adapting to unforeseen image corruptions. The image-aware augmentation can
enrich the high diversity of training images with the help of common image
operations. The model-aware augmentation strategy that improves the diversity
of input data by considering the model&apos;s randomness. The proposed method is
model-agnostic, and it can plug and play into arbitrary state-of-the-art human
parsing frameworks. The experimental results show that the proposed method
demonstrates good universality which can improve the robustness of the human
parsing models and even the semantic segmentation models when facing various
image common corruptions. Meanwhile, it can still obtain approximate
performance on clean data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sanyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xiaochun Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1&quot;&gt;Guo-Jun Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01406">
<title>Learning Residual Elastic Warps for Image Stitching under Dirichlet Boundary Condition. (arXiv:2309.01406v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01406</link>
<description rdf:parseType="Literal">&lt;p&gt;Trendy suggestions for learning-based elastic warps enable the deep image
stitchings to align images exposed to large parallax errors. Despite the
remarkable alignments, the methods struggle with occasional holes or
discontinuity between overlapping and non-overlapping regions of a target image
as the applied training strategy mostly focuses on overlap region alignment. As
a result, they require additional modules such as seam finder and image
inpainting for hiding discontinuity and filling holes, respectively. In this
work, we suggest Recurrent Elastic Warps (REwarp) that address the problem with
Dirichlet boundary condition and boost performances by residual learning for
recurrent misalign correction. Specifically, REwarp predicts a homography and a
Thin-plate Spline (TPS) under the boundary constraint for discontinuity and
hole-free image stitching. Our experiments show the favorable aligns and the
competitive computational costs of REwarp compared to the existing stitching
methods. Our source code is available at https://github.com/minshu-kim/REwarp.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yongjun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1&quot;&gt;Woo Kyoung Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1&quot;&gt;Kyong Hwan Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01409">
<title>Implicit Neural Image Stitching With Enhanced and Blended Feature Reconstruction. (arXiv:2309.01409v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01409</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing frameworks for image stitching often provide visually reasonable
stitchings. However, they suffer from blurry artifacts and disparities in
illumination, depth level, etc. Although the recent learning-based stitchings
relax such disparities, the required methods impose sacrifice of image
qualities failing to capture high-frequency details for stitched images. To
address the problem, we propose a novel approach, implicit Neural Image
Stitching (NIS) that extends arbitrary-scale super-resolution. Our method
estimates Fourier coefficients of images for quality-enhancing warps. Then, the
suggested model blends color mismatches and misalignment in the latent space
and decodes the features into RGB values of stitched images. Our experiments
show that our approach achieves improvement in resolving the low-definition
imaging of the previous deep image stitching with favorable accelerated
image-enhancing methods. Our source code is available at
https://github.com/minshu-kim/NIS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaewon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1&quot;&gt;Byeonghun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1&quot;&gt;Sunghoon Im&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1&quot;&gt;Kyong Hwan Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01582">
<title>Improving Visual Quality and Transferability of Adversarial Attacks on Face Recognition Simultaneously with Adversarial Restoration. (arXiv:2309.01582v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01582</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial face examples possess two critical properties: Visual Quality and
Transferability. However, existing approaches rarely address these properties
simultaneously, leading to subpar results. To address this issue, we propose a
novel adversarial attack technique known as Adversarial Restoration
(AdvRestore), which enhances both visual quality and transferability of
adversarial face examples by leveraging a face restoration prior. In our
approach, we initially train a Restoration Latent Diffusion Model (RLDM)
designed for face restoration. Subsequently, we employ the inference process of
RLDM to generate adversarial face examples. The adversarial perturbations are
applied to the intermediate features of RLDM. Additionally, by treating RLDM
face restoration as a sibling task, the transferability of the generated
adversarial face examples is further improved. Our experimental results
validate the effectiveness of the proposed attack method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Fengfan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1&quot;&gt;Hefei Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiazhong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Ping Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01627">
<title>Cross-Consistent Deep Unfolding Network for Adaptive All-In-One Video Restoration. (arXiv:2309.01627v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01627</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing Video Restoration (VR) methods always necessitate the individual
deployment of models for each adverse weather to remove diverse adverse weather
degradations, lacking the capability for adaptive processing of degradations.
Such limitation amplifies the complexity and deployment costs in practical
applications. To overcome this deficiency, in this paper, we propose a
Cross-consistent Deep Unfolding Network (CDUN) for All-In-One VR, which enables
the employment of a single model to remove diverse degradations for the first
time. Specifically, the proposed CDUN accomplishes a novel iterative
optimization framework, capable of restoring frames corrupted by corresponding
degradations according to the degradation features given in advance. To empower
the framework for eliminating diverse degradations, we devise a Sequence-wise
Adaptive Degradation Estimator (SADE) to estimate degradation features for the
input corrupted video. By orchestrating these two cascading procedures, CDUN
achieves adaptive processing for diverse degradation. In addition, we introduce
a window-based inter-frame fusion strategy to utilize information from more
adjacent frames. This strategy involves the progressive stacking of temporal
windows in multiple iterations, effectively enlarging the temporal receptive
field and enabling each frame&apos;s restoration to leverage information from
distant frames. Extensive experiments demonstrate that the proposed method
achieves state-of-the-art performance in All-In-One VR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yuanshuo Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1&quot;&gt;Mingwen Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1&quot;&gt;Yecong Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lixu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wangmeng Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1&quot;&gt;Deyu Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01728">
<title>Generative-based Fusion Mechanism for Multi-Modal Tracking. (arXiv:2309.01728v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01728</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models (GMs) have received increasing research interest for their
remarkable capacity to achieve comprehensive understanding. However, their
potential application in the domain of multi-modal tracking has remained
relatively unexplored. In this context, we seek to uncover the potential of
harnessing generative techniques to address the critical challenge, information
fusion, in multi-modal tracking. In this paper, we delve into two prominent GM
techniques, namely, Conditional Generative Adversarial Networks (CGANs) and
Diffusion Models (DMs). Different from the standard fusion process where the
features from each modality are directly fed into the fusion block, we
condition these multi-modal features with random noise in the GM framework,
effectively transforming the original training samples into harder instances.
This design excels at extracting discriminative clues from the features,
enhancing the ultimate tracking performance. To quantitatively gauge the
effectiveness of our approach, we conduct extensive experiments across two
multi-modal tracking tasks, three baseline methods, and three challenging
benchmarks. The experimental results demonstrate that the proposed
generative-based fusion mechanism achieves state-of-the-art performance,
setting new records on LasHeR and RGBD1K.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zhangyong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tianyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xuefeng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiao-Jun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1&quot;&gt;Josef Kittler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02556">
<title>Domain Adaptation for Efficiently Fine-tuning Vision Transformer with Encrypted Images. (arXiv:2309.02556v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02556</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, deep neural networks (DNNs) trained with transformed data
have been applied to various applications such as privacy-preserving learning,
access control, and adversarial defenses. However, the use of transformed data
decreases the performance of models. Accordingly, in this paper, we propose a
novel method for fine-tuning models with transformed images under the use of
the vision transformer (ViT). The proposed domain adaptation method does not
cause the accuracy degradation of models, and it is carried out on the basis of
the embedding structure of ViT. In experiments, we confirmed that the proposed
method prevents accuracy degradation even when using encrypted images with the
CIFAR-10 and CIFAR-100 datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagamori_T/0/1/0/all/0/1&quot;&gt;Teru Nagamori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shiota_S/0/1/0/all/0/1&quot;&gt;Sayaka Shiota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1&quot;&gt;Hitoshi Kiya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02719">
<title>DMKD: Improving Feature-based Knowledge Distillation for Object Detection Via Dual Masking Augmentation. (arXiv:2309.02719v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02719</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent mainstream masked distillation methods function by reconstructing
selectively masked areas of a student network from the feature map of its
teacher counterpart. In these methods, the masked regions need to be properly
selected, such that reconstructed features encode sufficient discrimination and
representation capability like the teacher feature. However, previous masked
distillation methods only focus on spatial masking, making the resulting masked
areas biased towards spatial importance without encoding informative channel
clues. In this study, we devise a Dual Masked Knowledge Distillation (DMKD)
framework which can capture both spatially important and channel-wise
informative clues for comprehensive masked feature reconstruction. More
specifically, we employ dual attention mechanism for guiding the respective
masking branches, leading to reconstructed feature encoding dual significance.
Furthermore, fusing the reconstructed features is achieved by self-adjustable
weighting strategy for effective feature distillation. Our experiments on
object detection task demonstrate that the student networks achieve performance
gains of 4.1% and 4.3% with the help of our method when RetinaNet and Cascade
Mask R-CNN are respectively used as the teacher networks, while outperforming
the other state-of-the-art distillation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yin Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhijian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jianhua Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xili Wan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02855">
<title>Bandwidth-efficient Inference for Neural Image Compression. (arXiv:2309.02855v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02855</link>
<description rdf:parseType="Literal">&lt;p&gt;With neural networks growing deeper and feature maps growing larger, limited
communication bandwidth with external memory (or DRAM) and power constraints
become a bottleneck in implementing network inference on mobile and edge
devices. In this paper, we propose an end-to-end differentiable bandwidth
efficient neural inference method with the activation compressed by neural data
compression method. Specifically, we propose a transform-quantization-entropy
coding pipeline for activation compression with symmetric exponential Golomb
coding and a data-dependent Gaussian entropy model for arithmetic coding.
Optimized with existing model quantization methods, low-level task of image
compression can achieve up to 19x bandwidth reduction with 6.21x energy saving.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1&quot;&gt;Shanzhi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tongda Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yongsheng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuanyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanghao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingjing Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02843">
<title>Knowledge Distillation Layer that Lets the Student Decide. (arXiv:2309.02843v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2309.02843</link>
<description rdf:parseType="Literal">&lt;p&gt;Typical technique in knowledge distillation (KD) is regularizing the learning
of a limited capacity model (student) by pushing its responses to match a
powerful model&apos;s (teacher). Albeit useful especially in the penultimate layer
and beyond, its action on student&apos;s feature transform is rather implicit,
limiting its practice in the intermediate layers. To explicitly embed the
teacher&apos;s knowledge in feature transform, we propose a learnable KD layer for
the student which improves KD with two distinct abilities: i) learning how to
leverage the teacher&apos;s knowledge, enabling to discard nuisance information, and
ii) feeding forward the transferred knowledge deeper. Thus, the student enjoys
the teacher&apos;s knowledge during the inference besides training. Formally, we
repurpose 1x1-BN-ReLU-1x1 convolution block to assign a semantic vector to each
local region according to the template (supervised by the teacher) that the
corresponding region of the student matches. To facilitate template learning in
the intermediate layers, we propose a novel form of supervision based on the
teacher&apos;s decisions. Through rigorous experimentation, we demonstrate the
effectiveness of our approach on 3 popular classification benchmarks. Code is
available at: https://github.com/adagorgun/letKD-framework
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorgun_A/0/1/0/all/0/1&quot;&gt;Ada Gorgun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurbuz_Y/0/1/0/all/0/1&quot;&gt;Yeti Z. Gurbuz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alatan_A/0/1/0/all/0/1&quot;&gt;A. Aydin Alatan&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>