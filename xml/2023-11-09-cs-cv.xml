<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-07T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03386" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03390" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03402" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03426" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03427" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03478" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03500" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03537" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03547" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03557" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03570" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03572" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03606" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03622" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03650" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03745" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03749" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03774" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03784" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03785" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03830" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03884" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03904" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03912" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03938" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03943" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03959" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03965" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03967" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03970" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2012.04634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2012.08026" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.11048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.11948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.03897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.08171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.02062" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.12850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.01375" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.15042" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.03418" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.14317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.16412" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.05404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.09702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.03679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.07025" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.09043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09354" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.13409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.01191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07187" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09025" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.04190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05139" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11523" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02641" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09478" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.14918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00660" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01804" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03054" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03071" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03076" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03287" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.03380">
<title>An attempt to generate new bridge types from latent space of variational autoencoder. (arXiv:2311.03380v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03380</link>
<description rdf:parseType="Literal">&lt;p&gt;Try to generate new bridge types using generative artificial intelligence
technology. The grayscale images of the bridge facade with the change of
component width was rendered by 3dsMax animation software, and then the OpenCV
module performed an appropriate amount of geometric transformation (rotation,
horizontal scale, vertical scale) to obtain the image dataset of three-span
beam bridge, arch bridge, cable-stayed bridge and suspension bridge. Based on
Python programming language, TensorFlow and Keras deep learning platform
framework, variational autoencoder was constructed and trained, and
low-dimensional bridge-type latent space that is convenient for vector
operations was obtained. Variational autoencoder can combine two bridge types
on the basis of the original of human into one that is a new bridge type.
Generative artificial intelligence technology can assist bridge designers in
bridge-type innovation, and can be used as copilot.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongjun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03386">
<title>A Simple and Efficient Baseline for Data Attribution on Images. (arXiv:2311.03386v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03386</link>
<description rdf:parseType="Literal">&lt;p&gt;Data attribution methods play a crucial role in understanding machine
learning models, providing insight into which training data points are most
responsible for model outputs during deployment. However, current
state-of-the-art approaches require a large ensemble of as many as 300,000
models to accurately attribute model predictions. These approaches therefore
come at a high computational cost, are memory intensive, and are hard to scale
to large models or datasets. In this work, we focus on a minimalist baseline,
utilizing the feature space of a backbone pretrained via self-supervised
learning to perform data attribution. Our method is model-agnostic and scales
easily to large datasets. We show results on CIFAR-10 and ImageNet, achieving
strong performance that rivals or outperforms state-of-the-art approaches at a
fraction of the compute or memory cost. Contrary to prior work, our results
reinforce the intuition that a model&apos;s prediction on one image is most impacted
by visually similar training samples. Our approach serves as a simple and
efficient baseline for data attribution on images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singla_V/0/1/0/all/0/1&quot;&gt;Vasu Singla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sandoval_Segura_P/0/1/0/all/0/1&quot;&gt;Pedro Sandoval-Segura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1&quot;&gt;Micah Goldblum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1&quot;&gt;Jonas Geiping&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1&quot;&gt;Tom Goldstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03387">
<title>Determination of droplet size from wide-angle light scattering image data using convolutional neural networks. (arXiv:2311.03387v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03387</link>
<description rdf:parseType="Literal">&lt;p&gt;Wide-angle light scattering (WALS) offers the possibility of a highly
temporally and spatially resolved measurement of droplets in spray-based
methods for nanoparticle synthesis. The size of these droplets is a critical
variable affecting the final properties of synthesized materials such as
hetero-aggregates. However, conventional methods for determining droplet sizes
from WALS image data are labor-intensive and may introduce biases, particularly
when applied to complex systems like spray flame synthesis (SFS). To address
these challenges, we introduce a fully automatic machine learning-based
approach that employs convolutional neural networks (CNNs) in order to
streamline the droplet sizing process. This CNN-based methodology offers
further advantages: it requires few manual labels and can utilize transfer
learning, making it a promising alternative to conventional methods,
specifically with respect to efficiency. To evaluate the performance of our
machine learning models, we consider WALS data from an ethanol spray flame
process at various heights above the burner surface (HABs), where the models
are trained and cross-validated on a large dataset comprising nearly 35000 WALS
images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirstein_T/0/1/0/all/0/1&quot;&gt;Tom Kirstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assmann_S/0/1/0/all/0/1&quot;&gt;Simon A&amp;#xdf;mann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furat_O/0/1/0/all/0/1&quot;&gt;Orkun Furat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Will_S/0/1/0/all/0/1&quot;&gt;Stefan Will&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_V/0/1/0/all/0/1&quot;&gt;Volker Schmidt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03390">
<title>FPGA-QHAR: Throughput-Optimized for Quantized Human Action Recognition on The Edge. (arXiv:2311.03390v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03390</link>
<description rdf:parseType="Literal">&lt;p&gt;Accelerating Human Action Recognition (HAR) efficiently for real-time
surveillance and robotic systems on edge chips remains a challenging research
field, given its high computational and memory requirements. This paper
proposed an integrated end-to-end HAR scalable HW/SW accelerator co-design
based on an enhanced 8-bit quantized Two-Stream SimpleNet-PyTorch CNN
architecture. Our network accelerator was trained on UCF101 and UCF24 datasets
and implemented on edge SoC-FPGA. Our development uses partially streaming
dataflow architecture to achieve higher throughput versus network design and
resource utilization trade-off. We also fused all convolutional, batch-norm,
and ReLU operations into a single homogeneous layer and utilized the
Lucas-Kanade motion flow method to enable a high parallelism accelerator design
and optimized on-chip engine computing.Furthermore, our proposed methodology
achieved nearly 81% prediction accuracy with an approximately 24 FPS real-time
inference throughput at 187MHz on ZCU104, which is 1.7x - 1.9x higher than the
prior research. Lastly, the designed framework was benchmarked against several
hardware chips for higher throughput and performance measurements and is now
available as an open-source project on GitHub for training and implementation
on edge platforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alhussain_A/0/1/0/all/0/1&quot;&gt;Azzam Alhussain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Mingjie Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03402">
<title>CycleCL: Self-supervised Learning for Periodic Videos. (arXiv:2311.03402v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03402</link>
<description rdf:parseType="Literal">&lt;p&gt;Analyzing periodic video sequences is a key topic in applications such as
automatic production systems, remote sensing, medical applications, or physical
training. An example is counting repetitions of a physical exercise. Due to the
distinct characteristics of periodic data, self-supervised methods designed for
standard image datasets do not capture changes relevant to the progression of
the cycle and fail to ignore unrelated noise. They thus do not work well on
periodic data. In this paper, we propose CycleCL, a self-supervised learning
method specifically designed to work with periodic data. We start from the
insight that a good visual representation for periodic data should be sensitive
to the phase of a cycle, but be invariant to the exact repetition, i.e. it
should generate identical representations for a specific phase throughout all
repetitions. We exploit the repetitions in videos to design a novel contrastive
learning method based on a triplet loss that optimizes for these desired
properties. Our method uses pre-trained features to sample pairs of frames from
approximately the same phase and negative pairs of frames from different
phases. Then, we iterate between optimizing a feature encoder and resampling
triplets, until convergence. By optimizing a model this way, we are able to
learn features that have the mentioned desired properties. We evaluate CycleCL
on an industrial and multiple human actions datasets, where it significantly
outperforms previous video-based self-supervised learning methods on all tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Destro_M/0/1/0/all/0/1&quot;&gt;Matteo Destro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gygli_M/0/1/0/all/0/1&quot;&gt;Michael Gygli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03422">
<title>Efficient and Low-Footprint Object Classification using Spatial Contrast. (arXiv:2311.03422v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03422</link>
<description rdf:parseType="Literal">&lt;p&gt;Event-based vision sensors traditionally compute temporal contrast that
offers potential for low-power and low-latency sensing and computing. In this
research, an alternative paradigm for event-based sensors using localized
spatial contrast (SC) under two different thresholding techniques, relative and
absolute, is investigated. Given the slow maturity of spatial contrast in
comparison to temporal-based sensors, a theoretical simulated output of such a
hardware sensor is explored. Furthermore, we evaluate traffic sign
classification using the German Traffic Sign dataset (GTSRB) with well-known
Deep Neural Networks (DNNs). This study shows that spatial contrast can
effectively capture salient image features needed for classification using a
Binarized DNN with significant reduction in input data usage (at least 12X) and
memory resources (17.5X), compared to high precision RGB images and DNN, with
only a small loss (~2%) in macro F1-score. Binarized MicronNet achieves an
F1-score of 94.4% using spatial contrast, compared to only 56.3% when using RGB
input images. Thus, SC offers great promise for deployment in power and
resource constrained edge computing environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belding_M/0/1/0/all/0/1&quot;&gt;Matthew Belding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stumpp_D/0/1/0/all/0/1&quot;&gt;Daniel C. Stumpp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kubendran_R/0/1/0/all/0/1&quot;&gt;Rajkumar Kubendran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03426">
<title>GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values. (arXiv:2311.03426v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03426</link>
<description rdf:parseType="Literal">&lt;p&gt;Massive transformer-based models face several challenges, including slow and
computationally intensive pre-training and over-parametrization. This paper
addresses these challenges by proposing a versatile method called GQKVA, which
generalizes query, key, and value grouping techniques. GQKVA is designed to
speed up transformer pre-training while reducing the model size. Our
experiments with various GQKVA variants highlight a clear trade-off between
performance and model size, allowing for customized choices based on resource
and time limitations. Our findings also indicate that the conventional
multi-head attention approach is not always the best choice, as there are
lighter and faster alternatives available. We tested our method on ViT, which
achieved an approximate 0.3% increase in accuracy while reducing the model size
by about 4% in the task of image classification. Additionally, our most
aggressive model reduction experiment resulted in a reduction of approximately
15% in model size, with only around a 1% drop in accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javadi_F/0/1/0/all/0/1&quot;&gt;Farnoosh Javadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_W/0/1/0/all/0/1&quot;&gt;Walid Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajimolahoseini_H/0/1/0/all/0/1&quot;&gt;Habib Hajimolahoseini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ataiefard_F/0/1/0/all/0/1&quot;&gt;Foozhan Ataiefard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassanpour_M/0/1/0/all/0/1&quot;&gt;Mohammad Hassanpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asani_S/0/1/0/all/0/1&quot;&gt;Saina Asani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_A/0/1/0/all/0/1&quot;&gt;Austin Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awad_O/0/1/0/all/0/1&quot;&gt;Omar Mohamed Awad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kangling Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03427">
<title>TSP-Transformer: Task-Specific Prompts Boosted Transformer for Holistic Scene Understanding. (arXiv:2311.03427v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03427</link>
<description rdf:parseType="Literal">&lt;p&gt;Holistic scene understanding includes semantic segmentation, surface normal
estimation, object boundary detection, depth estimation, etc. The key aspect of
this problem is to learn representation effectively, as each subtask builds
upon not only correlated but also distinct attributes. Inspired by
visual-prompt tuning, we propose a Task-Specific Prompts Transformer, dubbed
TSP-Transformer, for holistic scene understanding. It features a vanilla
transformer in the early stage and tasks-specific prompts transformer encoder
in the lateral stage, where tasks-specific prompts are augmented. By doing so,
the transformer layer learns the generic information from the shared parts and
is endowed with task-specific capacity. First, the tasks-specific prompts serve
as induced priors for each task effectively. Moreover, the task-specific
prompts can be seen as switches to favor task-specific representation learning
for different tasks. Extensive experiments on NYUD-v2 and PASCAL-Context show
that our method achieves state-of-the-art performance, validating the
effectiveness of our method for holistic scene understanding. We also provide
our code in the following link https://github.com/tb2-sy/TSP-Transformer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zibo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1&quot;&gt;Dongze Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Binbin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaomei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhengxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shenghua Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03478">
<title>Multi Loss-based Feature Fusion and Top Two Voting Ensemble Decision Strategy for Facial Expression Recognition in the Wild. (arXiv:2311.03478v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03478</link>
<description rdf:parseType="Literal">&lt;p&gt;Facial expression recognition (FER) in the wild is a challenging task
affected by the image quality and has attracted broad interest in computer
vision. There is no research using feature fusion and ensemble strategy for FER
simultaneously. Different from previous studies, this paper applies both
internal feature fusion for a single model and feature fusion among multiple
networks, as well as the ensemble strategy. This paper proposes one novel
single model named R18+FAML, as well as one ensemble model named
R18+FAML-FGA-T2V to improve the performance of the FER in the wild. Based on
the structure of ResNet18 (R18), R18+FAML combines internal Feature fusion and
three Attention blocks using Multiple Loss functions (FAML) to improve the
diversity of the feature extraction. To improve the performance of R18+FAML, we
propose a Feature fusion among networks based on the Genetic Algorithm (FGA),
which can fuse the convolution kernels for feature extraction of multiple
networks. On the basis of R18+FAML and FGA, we propose one ensemble strategy,
i.e., the Top Two Voting (T2V) to support the classification of FER, which can
consider more classification information comprehensively. Combining the above
strategies, R18+FAML-FGA-T2V can focus on the main expression-aware areas.
Extensive experiments demonstrate that our single model R18+FAML and the
ensemble model R18+FAML-FGA-T2V achieve the accuracies of $\left( 90.32, 62.17,
65.83 \right)\%$ and $\left( 91.59, 63.27, 66.63 \right)\%$ on three
challenging unbalanced FER datasets RAF-DB, AffectNet-8 and AffectNet-7
respectively, both outperforming the state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Guangyao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yuanlun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_W/0/1/0/all/0/1&quot;&gt;Wenhong Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03500">
<title>Predicting Age from White Matter Diffusivity with Residual Learning. (arXiv:2311.03500v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.03500</link>
<description rdf:parseType="Literal">&lt;p&gt;Imaging findings inconsistent with those expected at specific chronological
age ranges may serve as early indicators of neurological disorders and
increased mortality risk. Estimation of chronological age, and deviations from
expected results, from structural MRI data has become an important task for
developing biomarkers that are sensitive to such deviations. Complementary to
structural analysis, diffusion tensor imaging (DTI) has proven effective in
identifying age-related microstructural changes within the brain white matter,
thereby presenting itself as a promising additional modality for brain age
prediction. Although early studies have sought to harness DTI&apos;s advantages for
age estimation, there is no evidence that the success of this prediction is
owed to the unique microstructural and diffusivity features that DTI provides,
rather than the macrostructural features that are also available in DTI data.
Therefore, we seek to develop white-matter-specific age estimation to capture
deviations from normal white matter aging. Specifically, we deliberately
disregard the macrostructural information when predicting age from DTI scalar
images, using two distinct methods. The first method relies on extracting only
microstructural features from regions of interest. The second applies 3D
residual neural networks (ResNets) to learn features directly from the images,
which are non-linearly registered and warped to a template to minimize
macrostructural variations. When tested on unseen data, the first method yields
mean absolute error (MAE) of 6.11 years for cognitively normal participants and
MAE of 6.62 years for cognitively impaired participants, while the second
method achieves MAE of 4.69 years for cognitively normal participants and MAE
of 4.96 years for cognitively impaired participants. We find that the ResNet
model captures subtler, non-macrostructural features for brain age prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chenyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Michael E. Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Ho Hin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khairi_N/0/1/0/all/0/1&quot;&gt;Nazirah Mohd Khairi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kanakaraj_P/0/1/0/all/0/1&quot;&gt;Praitayini Kanakaraj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Newlin_N/0/1/0/all/0/1&quot;&gt;Nancy R. Newlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Archer_D/0/1/0/all/0/1&quot;&gt;Derek B. Archer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jefferson_A/0/1/0/all/0/1&quot;&gt;Angela L. Jefferson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Taylor_W/0/1/0/all/0/1&quot;&gt;Warren D. Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Boyd_B/0/1/0/all/0/1&quot;&gt;Brian D. Boyd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Beason_Held_L/0/1/0/all/0/1&quot;&gt;Lori L. Beason-Held&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Resnick_S/0/1/0/all/0/1&quot;&gt;Susan M. Resnick&lt;/a&gt;, The &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Team_BIOCARD_Study/0/1/0/all/0/1&quot;&gt;BIOCARD Study Team&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1&quot;&gt;Yuankai Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schaik_K/0/1/0/all/0/1&quot;&gt;Katherine D. Van Schaik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schilling_K/0/1/0/all/0/1&quot;&gt;Kurt G. Schilling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moyer_D/0/1/0/all/0/1&quot;&gt;Daniel Moyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Isgum_I/0/1/0/all/0/1&quot;&gt;Ivana I&amp;#x161;gum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1&quot;&gt;Bennett A. Landman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03517">
<title>SoundCam: A Dataset for Finding Humans Using Room Acoustics. (arXiv:2311.03517v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2311.03517</link>
<description rdf:parseType="Literal">&lt;p&gt;A room&apos;s acoustic properties are a product of the room&apos;s geometry, the
objects within the room, and their specific positions. A room&apos;s acoustic
properties can be characterized by its impulse response (RIR) between a source
and listener location, or roughly inferred from recordings of natural signals
present in the room. Variations in the positions of objects in a room can
effect measurable changes in the room&apos;s acoustic properties, as characterized
by the RIR. Existing datasets of RIRs either do not systematically vary
positions of objects in an environment, or they consist of only simulated RIRs.
We present SoundCam, the largest dataset of unique RIRs from in-the-wild rooms
publicly released to date. It includes 5,000 10-channel real-world measurements
of room impulse responses and 2,000 10-channel recordings of music in three
different rooms, including a controlled acoustic lab, an in-the-wild living
room, and a conference room, with different humans in positions throughout each
room. We show that these measurements can be used for interesting tasks, such
as detecting and identifying humans, and tracking their positions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mason Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clarke_S/0/1/0/all/0/1&quot;&gt;Samuel Clarke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jui-Hsien Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Ruohan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03518">
<title>High-resolution power equipment recognition based on improved self-attention. (arXiv:2311.03518v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03518</link>
<description rdf:parseType="Literal">&lt;p&gt;The current trend of automating inspections at substations has sparked a
surge in interest in the field of transformer image recognition. However, due
to restrictions in the number of parameters in existing models, high-resolution
images can&apos;t be directly applied, leaving significant room for enhancing
recognition accuracy. Addressing this challenge, the paper introduces a novel
improvement on deep self-attention networks tailored for this issue. The
proposed model comprises four key components: a foundational network, a region
proposal network, a module for extracting and segmenting target areas, and a
final prediction network. The innovative approach of this paper differentiates
itself by decoupling the processes of part localization and recognition,
initially using low-resolution images for localization followed by
high-resolution images for recognition. Moreover, the deep self-attention
network&apos;s prediction mechanism uniquely incorporates the semantic context of
images, resulting in substantially improved recognition performance.
Comparative experiments validate that this method outperforms the two other
prevalent target recognition models, offering a groundbreaking perspective for
automating electrical equipment inspections.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Siyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1&quot;&gt;Xin Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhen Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sizhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xun Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03537">
<title>Leveraging point annotations in segmentation learning with boundary loss. (arXiv:2311.03537v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03537</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates the combination of intensity-based distance maps with
boundary loss for point-supervised semantic segmentation. By design the
boundary loss imposes a stronger penalty on the false positives the farther
away from the object they occur. Hence it is intuitively inappropriate for weak
supervision, where the ground truth label may be much smaller than the actual
object and a certain amount of false positives (w.r.t. the weak ground truth)
is actually desirable. Using intensity-aware distances instead may alleviate
this drawback, allowing for a certain amount of false positives without a
significant increase to the training loss. The motivation for applying the
boundary loss directly under weak supervision lies in its great success for
fully supervised segmentation tasks, but also in not requiring extra priors or
outside information that is usually required -- in some form -- with existing
weakly supervised methods in the literature. This formulation also remains
potentially more attractive than existing CRF-based regularizers, due to its
simplicity and computational efficiency. We perform experiments on two
multi-class datasets; ACDC (heart segmentation) and POEM (whole-body abdominal
organ segmentation). Preliminary results are encouraging and show that this
supervision strategy has great potential. On ACDC it outperforms the CRF-loss
based approach, and on POEM data it performs on par with it. The code for all
our experiments is openly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breznik_E/0/1/0/all/0/1&quot;&gt;Eva Breznik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kervadec_H/0/1/0/all/0/1&quot;&gt;Hoel Kervadec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malmberg_F/0/1/0/all/0/1&quot;&gt;Filip Malmberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kullberg_J/0/1/0/all/0/1&quot;&gt;Joel Kullberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahlstrom_H/0/1/0/all/0/1&quot;&gt;H&amp;#xe5;kan Ahlstr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruijne_M/0/1/0/all/0/1&quot;&gt;Marleen de Bruijne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strand_R/0/1/0/all/0/1&quot;&gt;Robin Strand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03547">
<title>InterVLS: Interactive Model Understanding and Improvement with Vision-Language Surrogates. (arXiv:2311.03547v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.03547</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models are widely used in critical applications, highlighting
the need for pre-deployment model understanding and improvement. Visual
concept-based methods, while increasingly used for this purpose, face
challenges: (1) most concepts lack interpretability, (2) existing methods
require model knowledge, often unavailable at run time. Additionally, (3) there
lacks a no-code method for post-understanding model improvement. Addressing
these, we present InterVLS. The system facilitates model understanding by
discovering text-aligned concepts, measuring their influence with
model-agnostic linear surrogates. Employing visual analytics, InterVLS offers
concept-based explanations and performance insights. It enables users to adjust
concept influences to update a model, facilitating no-code model improvement.
We evaluate InterVLS in a user study, illustrating its functionality with two
scenarios. Results indicates that InterVLS is effective to help users identify
influential concepts to a model, gain insights and adjust concept influence to
improve the model. We conclude with a discussion based on our study results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jinbin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Wenbin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gou_L/0/1/0/all/0/1&quot;&gt;Liang Gou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1&quot;&gt;Liu Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bryan_C/0/1/0/all/0/1&quot;&gt;Chris Bryan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03550">
<title>United We Stand, Divided We Fall: UnityGraph for Unsupervised Procedure Learning from Videos. (arXiv:2311.03550v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03550</link>
<description rdf:parseType="Literal">&lt;p&gt;Given multiple videos of the same task, procedure learning addresses
identifying the key-steps and determining their order to perform the task. For
this purpose, existing approaches use the signal generated from a pair of
videos. This makes key-steps discovery challenging as the algorithms lack
inter-videos perspective. Instead, we propose an unsupervised Graph-based
Procedure Learning (GPL) framework. GPL consists of the novel UnityGraph that
represents all the videos of a task as a graph to obtain both intra-video and
inter-videos context. Further, to obtain similar embeddings for the same
key-steps, the embeddings of UnityGraph are updated in an unsupervised manner
using the Node2Vec algorithm. Finally, to identify the key-steps, we cluster
the embeddings using KMeans. We test GPL on benchmark ProceL, CrossTask, and
EgoProceL datasets and achieve an average improvement of 2% on third-person
datasets and 3.6% on EgoProceL over the state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1&quot;&gt;Siddhant Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1&quot;&gt;Chetan Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1&quot;&gt;C.V. Jawahar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03557">
<title>Spatio-Temporal Similarity Measure based Multi-Task Learning for Predicting Alzheimer&apos;s Disease Progression using MRI Data. (arXiv:2311.03557v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03557</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying and utilising various biomarkers for tracking Alzheimer&apos;s disease
(AD) progression have received many recent attentions and enable helping
clinicians make the prompt decisions. Traditional progression models focus on
extracting morphological biomarkers in regions of interest (ROIs) from MRI/PET
images, such as regional average cortical thickness and regional volume. They
are effective but ignore the relationships between brain ROIs over time, which
would lead to synergistic deterioration. For exploring the synergistic
deteriorating relationship between these biomarkers, in this paper, we propose
a novel spatio-temporal similarity measure based multi-task learning approach
for effectively predicting AD progression and sensitively capturing the
critical relationships between biomarkers. Specifically, we firstly define a
temporal measure for estimating the magnitude and velocity of biomarker change
over time, which indicate a changing trend(temporal). Converting this trend
into the vector, we then compare this variability between biomarkers in a
unified vector space(spatial). The experimental results show that compared with
directly ROI based learning, our proposed method is more effective in
predicting disease progression. Our method also enables performing longitudinal
stability selection to identify the changing relationships between biomarkers,
which play a key role in disease progression. We prove that the synergistic
deteriorating biomarkers between cortical volumes or surface areas have a
significant effect on the cognitive prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xulong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Menghui Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1&quot;&gt;Jun Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1&quot;&gt;Po Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03561">
<title>Sea You Later: Metadata-Guided Long-Term Re-Identification for UAV-Based Multi-Object Tracking. (arXiv:2311.03561v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03561</link>
<description rdf:parseType="Literal">&lt;p&gt;Re-identification (ReID) in multi-object tracking (MOT) for UAVs in maritime
computer vision has been challenging for several reasons. More specifically,
short-term re-identification (ReID) is difficult due to the nature of the
characteristics of small targets and the sudden movement of the drone&apos;s gimbal.
Long-term ReID suffers from the lack of useful appearance diversity. In
response to these challenges, we present an adaptable motion-based MOT
algorithm, called Metadata Guided MOT (MG-MOT). This algorithm effectively
merges short-term tracking data into coherent long-term tracks, harnessing
crucial metadata from UAVs, including GPS position, drone altitude, and camera
orientations. Extensive experiments are conducted to validate the efficacy of
our MOT algorithm. Utilizing the challenging SeaDroneSee tracking dataset,
which encompasses the aforementioned scenarios, we achieve a much-improved
performance in the latest edition of the UAV-based Maritime Object Tracking
Challenge with a state-of-the-art HOTA of 69.5% and an IDF1 of 85.9% on the
testing split.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng-Yen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hsiang-Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhongyu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1&quot;&gt;Heng-Cheng Kuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1&quot;&gt;Jie Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chung-I Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jenq-Neng Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03570">
<title>Cal-DETR: Calibrated Detection Transformer. (arXiv:2311.03570v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03570</link>
<description rdf:parseType="Literal">&lt;p&gt;Albeit revealing impressive predictive performance for several computer
vision tasks, deep neural networks (DNNs) are prone to making overconfident
predictions. This limits the adoption and wider utilization of DNNs in many
safety-critical applications. There have been recent efforts toward calibrating
DNNs, however, almost all of them focus on the classification task.
Surprisingly, very little attention has been devoted to calibrating modern
DNN-based object detectors, especially detection transformers, which have
recently demonstrated promising detection performance and are influential in
many decision-making systems. In this work, we address the problem by proposing
a mechanism for calibrated detection transformers (Cal-DETR), particularly for
Deformable-DETR, UP-DETR and DINO. We pursue the train-time calibration route
and make the following contributions. First, we propose a simple yet effective
approach for quantifying uncertainty in transformer-based object detectors.
Second, we develop an uncertainty-guided logit modulation mechanism that
leverages the uncertainty to modulate the class logits. Third, we develop a
logit mixing approach that acts as a regularizer with detection-specific losses
and is also complementary to the uncertainty-guided logit modulation technique
to further improve the calibration performance. Lastly, we conduct extensive
experiments across three in-domain and four out-domain scenarios. Results
corroborate the effectiveness of Cal-DETR against the competing train-time
methods in calibrating both in-domain and out-domain detections while
maintaining or even improving the detection performance. Our codebase and
pre-trained models can be accessed at
\url{https://github.com/akhtarvision/cal-detr}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munir_M/0/1/0/all/0/1&quot;&gt;Muhammad Akhtar Munir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Muhammad Haris Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1&quot;&gt;Mohsen Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03572">
<title>Unsupervised Region-Growing Network for Object Segmentation in Atmospheric Turbulence. (arXiv:2311.03572v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03572</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a two-stage unsupervised foreground object
segmentation network tailored for dynamic scenes affected by atmospheric
turbulence. In the first stage, we utilize averaged optical flow from
turbulence-distorted image sequences to feed a novel region-growing algorithm,
crafting preliminary masks for each moving object in the video. In the second
stage, we employ a U-Net architecture with consistency and grouping losses to
further refine these masks optimizing their spatio-temporal alignment. Our
approach does not require labeled training data and works across varied
turbulence strengths for long-range video. Furthermore, we release the first
moving object segmentation dataset of turbulence-affected videos, complete with
manually annotated ground truth masks. Our method, evaluated on this new
dataset, demonstrates superior segmentation accuracy and robustness as compared
to current state-of-the-art unsupervised methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_D/0/1/0/all/0/1&quot;&gt;Dehao Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_R/0/1/0/all/0/1&quot;&gt;Ripon Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayasuriya_S/0/1/0/all/0/1&quot;&gt;Suren Jayasuriya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jinwei Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1&quot;&gt;Nianyi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03606">
<title>Multimodal Stress Detection Using Facial Landmarks and Biometric Signals. (arXiv:2311.03606v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03606</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of various sensing technologies is improving measurements of
stress and the well-being of individuals. Although progress has been made with
single signal modalities like wearables and facial emotion recognition,
integrating multiple modalities provides a more comprehensive understanding of
stress, given that stress manifests differently across different people.
Multi-modal learning aims to capitalize on the strength of each modality rather
than relying on a single signal. Given the complexity of processing and
integrating high-dimensional data from limited subjects, more research is
needed. Numerous research efforts have been focused on fusing stress and
emotion signals at an early stage, e.g., feature-level fusion using basic
machine learning methods and 1D-CNN Methods. This paper proposes a multi-modal
learning approach for stress detection that integrates facial landmarks and
biometric signals. We test this multi-modal integration with various
early-fusion and late-fusion techniques to integrate the 1D-CNN model from
biometric signals and 2-D CNN using facial landmarks. We evaluate these
architectures using a rigorous test of models&apos; generalizability using the
leave-one-subject-out mechanism, i.e., all samples related to a single subject
are left out to train the model. Our findings show that late-fusion achieved
94.39\% accuracy, and early-fusion surpassed it with a 98.38\% accuracy rate.
This research contributes valuable insights into enhancing stress detection
through a multi-modal approach. The proposed research offers important
knowledge in improving stress detection using a multi-modal approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1&quot;&gt;Majid Hosseini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bodaghi_M/0/1/0/all/0/1&quot;&gt;Morteza Bodaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhupatiraju_R/0/1/0/all/0/1&quot;&gt;Ravi Teja Bhupatiraju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maida_A/0/1/0/all/0/1&quot;&gt;Anthony Maida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottumukkala_R/0/1/0/all/0/1&quot;&gt;Raju Gottumukkala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03620">
<title>FusionViT: Hierarchical 3D Object Detection via LiDAR-Camera Vision Transformer Fusion. (arXiv:2311.03620v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03620</link>
<description rdf:parseType="Literal">&lt;p&gt;For 3D object detection, both camera and lidar have been demonstrated to be
useful sensory devices for providing complementary information about the same
scenery with data representations in different modalities, e.g., 2D RGB image
vs 3D point cloud. An effective representation learning and fusion of such
multi-modal sensor data is necessary and critical for better 3D object
detection performance. To solve the problem, in this paper, we will introduce a
novel vision transformer-based 3D object detection model, namely FusionViT.
Different from the existing 3D object detection approaches, FusionViT is a
pure-ViT based framework, which adopts a hierarchical architecture by extending
the transformer model to embed both images and point clouds for effective
representation learning. Such multi-modal data embedding representations will
be further fused together via a fusion vision transformer model prior to
feeding the learned features to the object detection head for both detection
and localization of the 3D objects in the input scenery. To demonstrate the
effectiveness of FusionViT, extensive experiments have been done on real-world
traffic object detection benchmark datasets KITTI and Waymo Open. Notably, our
FusionViT model can achieve state-of-the-art performance and outperforms not
only the existing baseline methods that merely rely on camera images or lidar
point clouds, but also the latest multi-modal image-point cloud deep fusion
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_X/0/1/0/all/0/1&quot;&gt;Xinhao Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiawei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03622">
<title>TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer. (arXiv:2311.03622v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.03622</link>
<description rdf:parseType="Literal">&lt;p&gt;Model-based RL is a promising approach for real-world robotics due to its
improved sample efficiency and generalization capabilities compared to
model-free RL. However, effective model-based RL solutions for vision-based
real-world applications require bridging the sim-to-real gap for any world
model learnt. Due to its significant computational cost, standard domain
randomisation does not provide an effective solution to this problem. This
paper proposes TWIST (Teacher-Student World Model Distillation for Sim-to-Real
Transfer) to achieve efficient sim-to-real transfer of vision-based model-based
RL using distillation. Specifically, TWIST leverages state observations as
readily accessible, privileged information commonly garnered from a simulator
to significantly accelerate sim-to-real transfer. Specifically, a teacher world
model is trained efficiently on state information. At the same time, a matching
dataset is collected of domain-randomised image observations. The teacher world
model then supervises a student world model that takes the domain-randomised
image observations as input. By distilling the learned latent dynamics model
from the teacher to the student model, TWIST achieves efficient and effective
sim-to-real transfer for vision-based model-based RL tasks. Experiments in
simulated and real robotics tasks demonstrate that our approach outperforms
naive domain randomisation and model-free methods in terms of sample efficiency
and task performance of sim-to-real transfer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamada_J/0/1/0/all/0/1&quot;&gt;Jun Yamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rigter_M/0/1/0/all/0/1&quot;&gt;Marc Rigter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_J/0/1/0/all/0/1&quot;&gt;Jack Collins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Posner_I/0/1/0/all/0/1&quot;&gt;Ingmar Posner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03629">
<title>Random Field Augmentations for Self-Supervised Representation Learning. (arXiv:2311.03629v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03629</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised representation learning is heavily dependent on data
augmentations to specify the invariances encoded in representations. Previous
work has shown that applying diverse data augmentations is crucial to
downstream performance, but augmentation techniques remain under-explored. In
this work, we propose a new family of local transformations based on Gaussian
random fields to generate image augmentations for self-supervised
representation learning. These transformations generalize the well-established
affine and color transformations (translation, rotation, color jitter, etc.)
and greatly increase the space of augmentations by allowing transformation
parameter values to vary from pixel to pixel. The parameters are treated as
continuous functions of spatial coordinates, and modeled as independent
Gaussian random fields. Empirical results show the effectiveness of the new
transformations for self-supervised representation learning. Specifically, we
achieve a 1.7% top-1 accuracy improvement over baseline on ImageNet downstream
classification, and a 3.6% improvement on out-of-distribution iNaturalist
downstream classification. However, due to the flexibility of the new
transformations, learned representations are sensitive to hyperparameters.
While mild transformations improve representations, we observe that strong
transformations can degrade the structure of an image, indicating that
balancing the diversity and strength of augmentations is important for
improving generalization of learned representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansfield_P/0/1/0/all/0/1&quot;&gt;Philip Andrew Mansfield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afkanpour_A/0/1/0/all/0/1&quot;&gt;Arash Afkanpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morningstar_W/0/1/0/all/0/1&quot;&gt;Warren Richard Morningstar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1&quot;&gt;Karan Singhal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03648">
<title>Instruct Me More! Random Prompting for Visual In-Context Learning. (arXiv:2311.03648v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03648</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale models trained on extensive datasets, have emerged as the
preferred approach due to their high generalizability across various tasks.
In-context learning (ICL), a popular strategy in natural language processing,
uses such models for different tasks by providing instructive prompts but
without updating model parameters. This idea is now being explored in computer
vision, where an input-output image pair (called an in-context pair) is
supplied to the model with a query image as a prompt to exemplify the desired
output. The efficacy of visual ICL often depends on the quality of the prompts.
We thus introduce a method coined Instruct Me More (InMeMo), which augments
in-context pairs with a learnable perturbation (prompt), to explore its
potential. Our experiments on mainstream tasks reveal that InMeMo surpasses the
current state-of-the-art performance. Specifically, compared to the baseline
without learnable prompt, InMeMo boosts mIoU scores by 7.35 and 15.13 for
foreground segmentation and single object detection tasks, respectively. Our
findings suggest that InMeMo offers a versatile and efficient way to enhance
the performance of visual ICL with lightweight training. Code is available at
https://github.com/Jackieam/InMeMo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiahao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bowen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liangzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1&quot;&gt;Yuta Nakashima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagahara_H/0/1/0/all/0/1&quot;&gt;Hajime Nagahara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03650">
<title>Image Generation and Learning Strategy for Deep Document Forgery Detection. (arXiv:2311.03650v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03650</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, document processing has flourished and brought numerous
benefits. However, there has been a significant rise in reported cases of
forged document images. Specifically, recent advancements in deep neural
network (DNN) methods for generative tasks may amplify the threat of document
forgery. Traditional approaches for forged document images created by prevalent
copy-move methods are unsuitable against those created by DNN-based methods, as
we have verified. To address this issue, we construct a training dataset of
document forgery images, named FD-VIED, by emulating possible attacks, such as
text addition, removal, and replacement with recent DNN-methods. Additionally,
we introduce an effective pre-training approach through self-supervised
learning with both natural images and document images. In our experiments, we
demonstrate that our approach enhances detection performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okamoto_Y/0/1/0/all/0/1&quot;&gt;Yamato Okamoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Genki_O/0/1/0/all/0/1&quot;&gt;Osada Genki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yahiro_I/0/1/0/all/0/1&quot;&gt;Iu Yahiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasegawa_R/0/1/0/all/0/1&quot;&gt;Rintaro Hasegawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1&quot;&gt;Peifei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kataoka_H/0/1/0/all/0/1&quot;&gt;Hirokatsu Kataoka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03679">
<title>Unsupervised convolutional neural network fusion approach for change detection in remote sensing images. (arXiv:2311.03679v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03679</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of deep learning, a variety of change detection
methods based on deep learning have emerged in recent years. However, these
methods usually require a large number of training samples to train the network
model, so it is very expensive. In this paper, we introduce a completely
unsupervised shallow convolutional neural network (USCNN) fusion approach for
change detection. Firstly, the bi-temporal images are transformed into
different feature spaces by using convolution kernels of different sizes to
extract multi-scale information of the images. Secondly, the output features of
bi-temporal images at the same convolution kernels are subtracted to obtain the
corresponding difference images, and the difference feature images at the same
scale are fused into one feature image by using 1 * 1 convolution layer.
Finally, the output features of different scales are concatenated and a 1 * 1
convolution layer is used to fuse the multi-scale information of the image. The
model parameters are obtained by a redesigned sparse function. Our model has
three features: the entire training process is conducted in an unsupervised
manner, the network architecture is shallow, and the objective function is
sparse. Thus, it can be seen as a kind of lightweight network model.
Experimental results on four real remote sensing datasets indicate the
feasibility and effectiveness of the proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1&quot;&gt;Weidong Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1&quot;&gt;Pei Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1&quot;&gt;Li Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03713">
<title>Multimodal deep representation learning for quantum cross-platform verification. (arXiv:2311.03713v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2311.03713</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-platform verification, a critical undertaking in the realm of
early-stage quantum computing, endeavors to characterize the similarity of two
imperfect quantum devices executing identical algorithms, utilizing minimal
measurements. While the random measurement approach has been instrumental in
this context, the quasi-exponential computational demand with increasing qubit
count hurdles its feasibility in large-qubit scenarios. To bridge this
knowledge gap, here we introduce an innovative multimodal learning approach,
recognizing that the formalism of data in this task embodies two distinct
modalities: measurement outcomes and classical description of compiled circuits
on explored quantum devices, both enriched with unique information. Building
upon this insight, we devise a multimodal neural network to independently
extract knowledge from these modalities, followed by a fusion operation to
create a comprehensive data representation. The learned representation can
effectively characterize the similarity between the explored quantum devices
when executing new quantum algorithms not present in the training data. We
evaluate our proposal on platforms featuring diverse noise models, encompassing
system sizes up to 50 qubits. The achieved results demonstrate a
three-orders-of-magnitude improvement in prediction accuracy compared to the
random measurements and offer compelling evidence of the complementary roles
played by each modality in cross-platform verification. These findings pave the
way for harnessing the power of multimodal learning to overcome challenges in
wider quantum system learning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yang Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhenliang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Hsieh_M/0/1/0/all/0/1&quot;&gt;Min-hsiu Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03716">
<title>LLM as an Art Director (LaDi): Using LLMs to improve Text-to-Media Generators. (arXiv:2311.03716v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.03716</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in text-to-image generation have revolutionized numerous
fields, including art and cinema, by automating the generation of high-quality,
context-aware images and video. However, the utility of these technologies is
often limited by the inadequacy of text prompts in guiding the generator to
produce artistically coherent and subject-relevant images. In this paper, We
describe the techniques that can be used to make Large Language Models (LLMs)
act as Art Directors that enhance image and video generation. We describe our
unified system for this called &quot;LaDi&quot;. We explore how LaDi integrates multiple
techniques for augmenting the capabilities of text-to-image generators (T2Is)
and text-to-video generators (T2Vs), with a focus on constrained decoding,
intelligent prompting, fine-tuning, and retrieval. LaDi and these techniques
are being used today in apps and platforms developed by Plai Labs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roush_A/0/1/0/all/0/1&quot;&gt;Allen Roush&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zakirov_E/0/1/0/all/0/1&quot;&gt;Emil Zakirov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shirokov_A/0/1/0/all/0/1&quot;&gt;Artemiy Shirokov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lunina_P/0/1/0/all/0/1&quot;&gt;Polina Lunina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gane_J/0/1/0/all/0/1&quot;&gt;Jack Gane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duffy_A/0/1/0/all/0/1&quot;&gt;Alexander Duffy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basil_C/0/1/0/all/0/1&quot;&gt;Charlie Basil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whitcomb_A/0/1/0/all/0/1&quot;&gt;Aber Whitcomb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benedetto_J/0/1/0/all/0/1&quot;&gt;Jim Benedetto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeWolfe_C/0/1/0/all/0/1&quot;&gt;Chris DeWolfe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03722">
<title>Inertial Guided Uncertainty Estimation of Feature Correspondence in Visual-Inertial Odometry/SLAM. (arXiv:2311.03722v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.03722</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual odometry and Simultaneous Localization And Mapping (SLAM) has been
studied as one of the most important tasks in the areas of computer vision and
robotics, to contribute to autonomous navigation and augmented reality systems.
In case of feature-based odometry/SLAM, a moving visual sensor observes a set
of 3D points from different viewpoints, correspondences between the projected
2D points in each image are usually established by feature tracking and
matching. However, since the corresponding point could be erroneous and noisy,
reliable uncertainty estimation can improve the accuracy of odometry/SLAM
methods. In addition, inertial measurement unit is utilized to aid the visual
sensor in terms of Visual-Inertial fusion. In this paper, we propose a method
to estimate the uncertainty of feature correspondence using an inertial
guidance robust to image degradation caused by motion blur, illumination change
and occlusion. Modeling a guidance distribution to sample possible
correspondence, we fit the distribution to an energy function based on image
error, yielding more robust uncertainty than conventional methods. We also
demonstrate the feasibility of our approach by incorporating it into one of
recent visual-inertial odometry/SLAM algorithms for public datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Seongwook Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jaehyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sull_S/0/1/0/all/0/1&quot;&gt;Sanghoon Sull&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03725">
<title>DeepInspect: An AI-Powered Defect Detection for Manufacturing Industries. (arXiv:2311.03725v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03725</link>
<description rdf:parseType="Literal">&lt;p&gt;Utilizing Convolutional Neural Networks (CNNs), Recurrent Neural Networks
(RNNs), and Generative Adversarial Networks (GANs), our system introduces an
innovative approach to defect detection in manufacturing. This technology
excels in precisely identifying faults by extracting intricate details from
product photographs, utilizing RNNs to detect evolving errors and generating
synthetic defect data to bolster the model&apos;s robustness and adaptability across
various defect scenarios. The project leverages a deep learning framework to
automate real-time flaw detection in the manufacturing process. It harnesses
extensive datasets of annotated images to discern complex defect patterns. This
integrated system seamlessly fits into production workflows, thereby boosting
efficiency and elevating product quality. As a result, it reduces waste and
operational costs, ultimately enhancing market competitiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumbhar_A/0/1/0/all/0/1&quot;&gt;Arti Kumbhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chougale_A/0/1/0/all/0/1&quot;&gt;Amruta Chougale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lokhande_P/0/1/0/all/0/1&quot;&gt;Priya Lokhande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navaghane_S/0/1/0/all/0/1&quot;&gt;Saloni Navaghane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burud_A/0/1/0/all/0/1&quot;&gt;Aditi Burud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nimbalkar_S/0/1/0/all/0/1&quot;&gt;Saee Nimbalkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03742">
<title>3DifFusionDet: Diffusion Model for 3D Object Detection with Robust LiDAR-Camera Fusion. (arXiv:2311.03742v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03742</link>
<description rdf:parseType="Literal">&lt;p&gt;Good 3D object detection performance from LiDAR-Camera sensors demands
seamless feature alignment and fusion strategies. We propose the 3DifFusionDet
framework in this paper, which structures 3D object detection as a denoising
diffusion process from noisy 3D boxes to target boxes. In this framework,
ground truth boxes diffuse in a random distribution for training, and the model
learns to reverse the noising process. During inference, the model gradually
refines a set of boxes that were generated at random to the outcomes. Under the
feature align strategy, the progressive refinement method could make a
significant contribution to robust LiDAR-Camera fusion. The iterative
refinement process could also demonstrate great adaptability by applying the
framework to various detecting circumstances where varying levels of accuracy
and speed are required. Extensive experiments on KITTI, a benchmark for
real-world traffic object identification, revealed that 3DifFusionDet is able
to perform favorably in comparison to earlier, well-respected detectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_X/0/1/0/all/0/1&quot;&gt;Xinhao Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drager_S/0/1/0/all/0/1&quot;&gt;Simon Dr&amp;#xe4;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiawei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03745">
<title>Unsupervised Video Summarization. (arXiv:2311.03745v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03745</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a new, unsupervised method for automatic video
summarization using ideas from generative adversarial networks but eliminating
the discriminator, having a simple loss function, and separating training of
different parts of the model. An iterative training strategy is also applied by
alternately training the reconstructor and the frame selector for multiple
iterations. Furthermore, a trainable mask vector is added to the model in
summary generation during training and evaluation. The method also includes an
unsupervised model selection algorithm. Results from experiments on two public
datasets (SumMe and TVSum) and four datasets we created (Soccer, LoL, MLB, and
ShortMLB) demonstrate the effectiveness of each component on the model
performance, particularly the iterative training strategy. Evaluations and
comparisons with the state-of-the-art methods highlight the advantages of the
proposed method in performance, stability, and training efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hanqing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klabjan_D/0/1/0/all/0/1&quot;&gt;Diego Klabjan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Utke_J/0/1/0/all/0/1&quot;&gt;Jean Utke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03747">
<title>SBCFormer: Lightweight Network Capable of Full-size ImageNet Classification at 1 FPS on Single Board Computers. (arXiv:2311.03747v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03747</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer vision has become increasingly prevalent in solving real-world
problems across diverse domains, including smart agriculture, fishery, and
livestock management. These applications may not require processing many image
frames per second, leading practitioners to use single board computers (SBCs).
Although many lightweight networks have been developed for mobile/edge devices,
they primarily target smartphones with more powerful processors and not SBCs
with the low-end CPUs. This paper introduces a CNN-ViT hybrid network called
SBCFormer, which achieves high accuracy and fast computation on such low-end
CPUs. The hardware constraints of these CPUs make the Transformer&apos;s attention
mechanism preferable to convolution. However, using attention on low-end CPUs
presents a challenge: high-resolution internal feature maps demand excessive
computational resources, but reducing their resolution results in the loss of
local image details. SBCFormer introduces an architectural design to address
this issue. As a result, SBCFormer achieves the highest trade-off between
accuracy and speed on a Raspberry Pi 4 Model B with an ARM-Cortex A72 CPU. For
the first time, it achieves an ImageNet-1K top-1 accuracy of around 80% at a
speed of 1.0 frame/sec on the SBC. Code is available at
https://github.com/xyongLu/SBCFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiangyong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1&quot;&gt;Masanori Suganuma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1&quot;&gt;Takayuki Okatani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03749">
<title>Multiclass Segmentation using Teeth Attention Modules for Dental X-ray Images. (arXiv:2311.03749v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03749</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposed a cutting-edge multiclass teeth segmentation architecture
that integrates an M-Net-like structure with Swin Transformers and a novel
component named Teeth Attention Block (TAB). Existing teeth image segmentation
methods have issues with less accurate and unreliable segmentation outcomes due
to the complex and varying morphology of teeth, although teeth segmentation in
dental panoramic images is essential for dental disease diagnosis. We propose a
novel teeth segmentation model incorporating an M-Net-like structure with Swin
Transformers and TAB. The proposed TAB utilizes a unique attention mechanism
that focuses specifically on the complex structures of teeth. The attention
mechanism in TAB precisely highlights key elements of teeth features in
panoramic images, resulting in more accurate segmentation outcomes. The
proposed architecture effectively captures local and global contextual
information, accurately defining each tooth and its surrounding structures.
Furthermore, we employ a multiscale supervision strategy, which leverages the
left and right legs of the U-Net structure, boosting the performance of the
segmentation with enhanced feature representation. The squared Dice loss is
utilized to tackle the class imbalance issue, ensuring accurate segmentation
across all classes. The proposed method was validated on a panoramic teeth
X-ray dataset, which was taken in a real-world dental diagnosis. The
experimental results demonstrate the efficacy of our proposed architecture for
tooth segmentation on multiple benchmark dental image datasets, outperforming
existing state-of-the-art methods in objective metrics and visual examinations.
This study has the potential to significantly enhance dental image analysis and
contribute to advances in dental applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghafoor_A/0/1/0/all/0/1&quot;&gt;Afnan Ghafoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1&quot;&gt;Seong-Yong Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1&quot;&gt;Bumshik Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03762">
<title>Image change detection with only a few samples. (arXiv:2311.03762v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03762</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers image change detection with only a small number of
samples, which is a significant problem in terms of a few annotations
available. A major impediment of image change detection task is the lack of
large annotated datasets covering a wide variety of scenes. Change detection
models trained on insufficient datasets have shown poor generalization
capability. To address the poor generalization issue, we propose using simple
image processing methods for generating synthetic but informative datasets, and
design an early fusion network based on object detection which could outperform
the siamese neural network. Our key insight is that the synthetic data enables
the trained model to have good generalization ability for various scenarios. We
compare the model trained on the synthetic data with that on the real-world
data captured from a challenging dataset, CDNet, using six different test sets.
The results demonstrate that the synthetic data is informative enough to
achieve higher generalization ability than the insufficient real-world data.
Besides, the experiment shows that utilizing a few (often tens of) samples to
fine-tune the model trained on the synthetic data will achieve excellent
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Ke Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhaoyi Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1&quot;&gt;Haoyue Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03770">
<title>Lightweight Portrait Matting via Regional Attention and Refinement. (arXiv:2311.03770v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03770</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a lightweight model for high resolution portrait matting. The
model does not use any auxiliary inputs such as trimaps or background captures
and achieves real time performance for HD videos and near real time for 4K. Our
model is built upon a two-stage framework with a low resolution network for
coarse alpha estimation followed by a refinement network for local region
improvement. However, a naive implementation of the two-stage model suffers
from poor matting quality if not utilizing any auxiliary inputs. We address the
performance gap by leveraging the vision transformer (ViT) as the backbone of
the low resolution network, motivated by the observation that the tokenization
step of ViT can reduce spatial resolution while retain as much pixel
information as possible. To inform local regions of the context, we propose a
novel cross region attention (CRA) module in the refinement network to
propagate the contextual information across the neighboring regions. We
demonstrate that our method achieves superior results and outperforms other
baselines on three benchmark datasets while only uses $1/20$ of the FLOPS
compared to the existing state-of-the-art model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yatao Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zharkov_I/0/1/0/all/0/1&quot;&gt;Ilya Zharkov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03774">
<title>Meta-Adapter: An Online Few-shot Learner for Vision-Language Model. (arXiv:2311.03774v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03774</link>
<description rdf:parseType="Literal">&lt;p&gt;The contrastive vision-language pre-training, known as CLIP, demonstrates
remarkable potential in perceiving open-world visual concepts, enabling
effective zero-shot image recognition. Nevertheless, few-shot learning methods
based on CLIP typically require offline fine-tuning of the parameters on
few-shot samples, resulting in longer inference time and the risk of
over-fitting in certain domains. To tackle these challenges, we propose the
Meta-Adapter, a lightweight residual-style adapter, to refine the CLIP features
guided by the few-shot samples in an online manner. With a few training
samples, our method can enable effective few-shot learning capabilities and
generalize to unseen data or tasks without additional fine-tuning, achieving
competitive performance and high efficiency. Without bells and whistles, our
approach outperforms the state-of-the-art online few-shot learning method by an
average of 3.6\% on eight image classification datasets with higher inference
speed. Furthermore, our model is simple and flexible, serving as a
plug-and-play module directly applicable to downstream tasks. Without further
fine-tuning, Meta-Adapter obtains notable performance improvements in
open-vocabulary object detection and segmentation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Cheng Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Lin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_R/0/1/0/all/0/1&quot;&gt;Ruoyi Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Hongbin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yixiao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03782">
<title>CapST: An Enhanced and Lightweight Method for Deepfake Video Classification. (arXiv:2311.03782v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03782</link>
<description rdf:parseType="Literal">&lt;p&gt;The proliferation of deepfake videos, synthetic media produced through
advanced Artificial Intelligence techniques has raised significant concerns
across various sectors, encompassing realms such as politics, entertainment,
and security. In response, this research introduces an innovative and
streamlined model designed to classify deepfake videos generated by five
distinct encoders adeptly. Our approach not only achieves state of the art
performance but also optimizes computational resources. At its core, our
solution employs part of a VGG19bn as a backbone to efficiently extract
features, a strategy proven effective in image-related tasks. We integrate a
Capsule Network coupled with a Spatial Temporal attention mechanism to bolster
the model&apos;s classification capabilities while conserving resources. This
combination captures intricate hierarchies among features, facilitating robust
identification of deepfake attributes. Delving into the intricacies of our
innovation, we introduce an existing video level fusion technique that artfully
capitalizes on temporal attention mechanisms. This mechanism serves to handle
concatenated feature vectors, capitalizing on the intrinsic temporal
dependencies embedded within deepfake videos. By aggregating insights across
frames, our model gains a holistic comprehension of video content, resulting in
more precise predictions. Experimental results on an extensive benchmark
dataset of deepfake videos called DFDM showcase the efficacy of our proposed
method. Notably, our approach achieves up to a 4 percent improvement in
accurately categorizing deepfake videos compared to baseline models, all while
demanding fewer computational resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1&quot;&gt;Wasim Ahmad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yan-Tsung Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yuan-Hao Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganfure_G/0/1/0/all/0/1&quot;&gt;Gaddisa Olani Ganfure&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Sarwar Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahzad_S/0/1/0/all/0/1&quot;&gt;Sahibzada Adil Shahzad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03784">
<title>UP-NeRF: Unconstrained Pose-Prior-Free Neural Radiance Fields. (arXiv:2311.03784v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03784</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Field (NeRF) has enabled novel view synthesis with high
fidelity given images and camera poses. Subsequent works even succeeded in
eliminating the necessity of pose priors by jointly optimizing NeRF and camera
pose. However, these works are limited to relatively simple settings such as
photometrically consistent and occluder-free image collections or a sequence of
images from a video. So they have difficulty handling unconstrained images with
varying illumination and transient occluders. In this paper, we propose
\textbf{UP-NeRF} (\textbf{U}nconstrained \textbf{P}ose-prior-free
\textbf{Ne}ural \textbf{R}adiance \textbf{F}ields) to optimize NeRF with
unconstrained image collections without camera pose prior. We tackle these
challenges with surrogate tasks that optimize color-insensitive feature fields
and a separate module for transient occluders to block their influence on pose
estimation. In addition, we introduce a candidate head to enable more robust
pose estimation and transient-aware depth supervision to minimize the effect of
incorrect prior. Our experiments verify the superior performance of our method
compared to the baselines including BARF and its variants in a challenging
internet photo collection, \textit{Phototourism} dataset. The code of UP-NeRF
is available at \url{https://github.com/mlvlab/UP-NeRF}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1&quot;&gt;Injae Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1&quot;&gt;Minhyuk Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyunwoo J. Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03785">
<title>Self-MI: Efficient Multimodal Fusion via Self-Supervised Multi-Task Learning with Auxiliary Mutual Information Maximization. (arXiv:2311.03785v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03785</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal representation learning poses significant challenges in capturing
informative and distinct features from multiple modalities. Existing methods
often struggle to exploit the unique characteristics of each modality due to
unified multimodal annotations. In this study, we propose Self-MI in the
self-supervised learning fashion, which also leverage Contrastive Predictive
Coding (CPC) as an auxiliary technique to maximize the Mutual Information (MI)
between unimodal input pairs and the multimodal fusion result with unimodal
inputs. Moreover, we design a label generation module, $ULG_{MI}$ for short,
that enables us to create meaningful and informative labels for each modality
in a self-supervised manner. By maximizing the Mutual Information, we encourage
better alignment between the multimodal fusion and the individual modalities,
facilitating improved multimodal fusion. Extensive experiments on three
benchmark datasets including CMU-MOSI, CMU-MOSEI, and SIMS, demonstrate the
effectiveness of Self-MI in enhancing the multimodal fusion task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1&quot;&gt;Cam-Van Thi Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1&quot;&gt;Ngoc-Hoa Thi Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1&quot;&gt;Duc-Trong Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_Q/0/1/0/all/0/1&quot;&gt;Quang-Thuy Ha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03799">
<title>Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models. (arXiv:2311.03799v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03799</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-object interaction (HOI) detection aims to comprehend the intricate
relationships between humans and objects, predicting $&amp;lt;human, action, object&amp;gt;$
triplets, and serving as the foundation for numerous computer vision tasks. The
complexity and diversity of human-object interactions in the real world,
however, pose significant challenges for both annotation and recognition,
particularly in recognizing interactions within an open world context. This
study explores the universal interaction recognition in an open-world setting
through the use of Vision-Language (VL) foundation models and large language
models (LLMs). The proposed method is dubbed as \emph{\textbf{UniHOI}}. We
conduct a deep analysis of the three hierarchical features inherent in visual
HOI detectors and propose a method for high-level relation extraction aimed at
VL foundation models, which we call HO prompt-based learning. Our design
includes an HO Prompt-guided Decoder (HOPD), facilitates the association of
high-level relation representations in the foundation model with various HO
pairs within the image. Furthermore, we utilize a LLM (\emph{i.e.} GPT) for
interaction interpretation, generating a richer linguistic understanding for
complex HOIs. For open-category interaction recognition, our method supports
either of two input types: interaction phrase or interpretive sentence. Our
efficient architecture design and learning methods effectively unleash the
potential of the VL foundation models and LLMs, allowing UniHOI to surpass all
existing methods with a substantial margin, under both supervised and zero-shot
settings. The code and pre-trained weights are available at:
\url{https://github.com/Caoyichao/UniHOI}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yichao Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1&quot;&gt;Qingfei Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1&quot;&gt;Xiu Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Chen Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1&quot;&gt;Shan You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiaobo Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03828">
<title>Multi-view Information Integration and Propagation for Occluded Person Re-identification. (arXiv:2311.03828v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03828</link>
<description rdf:parseType="Literal">&lt;p&gt;Occluded person re-identification (re-ID) presents a challenging task due to
occlusion perturbations. Although great efforts have been made to prevent the
model from being disturbed by occlusion noise, most current solutions only
capture information from a single image, disregarding the rich complementary
information available in multiple images depicting the same pedestrian. In this
paper, we propose a novel framework called Multi-view Information Integration
and Propagation (MVI$^{2}$P). Specifically, realizing the potential of
multi-view images in effectively characterizing the occluded target pedestrian,
we integrate feature maps of which to create a comprehensive representation.
During this process, to avoid introducing occlusion noise, we develop a
CAMs-aware Localization module that selectively integrates information
contributing to the identification. Additionally, considering the divergence in
the discriminative nature of different images, we design a probability-aware
Quantification module to emphatically integrate highly reliable information.
Moreover, as multiple images with the same identity are not accessible in the
testing stage, we devise an Information Propagation (IP) mechanism to distill
knowledge from the comprehensive representation to that of a single occluded
image. Extensive experiments and analyses have unequivocally demonstrated the
effectiveness and superiority of the proposed MVI$^{2}$P. The code will be
released at \url{https://github.com/nengdong96/MVIIP}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1&quot;&gt;Neng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shuanglin Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jinhui Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liyan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03830">
<title>Reducing Spatial Fitting Error in Distillation of Denoising Diffusion Models. (arXiv:2311.03830v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03830</link>
<description rdf:parseType="Literal">&lt;p&gt;Denoising Diffusion models have exhibited remarkable capabilities in image
generation. However, generating high-quality samples requires a large number of
iterations. Knowledge distillation for diffusion models is an effective method
to address this limitation with a shortened sampling process but causes
degraded generative quality. Based on our analysis with bias-variance
decomposition and experimental observations, we attribute the degradation to
the spatial fitting error occurring in the training of both the teacher and
student model. Accordingly, we propose $\textbf{S}$patial
$\textbf{F}$itting-$\textbf{E}$rror $\textbf{R}$eduction
$\textbf{D}$istillation model ($\textbf{SFERD}$). SFERD utilizes attention
guidance from the teacher model and a designed semantic gradient predictor to
reduce the student&apos;s fitting error. Empirically, our proposed model facilitates
high-quality sample generation in a few function evaluations. We achieve an FID
of 5.31 on CIFAR-10 and 9.39 on ImageNet 64$\times$64 with only one step,
outperforming existing diffusion methods. Our study provides a new perspective
on diffusion distillation by highlighting the intrinsic denoising ability of
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shengzhe Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Z/0/1/0/all/0/1&quot;&gt;Zejian Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shengyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1&quot;&gt;Lefan Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Changyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lingyun Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03866">
<title>SCONE-GAN: Semantic Contrastive learning-based Generative Adversarial Network for an end-to-end image translation. (arXiv:2311.03866v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03866</link>
<description rdf:parseType="Literal">&lt;p&gt;SCONE-GAN presents an end-to-end image translation, which is shown to be
effective for learning to generate realistic and diverse scenery images. Most
current image-to-image translation approaches are devised as two mappings: a
translation from the source to target domain and another to represent its
inverse. While successful in many applications, these approaches may suffer
from generating trivial solutions with limited diversity. That is because these
methods learn more frequent associations rather than the scene structures. To
mitigate the problem, we propose SCONE-GAN that utilises graph convolutional
networks to learn the objects dependencies, maintain the image structure and
preserve its semantics while transferring images into the target domain. For
more realistic and diverse image generation we introduce style reference image.
We enforce the model to maximize the mutual information between the style image
and output. The proposed method explicitly maximizes the mutual information
between the related patches, thus encouraging the generator to produce more
diverse images. We validate the proposed algorithm for image-to-image
translation and stylizing outdoor images. Both qualitative and quantitative
results demonstrate the effectiveness of our approach on four dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbasnejad_I/0/1/0/all/0/1&quot;&gt;Iman Abbasnejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zambetta_F/0/1/0/all/0/1&quot;&gt;Fabio Zambetta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1&quot;&gt;Flora Salim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiley_T/0/1/0/all/0/1&quot;&gt;Timothy Wiley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1&quot;&gt;Jeffrey Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallagher_R/0/1/0/all/0/1&quot;&gt;Russell Gallagher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbasnejad_E/0/1/0/all/0/1&quot;&gt;Ehsan Abbasnejad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03867">
<title>A Comparative Study of Knowledge Transfer Methods for Misaligned Urban Building Labels. (arXiv:2311.03867v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03867</link>
<description rdf:parseType="Literal">&lt;p&gt;Misalignment in Earth observation (EO) images and building labels impact the
training of accurate convolutional neural networks (CNNs) for semantic
segmentation of building footprints. Recently, three Teacher-Student knowledge
transfer methods have been introduced to address this issue: supervised domain
adaptation (SDA), knowledge distillation (KD), and deep mutual learning (DML).
However, these methods are merely studied for different urban buildings
(low-rise, mid-rise, high-rise, and skyscrapers), where misalignment increases
with building height and spatial resolution. In this study, we present a
workflow for the systematic comparative study of the three methods. The
workflow first identifies the best (with the highest evaluation scores)
hyperparameters, lightweight CNNs for the Student (among 43 CNNs from Computer
Vision), and encoder-decoder networks (EDNs) for both Teachers and Students.
Secondly, three building footprint datasets are developed to train and evaluate
the identified Teachers and Students in the three transfer methods. The results
show that U-Net with VGG19 (U-VGG19) is the best Teacher, and
U-EfficientNetv2B3 and U-EfficientNet-lite0 are among the best Students. With
these Teacher-Student pairs, SDA could yield upto 0.943, 0.868, 0.912, and
0.697 F1 scores in the low-rise, mid-rise, high-rise, and skyscrapers
respectively. KD and DML provide model compression of upto 82%, despite
marginal loss in performance. This new comparison concludes that SDA is the
most effective method to address the misalignment problem, while KD and DML can
efficiently compress network size without significant loss in performance. The
158 experiments and datasets developed in this study will be valuable to
minimise the misaligned labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neupane_B/0/1/0/all/0/1&quot;&gt;Bipul Neupane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aryal_J/0/1/0/all/0/1&quot;&gt;Jagannath Aryal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajabifard_A/0/1/0/all/0/1&quot;&gt;Abbas Rajabifard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03873">
<title>Mini but Mighty: Finetuning ViTs with Mini Adapters. (arXiv:2311.03873v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03873</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) have become one of the dominant architectures in
computer vision, and pre-trained ViT models are commonly adapted to new tasks
via fine-tuning. Recent works proposed several parameter-efficient transfer
learning methods, such as adapters, to avoid the prohibitive training and
storage cost of finetuning. In this work, we observe that adapters perform
poorly when the dimension of adapters is small, and we propose MiMi, a training
framework that addresses this issue. We start with large adapters which can
reach high performance, and iteratively reduce their size. To enable automatic
estimation of the hidden dimension of every adapter, we also introduce a new
scoring function, specifically designed for adapters, that compares the neuron
importance across layers. Our method outperforms existing methods in finding
the best trade-off between accuracy and trained parameters across the three
dataset benchmarks DomainNet, VTAB, and Multi-task, for a total of 29 datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marouf_I/0/1/0/all/0/1&quot;&gt;Imad Eddine Marouf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tartaglione_E/0/1/0/all/0/1&quot;&gt;Enzo Tartaglione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Lathuili&amp;#xe8;re&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03884">
<title>MeVGAN: GAN-based Plugin Model for Video Generation with Applications in Colonoscopy. (arXiv:2311.03884v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.03884</link>
<description rdf:parseType="Literal">&lt;p&gt;Video generation is important, especially in medicine, as much data is given
in this form. However, video generation of high-resolution data is a very
demanding task for generative models, due to the large need for memory. In this
paper, we propose Memory Efficient Video GAN (MeVGAN) - a Generative
Adversarial Network (GAN) which uses plugin-type architecture. We use a
pre-trained 2D-image GAN and only add a simple neural network to construct
respective trajectories in the noise space, so that the trajectory forwarded
through the GAN model constructs a real-life video. We apply MeVGAN in the task
of generating colonoscopy videos. Colonoscopy is an important medical
procedure, especially beneficial in screening and managing colorectal cancer.
However, because colonoscopy is difficult and time-consuming to learn,
colonoscopy simulators are widely used in educating young colonoscopists. We
show that MeVGAN can produce good quality synthetic colonoscopy videos, which
can be potentially used in virtual simulators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Struski_L/0/1/0/all/0/1&quot;&gt;&amp;#x141;ukasz Struski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Urbanczyk_T/0/1/0/all/0/1&quot;&gt;Tomasz Urba&amp;#x144;czyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bucki_K/0/1/0/all/0/1&quot;&gt;Krzysztof Bucki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cupial_B/0/1/0/all/0/1&quot;&gt;Bart&amp;#x142;omiej Cupia&amp;#x142;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kaczynska_A/0/1/0/all/0/1&quot;&gt;Aneta Kaczy&amp;#x144;ska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Spurek_P/0/1/0/all/0/1&quot;&gt;Przemys&amp;#x142;aw Spurek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tabor_J/0/1/0/all/0/1&quot;&gt;Jacek Tabor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03904">
<title>RobustMat: Neural Diffusion for Street Landmark Patch Matching under Challenging Environments. (arXiv:2311.03904v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03904</link>
<description rdf:parseType="Literal">&lt;p&gt;For autonomous vehicles (AVs), visual perception techniques based on sensors
like cameras play crucial roles in information acquisition and processing. In
various computer perception tasks for AVs, it may be helpful to match landmark
patches taken by an onboard camera with other landmark patches captured at a
different time or saved in a street scene image database. To perform matching
under challenging driving environments caused by changing seasons, weather, and
illumination, we utilize the spatial neighborhood information of each patch. We
propose an approach, named RobustMat, which derives its robustness to
perturbations from neural differential equations. A convolutional neural ODE
diffusion module is used to learn the feature representation for the landmark
patches. A graph neural PDE diffusion module then aggregates information from
neighboring landmark patches in the street scene. Finally, feature similarity
learning outputs the final matching score. Our approach is evaluated on several
street scene datasets and demonstrated to achieve state-of-the-art matching
results under environmental perturbations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+She_R/0/1/0/all/0/1&quot;&gt;Rui She&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Q/0/1/0/all/0/1&quot;&gt;Qiyu Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuan-Rui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Kai Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tay_W/0/1/0/all/0/1&quot;&gt;Wee Peng Tay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03912">
<title>FLORA: Fine-grained Low-Rank Architecture Search for Vision Transformer. (arXiv:2311.03912v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03912</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViT) have recently demonstrated success across a myriad
of computer vision tasks. However, their elevated computational demands pose
significant challenges for real-world deployment. While low-rank approximation
stands out as a renowned method to reduce computational loads, efficiently
automating the target rank selection in ViT remains a challenge. Drawing from
the notable similarity and alignment between the processes of rank selection
and One-Shot NAS, we introduce FLORA, an end-to-end automatic framework based
on NAS. To overcome the design challenge of supernet posed by vast search
space, FLORA employs a low-rank aware candidate filtering strategy. This method
adeptly identifies and eliminates underperforming candidates, effectively
alleviating potential undertraining and interference among subnetworks. To
further enhance the quality of low-rank supernets, we design a low-rank
specific training paradigm. First, we propose weight inheritance to construct
supernet and enable gradient sharing among low-rank modules. Secondly, we adopt
low-rank aware sampling to strategically allocate training resources, taking
into account inherited information from pre-trained models. Empirical results
underscore FLORA&apos;s efficacy. With our method, a more fine-grained rank
configuration can be generated automatically and yield up to 33% extra FLOPs
reduction compared to a simple uniform configuration. More specific,
FLORA-DeiT-B/FLORA-Swin-B can save up to 55%/42% FLOPs almost without
performance degradtion. Importantly, FLORA boasts both versatility and
orthogonality, offering an extra 21%-26% FLOPs reduction when integrated with
leading compression techniques or compact hybrid structures. Our code is
publicly available at https://github.com/shadowpa0327/FLORA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Chi-Chih Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1&quot;&gt;Yuan-Yao Sung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shixing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_N/0/1/0/all/0/1&quot;&gt;Ning-Chi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marculescu_D/0/1/0/all/0/1&quot;&gt;Diana Marculescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1&quot;&gt;Kai-Chiang Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03938">
<title>Analysis of NaN Divergence in Training Monocular Depth Estimation Model. (arXiv:2311.03938v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03938</link>
<description rdf:parseType="Literal">&lt;p&gt;The latest advances in deep learning have facilitated the development of
highly accurate monocular depth estimation models. However, when training a
monocular depth estimation network, practitioners and researchers have observed
not a number (NaN) loss, which disrupts gradient descent optimization. Although
several practitioners have reported the stochastic and mysterious occurrence of
NaN loss that bothers training, its root cause is not discussed in the
literature. This study conducted an in-depth analysis of NaN loss during
training a monocular depth estimation network and identified three types of
vulnerabilities that cause NaN loss: 1) the use of square root loss, which
leads to an unstable gradient; 2) the log-sigmoid function, which exhibits
numerical stability issues; and 3) certain variance implementations, which
yield incorrect computations. Furthermore, for each vulnerability, the
occurrence of NaN loss was demonstrated and practical guidelines to prevent NaN
loss were presented. Experiments showed that both optimization stability and
performance on monocular depth estimation could be improved by following our
guidelines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Bum Jun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1&quot;&gt;Hyeonah Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sang Woo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03943">
<title>CLIP Guided Image-perceptive Prompt Learning for Image Enhancement. (arXiv:2311.03943v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03943</link>
<description rdf:parseType="Literal">&lt;p&gt;Image enhancement is a significant research area in the fields of computer
vision and image processing. In recent years, many learning-based methods for
image enhancement have been developed, where the Look-up-table (LUT) has proven
to be an effective tool. In this paper, we delve into the potential of
Contrastive Language-Image Pre-Training (CLIP) Guided Prompt Learning,
proposing a simple structure called CLIP-LUT for image enhancement. We found
that the prior knowledge of CLIP can effectively discern the quality of
degraded images, which can provide reliable guidance. To be specific, We
initially learn image-perceptive prompts to distinguish between original and
target images using CLIP model, in the meanwhile, we introduce a very simple
network by incorporating a simple baseline to predict the weights of three
different LUT as enhancement network. The obtained prompts are used to steer
the enhancement network like a loss function and improve the performance of
model. We demonstrate that by simply combining a straightforward method with
CLIP, we can obtain satisfactory results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zinuo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1&quot;&gt;Qiuhong Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weiwen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03959">
<title>Improving the Effectiveness of Deep Generative Data. (arXiv:2311.03959v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03959</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent deep generative models (DGMs) such as generative adversarial networks
(GANs) and diffusion probabilistic models (DPMs) have shown their impressive
ability in generating high-fidelity photorealistic images. Although looking
appealing to human eyes, training a model on purely synthetic images for
downstream image processing tasks like image classification often results in an
undesired performance drop compared to training on real data. Previous works
have demonstrated that enhancing a real dataset with synthetic images from DGMs
can be beneficial. However, the improvements were subjected to certain
circumstances and yet were not comparable to adding the same number of real
images. In this work, we propose a new taxonomy to describe factors
contributing to this commonly observed phenomenon and investigate it on the
popular CIFAR-10 dataset. We hypothesize that the Content Gap accounts for a
large portion of the performance drop when using synthetic images from DGM and
propose strategies to better utilize them in downstream tasks. Extensive
experiments on multiple datasets showcase that our method outperforms baselines
on downstream classification tasks both in case of training on synthetic only
(Synthetic-to-Real) and training on a mix of real and synthetic data (Data
Augmentation), particularly in the data-scarce scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmedding_S/0/1/0/all/0/1&quot;&gt;Sabrina Schmedding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1&quot;&gt;Marco F. Huber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03964">
<title>Enhancing Multimodal Compositional Reasoning of Visual Language Models with Generative Negative Mining. (arXiv:2311.03964v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03964</link>
<description rdf:parseType="Literal">&lt;p&gt;Contemporary large-scale visual language models (VLMs) exhibit strong
representation capacities, making them ubiquitous for enhancing image and text
understanding tasks. They are often trained in a contrastive manner on a large
and diverse corpus of images and corresponding text captions scraped from the
internet. Despite this, VLMs often struggle with compositional reasoning tasks
which require a fine-grained understanding of the complex interactions of
objects and their attributes. This failure can be attributed to two main
factors: 1) Contrastive approaches have traditionally focused on mining
negative examples from existing datasets. However, the mined negative examples
might not be difficult for the model to discriminate from the positive. An
alternative to mining would be negative sample generation 2) But existing
generative approaches primarily focus on generating hard negative texts
associated with a given image. Mining in the other direction, i.e., generating
negative image samples associated with a given text has been ignored. To
overcome both these limitations, we propose a framework that not only mines in
both directions but also generates challenging negative samples in both
modalities, i.e., images and texts. Leveraging these generative hard negative
samples, we significantly enhance VLMs&apos; performance in tasks involving
multimodal compositional reasoning. Our code and dataset are released at
https://ugorsahin.github.io/enhancing-multimodal-compositional-reasoning-of-vlm.html.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahin_U/0/1/0/all/0/1&quot;&gt;Ugur Sahin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_Q/0/1/0/all/0/1&quot;&gt;Qadeer Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1&quot;&gt;Daniel Cremers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1&quot;&gt;Volker Tresp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03965">
<title>Fast Sun-aligned Outdoor Scene Relighting based on TensoRF. (arXiv:2311.03965v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03965</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we introduce our method of outdoor scene relighting for Neural
Radiance Fields (NeRF) named Sun-aligned Relighting TensoRF (SR-TensoRF).
SR-TensoRF offers a lightweight and rapid pipeline aligned with the sun,
thereby achieving a simplified workflow that eliminates the need for
environment maps. Our sun-alignment strategy is motivated by the insight that
shadows, unlike viewpoint-dependent albedo, are determined by light direction.
We directly use the sun direction as an input during shadow generation,
simplifying the requirements of the inference process significantly. Moreover,
SR-TensoRF leverages the training efficiency of TensoRF by incorporating our
proposed cubemap concept, resulting in notable acceleration in both training
and rendering processes compared to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yeonjin Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yearim Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1&quot;&gt;Seunghyeon Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Jung Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1&quot;&gt;Nojun Kwak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03967">
<title>CeCNN: Copula-enhanced convolutional neural networks in joint prediction of refraction error and axial length based on ultra-widefield fundus images. (arXiv:2311.03967v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03967</link>
<description rdf:parseType="Literal">&lt;p&gt;Ultra-widefield (UWF) fundus images are replacing traditional fundus images
in screening, detection, prediction, and treatment of complications related to
myopia because their much broader visual range is advantageous for highly
myopic eyes. Spherical equivalent (SE) is extensively used as the main myopia
outcome measure, and axial length (AL) has drawn increasing interest as an
important ocular component for assessing myopia. Cutting-edge studies show that
SE and AL are strongly correlated. Using the joint information from SE and AL
is potentially better than using either separately. In the deep learning
community, though there is research on multiple-response tasks with a 3D image
biomarker, dependence among responses is only sporadically taken into
consideration. Inspired by the spirit that information extracted from the data
by statistical methods can improve the prediction accuracy of deep learning
models, we formulate a class of multivariate response regression models with a
higher-order tensor biomarker, for the bivariate tasks of
regression-classification and regression-regression. Specifically, we propose a
copula-enhanced convolutional neural network (CeCNN) framework that
incorporates the dependence between responses through a Gaussian copula (with
parameters estimated from a warm-up CNN) and uses the induced copula-likelihood
loss with the backbone CNNs. We establish the statistical framework and
algorithms for the aforementioned two bivariate tasks. We show that the CeCNN
has better prediction accuracy after adding the dependency information to the
backbone models. The modeling and the proposed CeCNN algorithm are applicable
beyond the UWF scenario and can be effective with other backbones beyond ResNet
and LeNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1&quot;&gt;Chong Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Danjuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Meiyan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xingyao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1&quot;&gt;Bo Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Catherine C. Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welsh_A/0/1/0/all/0/1&quot;&gt;A.H. Welsh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03970">
<title>Bias and Diversity in Synthetic-based Face Recognition. (arXiv:2311.03970v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03970</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthetic data is emerging as a substitute for authentic data to solve
ethical and legal challenges in handling authentic face data. The current
models can create real-looking face images of people who do not exist. However,
it is a known and sensitive problem that face recognition systems are
susceptible to bias, i.e. performance differences between different demographic
and non-demographics attributes, which can lead to unfair decisions. In this
work, we investigate how the diversity of synthetic face recognition datasets
compares to authentic datasets, and how the distribution of the training data
of the generative models affects the distribution of the synthetic data. To do
this, we looked at the distribution of gender, ethnicity, age, and head
position. Furthermore, we investigated the concrete bias of three recent
synthetic-based face recognition models on the studied attributes in comparison
to a baseline model trained on authentic data. Our results show that the
generator generate a similar distribution as the used training data in terms of
the different attributes. With regard to bias, it can be seen that the
synthetic-based models share a similar bias behavior with the authentic-based
models. However, with the uncovered lower intra-identity attribute consistency
seems to be beneficial in reducing bias.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1&quot;&gt;Marco Huber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1&quot;&gt;Anh Thi Luu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1&quot;&gt;Fadi Boutros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1&quot;&gt;Arjan Kuijper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1&quot;&gt;Naser Damer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2012.04634">
<title>Accurate 3D Object Detection using Energy-Based Models. (arXiv:2012.04634v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2012.04634</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate 3D object detection (3DOD) is crucial for safe navigation of complex
environments by autonomous robots. Regressing accurate 3D bounding boxes in
cluttered environments based on sparse LiDAR data is however a highly
challenging problem. We address this task by exploring recent advances in
conditional energy-based models (EBMs) for probabilistic regression. While
methods employing EBMs for regression have demonstrated impressive performance
on 2D object detection in images, these techniques are not directly applicable
to 3D bounding boxes. In this work, we therefore design a differentiable
pooling operator for 3D bounding boxes, serving as the core module of our EBM
network. We further integrate this general approach into the state-of-the-art
3D object detector SA-SSD. On the KITTI dataset, our proposed approach
consistently outperforms the SA-SSD baseline across all 3DOD metrics,
demonstrating the potential of EBM-based regression for highly accurate 3DOD.
Code is available at https://github.com/fregu856/ebms_3dod.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gustafsson_F/0/1/0/all/0/1&quot;&gt;Fredrik K. Gustafsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1&quot;&gt;Martin Danelljan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1&quot;&gt;Thomas B. Sch&amp;#xf6;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2012.08026">
<title>Classification of Smoking and Calling using Deep Learning. (arXiv:2012.08026v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2012.08026</link>
<description rdf:parseType="Literal">&lt;p&gt;Since 2014, very deep convolutional neural networks have been proposed and
become the must-have weapon for champions in all kinds of competition. In this
report, a pipeline is introduced to perform the classification of smoking and
calling by modifying the pretrained inception V3. Brightness enhancing based on
deep learning is implemented to improve the classification of this
classification task along with other useful training tricks. Based on the
quality and quantity results, it can be concluded that this pipeline with small
biased samples is practical and useful with high accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Miaowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohacey_A/0/1/0/all/0/1&quot;&gt;Alexander William Mohacey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apfel_J/0/1/0/all/0/1&quot;&gt;James Apfel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.11048">
<title>K-Lane: Lidar Lane Dataset and Benchmark for Urban Roads and Highways. (arXiv:2110.11048v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2110.11048</link>
<description rdf:parseType="Literal">&lt;p&gt;Lane detection is a critical function for autonomous driving. With the recent
development of deep learning and the publication of camera lane datasets and
benchmarks, camera lane detection networks (CLDNs) have been remarkably
developed. Unfortunately, CLDNs rely on camera images which are often distorted
near the vanishing line and prone to poor lighting condition. This is in
contrast with Lidar lane detection networks (LLDNs), which can directly extract
the lane lines on the bird&apos;s eye view (BEV) for motion planning and operate
robustly under various lighting conditions. However, LLDNs have not been
actively studied, mostly due to the absence of large public lidar lane
datasets. In this paper, we introduce KAIST-Lane (K-Lane), the world&apos;s first
and the largest public urban road and highway lane dataset for Lidar. K-Lane
has more than 15K frames and contains annotations of up to six lanes under
various road and traffic conditions, e.g., occluded roads of multiple occlusion
levels, roads at day and night times, merging (converging and diverging) and
curved lanes. We also provide baseline networks we term Lidar lane detection
networks utilizing global feature correlator (LLDN-GFC). LLDN-GFC exploits the
spatial characteristics of lane lines on the point cloud, which are sparse,
thin, and stretched along the entire ground plane of the point cloud. From
experimental results, LLDN-GFC achieves the state-of-the-art performance with
an F1- score of 82.1%, on the K-Lane. Moreover, LLDN-GFC shows strong
performance under various lighting conditions, which is unlike CLDNs, and also
robust even in the case of severe occlusions, unlike LLDNs using the
conventional CNN. The K-Lane, LLDN-GFC training code, pre-trained models, and
complete development kits including evaluation, visualization and annotation
tools are available at https://github.com/kaist-avelab/k-lane.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paek_D/0/1/0/all/0/1&quot;&gt;Donghee Paek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1&quot;&gt;Seung-Hyun Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wijaya_K/0/1/0/all/0/1&quot;&gt;Kevin Tirta Wijaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.11948">
<title>Learning Proposals for Practical Energy-Based Regression. (arXiv:2110.11948v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2110.11948</link>
<description rdf:parseType="Literal">&lt;p&gt;Energy-based models (EBMs) have experienced a resurgence within machine
learning in recent years, including as a promising alternative for
probabilistic regression. However, energy-based regression requires a proposal
distribution to be manually designed for training, and an initial estimate has
to be provided at test-time. We address both of these issues by introducing a
conceptually simple method to automatically learn an effective proposal
distribution, which is parameterized by a separate network head. To this end,
we derive a surprising result, leading to a unified training objective that
jointly minimizes the KL divergence from the proposal to the EBM, and the
negative log-likelihood of the EBM. At test-time, we can then employ importance
sampling with the trained proposal to efficiently evaluate the learned EBM and
produce stand-alone predictions. Furthermore, we utilize our derived training
objective to learn mixture density networks (MDNs) with a jointly trained
energy-based teacher, consistently outperforming conventional MDN training on
four real-world regression tasks within computer vision. Code is available at
https://github.com/fregu856/ebms_proposals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gustafsson_F/0/1/0/all/0/1&quot;&gt;Fredrik K. Gustafsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1&quot;&gt;Martin Danelljan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1&quot;&gt;Thomas B. Sch&amp;#xf6;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.03897">
<title>Geodesic Multi-Modal Mixup for Robust Fine-Tuning. (arXiv:2203.03897v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.03897</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained multi-modal models, such as CLIP, provide transferable embeddings
and show promising results in diverse applications. However, the analysis of
learned multi-modal embeddings is relatively unexplored, and the embedding
transferability can be improved. In this work, we observe that CLIP holds
separated embedding subspaces for two different modalities, and then we
investigate it through the lens of uniformity-alignment to measure the quality
of learned representation. Both theoretically and empirically, we show that
CLIP retains poor uniformity and alignment even after fine-tuning. Such a lack
of alignment and uniformity might restrict the transferability and robustness
of embeddings. To this end, we devise a new fine-tuning method for robust
representation equipping better alignment and uniformity. First, we propose a
Geodesic Multi-Modal Mixup that mixes the embeddings of image and text to
generate hard negative samples on the hypersphere. Then, we fine-tune the model
on hard negatives as well as original negatives and positives with contrastive
loss. Based on the theoretical analysis about hardness guarantee and limiting
behavior, we justify the use of our method. Extensive experiments on retrieval,
calibration, few- or zero-shot classification (under distribution shift),
embedding arithmetic, and image captioning further show that our method
provides transferable representations, enabling robust model adaptation on
diverse tasks. Code: https://github.com/changdaeoh/multimodal-mixup
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1&quot;&gt;Changdae Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+So_J/0/1/0/all/0/1&quot;&gt;Junhyuk So&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1&quot;&gt;Hoyoon Byun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_Y/0/1/0/all/0/1&quot;&gt;YongTaek Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1&quot;&gt;Minchul Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1&quot;&gt;Jong-June Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1&quot;&gt;Kyungwoo Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.08171">
<title>K-Radar: 4D Radar Object Detection for Autonomous Driving in Various Weather Conditions. (arXiv:2206.08171v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.08171</link>
<description rdf:parseType="Literal">&lt;p&gt;Unlike RGB cameras that use visible light bands (384$\sim$769 THz) and Lidars
that use infrared bands (361$\sim$331 THz), Radars use relatively longer
wavelength radio bands (77$\sim$81 GHz), resulting in robust measurements in
adverse weathers. Unfortunately, existing Radar datasets only contain a
relatively small number of samples compared to the existing camera and Lidar
datasets. This may hinder the development of sophisticated data-driven deep
learning techniques for Radar-based perception. Moreover, most of the existing
Radar datasets only provide 3D Radar tensor (3DRT) data that contain power
measurements along the Doppler, range, and azimuth dimensions. As there is no
elevation information, it is challenging to estimate the 3D bounding box of an
object from 3DRT. In this work, we introduce KAIST-Radar (K-Radar), a novel
large-scale object detection dataset and benchmark that contains 35K frames of
4D Radar tensor (4DRT) data with power measurements along the Doppler, range,
azimuth, and elevation dimensions, together with carefully annotated 3D
bounding box labels of objects on the roads. K-Radar includes challenging
driving conditions such as adverse weathers (fog, rain, and snow) on various
road structures (urban, suburban roads, alleyways, and highways). In addition
to the 4DRT, we provide auxiliary measurements from carefully calibrated
high-resolution Lidars, surround stereo cameras, and RTK-GPS. We also provide
4DRT-based object detection baseline neural networks (baseline NNs) and show
that the height information is crucial for 3D object detection. And by
comparing the baseline NN with a similarly-structured Lidar-based neural
network, we demonstrate that 4D Radar is a more robust sensor for adverse
weather conditions. All codes are available at
https://github.com/kaist-avelab/k-radar.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paek_D/0/1/0/all/0/1&quot;&gt;Dong-Hee Paek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1&quot;&gt;Seung-Hyun Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wijaya_K/0/1/0/all/0/1&quot;&gt;Kevin Tirta Wijaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.02062">
<title>Image Amodal Completion: A Survey. (arXiv:2207.02062v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.02062</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing computer vision systems can compete with humans in understanding the
visible parts of objects, but still fall far short of humans when it comes to
depicting the invisible parts of partially occluded objects. Image amodal
completion aims to equip computers with human-like amodal completion functions
to understand an intact object despite it being partially occluded. The main
purpose of this survey is to provide an intuitive understanding of the research
hotspots, key technologies and future trends in the field of image amodal
completion. Firstly, we present a comprehensive review of the latest literature
in this emerging field, exploring three key tasks in image amodal completion,
including amodal shape completion, amodal appearance completion, and order
perception. Then we examine popular datasets related to image amodal completion
along with their common data collection methods and evaluation metrics.
Finally, we discuss real-world applications and future research directions for
image amodal completion, facilitating the reader&apos;s understanding of the
challenges of existing technologies and upcoming research trends.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ao_J/0/1/0/all/0/1&quot;&gt;Jiayang Ao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1&quot;&gt;Qiuhong Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehinger_K/0/1/0/all/0/1&quot;&gt;Krista A. Ehinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.12850">
<title>SSIVD-Net: A Novel Salient Super Image Classification &amp; Detection Technique for Weaponized Violence. (arXiv:2207.12850v8 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.12850</link>
<description rdf:parseType="Literal">&lt;p&gt;Detection of violence and weaponized violence in closed-circuit television
(CCTV) footage requires a comprehensive approach. In this work, we introduce
the \emph{Smart-City CCTV Violence Detection (SCVD)} dataset, specifically
designed to facilitate the learning of weapon distribution in surveillance
videos. To tackle the complexities of analyzing 3D surveillance video for
violence recognition tasks, we propose a novel technique called
\emph{SSIVD-Net} (\textbf{S}alient-\textbf{S}uper-\textbf{I}mage for
\textbf{V}iolence \textbf{D}etection). Our method reduces 3D video data
complexity, dimensionality, and information loss while improving inference,
performance, and explainability through salient-super-Image representations.
Considering the scalability and sustainability requirements of futuristic smart
cities, the authors introduce the \emph{Salient-Classifier}, a novel
architecture combining a kernelized approach with a residual learning strategy.
We evaluate variations of SSIVD-Net and Salient Classifier on our SCVD dataset
and benchmark against state-of-the-art (SOTA) models commonly employed in
violence detection. Our approach exhibits significant improvements in detecting
both weaponized and non-weaponized violence instances. By advancing the SOTA in
violence detection, our work offers a practical and scalable solution suitable
for real-world applications. The proposed methodology not only addresses the
challenges of violence detection in CCTV footage but also contributes to the
understanding of weapon distribution in smart surveillance. Ultimately, our
research findings should enable smarter and more secure cities, as well as
enhance public safety measures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aremu_T/0/1/0/all/0/1&quot;&gt;Toluwani Aremu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhiyuan_L/0/1/0/all/0/1&quot;&gt;Li Zhiyuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alameeri_R/0/1/0/all/0/1&quot;&gt;Reem Alameeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Mustaqeem Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saddik_A/0/1/0/all/0/1&quot;&gt;Abdulmotaleb El Saddik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.01375">
<title>A Variational Approach for Joint Image Recovery and Feature Extraction Based on Spatially-Varying Generalised Gaussian Models. (arXiv:2209.01375v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.01375</link>
<description rdf:parseType="Literal">&lt;p&gt;The joint problem of reconstruction / feature extraction is a challenging
task in image processing. It consists in performing, in a joint manner, the
restoration of an image and the extraction of its features. In this work, we
firstly propose a novel nonsmooth and non-convex variational formulation of the
problem. For this purpose, we introduce a versatile generalised Gaussian prior
whose parameters, including its exponent, are space-variant. Secondly, we
design an alternating proximal-based optimisation algorithm that efficiently
exploits the structure of the proposed non-convex objective function. We also
analyse the convergence of this algorithm. As shown in numerical experiments
conducted on joint deblurring/segmentation tasks, the proposed method provides
high-quality results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chouzenoux_E/0/1/0/all/0/1&quot;&gt;Emilie Chouzenoux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corbineau_M/0/1/0/all/0/1&quot;&gt;Marie-Caroline Corbineau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pesquet_J/0/1/0/all/0/1&quot;&gt;Jean-Christophe Pesquet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scrivanti_G/0/1/0/all/0/1&quot;&gt;Gabriele Scrivanti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.15042">
<title>Generalizability of Adversarial Robustness Under Distribution Shifts. (arXiv:2209.15042v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.15042</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent progress in empirical and certified robustness promises to deliver
reliable and deployable Deep Neural Networks (DNNs). Despite that success, most
existing evaluations of DNN robustness have been done on images sampled from
the same distribution on which the model was trained. However, in the real
world, DNNs may be deployed in dynamic environments that exhibit significant
distribution shifts. In this work, we take a first step towards thoroughly
investigating the interplay between empirical and certified adversarial
robustness on one hand and domain generalization on another. To do so, we train
robust models on multiple domains and evaluate their accuracy and robustness on
an unseen domain. We observe that: (1) both empirical and certified robustness
generalize to unseen domains, and (2) the level of generalizability does not
correlate well with input visual similarity, measured by the FID between source
and target domains. We also extend our study to cover a real-world medical
application, in which adversarial augmentation significantly boosts the
generalization of robustness with minimal effect on clean data accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alhamoud_K/0/1/0/all/0/1&quot;&gt;Kumail Alhamoud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hammoud_H/0/1/0/all/0/1&quot;&gt;Hasan Abed Al Kader Hammoud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1&quot;&gt;Motasem Alfarra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1&quot;&gt;Bernard Ghanem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.03418">
<title>Quantum Radiance Fields: A Quantum-Powered Photorealistic Rendering. (arXiv:2211.03418v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.03418</link>
<description rdf:parseType="Literal">&lt;p&gt;Achieving photorealistic rendering of real-world scenes poses a significant
challenge with diverse applications, including mixed reality and virtual
reality. Neural networks, extensively explored in solving differential
equations, have previously been introduced as implicit representations for
photorealistic rendering. However, achieving realism through traditional
computing methods is arduous due to the time-consuming optical ray tracing, as
it necessitates extensive numerical integration of color, transparency, and
opacity values for each sampling point during the rendering process. In this
paper, we introduce Quantum Radiance Fields (QRF), which incorporate quantum
circuits, quantum activation functions, and quantum volume rendering to
represent scenes implicitly. Our results demonstrate that QRF effectively
confronts the computational challenges associated with extensive numerical
integration by harnessing the parallelism capabilities of quantum computing.
Furthermore, current neural networks struggle with capturing fine signal
details and accurately modeling high-frequency information and higher-order
derivatives. Quantum computing&apos;s higher order of nonlinearity provides a
distinct advantage in this context. Consequently, QRF leverages two key
strengths of quantum computing: highly non-linear processing and extensive
parallelism, making it a potent tool for achieving photorealistic rendering of
real-world scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;YuanFu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Min Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.14317">
<title>Hard to Track Objects with Irregular Motions and Similar Appearances? Make It Easier by Buffering the Matching Space. (arXiv:2211.14317v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.14317</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a Cascaded Buffered IoU (C-BIoU) tracker to track multiple objects
that have irregular motions and indistinguishable appearances. When appearance
features are unreliable and geometric features are confused by irregular
motions, applying conventional Multiple Object Tracking (MOT) methods may
generate unsatisfactory results. To address this issue, our C-BIoU tracker adds
buffers to expand the matching space of detections and tracks, which mitigates
the effect of irregular motions in two aspects: one is to directly match
identical but non-overlapping detections and tracks in adjacent frames, and the
other is to compensate for the motion estimation bias in the matching space. In
addition, to reduce the risk of overexpansion of the matching space, cascaded
matching is employed: first matching alive tracks and detections with a small
buffer, and then matching unmatched tracks and detections with a large buffer.
Despite its simplicity, our C-BIoU tracker works surprisingly well and achieves
state-of-the-art results on MOT datasets that focus on irregular motions and
indistinguishable appearances. Moreover, the C-BIoU tracker is the dominant
component for our 2-nd place solution in the CVPR&apos;22 SoccerNet MOT and ECCV&apos;22
MOTComplex DanceTrack challenges. Finally, we analyze the limitation of our
C-BIoU tracker in ablation studies and discuss its application scope.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Odashima_S/0/1/0/all/0/1&quot;&gt;Shigeyuki Odashima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masui_S/0/1/0/all/0/1&quot;&gt;Shoichi Masui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Shan Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.16412">
<title>Procedural Image Programs for Representation Learning. (arXiv:2211.16412v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.16412</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning image representations using synthetic data allows training neural
networks without some of the concerns associated with real images, such as
privacy and bias. Existing work focuses on a handful of curated generative
processes which require expert knowledge to design, making it hard to scale up.
To overcome this, we propose training with a large dataset of twenty-one
thousand programs, each one generating a diverse set of synthetic images. These
programs are short code snippets, which are easy to modify and fast to execute
using OpenGL. The proposed dataset can be used for both supervised and
unsupervised representation learning, and reduces the gap between pre-training
with real and procedurally generated images by 38%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baradad_M/0/1/0/all/0/1&quot;&gt;Manel Baradad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chun-Fu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wulff_J/0/1/0/all/0/1&quot;&gt;Jonas Wulff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tongzhou Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1&quot;&gt;Rogerio Feris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1&quot;&gt;Antonio Torralba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1&quot;&gt;Phillip Isola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.05404">
<title>Cap2Aug: Caption guided Image to Image data Augmentation. (arXiv:2212.05404v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.05404</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual recognition in a low-data regime is challenging and often prone to
overfitting. To mitigate this issue, several data augmentation strategies have
been proposed. However, standard transformations, e.g., rotation, cropping, and
flipping provide limited semantic variations. To this end, we propose Cap2Aug,
an image-to-image diffusion model-based data augmentation strategy using image
captions as text prompts. We generate captions from the limited training images
and using these captions edit the training images using an image-to-image
stable diffusion model to generate semantically meaningful augmentations. This
strategy generates augmented versions of images similar to the training images
yet provides semantic diversity across the samples. We show that the variations
within the class can be captured by the captions and then translated to
generate diverse samples using the image-to-image diffusion model guided by the
captions. However, naive learning on synthetic images is not adequate due to
the domain gap between real and synthetic images. Thus, we employ a maximum
mean discrepancy (MMD) loss to align the synthetic images to the real images
for minimizing the domain gap. We evaluate our method on few-shot and long-tail
classification tasks and obtain performance improvements over state-of-the-art,
especially in the low-data regimes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1&quot;&gt;Aniket Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1&quot;&gt;Anshul Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_K/0/1/0/all/0/1&quot;&gt;Ketul Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1&quot;&gt;Anirban Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1&quot;&gt;Rama Chellappa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.09702">
<title>Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification. (arXiv:2301.09702v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.09702</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain adaptive (UDA) person re-identification (re-ID) aims to
learn identity information from labeled images in source domains and apply it
to unlabeled images in a target domain. One major issue with many unsupervised
re-identification methods is that they do not perform well relative to large
domain variations such as illumination, viewpoint, and occlusions. In this
paper, we propose a Synthesis Model Bank (SMB) to deal with illumination
variation in unsupervised person re-ID. The proposed SMB consists of several
convolutional neural networks (CNN) for feature extraction and Mahalanobis
matrices for distance metrics. They are trained using synthetic data with
different illumination conditions such that their synergistic effect makes the
SMB robust against illumination variation. To better quantify the illumination
intensity and improve the quality of synthetic images, we introduce a new 3D
virtual-human dataset for GAN-based image synthesis. From our experiments, the
proposed SMB outperforms other synthesis methods on several re-ID benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jiaqi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Reibman_A/0/1/0/all/0/1&quot;&gt;Amy R. Reibman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Delp_E/0/1/0/all/0/1&quot;&gt;Edward J. Delp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.03679">
<title>How Reliable is Your Regression Model&apos;s Uncertainty Under Real-World Distribution Shifts?. (arXiv:2302.03679v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.03679</link>
<description rdf:parseType="Literal">&lt;p&gt;Many important computer vision applications are naturally formulated as
regression problems. Within medical imaging, accurate regression models have
the potential to automate various tasks, helping to lower costs and improve
patient outcomes. Such safety-critical deployment does however require reliable
estimation of model uncertainty, also under the wide variety of distribution
shifts that might be encountered in practice. Motivated by this, we set out to
investigate the reliability of regression uncertainty estimation methods under
various real-world distribution shifts. To that end, we propose an extensive
benchmark of 8 image-based regression datasets with different types of
challenging distribution shifts. We then employ our benchmark to evaluate many
of the most common uncertainty estimation methods, as well as two
state-of-the-art uncertainty scores from the task of out-of-distribution
detection. We find that while methods are well calibrated when there is no
distribution shift, they all become highly overconfident on many of the
benchmark datasets. This uncovers important limitations of current uncertainty
estimation methods, and the proposed benchmark therefore serves as a challenge
to the research community. We hope that our benchmark will spur more work on
how to develop truly reliable regression uncertainty estimation methods. Code
is available at https://github.com/fregu856/regression_uncertainty.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gustafsson_F/0/1/0/all/0/1&quot;&gt;Fredrik K. Gustafsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1&quot;&gt;Martin Danelljan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1&quot;&gt;Thomas B. Sch&amp;#xf6;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.07025">
<title>Optimal Transport for Change Detection on LiDAR Point Clouds. (arXiv:2302.07025v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.07025</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised change detection between airborne LiDAR data points, taken at
separate times over the same location, can be difficult due to unmatching
spatial support and noise from the acquisition system. Most current approaches
to detect changes in point clouds rely heavily on the computation of Digital
Elevation Models (DEM) images and supervised methods. Obtaining a DEM leads to
LiDAR informational loss due to pixelisation, and supervision requires large
amounts of labelled data often unavailable in real-world scenarios. We propose
an unsupervised approach based on the computation of the transport of 3D LiDAR
points over two temporal supports. The method is based on unbalanced optimal
transport and can be generalised to any change detection problem with LiDAR
data. We apply our approach to publicly available datasets for monitoring urban
sprawling in various noise and resolution configurations that mimic several
sensors used in practice. Our method allows for unsupervised multi-class
classification and outperforms the previous state-of-the-art unsupervised
approaches by a significant margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiorucci_M/0/1/0/all/0/1&quot;&gt;Marco Fiorucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naylor_P/0/1/0/all/0/1&quot;&gt;Peter Naylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamada_M/0/1/0/all/0/1&quot;&gt;Makoto Yamada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.09043">
<title>Self-Supervised Representation Learning from Temporal Ordering of Automated Driving Sequences. (arXiv:2302.09043v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.09043</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised feature learning enables perception systems to benefit from
the vast raw data recorded by vehicle fleets worldwide. While video-level
self-supervised learning approaches have shown strong generalizability on
classification tasks, the potential to learn dense representations from
sequential data has been relatively unexplored. In this work, we propose TempO,
a temporal ordering pretext task for pre-training region-level feature
representations for perception tasks. We embed each frame by an unordered set
of proposal feature vectors, a representation that is natural for object
detection or tracking systems, and formulate the sequential ordering by
predicting frame transition probabilities in a transformer-based multi-frame
architecture whose complexity scales less than quadratic with respect to the
sequence length. Extensive evaluations on the BDD100K, nuImages, and MOT17
datasets show that our TempO pre-training approach outperforms single-frame
self-supervised learning methods as well as supervised transfer learning
initialization strategies, achieving an improvement of +0.7% in mAP for object
detection and +2.0% in the HOTA score for multi-object tracking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1&quot;&gt;Christopher Lang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braun_A/0/1/0/all/0/1&quot;&gt;Alexander Braun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schillingmann_L/0/1/0/all/0/1&quot;&gt;Lars Schillingmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haug_K/0/1/0/all/0/1&quot;&gt;Karsten Haug&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1&quot;&gt;Abhinav Valada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11522">
<title>Evaluation of Extra Pixel Interpolation with Mask Processing for Medical Image Segmentation with Deep Learning. (arXiv:2302.11522v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11522</link>
<description rdf:parseType="Literal">&lt;p&gt;Current mask processing operations rely on interpolation algorithms that do
not produce extra pixels, such as nearest neighbor (NN) interpolation, as
opposed to algorithms that do produce extra pixels, like bicubic (BIC) or
bilinear (BIL) interpolation. In our previous study, the author proposed an
alternative approach to NN-based mask processing and evaluated its effects on
deep learning training outcomes. In this study, the author evaluated the
effects of both BIC-based image and mask processing and BIC-and-NN-based image
and mask processing versus NN-based image and mask processing. The evaluation
revealed that the BIC-BIC model/network was an 8.9578 % (with image size 256 x
256) and a 1.0496 % (with image size 384 x 384) increase of the NN-NN network
compared to the NN-BIC network which was an 8.3127 % (with image size 256 x
256) and a 0.2887 % (with image size 384 x 384) increase of the NN-NN network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rukundo_O/0/1/0/all/0/1&quot;&gt;Olivier Rukundo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06596">
<title>Amodal Intra-class Instance Segmentation: Synthetic Datasets and Benchmark. (arXiv:2303.06596v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06596</link>
<description rdf:parseType="Literal">&lt;p&gt;Images of realistic scenes often contain intra-class objects that are heavily
occluded from each other, making the amodal perception task that requires
parsing the occluded parts of the objects challenging. Although important for
downstream tasks such as robotic grasping systems, the lack of large-scale
amodal datasets with detailed annotations makes it difficult to model
intra-class occlusions explicitly. This paper introduces two new amodal
datasets for image amodal completion tasks, which contain a total of over 267K
images of intra-class occlusion scenarios, annotated with multiple masks,
amodal bounding boxes, dual order relations and full appearance for instances
and background. We also present a point-supervised scheme with layer priors for
amodal instance segmentation specifically designed for intra-class occlusion
scenarios. Experiments show that our weakly supervised approach outperforms the
SOTA fully supervised methods, while our layer priors design exhibits
remarkable performance improvements in the case of intra-class occlusion in
both synthetic and real images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ao_J/0/1/0/all/0/1&quot;&gt;Jiayang Ao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1&quot;&gt;Qiuhong Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehinger_K/0/1/0/all/0/1&quot;&gt;Krista A. Ehinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09354">
<title>The NCI Imaging Data Commons as a platform for reproducible research in computational pathology. (arXiv:2303.09354v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09354</link>
<description rdf:parseType="Literal">&lt;p&gt;Background and Objectives: Reproducibility is a major challenge in developing
machine learning (ML)-based solutions in computational pathology (CompPath).
The NCI Imaging Data Commons (IDC) provides &amp;gt;120 cancer image collections
according to the FAIR principles and is designed to be used with cloud ML
services. Here, we explore its potential to facilitate reproducibility in
CompPath research.
&lt;/p&gt;
&lt;p&gt;Methods: Using the IDC, we implemented two experiments in which a
representative ML-based method for classifying lung tumor tissue was trained
and/or evaluated on different datasets. To assess reproducibility, the
experiments were run multiple times with separate but identically configured
instances of common ML services.
&lt;/p&gt;
&lt;p&gt;Results: The AUC values of different runs of the same experiment were
generally consistent. However, we observed small variations in AUC values of up
to 0.045, indicating a practical limit to reproducibility.
&lt;/p&gt;
&lt;p&gt;Conclusions: We conclude that the IDC facilitates approaching the
reproducibility limit of CompPath research (i) by enabling researchers to reuse
exactly the same datasets and (ii) by integrating with cloud ML services so
that experiments can be run in identically configured computing environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schacherer_D/0/1/0/all/0/1&quot;&gt;Daniela P. Schacherer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herrmann_M/0/1/0/all/0/1&quot;&gt;Markus D. Herrmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clunie_D/0/1/0/all/0/1&quot;&gt;David A. Clunie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofener_H/0/1/0/all/0/1&quot;&gt;Henning H&amp;#xf6;fener&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clifford_W/0/1/0/all/0/1&quot;&gt;William Clifford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Longabaugh_W/0/1/0/all/0/1&quot;&gt;William J.R. Longabaugh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pieper_S/0/1/0/all/0/1&quot;&gt;Steve Pieper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kikinis_R/0/1/0/all/0/1&quot;&gt;Ron Kikinis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fedorov_A/0/1/0/all/0/1&quot;&gt;Andrey Fedorov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Homeyer_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Homeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10093">
<title>Investigating the Role of Attribute Context in Vision-Language Models for Object Recognition and Detection. (arXiv:2303.10093v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10093</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-language alignment learned from image-caption pairs has been shown to
benefit tasks like object recognition and detection. Methods are mostly
evaluated in terms of how well object class names are learned, but captions
also contain rich attribute context that should be considered when learning
object alignment. It is unclear how methods use this context in learning, as
well as whether models succeed when tasks require attribute and object
understanding. To address this gap, we conduct extensive analysis of the role
of attributes in vision-language models. We specifically measure model
sensitivity to the presence and meaning of attribute context, gauging influence
on object embeddings through unsupervised phrase grounding and classification
via description methods. We further evaluate the utility of attribute context
in training for open-vocabulary object detection, fine-grained text-region
retrieval, and attribution tasks. Our results show that attribute context can
be wasted when learning alignment for detection, attribute meaning is not
adequately considered in embeddings, and describing classes by only their
attributes is ineffective. A viable strategy that we find to increase benefits
from attributes is contrastive training with adjective-based negative captions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buettner_K/0/1/0/all/0/1&quot;&gt;Kyle Buettner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1&quot;&gt;Adriana Kovashka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13764">
<title>GQE-Net: A Graph-based Quality Enhancement Network for Point Cloud Color Attribute. (arXiv:2303.13764v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13764</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, point clouds have become increasingly popular for
representing three-dimensional (3D) visual objects and scenes. To efficiently
store and transmit point clouds, compression methods have been developed, but
they often result in a degradation of quality. To reduce color distortion in
point clouds, we propose a graph-based quality enhancement network (GQE-Net)
that uses geometry information as an auxiliary input and graph convolution
blocks to extract local features efficiently. Specifically, we use a
parallel-serial graph attention module with a multi-head graph attention
mechanism to focus on important points or features and help them fuse together.
Additionally, we design a feature refinement module that takes into account the
normals and geometry distance between points. To work within the limitations of
GPU memory capacity, the distorted point cloud is divided into overlap-allowed
3D patches, which are sent to GQE-Net for quality enhancement. To account for
differences in data distribution among different color components, three models
are trained for the three color components. Experimental results show that our
method achieves state-of-the-art performance. For example, when implementing
GQE-Net on a recent test model of the geometry-based point cloud compression
(G-PCC) standard, 0.43 dB, 0.25 dB, and 0.36 dB Bjontegaard delta
(BD)-peak-signal-to-noise ratio (PSNR), corresponding to 14.0%, 9.3%, and 14.5%
BD-rate savings can be achieved on dense point clouds for the Y, Cb, and Cr
components, respectively. The source code of our method is available at
https://github.com/xjr998/GQE-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Jinrui Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Hui Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hamzaoui_R/0/1/0/all/0/1&quot;&gt;Raouf Hamzaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Junhui Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01834">
<title>Neural Field Convolutions by Repeated Differentiation. (arXiv:2304.01834v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01834</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural fields are evolving towards a general-purpose continuous
representation for visual computing. Yet, despite their numerous appealing
properties, they are hardly amenable to signal processing. As a remedy, we
present a method to perform general continuous convolutions with general
continuous signals such as neural fields. Observing that piecewise polynomial
kernels reduce to a sparse set of Dirac deltas after repeated differentiation,
we leverage convolution identities and train a repeated integral field to
efficiently execute large-scale convolutions. We demonstrate our approach on a
variety of data modalities and spatially-varying kernels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nsampi_N/0/1/0/all/0/1&quot;&gt;Ntumba Elie Nsampi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Djeacoumar_A/0/1/0/all/0/1&quot;&gt;Adarsh Djeacoumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seidel_H/0/1/0/all/0/1&quot;&gt;Hans-Peter Seidel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1&quot;&gt;Tobias Ritschel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leimkuhler_T/0/1/0/all/0/1&quot;&gt;Thomas Leimk&amp;#xfc;hler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11824">
<title>Shape from Shading for Robotic Manipulation. (arXiv:2304.11824v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2304.11824</link>
<description rdf:parseType="Literal">&lt;p&gt;Controlling illumination can generate high quality information about object
surface normals and depth discontinuities at a low computational cost. In this
work we demonstrate a robot workspace-scaled controlled illumination approach
that generates high quality information for table top scale objects for robotic
manipulation. With our low angle of incidence directional illumination
approach, we can precisely capture surface normals and depth discontinuities of
monochromatic Lambertian objects. We show that this approach to shape
estimation is 1) valuable for general purpose grasping with a single point
vacuum gripper, 2) can measure the deformation of known objects, and 3) can
estimate pose of known objects and track unknown objects in the robot&apos;s
workspace.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhury_A/0/1/0/all/0/1&quot;&gt;Arkadeep Narayan Chaudhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keselman_L/0/1/0/all/0/1&quot;&gt;Leonid Keselman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atkeson_C/0/1/0/all/0/1&quot;&gt;Christopher G. Atkeson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.13409">
<title>Efficient Explainable Face Verification based on Similarity Score Argument Backpropagation. (arXiv:2304.13409v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.13409</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainable Face Recognition is gaining growing attention as the use of the
technology is gaining ground in security-critical applications. Understanding
why two faces images are matched or not matched by a given face recognition
system is important to operators, users, anddevelopers to increase trust,
accountability, develop better systems, and highlight unfair behavior. In this
work, we propose xSSAB, an approach to back-propagate similarity score-based
arguments that support or oppose the face matching decision to visualize
spatial maps that indicate similar and dissimilar areas as interpreted by the
underlying FR model. Furthermore, we present Patch-LFW, a new explainable face
verification benchmark that enables along with a novel evaluation protocol, the
first quantitative evaluation of the validity of similarity and dissimilarity
maps in explainable face recognition approaches. We compare our efficient
approach to state-of-the-art approaches demonstrating a superior trade-off
between efficiency and performance. The code as well as the proposed Patch-LFW
is publicly available at: https://github.com/marcohuber/xSSAB.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1&quot;&gt;Marco Huber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1&quot;&gt;Anh Thi Luu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terhorst_P/0/1/0/all/0/1&quot;&gt;Philipp Terh&amp;#xf6;rst&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1&quot;&gt;Naser Damer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.01191">
<title>EasyHeC: Accurate and Automatic Hand-eye Calibration via Differentiable Rendering and Space Exploration. (arXiv:2305.01191v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2305.01191</link>
<description rdf:parseType="Literal">&lt;p&gt;Hand-eye calibration is a critical task in robotics, as it directly affects
the efficacy of critical operations such as manipulation and grasping.
Traditional methods for achieving this objective necessitate the careful design
of joint poses and the use of specialized calibration markers, while most
recent learning-based approaches using solely pose regression are limited in
their abilities to diagnose inaccuracies. In this work, we introduce a new
approach to hand-eye calibration called EasyHeC, which is markerless,
white-box, and delivers superior accuracy and robustness. We propose to use two
key technologies: differentiable rendering-based camera pose optimization and
consistency-based joint space exploration, which enables accurate end-to-end
optimization of the calibration process and eliminates the need for the
laborious manual design of robot joint poses. Our evaluation demonstrates
superior performance in synthetic and real-world datasets, enhancing downstream
manipulation tasks by providing precise camera poses for locating and
interacting with objects. The code is available at the project page:
https://ootts.github.io/easyhec.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Linghao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiaowei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12704">
<title>Rotation-Constrained Cross-View Feature Fusion for Multi-View Appearance-based Gaze Estimation. (arXiv:2305.12704v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12704</link>
<description rdf:parseType="Literal">&lt;p&gt;Appearance-based gaze estimation has been actively studied in recent years.
However, its generalization performance for unseen head poses is still a
significant limitation for existing methods. This work proposes a generalizable
multi-view gaze estimation task and a cross-view feature fusion method to
address this issue. In addition to paired images, our method takes the relative
rotation matrix between two cameras as additional input. The proposed network
learns to extract rotatable feature representation by using relative rotation
as a constraint and adaptively fuses the rotatable features via stacked fusion
modules. This simple yet efficient approach significantly improves
generalization performance under unseen head poses without significantly
increasing computational cost. The model can be trained with random
combinations of cameras without fixing the positioning and can generalize to
unseen camera pairs during inference. Through experiments using multiple
datasets, we demonstrate the advantage of the proposed method over baseline
methods, including state-of-the-art domain generalization approaches. The code
will be available at \url{https://github.com/ut-vision/Rot-MVGaze}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hisadome_Y/0/1/0/all/0/1&quot;&gt;Yoichiro Hisadome&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tianyi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1&quot;&gt;Jiawei Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugano_Y/0/1/0/all/0/1&quot;&gt;Yusuke Sugano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17134">
<title>NeuManifold: Neural Watertight Manifold Reconstruction with Efficient and High-Quality Rendering Support. (arXiv:2305.17134v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17134</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a method for generating high-quality watertight manifold meshes
from multi-view input images. Existing volumetric rendering methods are robust
in optimization but tend to generate noisy meshes with poor topology.
Differentiable rasterization-based methods can generate high-quality meshes but
are sensitive to initialization. Our method combines the benefits of both
worlds; we take the geometry initialization obtained from neural volumetric
fields, and further optimize the geometry as well as a compact neural texture
representation with differentiable rasterizers. Through extensive experiments,
we demonstrate that our method can generate accurate mesh reconstructions with
faithful appearance that are comparable to previous volume rendering methods
while being an order of magnitude faster in rendering. We also show that our
generated mesh and neural texture reconstruction is compatible with existing
graphics pipelines and enables downstream 3D applications such as simulation.
Project page: https://sarahweiii.github.io/neumanifold/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xinyue Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1&quot;&gt;Fanbo Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1&quot;&gt;Sai Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Anpei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1&quot;&gt;Kalyan Sunkavalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zexiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18729">
<title>Real-World Image Variation by Aligning Diffusion Inversion Chain. (arXiv:2305.18729v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18729</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent diffusion model advancements have enabled high-fidelity images to be
generated using text prompts. However, a domain gap exists between generated
images and real-world images, which poses a challenge in generating
high-quality variations of real-world images. Our investigation uncovers that
this domain gap originates from a latents&apos; distribution gap in different
diffusion processes. To address this issue, we propose a novel inference
pipeline called Real-world Image Variation by ALignment (RIVAL) that utilizes
diffusion models to generate image variations from a single image exemplar. Our
pipeline enhances the generation quality of image variations by aligning the
image generation process to the source image&apos;s inversion chain. Specifically,
we demonstrate that step-wise latent distribution alignment is essential for
generating high-quality variations. To attain this, we design a cross-image
self-attention injection for feature interaction and a step-wise distribution
normalization to align the latent features. Incorporating these alignment
processes into a diffusion model allows RIVAL to generate high-quality image
variations without further parameter optimization. Our experimental results
demonstrate that our proposed approach outperforms existing methods concerning
semantic similarity and perceptual quality. This generalized inference pipeline
can be easily applied to other diffusion-based generation tasks, such as
image-conditioned text-to-image generation and stylization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuechen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Jinbo Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_E/0/1/0/all/0/1&quot;&gt;Eric Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jiaya Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01747">
<title>UMDFood: Vision-language models boost food composition compilation. (arXiv:2306.01747v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01747</link>
<description rdf:parseType="Literal">&lt;p&gt;Nutrition information is crucial in precision nutrition and the food
industry. The current food composition compilation paradigm relies on laborious
and experience-dependent methods. However, these methods struggle to keep up
with the dynamic consumer market, resulting in delayed and incomplete nutrition
data. In addition, earlier machine learning methods overlook the information in
food ingredient statements or ignore the features of food images. To this end,
we propose a novel vision-language model, UMDFood-VL, using front-of-package
labeling and product images to accurately estimate food composition profiles.
In order to empower model training, we established UMDFood-90k, the most
comprehensive multimodal food database to date, containing 89,533 samples, each
labeled with image and text-based ingredient descriptions and 11 nutrient
annotations. UMDFood-VL achieves the macro-AUCROC up to 0.921 for fat content
estimation, which is significantly higher than existing baseline methods and
satisfies the practical requirements of food composition compilation.
Meanwhile, up to 82.2% of selected products&apos; estimated error between chemical
analysis results and model estimation results are less than 10%. This
performance sheds light on generalization towards other food and
nutrition-related data compilation and catalyzation for the evolution of
generative AI-based technology in other food applications that require
personalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1&quot;&gt;Peihua Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yixin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1&quot;&gt;Ning Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Backes_M/0/1/0/all/0/1&quot;&gt;Michael Backes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1&quot;&gt;Cheng-I Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07716">
<title>Dynamically Masked Discriminator for Generative Adversarial Networks. (arXiv:2306.07716v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07716</link>
<description rdf:parseType="Literal">&lt;p&gt;Training Generative Adversarial Networks (GANs) remains a challenging
problem. The discriminator trains the generator by learning the distribution of
real/generated data. However, the distribution of generated data changes
throughout the training process, which is difficult for the discriminator to
learn. In this paper, we propose a novel method for GANs from the viewpoint of
online continual learning. We observe that the discriminator model, trained on
historically generated data, often slows down its adaptation to the changes in
the new arrival generated data, which accordingly decreases the quality of
generated results. By treating the generated data in training as a stream, we
propose to detect whether the discriminator slows down the learning of new
knowledge in generated data. Therefore, we can explicitly enforce the
discriminator to learn new knowledge fast. Particularly, we propose a new
discriminator, which automatically detects its retardation and then dynamically
masks its features, such that the discriminator can adaptively learn the
temporally-vary distribution of generated data. Experimental results show our
method outperforms the state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wentian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haozhe Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jinheng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yawen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuexiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yefeng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1&quot;&gt;Bernard Ghanem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09858">
<title>Prototype Learning for Explainable Brain Age Prediction. (arXiv:2306.09858v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09858</link>
<description rdf:parseType="Literal">&lt;p&gt;The lack of explainability of deep learning models limits the adoption of
such models in clinical practice. Prototype-based models can provide inherent
explainable predictions, but these have predominantly been designed for
classification tasks, despite many important tasks in medical imaging being
continuous regression problems. Therefore, in this work, we present ExPeRT: an
explainable prototype-based model specifically designed for regression tasks.
Our proposed model makes a sample prediction from the distances to a set of
learned prototypes in latent space, using a weighted mean of prototype labels.
The distances in latent space are regularized to be relative to label
differences, and each of the prototypes can be visualized as a sample from the
training set. The image-level distances are further constructed from
patch-level distances, in which the patches of both images are structurally
matched using optimal transport. This thus provides an example-based
explanation with patch-level detail at inference time. We demonstrate our
proposed model for brain age prediction on two imaging datasets: adult MR and
fetal ultrasound. Our approach achieved state-of-the-art prediction performance
while providing insight into the model&apos;s reasoning process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hesse_L/0/1/0/all/0/1&quot;&gt;Linde S. Hesse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dinsdale_N/0/1/0/all/0/1&quot;&gt;Nicola K. Dinsdale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namburete_A/0/1/0/all/0/1&quot;&gt;Ana I. L. Namburete&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17466">
<title>MedAugment: Universal Automatic Data Augmentation Plug-in for Medical Image Analysis. (arXiv:2306.17466v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17466</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation (DA) has been widely leveraged in the realm of computer
vision to alleviate the data shortage, whereas the DA in medical image analysis
(MIA) faces multiple challenges. The prevalent DA approaches in MIA encompass
conventional DA, synthetic DA, and automatic DA. However, the utilization of
these approaches poses various challenges such as experience-driven design and
intensive computation cost. Here, we propose an efficient and effective
automatic DA method termed MedAugment. We propose the pixel augmentation space
and spatial augmentation space and exclude the operations that can break the
details and features within medical images. Besides, we propose a novel
sampling strategy by sampling a limited number of operations from the two
spaces. Moreover, we present a hyperparameter mapping relationship to produce a
rational augmentation level and make the MedAugment fully controllable using a
single hyperparameter. These revisions address the differences between natural
and medical images. Extensive experimental results on four classification and
three segmentation datasets demonstrate the superiority of MedAugment. We posit
that the plug-and-use and training-free MedAugment holds the potential to make
a valuable contribution to the medical field, particularly benefiting medical
experts lacking foundational expertise in deep learning. Code is available at
https://github.com/NUS-Tim/MedAugment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhaoshan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lv_Q/0/1/0/all/0/1&quot;&gt;Qiujie Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yifan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ziduo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Lei Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03404">
<title>RGB-D Mapping and Tracking in a Plenoxel Radiance Field. (arXiv:2307.03404v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03404</link>
<description rdf:parseType="Literal">&lt;p&gt;The widespread adoption of Neural Radiance Fields (NeRFs) have ensured
significant advances in the domain of novel view synthesis in recent years.
These models capture a volumetric radiance field of a scene, creating highly
convincing, dense, photorealistic models through the use of simple,
differentiable rendering equations. Despite their popularity, these algorithms
suffer from severe ambiguities in visual data inherent to the RGB sensor, which
means that although images generated with view synthesis can visually appear
very believable, the underlying 3D model will often be wrong. This considerably
limits the usefulness of these models in practical applications like Robotics
and Extended Reality (XR), where an accurate dense 3D reconstruction otherwise
would be of significant value. In this paper, we present the vital differences
between view synthesis models and 3D reconstruction models. We also comment on
why a depth sensor is essential for modeling accurate geometry in general
outward-facing scenes using the current paradigm of novel view synthesis
methods. Focusing on the structure-from-motion task, we practically demonstrate
this need by extending the Plenoxel radiance field model: Presenting an
analytical differential approach for dense mapping and tracking with radiance
fields based on RGB-D data without a neural network. Our method achieves
state-of-the-art results in both mapping and tracking tasks, while also being
faster than competing neural network-based approaches. The code is available
at: https://github.com/ysus33/RGB-D_Plenoxel_Mapping_Tracking.git
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teigen_A/0/1/0/all/0/1&quot;&gt;Andreas L. Teigen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1&quot;&gt;Yeonsoo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stahl_A/0/1/0/all/0/1&quot;&gt;Annette Stahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mester_R/0/1/0/all/0/1&quot;&gt;Rudolf Mester&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07187">
<title>Erasing, Transforming, and Noising Defense Network for Occluded Person Re-Identification. (arXiv:2307.07187v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07187</link>
<description rdf:parseType="Literal">&lt;p&gt;Occlusion perturbation presents a significant challenge in person
re-identification (re-ID), and existing methods that rely on external visual
cues require additional computational resources and only consider the issue of
missing information caused by occlusion. In this paper, we propose a simple yet
effective framework, termed Erasing, Transforming, and Noising Defense Network
(ETNDNet), which treats occlusion as a noise disturbance and solves occluded
person re-ID from the perspective of adversarial defense. In the proposed
ETNDNet, we introduce three strategies: Firstly, we randomly erase the feature
map to create an adversarial representation with incomplete information,
enabling adversarial learning of identity loss to protect the re-ID system from
the disturbance of missing information. Secondly, we introduce random
transformations to simulate the position misalignment caused by occlusion,
training the extractor and classifier adversarially to learn robust
representations immune to misaligned information. Thirdly, we perturb the
feature map with random values to address noisy information introduced by
obstacles and non-target pedestrians, and employ adversarial gaming in the
re-ID system to enhance its resistance to occlusion noise. Without bells and
whistles, ETNDNet has three key highlights: (i) it does not require any
external modules with parameters, (ii) it effectively handles various issues
caused by occlusion from obstacles and non-target pedestrians, and (iii) it
designs the first GAN-based adversarial defense paradigm for occluded person
re-ID. Extensive experiments on five public datasets fully demonstrate the
effectiveness, superiority, and practicality of the proposed ETNDNet. The code
will be released at \url{https://github.com/nengdong96/ETNDNet}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1&quot;&gt;Neng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liyan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shuanglin Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jinhui Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09000">
<title>DealMVC: Dual Contrastive Calibration for Multi-view Clustering. (arXiv:2308.09000v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09000</link>
<description rdf:parseType="Literal">&lt;p&gt;Benefiting from the strong view-consistent information mining capacity,
multi-view contrastive clustering has attracted plenty of attention in recent
years. However, we observe the following drawback, which limits the clustering
performance from further improvement. The existing multi-view models mainly
focus on the consistency of the same samples in different views while ignoring
the circumstance of similar but different samples in cross-view scenarios. To
solve this problem, we propose a novel Dual contrastive calibration network for
Multi-View Clustering (DealMVC). Specifically, we first design a fusion
mechanism to obtain a global cross-view feature. Then, a global contrastive
calibration loss is proposed by aligning the view feature similarity graph and
the high-confidence pseudo-label graph. Moreover, to utilize the diversity of
multi-view information, we propose a local contrastive calibration loss to
constrain the consistency of pair-wise view features. The feature structure is
regularized by reliable class information, thus guaranteeing similar samples
have similar features in different views. During the training procedure, the
interacted cross-view feature is jointly optimized at both local and global
levels. In comparison with other state-of-the-art approaches, the comprehensive
experimental results obtained from eight benchmark datasets provide substantial
validation of the effectiveness and superiority of our algorithm. We release
the code of DealMVC at https://github.com/xihongyang1999/DealMVC on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xihong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jiaqi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Ke Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yi Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Suyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Sihang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinwang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1&quot;&gt;En Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09025">
<title>SR-GAN for SR-gamma: super resolution of photon calorimeter images at collider experiments. (arXiv:2308.09025v2 [hep-ex] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09025</link>
<description rdf:parseType="Literal">&lt;p&gt;We study single-image super-resolution algorithms for photons at collider
experiments based on generative adversarial networks. We treat the energy
depositions of simulated electromagnetic showers of photons and neutral-pion
decays in a toy electromagnetic calorimeter as 2D images and we train
super-resolution networks to generate images with an artificially increased
resolution by a factor of four in each dimension. The generated images are able
to reproduce features of the electromagnetic showers that are not obvious from
the images at nominal resolution. Using the artificially-enhanced images for
the reconstruction of shower-shape variables and of the position of the shower
center results in significant improvements. We additionally investigate the
utilization of the generated images as a pre-processing step for deep-learning
photon-identification algorithms and observe improvements in the case of
training samples of small size.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Erdmann_J/0/1/0/all/0/1&quot;&gt;Johannes Erdmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Graaf_A/0/1/0/all/0/1&quot;&gt;Aaron van der Graaf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Mausolf_F/0/1/0/all/0/1&quot;&gt;Florian Mausolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Nackenhorst_O/0/1/0/all/0/1&quot;&gt;Olaf Nackenhorst&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12736">
<title>FastSurfer-HypVINN: Automated sub-segmentation of the hypothalamus and adjacent structures on high-resolutional brain MRI. (arXiv:2308.12736v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12736</link>
<description rdf:parseType="Literal">&lt;p&gt;The hypothalamus plays a crucial role in the regulation of a broad range of
physiological, behavioural, and cognitive functions. However, despite its
importance, only a few small-scale neuroimaging studies have investigated its
substructures, likely due to the lack of fully automated segmentation tools to
address scalability and reproducibility issues of manual segmentation. While
the only previous attempt to automatically sub-segment the hypothalamus with a
neural network showed promise for 1.0 mm isotropic T1-weighted (T1w) MRI, there
is a need for an automated tool to sub-segment also high-resolutional (HiRes)
MR scans, as they are becoming widely available, and include structural detail
also from multi-modal MRI. We, therefore, introduce a novel, fast, and fully
automated deep learning method named HypVINN for sub-segmentation of the
hypothalamus and adjacent structures on 0.8 mm isotropic T1w and T2w brain MR
images that is robust to missing modalities. We extensively validate our model
with respect to segmentation accuracy, generalizability, in-session test-retest
reliability, and sensitivity to replicate hypothalamic volume effects (e.g.
sex-differences). The proposed method exhibits high segmentation performance
both for standalone T1w images as well as for T1w/T2w image pairs. Even with
the additional capability to accept flexible inputs, our model matches or
exceeds the performance of state-of-the-art methods with fixed inputs. We,
further, demonstrate the generalizability of our method in experiments with 1.0
mm MR scans from both the Rhineland Study and the UK Biobank. Finally, HypVINN
can perform the segmentation in less than a minute (GPU) and will be available
in the open source FastSurfer neuroimaging software suite, offering a
validated, efficient, and scalable solution for evaluating imaging-derived
phenotypes of the hypothalamus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Estrada_S/0/1/0/all/0/1&quot;&gt;Santiago Estrada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kugler_D/0/1/0/all/0/1&quot;&gt;David K&amp;#xfc;gler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bahrami_E/0/1/0/all/0/1&quot;&gt;Emad Bahrami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousa_D/0/1/0/all/0/1&quot;&gt;Dilshad Mousa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breteler_M/0/1/0/all/0/1&quot;&gt;Monique M.B. Breteler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aziz_N/0/1/0/all/0/1&quot;&gt;N. Ahmad Aziz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reuter_M/0/1/0/all/0/1&quot;&gt;Martin Reuter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.04190">
<title>SegmentAnything helps microscopy images based automatic and quantitative organoid detection and analysis. (arXiv:2309.04190v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.04190</link>
<description rdf:parseType="Literal">&lt;p&gt;Organoids are self-organized 3D cell clusters that closely mimic the
architecture and function of in vivo tissues and organs. Quantification of
organoid morphology helps in studying organ development, drug discovery, and
toxicity assessment. Recent microscopy techniques provide a potent tool to
acquire organoid morphology features, but manual image analysis remains a labor
and time-intensive process. Thus, this paper proposes a comprehensive pipeline
for microscopy analysis that leverages the SegmentAnything to precisely
demarcate individual organoids. Additionally, we introduce a set of
morphological properties, including perimeter, area, radius, non-smoothness,
and non-circularity, allowing researchers to analyze the organoid structures
quantitatively and automatically. To validate the effectiveness of our
approach, we conducted tests on bright-field images of human induced
pluripotent stem cells (iPSCs) derived neural-epithelial (NE) organoids. The
results obtained from our automatic pipeline closely align with manual organoid
detection and measurement, showcasing the capability of our proposed method in
accelerating organoids morphology analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xing_X/0/1/0/all/0/1&quot;&gt;Xiaodan Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chunling Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yunzhe Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kurniawan_N/0/1/0/all/0/1&quot;&gt;Nicholas Kurniawan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05139">
<title>A Skeleton-based Approach For Rock Crack Detection Towards A Climbing Robot Application. (arXiv:2309.05139v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05139</link>
<description rdf:parseType="Literal">&lt;p&gt;Conventional wheeled robots are unable to traverse scientifically
interesting, but dangerous, cave environments. Multi-limbed climbing robot
designs, such as ReachBot, are able to grasp irregular surface features and
execute climbing motions to overcome obstacles, given suitable grasp locations.
To support grasp site identification, we present a method for detecting rock
cracks and edges, the SKeleton Intersection Loss (SKIL). SKIL is a loss
designed for thin object segmentation that leverages the skeleton of the label.
A dataset of rock face images was collected, manually annotated, and augmented
with generated data. A new group of metrics, LineAcc, has been proposed for
thin object segmentation such that the impact of the object width on the score
is minimized. In addition, the metric is less sensitive to translation which
can often lead to a score of zero when computing classical metrics such as Dice
on thin objects. Our fine-tuned models outperform previous methods on similar
thin object segmentation tasks such as blood vessel segmentation and show
promise for integration onto a robotic system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_J/0/1/0/all/0/1&quot;&gt;Josselin Somerville Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giacomelli_P/0/1/0/all/0/1&quot;&gt;Paul-Emile Giacomelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gozlan_Y/0/1/0/all/0/1&quot;&gt;Yoni Gozlan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Di_J/0/1/0/all/0/1&quot;&gt;Julia Di&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11523">
<title>RMT: Retentive Networks Meet Vision Transformers. (arXiv:2309.11523v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.11523</link>
<description rdf:parseType="Literal">&lt;p&gt;Retentive Network first emerged in the domain of NLP and immediately gained
widespread attention due to its remarkable performance. A significant portion
of its impressive capabilities stems from its explicit decay mechanism, which
incorporates valuable prior knowledge. However, this explicit decay is
unidirectional and one-dimensional, making it unsuitable for the bidirectional,
two-dimensional modeling required in image-based tasks. To solve this, we
propose a bidirectional, two-dimensional form of explicit decay specifically
designed for vision models to introduce distance-related prior knowledge.
Besides, unlike language models, the vision backbones use the same parallel
form during training and inference. If this parallel form is replaced with
recurrent or chunk-wise recurrent form, the parallelism of the model will be
significantly disrupted, resulting in extremely slow inference speed. So we
discard the two additional inference modes present in the original RetNet,
retaining only the parallel form. Specifically, we incorporate bidirectional,
two-dimensional explicit decay into the Self-Attention to form
\textbf{Re}tentive \textbf{S}elf-\textbf{A}ttention (ReSA). Furthermore, to
reduce the complexity of global modeling, we decompose ReSA along the two axes
of the image. Building upon ReSA, we construct RMT, a strong vision backbone.
Abundant experiments have demonstrated that our RMT exhibits exceptional
performance across various computer vision tasks. For example, RMT achieves
\textbf{84.1\%} Top1-acc on ImageNet-1k using merely \textbf{4.5G} FLOPs. To
the best of our knowledge, among all models, RMT achieves the highest Top1-acc
when models are of similar size and trained with the same strategy. Moreover,
RMT significantly outperforms existing vision backbones in downstream tasks.
Code will be released at https://github.com/qhfan/RMT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1&quot;&gt;Qihang Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Huaibo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingrui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongmin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ran He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00496">
<title>The Sparsity Roofline: Understanding the Hardware Limits of Sparse Neural Networks. (arXiv:2310.00496v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00496</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the Sparsity Roofline, a visual performance model for evaluating
sparsity in neural networks. The Sparsity Roofline jointly models network
accuracy, sparsity, and theoretical inference speedup. Our approach does not
require implementing and benchmarking optimized kernels, and the theoretical
speedup becomes equal to the actual speedup when the corresponding dense and
sparse kernels are well-optimized. We achieve this through a novel analytical
model for predicting sparse network performance, and validate the predicted
speedup using several real-world computer vision architectures pruned across a
range of sparsity patterns and degrees. We demonstrate the utility and
ease-of-use of our model through two case studies: (1) we show how machine
learning researchers can predict the performance of unimplemented or
unoptimized block-structured sparsity patterns, and (2) we show how hardware
designers can predict the performance implications of new sparsity patterns and
sparse data formats in hardware. In both scenarios, the Sparsity Roofline helps
performance experts identify sparsity regimes with the highest performance
potential.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shinn_C/0/1/0/all/0/1&quot;&gt;Cameron Shinn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCarthy_C/0/1/0/all/0/1&quot;&gt;Collin McCarthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muralidharan_S/0/1/0/all/0/1&quot;&gt;Saurav Muralidharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osama_M/0/1/0/all/0/1&quot;&gt;Muhammad Osama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Owens_J/0/1/0/all/0/1&quot;&gt;John D. Owens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01258">
<title>MobileNVC: Real-time 1080p Neural Video Compression on a Mobile Device. (arXiv:2310.01258v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01258</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural video codecs have recently become competitive with standard codecs
such as HEVC in the low-delay setting. However, most neural codecs are large
floating-point networks that use pixel-dense warping operations for temporal
modeling, making them too computationally expensive for deployment on mobile
devices. Recent work has demonstrated that running a neural decoder in real
time on mobile is feasible, but shows this only for 720p RGB video. This work
presents the first neural video codec that decodes 1080p YUV420 video in real
time on a mobile device. Our codec relies on two major contributions. First, we
design an efficient codec that uses a block-based motion compensation algorithm
available on the warping core of the mobile accelerator, and we show how to
quantize this model to integer precision. Second, we implement a fast decoder
pipeline that concurrently runs neural network components on the neural signal
processor, parallel entropy coding on the mobile GPU, and warping on the
warping core. Our codec outperforms the previous on-device codec by a large
margin with up to 48% BD-rate savings, while reducing the MAC count on the
receiver side by $10 \times$. We perform a careful ablation to demonstrate the
effect of the introduced motion compensation scheme, and ablate the effect of
model quantization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rozendaal_T/0/1/0/all/0/1&quot;&gt;Ties van Rozendaal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Singhal_T/0/1/0/all/0/1&quot;&gt;Tushar Singhal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Le_H/0/1/0/all/0/1&quot;&gt;Hoang Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sautiere_G/0/1/0/all/0/1&quot;&gt;Guillaume Sautiere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Said_A/0/1/0/all/0/1&quot;&gt;Amir Said&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Buska_K/0/1/0/all/0/1&quot;&gt;Krishna Buska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raha_A/0/1/0/all/0/1&quot;&gt;Anjuman Raha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kalatzis_D/0/1/0/all/0/1&quot;&gt;Dimitris Kalatzis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mehta_H/0/1/0/all/0/1&quot;&gt;Hitarth Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mayer_F/0/1/0/all/0/1&quot;&gt;Frank Mayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nagel_M/0/1/0/all/0/1&quot;&gt;Markus Nagel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wiggers_A/0/1/0/all/0/1&quot;&gt;Auke Wiggers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02641">
<title>Deformation-Invariant Neural Network and Its Applications in Distorted Image Restoration and Analysis. (arXiv:2310.02641v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02641</link>
<description rdf:parseType="Literal">&lt;p&gt;Images degraded by geometric distortions pose a significant challenge to
imaging and computer vision tasks such as object recognition. Deep
learning-based imaging models usually fail to give accurate performance for
geometrically distorted images. In this paper, we propose the
deformation-invariant neural network (DINN), a framework to address the problem
of imaging tasks for geometrically distorted images. The DINN outputs
consistent latent features for images that are geometrically distorted but
represent the same underlying object or scene. The idea of DINN is to
incorporate a simple component, called the quasiconformal transformer network
(QCTN), into other existing deep networks for imaging tasks. The QCTN is a deep
neural network that outputs a quasiconformal map, which can be used to
transform a geometrically distorted image into an improved version that is
closer to the distribution of natural or good images. It first outputs a
Beltrami coefficient, which measures the quasiconformality of the output
deformation map. By controlling the Beltrami coefficient, the local geometric
distortion under the quasiconformal mapping can be controlled. The QCTN is
lightweight and simple, which can be readily integrated into other existing
deep neural networks to enhance their performance. Leveraging our framework, we
have developed an image classification network that achieves accurate
classification of distorted images. Our proposed framework has been applied to
restore geometrically distorted images by atmospheric turbulence and water
turbulence. DINN outperforms existing GAN-based restoration methods under these
scenarios, demonstrating the effectiveness of the proposed framework.
Additionally, we apply our proposed framework to the 1-1 verification of human
face images under atmospheric turbulence and achieve satisfactory performance,
further demonstrating the efficacy of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Han Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qiguang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lui_L/0/1/0/all/0/1&quot;&gt;Lok Ming Lui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09478">
<title>MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning. (arXiv:2310.09478v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09478</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models have shown their remarkable capabilities as a general
interface for various language-related applications. Motivated by this, we
target to build a unified interface for completing many vision-language tasks
including image description, visual question answering, and visual grounding,
among others. The challenge is to use a single model for performing diverse
vision-language tasks effectively with simple multi-modal instructions. Towards
this objective, we introduce MiniGPT-v2, a model that can be treated as a
unified interface for better handling various vision-language tasks. We propose
using unique identifiers for different tasks when training the model. These
identifiers enable our model to better distinguish each task instruction
effortlessly and also improve the model learning efficiency for each task.
After the three-stage training, the experimental results show that MiniGPT-v2
achieves strong performance on many visual question-answering and visual
grounding benchmarks compared to other vision-language generalist models. Our
model and codes are available at https://minigpt-v2.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Deyao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xiaoqian Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zechun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pengchuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamoorthi_R/0/1/0/all/0/1&quot;&gt;Raghuraman Krishnamoorthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1&quot;&gt;Vikas Chandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yunyang Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1&quot;&gt;Mohamed Elhoseiny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10149">
<title>Recursive Segmentation Living Image: An eXplainable AI (XAI) Approach for Computing Structural Beauty of Images or the Livingness of Space. (arXiv:2310.10149v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10149</link>
<description rdf:parseType="Literal">&lt;p&gt;This study introduces the concept of &quot;structural beauty&quot; as an objective
computational approach for evaluating the aesthetic appeal of images. Through
the utilization of the Segment anything model (SAM), we propose a method that
leverages recursive segmentation to extract finer-grained substructures.
Additionally, by reconstructing the hierarchical structure, we obtain a more
accurate representation of substructure quantity and hierarchy. This approach
reproduces and extends our previous research, allowing for the simultaneous
assessment of Livingness in full-color images without the need for grayscale
conversion or separate computations for foreground and background Livingness.
Furthermore, the application of our method to the Scenic or Not dataset, a
repository of subjective scenic ratings, demonstrates a high degree of
consistency with subjective ratings in the 0-6 score range. This underscores
that structural beauty is not solely a subjective perception, but a
quantifiable attribute accessible through objective computation. Through our
case studies, we have arrived at three significant conclusions. 1) our method
demonstrates the capability to accurately segment meaningful objects, including
trees, buildings, and windows, as well as abstract substructures within
paintings. 2) we observed that the clarity of an image impacts our
computational results; clearer images tend to yield higher Livingness scores.
However, for equally blurry images, Livingness does not exhibit a significant
reduction, aligning with human visual perception. 3) our approach fundamentally
differs from methods employing Convolutional Neural Networks (CNNs) for
predicting image scores. Our method not only provides computational results but
also offers transparency and interpretability, positioning it as a novel avenue
in the realm of Explainable AI (XAI).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qianxiang_Y/0/1/0/all/0/1&quot;&gt;Yao Qianxiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bin Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.14918">
<title>ARNIQA: Learning Distortion Manifold for Image Quality Assessment. (arXiv:2310.14918v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.14918</link>
<description rdf:parseType="Literal">&lt;p&gt;No-Reference Image Quality Assessment (NR-IQA) aims to develop methods to
measure image quality in alignment with human perception without the need for a
high-quality reference image. In this work, we propose a self-supervised
approach named ARNIQA (leArning distoRtion maNifold for Image Quality
Assessment) for modeling the image distortion manifold to obtain quality
representations in an intrinsic manner. First, we introduce an image
degradation model that randomly composes ordered sequences of consecutively
applied distortions. In this way, we can synthetically degrade images with a
large variety of degradation patterns. Second, we propose to train our model by
maximizing the similarity between the representations of patches of different
images distorted equally, despite varying content. Therefore, images degraded
in the same manner correspond to neighboring positions within the distortion
manifold. Finally, we map the image representations to the quality scores with
a simple linear regressor, thus without fine-tuning the encoder weights. The
experiments show that our approach achieves state-of-the-art performance on
several datasets. In addition, ARNIQA demonstrates improved data efficiency,
generalization capabilities, and robustness compared to competing methods. The
code and the model are publicly available at
https://github.com/miccunifi/ARNIQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agnolucci_L/0/1/0/all/0/1&quot;&gt;Lorenzo Agnolucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galteri_L/0/1/0/all/0/1&quot;&gt;Leonardo Galteri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertini_M/0/1/0/all/0/1&quot;&gt;Marco Bertini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1&quot;&gt;Alberto Del Bimbo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16858">
<title>4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance Fields via Semantic Distillation. (arXiv:2310.16858v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16858</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper targets interactive object-level editing (e.g., deletion,
recoloring, transformation, composition) in dynamic scenes. Recently, some
methods aiming for flexible editing static scenes represented by neural
radiance field (NeRF) have shown impressive synthesis quality, while similar
capabilities in time-variant dynamic scenes remain limited. To solve this
problem, we propose 4D-Editor, an interactive semantic-driven editing
framework, allowing editing multiple objects in a dynamic NeRF with user
strokes on a single frame. We propose an extension to the original dynamic NeRF
by incorporating a hybrid semantic feature distillation to maintain
spatial-temporal consistency after editing. In addition, we design Recursive
Selection Refinement that significantly boosts object segmentation accuracy
within a dynamic NeRF to aid the editing process. Moreover, we develop
Multi-view Reprojection Inpainting to fill holes caused by incomplete scene
capture after editing. Extensive experiments and editing examples on real-world
demonstrate that 4D-Editor achieves photo-realistic editing on dynamic NeRFs.
Project page: https://patrickddj.github.io/4D-Editor
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1&quot;&gt;Dadong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_Z/0/1/0/all/0/1&quot;&gt;Zhihui Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiaobo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xidong Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18620">
<title>ODM3D: Alleviating Foreground Sparsity for Semi-Supervised Monocular 3D Object Detection. (arXiv:2310.18620v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18620</link>
<description rdf:parseType="Literal">&lt;p&gt;Monocular 3D object detection (M3OD) is a significant yet inherently
challenging task in autonomous driving due to absence of explicit depth cues in
a single RGB image. In this paper, we strive to boost currently underperforming
monocular 3D object detectors by leveraging an abundance of unlabelled data via
semi-supervised learning. Our proposed ODM3D framework entails cross-modal
knowledge distillation at various levels to inject LiDAR-domain knowledge into
a monocular detector during training. By identifying foreground sparsity as the
main culprit behind existing methods&apos; suboptimal training, we exploit the
precise localisation information embedded in LiDAR points to enable more
foreground-attentive and efficient distillation via the proposed BEV occupancy
guidance mask, leading to notably improved knowledge transfer and M3OD
performance. Besides, motivated by insights into why existing cross-modal
GT-sampling techniques fail on our task at hand, we further design a novel
cross-modal object-wise data augmentation strategy for effective RGB-LiDAR
joint learning. Our method ranks 1st in both KITTI validation and test
benchmarks, significantly surpassing all existing monocular methods, supervised
or semi-supervised, on both BEV and 3D detection metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weijia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dongnan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Weidong Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18651">
<title>LG-Self: Local-Global Self-Supervised Visual Representation Learning. (arXiv:2310.18651v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18651</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised representation learning methods mainly focus on image-level
instance discrimination. This study explores the potential benefits of
incorporating patch-level discrimination into existing methods to enhance the
quality of learned representations by simultaneously looking at local and
global visual features. Towards this idea, we present a straightforward yet
effective patch-matching algorithm that can find the corresponding patches
across the augmented views of an image. The augmented views are subsequently
fed into a self-supervised learning framework employing Vision Transformer
(ViT) as its backbone. The result is the generation of both image-level and
patch-level representations. Leveraging the proposed patch-matching algorithm,
the model minimizes the representation distance between not only the CLS tokens
but also the corresponding patches. As a result, the model gains a more
comprehensive understanding of both the entirety of the image as well as its
finer details. We pretrain the proposed method on small, medium, and
large-scale datasets. It is shown that our approach could outperform
state-of-the-art image-level representation learning methods on both image
classification and downstream tasks. Keywords: Self-Supervised Learning; Visual
Representations; Local-Global Representation Learning; Patch-Wise
Representation Learning; Vision Transformer (ViT)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javidani_A/0/1/0/all/0/1&quot;&gt;Ali Javidani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadeghi_M/0/1/0/all/0/1&quot;&gt;Mohammad Amin Sadeghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araabi_B/0/1/0/all/0/1&quot;&gt;Babak Nadjar Araabi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19109">
<title>Dynamic Task and Weight Prioritization Curriculum Learning for Multimodal Imagery. (arXiv:2310.19109v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19109</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores post-disaster analytics using multimodal deep learning
models trained with curriculum learning method. Studying post-disaster
analytics is important as it plays a crucial role in mitigating the impact of
disasters by providing timely and accurate insights into the extent of damage
and the allocation of resources. We propose a curriculum learning strategy to
enhance the performance of multimodal deep learning models. Curriculum learning
emulates the progressive learning sequence in human education by training deep
learning models on increasingly complex data. Our primary objective is to
develop a curriculum-trained multimodal deep learning model, with a particular
focus on visual question answering (VQA) capable of jointly processing image
and text data, in conjunction with semantic segmentation for disaster analytics
using the
FloodNet\footnote{https://github.com/BinaLab/FloodNet-Challenge-EARTHVISION2021}
dataset. To achieve this, U-Net model is used for semantic segmentation and
image encoding. A custom built text classifier is used for visual question
answering. Existing curriculum learning methods rely on manually defined
difficulty functions. We introduce a novel curriculum learning approach termed
Dynamic Task and Weight Prioritization (DATWEP), which leverages a
gradient-based method to automatically decide task difficulty during curriculum
learning training, thereby eliminating the need for explicit difficulty
computation. The integration of DATWEP into our multimodal model shows
improvement on VQA performance. Source code is available at
https://github.com/fualsan/DATWEP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alsan_H/0/1/0/all/0/1&quot;&gt;Huseyin Fuat Alsan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arsan_T/0/1/0/all/0/1&quot;&gt;Taner Arsan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19695">
<title>Deep-learning-based decomposition of overlapping-sparse images: application at the vertex of neutrino interactions. (arXiv:2310.19695v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19695</link>
<description rdf:parseType="Literal">&lt;p&gt;Image decomposition plays a crucial role in various computer vision tasks,
enabling the analysis and manipulation of visual content at a fundamental
level. Overlapping images, which occur when multiple objects or scenes
partially occlude each other, pose unique challenges for decomposition
algorithms. The task intensifies when working with sparse images, where the
scarcity of meaningful information complicates the precise extraction of
components. This paper presents a solution that leverages the power of deep
learning to accurately extract individual objects within multi-dimensional
overlapping-sparse images, with a direct application in high-energy physics
with decomposition of overlaid elementary particles obtained from imaging
detectors. In particular, the proposed approach tackles a highly complex yet
unsolved problem: identifying and measuring independent particles at the vertex
of neutrino interactions, where one expects to observe detector images with
multiple indiscernible overlapping charged particles. By decomposing the image
of the detector activity at the vertex through deep learning, it is possible to
infer the kinematic parameters of the identified low-momentum particles - which
otherwise would remain neglected - and enhance the reconstructed energy
resolution of the neutrino event. We also present an additional step - that can
be tuned directly on detector data - combining the above method with a
fully-differentiable generative model to improve the image decomposition
further and, consequently, the resolution of the measured parameters, achieving
unprecedented results. This improvement is crucial for precisely measuring the
parameters that govern neutrino flavour oscillations and searching for
asymmetries between matter and antimatter.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_Monsalve_S/0/1/0/all/0/1&quot;&gt;Sa&amp;#xfa;l Alonso-Monsalve&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sgalaberna_D/0/1/0/all/0/1&quot;&gt;Davide Sgalaberna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xingyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molines_A/0/1/0/all/0/1&quot;&gt;Adrien Molines&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGrew_C/0/1/0/all/0/1&quot;&gt;Clark McGrew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubbia_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Rubbia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00660">
<title>TPSeNCE: Towards Artifact-Free Realistic Rain Generation for Deraining and Object Detection in Rain. (arXiv:2311.00660v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00660</link>
<description rdf:parseType="Literal">&lt;p&gt;Rain generation algorithms have the potential to improve the generalization
of deraining methods and scene understanding in rainy conditions. However, in
practice, they produce artifacts and distortions and struggle to control the
amount of rain generated due to a lack of proper constraints. In this paper, we
propose an unpaired image-to-image translation framework for generating
realistic rainy images. We first introduce a Triangular Probability Similarity
(TPS) constraint to guide the generated images toward clear and rainy images in
the discriminator manifold, thereby minimizing artifacts and distortions during
rain generation. Unlike conventional contrastive learning approaches, which
indiscriminately push negative samples away from the anchors, we propose a
Semantic Noise Contrastive Estimation (SeNCE) strategy and reassess the pushing
force of negative samples based on the semantic similarity between the clear
and the rainy images and the feature similarity between the anchor and the
negative samples. Experiments demonstrate realistic rain generation with
minimal artifacts and distortions, which benefits image deraining and object
detection in rain. Furthermore, the method can be used to generate realistic
snowy and night images, underscoring its potential for broader applicability.
Code is available at https://github.com/ShenZheng2000/TPSeNCE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Changjie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narasimhan_S/0/1/0/all/0/1&quot;&gt;Srinivasa G. Narasimhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00729">
<title>ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection. (arXiv:2311.00729v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00729</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal action detection (TAD) involves the localization and classification
of action instances within untrimmed videos. While standard TAD follows fully
supervised learning with closed-set setting on large training data, recent
zero-shot TAD methods showcase the promising open-set setting by leveraging
large-scale contrastive visual-language (ViL) pretrained models. However,
existing zero-shot TAD methods have limitations on how to properly construct
the strong relationship between two interdependent tasks of localization and
classification and adapt ViL model to video understanding. In this work, we
present ZEETAD, featuring two modules: dual-localization and zero-shot proposal
classification. The former is a Transformer-based module that detects action
events while selectively collecting crucial semantic embeddings for later
recognition. The latter one, CLIP-based module, generates semantic embeddings
from text and frame inputs for each temporal unit. Additionally, we enhance
discriminative capability on unseen classes by minimally updating the frozen
CLIP encoder with lightweight adapters. Extensive experiments on THUMOS14 and
ActivityNet-1.3 datasets demonstrate our approach&apos;s superior performance in
zero-shot TAD and effective knowledge transfer from ViL models to unseen action
categories.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phan_T/0/1/0/all/0/1&quot;&gt;Thinh Phan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_K/0/1/0/all/0/1&quot;&gt;Khoa Vo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1&quot;&gt;Duy Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doretto_G/0/1/0/all/0/1&quot;&gt;Gianfranco Doretto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adjeroh_D/0/1/0/all/0/1&quot;&gt;Donald Adjeroh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngan Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01804">
<title>inkn&apos;hue: Enhancing Manga Colorization from Multiple Priors with Alignment Multi-Encoder VAE. (arXiv:2311.01804v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01804</link>
<description rdf:parseType="Literal">&lt;p&gt;Manga, a form of Japanese comics and distinct visual storytelling, has
captivated readers worldwide. Traditionally presented in black and white,
manga&apos;s appeal lies in its ability to convey complex narratives and emotions
through intricate line art and shading. Yet, the desire to experience manga in
vibrant colors has sparked the pursuit of manga colorization, a task of
paramount significance for artists. However, existing methods, originally
designed for line art and sketches, face challenges when applied to manga.
These methods often fall short in achieving the desired results, leading to the
need for specialized manga-specific solutions. Existing approaches frequently
rely on a single training step or extensive manual artist intervention, which
can yield less satisfactory outcomes. To address these challenges, we propose a
specialized framework for manga colorization. Leveraging established models for
shading and vibrant coloring, our approach aligns both using a multi-encoder
VAE. This structured workflow ensures clear and colorful results, with the
option to incorporate reference images and manual hints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiramahapokee_T/0/1/0/all/0/1&quot;&gt;Tawin Jiramahapokee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01929">
<title>ProS: Facial Omni-Representation Learning via Prototype-based Self-Distillation. (arXiv:2311.01929v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01929</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel approach, called Prototype-based
Self-Distillation (ProS), for unsupervised face representation learning. The
existing supervised methods heavily rely on a large amount of annotated
training facial data, which poses challenges in terms of data collection and
privacy concerns. To address these issues, we propose ProS, which leverages a
vast collection of unlabeled face images to learn a comprehensive facial
omni-representation. In particular, ProS consists of two vision-transformers
(teacher and student models) that are trained with different augmented images
(cropping, blurring, coloring, etc.). Besides, we build a face-aware retrieval
system along with augmentations to obtain the curated images comprising
predominantly facial areas. To enhance the discrimination of learned features,
we introduce a prototype-based matching loss that aligns the similarity
distributions between features (teacher or student) and a set of learnable
prototypes. After pre-training, the teacher vision transformer serves as a
backbone for downstream tasks, including attribute estimation, expression
recognition, and landmark alignment, achieved through simple fine-tuning with
additional layers. Extensive experiments demonstrate that our method achieves
state-of-the-art performance on various tasks, both in full and few-shot
settings. Furthermore, we investigate pre-training with synthetic face images,
and ProS exhibits promising performance in this scenario as well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Di_X/0/1/0/all/0/1&quot;&gt;Xing Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yiyu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yu Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02747">
<title>Attention Modules Improve Image-Level Anomaly Detection for Industrial Inspection: A DifferNet Case Study. (arXiv:2311.02747v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02747</link>
<description rdf:parseType="Literal">&lt;p&gt;Within (semi-)automated visual industrial inspection, learning-based
approaches for assessing visual defects, including deep neural networks, enable
the processing of otherwise small defect patterns in pixel size on
high-resolution imagery. The emergence of these often rarely occurring defect
patterns explains the general need for labeled data corpora. To alleviate this
issue and advance the current state of the art in unsupervised visual
inspection, this work proposes a DifferNet-based solution enhanced with
attention modules: AttentDifferNet. It improves image-level detection and
classification capabilities on three visual anomaly detection datasets for
industrial inspection: InsPLAD-fault, MVTec AD, and Semiconductor Wafer. In
comparison to the state of the art, AttentDifferNet achieves improved results,
which are, in turn, highlighted throughout our quali-quantitative study. Our
quantitative evaluation shows an average improvement - compared to DifferNet -
of 1.77 +/- 0.25 percentage points in overall AUROC considering all three
datasets, reaching SOTA results in InsPLAD-fault, an industrial inspection
in-the-wild dataset. As our variants to AttentDifferNet show great prospects in
the context of currently investigated approaches, a baseline is formulated,
emphasizing the importance of attention for industrial anomaly detection both
in the wild and in controlled environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Luiz Buarque Vieira e Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simoes_F/0/1/0/all/0/1&quot;&gt;Francisco Sim&amp;#xf5;es&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kowerko_D/0/1/0/all/0/1&quot;&gt;Danny Kowerko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlosser_T/0/1/0/all/0/1&quot;&gt;Tobias Schlosser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Battisti_F/0/1/0/all/0/1&quot;&gt;Felipe Battisti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teichrieb_V/0/1/0/all/0/1&quot;&gt;Veronica Teichrieb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02831">
<title>SemanticTopoLoop: Semantic Loop Closure With 3D Topological Graph Based on Quadric-Level Object Map. (arXiv:2311.02831v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02831</link>
<description rdf:parseType="Literal">&lt;p&gt;Loop closure, as one of the crucial components in SLAM, plays an essential
role in correcting the accumulated errors. Traditional appearance-based
methods, such as bag-of-words models, are often limited by local 2D features
and the volume of training data, making them less versatile and robust in
real-world scenarios, leading to missed detections or false positives
detections in loop closure. To address these issues, we first propose a
object-level data association method based on multi-level verification, which
can associate 2D semantic features of current frame with 3D objects landmarks
of map. Next, taking advantage of these association relations, we introduce a
semantic loop closure method based on quadric-level object map topology, which
represents scenes through the topological graph of objects and achieves
accurate loop closure at a wide field of view by comparing differences in the
topological graphs. Finally, we integrate these two methods into a complete
object-aware SLAM system. Qualitative experiments and ablation studies
demonstrate the effectiveness and robustness of the proposed object-level data
association algorithm. Quantitative experiments show that our semantic loop
closure method outperforms existing state-of-the-art methods in terms of
precision, recall and localization accuracy metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhenzhong Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02877">
<title>Inner-IoU: More Effective Intersection over Union Loss with Auxiliary Bounding Box. (arXiv:2311.02877v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02877</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of detectors, Bounding Box Regression (BBR) loss
function has constantly updated and optimized. However, the existing IoU-based
BBR still focus on accelerating convergence by adding new loss terms, ignoring
the limitations of IoU loss term itself. Although theoretically IoU loss can
effectively describe the state of bounding box regression,in practical
applications, it cannot adjust itself according to different detectors and
detection tasks, and does not have strong generalization. Based on the above,
we first analyzed the BBR model and concluded that distinguishing different
regression samples and using different scales of auxiliary bounding boxes to
calculate losses can effectively accelerate the bounding box regression
process. For high IoU samples, using smaller auxiliary bounding boxes to
calculate losses can accelerate convergence, while larger auxiliary bounding
boxes are suitable for low IoU samples. Then, we propose Inner-IoU loss, which
calculates IoU loss through auxiliary bounding boxes. For different datasets
and detectors, we introduce a scaling factor ratio to control the scale size of
the auxiliary bounding boxes for calculating losses. Finally, integrate
Inner-IoU into the existing IoU-based loss functions for simulation and
comparative experiments. The experiment result demonstrate a further
enhancement in detection performance with the utilization of the method
proposed in this paper, verifying the effectiveness and generalization ability
of Inner-IoU loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Cong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuaijie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03054">
<title>AnyText: Multilingual Visual Text Generation And Editing. (arXiv:2311.03054v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03054</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion model based Text-to-Image has achieved impressive achievements
recently. Although current technology for synthesizing images is highly
advanced and capable of generating images with high fidelity, it is still
possible to give the show away when focusing on the text area in the generated
image. To address this issue, we introduce AnyText, a diffusion-based
multilingual visual text generation and editing model, that focuses on
rendering accurate and coherent text in the image. AnyText comprises a
diffusion pipeline with two primary elements: an auxiliary latent module and a
text embedding module. The former uses inputs like text glyph, position, and
masked image to generate latent features for text generation or editing. The
latter employs an OCR model for encoding stroke data as embeddings, which blend
with image caption embeddings from the tokenizer to generate texts that
seamlessly integrate with the background. We employed text-control diffusion
loss and text perceptual loss for training to further enhance writing accuracy.
AnyText can write characters in multiple languages, to the best of our
knowledge, this is the first work to address multilingual visual text
generation. It is worth mentioning that AnyText can be plugged into existing
diffusion models from the community for rendering or editing text accurately.
After conducting extensive evaluation experiments, our method has outperformed
all other approaches by a significant margin. Additionally, we contribute the
first large-scale multilingual text images dataset, AnyWord-3M, containing 3
million image-text pairs with OCR annotations in multiple languages. Based on
AnyWord-3M dataset, we propose AnyText-benchmark for the evaluation of visual
text generation accuracy and quality. Our project will be open-sourced on
https://github.com/tyxsspa/AnyText to improve and promote the development of
text generation technology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuo_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Tuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1&quot;&gt;Wangmeng Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jun-Yan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1&quot;&gt;Yifeng Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xuansong Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03071">
<title>OrthoNets: Orthogonal Channel Attention Networks. (arXiv:2311.03071v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03071</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing an effective channel attention mechanism implores one to find a
lossy-compression method allowing for optimal feature representation. Despite
recent progress in the area, it remains an open problem. FcaNet, the current
state-of-the-art channel attention mechanism, attempted to find such an
information-rich compression using Discrete Cosine Transforms (DCTs). One
drawback of FcaNet is that there is no natural choice of the DCT frequencies.
To circumvent this issue, FcaNet experimented on ImageNet to find optimal
frequencies. We hypothesize that the choice of frequency plays only a
supporting role and the primary driving force for the effectiveness of their
attention filters is the orthogonality of the DCT kernels. To test this
hypothesis, we construct an attention mechanism using randomly initialized
orthogonal filters. Integrating this mechanism into ResNet, we create OrthoNet.
We compare OrthoNet to FcaNet (and other attention mechanisms) on Birds,
MS-COCO, and Places356 and show superior performance. On the ImageNet dataset,
our method competes with or surpasses the current state-of-the-art. Our results
imply that an optimal choice of filter is elusive and generalization can be
achieved with a sufficiently large number of orthogonal filters. We further
investigate other general principles for implementing channel attention, such
as its position in the network and channel groupings. Our code is publicly
available at https://github.com/hady1011/OrthoNets/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salman_H/0/1/0/all/0/1&quot;&gt;Hadi Salman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parks_C/0/1/0/all/0/1&quot;&gt;Caleb Parks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swan_M/0/1/0/all/0/1&quot;&gt;Matthew Swan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gauch_J/0/1/0/all/0/1&quot;&gt;John Gauch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03076">
<title>SugarViT -- Multi-objective Regression of UAV Images with Vision Transformers and Deep Label Distribution Learning Demonstrated on Disease Severity Prediction in Sugar Beet. (arXiv:2311.03076v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03076</link>
<description rdf:parseType="Literal">&lt;p&gt;Remote sensing and artificial intelligence are pivotal technologies of
precision agriculture nowadays. The efficient retrieval of large-scale field
imagery combined with machine learning techniques shows success in various
tasks like phenotyping, weeding, cropping, and disease control. This work will
introduce a machine learning framework for automatized large-scale
plant-specific trait annotation for the use case disease severity scoring for
Cercospora Leaf Spot (CLS) in sugar beet. With concepts of Deep Label
Distribution Learning (DLDL), special loss functions, and a tailored model
architecture, we develop an efficient Vision Transformer based model for
disease severity scoring called SugarViT. One novelty in this work is the
combination of remote sensing data with environmental parameters of the
experimental sites for disease severity prediction. Although the model is
evaluated on this special use case, it is held as generic as possible to also
be applicable to various image-based classification and regression tasks. With
our framework, it is even possible to learn models on multi-objective problems
as we show by a pretraining on environmental metadata.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunder_M/0/1/0/all/0/1&quot;&gt;Maurice G&amp;#xfc;nder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamati_F/0/1/0/all/0/1&quot;&gt;Facundo Ram&amp;#xf3;n Ispizua Yamati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alcantara_A/0/1/0/all/0/1&quot;&gt;Abel Andree Barreto Alc&amp;#xe1;ntara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahlein_A/0/1/0/all/0/1&quot;&gt;Anne-Katrin Mahlein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sifa_R/0/1/0/all/0/1&quot;&gt;Rafet Sifa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauckhage_C/0/1/0/all/0/1&quot;&gt;Christian Bauckhage&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03287">
<title>Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges. (arXiv:2311.03287v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03287</link>
<description rdf:parseType="Literal">&lt;p&gt;While GPT-4V(ision) impressively models both visual and textual information
simultaneously, it&apos;s hallucination behavior has not been systematically
assessed. To bridge this gap, we introduce a new benchmark, namely, the Bias
and Interference Challenges in Visual Language Models (Bingo). This benchmark
is designed to evaluate and shed light on the two common types of
hallucinations in visual language models: bias and interference. Here, bias
refers to the model&apos;s tendency to hallucinate certain types of responses,
possibly due to imbalance in its training data. Interference pertains to
scenarios where the judgment of GPT-4V(ision) can be disrupted due to how the
text prompt is phrased or how the input image is presented. We identify a
notable regional bias, whereby GPT-4V(ision) is better at interpreting Western
images or images with English writing compared to images from other countries
or containing text in other languages. Moreover, GPT-4V(ision) is vulnerable to
leading questions and is often confused when interpreting multiple images
together. Popular mitigation approaches, such as self-correction and
chain-of-thought reasoning, are not effective in resolving these challenges. We
also identified similar biases and interference vulnerabilities with LLaVA and
Bard. Our results characterize the hallucination challenges in GPT-4V(ision)
and state-of-the-art visual-language models, and highlight the need for new
solutions. The Bingo benchmark is available at https://github.com/gzcch/Bingo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1&quot;&gt;Chenhang Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yiyang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xinyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shirley Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Linjun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Huaxiu Yao&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>