<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-02T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00876" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00883" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00907" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00974" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00976" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00996" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01001" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01007" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01054" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01056" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01078" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01089" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01099" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01124" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01172" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01179" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01180" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01183" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01199" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01204" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01242" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01265" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01301" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01326" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01330" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2104.02206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2108.11451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.02164" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.09041" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.04718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01530" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06035" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.10399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19852" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10307" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11460" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.17353" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00544" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.00867">
<title>Tensor Networks for Explainable Machine Learning in Cybersecurity. (arXiv:2401.00867v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00867</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we show how tensor networks help in developing explainability
of machine learning algorithms. Specifically, we develop an unsupervised
clustering algorithm based on Matrix Product States (MPS) and apply it in the
context of a real use-case of adversary-generated threat intelligence. Our
investigation proves that MPS rival traditional deep learning models such as
autoencoders and GANs in terms of performance, while providing much richer
model interpretability. Our approach naturally facilitates the extraction of
feature-wise probabilities, Von Neumann Entropy, and mutual information,
offering a compelling narrative for classification of anomalies and fostering
an unprecedented level of transparency and interpretability, something
fundamental to understand the rationale behind artificial intelligence
decisions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aizpurua_B/0/1/0/all/0/1&quot;&gt;Borja Aizpurua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orus_R/0/1/0/all/0/1&quot;&gt;Roman Orus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00870">
<title>Teach Large Language Models to Forget Privacy. (arXiv:2401.00870v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.00870</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have proven powerful, but the risk of privacy
leakage remains a significant concern. Traditional privacy-preserving methods,
such as Differential Privacy and Homomorphic Encryption, are inadequate for
black-box API-only settings, demanding either model transparency or heavy
computational resources. We propose Prompt2Forget (P2F), the first framework
designed to tackle the LLM local privacy challenge by teaching LLM to forget.
The method involves decomposing full questions into smaller segments,
generating fabricated answers, and obfuscating the model&apos;s memory of the
original input. A benchmark dataset was crafted with questions containing
privacy-sensitive information from diverse fields. P2F achieves zero-shot
generalization, allowing adaptability across a wide range of use cases without
manual adjustments. Experimental results indicate P2F&apos;s robust capability to
obfuscate LLM&apos;s memory, attaining a forgetfulness score of around 90\% without
any utility loss. This represents an enhancement of up to 63\% when contrasted
with the naive direct instruction technique, highlighting P2F&apos;s efficacy in
mitigating memory retention of sensitive information within LLMs. Our findings
establish the first benchmark in the novel field of the LLM forgetting task,
representing a meaningful advancement in privacy preservation in the emerging
LLM domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1&quot;&gt;Ran Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yujun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenqian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_P/0/1/0/all/0/1&quot;&gt;Peihua Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1&quot;&gt;Yan Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yinchuan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00876">
<title>Balanced Graph Structure Information for Brain Disease Detection. (arXiv:2401.00876v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00876</link>
<description rdf:parseType="Literal">&lt;p&gt;Analyzing connections between brain regions of interest (ROI) is vital to
detect neurological disorders such as autism or schizophrenia. Recent
advancements employ graph neural networks (GNNs) to utilize graph structures in
brains, improving detection performances. Current methods use correlation
measures between ROI&apos;s blood-oxygen-level-dependent (BOLD) signals to generate
the graph structure. Other methods use the training samples to learn the
optimal graph structure through end-to-end learning. However, implementing
those methods independently leads to some issues with noisy data for the
correlation graphs and overfitting problems for the optimal graph. In this
work, we proposed Bargrain (balanced graph structure for brains), which models
two graph structures: filtered correlation matrix and optimal sample graph
using graph convolution networks (GCNs). This approach aims to get advantages
from both graphs and address the limitations of only relying on a single type
of structure. Based on our extensive experiment, Bargrain outperforms
state-of-the-art methods in classification tasks on brain disease datasets, as
measured by average F1 scores.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Febrinanto_F/0/1/0/all/0/1&quot;&gt;Falih Gozi Febrinanto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mujie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Feng Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00880">
<title>Towards Bridging the Gap between High-Level Reasoning and Execution on Robots. (arXiv:2401.00880v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00880</link>
<description rdf:parseType="Literal">&lt;p&gt;When reasoning about actions, e.g., by means of task planning or agent
programming with Golog, the robot&apos;s actions are typically modeled on an
abstract level, where complex actions such as picking up an object are treated
as atomic primitives with deterministic effects and preconditions that only
depend on the current state. However, when executing such an action on a robot
it can no longer be seen as a primitive. Instead, action execution is a complex
task involving multiple steps with additional temporal preconditions and timing
constraints. Furthermore, the action may be noisy, e.g., producing erroneous
sensing results and not always having the desired effects. While these aspects
are typically ignored in reasoning tasks, they need to be dealt with during
execution. In this thesis, we propose several approaches towards closing this
gap.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1&quot;&gt;Till Hofmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00883">
<title>Automating Leukemia Diagnosis with Autoencoders: A Comparative Study. (arXiv:2401.00883v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00883</link>
<description rdf:parseType="Literal">&lt;p&gt;Leukemia is one of the most common and death-threatening types of cancer that
threaten human life. Medical data from some of the patient&apos;s critical
parameters contain valuable information hidden among these data. On this
subject, deep learning can be used to extract this information. In this paper,
AutoEncoders have been used to develop valuable features to help the precision
of leukemia diagnosis. It has been attempted to get the best activation
function and optimizer to use in AutoEncoder and designed the best architecture
for this neural network. The proposed architecture is compared with this area&apos;s
classical machine learning models. Our proposed method performs better than
other machine learning in precision and f1-score metrics by more than 11%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sayyadpour_M/0/1/0/all/0/1&quot;&gt;Minoo Sayyadpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moghaddamniya_N/0/1/0/all/0/1&quot;&gt;Nasibe Moghaddamniya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banirostam_T/0/1/0/all/0/1&quot;&gt;Touraj Banirostam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00893">
<title>Social-LLM: Modeling User Behavior at Scale using Language Models and Social Network Data. (arXiv:2401.00893v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2401.00893</link>
<description rdf:parseType="Literal">&lt;p&gt;The proliferation of social network data has unlocked unprecedented
opportunities for extensive, data-driven exploration of human behavior. The
structural intricacies of social networks offer insights into various
computational social science issues, particularly concerning social influence
and information diffusion. However, modeling large-scale social network data
comes with computational challenges. Though large language models make it
easier than ever to model textual content, any advanced network representation
methods struggle with scalability and efficient deployment to out-of-sample
users. In response, we introduce a novel approach tailored for modeling social
network data in user detection tasks. This innovative method integrates
localized social network interactions with the capabilities of large language
models. Operating under the premise of social network homophily, which posits
that socially connected users share similarities, our approach is designed to
address these challenges. We conduct a thorough evaluation of our method across
seven real-world social network datasets, spanning a diverse range of topics
and detection tasks, showcasing its applicability to advance research in
computational social science.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Julie Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrara_E/0/1/0/all/0/1&quot;&gt;Emilio Ferrara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00897">
<title>Masked Modeling for Self-supervised Representation Learning on Vision and Beyond. (arXiv:2401.00897v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00897</link>
<description rdf:parseType="Literal">&lt;p&gt;As the deep learning revolution marches on, self-supervised learning has
garnered increasing attention in recent years thanks to its remarkable
representation learning ability and the low dependence on labeled data. Among
these varied self-supervised techniques, masked modeling has emerged as a
distinctive approach that involves predicting parts of the original data that
are proportionally masked during training. This paradigm enables deep models to
learn robust representations and has demonstrated exceptional performance in
the context of computer vision, natural language processing, and other
modalities. In this survey, we present a comprehensive review of the masked
modeling framework and its methodology. We elaborate on the details of
techniques within masked modeling, including diverse masking strategies,
recovering targets, network architectures, and more. Then, we systematically
investigate its wide-ranging applications across domains. Furthermore, we also
explore the commonalities and differences between masked modeling methods in
different fields. Toward the end of this paper, we conclude by discussing the
limitations of current techniques and point out several potential avenues for
advancing masked modeling research. A paper list project with this survey is
available at \url{https://github.com/Lupin1998/Awesome-MIM}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Siyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Luyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zedong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Di Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lirong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zicheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1&quot;&gt;Jun Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Cheng Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Baigui Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Stan Z. Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00907">
<title>LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning Language Models. (arXiv:2401.00907v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00907</link>
<description rdf:parseType="Literal">&lt;p&gt;Fine-tuning Large Language Models (LLMs) adapts a trained model to specific
downstream tasks, significantly improving task-specific performance. Supervised
Fine-Tuning (SFT) is a common approach, where an LLM is trained to produce
desired answers. However, LLMs trained with SFT sometimes make simple mistakes
and result in hallucinations on reasoning tasks such as question-answering.
Without external feedback, it is difficult for SFT to learn a good mapping
between the question and the desired answer, especially with a small dataset.
This paper introduces an alternative to SFT called Natural Language Feedback
for Finetuning LLMs (LaFFi). LaFFi has LLMs directly predict the feedback they
will receive from an annotator. We find that requiring such reflection can
significantly improve the accuracy in in-domain question-answering tasks,
providing a promising direction for the application of natural language
feedback in the realm of SFT LLMs. Additional ablation studies show that the
portion of human-annotated data in the annotated datasets affects the
fine-tuning performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qianxi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yingyue Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;Jikun Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tianpei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jun Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1&quot;&gt;Matthew E. Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00916">
<title>Data Assimilation in Chaotic Systems Using Deep Reinforcement Learning. (arXiv:2401.00916v1 [math.DS])</title>
<link>http://arxiv.org/abs/2401.00916</link>
<description rdf:parseType="Literal">&lt;p&gt;Data assimilation (DA) plays a pivotal role in diverse applications, ranging
from climate predictions and weather forecasts to trajectory planning for
autonomous vehicles. A prime example is the widely used ensemble Kalman filter
(EnKF), which relies on linear updates to minimize variance among the ensemble
of forecast states. Recent advancements have seen the emergence of deep
learning approaches in this domain, primarily within a supervised learning
framework. However, the adaptability of such models to untrained scenarios
remains a challenge. In this study, we introduce a novel DA strategy that
utilizes reinforcement learning (RL) to apply state corrections using full or
partial observations of the state variables. Our investigation focuses on
demonstrating this approach to the chaotic Lorenz &apos;63 system, where the agent&apos;s
objective is to minimize the root-mean-squared error between the observations
and corresponding forecast states. Consequently, the agent develops a
correction strategy, enhancing model forecasts based on available system state
observations. Our strategy employs a stochastic action policy, enabling a Monte
Carlo-based DA framework that relies on randomly sampling the policy to
generate an ensemble of assimilated realizations. Results demonstrate that the
developed RL algorithm performs favorably when compared to the EnKF.
Additionally, we illustrate the agent&apos;s capability to assimilate non-Gaussian
data, addressing a significant limitation of the EnKF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hammoud_M/0/1/0/all/0/1&quot;&gt;Mohamad Abed El Rahman Hammoud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Raboudi_N/0/1/0/all/0/1&quot;&gt;Naila Raboudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Titi_E/0/1/0/all/0/1&quot;&gt;Edriss S. Titi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Knio_O/0/1/0/all/0/1&quot;&gt;Omar Knio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hoteit_I/0/1/0/all/0/1&quot;&gt;Ibrahim Hoteit&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00926">
<title>Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level Feature Fusion for Aiding Diagnosis of Blood Diseases. (arXiv:2401.00926v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00926</link>
<description rdf:parseType="Literal">&lt;p&gt;In standard hospital blood tests, the traditional process requires doctors to
manually isolate leukocytes from microscopic images of patients&apos; blood using
microscopes. These isolated leukocytes are then categorized via automatic
leukocyte classifiers to determine the proportion and volume of different types
of leukocytes present in the blood samples, aiding disease diagnosis. This
methodology is not only time-consuming and labor-intensive, but it also has a
high propensity for errors due to factors such as image quality and
environmental conditions, which could potentially lead to incorrect subsequent
classifications and misdiagnosis. To address these issues, this paper proposes
an innovative method of leukocyte detection: the Multi-level Feature Fusion and
Deformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte
scale disparity, we designed the High-level Screening-feature Fusion Pyramid
(HS-FPN), enabling multi-level fusion. This model uses high-level features as
weights to filter low-level feature information via a channel attention module
and then merges the screened information with the high-level features, thus
enhancing the model&apos;s feature expression capability. Further, we address the
issue of leukocyte feature scarcity by incorporating a multi-scale deformable
self-attention module in the encoder and using the self-attention and
cross-deformable attention mechanisms in the decoder, which aids in the
extraction of the global features of the leukocyte feature maps. The
effectiveness, superiority, and generalizability of the proposed MFDS-DETR
method are confirmed through comparisons with other cutting-edge leukocyte
detection models using the private WBCDD, public LISC and BCCD datasets. Our
source code and private WBCCD dataset are available at
https://github.com/JustlfC03/MFDS-DETR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenyan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Ben Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changmiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xianjun Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yuxing Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1&quot;&gt;Feiwei Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yu Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00961">
<title>Automated Model Selection for Tabular Data. (arXiv:2401.00961v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00961</link>
<description rdf:parseType="Literal">&lt;p&gt;Structured data in the form of tabular datasets contain features that are
distinct and discrete, with varying individual and relative importances to the
target. Combinations of one or more features may be more predictive and
meaningful than simple individual feature contributions. R&apos;s mixed effect
linear models library allows users to provide such interactive feature
combinations in the model design. However, given many features and possible
interactions to select from, model selection becomes an exponentially difficult
task. We aim to automate the model selection process for predictions on tabular
datasets incorporating feature interactions while keeping computational costs
small. The framework includes two distinct approaches for feature selection: a
Priority-based Random Grid Search and a Greedy Search method. The
Priority-based approach efficiently explores feature combinations using prior
probabilities to guide the search. The Greedy method builds the solution
iteratively by adding or removing features based on their impact. Experiments
on synthetic demonstrate the ability to effectively capture predictive feature
combinations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amballa_A/0/1/0/all/0/1&quot;&gt;Avinash Amballa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mekala_A/0/1/0/all/0/1&quot;&gt;Anmol Mekala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akkinapalli_G/0/1/0/all/0/1&quot;&gt;Gayathri Akkinapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madine_M/0/1/0/all/0/1&quot;&gt;Manas Madine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yarrabolu_N/0/1/0/all/0/1&quot;&gt;Naga Pavana Priya Yarrabolu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grabowicz_P/0/1/0/all/0/1&quot;&gt;Przemyslaw A. Grabowicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00964">
<title>Data Augmentation Techniques for Cross-Domain WiFi CSI-based Human Activity Recognition. (arXiv:2401.00964v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00964</link>
<description rdf:parseType="Literal">&lt;p&gt;The recognition of human activities based on WiFi Channel State Information
(CSI) enables contactless and visual privacy-preserving sensing in indoor
environments. However, poor model generalization, due to varying environmental
conditions and sensing hardware, is a well-known problem in this space. To
address this issue, in this work, data augmentation techniques commonly used in
image-based learning are applied to WiFi CSI to investigate their effects on
model generalization performance in cross-scenario and cross-system settings.
In particular, we focus on the generalization between line-of-sight (LOS) and
non-line-of-sight (NLOS) through-wall scenarios, as well as on the
generalization between different antenna systems, which remains under-explored.
We collect and make publicly available a dataset of CSI amplitude spectrograms
of human activities. Utilizing this data, an ablation study is conducted in
which activity recognition models based on the EfficientNetV2 architecture are
trained, allowing us to assess the effects of each augmentation on model
generalization performance. The gathered results show that specific
combinations of simple data augmentation techniques applied to CSI amplitude
data can significantly improve cross-scenario and cross-system generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strohmayer_J/0/1/0/all/0/1&quot;&gt;Julian Strohmayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kampel_M/0/1/0/all/0/1&quot;&gt;Martin Kampel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00974">
<title>Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models. (arXiv:2401.00974v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00974</link>
<description rdf:parseType="Literal">&lt;p&gt;Devising procedures for downstream task-oriented generative model selections
is an unresolved problem of practical importance. Existing studies focused on
the utility of a single family of generative models. They provided limited
insights on how synthetic data practitioners select the best family generative
models for synthetic training tasks given a specific combination of machine
learning model class and performance metric. In this paper, we approach the
downstream task-oriented generative model selections problem in the case of
training fraud detection models and investigate the best practice given
different combinations of model interpretability and model performance
constraints. Our investigation supports that, while both Neural
Network(NN)-based and Bayesian Network(BN)-based generative models are both
good to complete synthetic training task under loose model interpretability
constrain, the BN-based generative models is better than NN-based when
synthetic training fraud detection model under strict model interpretability
constrain. Our results provides practical guidance for machine learning
practitioner who is interested in replacing their training dataset from real to
synthetic, and shed lights on more general downstream task-oriented generative
model selection problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yinan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chi-Hua Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potluru_V/0/1/0/all/0/1&quot;&gt;Vamsi K. Potluru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balch_T/0/1/0/all/0/1&quot;&gt;Tucker Balch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1&quot;&gt;Guang Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00976">
<title>Nature-Inspired Algorithms in Optimization: Introduction, Hybridization and Insights. (arXiv:2401.00976v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2401.00976</link>
<description rdf:parseType="Literal">&lt;p&gt;Many problems in science and engineering are optimization problems, which may
require sophisticated optimization techniques to solve. Nature-inspired
algorithms are a class of metaheuristic algorithms for optimization, and some
algorithms or variants are often developed by hybridization. Benchmarking is
also important in evaluating the performance of optimization algorithms. This
chapter focuses on the overview of optimization, nature-inspired algorithms and
the role of hybridization. We will also highlight some issues with
hybridization of algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin-She Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00986">
<title>Real-Time Object Detection in Occluded Environment with Background Cluttering Effects Using Deep Learning. (arXiv:2401.00986v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00986</link>
<description rdf:parseType="Literal">&lt;p&gt;Detection of small, undetermined moving objects or objects in an occluded
environment with a cluttered background is the main problem of computer vision.
This greatly affects the detection accuracy of deep learning models. To
overcome these problems, we concentrate on deep learning models for real-time
detection of cars and tanks in an occluded environment with a cluttered
background employing SSD and YOLO algorithms and improved precision of
detection and reduce problems faced by these models. The developed method makes
the custom dataset and employs a preprocessing technique to clean the noisy
dataset. For training the developed model we apply the data augmentation
technique to balance and diversify the data. We fine-tuned, trained, and
evaluated these models on the established dataset by applying these techniques
and highlighting the results we got more accurately than without applying these
techniques. The accuracy and frame per second of the SSD-Mobilenet v2 model are
higher than YOLO V3 and YOLO V4. Furthermore, by employing various techniques
like data enhancement, noise reduction, parameter optimization, and model
fusion we improve the effectiveness of detection and recognition. We further
added a counting algorithm, and target attributes experimental comparison, and
made a graphical user interface system for the developed model with features of
object counting, alerts, status, resolution, and frame per second.
Subsequently, to justify the importance of the developed method analysis of
YOLO V3, V4, and SSD were incorporated. Which resulted in the overall
completion of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aamir_S/0/1/0/all/0/1&quot;&gt;Syed Muhammad Aamir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Hongbin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Malak Abid Ali Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aaqib_M/0/1/0/all/0/1&quot;&gt;Muhammad Aaqib&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00996">
<title>Safety and Performance, Why Not Both? Bi-Objective Optimized Model Compression against Heterogeneous Attacks Toward AI Software Deployment. (arXiv:2401.00996v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00996</link>
<description rdf:parseType="Literal">&lt;p&gt;The size of deep learning models in artificial intelligence (AI) software is
increasing rapidly, hindering the large-scale deployment on resource-restricted
devices (e.g., smartphones). To mitigate this issue, AI software compression
plays a crucial role, which aims to compress model size while keeping high
performance. However, the intrinsic defects in a big model may be inherited by
the compressed one. Such defects may be easily leveraged by adversaries, since
a compressed model is usually deployed in a large number of devices without
adequate protection. In this article, we aim to address the safe model
compression problem from the perspective of safety-performance co-optimization.
Specifically, inspired by the test-driven development (TDD) paradigm in
software engineering, we propose a test-driven sparse training framework called
SafeCompress. By simulating the attack mechanism as safety testing,
SafeCompress can automatically compress a big model to a small one following
the dynamic sparse training paradigm. Then, considering two kinds of
representative and heterogeneous attack mechanisms, i.e., black-box membership
inference attack and white-box membership inference attack, we develop two
concrete instances called BMIA-SafeCompress and WMIA-SafeCompress. Further, we
implement another instance called MMIA-SafeCompress by extending SafeCompress
to defend against the occasion when adversaries conduct black-box and white-box
membership inference attacks simultaneously. We conduct extensive experiments
on five datasets for both computer vision and natural language processing
tasks. The results show the effectiveness and generalizability of our
framework. We also discuss how to adapt SafeCompress to other attacks besides
membership inference attack, demonstrating the flexibility of SafeCompress.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Leye Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiao Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Anmin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1&quot;&gt;Tao Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01001">
<title>Metalearning-Informed Competence in Children: Implications for Responsible Brain-Inspired Artificial Intelligence. (arXiv:2401.01001v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2401.01001</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper offers a novel conceptual framework comprising four essential
cognitive mechanisms that operate concurrently and collaboratively to enable
metalearning (knowledge and regulation of learning) strategy implementation in
young children. A roadmap incorporating the core mechanisms and the associated
strategies is presented as an explanation of the developing brain&apos;s remarkable
cross-context learning competence. The tetrad of fundamental complementary
processes is chosen to collectively represent the bare-bones metalearning
architecture that can be extended to artificial intelligence (AI) systems
emulating brain-like learning and problem-solving skills. Utilizing the
metalearning-enabled young mind as a model for brain-inspired computing, this
work further discusses important implications for morally grounded AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Singh_C/0/1/0/all/0/1&quot;&gt;Chaitanya Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01007">
<title>Towards Net-Zero Carbon Emissions in Network AI for 6G and Beyond. (arXiv:2401.01007v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2401.01007</link>
<description rdf:parseType="Literal">&lt;p&gt;A global effort has been initiated to reduce the worldwide greenhouse gas
(GHG) emissions, primarily carbon emissions, by half by 2030 and reach net-zero
by 2050. The development of 6G must also be compliant with this goal.
Unfortunately, developing a sustainable and net-zero emission systems to meet
the users&apos; fast growing demands on mobile services, especially smart services
and applications, may be much more challenging than expected. Particularly,
despite the energy efficiency improvement in both hardware and software
designs, the overall energy consumption and carbon emission of mobile networks
are still increasing at a tremendous speed. The growing penetration of
resource-demanding AI algorithms and solutions further exacerbate this
challenge. In this article, we identify the major emission sources and
introduce an evaluation framework for analyzing the lifecycle of network AI
implementations. A novel joint dynamic energy trading and task allocation
optimization framework, called DETA, has been introduced to reduce the overall
carbon emissions. We consider a federated edge intelligence-based network AI
system as a case study to verify the effectiveness of our proposed solution.
Experimental results based on a hardware prototype suggest that our proposed
solution can reduce carbon emissions of network AI systems by up to 74.9%.
Finally, open problems and future directions are discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1&quot;&gt;Xiaohu Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1&quot;&gt;Guangming Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01008">
<title>Fast Inference Through The Reuse Of Attention Maps In Diffusion Models. (arXiv:2401.01008v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01008</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image diffusion models have demonstrated unprecedented abilities at
flexible and realistic image synthesis. However, the iterative process required
to produce a single image is costly and incurs a high latency, prompting
researchers to further investigate its efficiency. Typically, improvements in
latency have been achieved in two ways: (1) training smaller models through
knowledge distillation (KD); and (2) adopting techniques from ODE-theory to
facilitate larger step sizes. In contrast, we propose a training-free approach
that does not alter the step-size of the sampler. Specifically, we find the
repeated calculation of attention maps to be both costly and redundant;
therefore, we propose a structured reuse of attention maps during sampling. Our
initial reuse policy is motivated by rudimentary ODE-theory, which suggests
that reuse is most suitable late in the sampling procedure. After noting a
number of limitations in this theoretical approach, we empirically search for a
better policy. Unlike methods that rely on KD, our reuse policies can easily be
adapted to a variety of setups in a plug-and-play manner. Furthermore, when
applied to Stable Diffusion-1.5, our reuse policies reduce latency with minimal
repercussions on sample quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hunter_R/0/1/0/all/0/1&quot;&gt;Rosco Hunter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dudziak_L/0/1/0/all/0/1&quot;&gt;&amp;#x141;ukasz Dudziak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelfattah_M/0/1/0/all/0/1&quot;&gt;Mohamed S. Abdelfattah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehrotra_A/0/1/0/all/0/1&quot;&gt;Abhinav Mehrotra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1&quot;&gt;Sourav Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hongkai Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01040">
<title>Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI. (arXiv:2401.01040v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.01040</link>
<description rdf:parseType="Literal">&lt;p&gt;The remarkable advancements in artificial intelligence (AI), primarily driven
by deep neural networks, have significantly impacted various aspects of our
lives. However, the current challenges surrounding unsustainable computational
trajectories, limited robustness, and a lack of explainability call for the
development of next-generation AI systems. Neuro-symbolic AI (NSAI) emerges as
a promising paradigm, fusing neural, symbolic, and probabilistic approaches to
enhance interpretability, robustness, and trustworthiness while facilitating
learning from much less data. Recent NSAI systems have demonstrated great
potential in collaborative human-AI scenarios with reasoning and cognitive
capabilities. In this paper, we provide a systematic review of recent progress
in NSAI and analyze the performance characteristics and computational operators
of NSAI models. Furthermore, we discuss the challenges and potential future
directions of NSAI from both system and architectural perspectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1&quot;&gt;Zishen Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Che-Kai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hanchen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chaojian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1&quot;&gt;Haoran You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yonggan Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1&quot;&gt;Cheng Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_T/0/1/0/all/0/1&quot;&gt;Tushar Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yingyan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raychowdhury_A/0/1/0/all/0/1&quot;&gt;Arijit Raychowdhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01044">
<title>Auffusion: Leveraging the Power of Diffusion and Large Language Models for Text-to-Audio Generation. (arXiv:2401.01044v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.01044</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in diffusion models and large language models (LLMs) have
significantly propelled the field of AIGC. Text-to-Audio (TTA), a burgeoning
AIGC application designed to generate audio from natural language prompts, is
attracting increasing attention. However, existing TTA studies often struggle
with generation quality and text-audio alignment, especially for complex
textual inputs. Drawing inspiration from state-of-the-art Text-to-Image (T2I)
diffusion models, we introduce Auffusion, a TTA system adapting T2I model
frameworks to TTA task, by effectively leveraging their inherent generative
strengths and precise cross-modal alignment. Our objective and subjective
evaluations demonstrate that Auffusion surpasses previous TTA approaches using
limited data and computational resource. Furthermore, previous studies in T2I
recognizes the significant impact of encoder choice on cross-modal alignment,
like fine-grained details and object bindings, while similar evaluation is
lacking in prior TTA works. Through comprehensive ablation studies and
innovative cross-attention map visualizations, we provide insightful
assessments of text-audio alignment in TTA. Our findings reveal Auffusion&apos;s
superior capability in generating audios that accurately match textual
descriptions, which further demonstrated in several related tasks, such as
audio style transfer, inpainting and other manipulations. Our implementation
and demos are available at https://auffusion.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1&quot;&gt;Jinlong Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yayue Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yingming Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Ya Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01054">
<title>Elastic Multi-Gradient Descent for Parallel Continual Learning. (arXiv:2401.01054v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.01054</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of Continual Learning (CL) is to continuously learn from new data
streams and accomplish the corresponding tasks. Previously studied CL assumes
that data are given in sequence nose-to-tail for different tasks, thus indeed
belonging to Serial Continual Learning (SCL). This paper studies the novel
paradigm of Parallel Continual Learning (PCL) in dynamic multi-task scenarios,
where a diverse set of tasks is encountered at different time points. PCL
presents challenges due to the training of an unspecified number of tasks with
varying learning progress, leading to the difficulty of guaranteeing effective
model updates for all encountered tasks. In our previous conference work, we
focused on measuring and reducing the discrepancy among gradients in a
multi-objective optimization problem, which, however, may still contain
negative transfers in every model update. To address this issue, in the dynamic
multi-objective optimization problem, we introduce task-specific elastic
factors to adjust the descent direction towards the Pareto front. The proposed
method, called Elastic Multi-Gradient Descent (EMGD), ensures that each update
follows an appropriate Pareto descent direction, minimizing any negative impact
on previously learned tasks. To balance the training between old and new tasks,
we also propose a memory editing mechanism guided by the gradient computed
using EMGD. This editing process updates the stored data points, reducing
interference in the Pareto descent direction from previous tasks. Experiments
on public datasets validate the effectiveness of our EMGD in the PCL setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_F/0/1/0/all/0/1&quot;&gt;Fan Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1&quot;&gt;Wei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuepan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Qing Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1&quot;&gt;Fanhua Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_L/0/1/0/all/0/1&quot;&gt;Liang Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01055">
<title>LLaMA Beyond English: An Empirical Study on Language Capability Transfer. (arXiv:2401.01055v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.01055</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent times, substantial advancements have been witnessed in large
language models (LLMs), exemplified by ChatGPT, showcasing remarkable
proficiency across a range of complex tasks. However, many mainstream LLMs
(e.g. LLaMA) are pretrained on English-dominant corpus, which limits their
performance in other non-English languages. In this paper, we focus on how to
effectively transfer the capabilities of language generation and following
instructions to a non-English language. To answer this question, we conduct an
extensive empirical investigation based on LLaMA, accumulating over 1440 GPU
hours. We analyze the impact of key factors such as vocabulary extension,
further pretraining, and instruction tuning on transfer. To accurately assess
the model&apos;s level of knowledge, we employ four widely used standardized testing
benchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a
comprehensive evaluation of the model&apos;s response quality is conducted,
considering aspects such as accuracy, fluency, informativeness, logical
coherence, and harmlessness, based on LLM-Eval, a benchmarks consisting
instruction tasks from 17 diverse categories. Our evaluation results
demonstrate that comparable performance to state-of-the-art transfer models can
be achieved with less than 1% of the pretraining data, both in terms of
knowledge alignment and response quality. Furthermore, the experimental
outcomes across the thirteen low-resource languages also exhibit similar
trends. We anticipate that the conclusions revealed by the experiments will aid
the community in developing non-English LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhihao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1&quot;&gt;Tao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01056">
<title>Enhancing Automatic Modulation Recognition through Robust Global Feature Extraction. (arXiv:2401.01056v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2401.01056</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic Modulation Recognition (AMR) plays a crucial role in wireless
communication systems. Deep learning AMR strategies have achieved tremendous
success in recent years. Modulated signals exhibit long temporal dependencies,
and extracting global features is crucial in identifying modulation schemes.
Traditionally, human experts analyze patterns in constellation diagrams to
classify modulation schemes. Classical convolutional-based networks, due to
their limited receptive fields, excel at extracting local features but struggle
to capture global relationships. To address this limitation, we introduce a
novel hybrid deep framework named TLDNN, which incorporates the architectures
of the transformer and long short-term memory (LSTM). We utilize the
self-attention mechanism of the transformer to model the global correlations in
signal sequences while employing LSTM to enhance the capture of temporal
dependencies. To mitigate the impact like RF fingerprint features and channel
characteristics on model generalization, we propose data augmentation
strategies known as segment substitution (SS) to enhance the model&apos;s robustness
to modulation-related features. Experimental results on widely-used datasets
demonstrate that our method achieves state-of-the-art performance and exhibits
significant advantages in terms of complexity. Our proposed framework serves as
a foundational backbone that can be extended to different datasets. We have
verified the effectiveness of our augmentation approach in enhancing the
generalization of the models, particularly in few-shot scenarios. Code is
available at \url{https://github.com/AMR-Master/TLDNN}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qu_Y/0/1/0/all/0/1&quot;&gt;Yunpeng Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhilin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zeng_R/0/1/0/all/0/1&quot;&gt;Rui Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jintao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jian Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01065">
<title>BEV-CLIP: Multi-modal BEV Retrieval Methodology for Complex Scene in Autonomous Driving. (arXiv:2401.01065v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01065</link>
<description rdf:parseType="Literal">&lt;p&gt;The demand for the retrieval of complex scene data in autonomous driving is
increasing, especially as passenger vehicles have been equipped with the
ability to navigate urban settings, with the imperative to address long-tail
scenarios. Meanwhile, under the pre-existing two dimensional image retrieval
method, some problems may arise with scene retrieval, such as lack of global
feature representation and subpar text retrieval ability. To address these
issues, we have proposed \textbf{BEV-CLIP}, the first multimodal Bird&apos;s-Eye
View(BEV) retrieval methodology that utilizes descriptive text as an input to
retrieve corresponding scenes. This methodology applies the semantic feature
extraction abilities of a large language model (LLM) to facilitate zero-shot
retrieval of extensive text descriptions, and incorporates semi-structured
information from a knowledge graph to improve the semantic richness and variety
of the language embedding. Our experiments result in 87.66% accuracy on
NuScenes dataset in text-to-BEV feature retrieval. The demonstrated cases in
our paper support that our retrieval method is also indicated to be effective
in identifying certain long-tail corner scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1&quot;&gt;Dafeng Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1&quot;&gt;Tian Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1&quot;&gt;Zhengyu Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1&quot;&gt;Changwei Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_C/0/1/0/all/0/1&quot;&gt;Chengkai Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_P/0/1/0/all/0/1&quot;&gt;Peng Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_K/0/1/0/all/0/1&quot;&gt;Kun Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jingchen Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yixing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01068">
<title>Discovering Significant Topics from Legal Decisions with Selective Inference. (arXiv:2401.01068v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.01068</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose and evaluate an automated pipeline for discovering significant
topics from legal decision texts by passing features synthesized with topic
models through penalised regressions and post-selection significance tests. The
method identifies case topics significantly correlated with outcomes,
topic-word distributions which can be manually-interpreted to gain insights
about significant topics, and case-topic weights which can be used to identify
representative cases for each topic. We demonstrate the method on a new dataset
of domain name disputes and a canonical dataset of European Court of Human
Rights violation cases. Topic models based on latent semantic analysis as well
as language model embeddings are evaluated. We show that topics derived by the
pipeline are consistent with legal doctrines in both areas and can be useful in
other related legal analysis tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soh_J/0/1/0/all/0/1&quot;&gt;Jerrold Soh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01078">
<title>Vietnamese Poem Generation &amp; The Prospect Of Cross-Language Poem-To-Poem Translation. (arXiv:2401.01078v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.01078</link>
<description rdf:parseType="Literal">&lt;p&gt;Poetry generation has been a challenging task in the field of Natural
Language Processing, as it requires the model to understand the nuances of
language, sentiment, and style. In this paper, we propose using Large Language
Models to generate Vietnamese poems from natural language prompts, thereby
facilitating an intuitive process with enhanced content control. Our most
efficacious model, the GPT-3 Babbage variant, achieves a custom evaluation
score of 0.8, specifically tailored to the &quot;luc bat&quot; genre of Vietnamese
poetry. Furthermore, we also explore the idea of paraphrasing poems into normal
text prompts and yield a relatively high score of 0.718 in the &quot;luc bat&quot; genre.
This experiment presents the potential for cross-Language poem-to-poem
translation with translated poems as the inputs while concurrently maintaining
complete control over the generated content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minh_T/0/1/0/all/0/1&quot;&gt;Triet Huynh Minh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Q/0/1/0/all/0/1&quot;&gt;Quan Le Bao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01089">
<title>Quokka: An Open-source Large Language Model ChatBot for Material Science. (arXiv:2401.01089v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.01089</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents the development of a specialized chatbot for materials
science, leveraging the Llama-2 language model, and continuing pre-training on
the expansive research articles in the materials science domain from the S2ORC
dataset. The methodology involves an initial pretraining phase on over one
million domain-specific papers, followed by an instruction-tuning process to
refine the chatbot&apos;s capabilities. The chatbot is designed to assist
researchers, educators, and students by providing instant, context-aware
responses to queries in the field of materials science. We make the four
trained checkpoints (7B, 13B, with or without chat ability) freely available to
the research community at https://github.com/Xianjun-Yang/Quokka.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xianjun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1&quot;&gt;Stephen D. Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petzold_L/0/1/0/all/0/1&quot;&gt;Linda Petzold&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01099">
<title>Efficient Parallel Audio Generation using Group Masked Language Modeling. (arXiv:2401.01099v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2401.01099</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a fast and high-quality codec language model for parallel audio
generation. While SoundStorm, a state-of-the-art parallel audio generation
model, accelerates inference speed compared to autoregressive models, it still
suffers from slow inference due to iterative sampling. To resolve this problem,
we propose Group-Masked Language Modeling~(G-MLM) and Group Iterative Parallel
Decoding~(G-IPD) for efficient parallel audio generation. Both the training and
sampling schemes enable the model to synthesize high-quality audio with a small
number of iterations by effectively modeling the group-wise conditional
dependencies. In addition, our model employs a cross-attention-based
architecture to capture the speaker style of the prompt voice and improves
computational efficiency. Experimental results demonstrate that our proposed
model outperforms the baselines in prompt-based audio generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jeong_M/0/1/0/all/0/1&quot;&gt;Myeonghun Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minchan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Joun Yeop Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_N/0/1/0/all/0/1&quot;&gt;Nam Soo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01104">
<title>AI-FLARES: Artificial Intelligence for the Analysis of Solar Flares Data. (arXiv:2401.01104v1 [astro-ph.SR])</title>
<link>http://arxiv.org/abs/2401.01104</link>
<description rdf:parseType="Literal">&lt;p&gt;AI-FLARES (Artificial Intelligence for the Analysis of Solar Flares Data) is
a research project funded by the Agenzia Spaziale Italiana and by the Istituto
Nazionale di Astrofisica within the framework of the ``Attivit\`a di Studio per
la Comunit\`a Scientifica Nazionale Sole, Sistema Solare ed Esopianeti&apos;&apos;
program. The topic addressed by this project was the development and use of
computational methods for the analysis of remote sensing space data associated
to solar flare emission. This paper overviews the main results obtained by the
project, with specific focus on solar flare forecasting, reconstruction of
morphologies of the flaring sources, and interpretation of acceleration
mechanisms triggered by solar flares.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Piana_M/0/1/0/all/0/1&quot;&gt;Michele Piana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Benvenuto_F/0/1/0/all/0/1&quot;&gt;Federico Benvenuto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Massone_A/0/1/0/all/0/1&quot;&gt;Anna Maria Massone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Campi_C/0/1/0/all/0/1&quot;&gt;Cristina Campi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Guastavino_S/0/1/0/all/0/1&quot;&gt;Sabrina Guastavino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Marchetti_F/0/1/0/all/0/1&quot;&gt;Francesco Marchetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Massa_P/0/1/0/all/0/1&quot;&gt;Paolo Massa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Perracchione_E/0/1/0/all/0/1&quot;&gt;Emma Perracchione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Volpara_A/0/1/0/all/0/1&quot;&gt;Anna Volpara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01119">
<title>Utilizing Autoregressive Networks for Full Lifecycle Data Generation of Rolling Bearings for RUL Prediction. (arXiv:2401.01119v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.01119</link>
<description rdf:parseType="Literal">&lt;p&gt;The prediction of rolling bearing lifespan is of significant importance in
industrial production. However, the scarcity of high-quality, full lifecycle
data has been a major constraint in achieving precise predictions. To address
this challenge, this paper introduces the CVGAN model, a novel framework
capable of generating one-dimensional vibration signals in both horizontal and
vertical directions, conditioned on historical vibration data and remaining
useful life. In addition, we propose an autoregressive generation method that
can iteratively utilize previously generated vibration information to guide the
generation of current signals. The effectiveness of the CVGAN model is
validated through experiments conducted on the PHM 2012 dataset. Our findings
demonstrate that the CVGAN model, in terms of both MMD and FID metrics,
outperforms many advanced methods in both autoregressive and non-autoregressive
generation modes. Notably, training using the full lifecycle data generated by
the CVGAN model significantly improves the performance of the predictive model.
This result highlights the effectiveness of the data generated by CVGans in
enhancing the predictive power of these models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junliang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qinghua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1&quot;&gt;Guanhua Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1&quot;&gt;Guoxi Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01124">
<title>Explainable Adaptive Tree-based Model Selection for Time Series Forecasting. (arXiv:2401.01124v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.01124</link>
<description rdf:parseType="Literal">&lt;p&gt;Tree-based models have been successfully applied to a wide variety of tasks,
including time series forecasting. They are increasingly in demand and widely
accepted because of their comparatively high level of interpretability.
However, many of them suffer from the overfitting problem, which limits their
application in real-world decision-making. This problem becomes even more
severe in online-forecasting settings where time series observations are
incrementally acquired, and the distributions from which they are drawn may
keep changing over time. In this context, we propose a novel method for the
online selection of tree-based models using the TreeSHAP explainability method
in the task of time series forecasting. We start with an arbitrary set of
different tree-based models. Then, we outline a performance-based ranking with
a coherent design to make TreeSHAP able to specialize the tree-based
forecasters across different regions in the input time series. In this
framework, adequate model selection is performed online, adaptively following
drift detection in the time series. In addition, explainability is supported on
three levels, namely online input importance, model selection, and model output
explanation. An extensive empirical study on various real-world datasets
demonstrates that our method achieves excellent or on-par results in comparison
to the state-of-the-art approaches as well as several baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jakobs_M/0/1/0/all/0/1&quot;&gt;Matthias Jakobs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saadallah_A/0/1/0/all/0/1&quot;&gt;Amal Saadallah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01141">
<title>Spiker+: a framework for the generation of efficient Spiking Neural Networks FPGA accelerators for inference at the edge. (arXiv:2401.01141v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2401.01141</link>
<description rdf:parseType="Literal">&lt;p&gt;Including Artificial Neural Networks in embedded systems at the edge allows
applications to exploit Artificial Intelligence capabilities directly within
devices operating at the network periphery. This paper introduces Spiker+, a
comprehensive framework for generating efficient, low-power, and low-area
customized Spiking Neural Networks (SNN) accelerators on FPGA for inference at
the edge. Spiker+ presents a configurable multi-layer hardware SNN, a library
of highly efficient neuron architectures, and a design framework, enabling the
development of complex neural network accelerators with few lines of Python
code. Spiker+ is tested on two benchmark datasets, the MNIST and the Spiking
Heidelberg Digits (SHD). On the MNIST, it demonstrates competitive performance
compared to state-of-the-art SNN accelerators. It outperforms them in terms of
resource allocation, with a requirement of 7,612 logic cells and 18 Block RAMs
(BRAMs), which makes it fit in very small FPGA, and power consumption, draining
only 180mW for a complete inference on an input image. The latency is
comparable to the ones observed in the state-of-the-art, with 780us/img. To the
authors&apos; knowledge, Spiker+ is the first SNN accelerator tested on the SHD. In
this case, the accelerator requires 18,268 logic cells and 51 BRAM, with an
overall power consumption of 430mW and a latency of 54 us for a complete
inference on input data. This underscores the significance of Spiker+ in the
hardware-accelerated SNN landscape, making it an excellent solution to deploy
configurable and tunable SNN architectures in resource and power-constrained
edge applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carpegna_A/0/1/0/all/0/1&quot;&gt;Alessio Carpegna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savino_A/0/1/0/all/0/1&quot;&gt;Alessandro Savino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carlo_S/0/1/0/all/0/1&quot;&gt;Stefano Di Carlo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01172">
<title>Quadratic Time-Frequency Analysis of Vibration Signals for Diagnosing Bearing Faults. (arXiv:2401.01172v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.01172</link>
<description rdf:parseType="Literal">&lt;p&gt;Diagnosis of bearing faults is paramount to reducing maintenance costs and
operational breakdowns. Bearing faults are primary contributors to machine
vibrations, and analyzing their signal morphology offers insights into their
health status. Unfortunately, existing approaches are optimized for controlled
environments, neglecting realistic conditions such as time-varying rotational
speeds and the vibration&apos;s non-stationary nature. This paper presents a fusion
of time-frequency analysis and deep learning techniques to diagnose bearing
faults under time-varying speeds and varying noise levels. First, we formulate
the bearing fault-induced vibrations and discuss the link between their
non-stationarity and the bearing&apos;s inherent and operational parameters. We also
elucidate quadratic time-frequency distributions and validate their
effectiveness in resolving distinctive dynamic patterns associated with
different bearing faults. Based on this, we design a time-frequency
convolutional neural network (TF-CNN) to diagnose various faults in
rolling-element bearings. Our experimental findings undeniably demonstrate the
superior performance of TF-CNN in comparison to recently developed techniques.
They also assert its versatility in capturing fault-relevant non-stationary
features that couple with speed changes and show its exceptional resilience to
noise, consistently surpassing competing methods across various signal-to-noise
ratios and performance metrics. Altogether, the TF-CNN achieves substantial
accuracy improvements up to 15%, in severe noise conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Sad_M/0/1/0/all/0/1&quot;&gt;Mohammad Al-Sa&amp;#x27;d&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jalonen_T/0/1/0/all/0/1&quot;&gt;Tuomas Jalonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiranyaz_S/0/1/0/all/0/1&quot;&gt;Serkan Kiranyaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1&quot;&gt;Moncef Gabbouj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01179">
<title>Freeze the backbones: A Parameter-Efficient Contrastive Approach to Robust Medical Vision-Language Pre-training. (arXiv:2401.01179v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01179</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern healthcare often utilises radiographic images alongside textual
reports for diagnostics, encouraging the use of Vision-Language Self-Supervised
Learning (VL-SSL) with large pre-trained models to learn versatile medical
vision representations. However, most existing VL-SSL frameworks are trained
end-to-end, which is computation-heavy and can lose vital prior information
embedded in pre-trained encoders. To address both issues, we introduce the
backbone-agnostic Adaptor framework, which preserves medical knowledge in
pre-trained image and text encoders by keeping them frozen, and employs a
lightweight Adaptor module for cross-modal learning. Experiments on medical
image classification and segmentation tasks across three datasets reveal that
our framework delivers competitive performance while cutting trainable
parameters by over 90% compared to current pre-training approaches. Notably,
when fine-tuned with just 1% of data, Adaptor outperforms several
Transformer-based methods trained on full datasets in medical image
segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1&quot;&gt;Jiuming Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Che Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Sibo Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yike Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arcucci_R/0/1/0/all/0/1&quot;&gt;Rossella Arcucci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01180">
<title>Accurate and Efficient Urban Street Tree Inventory with Deep Learning on Mobile Phone Imagery. (arXiv:2401.01180v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01180</link>
<description rdf:parseType="Literal">&lt;p&gt;Deforestation, a major contributor to climate change, poses detrimental
consequences such as agricultural sector disruption, global warming, flash
floods, and landslides. Conventional approaches to urban street tree inventory
suffer from inaccuracies and necessitate specialised equipment. To overcome
these challenges, this paper proposes an innovative method that leverages deep
learning techniques and mobile phone imaging for urban street tree inventory.
Our approach utilises a pair of images captured by smartphone cameras to
accurately segment tree trunks and compute the diameter at breast height (DBH).
Compared to traditional methods, our approach exhibits several advantages,
including superior accuracy, reduced dependency on specialised equipment, and
applicability in hard-to-reach areas. We evaluated our method on a
comprehensive dataset of 400 trees and achieved a DBH estimation accuracy with
an error rate of less than 2.5%. Our method holds significant potential for
substantially improving forest management practices. By enhancing the accuracy
and efficiency of tree inventory, our model empowers urban management to
mitigate the adverse effects of deforestation and climate change.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Asim Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nawaz_U/0/1/0/all/0/1&quot;&gt;Umair Nawaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulhaq_A/0/1/0/all/0/1&quot;&gt;Anwaar Ulhaq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gondal_I/0/1/0/all/0/1&quot;&gt;Iqbal Gondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1&quot;&gt;Sajid Javed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01183">
<title>Unifying Structured Data as Graph for Data-to-Text Pre-Training. (arXiv:2401.01183v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.01183</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-to-text (D2T) generation aims to transform structured data into natural
language text. Data-to-text pre-training has proved to be powerful in enhancing
D2T generation and yields impressive performances. However, previous
pre-training methods either oversimplified structured data into a sequence
without considering input structures or designed training objectives tailored
for a specific data structure (e.g., table or knowledge graph). In this paper,
we unify different types of structured data (i.e., table, key-value data,
knowledge graph) into the graph format and cast different data-to-text
generation tasks as graph-to-text generation. To effectively exploit the
structural information of the input graph, we propose a structure-enhanced
pre-training method for D2T generation by designing a structure-enhanced
Transformer. Concretely, we devise a position matrix for the Transformer,
encoding relative positional information of connected nodes in the input graph.
In addition, we propose a new attention matrix to incorporate graph structures
into the original Transformer by taking the available explicit connectivity
structure into account. Extensive experiments on six benchmark datasets show
the effectiveness of our model. Our source codes are available at
https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/unid2t.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shujie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_R/0/1/0/all/0/1&quot;&gt;Ruiying Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Min Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Binhua Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1&quot;&gt;Guanghu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Wanwei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1&quot;&gt;Shao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Can Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yongbin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01189">
<title>NID-SLAM: Neural Implicit Representation-based RGB-D SLAM in dynamic environments. (arXiv:2401.01189v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.01189</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural implicit representations have been explored to enhance visual SLAM
algorithms, especially in providing high-fidelity dense map. Existing methods
operate robustly in static scenes but struggle with the disruption caused by
moving objects. In this paper we present NID-SLAM, which significantly improves
the performance of neural SLAM in dynamic environments. We propose a new
approach to enhance inaccurate regions in semantic masks, particularly in
marginal areas. Utilizing the geometric information present in depth images,
this method enables accurate removal of dynamic objects, thereby reducing the
probability of camera drift. Additionally, we introduce a keyframe selection
strategy for dynamic scenes, which enhances camera tracking robustness against
large-scale objects and improves the efficiency of mapping. Experiments on
publicly available RGB-D datasets demonstrate that our method outperforms
competitive neural SLAM approaches in tracking accuracy and mapping quality in
dynamic environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Ziheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1&quot;&gt;Jianwei Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qingfeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1&quot;&gt;Tao Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01197">
<title>Uncertainty Resolution in Misinformation Detection. (arXiv:2401.01197v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.01197</link>
<description rdf:parseType="Literal">&lt;p&gt;Misinformation poses a variety of risks, such as undermining public trust and
distorting factual discourse. Large Language Models (LLMs) like GPT-4 have been
shown effective in mitigating misinformation, particularly in handling
statements where enough context is provided. However, they struggle to assess
ambiguous or context-deficient statements accurately. This work introduces a
new method to resolve uncertainty in such statements. We propose a framework to
categorize missing information and publish category labels for the LIAR-New
dataset, which is adaptable to cross-domain content with missing information.
We then leverage this framework to generate effective user queries for missing
context. Compared to baselines, our method improves the rate at which generated
questions are answerable by the user by 38 percentage points and classification
performance by over 10 percentage points macro F1. Thus, this approach may
provide a valuable component for future misinformation mitigation pipelines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orlovskiy_Y/0/1/0/all/0/1&quot;&gt;Yury Orlovskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thibault_C/0/1/0/all/0/1&quot;&gt;Camille Thibault&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imouza_A/0/1/0/all/0/1&quot;&gt;Anne Imouza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Godbout_J/0/1/0/all/0/1&quot;&gt;Jean-Fran&amp;#xe7;ois Godbout&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabbany_R/0/1/0/all/0/1&quot;&gt;Reihaneh Rabbany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelrine_K/0/1/0/all/0/1&quot;&gt;Kellin Pelrine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01199">
<title>JMA: a General Algorithm to Craft Nearly Optimal Targeted Adversarial Example. (arXiv:2401.01199v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.01199</link>
<description rdf:parseType="Literal">&lt;p&gt;Most of the approaches proposed so far to craft targeted adversarial examples
against Deep Learning classifiers are highly suboptimal and typically rely on
increasing the likelihood of the target class, thus implicitly focusing on
one-hot encoding settings. In this paper, we propose a more general,
theoretically sound, targeted attack that resorts to the minimization of a
Jacobian-induced MAhalanobis distance (JMA) term, taking into account the
effort (in the input space) required to move the latent space representation of
the input sample in a given direction. The minimization is solved by exploiting
the Wolfe duality theorem, reducing the problem to the solution of a
Non-Negative Least Square (NNLS) problem. The proposed algorithm provides an
optimal solution to a linearized version of the adversarial example problem
originally introduced by Szegedy et al. \cite{szegedy2013intriguing}. The
experiments we carried out confirm the generality of the proposed attack which
is proven to be effective under a wide variety of output encoding schemes.
Noticeably, the JMA attack is also effective in a multi-label classification
scenario, being capable to induce a targeted modification of up to half the
labels in a complex multilabel classification scenario with 20 labels, a
capability that is out of reach of all the attacks proposed so far. As a
further advantage, the JMA attack usually requires very few iterations, thus
resulting more efficient than existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tondi_B/0/1/0/all/0/1&quot;&gt;Benedetta Tondi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1&quot;&gt;Wei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barni_M/0/1/0/all/0/1&quot;&gt;Mauro Barni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01200">
<title>Skin cancer diagnosis using NIR spectroscopy data of skin lesions in vivo using machine learning algorithms. (arXiv:2401.01200v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01200</link>
<description rdf:parseType="Literal">&lt;p&gt;Skin lesions are classified in benign or malignant. Among the malignant,
melanoma is a very aggressive cancer and the major cause of deaths. So, early
diagnosis of skin cancer is very desired. In the last few years, there is a
growing interest in computer aided diagnostic (CAD) using most image and
clinical data of the lesion. These sources of information present limitations
due to their inability to provide information of the molecular structure of the
lesion. NIR spectroscopy may provide an alternative source of information to
automated CAD of skin lesions. The most commonly used techniques and
classification algorithms used in spectroscopy are Principal Component Analysis
(PCA), Partial Least Squares - Discriminant Analysis (PLS-DA), and Support
Vector Machines (SVM). Nonetheless, there is a growing interest in applying the
modern techniques of machine and deep learning (MDL) to spectroscopy. One of
the main limitations to apply MDL to spectroscopy is the lack of public
datasets. Since there is no public dataset of NIR spectral data to skin
lesions, as far as we know, an effort has been made and a new dataset named
NIR-SC-UFES, has been collected, annotated and analyzed generating the
gold-standard for classification of NIR spectral data to skin cancer. Next, the
machine learning algorithms XGBoost, CatBoost, LightGBM, 1D-convolutional
neural network (1D-CNN) were investigated to classify cancer and non-cancer
skin lesions. Experimental results indicate the best performance obtained by
LightGBM with pre-processing using standard normal variate (SNV), feature
extraction providing values of 0.839 for balanced accuracy, 0.851 for recall,
0.852 for precision, and 0.850 for F-score. The obtained results indicate the
first steps in CAD of skin lesions aiming the automated triage of patients with
skin lesions in vivo using NIR spectral data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loss_F/0/1/0/all/0/1&quot;&gt;Flavio P. Loss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cunha_P/0/1/0/all/0/1&quot;&gt;Pedro H. da Cunha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rocha_M/0/1/0/all/0/1&quot;&gt;Matheus B. Rocha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zanoni_M/0/1/0/all/0/1&quot;&gt;Madson Poltronieri Zanoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lima_L/0/1/0/all/0/1&quot;&gt;Leandro M. de Lima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nascimento_I/0/1/0/all/0/1&quot;&gt;Isadora Tavares Nascimento&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezende_I/0/1/0/all/0/1&quot;&gt;Isabella Rezende&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canuto_T/0/1/0/all/0/1&quot;&gt;Tania R. P. Canuto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vieira_L/0/1/0/all/0/1&quot;&gt;Luciana de Paula Vieira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossoni_R/0/1/0/all/0/1&quot;&gt;Renan Rossoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1&quot;&gt;Maria C. S. Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frasson_P/0/1/0/all/0/1&quot;&gt;Patricia Lyra Frasson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romao_W/0/1/0/all/0/1&quot;&gt;Wanderson Rom&amp;#xe3;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filgueiras_P/0/1/0/all/0/1&quot;&gt;Paulo R. Filgueiras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krohling_R/0/1/0/all/0/1&quot;&gt;Renato A. Krohling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01204">
<title>PPBFL: A Privacy Protected Blockchain-based Federated Learning Model. (arXiv:2401.01204v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.01204</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of machine learning and growing concerns about
data privacy, federated learning has become an increasingly prominent focus.
However, challenges such as attacks on model parameters and the lack of
incentive mechanisms hinder the effectiveness of federated learning. Therefore,
we propose a Privacy Protected Blockchain-based Federated Learning Model
(PPBFL) to enhance the security of federated learning and promote the active
participation of nodes in model training. Blockchain ensures that model
parameters stored in the InterPlanetary File System (IPFS) remain unaltered. A
novel adaptive differential privacy addition algorithm is simultaneously
applied to local and global models, preserving the privacy of local models and
preventing a decrease in the security of the global model due to the presence
of numerous local models in federated learning. Additionally, we introduce a
new mix transactions mechanism to better protect the identity privacy of local
training clients. Security analysis and experimental results demonstrate that
PPBFL outperforms baseline methods in both model performance and security.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1&quot;&gt;Chunhe Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wanshuang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianbo Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01218">
<title>Zero-Shot Position Debiasing for Large Language Models. (arXiv:2401.01218v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.01218</link>
<description rdf:parseType="Literal">&lt;p&gt;Fine-tuning has been demonstrated to be an effective method to improve the
domain performance of large language models (LLMs). However, LLMs might fit the
dataset bias and shortcuts for prediction, leading to poor generation
performance. Experimental result shows that LLMs are prone to exhibit position
bias, i.e., leveraging information positioned at the beginning or end, or
specific positional cues within the input. Existing works on mitigating
position bias require external bias knowledge or annotated non-biased samples,
which is unpractical in reality. In this work, we propose a zero-shot position
debiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages
unsupervised responses from pre-trained LLMs for debiasing, thus without any
external knowledge or datasets. To improve the quality of unsupervised
responses, we propose a master-slave alignment (MSA) module to prune these
responses. Experiments on eight datasets and five tasks show that ZOE
consistently outperforms existing methods in mitigating four types of position
biases. Besides, ZOE achieves this by sacrificing only a small performance on
biased samples, which is simple and effective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhongkun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhaochun Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhumin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1&quot;&gt;Pengjie Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01227">
<title>IdentiFace : A VGG Based Multimodal Facial Biometric System. (arXiv:2401.01227v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.01227</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of facial biometric systems has contributed greatly to the
development of the computer vision field. Nowadays, there&apos;s always a need to
develop a multimodal system that combines multiple biometric traits in an
efficient, meaningful way. In this paper, we introduce &quot;IdentiFace&quot; which is a
multimodal facial biometric system that combines the core of facial recognition
with some of the most important soft biometric traits such as gender, face
shape, and emotion. We also focused on developing the system using only VGG-16
inspired architecture with minor changes across different subsystems. This
unification allows for simpler integration across modalities. It makes it
easier to interpret the learned features between the tasks which gives a good
indication about the decision-making process across the facial modalities and
potential connection. For the recognition problem, we acquired a 99.2% test
accuracy for five classes with high intra-class variations using data collected
from the FERET database[1]. We achieved 99.4% on our dataset and 95.15% on the
public dataset[2] in the gender recognition problem. We were also able to
achieve a testing accuracy of 88.03% in the face-shape problem using the
celebrity face-shape dataset[3]. Finally, we achieved a decent testing accuracy
of 66.13% in the emotion task which is considered a very acceptable accuracy
compared to related work on the FER2013 dataset[4].
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabea_M/0/1/0/all/0/1&quot;&gt;Mahmoud Rabea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_H/0/1/0/all/0/1&quot;&gt;Hanya Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmoud_S/0/1/0/all/0/1&quot;&gt;Sohaila Mahmoud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sayed_N/0/1/0/all/0/1&quot;&gt;Nourhan Sayed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01242">
<title>Encoding Binary Events from Continuous Time Series in Rooted Trees using Contrastive Learning. (arXiv:2401.01242v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.01242</link>
<description rdf:parseType="Literal">&lt;p&gt;Broadband infrastructure owners do not always know how their customers are
connected in the local networks, which are structured as rooted trees. A recent
study is able to infer the topology of a local network using discrete time
series data from the leaves of the tree (customers). In this study we propose a
contrastive approach for learning a binary event encoder from continuous time
series data. As a preliminary result, we show that our approach has some
potential in learning a valuable encoder.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasmussen_T/0/1/0/all/0/1&quot;&gt;Tobias Engelhardt Rasmussen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sorensen_S/0/1/0/all/0/1&quot;&gt;Siv S&amp;#xf8;rensen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01259">
<title>Do Concept Bottleneck Models Obey Locality?. (arXiv:2401.01259v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.01259</link>
<description rdf:parseType="Literal">&lt;p&gt;Concept-based learning improves a deep learning model&apos;s interpretability by
explaining its predictions via human-understandable concepts. Deep learning
models trained under this paradigm heavily rely on the assumption that neural
networks can learn to predict the presence or absence of a given concept
independently of other concepts. Recent work, however, strongly suggests that
this assumption may fail to hold in Concept Bottleneck Models (CBMs), a
quintessential family of concept-based interpretable architectures. In this
paper, we investigate whether CBMs correctly capture the degree of conditional
independence across concepts when such concepts are localised both spatially,
by having their values entirely defined by a fixed subset of features, and
semantically, by having their values correlated with only a fixed subset of
predefined concepts. To understand locality, we analyse how changes to features
outside of a concept&apos;s spatial or semantic locality impact concept predictions.
Our results suggest that even in well-defined scenarios where the presence of a
concept is localised to a fixed feature subspace, or whose semantics are
correlated to a small subset of other concepts, CBMs fail to learn this
locality. These results cast doubt upon the quality of concept representations
learnt by CBMs and strongly suggest that concept-based explanations may be
fragile to changes outside their localities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raman_N/0/1/0/all/0/1&quot;&gt;Naveen Raman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zarlenga_M/0/1/0/all/0/1&quot;&gt;Mateo Espinosa Zarlenga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1&quot;&gt;Juyeon Heo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamnik_M/0/1/0/all/0/1&quot;&gt;Mateja Jamnik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01262">
<title>Fairness Certification for Natural Language Processing and Large Language Models. (arXiv:2401.01262v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.01262</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural Language Processing (NLP) plays an important role in our daily lives,
particularly due to the enormous progress of Large Language Models (LLM).
However, NLP has many fairness-critical use cases, e.g., as an expert system in
recruitment or as an LLM-based tutor in education. Since NLP is based on human
language, potentially harmful biases can diffuse into NLP systems and produce
unfair results, discriminate against minorities or generate legal issues.
Hence, it is important to develop a fairness certification for NLP approaches.
We follow a qualitative research approach towards a fairness certification for
NLP. In particular, we have reviewed a large body of literature on algorithmic
fairness, and we have conducted semi-structured expert interviews with a wide
range of experts from that area. We have systematically devised six fairness
criteria for NLP, which can be further refined into 18 sub-categories. Our
criteria offer a foundation for operationalizing and testing processes to
certify fairness, both from the perspective of the auditor and the audited
organization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freiberger_V/0/1/0/all/0/1&quot;&gt;Vincent Freiberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buchmann_E/0/1/0/all/0/1&quot;&gt;Erik Buchmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01265">
<title>Optimal Synthesis of Finite State Machines with Universal Gates using Evolutionary Algorithm. (arXiv:2401.01265v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2401.01265</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents an optimization method for the synthesis of finite state
machines. The focus is on the reduction in the on-chip area and the cost of the
circuit. A list of finite state machines from MCNC91 benchmark circuits have
been evolved using Cartesian Genetic Programming. On the average, almost 30% of
reduction in the total number of gates has been achieved. The effects of some
parameters on the evolutionary process have also been discussed in the paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ullah_N/0/1/0/all/0/1&quot;&gt;Noor Ullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yahya_K/0/1/0/all/0/1&quot;&gt;Khawaja M. Yahya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_I/0/1/0/all/0/1&quot;&gt;Irfan Ahmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01269">
<title>LLbezpeky: Leveraging Large Language Models for Vulnerability Detection. (arXiv:2401.01269v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.01269</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the continued research and progress in building secure systems,
Android applications continue to be ridden with vulnerabilities, necessitating
effective detection methods. Current strategies involving static and dynamic
analysis tools come with limitations like overwhelming number of false
positives and limited scope of analysis which make either difficult to adopt.
Over the past years, machine learning based approaches have been extensively
explored for vulnerability detection, but its real-world applicability is
constrained by data requirements and feature engineering challenges. Large
Language Models (LLMs), with their vast parameters, have shown tremendous
potential in understanding semnatics in human as well as programming languages.
We dive into the efficacy of LLMs for detecting vulnerabilities in the context
of Android security. We focus on building an AI-driven workflow to assist
developers in identifying and rectifying vulnerabilities. Our experiments show
that LLMs outperform our expectations in finding issues within applications
correctly flagging insecure apps in 91.67% of cases in the Ghera benchmark. We
use inferences from our experiments towards building a robust and actionable
vulnerability detection system and demonstrate its effectiveness. Our
experiments also shed light on how different various simple configurations can
affect the True Positive (TP) and False Positive (FP) rates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathews_N/0/1/0/all/0/1&quot;&gt;Noble Saji Mathews&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brus_Y/0/1/0/all/0/1&quot;&gt;Yelizaveta Brus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aafer_Y/0/1/0/all/0/1&quot;&gt;Yousra Aafer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagappan_M/0/1/0/all/0/1&quot;&gt;Mei Nagappan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McIntosh_S/0/1/0/all/0/1&quot;&gt;Shane McIntosh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01286">
<title>A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.01286</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown extraordinary capabilities in
understanding and generating text that closely mirrors human communication.
However, a primary limitation lies in the significant computational demands
during training, arising from their extensive parameterization. This challenge
is further intensified by the dynamic nature of the world, necessitating
frequent updates to LLMs to correct outdated information or integrate new
knowledge, thereby ensuring their continued relevance. Note that many
applications demand continual model adjustments post-training to address
deficiencies or undesirable behaviors. There is an increasing interest in
efficient, lightweight methods for on-the-fly model modifications. To this end,
recent years have seen a burgeoning in the techniques of knowledge editing for
LLMs, which aim to efficiently modify LLMs&apos; behaviors within specific domains
while preserving overall performance across various inputs. In this paper, we
first define the knowledge editing problem and then provide a comprehensive
review of cutting-edge approaches. Drawing inspiration from educational and
cognitive research theories, we propose a unified categorization criterion that
classifies knowledge editing methods into three groups: resorting to external
knowledge, merging knowledge into the model, and editing intrinsic knowledge.
Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive
empirical evaluation of representative knowledge editing approaches.
Additionally, we provide an in-depth analysis of knowledge location, which can
provide a deeper understanding of the knowledge structures inherent within
LLMs. Finally, we discuss several potential applications of knowledge editing,
outlining its broad and impactful implications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yunzhi Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1&quot;&gt;Bozhong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Shumin Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengru Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1&quot;&gt;Zekun Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1&quot;&gt;Shengyu Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jintian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1&quot;&gt;Yuansheng Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Siyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Ziwen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jia-Chen Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1&quot;&gt;Pengjun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Lei Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaowei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01288">
<title>Physics-informed Generalizable Wireless Channel Modeling with Segmentation and Deep Learning: Fundamentals, Methodologies, and Challenges. (arXiv:2401.01288v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2401.01288</link>
<description rdf:parseType="Literal">&lt;p&gt;Channel modeling is fundamental in advancing wireless systems and has thus
attracted considerable research focus. Recent trends have seen a growing
reliance on data-driven techniques to facilitate the modeling process and yield
accurate channel predictions. In this work, we first provide a concise overview
of data-driven channel modeling methods, highlighting their limitations.
Subsequently, we introduce the concept and advantages of physics-informed
neural network (PINN)-based modeling and a summary of recent contributions in
this area. Our findings demonstrate that PINN-based approaches in channel
modeling exhibit promising attributes such as generalizability,
interpretability, and robustness. We offer a comprehensive architecture for
PINN methodology, designed to inform and inspire future model development. A
case-study of our recent work on precise indoor channel prediction with
semantic segmentation and deep learning is presented. The study concludes by
addressing the challenges faced and suggesting potential research directions in
this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1&quot;&gt;Ethan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Haijian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1&quot;&gt;Mingyue Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01301">
<title>Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models. (arXiv:2401.01301v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.01301</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have the potential to transform the practice of
law, but this potential is threatened by the presence of legal hallucinations
-- responses from these models that are not consistent with legal facts. We
investigate the extent of these hallucinations using an original suite of legal
queries, comparing LLMs&apos; responses to structured legal metadata and examining
their consistency. Our work makes four key contributions: (1) We develop a
typology of legal hallucinations, providing a conceptual framework for future
research in this area. (2) We find that legal hallucinations are alarmingly
prevalent, occurring between 69% of the time with ChatGPT 3.5 and 88% with
Llama 2, when these models are asked specific, verifiable questions about
random federal court cases. (3) We illustrate that LLMs often fail to correct a
user&apos;s incorrect legal assumptions in a contra-factual question setup. (4) We
provide evidence that LLMs cannot always predict, or do not always know, when
they are producing legal hallucinations. Taken together, these findings caution
against the rapid and unsupervised integration of popular LLMs into legal
tasks. Even experienced lawyers must remain wary of legal hallucinations, and
the risks are highest for those who stand to benefit from LLMs the most -- pro
se litigants or those without access to traditional legal resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahl_M/0/1/0/all/0/1&quot;&gt;Matthew Dahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magesh_V/0/1/0/all/0/1&quot;&gt;Varun Magesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suzgun_M/0/1/0/all/0/1&quot;&gt;Mirac Suzgun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1&quot;&gt;Daniel E. Ho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01304">
<title>Experimental Validation of Sensor Fusion-based GNSS Spoofing Attack Detection Framework for Autonomous Vehicles. (arXiv:2401.01304v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.01304</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we validate the performance of the a sensor fusion-based
Global Navigation Satellite System (GNSS) spoofing attack detection framework
for Autonomous Vehicles (AVs). To collect data, a vehicle equipped with a GNSS
receiver, along with Inertial Measurement Unit (IMU) is used. The detection
framework incorporates two strategies: The first strategy involves comparing
the predicted location shift, which is the distance traveled between two
consecutive timestamps, with the inertial sensor-based location shift. For this
purpose, data from low-cost in-vehicle inertial sensors such as the
accelerometer and gyroscope sensor are fused and fed into a long short-term
memory (LSTM) neural network. The second strategy employs a Random-Forest
supervised machine learning model to detect and classify turns, distinguishing
between left and right turns using the output from the steering angle sensor.
In experiments, two types of spoofing attack models: turn-by-turn and wrong
turn are simulated. These spoofing attacks are modeled as SQL injection
attacks, where, upon successful implementation, the navigation system perceives
injected spoofed location information as legitimate while being unable to
detect legitimate GNSS signals. Importantly, the IMU data remains uncompromised
throughout the spoofing attack. To test the effectiveness of the detection
framework, experiments are conducted in Tuscaloosa, AL, mimicking urban road
structures. The results demonstrate the framework&apos;s ability to detect various
sophisticated GNSS spoofing attacks, even including slow position drifting
attacks. Overall, the experimental results showcase the robustness and efficacy
of the sensor fusion-based spoofing attack detection approach in safeguarding
AVs against GNSS spoofing threats.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1&quot;&gt;Sagar Dasgupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shakib_K/0/1/0/all/0/1&quot;&gt;Kazi Hassan Shakib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Mizanur Rahman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01325">
<title>LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning. (arXiv:2401.01325v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.01325</link>
<description rdf:parseType="Literal">&lt;p&gt;This work elicits LLMs&apos; inherent ability to handle long contexts without
fine-tuning. The limited length of the training sequence during training may
limit the application of Large Language Models (LLMs) on long input sequences
for inference. In this work, we argue that existing LLMs themselves have
inherent capabilities for handling long contexts. Based on this argument, we
suggest extending LLMs&apos; context window by themselves to fully utilize the
inherent ability.We propose Self-Extend to stimulate LLMs&apos; long context
handling potential. The basic idea is to construct bi-level attention
information: the group level and the neighbor level. The two levels are
computed by the original model&apos;s self-attention, which means the proposed does
not require any training. With only four lines of code modification, the
proposed method can effortlessly extend existing LLMs&apos; context window without
any fine-tuning. We conduct comprehensive experiments and the results show that
the proposed method can effectively extend existing LLMs&apos; context window&apos;s
length.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Hongye Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiaotian Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jingfeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhimeng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zirui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Chia-Yuan Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huiyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xia Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01326">
<title>An Autoregressive Text-to-Graph Framework for Joint Entity and Relation Extraction. (arXiv:2401.01326v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.01326</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel method for joint entity and relation
extraction from unstructured text by framing it as a conditional sequence
generation problem. In contrast to conventional generative information
extraction models that are left-to-right token-level generators, our approach
is \textit{span-based}. It generates a linearized graph where nodes represent
text spans and edges represent relation triplets. Our method employs a
transformer encoder-decoder architecture with pointing mechanism on a dynamic
vocabulary of spans and relation types. Our model can capture the structural
characteristics and boundaries of entities and relations through span
representations while simultaneously grounding the generated output in the
original text thanks to the pointing mechanism. Evaluation on benchmark
datasets validates the effectiveness of our approach, demonstrating competitive
results. Code is available at https://github.com/urchade/ATG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urchade_Z/0/1/0/all/0/1&quot;&gt;Zaratiana Urchade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomeh_N/0/1/0/all/0/1&quot;&gt;Nadi Tomeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holat_P/0/1/0/all/0/1&quot;&gt;Pierre Holat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charnois_T/0/1/0/all/0/1&quot;&gt;Thierry Charnois&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01330">
<title>TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview. (arXiv:2401.01330v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2401.01330</link>
<description rdf:parseType="Literal">&lt;p&gt;Conversational Information Seeking stands as a pivotal research area with
significant contributions from previous works. The TREC Interactive Knowledge
Assistance Track (iKAT) builds on the foundational work of the TREC
Conversational Assistance Track (CAsT). However, iKAT distinctively emphasizes
the creation and research of conversational search agents that adapt responses
based on user&apos;s prior interactions and present context. The challenge lies in
enabling Conversational Search Agents (CSA) to incorporate this personalized
context to efficiency and effectively guide users through the relevant
information to them. iKAT also emphasizes decisional search tasks, where users
sift through data and information to weigh up options in order to reach a
conclusion or perform an action. These tasks, prevalent in everyday
information-seeking decisions -- be it related to travel, health, or shopping
-- often revolve around a subset of high-level information operators where
queries or questions about the information space include: finding options,
comparing options, identifying the pros and cons of options, etc. Given the
different personas and their information need (expressed through the sequence
of questions), diverse conversation trajectories will arise -- because the
answers to these similar queries will be very different. In this paper, we
report on the first year of TREC iKAT, describing the task, topics, data
collection, and evaluation framework. We further review the submissions and
summarize the findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aliannejadi_M/0/1/0/all/0/1&quot;&gt;Mohammad Aliannejadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbasiantaeb_Z/0/1/0/all/0/1&quot;&gt;Zahra Abbasiantaeb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1&quot;&gt;Shubham Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalton_J/0/1/0/all/0/1&quot;&gt;Jeffery Dalton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azzopardi_L/0/1/0/all/0/1&quot;&gt;Leif Azzopardi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01335">
<title>Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models. (arXiv:2401.01335v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.01335</link>
<description rdf:parseType="Literal">&lt;p&gt;Harnessing the power of human-annotated data through Supervised Fine-Tuning
(SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we
delve into the prospect of growing a strong LLM out of a weak one without the
need for acquiring additional human-annotated data. We propose a new
fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a
supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism,
where the LLM refines its capability by playing against instances of itself.
More specifically, the LLM generates its own training data from its previous
iterations, refining its policy by discerning these self-generated responses
from those obtained from human-annotated data. Our method progressively
elevates the LLM from a nascent model to a formidable one, unlocking the full
potential of human-annotated demonstration data for SFT. Theoretically, we
prove that the global optimum to the training objective function of our method
is achieved only when the LLM policy aligns with the target data distribution.
Empirically, we evaluate our method on several benchmark datasets including the
HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our
results show that SPIN can significantly improve the LLM&apos;s performance across a
variety of benchmarks and even outperform models trained through direct
preference optimization (DPO) supplemented with extra GPT-4 preference data.
This sheds light on the promise of self-play, enabling the achievement of
human-level performance in LLMs without the need for expert opponents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zixiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yihe Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Huizhuo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_K/0/1/0/all/0/1&quot;&gt;Kaixuan Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1&quot;&gt;Quanquan Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2104.02206">
<title>Tuned Compositional Feature Replays for Efficient Stream Learning. (arXiv:2104.02206v8 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2104.02206</link>
<description rdf:parseType="Literal">&lt;p&gt;Our brains extract durable, generalizable knowledge from transient
experiences of the world. Artificial neural networks come nowhere close to this
ability. When tasked with learning to classify objects by training on
non-repeating video frames in temporal order (online stream learning), models
that learn well from shuffled datasets catastrophically forget old knowledge
upon learning new stimuli. We propose a new continual learning algorithm,
Compositional Replay Using Memory Blocks (CRUMB), which mitigates forgetting by
replaying feature maps reconstructed by combining generic parts. CRUMB
concatenates trainable and re-usable &quot;memory block&quot; vectors to compositionally
reconstruct feature map tensors in convolutional neural networks. Storing the
indices of memory blocks used to reconstruct new stimuli enables memories of
the stimuli to be replayed during later tasks. This reconstruction mechanism
also primes the neural network to minimize catastrophic forgetting by biasing
it towards attending to information about object shapes more than information
about image textures, and stabilizes the network during stream learning by
providing a shared feature-level basis for all training examples. These
properties allow CRUMB to outperform an otherwise identical algorithm that
stores and replays raw images, while occupying only 3.6% as much memory. We
stress-tested CRUMB alongside 13 competing methods on 7 challenging datasets.
To address the limited number of existing online stream learning datasets, we
introduce 2 new benchmarks by adapting existing datasets for stream learning.
With only 3.7-4.1% as much memory and 15-43% as much runtime, CRUMB mitigates
catastrophic forgetting more effectively than the state-of-the-art. Our code is
available at https://github.com/MorganBDT/crumb.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talbot_M/0/1/0/all/0/1&quot;&gt;Morgan B. Talbot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zawar_R/0/1/0/all/0/1&quot;&gt;Rushikesh Zawar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badkundri_R/0/1/0/all/0/1&quot;&gt;Rohil Badkundri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengmi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1&quot;&gt;Gabriel Kreiman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2108.11451">
<title>From Statistical Relational to Neurosymbolic Artificial Intelligence: a Survey. (arXiv:2108.11451v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2108.11451</link>
<description rdf:parseType="Literal">&lt;p&gt;This survey explores the integration of learning and reasoning in two
different fields of artificial intelligence: neurosymbolic and statistical
relational artificial intelligence. Neurosymbolic artificial intelligence
(NeSy) studies the integration of symbolic reasoning and neural networks, while
statistical relational artificial intelligence (StarAI) focuses on integrating
logic with probabilistic graphical models. This survey identifies seven shared
dimensions between these two subfields of AI. These dimensions can be used to
characterize different NeSy and StarAI systems. They are concerned with (1) the
approach to logical inference, whether model or proof-based; (2) the syntax of
the used logical theories; (3) the logical semantics of the systems and their
extensions to facilitate learning; (4) the scope of learning, encompassing
either parameter or structure learning; (5) the presence of symbolic and
subsymbolic representations; (6) the degree to which systems capture the
original logic, probabilistic, and neural paradigms; and (7) the classes of
learning tasks the systems are applied to. By positioning various NeSy and
StarAI systems along these dimensions and pointing out similarities and
differences between them, this survey contributes fundamental concepts for
understanding the integration of learning and reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marra_G/0/1/0/all/0/1&quot;&gt;Giuseppe Marra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumancic_S/0/1/0/all/0/1&quot;&gt;Sebastijan Duman&amp;#x10d;i&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manhaeve_R/0/1/0/all/0/1&quot;&gt;Robin Manhaeve&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raedt_L/0/1/0/all/0/1&quot;&gt;Luc De Raedt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.02164">
<title>Estimating and Mitigating the Congestion Effect of Curbside Pick-ups and Drop-offs: A Causal Inference Approach. (arXiv:2206.02164v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.02164</link>
<description rdf:parseType="Literal">&lt;p&gt;Curb space is one of the busiest areas in urban road networks. Especially in
recent years, the rapid increase of ride-hailing trips and commercial
deliveries has induced massive pick-ups/drop-offs (PUDOs), which occupy the
limited curb space that was designed and built decades ago. These PUDOs could
jam curbside utilization and disturb the mainline traffic flow, evidently
leading to significant negative societal externalities. However, there is a
lack of an analytical framework that rigorously quantifies and mitigates the
congestion effect of PUDOs in the system view, particularly with little data
support and involvement of confounding effects. To bridge this research gap,
this paper develops a rigorous causal inference approach to estimate the
congestion effect of PUDOs on general regional networks. A causal graph is set
to represent the spatio-temporal relationship between PUDOs and traffic speed,
and a double and separated machine learning (DSML) method is proposed to
quantify how PUDOs affect traffic congestion. Additionally, a re-routing
formulation is developed and solved to encourage passenger walking and traffic
flow re-routing to achieve system optimization. Numerical experiments are
conducted using real-world data in the Manhattan area. On average, 100
additional units of PUDOs in a region could reduce the traffic speed by 3.70
and 4.54 mph on weekdays and weekends, respectively. Re-routing trips with
PUDOs on curb space could respectively reduce the system-wide total travel time
by 2.44% and 2.12% in Midtown and Central Park on weekdays. Sensitivity
analysis is also conducted to demonstrate the effectiveness and robustness of
the proposed framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaohui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1&quot;&gt;Sean Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teo_H/0/1/0/all/0/1&quot;&gt;Hock-Hai Teo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wei Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.09041">
<title>Approximation analysis of CNNs from a feature extraction view. (arXiv:2210.09041v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.09041</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning based on deep neural networks has been very successful in many
practical applications, but it lacks enough theoretical understanding due to
the network architectures and structures. In this paper we establish some
analysis for linear feature extraction by a deep multi-channel convolutional
neural networks (CNNs), which demonstrates the power of deep learning over
traditional linear transformations, like Fourier, wavelets, redundant
dictionary coding methods. Moreover, we give an exact construction presenting
how linear features extraction can be conducted efficiently with multi-channel
CNNs. It can be applied to lower the essential dimension for approximating a
high dimensional function. Rates of function approximation by such deep
networks implemented with channels and followed by fully-connected layers are
investigated as well. Harmonic analysis for factorizing linear features into
multi-resolution convolutions plays an essential role in our work.
Nevertheless, a dedicate vectorization of matrices is constructed, which
bridges 1D CNN and 2D CNN and allows us to have corresponding 2D analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianfei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Han Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Ding-Xuan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.04718">
<title>On the Application of Efficient Neural Mapping to Real-Time Indoor Localisation for Unmanned Ground Vehicles. (arXiv:2211.04718v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2211.04718</link>
<description rdf:parseType="Literal">&lt;p&gt;Global localisation from visual data is a challenging problem applicable to
many robotics domains. Prior works have shown that neural networks can be
trained to map images of an environment to absolute camera pose within that
environment, learning an implicit neural mapping in the process. In this work
we evaluate the applicability of such an approach to real-world robotics
scenarios, demonstrating that by constraining the problem to 2-dimensions and
significantly increasing the quantity of training data, a compact model capable
of real-time inference on embedded platforms can be used to achieve
localisation accuracy of several centimetres. We deploy our trained model
onboard a UGV platform, demonstrating its effectiveness in a waypoint
navigation task, wherein it is able to localise with a mean accuracy of 9cm at
a rate of 6fps running on the UGV onboard CPU, 35fps on an embedded GPU, or
220fps on a desktop GPU. Along with this work we will release a novel
localisation dataset comprising simulated and real environments, each with
training samples numbering in the tens of thousands.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holder_C/0/1/0/all/0/1&quot;&gt;Christopher J. Holder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1&quot;&gt;Muhammad Shafique&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13631">
<title>In-depth analysis of music structure as a text network. (arXiv:2303.13631v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13631</link>
<description rdf:parseType="Literal">&lt;p&gt;Music, enchanting and poetic, permeates every corner of human civilization.
Although music is not unfamiliar to people, our understanding of its essence
remains limited, and there is still no universally accepted scientific
description. This is primarily due to music being regarded as a product of both
reason and emotion, making it difficult to define. In this article, we focus on
the fundamental elements of music and construct an evolutionary network from
the perspective of music as a natural language, aligning with the statistical
characteristics of texts. Through this approach, we aim to comprehend the
structural differences in music across different periods, enabling a more
scientific exploration of music. Relying on the advantages of structuralism, we
can concentrate on the relationships and order between the physical elements of
music, rather than getting entangled in the blurred boundaries of science and
philosophy. The scientific framework we present not only conforms to past
conclusions in music, but also serves as a bridge that connects music to
natural language processing and knowledge graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsai_P/0/1/0/all/0/1&quot;&gt;Ping-Rui Tsai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1&quot;&gt;Yen-Ting Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nathan-Christopher Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hui-Ling Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hong-Yue Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zih-Jia Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_T/0/1/0/all/0/1&quot;&gt;Tzay-Ming Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11300">
<title>RS5M and GeoRSCLIP: A Large Scale Vision-Language Dataset and A Large Vision-Language Model for Remote Sensing. (arXiv:2306.11300v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11300</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text
paired data have demonstrated unprecedented image-text association
capabilities, achieving remarkable results across various downstream tasks. A
critical challenge is how to make use of existing large-scale pre-trained VLMs,
which are trained on common objects, to perform the domain-specific transfer
for accomplishing domain-related downstream tasks. A critical challenge is how
to make use of existing large-scale pre-trained VLMs, which are trained on
common objects, to perform the domain-specific transfer for accomplishing
domain-related downstream tasks. In this paper, we propose a new framework that
includes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap
between the General Vision-Language Model (GVLM) and domain-specific downstream
tasks. Moreover, we present an image-text paired dataset in the field of remote
sensing (RS), RS5M, which has 5 million RS images with English descriptions.
The dataset is obtained from filtering publicly available image-text paired
datasets and captioning label-only RS datasets with pre-trained VLM. These
constitute the first large-scale RS image-text paired dataset. Additionally, we
fine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning
methods on RS5M to implement the DVLM. Experimental results show that our
proposed dataset is highly effective for various tasks, and our model GeoRSCLIP
improves upon the baseline or previous state-of-the-art model by $3\%\sim20\%$
in Zero-shot Classification (ZSC), $3\%\sim6\%$ in Remote Sensing Cross-Modal
Text-Image Retrieval (RSCTIR) and $4\%\sim5\%$ in Semantic Localization (SeLo)
tasks. Dataset and models have been released in:
\url{https://github.com/om-ai-lab/RS5M}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zilun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tiancheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yulong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jianwei Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01530">
<title>Tomato Maturity Recognition with Convolutional Transformers. (arXiv:2307.01530v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01530</link>
<description rdf:parseType="Literal">&lt;p&gt;Tomatoes are a major crop worldwide, and accurately classifying their
maturity is important for many agricultural applications, such as harvesting,
grading, and quality control. In this paper, the authors propose a novel method
for tomato maturity classification using a convolutional transformer. The
convolutional transformer is a hybrid architecture that combines the strengths
of convolutional neural networks (CNNs) and transformers. Additionally, this
study introduces a new tomato dataset named KUTomaData, explicitly designed to
train deep-learning models for tomato segmentation and classification.
KUTomaData is a compilation of images sourced from a greenhouse in the UAE,
with approximately 700 images available for training and testing. The dataset
is prepared under various lighting conditions and viewing perspectives and
employs different mobile camera sensors, distinguishing it from existing
datasets. The contributions of this paper are threefold:Firstly, the authors
propose a novel method for tomato maturity classification using a modular
convolutional transformer. Secondly, the authors introduce a new tomato image
dataset that contains images of tomatoes at different maturity levels. Lastly,
the authors show that the convolutional transformer outperforms
state-of-the-art methods for tomato maturity classification. The effectiveness
of the proposed framework in handling cluttered and occluded tomato instances
was evaluated using two additional public datasets, Laboro Tomato and Rob2Pheno
Annotated Tomato, as benchmarks. The evaluation results across these three
datasets demonstrate the exceptional performance of our proposed framework,
surpassing the state-of-the-art by 58.14%, 65.42%, and 66.39% in terms of mean
average precision scores for KUTomaData, Laboro Tomato, and Rob2Pheno Annotated
Tomato, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Asim Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassan_T/0/1/0/all/0/1&quot;&gt;Taimur Hassan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafay_M/0/1/0/all/0/1&quot;&gt;Muhammad Shafay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fahmy_I/0/1/0/all/0/1&quot;&gt;Israa Fahmy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1&quot;&gt;Naoufel Werghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seneviratne_L/0/1/0/all/0/1&quot;&gt;Lakmal Seneviratne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussain_I/0/1/0/all/0/1&quot;&gt;Irfan Hussain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06035">
<title>Multimodality and Attention Increase Alignment in Natural Language Prediction Between Humans and Computational Models. (arXiv:2308.06035v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06035</link>
<description rdf:parseType="Literal">&lt;p&gt;The potential of multimodal generative artificial intelligence (mAI) to
replicate human grounded language understanding, including the pragmatic,
context-rich aspects of communication, remains to be clarified. Humans are
known to use salient multimodal features, such as visual cues, to facilitate
the processing of upcoming words. Correspondingly, multimodal computational
models can integrate visual and linguistic data using a visual attention
mechanism to assign next-word probabilities. To test whether these processes
align, we tasked both human participants (N = 200) as well as several
state-of-the-art computational models with evaluating the predictability of
forthcoming words after viewing short audio-only or audio-visual clips with
speech. During the task, the model&apos;s attention weights were recorded and human
attention was indexed via eye tracking. Results show that predictability
estimates from humans aligned more closely with scores generated from
multimodal models vs. their unimodal counterparts. Furthermore, including an
attention mechanism doubled alignment with human judgments when visual and
linguistic context facilitated predictions. In these cases, the model&apos;s
attention patches and human eye tracking significantly overlapped. Our results
indicate that improved modeling of naturalistic language processing in mAI does
not merely depend on training diet but can be driven by multimodality in
combination with attention-based architectures. Humans and computational models
alike can leverage the predictive constraints of multimodal information by
attending to relevant features in the input.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kewenig_V/0/1/0/all/0/1&quot;&gt;Viktor Kewenig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1&quot;&gt;Andrew Lampinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nastase_S/0/1/0/all/0/1&quot;&gt;Samuel A. Nastase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edwards_C/0/1/0/all/0/1&quot;&gt;Christopher Edwards&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DEstalenx_Q/0/1/0/all/0/1&quot;&gt;Quitterie Lacome DEstalenx&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rechardt_A/0/1/0/all/0/1&quot;&gt;Akilles Rechardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skipper_J/0/1/0/all/0/1&quot;&gt;Jeremy I Skipper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vigliocco_G/0/1/0/all/0/1&quot;&gt;Gabriella Vigliocco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12682">
<title>SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge. (arXiv:2308.12682v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12682</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have demonstrated impressive planning abilities
due to their vast &quot;world knowledge&quot;. Yet, obtaining plans that are both
feasible (grounded in affordances) and cost-effective (in plan length), remains
a challenge, despite recent progress. This contrasts with heuristic planning
methods that employ domain knowledge (formalized in action models such as PDDL)
and heuristic search to generate feasible, optimal plans. Inspired by this, we
propose to combine the power of LLMs and heuristic planning by leveraging the
world knowledge of LLMs and the principles of heuristic search. Our approach,
SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain
knowledge, that evaluates actions&apos; feasibility (Can) and long-term
reward/payoff (Pay), and heuristic search to select the best sequence of
actions. Our contributions are (1) a novel framing of the LLM planning problem
in the context of heuristic planning, (2) integrating grounding and
cost-effective elements into the generated plans, and (3) using heuristic
search over actions. Our extensive evaluations show that our model surpasses
other LLM planning approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hazra_R/0/1/0/all/0/1&quot;&gt;Rishi Hazra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martires_P/0/1/0/all/0/1&quot;&gt;Pedro Zuidberg Dos Martires&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raedt_L/0/1/0/all/0/1&quot;&gt;Luc De Raedt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09496">
<title>CLIP-based Synergistic Knowledge Transfer for Text-based Person Retrieval. (arXiv:2309.09496v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09496</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-based Person Retrieval (TPR) aims to retrieve the target person images
given a textual query. The primary challenge lies in bridging the substantial
gap between vision and language modalities, especially when dealing with
limited large-scale datasets. In this paper, we introduce a CLIP-based
Synergistic Knowledge Transfer (CSKT) approach for TPR. Specifically, to
explore the CLIP&apos;s knowledge on input side, we first propose a Bidirectional
Prompts Transferring (BPT) module constructed by text-to-image and
image-to-text bidirectional prompts and coupling projections. Secondly, Dual
Adapters Transferring (DAT) is designed to transfer knowledge on output side of
Multi-Head Attention (MHA) in vision and language. This synergistic two-way
collaborative mechanism promotes the early-stage feature fusion and efficiently
exploits the existing knowledge of CLIP. CSKT outperforms the state-of-the-art
approaches across three benchmark datasets when the training parameters merely
account for 7.4% of the entire model, demonstrating its remarkable efficiency,
effectiveness and generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yating Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yaowei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zimo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1&quot;&gt;Qingmin Liao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.10399">
<title>Exploiting Causality Signals in Medical Images: A Pilot Study with Empirical Results. (arXiv:2309.10399v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.10399</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel technique to discover and exploit weak causal signals
directly from images via neural networks for classification purposes. This way,
we model how the presence of a feature in one part of the image affects the
appearance of another feature in a different part of the image. Our method
consists of a convolutional neural network backbone and a causality-factors
extractor module, which computes weights to enhance each feature map according
to its causal influence in the scene. We develop different architecture
variants and empirically evaluate all the models on two public datasets of
prostate MRI images and breast histopathology slides for cancer diagnosis. We
study the effectiveness of our module both in fully-supervised and few-shot
learning, we assess its addition to existing attention-based solutions, we
conduct ablation studies, and investigate the explainability of our models via
class activation maps. Our findings show that our lightweight block extracts
meaningful information and improves the overall classification, together with
producing more robust predictions that focus on relevant parts of the image.
That is crucial in medical imaging, where accurate and reliable classifications
are essential for effective diagnosis and treatment planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carloni_G/0/1/0/all/0/1&quot;&gt;Gianluca Carloni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colantonio_S/0/1/0/all/0/1&quot;&gt;Sara Colantonio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14496">
<title>Era Splitting -- Invariant Learning for Decision Trees. (arXiv:2309.14496v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14496</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-life machine learning problems exhibit distributional shifts in the data
from one time to another or from on place to another. This behavior is beyond
the scope of the traditional empirical risk minimization paradigm, which
assumes i.i.d. distribution of data over time and across locations. The
emerging field of out-of-distribution (OOD) generalization addresses this
reality with new theory and algorithms which incorporate environmental, or
era-wise information into the algorithms. So far, most research has been
focused on linear models and/or neural networks. In this research we develop
two new splitting criteria for decision trees, which allow us to apply ideas
from OOD generalization research to decision tree models, including random
forest and gradient-boosting decision trees. The new splitting criteria use
era-wise information associated with each data point to allow tree-based models
to find split points that are optimal across all disjoint eras in the data,
instead of optimal over the entire data set pooled together, which is the
default setting. In this paper we describe the problem setup in the context of
financial markets. We describe the new splitting criteria in detail and develop
unique experiments to showcase the benefits of these new criteria, which
improve metrics in our experiments out-of-sample. The new criteria are
incorporated into the a state-of-the-art gradient boosted decision tree model
in the Scikit-Learn code base, which is made freely available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeLise_T/0/1/0/all/0/1&quot;&gt;Timothy DeLise&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15224">
<title>Collaborative Watermarking for Adversarial Speech Synthesis. (arXiv:2309.15224v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15224</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in neural speech synthesis have brought us technology that is not
only close to human naturalness, but is also capable of instant voice cloning
with little data, and is highly accessible with pre-trained models available.
Naturally, the potential flood of generated content raises the need for
synthetic speech detection and watermarking. Recently, considerable research
effort in synthetic speech detection has been related to the Automatic Speaker
Verification and Spoofing Countermeasure Challenge (ASVspoof), which focuses on
passive countermeasures. This paper takes a complementary view to generated
speech detection: a synthesis system should make an active effort to watermark
the generated speech in a way that aids detection by another machine, but
remains transparent to a human listener. We propose a collaborative training
scheme for synthetic speech watermarking and show that a HiFi-GAN neural
vocoder collaborating with the ASVspoof 2021 baseline countermeasure models
consistently improves detection performance over conventional classifier
training. Furthermore, we demonstrate how collaborative training can be paired
with augmentation strategies for added robustness against noise and
time-stretching. Finally, listening tests demonstrate that collaborative
training has little adverse effect on perceptual quality of vocoded speech.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Juvela_L/0/1/0/all/0/1&quot;&gt;Lauri Juvela&lt;/a&gt; (Aalto University, Finland), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt; (National Institute of Informatics, Japan)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16741">
<title>Multi-Modal Financial Time-Series Retrieval Through Latent Space Projections. (arXiv:2309.16741v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16741</link>
<description rdf:parseType="Literal">&lt;p&gt;Financial firms commonly process and store billions of time-series data,
generated continuously and at a high frequency. To support efficient data
storage and retrieval, specialized time-series databases and systems have
emerged. These databases support indexing and querying of time-series by a
constrained Structured Query Language(SQL)-like format to enable queries like
&quot;Stocks with monthly price returns greater than 5%&quot;, and expressed in rigid
formats. However, such queries do not capture the intrinsic complexity of high
dimensional time-series data, which can often be better described by images or
language (e.g., &quot;A stock in low volatility regime&quot;). Moreover, the required
storage, computational time, and retrieval complexity to search in the
time-series space are often non-trivial. In this paper, we propose and
demonstrate a framework to store multi-modal data for financial time-series in
a lower-dimensional latent space using deep encoders, such that the latent
space projections capture not only the time series trends but also other
desirable information or properties of the financial time-series data (such as
price volatility). Moreover, our approach allows user-friendly query
interfaces, enabling natural language text or sketches of time-series, for
which we have developed intuitive interfaces. We demonstrate the advantages of
our method in terms of computational efficiency and accuracy on real historical
data as well as synthetic data, and highlight the utility of latent-space
projections in the storage and retrieval of financial time-series data with
intuitive query modalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bamford_T/0/1/0/all/0/1&quot;&gt;Tom Bamford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coletta_A/0/1/0/all/0/1&quot;&gt;Andrea Coletta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fons_E/0/1/0/all/0/1&quot;&gt;Elizabeth Fons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalakrishnan_S/0/1/0/all/0/1&quot;&gt;Sriram Gopalakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vyetrenko_S/0/1/0/all/0/1&quot;&gt;Svitlana Vyetrenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balch_T/0/1/0/all/0/1&quot;&gt;Tucker Balch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veloso_M/0/1/0/all/0/1&quot;&gt;Manuela Veloso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02067">
<title>Content Bias in Deep Learning Image Age Approximation: A new Approach Towards better Explainability. (arXiv:2310.02067v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02067</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of temporal image forensics, it is not evident that a neural
network, trained on images from different time-slots (classes), exploits solely
image age related features. Usually, images taken in close temporal proximity
(e.g., belonging to the same age class) share some common content properties.
Such content bias can be exploited by a neural network. In this work, a novel
approach is proposed that evaluates the influence of image content. This
approach is verified using synthetic images (where content bias can be ruled
out) with an age signal embedded. Based on the proposed approach, it is shown
that a deep learning approach proposed in the context of age classification is
most likely highly dependent on the image content. As a possible
countermeasure, two different models from the field of image steganalysis,
along with three different preprocessing techniques to increase the
signal-to-noise ratio (age signal to image content), are evaluated using the
proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jochl_R/0/1/0/all/0/1&quot;&gt;Robert J&amp;#xf6;chl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uhl_A/0/1/0/all/0/1&quot;&gt;Andreas Uhl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10477">
<title>Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10477</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid development of large language models (LLMs) has not only provided
numerous opportunities but also presented significant challenges. This becomes
particularly evident when LLMs inadvertently generate harmful or toxic content,
either unintentionally or because of intentional inducement. Existing alignment
methods usually direct LLMs toward the favorable outcomes by utilizing
human-annotated, flawless instruction-response pairs. Conversely, this study
proposes a novel alignment technique based on mistake analysis, which
deliberately exposes LLMs to erroneous content to learn the reasons for
mistakes and how to avoid them. In this case, mistakes are repurposed into
valuable data for alignment, effectively helping to avoid the production of
erroneous responses. Without external models or human annotations, our method
leverages a model&apos;s intrinsic ability to discern undesirable mistakes and
improves the safety of its generated responses. Experimental results reveal
that our method outperforms existing alignment approaches in enhancing model
safety while maintaining the overall utility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chunwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kuo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jianhua Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1&quot;&gt;Lanqing Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1&quot;&gt;Fei Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenyong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1&quot;&gt;Dit-Yan Yeung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1&quot;&gt;Lifeng Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qun Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19852">
<title>AI Alignment: A Comprehensive Survey. (arXiv:2310.19852v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19852</link>
<description rdf:parseType="Literal">&lt;p&gt;AI alignment aims to make AI systems behave in line with human intentions and
values. As AI systems grow more capable, so do risks from misalignment. To
provide a comprehensive and up-to-date overview of the alignment field, in this
survey, we delve into the core concepts, methodology, and practice of
alignment. First, we identify four principles as the key objectives of AI
alignment: Robustness, Interpretability, Controllability, and Ethicality
(RICE). Guided by these four principles, we outline the landscape of current
alignment research and decompose them into two key components: forward
alignment and backward alignment. The former aims to make AI systems aligned
via alignment training, while the latter aims to gain evidence about the
systems&apos; alignment and govern them appropriately to avoid exacerbating
misalignment risks. On forward alignment, we discuss techniques for learning
from feedback and learning under distribution shift. On backward alignment, we
discuss assurance techniques and governance practices.
&lt;/p&gt;
&lt;p&gt;We also release and continually update the website (www.alignmentsurvey.com)
which features tutorials, collections of papers, blog posts, and other
resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jiaming Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_T/0/1/0/all/0/1&quot;&gt;Tianyi Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Boyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Borong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_H/0/1/0/all/0/1&quot;&gt;Hantao Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaile Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1&quot;&gt;Yawen Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhonghao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiayi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhaowei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1&quot;&gt;Fanzhi Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_K/0/1/0/all/0/1&quot;&gt;Kwan Yee Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Juntao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xuehai Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OGara_A/0/1/0/all/0/1&quot;&gt;Aidan O&amp;#x27;Gara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1&quot;&gt;Yingshan Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hua Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tse_B/0/1/0/all/0/1&quot;&gt;Brian Tse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McAleer_S/0/1/0/all/0/1&quot;&gt;Stephen McAleer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yaodong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhou Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Song-Chun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yike Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wen Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19923">
<title>Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19923</link>
<description rdf:parseType="Literal">&lt;p&gt;Text embedding models have emerged as powerful tools for transforming
sentences into fixed-sized feature vectors that encapsulate semantic
information. While these models are essential for tasks like information
retrieval, semantic clustering, and text re-ranking, most existing open-source
models, especially those built on architectures like BERT, struggle to
represent lengthy documents and often resort to truncation. One common approach
to mitigate this challenge involves splitting documents into smaller paragraphs
for embedding. However, this strategy results in a much larger set of vectors,
consequently leading to increased memory consumption and computationally
intensive vector searches with elevated latency.
&lt;/p&gt;
&lt;p&gt;To address these challenges, we introduce Jina Embeddings 2, an open-source
text embedding model capable of accommodating up to 8192 tokens. This model is
designed to transcend the conventional 512-token limit and adeptly process long
documents. Jina Embeddings 2 not only achieves state-of-the-art performance on
a range of embedding-related tasks in the MTEB benchmark but also matches the
performance of OpenAI&apos;s proprietary ada-002 model. Additionally, our
experiments indicate that an extended context can enhance performance in tasks
such as NarrativeQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunther_M/0/1/0/all/0/1&quot;&gt;Michael G&amp;#xfc;nther&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_J/0/1/0/all/0/1&quot;&gt;Jackmin Ong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohr_I/0/1/0/all/0/1&quot;&gt;Isabelle Mohr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdessalem_A/0/1/0/all/0/1&quot;&gt;Alaeddine Abdessalem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abel_T/0/1/0/all/0/1&quot;&gt;Tanguy Abel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akram_M/0/1/0/all/0/1&quot;&gt;Mohammad Kalim Akram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guzman_S/0/1/0/all/0/1&quot;&gt;Susana Guzman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mastrapas_G/0/1/0/all/0/1&quot;&gt;Georgios Mastrapas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sturua_S/0/1/0/all/0/1&quot;&gt;Saba Sturua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werk_M/0/1/0/all/0/1&quot;&gt;Maximilian Werk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1&quot;&gt;Han Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04021">
<title>A Study on the Calibration of In-context Learning. (arXiv:2312.04021v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04021</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate uncertainty quantification is crucial for the safe deployment of
language models (LMs), and prior research has demonstrated improvements in the
calibration of modern LMs. Our study focuses on in-context learning (ICL), a
prevalent method for adapting static LMs through tailored prompts, and examines
the balance between performance and calibration across a broad spectrum of
natural language understanding and reasoning tasks. Through comprehensive
experiments, we observe that, with an increasing number of ICL examples, models
initially exhibit increased miscalibration before achieving better calibration
and miscalibration tends to arise in low-shot settings. Moreover, we find that
methods aimed at improving usability, such as fine-tuning and chain-of-thought
(CoT) prompting, can lead to miscalibration and unreliable natural language
explanations, suggesting that new methods may be required for scenarios where
models are expected to be reliable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanlin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi-Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yaodong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madeka_D/0/1/0/all/0/1&quot;&gt;Dhruv Madeka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1&quot;&gt;Dean Foster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1&quot;&gt;Himabindu Lakkaraju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1&quot;&gt;Sham Kakade&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05019">
<title>Vision-based Learning for Drones: A Survey. (arXiv:2312.05019v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05019</link>
<description rdf:parseType="Literal">&lt;p&gt;Drones as advanced cyber-physical systems are undergoing a transformative
shift with the advent of vision-based learning, a field that is rapidly gaining
prominence due to its profound impact on drone autonomy and functionality.
Different from existing task-specific surveys, this review offers a
comprehensive overview of vision-based learning in drones, emphasizing its
pivotal role in enhancing their operational capabilities under various
scenarios. We start by elucidating the fundamental principles of vision-based
learning, highlighting how it significantly improves drones&apos; visual perception
and decision-making processes. We then categorize vision-based control methods
into indirect, semi-direct, and end-to-end approaches from the
perception-control perspective. We further explore various applications of
vision-based drones with learning capabilities, ranging from single-agent
systems to more complex multi-agent and heterogeneous system scenarios, and
underscore the challenges and innovations characterizing each area. Finally, we
explore open questions and potential solutions, paving the way for ongoing
research and development in this dynamic and rapidly evolving field. With
growing large language models (LLMs) and embodied intelligence, vision-based
learning for drones provides a promising but challenging road towards
artificial general intelligence (AGI) in 3D physical world.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jiaping Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rangya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuhang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feroskhan_M/0/1/0/all/0/1&quot;&gt;Mir Feroskhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08702">
<title>Rational Sensibility: LLM Enhanced Empathetic Response Generation Guided by Self-presentation Theory. (arXiv:2312.08702v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08702</link>
<description rdf:parseType="Literal">&lt;p&gt;Having the ability to empathize is crucial for accurately representing human
behavior during conversations. Despite numerous research aim to improve the
cognitive capability of models by incorporating external knowledge, there has
been limited attention on the sensible and rational expression of the
conversation itself, which are crucial components of the cognitive empathy.
Guided by self-presentation theory in sociology, we have designed an innovative
categorical approach that segregates historical dialogues into sensible and
rational sentences and subsequently elucidate the context through the designed
attention mechanism. However, the rational information within the conversation
is restricted and the external knowledge used in previous methods have
limitations of semantic contradiction and narrow vision field. Considering the
impressive performance of LLM in the domain of intelligent agent. We employ
LLaMA2-70b as a rational brain to analyze the profound logical information
maintained in conversations, which assists the model assessing the balance of
sensibility and rationality to produce quality empathetic responses.
Experimental evaluations demonstrate that our method outperforms other
comparable methods on both automatic and human evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Linzhuang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1&quot;&gt;Nan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jingxuan Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bihui Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_L/0/1/0/all/0/1&quot;&gt;Liping Bu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yin Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10144">
<title>Data-Efficient Multimodal Fusion on a Single GPU. (arXiv:2312.10144v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10144</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of multimodal alignment is to learn a single latent space that is
shared between multimodal inputs. The most powerful models in this space have
been trained using massive datasets of paired inputs and large-scale
computational resources, making them prohibitively expensive to train in many
practical scenarios. We surmise that existing unimodal encoders pre-trained on
large amounts of unimodal data should provide an effective bootstrap to create
multimodal models from unimodal ones at much lower costs. We therefore propose
FuseMix, a multimodal augmentation scheme that operates on the latent spaces of
arbitrary pre-trained unimodal encoders. Using FuseMix for multimodal
alignment, we achieve competitive performance -- and in certain cases
outperform state-of-the art methods -- in both image-text and audio-text
retrieval, with orders of magnitude less compute and data: for example, we
outperform CLIP on the Flickr30K text-to-image retrieval task with $\sim \!
600\times$ fewer GPU days and $\sim \! 80\times$ fewer image-text pairs.
Additionally, we show how our method can be applied to convert pre-trained
text-to-image generative models into audio-to-image ones. Code is available at:
https://github.com/layer6ai-labs/fusemix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vouitsis_N/0/1/0/all/0/1&quot;&gt;No&amp;#xeb;l Vouitsis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhaoyan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorti_S/0/1/0/all/0/1&quot;&gt;Satya Krishna Gorti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villecroze_V/0/1/0/all/0/1&quot;&gt;Valentin Villecroze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cresswell_J/0/1/0/all/0/1&quot;&gt;Jesse C. Cresswell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Guangwei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loaiza_Ganem_G/0/1/0/all/0/1&quot;&gt;Gabriel Loaiza-Ganem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volkovs_M/0/1/0/all/0/1&quot;&gt;Maksims Volkovs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10307">
<title>MusER: Musical Element-Based Regularization for Generating Symbolic Music with Emotion. (arXiv:2312.10307v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10307</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating music with emotion is an important task in automatic music
generation, in which emotion is evoked through a variety of musical elements
(such as pitch and duration) that change over time and collaborate with each
other. However, prior research on deep learning-based emotional music
generation has rarely explored the contribution of different musical elements
to emotions, let alone the deliberate manipulation of these elements to alter
the emotion of music, which is not conducive to fine-grained element-level
control over emotions. To address this gap, we present a novel approach
employing musical element-based regularization in the latent space to
disentangle distinct elements, investigate their roles in distinguishing
emotions, and further manipulate elements to alter musical emotions.
Specifically, we propose a novel VQ-VAE-based model named MusER. MusER
incorporates a regularization loss to enforce the correspondence between the
musical element sequences and the specific dimensions of latent variable
sequences, providing a new solution for disentangling discrete sequences.
Taking advantage of the disentangled latent vectors, a two-level decoding
strategy that includes multiple decoders attending to latent vectors with
different semantics is devised to better predict the elements. By visualizing
latent space, we conclude that MusER yields a disentangled and interpretable
latent space and gain insights into the contribution of distinct elements to
the emotional dimensions (i.e., arousal and valence). Experimental results
demonstrate that MusER outperforms the state-of-the-art models for generating
emotional music in both objective and subjective evaluation. Besides, we
rearrange music through element transfer and attempt to alter the emotion of
music by transferring emotion-distinguishable elements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shulei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xinyu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11231">
<title>Global Feature Pyramid Network. (arXiv:2312.11231v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11231</link>
<description rdf:parseType="Literal">&lt;p&gt;The visual feature pyramid has proven its effectiveness and efficiency in
target detection tasks. Yet, current methodologies tend to overly emphasize
inter-layer feature interaction, neglecting the crucial aspect of intra-layer
feature adjustment. Experience underscores the significant advantages of
intra-layer feature interaction in enhancing target detection tasks. While some
approaches endeavor to learn condensed intra-layer feature representations
using attention mechanisms or visual transformers, they overlook the
incorporation of global information interaction. This oversight results in
increased false detections and missed targets.To address this critical issue,
this paper introduces the Global Feature Pyramid Network (GFPNet), an augmented
version of PAFPN that integrates global information for enhanced target
detection. Specifically, we leverage a lightweight MLP to capture global
feature information, utilize the VNC encoder to process these features, and
employ a parallel learnable mechanism to extract intra-layer features from the
input image. Building on this foundation, we retain the PAFPN method to
facilitate inter-layer feature interaction, extracting rich feature details
across various levels.Compared to conventional feature pyramids, GFPN not only
effectively focuses on inter-layer feature information but also captures global
feature details, fostering intra-layer feature interaction and generating a
more comprehensive and impactful feature representation. GFPN consistently
demonstrates performance improvements over object detection baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1&quot;&gt;Weilin Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Ming Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yonggui Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11460">
<title>Hybrid Internal Model: Learning Agile Legged Locomotion with Simulated Robot Response. (arXiv:2312.11460v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11460</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust locomotion control depends on accurate state estimations. However, the
sensors of most legged robots can only provide partial and noisy observations,
making the estimation particularly challenging, especially for external states
like terrain frictions and elevation maps. Inspired by the classical Internal
Model Control principle, we consider these external states as disturbances and
introduce Hybrid Internal Model (HIM) to estimate them according to the
response of the robot. The response, which we refer to as the hybrid internal
embedding, contains the robot&apos;s explicit velocity and implicit stability
representation, corresponding to two primary goals for locomotion tasks:
explicitly tracking velocity and implicitly maintaining stability. We use
contrastive learning to optimize the embedding to be close to the robot&apos;s
successor state, in which the response is naturally embedded. HIM has several
appealing benefits: It only needs the robot&apos;s proprioceptions, i.e., those from
joint encoders and IMU as observations. It innovatively maintains consistent
observations between simulation reference and reality that avoids information
loss in mimicking learning. It exploits batch-level information that is more
robust to noises and keeps better sample efficiency. It only requires 1 hour of
training on an RTX 4090 to enable a quadruped robot to traverse any terrain
under any disturbances. A wealth of real-world experiments demonstrates its
agility, even in high-difficulty tasks and cases never occurred during the
training process, revealing remarkable open-world generalizability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1&quot;&gt;Junfeng Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zirui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Quanyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jiawei Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1&quot;&gt;Liu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1&quot;&gt;Jiangmiao Pang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.17353">
<title>Towards Auto-Modeling of Formal Verification for NextG Protocols: A Multimodal cross- and self-attention Large Language Model Approach. (arXiv:2312.17353v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2312.17353</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces Auto-modeling of Formal Verification with Real-world
Prompting for 5G and NextG protocols (AVRE), a novel system designed for the
formal verification of Next Generation (NextG) communication protocols,
addressing the increasing complexity and scalability challenges in network
protocol design and verification. Utilizing Large Language Models (LLMs), AVRE
transforms protocol descriptions into dependency graphs and formal models,
efficiently resolving ambiguities and capturing design intent. The system
integrates a transformer model with LLMs to autonomously establish quantifiable
dependency relationships through cross- and self-attention mechanisms. Enhanced
by iterative feedback from the HyFuzz experimental platform, AVRE significantly
advances the accuracy and relevance of formal verification in complex
communication protocols, offering a groundbreaking approach to validating
sophisticated communication systems. We compare CAL&apos;s performance with
state-of-the-art LLM-based models and traditional time sequence models,
demonstrating its superiority in accuracy and robustness, achieving an accuracy
of 95.94\% and an AUC of 0.98. This NLP-based approach enables, for the first
time, the creation of exploits directly from design documents, making
remarkable progress in scalable system verification and validation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jingda Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Ying Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00544">
<title>A Reliable Knowledge Processing Framework for Combustion Science using Foundation Models. (arXiv:2401.00544v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00544</link>
<description rdf:parseType="Literal">&lt;p&gt;This research explores the integration of large language models (LLMs) into
scientific data assimilation, focusing on combustion science as a case study.
Leveraging foundational models integrated with Retrieval-Augmented Generation
(RAG) framework, the study introduces an approach to process diverse combustion
research data, spanning experimental studies, simulations, and literature. The
multifaceted nature of combustion research emphasizes the critical role of
knowledge processing in navigating and extracting valuable information from a
vast and diverse pool of sources. The developed approach minimizes
computational and economic expenses while optimizing data privacy and accuracy.
It incorporates prompt engineering and offline open-source LLMs, offering user
autonomy in selecting base models. The study provides a thorough examination of
text segmentation strategies, conducts comparative studies between LLMs, and
explores various optimized prompts to demonstrate the effectiveness of the
framework. By incorporating an external database, the framework outperforms a
conventional LLM in generating accurate responses and constructing robust
arguments. Additionally, the study delves into the investigation of optimized
prompt templates for the purpose of efficient extraction of scientific
literature. The research addresses concerns related to hallucinations and false
research articles by introducing a custom workflow developed with a detection
algorithm to filter out inaccuracies. Despite identified areas for improvement,
the framework consistently delivers accurate domain-specific responses with
minimal human oversight. The prompt-agnostic approach introduced holds promise
for future deliberations. The study underscores the significance of integrating
LLMs and knowledge processing techniques in scientific research, providing a
foundation for advancements in data assimilation and utilization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1&quot;&gt;Vansh Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raman_V/0/1/0/all/0/1&quot;&gt;Venkat Raman&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>