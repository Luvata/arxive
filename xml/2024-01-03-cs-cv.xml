<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-01T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00025" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00027" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00028" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00029" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00036" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00080" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00094" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00127" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00148" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00151" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00155" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00159" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00208" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00237" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00248" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00260" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00268" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00271" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00275" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00285" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00371" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00390" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00391" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00414" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00416" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00431" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00438" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00440" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00456" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00460" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00523" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00551" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00608" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00616" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00639" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00652" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00653" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00663" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00692" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00728" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00740" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00816" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00825" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00833" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00847" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2102.13272" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.14174" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.02980" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.06424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.07436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.00946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.06950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.14416" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.10867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.14418" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05807" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.05673" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.13586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14669" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16283" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17939" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18601" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00354" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16846" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03407" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06412" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14181" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15031" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06594" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10835" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01004" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03356" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01324" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10324" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16580" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.17163" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.17205" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04430" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.00014">
<title>Resource-Limited Automated Ki67 Index Estimation in Breast Cancer. (arXiv:2401.00014v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2401.00014</link>
<description rdf:parseType="Literal">&lt;p&gt;The prediction of tumor progression and chemotherapy response has been
recently tackled exploiting Tumor Infiltrating Lymphocytes (TILs) and the
nuclear protein Ki67 as prognostic factors. Recently, deep neural networks
(DNNs) have been shown to achieve top results in estimating Ki67 expression and
simultaneous determination of intratumoral TILs score in breast cancer cells.
However, in the last ten years the extraordinary progress induced by deep
models proliferated at least as much as their resource demand. The exorbitant
computational costs required to query (and in some cases also to store) a deep
model represent a strong limitation in resource-limited contexts, like that of
IoT-based applications to support healthcare personnel. To this end, we propose
a resource consumption-aware DNN for the effective estimate of the percentage
of Ki67-positive cells in breast cancer screenings. Our approach reduced up to
75% and 89% the usage of memory and disk space respectively, up to 1.5x the
energy consumption, and preserved or improved the overall accuracy of a
benchmark state-of-the-art solution. Encouraged by such positive results, we
developed and structured the adopted framework so as to allow its general
purpose usage, along with a public software repository to support its usage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gliozzo_J/0/1/0/all/0/1&quot;&gt;J. Gliozzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Marino_G/0/1/0/all/0/1&quot;&gt;G. Marin&amp;#xf2;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bonometti_A/0/1/0/all/0/1&quot;&gt;A. Bonometti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Frasca_M/0/1/0/all/0/1&quot;&gt;M. Frasca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Malchiodi_D/0/1/0/all/0/1&quot;&gt;D. Malchiodi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00025">
<title>Any-point Trajectory Modeling for Policy Learning. (arXiv:2401.00025v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.00025</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning from demonstration is a powerful method for teaching robots new
skills, and more demonstration data often improves policy learning. However,
the high cost of collecting demonstration data is a significant bottleneck.
Videos, as a rich data source, contain knowledge of behaviors, physics, and
semantics, but extracting control-specific information from them is challenging
due to the lack of action labels. In this work, we introduce a novel framework,
Any-point Trajectory Modeling (ATM), that utilizes video demonstrations by
pre-training a trajectory model to predict future trajectories of arbitrary
points within a video frame. Once trained, these trajectories provide detailed
control guidance, enabling the learning of robust visuomotor policies with
minimal action-labeled data. Our method&apos;s effectiveness is demonstrated across
130 simulation tasks, focusing on language-conditioned manipulation tasks.
Visualizations and code are available at:
\url{https://xingyu-lin.github.io/atm}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1&quot;&gt;Chuan Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xingyu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+So_J/0/1/0/all/0/1&quot;&gt;John So&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1&quot;&gt;Qi Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00027">
<title>Efficient Multi-scale Network with Learnable Discrete Wavelet Transform for Blind Motion Deblurring. (arXiv:2401.00027v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00027</link>
<description rdf:parseType="Literal">&lt;p&gt;Coarse-to-fine schemes are widely used in traditional single-image motion
deblur; however, in the context of deep learning, existing multi-scale
algorithms not only require the use of complex modules for feature fusion of
low-scale RGB images and deep semantics, but also manually generate
low-resolution pairs of images that do not have sufficient confidence. In this
work, we propose a multi-scale network based on single-input and
multiple-outputs(SIMO) for motion deblurring. This simplifies the complexity of
algorithms based on a coarse-to-fine scheme. To alleviate restoration defects
impacting detail information brought about by using a multi-scale architecture,
we combine the characteristics of real-world blurring trajectories with a
learnable wavelet transform module to focus on the directional continuity and
frequency features of the step-by-step transitions between blurred images to
sharp images. In conclusion, we propose a multi-scale network with a learnable
discrete wavelet transform (MLWNet), which exhibits state-of-the-art
performance on multiple real-world deblurred datasets, in terms of both
subjective and objective quality as well as computational efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_T/0/1/0/all/0/1&quot;&gt;Tianheng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1&quot;&gt;Hanlin Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guoying Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huaping Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00028">
<title>An Empirical Study of Scaling Law for OCR. (arXiv:2401.00028v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00028</link>
<description rdf:parseType="Literal">&lt;p&gt;The laws of model size, data volume, computation and model performance have
been extensively studied in the field of Natural Language Processing (NLP).
However, the scaling laws in Optical Character Recognition (OCR) have not yet
been investigated. To address this, we conducted comprehensive studies that
involved examining the correlation between performance and the scale of models,
data volume and computation in the field of text recognition.Conclusively, the
study demonstrates smooth power laws between performance and model size, as
well as training data volume, when other influencing factors are held constant.
Additionally, we have constructed a large-scale dataset called REBU-Syn, which
comprises 6 million real samples and 18 million synthetic samples. Based on our
scaling law and new dataset, we have successfully trained a scene text
recognition model, achieving a new state-ofthe-art on 6 common test benchmarks
with a top-1 average accuracy of 97.42%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rang_M/0/1/0/all/0/1&quot;&gt;Miao Rang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1&quot;&gt;Zhenni Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chuanjian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1&quot;&gt;Kai Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00029">
<title>6D-Diff: A Keypoint Diffusion Framework for 6D Object Pose Estimation. (arXiv:2401.00029v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00029</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating the 6D object pose from a single RGB image often involves noise
and indeterminacy due to challenges such as occlusions and cluttered
backgrounds. Meanwhile, diffusion models have shown appealing performance in
generating high-quality images from random noise with high indeterminacy
through step-by-step denoising. Inspired by their denoising capability, we
propose a novel diffusion-based framework (6D-Diff) to handle the noise and
indeterminacy in object pose estimation for better performance. In our
framework, to establish accurate 2D-3D correspondence, we formulate 2D
keypoints detection as a reverse diffusion (denoising) process. To facilitate
such a denoising process, we design a Mixture-of-Cauchy-based forward diffusion
process and condition the reverse process on the object features. Extensive
experiments on the LM-O and YCB-V datasets demonstrate the effectiveness of our
framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Li Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1&quot;&gt;Haoxuan Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;Yujun Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00036">
<title>Discrete Distribution Networks. (arXiv:2401.00036v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00036</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel generative model, the Discrete Distribution Networks
(DDN), that approximates data distribution using hierarchical discrete
distributions. We posit that since the features within a network inherently
contain distributional information, liberating the network from a single output
to concurrently generate multiple samples proves to be highly effective.
Therefore, DDN fits the target distribution, including continuous ones, by
generating multiple discrete sample points. To capture finer details of the
target data, DDN selects the output that is closest to the Ground Truth (GT)
from the coarse results generated in the first layer. This selected output is
then fed back into the network as a condition for the second layer, thereby
generating new outputs more similar to the GT. As the number of DDN layers
increases, the representational space of the outputs expands exponentially, and
the generated samples become increasingly similar to the GT. This hierarchical
output pattern of discrete distributions endows DDN with two intriguing
properties: highly compressed representation and more general zero-shot
conditional generation. We demonstrate the efficacy of DDN and these intriguing
properties through experiments on CIFAR-10 and FFHQ.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00057">
<title>Generalization properties of contrastive world models. (arXiv:2401.00057v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00057</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work on object-centric world models aim to factorize representations
in terms of objects in a completely unsupervised or self-supervised manner.
Such world models are hypothesized to be a key component to address the
generalization problem. While self-supervision has shown improved performance
however, OOD generalization has not been systematically and explicitly tested.
In this paper, we conduct an extensive study on the generalization properties
of contrastive world model. We systematically test the model under a number of
different OOD generalization scenarios such as extrapolation to new object
attributes, introducing new conjunctions or new attributes. Our experiments
show that the contrastive world model fails to generalize under the different
OOD tests and the drop in performance depends on the extent to which the
samples are OOD. When visualizing the transition updates and convolutional
feature maps, we observe that any changes in object attributes (such as
previously unseen colors, shapes, or conjunctions of color and shape) breaks
down the factorization of object representations. Overall, our work highlights
the importance of object-centric representations for generalization and current
models are limited in their capacity to learn such representations required for
human-level generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramakrishnan_K/0/1/0/all/0/1&quot;&gt;Kandan Ramakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotton_R/0/1/0/all/0/1&quot;&gt;R. James Cotton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pitkow_X/0/1/0/all/0/1&quot;&gt;Xaq Pitkow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolias_A/0/1/0/all/0/1&quot;&gt;Andreas S. Tolias&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00065">
<title>Accelerating Process Development for 3D Printing of New Metal Alloys. (arXiv:2401.00065v1 [cond-mat.mtrl-sci])</title>
<link>http://arxiv.org/abs/2401.00065</link>
<description rdf:parseType="Literal">&lt;p&gt;Addressing the uncertainty and variability in the quality of 3D printed
metals can further the wide spread use of this technology. Process mapping for
new alloys is crucial for determining optimal process parameters that
consistently produce acceptable printing quality. Process mapping is typically
performed by conventional methods and is used for the design of experiments and
ex situ characterization of printed parts. On the other hand, in situ
approaches are limited because their observable features are limited and they
require complex high-cost setups to obtain temperature measurements to boost
accuracy. Our method relaxes these limitations by incorporating the temporal
features of molten metal dynamics during laser-metal interactions using video
vision transformers and high-speed imaging. Our approach can be used in
existing commercial machines and can provide in situ process maps for efficient
defect and variability quantification. The generalizability of the approach is
demonstrated by performing cross-dataset evaluations on alloys with different
compositions and intrinsic thermofluid properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Guirguis_D/0/1/0/all/0/1&quot;&gt;David Guirguis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Tucker_C/0/1/0/all/0/1&quot;&gt;Conrad Tucker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Beuth_J/0/1/0/all/0/1&quot;&gt;Jack Beuth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00067">
<title>Particle-Based Shape Modeling for Arbitrary Regions-of-Interest. (arXiv:2401.00067v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00067</link>
<description rdf:parseType="Literal">&lt;p&gt;Statistical Shape Modeling (SSM) is a quantitative method for analyzing
morphological variations in anatomical structures. These analyses often
necessitate building models on targeted anatomical regions of interest to focus
on specific morphological features. We propose an extension to \particle-based
shape modeling (PSM), a widely used SSM framework, to allow shape modeling to
arbitrary regions of interest. Existing methods to define regions of interest
are computationally expensive and have topological limitations. To address
these shortcomings, we use mesh fields to define free-form constraints, which
allow for delimiting arbitrary regions of interest on shape surfaces.
Furthermore, we add a quadratic penalty method to the model optimization to
enable computationally efficient enforcement of any combination of
cutting-plane and free-form constraints. We demonstrate the effectiveness of
this method on a challenging synthetic dataset and two medical datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morris_A/0/1/0/all/0/1&quot;&gt;Alan Morris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhabian_S/0/1/0/all/0/1&quot;&gt;Shireen Y. Elhabian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00080">
<title>A Large-Scale Re-identification Analysis in Sporting Scenarios: the Betrayal of Reaching a Critical Point. (arXiv:2401.00080v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00080</link>
<description rdf:parseType="Literal">&lt;p&gt;Re-identifying participants in ultra-distance running competitions can be
daunting due to the extensive distances and constantly changing terrain. To
overcome these challenges, computer vision techniques have been developed to
analyze runners&apos; faces, numbers on their bibs, and clothing. However, our study
presents a novel gait-based approach for runners&apos; re-identification (re-ID) by
leveraging various pre-trained human action recognition (HAR) models and loss
functions. Our results show that this approach provides promising results for
re-identifying runners in ultra-distance competitions. Furthermore, we
investigate the significance of distinct human body movements when athletes are
approaching their endurance limits and their potential impact on re-ID
accuracy. Our study examines how the recognition of a runner&apos;s gait is affected
by a competition&apos;s critical point (CP), defined as a moment of severe fatigue
and the point where the finish line comes into view, just a few kilometers away
from this location. We aim to determine how this CP can improve the accuracy of
athlete re-ID. Our experimental results demonstrate that gait recognition can
be significantly enhanced (up to a 9% increase in mAP) as athletes approach
this point. This highlights the potential of utilizing gait recognition in
real-world scenarios, such as ultra-distance competitions or long-duration
surveillance tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freire_Obregon_D/0/1/0/all/0/1&quot;&gt;David Freire-Obreg&amp;#xf3;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorenzo_Navarro_J/0/1/0/all/0/1&quot;&gt;Javier Lorenzo-Navarro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santana_O/0/1/0/all/0/1&quot;&gt;Oliverio J. Santana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Sosa_D/0/1/0/all/0/1&quot;&gt;Daniel Hern&amp;#xe1;ndez-Sosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castrillon_Santana_M/0/1/0/all/0/1&quot;&gt;Modesto Castrill&amp;#xf3;n-Santana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00094">
<title>Generating Enhanced Negatives for Training Language-Based Object Detectors. (arXiv:2401.00094v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00094</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent progress in language-based open-vocabulary object detection can be
largely attributed to finding better ways of leveraging large-scale data with
free-form text annotations. Training such models with a discriminative
objective function has proven successful, but requires good positive and
negative samples. However, the free-form nature and the open vocabulary of
object descriptions make the space of negatives extremely large. Prior works
randomly sample negatives or use rule-based techniques to build them. In
contrast, we propose to leverage the vast knowledge built into modern
generative models to automatically build negatives that are more relevant to
the original data. Specifically, we use large-language-models to generate
negative text descriptions, and text-to-image diffusion models to also generate
corresponding negative images. Our experimental analysis confirms the relevance
of the generated negative data, and its use in language-based detectors
improves performance on two complex benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shiyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Long Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+G_V/0/1/0/all/0/1&quot;&gt;Vijay Kumar B.G&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suh_Y/0/1/0/all/0/1&quot;&gt;Yumin Suh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1&quot;&gt;Dimitris N. Metaxas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1&quot;&gt;Manmohan Chandraker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulter_S/0/1/0/all/0/1&quot;&gt;Samuel Schulter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00110">
<title>Diffusion Model with Perceptual Loss. (arXiv:2401.00110v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00110</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models trained with mean squared error loss tend to generate
unrealistic samples. Current state-of-the-art models rely on classifier-free
guidance to improve sample quality, yet its surprising effectiveness is not
fully understood. In this paper, We show that the effectiveness of
classifier-free guidance partly originates from it being a form of implicit
perceptual guidance. As a result, we can directly incorporate perceptual loss
in diffusion training to improve sample quality. Since the score matching
objective used in diffusion training strongly resembles the denoising
autoencoder objective used in unsupervised training of perceptual networks, the
diffusion model itself is a perceptual network and can be used to generate
meaningful perceptual loss. We propose a novel self-perceptual objective that
results in diffusion models capable of generating more realistic samples. For
conditional generation, our method only improves sample quality without
entanglement with the conditional input and therefore does not sacrifice sample
diversity. Our method can also improve sample quality for unconditional
generation, which was not possible with classifier-free guidance before.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Shanchuan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00125">
<title>LLM-Assist: Enhancing Closed-Loop Planning with Language-Based Reasoning. (arXiv:2401.00125v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00125</link>
<description rdf:parseType="Literal">&lt;p&gt;Although planning is a crucial component of the autonomous driving stack,
researchers have yet to develop robust planning algorithms that are capable of
safely handling the diverse range of possible driving scenarios. Learning-based
planners suffer from overfitting and poor long-tail performance. On the other
hand, rule-based planners generalize well, but might fail to handle scenarios
that require complex driving maneuvers. To address these limitations, we
investigate the possibility of leveraging the common-sense reasoning
capabilities of Large Language Models (LLMs) such as GPT4 and Llama2 to
generate plans for self-driving vehicles. In particular, we develop a novel
hybrid planner that leverages a conventional rule-based planner in conjunction
with an LLM-based planner. Guided by commonsense reasoning abilities of LLMs,
our approach navigates complex scenarios which existing planners struggle with,
produces well-reasoned outputs while also remaining grounded through working
alongside the rule-based approach. Through extensive evaluation on the nuPlan
benchmark, we achieve state-of-the-art performance, outperforming all existing
pure learning- and rule-based methods across most metrics. Our code will be
available at https://llmassist.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharan_S/0/1/0/all/0/1&quot;&gt;S P Sharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pittaluga_F/0/1/0/all/0/1&quot;&gt;Francesco Pittaluga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+G_V/0/1/0/all/0/1&quot;&gt;Vijay Kumar B G&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1&quot;&gt;Manmohan Chandraker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00127">
<title>Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models. (arXiv:2401.00127v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00127</link>
<description rdf:parseType="Literal">&lt;p&gt;$ $The synergy of language and vision models has given rise to Large Language
and Vision Assistant models (LLVAs), designed to engage users in rich
conversational experiences intertwined with image-based queries. These
comprehensive multimodal models seamlessly integrate vision encoders with Large
Language Models (LLMs), expanding their applications in general-purpose
language and visual comprehension. The advent of Large Multimodal Models (LMMs)
heralds a new era in Artificial Intelligence (AI) assistance, extending the
horizons of AI utilization. This paper takes a unique perspective on LMMs,
exploring their efficacy in performing image classification tasks using
tailored prompts designed for specific datasets. We also investigate the LLVAs
zero-shot learning capabilities. Our study includes a benchmarking analysis
across four diverse datasets: MNIST, Cats Vs. Dogs, Hymnoptera (Ants Vs. Bees),
and an unconventional dataset comprising Pox Vs. Non-Pox skin images. The
results of our experiments demonstrate the model&apos;s remarkable performance,
achieving classification accuracies of 85\%, 100\%, 77\%, and 79\% for the
respective datasets without any fine-tuning. To bolster our analysis, we assess
the model&apos;s performance post fine-tuning for specific tasks. In one instance,
fine-tuning is conducted over a dataset comprising images of faces of children
with and without autism. Prior to fine-tuning, the model demonstrated a test
accuracy of 55\%, which significantly improved to 83\% post fine-tuning. These
results, coupled with our prior findings, underscore the transformative
potential of LLVAs and their versatile applications in real-world scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_A/0/1/0/all/0/1&quot;&gt;Ashhadul Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_M/0/1/0/all/0/1&quot;&gt;Md. Rafiul Biswas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaghouani_W/0/1/0/all/0/1&quot;&gt;Wajdi Zaghouani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belhaouari_S/0/1/0/all/0/1&quot;&gt;Samir Brahim Belhaouari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_Z/0/1/0/all/0/1&quot;&gt;Zubair Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00128">
<title>Quantifying intra-tumoral genetic heterogeneity of glioblastoma toward precision medicine using MRI and a data-inclusive machine learning algorithm. (arXiv:2401.00128v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00128</link>
<description rdf:parseType="Literal">&lt;p&gt;Glioblastoma (GBM) is one of the most aggressive and lethal human cancers.
Intra-tumoral genetic heterogeneity poses a significant challenge for
treatment. Biopsy is invasive, which motivates the development of non-invasive,
MRI-based machine learning (ML) models to quantify intra-tumoral genetic
heterogeneity for each patient. This capability holds great promise for
enabling better therapeutic selection to improve patient outcomes. We proposed
a novel Weakly Supervised Ordinal Support Vector Machine (WSO-SVM) to predict
regional genetic alteration status within each GBM tumor using MRI. WSO-SVM was
applied to a unique dataset of 318 image-localized biopsies with spatially
matched multiparametric MRI from 74 GBM patients. The model was trained to
predict the regional genetic alteration of three GBM driver genes (EGFR,
PDGFRA, and PTEN) based on features extracted from the corresponding region of
five MRI contrast images. For comparison, a variety of existing ML algorithms
were also applied. The classification accuracy of each gene was compared
between the different algorithms. The SHapley Additive exPlanations (SHAP)
method was further applied to compute contribution scores of different contrast
images. Finally, the trained WSO-SVM was used to generate prediction maps
within the tumoral area of each patient to help visualize the intra-tumoral
genetic heterogeneity. This study demonstrated the feasibility of using MRI and
WSO-SVM to enable non-invasive prediction of intra-tumoral regional genetic
alteration for each GBM patient, which can inform future adaptive therapies for
individualized oncology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lujia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hairong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DAngelo_F/0/1/0/all/0/1&quot;&gt;Fulvio D&amp;#x27;Angelo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Curtin_L/0/1/0/all/0/1&quot;&gt;Lee Curtin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sereduk_C/0/1/0/all/0/1&quot;&gt;Christopher P. Sereduk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leon_G/0/1/0/all/0/1&quot;&gt;Gustavo De Leon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singleton_K/0/1/0/all/0/1&quot;&gt;Kyle W. Singleton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urcuyo_J/0/1/0/all/0/1&quot;&gt;Javier Urcuyo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hawkins_Daarud_A/0/1/0/all/0/1&quot;&gt;Andrea Hawkins-Daarud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jackson_P/0/1/0/all/0/1&quot;&gt;Pamela R. Jackson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_C/0/1/0/all/0/1&quot;&gt;Chandan Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmerman_R/0/1/0/all/0/1&quot;&gt;Richard S. Zimmerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patra_D/0/1/0/all/0/1&quot;&gt;Devi P. Patra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bendok_B/0/1/0/all/0/1&quot;&gt;Bernard R. Bendok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1&quot;&gt;Kris A. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakaji_P/0/1/0/all/0/1&quot;&gt;Peter Nakaji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donev_K/0/1/0/all/0/1&quot;&gt;Kliment Donev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baxter_L/0/1/0/all/0/1&quot;&gt;Leslie C. Baxter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mrugala_M/0/1/0/all/0/1&quot;&gt;Maciej M. Mruga&amp;#x142;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ceccarelli_M/0/1/0/all/0/1&quot;&gt;Michele Ceccarelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iavarone_A/0/1/0/all/0/1&quot;&gt;Antonio Iavarone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swanson_K/0/1/0/all/0/1&quot;&gt;Kristin R. Swanson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1&quot;&gt;Nhan L. Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1&quot;&gt;Leland S. Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jing Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00135">
<title>Deep Radon Prior: A Fully Unsupervised Framework for Sparse-View CT Reconstruction. (arXiv:2401.00135v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.00135</link>
<description rdf:parseType="Literal">&lt;p&gt;Although sparse-view computed tomography (CT) has significantly reduced
radiation dose, it also introduces severe artifacts which degrade the image
quality. In recent years, deep learning-based methods for inverse problems have
made remarkable progress and have become increasingly popular in CT
reconstruction. However, most of these methods suffer several limitations:
dependence on high-quality training data, weak interpretability, etc. In this
study, we propose a fully unsupervised framework called Deep Radon Prior (DRP),
inspired by Deep Image Prior (DIP), to address the aforementioned limitations.
DRP introduces a neural network as an implicit prior into the iterative method,
thereby realizing cross-domain gradient feedback. During the reconstruction
process, the neural network is progressively optimized in multiple stages to
narrow the solution space in radon domain for the under-constrained imaging
protocol, and the convergence of the proposed method has been discussed in this
work. Compared with the popular pre-trained method, the proposed framework
requires no dataset and exhibits superior interpretability and generalization
ability. The experimental results demonstrate that the proposed method can
generate detailed images while effectively suppressing image
artifacts.Meanwhile, DRP achieves comparable or better performance than the
supervised methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yucheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Gang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xiang_X/0/1/0/all/0/1&quot;&gt;Xincheng Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cong_P/0/1/0/all/0/1&quot;&gt;Peng Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuewen Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00137">
<title>SSL-OTA: Unveiling Backdoor Threats in Self-Supervised Learning for Object Detection. (arXiv:2401.00137v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.00137</link>
<description rdf:parseType="Literal">&lt;p&gt;The extensive adoption of Self-supervised learning (SSL) has led to an
increased security threat from backdoor attacks. While existing research has
mainly focused on backdoor attacks in image classification, there has been
limited exploration into their implications for object detection. In this work,
we propose the first backdoor attack designed for object detection tasks in SSL
scenarios, termed Object Transform Attack (SSL-OTA). SSL-OTA employs a trigger
capable of altering predictions of the target object to the desired category,
encompassing two attacks: Data Poisoning Attack (NA) and Dual-Source Blending
Attack (DSBA). NA conducts data poisoning during downstream fine-tuning of the
object detector, while DSBA additionally injects backdoors into the pre-trained
encoder. We establish appropriate metrics and conduct extensive experiments on
benchmark datasets, demonstrating the effectiveness and utility of our proposed
attack. Notably, both NA and DSBA achieve high attack success rates (ASR) at
extremely low poisoning rates (0.5%). The results underscore the importance of
considering backdoor threats in SSL-based object detection and contribute a
novel perspective to the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qiannan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1&quot;&gt;Changchun Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1&quot;&gt;Liming Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Lu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhe Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Run Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chenhao Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00148">
<title>TPatch: A Triggered Physical Adversarial Patch. (arXiv:2401.00148v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.00148</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous vehicles increasingly utilize the vision-based perception module
to acquire information about driving environments and detect obstacles. Correct
detection and classification are important to ensure safe driving decisions.
Existing works have demonstrated the feasibility of fooling the perception
models such as object detectors and image classifiers with printed adversarial
patches. However, most of them are indiscriminately offensive to every passing
autonomous vehicle. In this paper, we propose TPatch, a physical adversarial
patch triggered by acoustic signals. Unlike other adversarial patches, TPatch
remains benign under normal circumstances but can be triggered to launch a
hiding, creating or altering attack by a designed distortion introduced by
signal injection attacks towards cameras. To avoid the suspicion of human
drivers and make the attack practical and robust in the real world, we propose
a content-based camouflage method and an attack robustness enhancement method
to strengthen it. Evaluations with three object detectors, YOLO V3/V5 and
Faster R-CNN, and eight image classifiers demonstrate the effectiveness of
TPatch in both the simulation and the real world. We also discuss possible
defenses at the sensor, algorithm, and system levels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wenjun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yushi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shibo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wenyuan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00151">
<title>CamPro: Camera-based Anti-Facial Recognition. (arXiv:2401.00151v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00151</link>
<description rdf:parseType="Literal">&lt;p&gt;The proliferation of images captured from millions of cameras and the
advancement of facial recognition (FR) technology have made the abuse of FR a
severe privacy threat. Existing works typically rely on obfuscation, synthesis,
or adversarial examples to modify faces in images to achieve anti-facial
recognition (AFR). However, the unmodified images captured by camera modules
that contain sensitive personally identifiable information (PII) could still be
leaked. In this paper, we propose a novel approach, CamPro, to capture inborn
AFR images. CamPro enables well-packed commodity camera modules to produce
images that contain little PII and yet still contain enough information to
support other non-sensitive vision applications, such as person detection.
Specifically, CamPro tunes the configuration setup inside the camera image
signal processor (ISP), i.e., color correction matrix and gamma correction, to
achieve AFR, and designs an image enhancer to keep the image quality for
possible human viewers. We implemented and validated CamPro on a
proof-of-concept camera, and our experiments demonstrate its effectiveness on
ten state-of-the-art black-box FR models. The results show that CamPro images
can significantly reduce face identification accuracy to 0.3\% while having
little impact on the targeted non-sensitive vision application. Furthermore, we
find that CamPro is resilient to adaptive attackers who have re-trained their
FR models using images generated by CamPro, even with full knowledge of
privacy-preserving ISP parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wenjun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiani Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yushi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wenyuan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00155">
<title>A comprehensive framework for occluded human pose estimation. (arXiv:2401.00155v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00155</link>
<description rdf:parseType="Literal">&lt;p&gt;Occlusion presents a significant challenge in human pose estimation. The
challenges posed by occlusion can be attributed to the following factors: 1)
Data: The collection and annotation of occluded human pose samples are
relatively challenging. 2) Feature: Occlusion can cause feature confusion due
to the high similarity between the target person and interfering individuals.
3) Inference: Robust inference becomes challenging due to the loss of complete
body structural information. The existing methods designed for occluded human
pose estimation usually focus on addressing only one of these factors. In this
paper, we propose a comprehensive framework DAG (Data, Attention, Graph) to
address the performance degradation caused by occlusion. Specifically, we
introduce the mask joints with instance paste data augmentation technique to
simulate occlusion scenarios. Additionally, an Adaptive Discriminative
Attention Module (ADAM) is proposed to effectively enhance the features of
target individuals. Furthermore, we present the Feature-Guided Multi-Hop GCN
(FGMP-GCN) to fully explore the prior knowledge of body structure and improve
pose estimation results. Through extensive experiments conducted on three
benchmark datasets for occluded human pose estimation, we demonstrate that the
proposed method outperforms existing methods. Code and data will be publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Linhao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Lin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xinxin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guangyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Kedong Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00159">
<title>Automatic hip osteoarthritis grading with uncertainty estimation from computed tomography using digitally-reconstructed radiographs. (arXiv:2401.00159v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.00159</link>
<description rdf:parseType="Literal">&lt;p&gt;Progression of hip osteoarthritis (hip OA) leads to pain and disability,
likely leading to surgical treatment such as hip arthroplasty at the terminal
stage. The severity of hip OA is often classified using the Crowe and
Kellgren-Lawrence (KL) classifications. However, as the classification is
subjective, we aimed to develop an automated approach to classify the disease
severity based on the two grades using digitally-reconstructed radiographs
(DRRs) from CT images. Automatic grading of the hip OA severity was performed
using deep learning-based models. The models were trained to predict the
disease grade using two grading schemes, i.e., predicting the Crowe and KL
grades separately, and predicting a new ordinal label combining both grades and
representing the disease progression of hip OA. The models were trained in
classification and regression settings. In addition, the model uncertainty was
estimated and validated as a predictor of classification accuracy. The models
were trained and validated on a database of 197 hip OA patients, and externally
validated on 52 patients. The model accuracy was evaluated using exact class
accuracy (ECA), one-neighbor class accuracy (ONCA), and balanced accuracy.The
deep learning models produced a comparable accuracy of approximately 0.65 (ECA)
and 0.95 (ONCA) in the classification and regression settings. The model
uncertainty was significantly larger in cases with large classification errors
(P&amp;lt;6e-3). In this study, an automatic approach for grading hip OA severity from
CT images was developed. The models have shown comparable performance with high
ONCA, which facilitates automated grading in large-scale CT databases and
indicates the potential for further disease progression analysis.
Classification accuracy was correlated with the model uncertainty, which would
allow for the prediction of classification errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Masuda_M/0/1/0/all/0/1&quot;&gt;Masachika Masuda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Soufi_M/0/1/0/all/0/1&quot;&gt;Mazen Soufi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Otake_Y/0/1/0/all/0/1&quot;&gt;Yoshito Otake&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Uemura_K/0/1/0/all/0/1&quot;&gt;Keisuke Uemura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kono_S/0/1/0/all/0/1&quot;&gt;Sotaro Kono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Takashima_K/0/1/0/all/0/1&quot;&gt;Kazuma Takashima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hamada_H/0/1/0/all/0/1&quot;&gt;Hidetoshi Hamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yi Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Takao_M/0/1/0/all/0/1&quot;&gt;Masaki Takao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Okada_S/0/1/0/all/0/1&quot;&gt;Seiji Okada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sugano_N/0/1/0/all/0/1&quot;&gt;Nobuhiko Sugano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sato_Y/0/1/0/all/0/1&quot;&gt;Yoshinobu Sato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00208">
<title>Inpaint4DNeRF: Promptable Spatio-Temporal NeRF Inpainting with Generative Diffusion Models. (arXiv:2401.00208v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00208</link>
<description rdf:parseType="Literal">&lt;p&gt;Current Neural Radiance Fields (NeRF) can generate photorealistic novel
views. For editing 3D scenes represented by NeRF, with the advent of generative
models, this paper proposes Inpaint4DNeRF to capitalize on state-of-the-art
stable diffusion models (e.g., ControlNet) for direct generation of the
underlying completed background content, regardless of static or dynamic. The
key advantages of this generative approach for NeRF inpainting are twofold.
First, after rough mask propagation, to complete or fill in previously occluded
content, we can individually generate a small subset of completed images with
plausible content, called seed images, from which simple 3D geometry proxies
can be derived. Second and the remaining problem is thus 3D multiview
consistency among all completed images, now guided by the seed images and their
3D proxies. Without other bells and whistles, our generative Inpaint4DNeRF
baseline framework is general which can be readily extended to 4D dynamic
NeRFs, where temporal consistency can be naturally handled in a similar way as
our multiview consistency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Han Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Haosen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruoxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chi-Keung Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1&quot;&gt;Yu-Wing Tai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00237">
<title>A Novel Approach for Defect Detection of Wind Turbine Blade Using Virtual Reality and Deep Learning. (arXiv:2401.00237v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00237</link>
<description rdf:parseType="Literal">&lt;p&gt;Wind turbines are subjected to continuous rotational stresses and unusual
external forces such as storms, lightning, strikes by flying objects, etc.,
which may cause defects in turbine blades. Hence, it requires a periodical
inspection to ensure proper functionality and avoid catastrophic failure. The
task of inspection is challenging due to the remote location and inconvenient
reachability by human inspection. Researchers used images with cropped defects
from the wind turbine in the literature. They neglected possible background
biases, which may hinder real-time and autonomous defect detection using aerial
vehicles such as drones or others. To overcome such challenges, in this paper,
we experiment with defect detection accuracy by having the defects with the
background using a two-step deep-learning methodology. In the first step, we
develop virtual models of wind turbines to synthesize the near-reality images
for four types of common defects - cracks, leading edge erosion, bending, and
light striking damage. The Unity perception package is used to generate wind
turbine blade defects images with variations in background, randomness, camera
angle, and light effects. In the second step, a customized U-Net architecture
is trained to classify and segment the defect in turbine blades. The outcomes
of U-Net architecture have been thoroughly tested and compared with 5-fold
validation datasets. The proposed methodology provides reasonable defect
detection accuracy, making it suitable for autonomous and remote inspection
through aerial vehicles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabbi_M/0/1/0/all/0/1&quot;&gt;Md Fazle Rabbi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emon_S/0/1/0/all/0/1&quot;&gt;Solayman Hossain Emon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishat_E/0/1/0/all/0/1&quot;&gt;Ehtesham Mahmud Nishat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzu-Liang/0/1/0/all/0/1&quot;&gt;Tzu-Liang&lt;/a&gt; (Bill) &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tseng/0/1/0/all/0/1&quot;&gt;Tseng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferdoushi_A/0/1/0/all/0/1&quot;&gt;Atira Ferdoushi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chun-Che Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Md Fashiar Rahman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00241">
<title>Image Super-resolution Reconstruction Network based on Enhanced Swin Transformer via Alternating Aggregation of Local-Global Features. (arXiv:2401.00241v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00241</link>
<description rdf:parseType="Literal">&lt;p&gt;The Swin Transformer image super-resolution reconstruction network only
relies on the long-range relationship of window attention and shifted window
attention to explore features. This mechanism has two limitations. On the one
hand, it only focuses on global features while ignoring local features. On the
other hand, it is only concerned with spatial feature interactions while
ignoring channel features and channel interactions, thus limiting its
non-linear mapping ability. To address the above limitations, this paper
proposes enhanced Swin Transformer modules via alternating aggregation of
local-global features. In the local feature aggregation stage, this paper
introduces shift convolution to realize the interaction between local spatial
information and channel information. This paper proposes a block sparse global
perception module in the global feature aggregation stage. This module
organizes the spatial information first, then sends the recombination
information into a spatial gating unit to implement the further interaction of
spatial and channel information. Then, a multi-scale self-attention module and
a low-parameter residual channel attention module are introduced to realize
information aggregation at different scales. Finally, the proposed network is
validated on five publicly available datasets. The experimental results show
that the proposed network outperforms the other state-of-the-art
super-resolution networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuming Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yingpin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Changhui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1&quot;&gt;Hanrong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1&quot;&gt;Binhui Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hui Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00247">
<title>Probing the Limits and Capabilities of Diffusion Models for the Anatomic Editing of Digital Twins. (arXiv:2401.00247v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00247</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerical simulations can model the physical processes that govern
cardiovascular device deployment. When such simulations incorporate digital
twins; computational models of patient-specific anatomy, they can expedite and
de-risk the device design process. Nonetheless, the exclusive use of
patient-specific data constrains the anatomic variability which can be
precisely or fully explored. In this study, we investigate the capacity of
Latent Diffusion Models (LDMs) to edit digital twins to create anatomic
variants, which we term digital siblings. Digital twins and their corresponding
siblings can serve as the basis for comparative simulations, enabling the study
of how subtle anatomic variations impact the simulated deployment of
cardiovascular devices, as well as the augmentation of virtual cohorts for
device assessment. However, while diffusion models have been characterized in
their ability to edit natural images, their capacity to anatomically edit
digital twins has yet to be studied. Using a case example centered on 3D
digital twins of cardiac anatomy, we implement various methods for generating
digital siblings and characterize them through morphological and topological
analyses. We specifically edit digital twins to introduce anatomic variation at
different spatial scales and within localized regions, demonstrating the
existence of bias towards common anatomic features. We further show that such
anatomic bias can be leveraged for virtual cohort augmentation through
selective editing, partially alleviating issues related to dataset imbalance
and lack of diversity. Our experimental framework thus delineates the limits
and capabilities of using latent diffusion models in synthesizing anatomic
variation for in silico trials.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadry_K/0/1/0/all/0/1&quot;&gt;Karim Kadry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Shreya Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nezami_F/0/1/0/all/0/1&quot;&gt;Farhad R. Nezami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edelman_E/0/1/0/all/0/1&quot;&gt;Elazer R. Edelman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00248">
<title>Promoting Segment Anything Model towards Highly Accurate Dichotomous Image Segmentation. (arXiv:2401.00248v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00248</link>
<description rdf:parseType="Literal">&lt;p&gt;Segmenting any object represents a crucial step towards achieving artificial
general intelligence, and the &quot;Segment Anything Model&quot; (SAM) has significantly
advanced the development of foundational models in computer vision. We have
high expectations regarding whether SAM can enhance highly accurate dichotomous
image segmentation. In fact, the evidence presented in this article
demonstrates that by inputting SAM with simple prompt boxes and utilizing the
results output by SAM as input for IS5Net, we can greatly improve the
effectiveness of highly accurate dichotomous image segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1&quot;&gt;Keren Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qijun Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00254">
<title>Masked Image Modeling via Dynamic Token Morphing. (arXiv:2401.00254v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00254</link>
<description rdf:parseType="Literal">&lt;p&gt;Masked Image Modeling (MIM) arises as a promising option for Vision
Transformers among various self-supervised learning (SSL) methods. The essence
of MIM lies in token-wise masked patch predictions, with targets patchified
from images; or generated by pre-trained tokenizers or models. We argue targets
from the pre-trained models usually exhibit spatial inconsistency, which makes
it excessively challenging for the model to follow to learn more discriminative
representations. To mitigate the issue, we introduce a novel self-supervision
signal based on Dynamic Token Morphing (DTM), which dynamically aggregates
contextually related tokens. DTM can be generally applied to various SSL
frameworks, yet we propose a simple MIM that employs DTM to effectively improve
the performance barely introducing extra training costs. Our experiments on
ImageNet-1K and ADE20K evidently demonstrate the superiority of our methods.
Furthermore, the comparative evaluation of iNaturalist and Fine-grained Visual
Classification datasets further validates the transferability of our method on
various downstream tasks. Our code will be released publicly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taekyung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1&quot;&gt;Dongyoon Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heo_B/0/1/0/all/0/1&quot;&gt;Byeongho Heo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00260">
<title>GazeCLIP: Towards Enhancing Gaze Estimation via Text Guidance. (arXiv:2401.00260v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00260</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past decade, visual gaze estimation has garnered growing attention
within the research community, thanks to its wide-ranging application
scenarios. While existing estimation approaches have achieved remarkable
success in enhancing prediction accuracy, they primarily infer gaze directions
from single-image signals and discard the huge potentials of the currently
dominant text guidance. Notably, visual-language collaboration has been
extensively explored across a range of visual tasks, such as image synthesis
and manipulation, leveraging the remarkable transferability of large-scale
Contrastive Language-Image Pre-training (CLIP) model. Nevertheless, existing
gaze estimation approaches ignore the rich semantic cues conveyed by linguistic
signals and priors in CLIP feature space, thereby yielding performance
setbacks. In pursuit of making up this gap, we delve deeply into the text-eye
collaboration protocol and introduce a novel gaze estimation framework in this
paper, referred to as GazeCLIP. Specifically, we intricately design a
linguistic description generator to produce text signals with coarse
directional cues. Additionally, a CLIP-based backbone that excels in
characterizing text-eye pairs for gaze estimation is presented. This is
followed by the implementation of a fine-grained multi-modal fusion module
aimed at modeling the interrelationships between heterogeneous inputs.
Extensive experiments on three challenging datasets demonstrate the superiority
of the proposed GazeCLIP which surpasses the previous approaches and achieves
the state-of-the-art estimation accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_H/0/1/0/all/0/1&quot;&gt;Hao Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mingjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chuanghui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunhua Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00268">
<title>COMMA: Co-Articulated Multi-Modal Learning. (arXiv:2401.00268v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00268</link>
<description rdf:parseType="Literal">&lt;p&gt;Pretrained large-scale vision-language models such as CLIP have demonstrated
excellent generalizability over a series of downstream tasks. However, they are
sensitive to the variation of input text prompts and need a selection of prompt
templates to achieve satisfactory performance. Recently, various methods have
been proposed to dynamically learn the prompts as the textual inputs to avoid
the requirements of laboring hand-crafted prompt engineering in the fine-tuning
process. We notice that these methods are suboptimal in two aspects. First, the
prompts of the vision and language branches in these methods are usually
separated or uni-directionally correlated. Thus, the prompts of both branches
are not fully correlated and may not provide enough guidance to align the
representations of both branches. Second, it&apos;s observed that most previous
methods usually achieve better performance on seen classes but cause
performance degeneration on unseen classes compared to CLIP. This is because
the essential generic knowledge learned in the pretraining stage is partly
forgotten in the fine-tuning process. In this paper, we propose Co-Articulated
Multi-Modal Learning (COMMA) to handle the above limitations. Especially, our
method considers prompts from both branches to generate the prompts to enhance
the representation alignment of both branches. Besides, to alleviate forgetting
about the essential knowledge, we minimize the feature discrepancy between the
learned prompts and the embeddings of hand-crafted prompts in the pre-trained
CLIP in the late transformer layers. We evaluate our method across three
representative tasks of generalization to novel classes, new target datasets
and unseen domain shifts. Experimental results demonstrate the superiority of
our method by exhibiting a favorable performance boost upon all tasks with high
efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1&quot;&gt;Lianyu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Liqing Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zekang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pun_C/0/1/0/all/0/1&quot;&gt;Chi-Man Pun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1&quot;&gt;Wei Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00271">
<title>HybridGait: A Benchmark for Spatial-Temporal Cloth-Changing Gait Recognition with Hybrid Explorations. (arXiv:2401.00271v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00271</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing gait recognition benchmarks mostly include minor clothing variations
in the laboratory environments, but lack persistent changes in appearance over
time and space. In this paper, we propose the first in-the-wild benchmark
CCGait for cloth-changing gait recognition, which incorporates diverse clothing
changes, indoor and outdoor scenes, and multi-modal statistics over 92 days. To
further address the coupling effect of clothing and viewpoint variations, we
propose a hybrid approach HybridGait that exploits both temporal dynamics and
the projected 2D information of 3D human meshes. Specifically, we introduce a
Canonical Alignment Spatial-Temporal Transformer (CA-STT) module to encode
human joint position-aware features, and fully exploit 3D dense priors via a
Silhouette-guided Deformation with 3D-2D Appearance Projection (SilD) strategy.
Our contributions are twofold: we provide a challenging benchmark CCGait that
captures realistic appearance changes across an expanded and space, and we
propose a hybrid framework HybridGait that outperforms prior works on CCGait
and Gait3D benchmarks. Our project page is available at
https://github.com/HCVLab/HybridGait.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yilan Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chunlin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_R/0/1/0/all/0/1&quot;&gt;Ruiyang Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Ye Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuexin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yanwei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingya Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00275">
<title>An $\ell^1$-Plug-and-Play Approach for Magnetic Particle Imaging Using a Zero Shot Denoiser with Validation on the 3D Open MPI Dataset. (arXiv:2401.00275v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.00275</link>
<description rdf:parseType="Literal">&lt;p&gt;Magnetic particle imaging (MPI) is an emerging medical imaging modality which
has gained increasing interest in recent years. Among the benefits of MPI are
its high temporal resolution, and that the technique does not expose the
specimen to any kind of ionizing radiation. It is based on the non-linear
response of magnetic nanoparticles to an applied magnetic field. From the
electric signal measured in receive coils, the particle concentration has to be
reconstructed. Due to the ill-posedness of the reconstruction problem, various
regularization methods have been proposed for reconstruction ranging from early
stopping methods, via classical Tikhonov regularization and iterative methods
to modern machine learning approaches. In this work, we contribute to the
latter class: we propose a plug-and-play approach based on a generic zero-shot
denoiser with an $\ell^1$-prior. Moreover, we develop parameter selection
strategies. Finally, we quantitatively and qualitatively evaluate the proposed
algorithmic scheme on the 3D Open MPI data set with different levels of
preprocessing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gapyak_V/0/1/0/all/0/1&quot;&gt;Vladyslav Gapyak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rentschler_C/0/1/0/all/0/1&quot;&gt;Corinna Rentschler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marz_T/0/1/0/all/0/1&quot;&gt;Thomas M&amp;#xe4;rz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Weinmann_A/0/1/0/all/0/1&quot;&gt;Andreas Weinmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00285">
<title>BusReF: Infrared-Visible images registration and fusion focus on reconstructible area using one set of features. (arXiv:2401.00285v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00285</link>
<description rdf:parseType="Literal">&lt;p&gt;In a scenario where multi-modal cameras are operating together, the problem
of working with non-aligned images cannot be avoided. Yet, existing image
fusion algorithms rely heavily on strictly registered input image pairs to
produce more precise fusion results, as a way to improve the performance of
downstream high-level vision tasks. In order to relax this assumption, one can
attempt to register images first. However, the existing methods for registering
multiple modalities have limitations, such as complex structures and reliance
on significant semantic information. This paper aims to address the problem of
image registration and fusion in a single framework, called BusRef. We focus on
Infrared-Visible image registration and fusion task (IVRF). In this framework,
the input unaligned image pairs will pass through three stages: Coarse
registration, Fine registration and Fusion. It will be shown that the unified
approach enables more robust IVRF. We also propose a novel training and
evaluation strategy, involving the use of masks to reduce the influence of
non-reconstructible regions on the loss functions, which greatly improves the
accuracy and robustness of the fusion task. Last but not least, a
gradient-aware fusion network is designed to preserve the complementary
information. The advanced performance of this algorithm is demonstrated by
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zeyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tianyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaojun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1&quot;&gt;Josef Kittler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00314">
<title>GAN-GA: A Generative Model based on Genetic Algorithm for Medical Image Generation. (arXiv:2401.00314v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.00314</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical imaging is an essential tool for diagnosing and treating diseases.
However, lacking medical images can lead to inaccurate diagnoses and
ineffective treatments. Generative models offer a promising solution for
addressing medical image shortage problems due to their ability to generate new
data from existing datasets and detect anomalies in this data. Data
augmentation with position augmentation methods like scaling, cropping,
flipping, padding, rotation, and translation could lead to more overfitting in
domains with little data, such as medical image data. This paper proposes the
GAN-GA, a generative model optimized by embedding a genetic algorithm. The
proposed model enhances image fidelity and diversity while preserving
distinctive features. The proposed medical image synthesis approach improves
the quality and fidelity of medical images, an essential aspect of image
interpretation. To evaluate synthesized images: Frechet Inception Distance
(FID) is used. The proposed GAN-GA model is tested by generating Acute
lymphoblastic leukemia (ALL) medical images, an image dataset, and is the first
time to be used in generative models. Our results were compared to those of
InfoGAN as a baseline model. The experimental results show that the proposed
optimized GAN-GA enhances FID scores by about 6.8\%, especially in earlier
training epochs. The source code and dataset will be available at:
https://github.com/Mustafa-AbdulRazek/InfoGAN-GA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+AbdulRazek_M/0/1/0/all/0/1&quot;&gt;M. AbdulRazek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khoriba_G/0/1/0/all/0/1&quot;&gt;G. Khoriba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Belal_M/0/1/0/all/0/1&quot;&gt;M. Belal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00320">
<title>DXAI: Explaining Classification by Image Decomposition. (arXiv:2401.00320v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00320</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new way to explain and to visualize neural network
classification through a decomposition-based explainable AI (DXAI). Instead of
providing an explanation heatmap, our method yields a decomposition of the
image into class-agnostic and class-distinct parts, with respect to the data
and chosen classifier. Following a fundamental signal processing paradigm of
analysis and synthesis, the original image is the sum of the decomposed parts.
We thus obtain a radically different way of explaining classification. The
class-agnostic part ideally is composed of all image features which do not
posses class information, where the class-distinct part is its complementary.
This new visualization can be more helpful and informative in certain
scenarios, especially when the attributes are dense, global and additive in
nature, for instance, when colors or textures are essential for class
distinction. Code is available at https://github.com/dxai2024/dxai.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadar_E/0/1/0/all/0/1&quot;&gt;Elnatan Kadar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilboa_G/0/1/0/all/0/1&quot;&gt;Guy Gilboa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00334">
<title>Explainability-Driven Leaf Disease Classification using Adversarial Training and Knowledge Distillation. (arXiv:2401.00334v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00334</link>
<description rdf:parseType="Literal">&lt;p&gt;This work focuses on plant leaf disease classification and explores three
crucial aspects: adversarial training, model explainability, and model
compression. The models&apos; robustness against adversarial attacks is enhanced
through adversarial training, ensuring accurate classification even in the
presence of threats. Leveraging explainability techniques, we gain insights
into the model&apos;s decision-making process, improving trust and transparency.
Additionally, we explore model compression techniques to optimize computational
efficiency while maintaining classification performance. Through our
experiments, we determine that on a benchmark dataset, the robustness can be
the price of the classification accuracy with performance reductions of 3%-20%
for regular tests and gains of 50%-70% for adversarial attack tests. We also
demonstrate that a student model can be 15-25 times more computationally
efficient for a slight performance reduction, distilling the knowledge of more
complex models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Echim_S/0/1/0/all/0/1&quot;&gt;Sebastian-Vasile Echim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taiatu_I/0/1/0/all/0/1&quot;&gt;Iulian-Marius T&amp;#x103;iatu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cercel_D/0/1/0/all/0/1&quot;&gt;Dumitru-Clementin Cercel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pop_F/0/1/0/all/0/1&quot;&gt;Florin Pop&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00343">
<title>SHARE: Single-view Human Adversarial REconstruction. (arXiv:2401.00343v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00343</link>
<description rdf:parseType="Literal">&lt;p&gt;The accuracy of 3D Human Pose and Shape reconstruction (HPS) from an image is
progressively improving. Yet, no known method is robust across all image
distortion. To address issues due to variations of camera poses, we introduce
SHARE, a novel fine-tuning method that utilizes adversarial data augmentation
to enhance the robustness of existing HPS techniques. We perform a
comprehensive analysis on the impact of camera poses on HPS reconstruction
outcomes. We first generated large-scale image datasets captured systematically
from diverse camera perspectives. We then established a mapping between camera
poses and reconstruction errors as a continuous function that characterizes the
relationship between camera poses and HPS quality. Leveraging this
representation, we introduce RoME (Regions of Maximal Error), a novel sampling
technique for our adversarial fine-tuning method.
&lt;/p&gt;
&lt;p&gt;The SHARE framework is generalizable across various single-view HPS methods
and we demonstrate its performance on HMR, SPIN, PARE, CLIFF and ExPose. Our
results illustrate a reduction in mean joint errors across single-view HPS
techniques, for images captured from multiple camera positions without
compromising their baseline performance. In many challenging cases, our method
surpasses the performance of existing models, highlighting its practical
significance for diverse real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Revankar_S/0/1/0/all/0/1&quot;&gt;Shreelekha Revankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1&quot;&gt;Shijia Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yu Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Junbang Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Huaishu Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Ming Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00365">
<title>HQ-VAE: Hierarchical Discrete Representation Learning with Variational Bayes. (arXiv:2401.00365v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00365</link>
<description rdf:parseType="Literal">&lt;p&gt;Vector quantization (VQ) is a technique to deterministically learn features
with discrete codebook representations. It is commonly performed with a
variational autoencoding model, VQ-VAE, which can be further extended to
hierarchical structures for making high-fidelity reconstructions. However, such
hierarchical extensions of VQ-VAE often suffer from the codebook/layer collapse
issue, where the codebook is not efficiently used to express the data, and
hence degrades reconstruction accuracy. To mitigate this problem, we propose a
novel unified framework to stochastically learn hierarchical discrete
representation on the basis of the variational Bayes framework, called
hierarchically quantized variational autoencoder (HQ-VAE). HQ-VAE naturally
generalizes the hierarchical variants of VQ-VAE, such as VQ-VAE-2 and
residual-quantized VAE (RQ-VAE), and provides them with a Bayesian training
scheme. Our comprehensive experiments on image datasets show that HQ-VAE
enhances codebook usage and improves reconstruction performance. We also
validated HQ-VAE in terms of its applicability to a different modality with an
audio dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takida_Y/0/1/0/all/0/1&quot;&gt;Yuhta Takida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ikemiya_Y/0/1/0/all/0/1&quot;&gt;Yukara Ikemiya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shibuya_T/0/1/0/all/0/1&quot;&gt;Takashi Shibuya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shimada_K/0/1/0/all/0/1&quot;&gt;Kazuki Shimada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1&quot;&gt;Woosung Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1&quot;&gt;Chieh-Hsin Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murata_N/0/1/0/all/0/1&quot;&gt;Naoki Murata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uesaka_T/0/1/0/all/0/1&quot;&gt;Toshimitsu Uesaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uchida_K/0/1/0/all/0/1&quot;&gt;Kengo Uchida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1&quot;&gt;Wei-Hsiang Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitsufuji_Y/0/1/0/all/0/1&quot;&gt;Yuki Mitsufuji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00370">
<title>UGPNet: Universal Generative Prior for Image Restoration. (arXiv:2401.00370v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00370</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent image restoration methods can be broadly categorized into two classes:
(1) regression methods that recover the rough structure of the original image
without synthesizing high-frequency details and (2) generative methods that
synthesize perceptually-realistic high-frequency details even though the
resulting image deviates from the original structure of the input. While both
directions have been extensively studied in isolation, merging their benefits
with a single framework has been rarely studied. In this paper, we propose
UGPNet, a universal image restoration framework that can effectively achieve
the benefits of both approaches by simply adopting a pair of an existing
regression model and a generative model. UGPNet first restores the image
structure of a degraded input using a regression model and synthesizes a
perceptually-realistic image with a generative model on top of the regressed
output. UGPNet then combines the regressed output and the synthesized output,
resulting in a final result that faithfully reconstructs the structure of the
original image in addition to perceptually-realistic textures. Our extensive
experiments on deblurring, denoising, and super-resolution demonstrate that
UGPNet can successfully exploit both regression and generative methods for
high-fidelity image restoration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hwayoon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_K/0/1/0/all/0/1&quot;&gt;Kyoungkook Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyeongmin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1&quot;&gt;Seung-Hwan Baek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1&quot;&gt;Sunghyun Cho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00371">
<title>Multi-Granularity Representation Learning for Sketch-based Dynamic Face Image Retrieval. (arXiv:2401.00371v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00371</link>
<description rdf:parseType="Literal">&lt;p&gt;In specific scenarios, face sketch can be used to identify a person. However,
drawing a face sketch often requires exceptional skill and is time-consuming,
limiting its widespread applications in actual scenarios. The new framework of
sketch less face image retrieval (SLFIR)[1] attempts to overcome the barriers
by providing a means for humans and machines to interact during the drawing
process. Considering SLFIR problem, there is a large gap between a partial
sketch with few strokes and any whole face photo, resulting in poor performance
at the early stages. In this study, we propose a multigranularity (MG)
representation learning (MGRL) method to address the SLFIR problem, in which we
learn the representation of different granularity regions for a partial sketch,
and then, by combining all MG regions of the sketches and images, the final
distance was determined. In the experiments, our method outperformed
state-of-the-art baselines in terms of early retrieval on two accessible
datasets. Codes are available at https://github.com/ddw2AIGROUP2CQUPT/MGRL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1&quot;&gt;Dawei Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1&quot;&gt;Shiyu Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guoyin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00374">
<title>EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Masked Audio Gesture Modeling. (arXiv:2401.00374v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00374</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose EMAGE, a framework to generate full-body human gestures from audio
and masked gestures, encompassing facial, local body, hands, and global
movements. To achieve this, we first introduce BEATX (BEAT-SMPLX-FLAME), a new
mesh-level holistic co-speech dataset. BEATX combines MoShed SMPLX body with
FLAME head parameters and further refines the modeling of head, neck, and
finger movements, offering a community-standardized, high-quality 3D motion
captured dataset. EMAGE leverages masked body gesture priors during training to
boost inference performance. It involves a Masked Audio Gesture Transformer,
facilitating joint training on audio-to-gesture generation and masked gesture
reconstruction to effectively encode audio and body gesture hints. Encoded body
hints from masked gestures are then separately employed to generate facial and
body movements. Moreover, EMAGE adaptively merges speech features from the
audio&apos;s rhythm and content and utilizes four compositional VQ-VAEs to enhance
the results&apos; fidelity and diversity. Experiments demonstrate that EMAGE
generates holistic gestures with state-of-the-art performance and is flexible
in accepting predefined spatial-temporal gesture inputs, generating complete,
audio-synchronized results. Our code and dataset are available at
https://pantomatrix.github.io/EMAGE/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haiyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zihao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Becherini_G/0/1/0/all/0/1&quot;&gt;Giorgio Becherini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yichen Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_M/0/1/0/all/0/1&quot;&gt;Mingyang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;You Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1&quot;&gt;Xuefei Zhe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iwamoto_N/0/1/0/all/0/1&quot;&gt;Naoya Iwamoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1&quot;&gt;Bo Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1&quot;&gt;Michael J. Black&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00390">
<title>Horizontal Federated Computer Vision. (arXiv:2401.00390v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00390</link>
<description rdf:parseType="Literal">&lt;p&gt;In the modern world, the amount of visual data recorded has been rapidly
increasing. In many cases, data is stored in geographically distinct locations
and thus requires a large amount of time and space to consolidate. Sometimes,
there are also regulations for privacy protection which prevent data
consolidation. In this work, we present federated implementations for object
detection and recognition using a federated Faster R-CNN (FRCNN) and image
segmentation using a federated Fully Convolutional Network (FCN). Our FRCNN was
trained on 5000 examples of the COCO2017 dataset while our FCN was trained on
the entire train set of the CamVid dataset. The proposed federated models
address the challenges posed by the increasing volume and decentralized nature
of visual data, offering efficient solutions in compliance with privacy
regulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandal_P/0/1/0/all/0/1&quot;&gt;Paul K. Mandal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leo_C/0/1/0/all/0/1&quot;&gt;Cole Leo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hurley_C/0/1/0/all/0/1&quot;&gt;Connor Hurley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00391">
<title>Controllable Safety-Critical Closed-loop Traffic Simulation via Guided Diffusion. (arXiv:2401.00391v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.00391</link>
<description rdf:parseType="Literal">&lt;p&gt;Evaluating the performance of autonomous vehicle planning algorithms
necessitates simulating long-tail traffic scenarios. Traditional methods for
generating safety-critical scenarios often fall short in realism and
controllability. Furthermore, these techniques generally neglect the dynamics
of agent interactions. To mitigate these limitations, we introduce a novel
closed-loop simulation framework rooted in guided diffusion models. Our
approach yields two distinct advantages: 1) the generation of realistic
long-tail scenarios that closely emulate real-world conditions, and 2) enhanced
controllability, enabling more comprehensive and interactive evaluations. We
achieve this through novel guidance objectives that enhance road progress while
lowering collision and off-road rates. We develop a novel approach to simulate
safety-critical scenarios through an adversarial term in the denoising process,
which allows the adversarial agent to challenge a planner with plausible
maneuvers, while all agents in the scene exhibit reactive and realistic
behaviors. We validate our framework empirically using the NuScenes dataset,
demonstrating improvements in both realism and controllability. These findings
affirm that guided diffusion models provide a robust and versatile foundation
for safety-critical, interactive traffic simulation, extending their utility
across the broader landscape of autonomous driving. For additional resources
and demonstrations, visit our project page at https://safe-sim.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1&quot;&gt;Wei-Jer Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pittaluga_F/0/1/0/all/0/1&quot;&gt;Francesco Pittaluga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1&quot;&gt;Masayoshi Tomizuka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1&quot;&gt;Wei Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1&quot;&gt;Manmohan Chandraker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00393">
<title>Generative Model-Driven Synthetic Training Image Generation: An Approach to Cognition in Rail Defect Detection. (arXiv:2401.00393v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00393</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in cognitive computing, with the integration of deep
learning techniques, have facilitated the development of intelligent cognitive
systems (ICS). This is particularly beneficial in the context of rail defect
detection, where the ICS would emulate human-like analysis of image data for
defect patterns. Despite the success of Convolutional Neural Networks (CNN) in
visual defect classification, the scarcity of large datasets for rail defect
detection remains a challenge due to infrequent accident events that would
result in defective parts and images. Contemporary researchers have addressed
this data scarcity challenge by exploring rule-based and generative data
augmentation models. Among these, Variational Autoencoder (VAE) models can
generate realistic data without extensive baseline datasets for noise modeling.
This study proposes a VAE-based synthetic image generation technique for rail
defects, incorporating weight decay regularization and image reconstruction
loss to prevent overfitting. The proposed method is applied to create a
synthetic dataset for the Canadian Pacific Railway (CPR) with just 50 real
samples across five classes. Remarkably, 500 synthetic samples are generated
with a minimal reconstruction loss of 0.021. A Visual Transformer (ViT) model
underwent fine-tuning using this synthetic CPR dataset, achieving high accuracy
rates (98%-99%) in classifying the five defect classes. This research offers a
promising solution to the data scarcity challenge in rail defect detection,
showcasing the potential for robust ICS development in this domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferdousi_R/0/1/0/all/0/1&quot;&gt;Rahatara Ferdousi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chunsheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1&quot;&gt;M. Anwar Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laamarti_F/0/1/0/all/0/1&quot;&gt;Fedwa Laamarti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1&quot;&gt;M. Shamim Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saddik_A/0/1/0/all/0/1&quot;&gt;Abdulmotaleb El Saddik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00403">
<title>Client-wise Modality Selection for Balanced Multi-modal Federated Learning. (arXiv:2401.00403v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00403</link>
<description rdf:parseType="Literal">&lt;p&gt;Selecting proper clients to participate in the iterative federated learning
(FL) rounds is critical to effectively harness a broad range of distributed
datasets. Existing client selection methods simply consider the variability
among FL clients with uni-modal data, however, have yet to consider clients
with multi-modalities. We reveal that traditional client selection scheme in
MFL may suffer from a severe modality-level bias, which impedes the
collaborative exploitation of multi-modal data, leading to insufficient local
data exploration and global aggregation. To tackle this challenge, we propose a
Client-wise Modality Selection scheme for MFL (CMSFed) that can comprehensively
utilize information from each modality via avoiding such client selection bias
caused by modality imbalance. Specifically, in each MFL round, the local data
from different modalities are selectively employed to participate in local
training and aggregation to mitigate potential modality imbalance of the global
model. To approximate the fully aggregated model update in a balanced way, we
introduce a novel local training loss function to enhance the weak modality and
align the divergent feature spaces caused by inconsistent modality adoption
strategies for different clients simultaneously. Then, a modality-level
gradient decoupling method is designed to derive respective submodular
functions to maintain the gradient diversity during the selection progress and
balance MFL according to local modality imbalance in each iteration. Our
extensive experiments showcase the superiority of CMSFed over baselines and its
effectiveness in multi-modal data exploitation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yunfeng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wenchao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haozhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_P/0/1/0/all/0/1&quot;&gt;Penghui Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Song Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00405">
<title>Generalizing Single-View 3D Shape Retrieval to Occlusions and Unseen Objects. (arXiv:2401.00405v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00405</link>
<description rdf:parseType="Literal">&lt;p&gt;Single-view 3D shape retrieval is a challenging task that is increasingly
important with the growth of available 3D data. Prior work that has studied
this task has not focused on evaluating how realistic occlusions impact
performance, and how shape retrieval methods generalize to scenarios where
either the target 3D shape database contains unseen shapes, or the input image
contains unseen objects. In this paper, we systematically evaluate single-view
3D shape retrieval along three different axes: the presence of object
occlusions and truncations, generalization to unseen 3D shape data, and
generalization to unseen objects in the input images. We standardize two
existing datasets of real images and propose a dataset generation pipeline to
produce a synthetic dataset of scenes with multiple objects exhibiting
realistic occlusions. Our experiments show that training on occlusion-free data
as was commonly done in prior work leads to significant performance degradation
for inputs with occlusion. We find that that by first pretraining on our
synthetic dataset with occlusions and then finetuning on real data, we can
significantly outperform models from prior work and demonstrate robustness to
both unseen 3D shapes and unseen objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qirui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1&quot;&gt;Daniel Ritchie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1&quot;&gt;Manolis Savva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1&quot;&gt;Angel X. Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00406">
<title>Low-cost Geometry-based Eye Gaze Detection using Facial Landmarks Generated through Deep Learning. (arXiv:2401.00406v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00406</link>
<description rdf:parseType="Literal">&lt;p&gt;Introduction: In the realm of human-computer interaction and behavioral
research, accurate real-time gaze estimation is critical. Traditional methods
often rely on expensive equipment or large datasets, which are impractical in
many scenarios. This paper introduces a novel, geometry-based approach to
address these challenges, utilizing consumer-grade hardware for broader
applicability. Methods: We leverage novel face landmark detection neural
networks capable of fast inference on consumer-grade chips to generate accurate
and stable 3D landmarks of the face and iris. From these, we derive a small set
of geometry-based descriptors, forming an 8-dimensional manifold representing
the eye and head movements. These descriptors are then used to formulate linear
equations for predicting eye-gaze direction. Results: Our approach demonstrates
the ability to predict gaze with an angular error of less than 1.9 degrees,
rivaling state-of-the-art systems while operating in real-time and requiring
negligible computational resources. Conclusion: The developed method marks a
significant step forward in gaze estimation technology, offering a highly
accurate, efficient, and accessible alternative to traditional systems. It
opens up new possibilities for real-time applications in diverse fields, from
gaming to psychological research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_E/0/1/0/all/0/1&quot;&gt;Esther Enhui Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;John Enzhou Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Joseph Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jacob Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1&quot;&gt;Runzhou Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00409">
<title>A Two-stream Hybrid CNN-Transformer Network for Skeleton-based Human Interaction Recognition. (arXiv:2401.00409v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00409</link>
<description rdf:parseType="Literal">&lt;p&gt;Human Interaction Recognition is the process of identifying interactive
actions between multiple participants in a specific situation. The aim is to
recognise the action interactions between multiple entities and their meaning.
Many single Convolutional Neural Network has issues, such as the inability to
capture global instance interaction features or difficulty in training, leading
to ambiguity in action semantics. In addition, the computational complexity of
the Transformer cannot be ignored, and its ability to capture local information
and motion features in the image is poor. In this work, we propose a Two-stream
Hybrid CNN-Transformer Network (THCT-Net), which exploits the local specificity
of CNN and models global dependencies through the Transformer. CNN and
Transformer simultaneously model the entity, time and space relationships
between interactive entities respectively. Specifically, Transformer-based
stream integrates 3D convolutions with multi-head self-attention to learn
inter-token correlations; We propose a new multi-branch CNN framework for
CNN-based streams that automatically learns joint spatio-temporal features from
skeleton sequences. The convolutional layer independently learns the local
features of each joint neighborhood and aggregates the features of all joints.
And the raw skeleton coordinates as well as their temporal difference are
integrated with a dual-branch paradigm to fuse the motion features of the
skeleton. Besides, a residual structure is added to speed up training
convergence. Finally, the recognition results of the two branches are fused
using parallel splicing. Experimental results on diverse and challenging
datasets, demonstrate that the proposed method can better comprehend and infer
the meaning and context of various actions, outperforming state-of-the-art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_R/0/1/0/all/0/1&quot;&gt;Ruoqi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jianqin Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00414">
<title>Is It Possible to Backdoor Face Forgery Detection with Natural Triggers?. (arXiv:2401.00414v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00414</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have significantly improved the performance of face
forgery detection models in discriminating Artificial Intelligent Generated
Content (AIGC). However, their security is significantly threatened by the
injection of triggers during model training (i.e., backdoor attacks). Although
existing backdoor defenses and manual data selection can mitigate those using
human-eye-sensitive triggers, such as patches or adversarial noises, the more
challenging natural backdoor triggers remain insufficiently researched. To
further investigate natural triggers, we propose a novel analysis-by-synthesis
backdoor attack against face forgery detection models, which embeds natural
triggers in the latent space. We thoroughly study such backdoor vulnerability
from two perspectives: (1) Model Discrimination (Optimization-Based Trigger):
we adopt a substitute detection model and find the trigger by minimizing the
cross-entropy loss; (2) Data Distribution (Custom Trigger): we manipulate the
uncommon facial attributes in the long-tailed distribution to generate poisoned
samples without the supervision from detection models. Furthermore, to
completely evaluate the detection models towards the latest AIGC, we utilize
both state-of-the-art StyleGAN and Stable Diffusion for trigger generation.
Finally, these backdoor triggers introduce specific semantic features to the
generated poisoned samples (e.g., skin textures and smile), which are more
natural and robust. Extensive experiments show that our method is superior from
three levels: (1) Attack Success Rate: ours achieves a high attack success rate
(over 99%) and incurs a small model accuracy drop (below 0.2%) with a low
poisoning rate (less than 3%); (2) Backdoor Defense: ours shows better robust
performance when faced with existing backdoor defense methods; (3) Human
Inspection: ours is less human-eye-sensitive from a comprehensive user study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiaoxuan Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Songlin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Ziwen He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jing Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00416">
<title>SVFAP: Self-supervised Video Facial Affect Perceiver. (arXiv:2401.00416v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00416</link>
<description rdf:parseType="Literal">&lt;p&gt;Video-based facial affect analysis has recently attracted increasing
attention owing to its critical role in human-computer interaction. Previous
studies mainly focus on developing various deep learning architectures and
training them in a fully supervised manner. Although significant progress has
been achieved by these supervised methods, the longstanding lack of large-scale
high-quality labeled data severely hinders their further improvements.
Motivated by the recent success of self-supervised learning in computer vision,
this paper introduces a self-supervised approach, termed Self-supervised Video
Facial Affect Perceiver (SVFAP), to address the dilemma faced by supervised
methods. Specifically, SVFAP leverages masked facial video autoencoding to
perform self-supervised pre-training on massive unlabeled facial videos.
Considering that large spatiotemporal redundancy exists in facial videos, we
propose a novel temporal pyramid and spatial bottleneck Transformer as the
encoder of SVFAP, which not only enjoys low computational cost but also
achieves excellent performance. To verify the effectiveness of our method, we
conduct experiments on nine datasets spanning three downstream tasks, including
dynamic facial expression recognition, dimensional emotion recognition, and
personality recognition. Comprehensive results demonstrate that SVFAP can learn
powerful affect-related representations via large-scale self-supervised
pre-training and it significantly outperforms previous state-of-the-art methods
on all datasets. Codes will be available at https://github.com/sunlicai/SVFAP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Licai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1&quot;&gt;Zheng Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kexin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mingyu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Haiyang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1&quot;&gt;Jianhua Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00420">
<title>SynCDR : Training Cross Domain Retrieval Models with Synthetic Data. (arXiv:2401.00420v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00420</link>
<description rdf:parseType="Literal">&lt;p&gt;In cross-domain retrieval, a model is required to identify images from the
same semantic category across two visual domains. For instance, given a sketch
of an object, a model needs to retrieve a real image of it from an online
store&apos;s catalog. A standard approach for such a problem is learning a feature
space of images where Euclidean distances reflect similarity. Even without
human annotations, which may be expensive to acquire, prior methods function
reasonably well using unlabeled images for training. Our problem constraint
takes this further to scenarios where the two domains do not necessarily share
any common categories in training data. This can occur when the two domains in
question come from different versions of some biometric sensor recording
identities of different people. We posit a simple solution, which is to
generate synthetic data to fill in these missing category examples across
domains. This, we do via category preserving translation of images from one
visual domain to another. We compare approaches specifically trained for this
translation for a pair of domains, as well as those that can use large-scale
pre-trained text-to-image diffusion models via prompts, and find that the
latter can generate better replacement synthetic data, leading to more accurate
cross-domain retrieval models. Code for our work is available at
https://github.com/samarth4149/SynCDR .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Samarth Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1&quot;&gt;Kate Saenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saligrama_V/0/1/0/all/0/1&quot;&gt;Venkatesh Saligrama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00421">
<title>From Text to Pixels: A Context-Aware Semantic Synergy Solution for Infrared and Visible Image Fusion. (arXiv:2401.00421v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00421</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid progression of deep learning technologies, multi-modality
image fusion has become increasingly prevalent in object detection tasks.
Despite its popularity, the inherent disparities in how different sources
depict scene content make fusion a challenging problem. Current fusion
methodologies identify shared characteristics between the two modalities and
integrate them within this shared domain using either iterative optimization or
deep learning architectures, which often neglect the intricate semantic
relationships between modalities, resulting in a superficial understanding of
inter-modal connections and, consequently, suboptimal fusion outcomes. To
address this, we introduce a text-guided multi-modality image fusion method
that leverages the high-level semantics from textual descriptions to integrate
semantics from infrared and visible images. This method capitalizes on the
complementary characteristics of diverse modalities, bolstering both the
accuracy and robustness of object detection. The codebook is utilized to
enhance a streamlined and concise depiction of the fused intra- and
inter-domain dynamics, fine-tuned for optimal performance in detection tasks.
We present a bilevel optimization strategy that establishes a nexus between the
joint problem of fusion and detection, optimizing both processes concurrently.
Furthermore, we introduce the first dataset of paired infrared and visible
images accompanied by text prompts, paving the way for future research.
Extensive experiments on several datasets demonstrate that our method not only
produces visually superior fusion results but also achieves a higher detection
mAP over existing methods, achieving state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xingyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1&quot;&gt;Yang Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhiying Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Long Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Risheng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00431">
<title>Wild2Avatar: Rendering Humans Behind Occlusions. (arXiv:2401.00431v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00431</link>
<description rdf:parseType="Literal">&lt;p&gt;Rendering the visual appearance of moving humans from occluded monocular
videos is a challenging task. Most existing research renders 3D humans under
ideal conditions, requiring a clear and unobstructed scene. Those methods
cannot be used to render humans in real-world scenes where obstacles may block
the camera&apos;s view and lead to partial occlusions. In this work, we present
Wild2Avatar, a neural rendering approach catered for occluded in-the-wild
monocular videos. We propose occlusion-aware scene parameterization for
decoupling the scene into three parts - occlusion, human, and background.
Additionally, extensive objective functions are designed to help enforce the
decoupling of the human from both the occlusion and the background and to
ensure the completeness of the human model. We verify the effectiveness of our
approach with experiments on in-the-wild videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tiange Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1&quot;&gt;Adam Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delp_S/0/1/0/all/0/1&quot;&gt;Scott Delp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kozuka_K/0/1/0/all/0/1&quot;&gt;Kazuki Kozuka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1&quot;&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1&quot;&gt;Ehsan Adeli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00435">
<title>Bidirectional Trained Tree-Structured Decoder for Handwritten Mathematical Expression Recognition. (arXiv:2401.00435v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00435</link>
<description rdf:parseType="Literal">&lt;p&gt;The Handwritten Mathematical Expression Recognition (HMER) task is a critical
branch in the field of OCR. Recent studies have demonstrated that incorporating
bidirectional context information significantly improves the performance of
HMER models. However, existing methods fail to effectively utilize
bidirectional context information during the inference stage. Furthermore,
current bidirectional training methods are primarily designed for string
decoders and cannot adequately generalize to tree decoders, which offer
superior generalization capabilities and structural analysis capacity. In order
to overcome these limitations, we propose the Mirror-Flipped Symbol Layout Tree
(MF-SLT) and Bidirectional Asynchronous Training (BAT) structure. Our method
extends the bidirectional training strategy to the tree decoder, allowing for
more effective training by leveraging bidirectional information. Additionally,
we analyze the impact of the visual and linguistic perception of the HMER model
separately and introduce the Shared Language Modeling (SLM) mechanism. Through
the SLM, we enhance the model&apos;s robustness and generalization when dealing with
visual ambiguity, particularly in scenarios with abundant training data. Our
approach has been validated through extensive experiments, demonstrating its
ability to achieve new state-of-the-art results on the CROHME 2014, 2016, and
2019 datasets, as well as the HME100K dataset. The code used in our experiments
will be publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hanbo Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1&quot;&gt;Pengfei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenrong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jiefeng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;Jun Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00436">
<title>Diff-PCR: Diffusion-Based Correspondence Searching in Doubly Stochastic Matrix Space for Point Cloud Registration. (arXiv:2401.00436v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00436</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiently finding optimal correspondences between point clouds is crucial
for solving both rigid and non-rigid point cloud registration problems.
Existing methods often rely on geometric or semantic feature embedding to
establish correspondences and estimate transformations or flow fields.
Recently, state-of-the-art methods have employed RAFT-like iterative updates to
refine the solution. However, these methods have certain limitations. Firstly,
their iterative refinement design lacks transparency, and their iterative
updates follow a fixed path during the refinement process, which can lead to
suboptimal results. Secondly, these methods overlook the importance of refining
or optimizing correspondences (or matching matrices) as a precursor to solving
transformations or flow fields. They typically compute candidate
correspondences based on distances in the point feature space. However, they
only project the candidate matching matrix into some matrix space once with
Sinkhorn or dual softmax operations to obtain final correspondences. This
one-shot projected matching matrix may be far from the globally optimal one,
and these approaches do not consider the distribution of the target matching
matrix. In this paper, we propose a novel approach that exploits the Denoising
Diffusion Model to predict a searching gradient for the optimal matching matrix
within the Doubly Stochastic Matrix Space. During the reverse denoising
process, our method iteratively searches for better solutions along this
denoising gradient, which points towards the maximum likelihood direction of
the target matching matrix. Our method offers flexibility by allowing the
search to start from any initial matching matrix provided by the online
backbone or white noise. Experimental evaluations on the 3DMatch/3DLoMatch and
4DMatch/4DLoMatch datasets demonstrate the effectiveness of our newly designed
framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qianliang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Haobo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yaqing Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Lei Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00438">
<title>SFGANS Self-supervised Future Generator for human ActioN Segmentation. (arXiv:2401.00438v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00438</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to locate and classify action segments in long untrimmed video is
of particular interest to many applications such as autonomous cars, robotics
and healthcare applications. Today, the most popular pipeline for action
segmentation is composed of encoding the frames into feature vectors, which are
then processed by a temporal model for segmentation. In this paper we present a
self-supervised method that comes in the middle of the standard pipeline and
generated refined representations of the original feature vectors. Experiments
show that this method improves the performance of existing models on different
sub-tasks of action segmentation, even without additional hyper parameter
tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berman_O/0/1/0/all/0/1&quot;&gt;Or Berman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldbraikh_A/0/1/0/all/0/1&quot;&gt;Adam Goldbraikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laufer_S/0/1/0/all/0/1&quot;&gt;Shlomi Laufer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00440">
<title>TSGAN: An Optical-to-SAR Dual Conditional GAN for Optical based SAR Temporal Shifting. (arXiv:2401.00440v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00440</link>
<description rdf:parseType="Literal">&lt;p&gt;In contrast to the well-investigated field of SAR-to-Optical translation,
this study explores the lesser-investigated domain of Optical-to-SAR
translation, a challenging field due to the ill-posed nature of this
translation. The complexity arises as a single optical data can have multiple
SAR representations based on the SAR viewing geometry. We propose a novel
approach, termed SAR Temporal Shifting, which inputs an optical data from the
desired timestamp along with a SAR data from a different temporal point but
with a consistent viewing geometry as the expected SAR data, both complemented
with a change map of optical data during the intervening period. This model
modifies the SAR data based on the changes observed in optical data to generate
the SAR data for the desired timestamp. Our model, a dual conditional
Generative Adversarial Network (GAN), named Temporal Shifting GAN (TSGAN),
incorporates a siamese encoder in both the Generator and the Discriminator. To
prevent the model from overfitting on the input SAR data, we employed a change
weighted loss function. Our approach surpasses traditional translation methods
by eliminating the GAN&apos;s fiction phenomenon, particularly in unchanged regions,
resulting in higher SSIM and PSNR in these areas. Additionally, modifications
to the Pix2Pix architecture and the inclusion of attention mechanisms have
enhanced the model&apos;s performance on all regions of the data. This research
paves the way for leveraging legacy optical datasets, the most abundant and
longstanding source of Earth datary data, extending their use to SAR domains
and temporal analyses. To foster further research, we provide the code,
datasets used in our study, and a framework for generating paired SAR-Optical
datasets for new regions of interest. These resources are available on
github.com/moienr/TemporalGAN
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rangzan_M/0/1/0/all/0/1&quot;&gt;Moien Rangzan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Attarchi_S/0/1/0/all/0/1&quot;&gt;Sara Attarchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gloaguen_R/0/1/0/all/0/1&quot;&gt;Richard Gloaguen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alavipanah_S/0/1/0/all/0/1&quot;&gt;Seyed Kazem Alavipanah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00442">
<title>A Comprehensive Overview of Fish-Eye Camera Distortion Correction Methods. (arXiv:2401.00442v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00442</link>
<description rdf:parseType="Literal">&lt;p&gt;The fisheye camera, with its unique wide field of view and other
characteristics, has found extensive applications in various fields. However,
the fisheye camera suffers from significant distortion compared to pinhole
cameras, resulting in distorted images of captured objects. Fish-eye camera
distortion is a common issue in digital image processing, requiring effective
correction techniques to enhance image quality. This review provides a
comprehensive overview of various methods used for fish-eye camera distortion
correction. The article explores the polynomial distortion model, which
utilizes polynomial functions to model and correct radial distortions.
Additionally, alternative approaches such as panorama mapping, grid mapping,
direct methods, and deep learning-based methods are discussed. The review
highlights the advantages, limitations, and recent advancements of each method,
enabling readers to make informed decisions based on their specific needs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1&quot;&gt;De-Wei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jun-Jie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhao-Yuan Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00456">
<title>Double-well Net for Image Segmentation. (arXiv:2401.00456v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00456</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, our goal is to integrate classical mathematical models with
deep neural networks by introducing two novel deep neural network models for
image segmentation known as Double-well Nets. Drawing inspiration from the
Potts model, our models leverage neural networks to represent a region force
functional. We extend the well-know MBO (Merriman-Bence-Osher) scheme to solve
the Potts model. The widely recognized Potts model is approximated using a
double-well potential and then solved by an operator-splitting method, which
turns out to be an extension of the well-known MBO scheme. Subsequently, we
replace the region force functional in the Potts model with a UNet-type
network, which is data-driven, and also introduce control variables to enhance
effectiveness. The resulting algorithm is a neural network activated by a
function that minimizes the double-well potential. What sets our proposed
Double-well Nets apart from many existing deep learning methods for image
segmentation is their strong mathematical foundation. They are derived from the
network approximation theory and employ the MBO scheme to approximately solve
the Potts model. By incorporating mathematical principles, Double-well Nets
bridge the MBO scheme and neural networks, and offer an alternative perspective
for designing networks with mathematical backgrounds. Through comprehensive
experiments, we demonstrate the performance of Double-well Nets, showcasing
their superior accuracy and robustness compared to state-of-the-art neural
networks. Overall, our work represents a valuable contribution to the field of
image segmentation by combining the strengths of classical variational models
and deep neural networks. The Double-well Nets introduce an innovative approach
that leverages mathematical foundations to enhance segmentation performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1&quot;&gt;Raymond Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_X/0/1/0/all/0/1&quot;&gt;Xue-Cheng Tai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00460">
<title>RainSD: Rain Style Diversification Module for Image Synthesis Enhancement using Feature-Level Style Distribution. (arXiv:2401.00460v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00460</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous driving technology nowadays targets to level 4 or beyond, but the
researchers are faced with some limitations for developing reliable driving
algorithms in diverse challenges. To promote the autonomous vehicles to spread
widely, it is important to address safety issues on this technology. Among
various safety concerns, the sensor blockage problem by severe weather
conditions can be one of the most frequent threats for multi-task learning
based perception algorithms during autonomous driving. To handle this problem,
the importance of the generation of proper datasets is becoming more
significant. In this paper, a synthetic road dataset with sensor blockage
generated from real road dataset BDD100K is suggested in the format of BDD100K
annotation. Rain streaks for each frame were made by an experimentally
established equation and translated utilizing the image-to-image translation
network based on style transfer. Using this dataset, the degradation of the
diverse multi-task networks for autonomous driving, such as lane detection,
driving area segmentation, and traffic object detection, has been thoroughly
evaluated and analyzed. The tendency of the performance degradation of deep
neural network-based perception systems for autonomous vehicle has been
analyzed in depth. Finally, we discuss the limitation and the future directions
of the deep neural network-based perception algorithms and autonomous driving
dataset generation based on image-to-image translation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1&quot;&gt;Hyeonjae Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Junghyun Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taesoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1&quot;&gt;Sungho Son&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jungki Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_G/0/1/0/all/0/1&quot;&gt;Gyeungho Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_Y/0/1/0/all/0/1&quot;&gt;Yongseob Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00463">
<title>Analyzing Local Representations of Self-supervised Vision Transformers. (arXiv:2401.00463v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00463</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a comparative analysis of various self-supervised
Vision Transformers (ViTs), focusing on their local representative power.
Inspired by large language models, we examine the abilities of ViTs to perform
various computer vision tasks with little to no fine-tuning. We design an
evaluation framework to analyze the quality of local, i.e. patch-level,
representations in the context of few-shot semantic segmentation, instance
identification, object retrieval, and tracking. We discover that contrastive
learning based methods like DINO produce more universal patch representations
that can be immediately applied for downstream tasks with no parameter tuning,
compared to masked image modeling. The embeddings learned using the latter
approach, e.g. in masked autoencoders, have high variance features that harm
distance-based algorithms, such as k-NN, and do not contain useful information
for most downstream tasks. Furthermore, we demonstrate that removing these
high-variance features enhances k-NN by providing an analysis of the benchmarks
for this work and for Scale-MAE, a recent extension of masked autoencoders.
Finally, we find an object instance retrieval setting where DINOv2, a model
pretrained on two orders of magnitude more data, performs worse than its less
compute-intensive counterpart DINO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vanyan_A/0/1/0/all/0/1&quot;&gt;Ani Vanyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barseghyan_A/0/1/0/all/0/1&quot;&gt;Alvard Barseghyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamazyan_H/0/1/0/all/0/1&quot;&gt;Hakob Tamazyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huroyan_V/0/1/0/all/0/1&quot;&gt;Vahan Huroyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khachatrian_H/0/1/0/all/0/1&quot;&gt;Hrant Khachatrian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1&quot;&gt;Martin Danelljan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00496">
<title>SAR-RARP50: Segmentation of surgical instrumentation and Action Recognition on Robot-Assisted Radical Prostatectomy Challenge. (arXiv:2401.00496v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00496</link>
<description rdf:parseType="Literal">&lt;p&gt;Surgical tool segmentation and action recognition are fundamental building
blocks in many computer-assisted intervention applications, ranging from
surgical skills assessment to decision support systems. Nowadays,
learning-based action recognition and segmentation approaches outperform
classical methods, relying, however, on large, annotated datasets. Furthermore,
action recognition and tool segmentation algorithms are often trained and make
predictions in isolation from each other, without exploiting potential
cross-task relationships. With the EndoVis 2022 SAR-RARP50 challenge, we
release the first multimodal, publicly available, in-vivo, dataset for surgical
action recognition and semantic instrumentation segmentation, containing 50
suturing video segments of Robotic Assisted Radical Prostatectomy (RARP). The
aim of the challenge is twofold. First, to enable researchers to leverage the
scale of the provided dataset and develop robust and highly accurate
single-task action recognition and tool segmentation approaches in the surgical
domain. Second, to further explore the potential of multitask-based learning
approaches and determine their comparative advantage against their single-task
counterparts. A total of 12 teams participated in the challenge, contributing 7
action recognition methods, 9 instrument segmentation techniques, and 4
multitask approaches that integrated both action recognition and instrument
segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Psychogyios_D/0/1/0/all/0/1&quot;&gt;Dimitrios Psychogyios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colleoni_E/0/1/0/all/0/1&quot;&gt;Emanuele Colleoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amsterdam_B/0/1/0/all/0/1&quot;&gt;Beatrice Van Amsterdam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chih-Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shu-Yu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuchong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1&quot;&gt;Fucang Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_B/0/1/0/all/0/1&quot;&gt;Baosheng Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guotai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boels_M/0/1/0/all/0/1&quot;&gt;Maxence Boels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1&quot;&gt;Jiayu Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sparks_R/0/1/0/all/0/1&quot;&gt;Rachel Sparks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasgupta_P/0/1/0/all/0/1&quot;&gt;Prokar Dasgupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granados_A/0/1/0/all/0/1&quot;&gt;Alejandro Granados&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1&quot;&gt;Sebastien Ourselin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mengya Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;An Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yanan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Long Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Hongliang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamada_A/0/1/0/all/0/1&quot;&gt;Atsushi Yamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harai_Y/0/1/0/all/0/1&quot;&gt;Yuriko Harai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishikawa_Y/0/1/0/all/0/1&quot;&gt;Yuto Ishikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayashi_K/0/1/0/all/0/1&quot;&gt;Kazuyuki Hayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simoens_J/0/1/0/all/0/1&quot;&gt;Jente Simoens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeBacker_P/0/1/0/all/0/1&quot;&gt;Pieter DeBacker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cisternino_F/0/1/0/all/0/1&quot;&gt;Francesco Cisternino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furnari_G/0/1/0/all/0/1&quot;&gt;Gabriele Furnari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mottrie_A/0/1/0/all/0/1&quot;&gt;Alex Mottrie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferraguti_F/0/1/0/all/0/1&quot;&gt;Federica Ferraguti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondo_S/0/1/0/all/0/1&quot;&gt;Satoshi Kondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasai_S/0/1/0/all/0/1&quot;&gt;Satoshi Kasai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirasawa_K/0/1/0/all/0/1&quot;&gt;Kousuke Hirasawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Soohee Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seung Hyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kyu Eun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_H/0/1/0/all/0/1&quot;&gt;Hyoun-Joong Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1&quot;&gt;Kui Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1&quot;&gt;Shan An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krell_S/0/1/0/all/0/1&quot;&gt;Stefanie Krell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bodenstedt_S/0/1/0/all/0/1&quot;&gt;Sebastian Bodenstedt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayobi_N/0/1/0/all/0/1&quot;&gt;Nicolas Ayobi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_A/0/1/0/all/0/1&quot;&gt;Alejandra Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_S/0/1/0/all/0/1&quot;&gt;Santiago Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puentes_J/0/1/0/all/0/1&quot;&gt;Juanita Puentes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1&quot;&gt;Pablo Arbelaez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohareri_O/0/1/0/all/0/1&quot;&gt;Omid Mohareri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1&quot;&gt;Danail Stoyanov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00523">
<title>Compressing Deep Image Super-resolution Models. (arXiv:2401.00523v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.00523</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning techniques have been applied in the context of image
super-resolution (SR), achieving remarkable advances in terms of reconstruction
performance. Existing techniques typically employ highly complex model
structures which result in large model sizes and slow inference speeds. This
often leads to high energy consumption and restricts their adoption for
practical applications. To address this issue, this work employs a three-stage
workflow for compressing deep SR models which significantly reduces their
memory requirement. Restoration performance has been maintained through
teacher-student knowledge distillation using a newly designed distillation
loss. We have applied this approach to two popular image super-resolution
networks, SwinIR and EDSR, to demonstrate its effectiveness. The resulting
compact models, SwinIRmini and EDSRmini, attain an 89% and 96% reduction in
both model size and floating-point operations (FLOPs) respectively, compared to
their original versions. They also retain competitive super-resolution
performance compared to their original models and other commonly used SR
approaches. The source code and pre-trained models for these two lightweight SR
approaches are released at https://pikapi22.github.io/CDISM/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nawala_J/0/1/0/all/0/1&quot;&gt;Jakub Nawala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bull_D/0/1/0/all/0/1&quot;&gt;David Bull&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00551">
<title>A Generalist FaceX via Learning Unified Facial Representation. (arXiv:2401.00551v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00551</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents FaceX framework, a novel facial generalist model capable
of handling diverse facial tasks simultaneously. To achieve this goal, we
initially formulate a unified facial representation for a broad spectrum of
facial editing tasks, which macroscopically decomposes a face into fundamental
identity, intra-personal variation, and environmental factors. Based on this,
we introduce Facial Omni-Representation Decomposing (FORD) for seamless
manipulation of various facial components, microscopically decomposing the core
aspects of most facial editing tasks. Furthermore, by leveraging the prior of a
pretrained StableDiffusion (SD) to enhance generation quality and accelerate
training, we design Facial Omni-Representation Steering (FORS) to first
assemble unified facial representations and then effectively steer the SD-aware
generation process by the efficient Facial Representation Controller (FRC).
%Without any additional features, Our versatile FaceX achieves competitive
performance compared to elaborate task-specific models on popular facial
editing tasks. Full codes and models will be available at
https://github.com/diffusion-facex/FaceX.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yue Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiangning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Junwei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangtai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yanhao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1&quot;&gt;Ying Tai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00604">
<title>SteinDreamer: Variance Reduction for Text-to-3D Score Distillation via Stein Identity. (arXiv:2401.00604v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00604</link>
<description rdf:parseType="Literal">&lt;p&gt;Score distillation has emerged as one of the most prevalent approaches for
text-to-3D asset synthesis. Essentially, score distillation updates 3D
parameters by lifting and back-propagating scores averaged over different
views. In this paper, we reveal that the gradient estimation in score
distillation is inherent to high variance. Through the lens of variance
reduction, the effectiveness of SDS and VSD can be interpreted as applications
of various control variates to the Monte Carlo estimator of the distilled
score. Motivated by this rethinking and based on Stein&apos;s identity, we propose a
more general solution to reduce variance for score distillation, termed Stein
Score Distillation (SSD). SSD incorporates control variates constructed by
Stein identity, allowing for arbitrary baseline functions. This enables us to
include flexible guidance priors and network architectures to explicitly
optimize for variance reduction. In our experiments, the overall pipeline,
dubbed SteinDreamer, is implemented by instantiating the control variate with a
monocular depth estimator. The results suggest that SSD can effectively reduce
the distillation variance and consistently improve visual quality for both
object- and scene-level generation. Moreover, we demonstrate that SteinDreamer
achieves faster convergence than existing methods due to more stable gradient
updates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peihao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zhiwen Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dejia Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dilin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1&quot;&gt;Sreyas Mohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iandola_F/0/1/0/all/0/1&quot;&gt;Forrest Iandola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranjan_R/0/1/0/all/0/1&quot;&gt;Rakesh Ranjan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yilei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1&quot;&gt;Vikas Chandra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00608">
<title>Bringing Back the Context: Camera Trap Species Identification as Link Prediction on Multimodal Knowledge Graphs. (arXiv:2401.00608v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00608</link>
<description rdf:parseType="Literal">&lt;p&gt;Camera traps are valuable tools in animal ecology for biodiversity monitoring
and conservation. However, challenges like poor generalization to deployment at
new unseen locations limit their practical application. Images are naturally
associated with heterogeneous forms of context possibly in different
modalities. In this work, we leverage the structured context associated with
the camera trap images to improve out-of-distribution generalization for the
task of species identification in camera traps. For example, a photo of a wild
animal may be associated with information about where and when it was taken, as
well as structured biology knowledge about the animal species. While typically
overlooked by existing work, bringing back such context offers several
potential benefits for better image understanding, such as addressing data
scarcity and enhancing generalization. However, effectively integrating such
heterogeneous context into the visual domain is a challenging problem. To
address this, we propose a novel framework that reformulates species
classification as link prediction in a multimodal knowledge graph (KG). This
framework seamlessly integrates various forms of multimodal context for visual
recognition. We apply this framework for out-of-distribution species
classification on the iWildCam2020-WILDS and Snapshot Mountain Zebra datasets
and achieve competitive performance with state-of-the-art approaches.
Furthermore, our framework successfully incorporates biological taxonomy for
improved generalization and enhances sample efficiency for recognizing
under-represented species.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pahuja_V/0/1/0/all/0/1&quot;&gt;Vardaan Pahuja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1&quot;&gt;Weidi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yu Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_C/0/1/0/all/0/1&quot;&gt;Cheng-Hao Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hong-You Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berger_Wolf_T/0/1/0/all/0/1&quot;&gt;Tanya Berger-Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stewart_C/0/1/0/all/0/1&quot;&gt;Charles Stewart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Song Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1&quot;&gt;Wei-Lun Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yu Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00616">
<title>GD^2-NeRF: Generative Detail Compensation via GAN and Diffusion for One-shot Generalizable Neural Radiance Fields. (arXiv:2401.00616v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00616</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we focus on the One-shot Novel View Synthesis (O-NVS) task
which targets synthesizing photo-realistic novel views given only one reference
image per scene. Previous One-shot Generalizable Neural Radiance Fields
(OG-NeRF) methods solve this task in an inference-time finetuning-free manner,
yet suffer the blurry issue due to the encoder-only architecture that highly
relies on the limited reference image. On the other hand, recent
diffusion-based image-to-3d methods show vivid plausible results via distilling
pre-trained 2D diffusion models into a 3D representation, yet require tedious
per-scene optimization. Targeting these issues, we propose the GD^2-NeRF, a
Generative Detail compensation framework via GAN and Diffusion that is both
inference-time finetuning-free and with vivid plausible details. In detail,
following a coarse-to-fine strategy, GD^2-NeRF is mainly composed of a
One-stage Parallel Pipeline (OPP) and a 3D-consistent Detail Enhancer
(Diff3DE). At the coarse stage, OPP first efficiently inserts the GAN model
into the existing OG-NeRF pipeline for primarily relieving the blurry issue
with in-distribution priors captured from the training dataset, achieving a
good balance between sharpness (LPIPS, FID) and fidelity (PSNR, SSIM). Then, at
the fine stage, Diff3DE further leverages the pre-trained image diffusion
models to complement rich out-distribution details while maintaining decent 3D
consistency. Extensive experiments on both the synthetic and real-world
datasets show that GD$^2$-NeRF noticeably improves the details while without
per-scene finetuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xiao Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zongxin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1&quot;&gt;Shuai Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00617">
<title>Towards Improved Proxy-based Deep Metric Learning via Data-Augmented Domain Adaptation. (arXiv:2401.00617v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00617</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Metric Learning (DML) plays an important role in modern computer vision
research, where we learn a distance metric for a set of image representations.
Recent DML techniques utilize the proxy to interact with the corresponding
image samples in the embedding space. However, existing proxy-based DML methods
focus on learning individual proxy-to-sample distance while the overall
distribution of samples and proxies lacks attention. In this paper, we present
a novel proxy-based DML framework that focuses on aligning the sample and proxy
distributions to improve the efficiency of proxy-based DML losses.
Specifically, we propose the Data-Augmented Domain Adaptation (DADA) method to
adapt the domain gap between the group of samples and proxies. To the best of
our knowledge, we are the first to leverage domain adaptation to boost the
performance of proxy-based DML. We show that our method can be easily plugged
into existing proxy-based DML losses. Our experiments on benchmarks, including
the popular CUB-200-2011, CARS196, Stanford Online Products, and In-Shop
Clothes Retrieval, show that our learning algorithm significantly improves the
existing proxy losses and achieves superior results compared to the existing
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1&quot;&gt;Li Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_K/0/1/0/all/0/1&quot;&gt;Kien Hua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00639">
<title>Geometry Depth Consistency in RGBD Relative Pose Estimation. (arXiv:2401.00639v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00639</link>
<description rdf:parseType="Literal">&lt;p&gt;Relative pose estimation for RGBD cameras is crucial in a number of
applications. Previous approaches either rely on the RGB aspect of the images
to estimate pose thus not fully making use of depth in the estimation process
or estimate pose from the 3D cloud of points that each image produces, thus not
making full use of RGB information. This paper shows that if one pair of
correspondences is hypothesized from the RGB-based ranked-ordered
correspondence list, then the space of remaining correspondences is restricted
to corresponding pairs of curves nested around the hypothesized correspondence,
implicitly capturing depth consistency. This simple Geometric Depth Constraint
(GDC) significantly reduces potential matches. In effect this becomes a filter
on possible correspondences that helps reduce the number of outliers and thus
expedites RANSAC significantly. As such, the same budget of time allows for
more RANSAC iterations and therefore additional robustness and a significant
speedup. In addition, the paper proposed a Nested RANSAC approach that also
speeds up the process, as shown through experiments on TUM, ICL-NUIM, and RGBD
Scenes v2 datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sourav Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chien_C/0/1/0/all/0/1&quot;&gt;Chiang-Heng Chien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kimia_B/0/1/0/all/0/1&quot;&gt;Benjamin Kimia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00652">
<title>From Covert Hiding to Visual Editing: Robust Generative Video Steganography. (arXiv:2401.00652v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00652</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional video steganography methods are based on modifying the covert
space for embedding, whereas we propose an innovative approach that embeds
secret message within semantic feature for steganography during the video
editing process. Although existing traditional video steganography methods
display a certain level of security and embedding capacity, they lack adequate
robustness against common distortions in online social networks (OSNs). In this
paper, we introduce an end-to-end robust generative video steganography network
(RoGVS), which achieves visual editing by modifying semantic feature of videos
to embed secret message. We employ face-swapping scenario to showcase the
visual editing effects. We first design a secret message embedding module to
adaptively hide secret message into the semantic feature of videos. Extensive
experiments display that the proposed RoGVS method applied to facial video
datasets demonstrate its superiority over existing video and image
steganography techniques in terms of both robustness and capacity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1&quot;&gt;Xueying Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1&quot;&gt;Wanli Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1&quot;&gt;Zhenliang Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1&quot;&gt;Qichao Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1&quot;&gt;Zhenxing Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinpeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00653">
<title>PROMPT-IML: Image Manipulation Localization with Pre-trained Foundation Models Through Prompt Tuning. (arXiv:2401.00653v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00653</link>
<description rdf:parseType="Literal">&lt;p&gt;Deceptive images can be shared in seconds with social networking services,
posing substantial risks. Tampering traces, such as boundary artifacts and
high-frequency information, have been significantly emphasized by massive
networks in the Image Manipulation Localization (IML) field. However, they are
prone to image post-processing operations, which limit the generalization and
robustness of existing methods. We present a novel Prompt-IML framework. We
observe that humans tend to discern the authenticity of an image based on both
semantic and high-frequency information, inspired by which, the proposed
framework leverages rich semantic knowledge from pre-trained visual foundation
models to assist IML. We are the first to design a framework that utilizes
visual foundation models specially for the IML task. Moreover, we design a
Feature Alignment and Fusion module to align and fuse features of semantic
features with high-frequency features, which aims at locating tampered regions
from multiple perspectives. Experimental results demonstrate that our model can
achieve better performance on eight typical fake image datasets and outstanding
robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuntao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuzhou Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1&quot;&gt;Qichao Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1&quot;&gt;Zhenxing Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinpeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sheng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00657">
<title>Optimizing ADMM and Over-Relaxed ADMM Parameters for Linear Quadratic Problems. (arXiv:2401.00657v1 [math.OC])</title>
<link>http://arxiv.org/abs/2401.00657</link>
<description rdf:parseType="Literal">&lt;p&gt;The Alternating Direction Method of Multipliers (ADMM) has gained significant
attention across a broad spectrum of machine learning applications.
Incorporating the over-relaxation technique shows potential for enhancing the
convergence rate of ADMM. However, determining optimal algorithmic parameters,
including both the associated penalty and relaxation parameters, often relies
on empirical approaches tailored to specific problem domains and contextual
scenarios. Incorrect parameter selection can significantly hinder ADMM&apos;s
convergence rate. To address this challenge, in this paper we first propose a
general approach to optimize the value of penalty parameter, followed by a
novel closed-form formula to compute the optimal relaxation parameter in the
context of linear quadratic problems (LQPs). We then experimentally validate
our parameter selection methods through random instantiations and diverse
imaging applications, encompassing diffeomorphic image registration, image
deblurring, and MRI reconstruction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jintao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Wenqi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lei_Y/0/1/0/all/0/1&quot;&gt;Yunwen Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yuchao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zhenkuan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Jinming Duan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00663">
<title>1st Place Solution for 5th LSVOS Challenge: Referring Video Object Segmentation. (arXiv:2401.00663v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00663</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent transformer-based models have dominated the Referring Video Object
Segmentation (RVOS) task due to the superior performance. Most prior works
adopt unified DETR framework to generate segmentation masks in
query-to-instance manner. In this work, we integrate strengths of that leading
RVOS models to build up an effective paradigm. We first obtain binary mask
sequences from the RVOS models. To improve the consistency and quality of
masks, we propose Two-Stage Multi-Model Fusion strategy. Each stage rationally
ensembles RVOS models based on framework design as well as training strategy,
and leverages different video object segmentation (VOS) models to enhance mask
coherence by object propagation mechanism. Our method achieves 75.7% J&amp;amp;F on
Ref-Youtube-VOS validation set and 70% J&amp;amp;F on test set, which ranks 1st place
on 5th Large-scale Video Object Segmentation Challenge (ICCV 2023) track 3.
Code is available at https://github.com/RobertLuo1/iccv2023_RVOS_Challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhuoyan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yicheng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yitong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yansong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yujiu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00692">
<title>Self-supervised learning for skin cancer diagnosis with limited training data. (arXiv:2401.00692v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.00692</link>
<description rdf:parseType="Literal">&lt;p&gt;Cancer diagnosis is a well-studied problem in machine learning since early
detection of cancer is often the determining factor in prognosis. Supervised
deep learning achieves excellent results in cancer image classification,
usually through transfer learning. However, these models require large amounts
of labelled data and for several types of cancer, large labelled datasets do
not exist. In this paper, we demonstrate that a model pre-trained using a
self-supervised learning algorithm known as Barlow Twins can outperform the
conventional supervised transfer learning pipeline. We juxtapose two base
models: i) pretrained in a supervised fashion on ImageNet; ii) pretrained in a
self-supervised fashion on ImageNet. Both are subsequently fine tuned on a
small labelled skin lesion dataset and evaluated on a large test set. We
achieve a mean test accuracy of 70\% for self-supervised transfer in comparison
to 66\% for supervised transfer. Interestingly, boosting performance further is
possible by self-supervised pretraining a second time (on unlabelled skin
lesion images) before subsequent fine tuning. This hints at an alternative path
to collecting more labelled data in settings where this is challenging - namely
just collecting more unlabelled images. Our framework is applicable to cancer
image classification models in the low-labelled data regime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Haggerty_H/0/1/0/all/0/1&quot;&gt;Hamish Haggerty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chandra_R/0/1/0/all/0/1&quot;&gt;Rohitash Chandra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00695">
<title>Credible Teacher for Semi-Supervised Object Detection in Open Scene. (arXiv:2401.00695v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00695</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-Supervised Object Detection (SSOD) has achieved resounding success by
leveraging unlabeled data to improve detection performance. However, in Open
Scene Semi-Supervised Object Detection (O-SSOD), unlabeled data may contains
unknown objects not observed in the labeled data, which will increase
uncertainty in the model&apos;s predictions for known objects. It is detrimental to
the current methods that mainly rely on self-training, as more uncertainty
leads to the lower localization and classification precision of pseudo labels.
To this end, we propose Credible Teacher, an end-to-end framework. Credible
Teacher adopts an interactive teaching mechanism using flexible labels to
prevent uncertain pseudo labels from misleading the model and gradually reduces
its uncertainty through the guidance of other credible pseudo labels. Empirical
results have demonstrated our method effectively restrains the adverse effect
caused by O-SSOD and significantly outperforms existing counterparts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1&quot;&gt;Jingyu Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanbin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00700">
<title>An attempt to generate new bridge types from latent space of generative adversarial network. (arXiv:2401.00700v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00700</link>
<description rdf:parseType="Literal">&lt;p&gt;Try to generate new bridge types using generative artificial intelligence
technology. Symmetric structured image dataset of three-span beam bridge, arch
bridge, cable-stayed bridge and suspension bridge are used . Based on Python
programming language, TensorFlow and Keras deep learning platform framework ,
as well as Wasserstein loss function and Lipschitz constraints, generative
adversarial network is constructed and trained. From the obtained low
dimensional bridge-type latent space sampling, new bridge types with asymmetric
structures can be generated. Generative adversarial network can create new
bridge types by organically combining different structural components on the
basis of human original bridge types. It has a certain degree of human original
ability. Generative artificial intelligence technology can open up imagination
space and inspire humanity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongjun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00701">
<title>Towards Efficient and Effective Text-to-Video Retrieval with Coarse-to-Fine Visual Representation Learning. (arXiv:2401.00701v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00701</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, text-to-video retrieval methods based on CLIP have
experienced rapid development. The primary direction of evolution is to exploit
the much wider gamut of visual and textual cues to achieve alignment.
Concretely, those methods with impressive performance often design a heavy
fusion block for sentence (words)-video (frames) interaction, regardless of the
prohibitive computation complexity. Nevertheless, these approaches are not
optimal in terms of feature utilization and retrieval efficiency. To address
this issue, we adopt multi-granularity visual feature learning, ensuring the
model&apos;s comprehensiveness in capturing visual content features spanning from
abstract to detailed levels during the training phase. To better leverage the
multi-granularity features, we devise a two-stage retrieval architecture in the
retrieval phase. This solution ingeniously balances the coarse and fine
granularity of retrieval content. Moreover, it also strikes a harmonious
equilibrium between retrieval effectiveness and efficiency. Specifically, in
training phase, we design a parameter-free text-gated interaction block (TIB)
for fine-grained video representation learning and embed an extra Pearson
Constraint to optimize cross-modal representation learning. In retrieval phase,
we use coarse-grained video representations for fast recall of top-k
candidates, which are then reranked by fine-grained video representations.
Extensive experiments on four benchmarks demonstrate the efficiency and
effectiveness. Notably, our method achieves comparable performance with the
current state-of-the-art methods while being nearly 50 times faster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_K/0/1/0/all/0/1&quot;&gt;Kaibin Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yanhua Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1&quot;&gt;Xinglin Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Quan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Han Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00708">
<title>Revisiting Nonlocal Self-Similarity from Continuous Representation. (arXiv:2401.00708v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00708</link>
<description rdf:parseType="Literal">&lt;p&gt;Nonlocal self-similarity (NSS) is an important prior that has been
successfully applied in multi-dimensional data processing tasks, e.g., image
and video recovery. However, existing NSS-based methods are solely suitable for
meshgrid data such as images and videos, but are not suitable for emerging
off-meshgrid data, e.g., point cloud and climate data. In this work, we revisit
the NSS from the continuous representation perspective and propose a novel
Continuous Representation-based NonLocal method (termed as CRNL), which has two
innovative features as compared with classical nonlocal methods. First, based
on the continuous representation, our CRNL unifies the measure of
self-similarity for on-meshgrid and off-meshgrid data and thus is naturally
suitable for both of them. Second, the nonlocal continuous groups can be more
compactly and efficiently represented by the coupled low-rank function
factorization, which simultaneously exploits the similarity within each group
and across different groups, while classical nonlocal methods neglect the
similarity across groups. This elaborately designed coupled mechanism allows
our method to enjoy favorable performance over conventional NSS methods in
terms of both effectiveness and efficiency. Extensive multi-dimensional data
processing experiments on-meshgrid (e.g., image inpainting and image denoising)
and off-meshgrid (e.g., climate data prediction and point cloud recovery)
validate the versatility, effectiveness, and efficiency of our CRNL as compared
with state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yisi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xile Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1&quot;&gt;Deyu Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00711">
<title>Text2Avatar: Text to 3D Human Avatar Generation with Codebook-Driven Body Controllable Attribute. (arXiv:2401.00711v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00711</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating 3D human models directly from text helps reduce the cost and time
of character modeling. However, achieving multi-attribute controllable and
realistic 3D human avatar generation is still challenging due to feature
coupling and the scarcity of realistic 3D human avatar datasets. To address
these issues, we propose Text2Avatar, which can generate realistic-style 3D
avatars based on the coupled text prompts. Text2Avatar leverages a discrete
codebook as an intermediate feature to establish a connection between text and
avatars, enabling the disentanglement of features. Furthermore, to alleviate
the scarcity of realistic style 3D human avatar data, we utilize a pre-trained
unconditional 3D human avatar generation model to obtain a large amount of 3D
avatar pseudo data, which allows Text2Avatar to achieve realistic style
generation. Experimental results demonstrate that our method can generate
realistic 3D avatars from coupled textual data, which is challenging for other
existing methods in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1&quot;&gt;Chaoqun Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yuqin Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ronghui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_A/0/1/0/all/0/1&quot;&gt;Achun Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yachao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00719">
<title>Depth Map Denoising Network and Lightweight Fusion Network for Enhanced 3D Face Recognition. (arXiv:2401.00719v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00719</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing availability of consumer depth sensors, 3D face
recognition (FR) has attracted more and more attention. However, the data
acquired by these sensors are often coarse and noisy, making them impractical
to use directly. In this paper, we introduce an innovative Depth map denoising
network (DMDNet) based on the Denoising Implicit Image Function (DIIF) to
reduce noise and enhance the quality of facial depth images for low-quality 3D
FR. After generating clean depth faces using DMDNet, we further design a
powerful recognition network called Lightweight Depth and Normal Fusion network
(LDNFNet), which incorporates a multi-branch fusion block to learn unique and
complementary features between different modalities such as depth and normal
images. Comprehensive experiments conducted on four distinct low-quality
databases demonstrate the effectiveness and robustness of our proposed methods.
Furthermore, when combining DMDNet and LDNFNet, we achieve state-of-the-art
results on the Lock3DFace database.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Ruizhuo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Ke Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1&quot;&gt;Chao Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenhui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Junlan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1&quot;&gt;Weihong Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00722">
<title>BRAU-Net++: U-Shaped Hybrid CNN-Transformer Network for Medical Image Segmentation. (arXiv:2401.00722v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00722</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate medical image segmentation is essential for clinical quantification,
disease diagnosis, treatment planning and many other applications. Both
convolution-based and transformer-based u-shaped architectures have made
significant success in various medical image segmentation tasks. The former can
efficiently learn local information of images while requiring much more
image-specific inductive biases inherent to convolution operation. The latter
can effectively capture long-range dependency at different feature scales using
self-attention, whereas it typically encounters the challenges of quadratic
compute and memory requirements with sequence length increasing. To address
this problem, through integrating the merits of these two paradigms in a
well-designed u-shaped architecture, we propose a hybrid yet effective
CNN-Transformer network, named BRAU-Net++, for an accurate medical image
segmentation task. Specifically, BRAU-Net++ uses bi-level routing attention as
the core building block to design our u-shaped encoder-decoder structure, in
which both encoder and decoder are hierarchically constructed, so as to learn
global semantic information while reducing computational complexity.
Furthermore, this network restructures skip connection by incorporating
channel-spatial attention which adopts convolution operations, aiming to
minimize local spatial information loss and amplify global
dimension-interaction of multi-scale features. Extensive experiments on three
public benchmark datasets demonstrate that our proposed approach surpasses
other state-of-the-art methods including its baseline: BRAU-Net under almost
all evaluation metrics. We achieve the average Dice-Similarity Coefficient
(DSC) of 82.47, 90.10, and 92.94 on Synapse multi-organ segmentation, ISIC-2018
Challenge, and CVC-ClinicDB, as well as the mIoU of 84.01 and 88.17 on
ISIC-2018 Challenge and CVC-ClinicDB, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_L/0/1/0/all/0/1&quot;&gt;Libin Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1&quot;&gt;Pengzhou Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Lu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaojuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yongmei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yudong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00728">
<title>MultiFusionNet: Multilayer Multimodal Fusion of Deep Neural Networks for Chest X-Ray Image Classification. (arXiv:2401.00728v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.00728</link>
<description rdf:parseType="Literal">&lt;p&gt;Chest X-ray imaging is a critical diagnostic tool for identifying pulmonary
diseases. However, manual interpretation of these images is time-consuming and
error-prone. Automated systems utilizing convolutional neural networks (CNNs)
have shown promise in improving the accuracy and efficiency of chest X-ray
image classification. While previous work has mainly focused on using feature
maps from the final convolution layer, there is a need to explore the benefits
of leveraging additional layers for improved disease classification. Extracting
robust features from limited medical image datasets remains a critical
challenge. In this paper, we propose a novel deep learning-based multilayer
multimodal fusion model that emphasizes extracting features from different
layers and fusing them. Our disease detection model considers the
discriminatory information captured by each layer. Furthermore, we propose the
fusion of different-sized feature maps (FDSFM) module to effectively merge
feature maps from diverse layers. The proposed model achieves a significantly
higher accuracy of 97.21% and 99.60% for both three-class and two-class
classifications, respectively. The proposed multilayer multimodal fusion model,
along with the FDSFM module, holds promise for accurate disease classification
and can also be extended to other disease classifications in chest X-ray
images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1&quot;&gt;Saurabh Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Arya_K/0/1/0/all/0/1&quot;&gt;K. V. Arya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meena_Y/0/1/0/all/0/1&quot;&gt;Yogesh Kumar Meena&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00729">
<title>NightRain: Nighttime Video Deraining via Adaptive-Rain-Removal and Adaptive-Correction. (arXiv:2401.00729v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00729</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing deep-learning-based methods for nighttime video deraining rely on
synthetic data due to the absence of real-world paired data. However, the
intricacies of the real world, particularly with the presence of light effects
and low-light regions affected by noise, create significant domain gaps,
hampering synthetic-trained models in removing rain streaks properly and
leading to over-saturation and color shifts. Motivated by this, we introduce
NightRain, a novel nighttime video deraining method with adaptive-rain-removal
and adaptive-correction. Our adaptive-rain-removal uses unlabeled rain videos
to enable our model to derain real-world rain videos, particularly in regions
affected by complex light effects. The idea is to allow our model to obtain
rain-free regions based on the confidence scores. Once rain-free regions and
the corresponding regions from our input are obtained, we can have region-based
paired real data. These paired data are used to train our model using a
teacher-student framework, allowing the model to iteratively learn from less
challenging regions to more challenging regions. Our adaptive-correction aims
to rectify errors in our model&apos;s predictions, such as over-saturation and color
shifts. The idea is to learn from clear night input training videos based on
the differences or distance between those input videos and their corresponding
predictions. Our model learns from these differences, compelling our model to
correct the errors. From extensive experiments, our method demonstrates
state-of-the-art performance. It achieves a PSNR of 26.73dB, surpassing
existing nighttime video deraining methods by a substantial margin of 13.7%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Beibei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yeying Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1&quot;&gt;Wending Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wei Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shunli Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1&quot;&gt;Robby Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00736">
<title>Diffusion Models, Image Super-Resolution And Everything: A Survey. (arXiv:2401.00736v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00736</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion Models (DMs) represent a significant advancement in image
Super-Resolution (SR), aligning technical image quality more closely with human
preferences and expanding SR applications. DMs address critical limitations of
previous methods, enhancing overall realism and details in SR images. However,
DMs suffer from color-shifting issues, and their high computational costs call
for efficient sampling alternatives, underscoring the challenge of balancing
computational efficiency and image quality. This survey gives an overview of
DMs applied to image SR and offers a detailed analysis that underscores the
unique characteristics and methodologies within this domain, distinct from
broader existing reviews in the field. It presents a unified view of DM
fundamentals and explores research directions, including alternative input
domains, conditioning strategies, guidance, corruption spaces, and zero-shot
methods. This survey provides insights into the evolution of image SR with DMs,
addressing current trends, challenges, and future directions in this rapidly
evolving field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moser_B/0/1/0/all/0/1&quot;&gt;Brian B. Moser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanbhag_A/0/1/0/all/0/1&quot;&gt;Arundhati S. Shanbhag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1&quot;&gt;Federico Raue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frolov_S/0/1/0/all/0/1&quot;&gt;Stanislav Frolov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palacio_S/0/1/0/all/0/1&quot;&gt;Sebastian Palacio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1&quot;&gt;Andreas Dengel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00739">
<title>DiffMorph: Text-less Image Morphing with Diffusion Models. (arXiv:2401.00739v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00739</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-conditioned image generation models are a prevalent use of AI image
synthesis, yet intuitively controlling output guided by an artist remains
challenging. Current methods require multiple images and textual prompts for
each object to specify them as concepts to generate a single customized image.
&lt;/p&gt;
&lt;p&gt;On the other hand, our work, \verb|DiffMorph|, introduces a novel approach
that synthesizes images that mix concepts without the use of textual prompts.
Our work integrates a sketch-to-image module to incorporate user sketches as
input. \verb|DiffMorph| takes an initial image with conditioning artist-drawn
sketches to generate a morphed image.
&lt;/p&gt;
&lt;p&gt;We employ a pre-trained text-to-image diffusion model and fine-tune it to
reconstruct each image faithfully. We seamlessly merge images and concepts from
sketches into a cohesive composition. The image generation capability of our
work is demonstrated through our results and a comparison of these with
prompt-based image generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1&quot;&gt;Shounak Chatterjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00740">
<title>Beyond Subspace Isolation: Many-to-Many Transformer for Light Field Image Super-resolution. (arXiv:2401.00740v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.00740</link>
<description rdf:parseType="Literal">&lt;p&gt;The effective extraction of spatial-angular features plays a crucial role in
light field image super-resolution (LFSR) tasks, and the introduction of
convolution and Transformers leads to significant improvement in this area.
Nevertheless, due to the large 4D data volume of light field images, many
existing methods opted to decompose the data into a number of lower-dimensional
subspaces and perform Transformers in each sub-space individually. As a side
effect, these methods inadvertently restrict the self-attention mechanisms to a
One-to-One scheme accessing only a limited subset of LF data, explicitly
preventing comprehensive optimization on all spatial and angular cues. In this
paper, we identify this limitation as subspace isolation and introduce a novel
Many-to-Many Transformer (M2MT) to address it. M2MT aggregates angular
information in the spatial subspace before performing the self-attention
mechanism. It enables complete access to all information across all
sub-aperture images (SAIs) in a light field image. Consequently, M2MT is
enabled to comprehensively capture long-range correlation dependencies. With
M2MT as the pivotal component, we develop a simple yet effective M2MT network
for LFSR. Our experimental results demonstrate that M2MT achieves
state-of-the-art performance across various public datasets. We further conduct
in-depth analysis using local attribution maps (LAM) to obtain visual
interpretability, and the results validate that M2MT is empowered with a truly
non-local context in both spatial and angular subspaces to mitigate subspace
isolation and acquire effective spatial-angular representation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zeke Zexi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chung_V/0/1/0/all/0/1&quot;&gt;Vera Yuk Ying Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yiran Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00763">
<title>New Job, New Gender? Measuring the Social Bias in Image Generation Models. (arXiv:2401.00763v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.00763</link>
<description rdf:parseType="Literal">&lt;p&gt;Image generation models can generate or edit images from a given text. Recent
advancements in image generation technology, exemplified by DALL-E and
Midjourney, have been groundbreaking. These advanced models, despite their
impressive capabilities, are often trained on massive Internet datasets, making
them susceptible to generating content that perpetuates social stereotypes and
biases, which can lead to severe consequences. Prior research on assessing bias
within image generation models suffers from several shortcomings, including
limited accuracy, reliance on extensive human labor, and lack of comprehensive
analysis. In this paper, we propose BiasPainter, a novel metamorphic testing
framework that can accurately, automatically and comprehensively trigger social
bias in image generation models. BiasPainter uses a diverse range of seed
images of individuals and prompts the image generation models to edit these
images using gender, race, and age-neutral queries. These queries span 62
professions, 39 activities, 57 types of objects, and 70 personality traits. The
framework then compares the edited images to the original seed images, focusing
on any changes related to gender, race, and age. BiasPainter adopts a testing
oracle that these characteristics should not be modified when subjected to
neutral prompts. Built upon this design, BiasPainter can trigger the social
bias and evaluate the fairness of image generation models. To evaluate the
effectiveness of BiasPainter, we use BiasPainter to test five widely-used
commercial image generation software and models, such as stable diffusion and
Midjourney. Experimental results show that 100\% of the generated test cases
can successfully trigger social bias in image generation models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenxuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1&quot;&gt;Haonan Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jen-tse Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Youliang Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1&quot;&gt;Haoyi Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1&quot;&gt;Nanyun Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1&quot;&gt;Michael R. Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00766">
<title>Bracketing is All You Need: Unifying Image Restoration and Enhancement Tasks with Multi-Exposure Images. (arXiv:2401.00766v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00766</link>
<description rdf:parseType="Literal">&lt;p&gt;It is challenging but highly desired to acquire high-quality photos with
clear content in low-light environments. Although multi-image processing
methods (using burst, dual-exposure, or multi-exposure images) have made
significant progress in addressing this issue, they typically focus exclusively
on specific restoration or enhancement tasks, being insufficient in exploiting
multi-image. Motivated by that multi-exposure images are complementary in
denoising, deblurring, high dynamic range imaging, and super-resolution, we
propose to utilize bracketing photography to unify restoration and enhancement
tasks in this work. Due to the difficulty in collecting real-world pairs, we
suggest a solution that first pre-trains the model with synthetic paired data
and then adapts it to real-world unlabeled images. In particular, a temporally
modulated recurrent network (TMRNet) and self-supervised adaptation method are
proposed. Moreover, we construct a data simulation pipeline to synthesize pairs
and collect real-world images from 200 nighttime scenarios. Experiments on both
datasets show that our method performs favorably against the state-of-the-art
multi-image processing ones. The dataset, code, and pre-trained models are
available at https://github.com/cszhilu1998/BracketIRE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhilu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuohao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Renlong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zifei Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wangmeng Zuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00789">
<title>Retrieval-Augmented Egocentric Video Captioning. (arXiv:2401.00789v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00789</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding human actions from videos of first-person view poses
significant challenges. Most prior approaches explore representation learning
on egocentric videos only, while overlooking the potential benefit of
exploiting existing large-scale third-person videos. In this paper, (1) we
develop EgoInstructor, a retrieval-augmented multimodal captioning model that
automatically retrieves semantically relevant third-person instructional videos
to enhance the video captioning of egocentric videos. (2) For training the
cross-view retrieval module, we devise an automatic pipeline to discover
ego-exo video pairs from distinct large-scale egocentric and exocentric
datasets. (3) We train the cross-view retrieval module with a novel EgoExoNCE
loss that pulls egocentric and exocentric video features closer by aligning
them to shared text features that describe similar actions. (4) Through
extensive experiments, our cross-view retrieval module demonstrates superior
performance across seven benchmarks. Regarding egocentric video captioning,
EgoInstructor exhibits significant improvements by leveraging third-person
videos as references.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jilan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yifei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Junlin Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuejie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1&quot;&gt;Rui Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weidi Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00816">
<title>GLIMPSE: Generalized Local Imaging with MLPs. (arXiv:2401.00816v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00816</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning is the current de facto state of the art in tomographic
imaging. A common approach is to feed the result of a simple inversion, for
example the backprojection, to a convolutional neural network (CNN) which then
computes the reconstruction. Despite strong results on &apos;in-distribution&apos; test
data similar to the training data, backprojection from sparse-view data
delocalizes singularities, so these approaches require a large receptive field
to perform well. As a consequence, they overfit to certain global structures
which leads to poor generalization on out-of-distribution (OOD) samples.
Moreover, their memory complexity and training time scale unfavorably with
image resolution, making them impractical for application at realistic clinical
resolutions, especially in 3D: a standard U-Net requires a substantial 140GB of
memory and 2600 seconds per epoch on a research-grade GPU when training on
1024x1024 images. In this paper, we introduce GLIMPSE, a local processing
neural network for computed tomography which reconstructs a pixel value by
feeding only the measurements associated with the neighborhood of the pixel to
a simple MLP. While achieving comparable or better performance with successful
CNNs like the U-Net on in-distribution test data, GLIMPSE significantly
outperforms them on OOD samples while maintaining a memory footprint almost
independent of image resolution; 5GB memory suffices to train on 1024x1024
images. Further, we built GLIMPSE to be fully differentiable, which enables
feats such as recovery of accurate projection angles if they are out of
calibration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khorashadizadeh_A/0/1/0/all/0/1&quot;&gt;AmirEhsan Khorashadizadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Debarnot_V/0/1/0/all/0/1&quot;&gt;Valentin Debarnot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianlin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dokmanic_I/0/1/0/all/0/1&quot;&gt;Ivan Dokmani&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00825">
<title>Sharp-NeRF: Grid-based Fast Deblurring Neural Radiance Fields Using Sharpness Prior. (arXiv:2401.00825v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00825</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRF) have shown remarkable performance in neural
rendering-based novel view synthesis. However, NeRF suffers from severe visual
quality degradation when the input images have been captured under imperfect
conditions, such as poor illumination, defocus blurring, and lens aberrations.
Especially, defocus blur is quite common in the images when they are normally
captured using cameras. Although few recent studies have proposed to render
sharp images of considerably high-quality, yet they still face many key
challenges. In particular, those methods have employed a Multi-Layer Perceptron
(MLP) based NeRF, which requires tremendous computational time. To overcome
these shortcomings, this paper proposes a novel technique Sharp-NeRF -- a
grid-based NeRF that renders clean and sharp images from the input blurry
images within half an hour of training. To do so, we used several grid-based
kernels to accurately model the sharpness/blurriness of the scene. The
sharpness level of the pixels is computed to learn the spatially varying blur
kernels. We have conducted experiments on the benchmarks consisting of blurry
images and have evaluated full-reference and non-reference metrics. The
qualitative and quantitative results have revealed that our approach renders
the sharp novel views with vivid colors and fine details, and it has
considerably faster training time than the previous works. Our project page is
available at https://benhenryl.github.io/SharpNeRF/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1&quot;&gt;Byeonghyeon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Howoong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_U/0/1/0/all/0/1&quot;&gt;Usman Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1&quot;&gt;Eunbyung Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00833">
<title>Rethinking RAFT for Efficient Optical Flow. (arXiv:2401.00833v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00833</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite significant progress in deep learning-based optical flow methods,
accurately estimating large displacements and repetitive patterns remains a
challenge. The limitations of local features and similarity search patterns
used in these algorithms contribute to this issue. Additionally, some existing
methods suffer from slow runtime and excessive graphic memory consumption. To
address these problems, this paper proposes a novel approach based on the RAFT
framework. The proposed Attention-based Feature Localization (AFL) approach
incorporates the attention mechanism to handle global feature extraction and
address repetitive patterns. It introduces an operator for matching pixels with
corresponding counterparts in the second frame and assigning accurate flow
values. Furthermore, an Amorphous Lookup Operator (ALO) is proposed to enhance
convergence speed and improve RAFTs ability to handle large displacements by
reducing data redundancy in its search operator and expanding the search space
for similarity extraction. The proposed method, Efficient RAFT
(Ef-RAFT),achieves significant improvements of 10% on the Sintel dataset and 5%
on the KITTI dataset over RAFT. Remarkably, these enhancements are attained
with a modest 33% reduction in speed and a mere 13% increase in memory usage.
The code is available at: https://github.com/n3slami/Ef-RAFT
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eslami_N/0/1/0/all/0/1&quot;&gt;Navid Eslami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arefi_F/0/1/0/all/0/1&quot;&gt;Farnoosh Arefi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansourian_A/0/1/0/all/0/1&quot;&gt;Amir M. Mansourian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1&quot;&gt;Shohreh Kasaei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00834">
<title>Deblurring 3D Gaussian Splatting. (arXiv:2401.00834v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00834</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies in Radiance Fields have paved the robust way for novel view
synthesis with their photorealistic rendering quality. Nevertheless, they
usually employ neural networks and volumetric rendering, which are costly to
train and impede their broad use in various real-time applications due to the
lengthy rendering time. Lately 3D Gaussians splatting-based approach has been
proposed to model the 3D scene, and it achieves remarkable visual quality while
rendering the images in real-time. However, it suffers from severe degradation
in the rendering quality if the training images are blurry. Blurriness commonly
occurs due to the lens defocusing, object motion, and camera shake, and it
inevitably intervenes in clean image acquisition. Several previous studies have
attempted to render clean and sharp images from blurry input images using
neural fields. The majority of those works, however, are designed only for
volumetric rendering-based neural radiance fields and are not straightforwardly
applicable to rasterization-based 3D Gaussian splatting methods. Thus, we
propose a novel real-time deblurring framework, deblurring 3D Gaussian
Splatting, using a small Multi-Layer Perceptron (MLP) that manipulates the
covariance of each 3D Gaussian to model the scene blurriness. While deblurring
3D Gaussian Splatting can still enjoy real-time rendering, it can reconstruct
fine and sharp details from blurry images. A variety of experiments have been
conducted on the benchmark, and the results have revealed the effectiveness of
our approach for deblurring. Qualitative results are available at
https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1&quot;&gt;Byeonghyeon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Howoong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiangyu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_U/0/1/0/all/0/1&quot;&gt;Usman Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1&quot;&gt;Eunbyung Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00847">
<title>Mocap Everyone Everywhere: Lightweight Motion Capture With Smartwatches and a Head-Mounted Camera. (arXiv:2401.00847v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00847</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a lightweight and affordable motion capture method based on two
smartwatches and a head-mounted camera. In contrast to the existing approaches
that use six or more expert-level IMU devices, our approach is much more
cost-effective and convenient. Our method can make wearable motion capture
accessible to everyone everywhere, enabling 3D full-body motion capture in
diverse environments. As a key idea to overcome the extreme sparsity and
ambiguities of sensor inputs, we integrate 6D head poses obtained from the
head-mounted cameras for motion estimation. To enable capture in expansive
indoor and outdoor scenes, we propose an algorithm to track and update floor
level changes to define head poses, coupled with a multi-stage
Transformer-based regression module. We also introduce novel strategies
leveraging visual cues of egocentric images to further enhance the motion
capture quality while reducing ambiguities. We demonstrate the performance of
our method on various challenging scenarios, including complex outdoor
environments and everyday motions including object interactions and social
interactions among multiple individuals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jiye Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1&quot;&gt;Hanbyul Joo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00849">
<title>COSMO: COntrastive Streamlined MultimOdal Model with Interleaved Pre-Training. (arXiv:2401.00849v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00849</link>
<description rdf:parseType="Literal">&lt;p&gt;In the evolution of Vision-Language Pre-training, shifting from short-text
comprehension to encompassing extended textual contexts is pivotal. Recent
autoregressive vision-language models like \cite{flamingo, palme}, leveraging
the long-context capability of Large Language Models, have excelled in few-shot
text generation tasks but face challenges in alignment tasks. Addressing this
gap, we introduce the contrastive loss into text generation models, presenting
the COntrastive-Streamlined MultimOdal framework (\ModelName), strategically
partitioning the language model into dedicated unimodal text processing and
adept multimodal data handling components. \ModelName, our unified framework,
merges unimodal and multimodal elements, enhancing model performance for tasks
involving textual and visual data while notably reducing learnable parameters.
However, these models demand extensive long-text datasets, yet the availability
of high-quality long-text video datasets remains limited. To bridge this gap,
this work introduces \VideoDatasetName, an inaugural interleaved video-text
dataset featuring comprehensive captions, marking a significant step forward.
Demonstrating its impact, we illustrate how \VideoDatasetName{} enhances model
performance in image-text tasks. With 34% learnable parameters and utilizing
72\% of the available data, our model demonstrates significant superiority over
OpenFlamingo~\cite{openflamingo}. For instance, in the 4-shot flickr captioning
task, performance notably improves from 57.2% to 65.\%. The contributions of
\ModelName{} and \VideoDatasetName{} are underscored by notable performance
gains across 14 diverse downstream datasets encompassing both image-text and
video-text tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Alex Jinpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kevin Qinghong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kevin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhengyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lijuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00850">
<title>Refining Pre-Trained Motion Models. (arXiv:2401.00850v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00850</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the difficulty of manually annotating motion in video, the current best
motion estimation methods are trained with synthetic data, and therefore
struggle somewhat due to a train/test gap. Self-supervised methods hold the
promise of training directly on real video, but typically perform worse. These
include methods trained with warp error (i.e., color constancy) combined with
smoothness terms, and methods that encourage cycle-consistency in the estimates
(i.e., tracking backwards should yield the opposite trajectory as tracking
forwards). In this work, we take on the challenge of improving state-of-the-art
supervised models with self-supervised training. We find that when the
initialization is supervised weights, most existing self-supervision techniques
actually make performance worse instead of better, which suggests that the
benefit of seeing the new data is overshadowed by the noise in the training
signal. Focusing on obtaining a ``clean&apos;&apos; training signal from real-world
unlabelled video, we propose to separate label-making and training into two
distinct stages. In the first stage, we use the pre-trained model to estimate
motion in a video, and then select the subset of motion estimates which we can
verify with cycle-consistency. This produces a sparse but accurate
pseudo-labelling of the video. In the second stage, we fine-tune the model to
reproduce these outputs, while also applying augmentations on the input. We
complement this boot-strapping method with simple techniques that densify and
re-balance the pseudo-labels, ensuring that we do not merely train on ``easy&apos;&apos;
tracks. We show that our method yields reliable gains over fully-supervised
methods in real videos, for both short-term (flow-based) and long-range
(multi-frame) pixel tracking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xinglong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1&quot;&gt;Adam W. Harley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1&quot;&gt;Leonidas J. Guibas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2102.13272">
<title>Accelerating Large Kernel Convolutions with Nested Winograd Transformation.pdf. (arXiv:2102.13272v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2102.13272</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent literature has shown that convolutional neural networks (CNNs) with
large kernels outperform vision transformers (ViTs) and CNNs with stacked small
kernels in many computer vision tasks, such as object detection and image
restoration. The Winograd transformation helps reduce the number of repetitive
multiplications in convolution and is widely supported by many commercial AI
processors. Researchers have proposed accelerating large kernel convolutions by
linearly decomposing them into many small kernel convolutions and then
sequentially accelerating each small kernel convolution with the Winograd
algorithm. This work proposes a nested Winograd algorithm that iteratively
decomposes a large kernel convolution into small kernel convolutions and proves
it to be more effective than the linear decomposition Winograd transformation
algorithm. Experiments show that compared to the linear decomposition Winograd
algorithm, the proposed algorithm reduces the total number of multiplications
by 1.4 to 10.5 times for computing 4x4 to 31x31 convolutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jingbo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xizi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsui_C/0/1/0/all/0/1&quot;&gt;Chi-Ying Tsui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.14174">
<title>Cross-Camera Human Motion Transfer by Time Series Analysis. (arXiv:2109.14174v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2109.14174</link>
<description rdf:parseType="Literal">&lt;p&gt;With advances in optical sensor technology, heterogeneous camera systems are
increasingly used for high-resolution (HR) video acquisition and analysis.
However, motion transfer across multiple cameras poses challenges. To address
this, we propose an algorithm based on time series analysis that identifies
motion seasonality and constructs an additive model to extract transferable
patterns. Validated on real-world data, our algorithm demonstrates
effectiveness and interpretability. Notably, it improves pose estimation in
low-resolution videos by leveraging patterns derived from HR counterparts,
enhancing practical utility. Code is available at:
https://github.com/IndigoPurple/TSAMT
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yaping Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanghan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_E/0/1/0/all/0/1&quot;&gt;Edmund Y. Lam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.02980">
<title>3D Object Detection from Images for Autonomous Driving: A Survey. (arXiv:2202.02980v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2202.02980</link>
<description rdf:parseType="Literal">&lt;p&gt;3D object detection from images, one of the fundamental and challenging
problems in autonomous driving, has received increasing attention from both
industry and academia in recent years. Benefiting from the rapid development of
deep learning technologies, image-based 3D detection has achieved remarkable
progress. Particularly, more than 200 works have studied this problem from 2015
to 2021, encompassing a broad spectrum of theories, algorithms, and
applications. However, to date no recent survey exists to collect and organize
this knowledge. In this paper, we fill this gap in the literature and provide
the first comprehensive survey of this novel and continuously growing research
field, summarizing the most commonly used pipelines for image-based 3D
detection and deeply analyzing each of their components. Additionally, we also
propose two new taxonomies to organize the state-of-the-art methods into
different categories, with the intent of providing a more systematic review of
existing methods and facilitating fair comparisons with future works. In
retrospect of what has been achieved so far, we also analyze the current
challenges in the field and discuss future directions for image-based 3D
detection research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xinzhu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simonelli_A/0/1/0/all/0/1&quot;&gt;Andrea Simonelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1&quot;&gt;Elisa Ricci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.06424">
<title>VariabilityTrack:Multi-Object Tracking with Variable Speed Object Movement. (arXiv:2203.06424v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.06424</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-object tracking (MOT) aims at estimating bounding boxes and identities
of objects in videos. Most methods can be roughly classified as
tracking-by-detection and joint-detection-association paradigms. Although the
latter has elicited more attention and demonstrates comparable performance
relative than the former, we claim that the tracking-by-detection paradigm is
still the optimal solution in terms of tracking accuracy,such as
ByteTrack,which achieves 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA on the test set of
MOT17 with 30 FPS running speed on a single V100 GPU.However, under complex
perspectives such as vehicle and UAV acceleration, the performance of such a
tracker using uniform Kalman filter will be greatly affected, resulting in
tracking loss.In this paper, we propose a variable speed Kalman filter
algorithm based on environmental feedback and improve the matching process,
which can greatly improve the tracking effect in complex variable speed scenes
while maintaining high tracking accuracy in relatively static scenes.
Eventually, higher MOTA and IDF1 results can be achieved on MOT17 test set than
ByteTrack
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1&quot;&gt;Run Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;JinLin Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Qiao Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.07436">
<title>SuperAnimal pretrained pose estimation models for behavioral analysis. (arXiv:2203.07436v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.07436</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantification of behavior is critical in applications ranging from
neuroscience, veterinary medicine and animal conservation efforts. A common key
step for behavioral analysis is first extracting relevant keypoints on animals,
known as pose estimation. However, reliable inference of poses currently
requires domain knowledge and manual labeling effort to build supervised
models. We present a series of technical innovations that enable a new method,
collectively called SuperAnimal, to develop unified foundation models that can
be used on over 45 species, without additional human labels. Concretely, we
introduce a method to unify the keypoint space across differently labeled
datasets (via our generalized data converter) and for training these diverse
datasets in a manner such that they don&apos;t catastrophically forget keypoints
given the unbalanced inputs (via our keypoint gradient masking and memory
replay approaches). These models show excellent performance across six pose
benchmarks. Then, to ensure maximal usability for end-users, we demonstrate how
to fine-tune the models on differently labeled data and provide tooling for
unsupervised video adaptation to boost performance and decrease jitter across
frames. If the models are fine-tuned, we show SuperAnimal models are
10-100$\times$ more data efficient than prior transfer-learning-based
approaches. We illustrate the utility of our models in behavioral
classification in mice and gait analysis in horses. Collectively, this presents
a data-efficient solution for animal pose estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1&quot;&gt;Shaokai Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filippova_A/0/1/0/all/0/1&quot;&gt;Anastasiia Filippova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lauer_J/0/1/0/all/0/1&quot;&gt;Jessy Lauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1&quot;&gt;Steffen Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vidal_M/0/1/0/all/0/1&quot;&gt;Maxime Vidal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_T/0/1/0/all/0/1&quot;&gt;Tian Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathis_A/0/1/0/all/0/1&quot;&gt;Alexander Mathis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathis_M/0/1/0/all/0/1&quot;&gt;Mackenzie Weygandt Mathis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.00946">
<title>Motion-aware Memory Network for Fast Video Salient Object Detection. (arXiv:2208.00946v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.00946</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous methods based on 3DCNN, convLSTM, or optical flow have achieved
great success in video salient object detection (VSOD). However, they still
suffer from high computational costs or poor quality of the generated saliency
maps. To solve these problems, we design a space-time memory (STM)-based
network, which extracts useful temporal information of the current frame from
adjacent frames as the temporal branch of VSOD. Furthermore, previous methods
only considered single-frame prediction without temporal association. As a
result, the model may not focus on the temporal information sufficiently. Thus,
we initially introduce object motion prediction between inter-frame into VSOD.
Our model follows standard encoder--decoder architecture. In the encoding
stage, we generate high-level temporal features by using high-level features
from the current and its adjacent frames. This approach is more efficient than
the optical flow-based methods. In the decoding stage, we propose an effective
fusion strategy for spatial and temporal branches. The semantic information of
the high-level features is used to fuse the object details in the low-level
features, and then the spatiotemporal features are obtained step by step to
reconstruct the saliency maps. Moreover, inspired by the boundary supervision
commonly used in image salient object detection (ISOD), we design a
motion-aware loss for predicting object boundary motion and simultaneously
perform multitask learning for VSOD and object motion prediction, which can
further facilitate the model to extract spatiotemporal features accurately and
maintain the object integrity. Extensive experiments on several datasets
demonstrated the effectiveness of our method and can achieve state-of-the-art
metrics on some datasets. The proposed model does not require optical flow or
other preprocessing, and can reach a speed of nearly 100 FPS during inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Haoran Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peipei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1&quot;&gt;Guodao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Dongdong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1&quot;&gt;Ronghua Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaofei He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.06950">
<title>Lossy Image Compression with Conditional Diffusion Models. (arXiv:2209.06950v7 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.06950</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper outlines an end-to-end optimized lossy image compression framework
using diffusion generative models. The approach relies on the transform coding
paradigm, where an image is mapped into a latent space for entropy coding and,
from there, mapped back to the data space for reconstruction. In contrast to
VAE-based neural compression, where the (mean) decoder is a deterministic
neural network, our decoder is a conditional diffusion model. Our approach thus
introduces an additional ``content&apos;&apos; latent variable on which the reverse
diffusion process is conditioned and uses this variable to store information
about the image. The remaining ``texture&apos;&apos; variables characterizing the
diffusion process are synthesized at decoding time. We show that the model&apos;s
performance can be tuned toward perceptual metrics of interest. Our extensive
experiments involving multiple datasets and image quality assessment metrics
show that our approach yields stronger reported FID scores than the GAN-based
model, while also yielding competitive performance with VAE-based models in
several distortion metrics. Furthermore, training the diffusion with
$\mathcal{X}$-parameterization enables high-quality reconstructions in only a
handful of decoding steps, greatly affecting the model&apos;s practicality. Our code
is available at: \url{https://github.com/buggyyang/CDC_compression}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Ruihan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mandt_S/0/1/0/all/0/1&quot;&gt;Stephan Mandt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.14416">
<title>Residual Back Projection With Untrained Neural Networks. (arXiv:2210.14416v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.14416</link>
<description rdf:parseType="Literal">&lt;p&gt;Background and Objective: The success of neural networks in a number of image
processing tasks has motivated their application in image reconstruction
problems in computed tomography (CT). While progress has been made in this
area, the lack of stability and theoretical guarantees for accuracy, together
with the scarcity of high-quality training data for specific imaging domains
pose challenges for many CT applications. In this paper, we present a framework
for iterative reconstruction (IR) in CT that leverages the hierarchical
structure of neural networks, without the need for training. Our framework
incorporates this structural information as a deep image prior (DIP), and uses
a novel residual back projection (RBP) connection that forms the basis for our
iterations.
&lt;/p&gt;
&lt;p&gt;Methods: We propose using an untrained U-net in conjunction with a novel
residual back projection to minimize an objective function and achieve
high-accuracy reconstruction. In each iteration, the weights of the untrained
U-net are optimized, and the output of the U-net in the current iteration is
used to update the input of the U-net in the next iteration through the
aforementioned RBP connection.
&lt;/p&gt;
&lt;p&gt;Results: Experimental results demonstrate that the RBP-DIP framework offers
improvements over other state-of-the-art conventional IR methods, as well as
pre-trained and untrained models with similar network structures under multiple
conditions. These improvements are particularly significant in the few-view,
limited-angle, and low-dose imaging configurations.
&lt;/p&gt;
&lt;p&gt;Conclusions: Applying to both parallel and fan beam X-ray imaging, our
framework shows significant improvement under multiple conditions. Furthermore,
the proposed framework requires no training data and can be adjusted on-demand
to adapt to different conditions (e.g. noise level, geometry, and imaged
object).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shu_Z/0/1/0/all/0/1&quot;&gt;Ziyu Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Entezari_A/0/1/0/all/0/1&quot;&gt;Alireza Entezari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.10867">
<title>Rethinking the Paradigm of Content Constraints in GAN-based Unpaired Image-to-Image Translation. (arXiv:2211.10867v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.10867</link>
<description rdf:parseType="Literal">&lt;p&gt;In an unpaired setting, lacking sufficient content constraints for
image-to-image translation (I2I) tasks, GAN-based approaches are usually prone
to model collapse. Current solutions can be divided into two categories,
reconstruction-based and Siamese network-based. The former requires that the
transformed or transforming image can be perfectly converted back to the
original image, which is sometimes too strict and limits the generative
performance. The latter involves feeding the original and generated images into
a feature extractor and then matching their outputs. This is not efficient
enough, and a universal feature extractor is not easily available. In this
paper, we propose EnCo, a simple but efficient way to maintain the content by
constraining the representational similarity in the latent space of patch-level
features from the same stage of the \textbf{En}coder and de\textbf{Co}der of
the generator. For the similarity function, we use a simple MSE loss instead of
contrastive loss, which is currently widely used in I2I tasks. Benefits from
the design, EnCo training is extremely efficient, while the features from the
encoder produce a more positive effect on the decoding, leading to more
satisfying generations. In addition, we rethink the role played by
discriminators in sampling patches and propose a discriminative
attention-guided (DAG) patch sampling strategy to replace random sampling. DAG
is parameter-free and only requires negligible computational overhead, while
significantly improving the performance of the model. Extensive experiments on
multiple datasets demonstrate the effectiveness and advantages of EnCo, and we
achieve multiple state-of-the-art compared to previous methods. Our code is
available at https://github.com/XiudingCai/EnCo-pytorch.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1&quot;&gt;Xiuding Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yaoyao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_D/0/1/0/all/0/1&quot;&gt;Dong Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1&quot;&gt;Linjie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yu Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10772">
<title>Low-Light Image and Video Enhancement: A Comprehensive Survey and Beyond. (arXiv:2212.10772v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10772</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a comprehensive survey of low-light image and video
enhancement, addressing two primary challenges in the field. The first
challenge is the prevalence of mixed over-/under-exposed images, which are not
adequately addressed by existing methods. In response, this work introduces two
enhanced variants of the SICE dataset: SICE_Grad and SICE_Mix, designed to
better represent these complexities. The second challenge is the scarcity of
suitable low-light video datasets for training and testing. To address this,
the paper introduces the Night Wenzhou dataset, a large-scale, high-resolution
video collection that features challenging fast-moving aerial scenes and
streetscapes with varied illuminations and degradation. This study also
conducts an extensive analysis of key techniques and performs comparative
experiments using the proposed and current benchmark datasets. The survey
concludes by highlighting emerging applications, discussing unresolved
challenges, and suggesting future research directions within the LLIE
community. The datasets are available at
https://github.com/ShenZheng2000/LLIE_Survey.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yiling Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jinqian Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Changjie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1&quot;&gt;Gaurav Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.14418">
<title>PCR-CG: Point Cloud Registration via Deep Explicit Color and Geometry. (arXiv:2302.14418v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.14418</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce PCR-CG: a novel 3D point cloud registration
module explicitly embedding the color signals into the geometry representation.
Different from previous methods that only use geometry representation, our
module is specifically designed to effectively correlate color into geometry
for the point cloud registration task. Our key contribution is a 2D-3D
cross-modality learning algorithm that embeds the deep features learned from
color signals to the geometry representation. With our designed 2D-3D
projection module, the pixel features in a square region centered at
correspondences perceived from images are effectively correlated with point
clouds. In this way, the overlapped regions can be inferred not only from point
cloud but also from the texture appearances. Adding color is non-trivial. We
compare against a variety of baselines designed for adding color to 3D, such as
exhaustively adding per-pixel features or RGB values in an implicit manner. We
leverage Predator [25] as the baseline method and incorporate our proposed
module onto it. To validate the effectiveness of 2D features, we ablate
different 2D pre-trained networks and show a positive correlation between the
pre-trained weights and the task performance. Our experimental results indicate
a significant improvement of 6.5% registration recall over the baseline method
on the 3DLoMatch benchmark. We additionally evaluate our approach on SOTA
methods and observe consistent improvements, such as an improvement of 2.4%
registration recall over GeoTransformer as well as 3.5% over CoFiNet. Our study
reveals a significant advantages of correlating explicit deep color features to
the point cloud in the registration task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Junle Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wenhui Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Ji Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05807">
<title>Aleth-NeRF: Low-light Condition View Synthesis with Concealing Fields. (arXiv:2303.05807v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05807</link>
<description rdf:parseType="Literal">&lt;p&gt;Common capture low-light scenes are challenging for most computer vision
techniques, including Neural Radiance Fields (NeRF). Vanilla NeRF is
viewer-centred simplifies the rendering process only as light emission from 3D
locations in the viewing direction, thus failing to model the low-illumination
induced darkness. Inspired by the emission theory of ancient Greeks that visual
perception is accomplished by rays casting from eyes, we make slight
modifications on vanilla NeRF to train on multiple views of low-light scenes,
we can thus render out the well-lit scene in an unsupervised manner. We
introduce a surrogate concept, Concealing Fields, that reduces the transport of
light during the volume rendering stage. Specifically, our proposed method,
Aleth-NeRF, directly learns from the dark image to understand volumetric object
representation and concealing field under priors. By simply eliminating
Concealing Fields, we can render a single or multi-view well-lit image(s) and
gain superior performance over other 2D low-light enhancement methods.
Additionally, we collect the first paired LOw-light and normal-light Multi-view
(LOM) datasets for future research. This version is invalid, please refer to
our new AAAI version: &lt;a href=&quot;/abs/2312.09093&quot;&gt;arXiv:2312.09093&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1&quot;&gt;Ziteng Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1&quot;&gt;Lin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xianzheng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1&quot;&gt;Tatsuya Harada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09541">
<title>Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains. (arXiv:2303.09541v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09541</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent text-to-image generative models have exhibited remarkable abilities in
generating high-fidelity and photo-realistic images. However, despite the
visually impressive results, these models often struggle to preserve plausible
human structure in the generations. Due to this reason, while generative models
have shown promising results in aiding downstream image recognition tasks by
generating large volumes of synthetic data, they are not suitable for improving
downstream human pose perception and understanding. In this work, we propose a
Diffusion model with Human Pose Correction (Diffusion-HPC), a text-conditioned
method that generates photo-realistic images with plausible posed humans by
injecting prior knowledge about human body structure. Our generated images are
accompanied by 3D meshes that serve as ground truths for improving Human Mesh
Recovery tasks, where a shortage of 3D training data has long been an issue.
Furthermore, we show that Diffusion-HPC effectively improves the realism of
human generations under varying conditioning strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1&quot;&gt;Zhenzhen Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bravo_Sanchez_L/0/1/0/all/0/1&quot;&gt;Laura Bravo-S&amp;#xe1;nchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_Levy_S/0/1/0/all/0/1&quot;&gt;Serena Yeung-Levy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.05673">
<title>Precise localization of corneal reflections in eye images using deep learning trained on synthetic data. (arXiv:2304.05673v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.05673</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a deep learning method for accurately localizing the center of a
single corneal reflection (CR) in an eye image. Unlike previous approaches, we
use a convolutional neural network (CNN) that was trained solely using
simulated data. Using only simulated data has the benefit of completely
sidestepping the time-consuming process of manual annotation that is required
for supervised training on real eye images. To systematically evaluate the
accuracy of our method, we first tested it on images with simulated CRs placed
on different backgrounds and embedded in varying levels of noise. Second, we
tested the method on high-quality videos captured from real eyes. Our method
outperformed state-of-the-art algorithmic methods on real eye images with a 35%
reduction in terms of spatial precision, and performed on par with
state-of-the-art on simulated images in terms of spatial accuracy.We conclude
that our method provides a precise method for CR center localization and
provides a solution to the data availability problem which is one of the
important common roadblocks in the development of deep learning models for gaze
estimation. Due to the superior CR center localization and ease of application,
our method has the potential to improve the accuracy and precision of CR-based
eye trackers
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byrne_S/0/1/0/all/0/1&quot;&gt;Sean Anthony Byrne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nystrom_M/0/1/0/all/0/1&quot;&gt;Marcus Nystr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maquiling_V/0/1/0/all/0/1&quot;&gt;Virmarie Maquiling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1&quot;&gt;Enkelejda Kasneci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niehorster_D/0/1/0/all/0/1&quot;&gt;Diederick C. Niehorster&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07597">
<title>An Instance Segmentation Dataset of Yeast Cells in Microstructures. (arXiv:2304.07597v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07597</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracting single-cell information from microscopy data requires accurate
instance-wise segmentations. Obtaining pixel-wise segmentations from microscopy
imagery remains a challenging task, especially with the added complexity of
microstructured environments. This paper presents a novel dataset for
segmenting yeast cells in microstructures. We offer pixel-wise instance
segmentation labels for both cells and trap microstructures. In total, we
release 493 densely annotated microscopy images. To facilitate a unified
comparison between novel segmentation algorithms, we propose a standardized
evaluation strategy for our dataset. The aim of the dataset and evaluation
strategy is to facilitate the development of new cell segmentation approaches.
The dataset is publicly available at
https://christophreich1996.github.io/yeast_in_microstructures_dataset/ .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1&quot;&gt;Christoph Reich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prangemeier_T/0/1/0/all/0/1&quot;&gt;Tim Prangemeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francani_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; O. Fran&amp;#xe7;ani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1&quot;&gt;Heinz Koeppl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08842">
<title>UDTIRI: An Online Open-Source Intelligent Road Inspection Benchmark Suite. (arXiv:2304.08842v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08842</link>
<description rdf:parseType="Literal">&lt;p&gt;In the nascent domain of urban digital twins (UDT), the prospects for
leveraging cutting-edge deep learning techniques are vast and compelling.
Particularly within the specialized area of intelligent road inspection (IRI),
a noticeable gap exists, underscored by the current dearth of dedicated
research efforts and the lack of large-scale well-annotated datasets. To foster
advancements in this burgeoning field, we have launched an online open-source
benchmark suite, referred to as UDTIRI. Along with this article, we introduce
the road pothole detection task, the first online competition published within
this benchmark suite. This task provides a well-annotated dataset, comprising
1,000 RGB images and their pixel/instance-level ground-truth annotations,
captured in diverse real-world scenarios under different illumination and
weather conditions. Our benchmark provides a systematic and thorough evaluation
of state-of-the-art object detection, semantic segmentation, and instance
segmentation networks, developed based on either convolutional neural networks
or Transformers. We anticipate that our benchmark will serve as a catalyst for
the integration of advanced UDT techniques into IRI. By providing algorithms
with a more comprehensive understanding of diverse road conditions, we seek to
unlock their untapped potential and foster innovation in this critical domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Sicen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiahang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Dacheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Denghuang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Shuai Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xingyi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qijun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1&quot;&gt;Rui Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.13586">
<title>Energy-Based Sliced Wasserstein Distance. (arXiv:2304.13586v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2304.13586</link>
<description rdf:parseType="Literal">&lt;p&gt;The sliced Wasserstein (SW) distance has been widely recognized as a
statistically effective and computationally efficient metric between two
probability measures. A key component of the SW distance is the slicing
distribution. There are two existing approaches for choosing this distribution.
The first approach is using a fixed prior distribution. The second approach is
optimizing for the best distribution which belongs to a parametric family of
distributions and can maximize the expected distance. However, both approaches
have their limitations. A fixed prior distribution is non-informative in terms
of highlighting projecting directions that can discriminate two general
probability measures. Doing optimization for the best distribution is often
expensive and unstable. Moreover, designing the parametric family of the
candidate distribution could be easily misspecified. To address the issues, we
propose to design the slicing distribution as an energy-based distribution that
is parameter-free and has the density proportional to an energy function of the
projected one-dimensional Wasserstein distance. We then derive a novel sliced
Wasserstein metric, energy-based sliced Waserstein (EBSW) distance, and
investigate its topological, statistical, and computational properties via
importance sampling, sampling importance resampling, and Markov Chain methods.
Finally, we conduct experiments on point-cloud gradient flow, color transfer,
and point-cloud reconstruction to show the favorable performance of the EBSW.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Khai Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ho_N/0/1/0/all/0/1&quot;&gt;Nhat Ho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11946">
<title>Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions. (arXiv:2305.11946v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11946</link>
<description rdf:parseType="Literal">&lt;p&gt;Statistical shape modeling (SSM) is an essential tool for analyzing
variations in anatomical morphology. In a typical SSM pipeline, 3D anatomical
images, gone through segmentation and rigid registration, are represented using
lower-dimensional shape features, on which statistical analysis can be
performed. Various methods for constructing compact shape representations have
been proposed, but they involve laborious and costly steps. We propose
Image2SSM, a novel deep-learning-based approach for SSM that leverages
image-segmentation pairs to learn a radial-basis-function (RBF)-based
representation of shapes directly from images. This RBF-based shape
representation offers a rich self-supervised signal for the network to estimate
a continuous, yet compact representation of the underlying surface that can
adapt to complex geometries in a data-driven manner. Image2SSM can characterize
populations of biological structures of interest by constructing statistical
landmark-based shape models of ensembles of anatomical shapes while requiring
minimal parameter tuning and no user assistance. Once trained, Image2SSM can be
used to infer low-dimensional shape representations from new unsegmented
images, paving the way toward scalable approaches for SSM, especially when
dealing with large cohorts. Experiments on synthetic and real datasets show the
efficacy of the proposed method compared to the state-of-art
correspondence-based method for SSM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhabian_S/0/1/0/all/0/1&quot;&gt;Shireen Y. Elhabian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14669">
<title>NegVSR: Augmenting Negatives for Generalized Noise Modeling in Real-World Video Super-Resolution. (arXiv:2305.14669v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14669</link>
<description rdf:parseType="Literal">&lt;p&gt;The capability of video super-resolution (VSR) to synthesize high-resolution
(HR) video from ideal datasets has been demonstrated in many works. However,
applying the VSR model to real-world video with unknown and complex degradation
remains a challenging task. First, existing degradation metrics in most VSR
methods are not able to effectively simulate real-world noise and blur. On the
contrary, simple combinations of classical degradation are used for real-world
noise modeling, which led to the VSR model often being violated by
out-of-distribution noise. Second, many SR models focus on noise simulation and
transfer. Nevertheless, the sampled noise is monotonous and limited. To address
the aforementioned problems, we propose a Negatives augmentation strategy for
generalized noise modeling in Video Super-Resolution (NegVSR) task.
Specifically, we first propose sequential noise generation toward real-world
data to extract practical noise sequences. Then, the degeneration domain is
widely expanded by negative augmentation to build up various yet challenging
real-world noise sets. We further propose the augmented negative guidance loss
to learn robust features among augmented negatives effectively. Extensive
experiments on real-world datasets (e.g., VideoLQ and FLIR) show that our
method outperforms state-of-the-art methods with clear margins, especially in
visual quality. Project page is available at: https://negvsr.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yexing Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meilin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhijing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xian_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Xian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yukai Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16283">
<title>CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graph Diffusion. (arXiv:2305.16283v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16283</link>
<description rdf:parseType="Literal">&lt;p&gt;Controllable scene synthesis aims to create interactive environments for
various industrial use cases. Scene graphs provide a highly suitable interface
to facilitate these applications by abstracting the scene context in a compact
manner. Existing methods, reliant on retrieval from extensive databases or
pre-trained shape embeddings, often overlook scene-object and object-object
relationships, leading to inconsistent results due to their limited generation
capacity. To address this issue, we present CommonScenes, a fully generative
model that converts scene graphs into corresponding controllable 3D scenes,
which are semantically realistic and conform to commonsense. Our pipeline
consists of two branches, one predicting the overall scene layout via a
variational auto-encoder and the other generating compatible shapes via latent
diffusion, capturing global scene-object and local inter-object relationships
in the scene graph while preserving shape diversity. The generated scenes can
be manipulated by editing the input scene graph and sampling the noise in the
diffusion model. Due to lacking a scene graph dataset offering high-quality
object-level meshes with relations, we also construct SG-FRONT, enriching the
off-the-shelf indoor dataset 3D-FRONT with additional scene graph labels.
Extensive experiments are conducted on SG-FRONT where CommonScenes shows clear
advantages over other methods regarding generation consistency, quality, and
diversity. Codes and the dataset will be released upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1&quot;&gt;Guangyao Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ornek_E/0/1/0/all/0/1&quot;&gt;Evin P&amp;#x131;nar &amp;#xd6;rnek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shun-Cheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Di_Y/0/1/0/all/0/1&quot;&gt;Yan Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1&quot;&gt;Federico Tombari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1&quot;&gt;Benjamin Busam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17939">
<title>Fourier Analysis on Robustness of Graph Convolutional Neural Networks for Skeleton-based Action Recognition. (arXiv:2305.17939v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17939</link>
<description rdf:parseType="Literal">&lt;p&gt;Using Fourier analysis, we explore the robustness and vulnerability of graph
convolutional neural networks (GCNs) for skeleton-based action recognition. We
adopt a joint Fourier transform (JFT), a combination of the graph Fourier
transform (GFT) and the discrete Fourier transform (DFT), to examine the
robustness of adversarially-trained GCNs against adversarial attacks and common
corruptions. Experimental results with the NTU RGB+D dataset reveal that
adversarial training does not introduce a robustness trade-off between
adversarial attacks and low-frequency perturbations, which typically occurs
during image classification based on convolutional neural networks. This
finding indicates that adversarial training is a practical approach to
enhancing robustness against adversarial attacks and common corruptions in
skeleton-based action recognition. Furthermore, we find that the Fourier
approach cannot explain vulnerability against skeletal part occlusion
corruption, which highlights its limitations. These findings extend our
understanding of the robustness of GCNs, potentially guiding the development of
more robust learning methods for skeleton-based action recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanaka_N/0/1/0/all/0/1&quot;&gt;Nariki Tanaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kera_H/0/1/0/all/0/1&quot;&gt;Hiroshi Kera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawamoto_K/0/1/0/all/0/1&quot;&gt;Kazuhiko Kawamoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18601">
<title>BRICS: Bi-level feature Representation of Image CollectionS. (arXiv:2305.18601v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18601</link>
<description rdf:parseType="Literal">&lt;p&gt;We present BRICS, a bi-level feature representation for image collections,
which consists of a key code space on top of a feature grid space.
Specifically, our representation is learned by an autoencoder to encode images
into continuous key codes, which are used to retrieve features from groups of
multi-resolution feature grids. Our key codes and feature grids are jointly
trained continuously with well-defined gradient flows, leading to high usage
rates of the feature grids and improved generative modeling compared to
discrete Vector Quantization (VQ). Differently from existing continuous
representations such as KL-regularized latent codes, our key codes are strictly
bounded in scale and variance. Overall, feature encoding by BRICS is compact,
efficient to train, and enables generative modeling over key codes using the
diffusion model. Experimental results show that our method achieves comparable
reconstruction results to VQ while having a smaller and more efficient decoder
network (50% fewer GFlops). By applying the diffusion model over our key code
space, we achieve state-of-the-art performance on image synthesis on the FFHQ
and LSUN-Church (29% lower than LDM, 32% lower than StyleGAN2, 44% lower than
Projected GAN on CLIP-FID) datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dingdong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1&quot;&gt;Ali Mahdavi-Amiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00354">
<title>Addressing Negative Transfer in Diffusion Models. (arXiv:2306.00354v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00354</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion-based generative models have achieved remarkable success in various
domains. It trains a shared model on denoising tasks that encompass different
noise levels simultaneously, representing a form of multi-task learning (MTL).
However, analyzing and improving diffusion models from an MTL perspective
remains under-explored. In particular, MTL can sometimes lead to the well-known
phenomenon of negative transfer, which results in the performance degradation
of certain tasks due to conflicts between tasks. In this paper, we first aim to
analyze diffusion training from an MTL standpoint, presenting two key
observations: (O1) the task affinity between denoising tasks diminishes as the
gap between noise levels widens, and (O2) negative transfer can arise even in
diffusion training. Building upon these observations, we aim to enhance
diffusion training by mitigating negative transfer. To achieve this, we propose
leveraging existing MTL methods, but the presence of a huge number of denoising
tasks makes this computationally expensive to calculate the necessary per-task
loss or gradient. To address this challenge, we propose clustering the
denoising tasks into small task clusters and applying MTL methods to them.
Specifically, based on (O2), we employ interval clustering to enforce temporal
proximity among denoising tasks within clusters. We show that interval
clustering can be solved using dynamic programming, utilizing signal-to-noise
ratio, timestep, and task affinity for clustering objectives. Through this, our
approach addresses the issue of negative transfer in diffusion models by
allowing for efficient computation of MTL methods. We validate the efficacy of
proposed clustering and its integration with MTL methods through various
experiments, demonstrating 1) improved generation quality and 2) faster
training convergence of diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Go_H/0/1/0/all/0/1&quot;&gt;Hyojun Go&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;JinYoung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yunsung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seunghyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Shinhyeok Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_H/0/1/0/all/0/1&quot;&gt;Hyeongdon Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Seungtaek Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07520">
<title>Instruct-ReID: A Multi-purpose Person Re-identification Task with Instructions. (arXiv:2306.07520v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07520</link>
<description rdf:parseType="Literal">&lt;p&gt;Human intelligence can retrieve any person according to both visual and
language descriptions. However, the current computer vision community studies
specific person re-identification (ReID) tasks in different scenarios
separately, which limits the applications in the real world. This paper strives
to resolve this problem by proposing a new instruct-ReID task that requires the
model to retrieve images according to the given image or language instructions.
Our instruct-ReID is a more general ReID setting, where existing 6 ReID tasks
can be viewed as special cases by designing different instructions. We propose
a large-scale OmniReID benchmark and an adaptive triplet loss as a baseline
method to facilitate research in this new setting. Experimental results show
that the proposed multi-purpose ReID model, trained on our OmniReID benchmark
without fine-tuning, can improve +0.5%, +0.6%, +7.7% mAP on Market1501, MSMT17,
CUHK03 for traditional ReID, +6.4%, +7.1%, +11.2% mAP on PRCC, VC-Clothes, LTCC
for clothes-changing ReID, +11.7% mAP on COCAS+ real2 for clothes template
based clothes-changing ReID when using only RGB images, +24.9% mAP on COCAS+
real2 for our newly defined language-instructed ReID, +4.3% on LLCM for
visible-infrared ReID, +2.6% on CUHK-PEDES for text-to-image ReID. The
datasets, the model, and code will be available at
https://github.com/hwz-zju/Instruct-ReID.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Weizhen He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yiheng Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shixiang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qihao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Qingsong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhou Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Lei Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Feng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_D/0/1/0/all/0/1&quot;&gt;Donglian Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yunfeng Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16846">
<title>Lightweight texture transfer based on texture feature preset. (arXiv:2306.16846v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16846</link>
<description rdf:parseType="Literal">&lt;p&gt;In the task of texture transfer, reference texture images typically exhibit
highly repetitive texture features, and the texture transfer results from
different content images under the same style also share remarkably similar
texture patterns. Encoding such highly similar texture features often requires
deep layers and a large number of channels, making it is also the main source
of the entire model&apos;s parameter count and computational load, and inference
time. We propose a lightweight texture transfer based on texture feature preset
(TFP). TFP takes full advantage of the high repetitiveness of texture features
by providing preset universal texture feature maps for a given style. These
preset feature maps can be fused and decoded directly with shallow color
transfer feature maps of any content to generate texture transfer results,
thereby avoiding redundant texture information from being encoded repeatedly.
The texture feature map we preset is encoded through noise input images with
consistent distribution (standard normal distribution). This consistent input
distribution can completely avoid the problem of texture transfer
differentiation, and by randomly sampling different noise inputs, we can obtain
different texture features and texture transfer results under the same
reference style. Compared to state-of-the-art techniques, our TFP not only
produces visually superior results but also reduces the model size by 3.2-3538
times and speeds up the process by 1.8-5.6 times.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;ShiQi Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03407">
<title>Spatially Varying Nanophotonic Neural Networks. (arXiv:2308.03407v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03407</link>
<description rdf:parseType="Literal">&lt;p&gt;The explosive growth of computation and energy cost of artificial
intelligence has spurred strong interests in new computing modalities as
potential alternatives to conventional electronic processors. Photonic
processors that execute operations using photons instead of electrons, have
promised to enable optical neural networks with ultra-low latency and power
consumption. However, existing optical neural networks, limited by the
underlying network designs, have achieved image recognition accuracy far below
that of state-of-the-art electronic neural networks. In this work, we close
this gap by embedding massively parallelized optical computation into flat
camera optics that perform neural network computation during the capture,
before recording an image on the sensor. Specifically, we harness large kernels
and propose a large-kernel spatially-varying convolutional neural network
learned via low-dimensional reparameterization techniques. We experimentally
instantiate the network with a flat meta-optical system that encompasses an
array of nanophotonic structures designed to induce angle-dependent responses.
Combined with an extremely lightweight electronic backend with approximately 2K
parameters we demonstrate a reconfigurable nanophotonic neural network reaches
72.76\% blind test classification accuracy on CIFAR-10 dataset, and, as such,
the first time, an optical neural network outperforms the first modern digital
neural network -- AlexNet (72.64\%) with 57M parameters, bringing optical
neural network into modern deep learning era.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1&quot;&gt;Kaixuan Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Froech_J/0/1/0/all/0/1&quot;&gt;Johannes Froech&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakravarthula_P/0/1/0/all/0/1&quot;&gt;Praneeth Chakravarthula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whitehead_J/0/1/0/all/0/1&quot;&gt;James Whitehead&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tseng_E/0/1/0/all/0/1&quot;&gt;Ethan Tseng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1&quot;&gt;Arka Majumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1&quot;&gt;Felix Heide&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06412">
<title>Taming Self-Training for Open-Vocabulary Object Detection. (arXiv:2308.06412v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06412</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies have shown promising performance in open-vocabulary object
detection (OVD) by utilizing pseudo labels (PLs) from pretrained vision and
language models (VLMs). However, teacher-student self-training, a powerful and
widely used paradigm to leverage PLs, is rarely explored for OVD. This work
identifies two challenges of using self-training in OVD: noisy PLs from VLMs
and frequent distribution changes of PLs. To address these challenges, we
propose SAS-Det that tames self-training for OVD from two key perspectives.
First, we present a split-and-fusion (SAF) head that splits a standard
detection into an open-branch and a closed-branch. This design can reduce noisy
supervision from pseudo boxes. Moreover, the two branches learn complementary
knowledge from different training data, significantly enhancing performance
when fused together. Second, in our view, unlike in closed-set tasks, the PL
distributions in OVD are solely determined by the teacher model. We introduce a
periodic update strategy to decrease the number of updates to the teacher,
thereby decreasing the frequency of changes in PL distributions, which
stabilizes the training process. Extensive experiments demonstrate SAS-Det is
both efficient and effective. SAS-Det outperforms recent models of the same
scale by a clear margin and achieves 37.4 AP50 and 29.1 APr on novel categories
of the COCO and LVIS benchmarks, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shiyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulter_S/0/1/0/all/0/1&quot;&gt;Samuel Schulter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Long Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhixing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+G_V/0/1/0/all/0/1&quot;&gt;Vijay Kumar B.G&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suh_Y/0/1/0/all/0/1&quot;&gt;Yumin Suh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1&quot;&gt;Manmohan Chandraker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1&quot;&gt;Dimitris N. Metaxas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07017">
<title>Contrastive Bi-Projector for Unsupervised Domain Adaption. (arXiv:2308.07017v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07017</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel unsupervised domain adaption (UDA) method based
on contrastive bi-projector (CBP), which can improve the existing UDA methods.
It is called CBPUDA here, which effectively promotes the feature extractors
(FEs) to reduce the generation of ambiguous features for classification and
domain adaption. The CBP differs from traditional bi-classifier-based methods
at that these two classifiers are replaced with two projectors of performing a
mapping from the input feature to two distinct features. These two projectors
and the FEs in the CBPUDA can be trained adversarially to obtain more refined
decision boundaries so that it can possess powerful classification performance.
Two properties of the proposed loss function are analyzed here. The first
property is to derive an upper bound of joint prediction entropy, which is used
to form the proposed loss function, contrastive discrepancy (CD) loss. The CD
loss takes the advantages of the contrastive learning and the bi-classifier.
The second property is to analyze the gradient of the CD loss and then overcome
the drawback of the CD loss. The result of the second property is utilized in
the development of the gradient scaling (GS) scheme in this paper. The GS
scheme can be exploited to tackle the unstable problem of the CD loss because
training the CBPUDA requires using contrastive learning and adversarial
learning at the same time. Therefore, using the CD loss with the GS scheme
overcomes the problem mentioned above to make features more compact for
intra-class and distinguishable for inter-class. Experimental results express
that the CBPUDA is superior to conventional UDA methods under consideration in
this paper for UDA and fine-grained UDA tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lin-Chieh Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsai_H/0/1/0/all/0/1&quot;&gt;Hung-Hsu Tsai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07931">
<title>Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation. (arXiv:2308.07931v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07931</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised and language-supervised image models contain rich knowledge
of the world that is important for generalization. Many robotic tasks, however,
require a detailed understanding of 3D geometry, which is often lacking in 2D
image features. This work bridges this 2D-to-3D gap for robotic manipulation by
leveraging distilled feature fields to combine accurate 3D geometry with rich
semantics from 2D foundation models. We present a few-shot learning method for
6-DOF grasping and placing that harnesses these strong spatial and semantic
priors to achieve in-the-wild generalization to unseen objects. Using features
distilled from a vision-language model, CLIP, we present a way to designate
novel objects for manipulation via free-text natural language, and demonstrate
its ability to generalize to unseen expressions and novel categories of
objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;William Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Ge Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1&quot;&gt;Alan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1&quot;&gt;Jansen Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1&quot;&gt;Leslie Pack Kaelbling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1&quot;&gt;Phillip Isola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08316">
<title>Dual-Stream Diffusion Net for Text-to-Video Generation. (arXiv:2308.08316v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08316</link>
<description rdf:parseType="Literal">&lt;p&gt;With the emerging diffusion models, recently, text-to-video generation has
aroused increasing attention. But an important bottleneck therein is that
generative videos often tend to carry some flickers and artifacts. In this
work, we propose a dual-stream diffusion net (DSDN) to improve the consistency
of content variations in generating videos. In particular, the designed two
diffusion streams, video content and motion branches, could not only run
separately in their private spaces for producing personalized video variations
as well as content, but also be well-aligned between the content and motion
domains through leveraging our designed cross-transformer interaction module,
which would benefit the smoothness of generated videos. Besides, we also
introduce motion decomposer and combiner to faciliate the operation on video
motion. Qualitative and quantitative experiments demonstrate that our method
could produce amazing continuous videos with fewer flickers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Binhui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Anbo Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1&quot;&gt;Zhen Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09091">
<title>Edit Temporal-Consistent Videos with Image Diffusion Model. (arXiv:2308.09091v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09091</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale text-to-image (T2I) diffusion models have been extended for
text-guided video editing, yielding impressive zero-shot video editing
performance. Nonetheless, the generated videos usually show spatial
irregularities and temporal inconsistencies as the temporal characteristics of
videos have not been faithfully modeled. In this paper, we propose an elegant
yet effective Temporal-Consistent Video Editing (TCVE) method to mitigate the
temporal inconsistency challenge for robust text-guided video editing. In
addition to the utilization of a pretrained T2I 2D Unet for spatial content
manipulation, we establish a dedicated temporal Unet architecture to faithfully
capture the temporal coherence of the input video sequences. Furthermore, to
establish coherence and interrelation between the spatial-focused and
temporal-focused components, a cohesive spatial-temporal modeling unit is
formulated. This unit effectively interconnects the temporal Unet with the
pretrained 2D Unet, thereby enhancing the temporal consistency of the generated
videos while preserving the capacity for video content manipulation.
Quantitative experimental results and visualization results demonstrate that
TCVE achieves state-of-the-art performance in both video temporal consistency
and video editing capability, surpassing existing benchmarks in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuanzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Anbo Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1&quot;&gt;Antoni B. Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1&quot;&gt;Zhen Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10273">
<title>Turning Waste into Wealth: Leveraging Low-Quality Samples for Enhancing Continuous Conditional Generative Adversarial Networks. (arXiv:2308.10273v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10273</link>
<description rdf:parseType="Literal">&lt;p&gt;Continuous Conditional Generative Adversarial Networks (CcGANs) enable
generative modeling conditional on continuous scalar variables (termed
regression labels). However, they can produce subpar fake images due to limited
training data. Although Negative Data Augmentation (NDA) effectively enhances
unconditional and class-conditional GANs by introducing anomalies into real
training images, guiding the GANs away from low-quality outputs, its impact on
CcGANs is limited, as it fails to replicate negative samples that may occur
during the CcGAN sampling. We present a novel NDA approach called Dual-NDA
specifically tailored for CcGANs to address this problem. Dual-NDA employs two
types of negative samples: visually unrealistic images generated from a
pre-trained CcGAN and label-inconsistent images created by manipulating real
images&apos; labels. Leveraging these negative samples, we introduce a novel
discriminator objective alongside a modified CcGAN training algorithm.
Empirical analysis on UTKFace and Steering Angle reveals that Dual-NDA
consistently enhances the visual fidelity and label consistency of fake images
generated by CcGANs, exhibiting a substantial performance gain over the vanilla
NDA. Moreover, by applying Dual-NDA, CcGANs demonstrate a remarkable
advancement beyond the capabilities of state-of-the-art conditional GANs and
diffusion models, establishing a new pinnacle of performance. Our codes can be
found at https://github.com/UBCDingXin/Dual-NDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xin Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yongwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zuheng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11766">
<title>Dictionary Attack on IMU-based Gait Authentication. (arXiv:2309.11766v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2309.11766</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel adversarial model for authentication systems that use gait
patterns recorded by the inertial measurement unit (IMU) built into
smartphones. The attack idea is inspired by and named after the concept of a
dictionary attack on knowledge (PIN or password) based authentication systems.
In particular, this work investigates whether it is possible to build a
dictionary of IMUGait patterns and use it to launch an attack or find an
imitator who can actively reproduce IMUGait patterns that match the target&apos;s
IMUGait pattern. Nine physically and demographically diverse individuals walked
at various levels of four predefined controllable and adaptable gait factors
(speed, step length, step width, and thigh-lift), producing 178 unique IMUGait
patterns. Each pattern attacked a wide variety of user authentication models.
The deeper analysis of error rates (before and after the attack) challenges the
belief that authentication systems based on IMUGait patterns are the most
difficult to spoof; further research is needed on adversarial models and
associated countermeasures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1&quot;&gt;Rajesh Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isik_C/0/1/0/all/0/1&quot;&gt;Can Isik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohan_C/0/1/0/all/0/1&quot;&gt;Chilukuri K. Mohan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14181">
<title>Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision. (arXiv:2309.14181v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14181</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid evolution of Multi-modality Large Language Models (MLLMs) has
catalyzed a shift in computer vision from specialized models to general-purpose
foundation models. Nevertheless, there is still an inadequacy in assessing the
abilities of MLLMs on low-level visual perception and understanding. To address
this gap, we present Q-Bench, a holistic benchmark crafted to systematically
evaluate potential abilities of MLLMs on three realms: low-level visual
perception, low-level visual description, and overall visual quality
assessment. a) To evaluate the low-level perception ability, we construct the
LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped
with a human-asked question focusing on its low-level attributes. We then
measure the correctness of MLLMs on answering these questions. b) To examine
the description ability of MLLMs on low-level information, we propose the
LLDescribe dataset consisting of long expert-labelled golden low-level text
descriptions on 499 images, and a GPT-involved comparison pipeline between
outputs of MLLMs and the golden descriptions. c) Besides these two tasks, we
further measure their visual quality assessment ability to align with human
opinion scores. Specifically, we design a softmax-based strategy that enables
MLLMs to predict quantifiable quality scores, and evaluate them on various
existing image quality assessment (IQA) datasets. Our evaluation across the
three abilities confirms that MLLMs possess preliminary low-level visual
skills. However, these skills are still unstable and relatively imprecise,
indicating the need for specific enhancements on MLLMs towards these abilities.
We hope that our benchmark can encourage the research community to delve deeper
to discover and enhance these untapped potentials of MLLMs. Project Page:
https://q-future.github.io/Q-Bench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haoning Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zicheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Erli Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaofeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1&quot;&gt;Liang Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Annan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wenxiu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1&quot;&gt;Qiong Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1&quot;&gt;Guangtao Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weisi Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15031">
<title>Nuclear Morphometry using a Deep Learning-based Algorithm has Prognostic Relevance for Canine Cutaneous Mast Cell Tumors. (arXiv:2309.15031v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15031</link>
<description rdf:parseType="Literal">&lt;p&gt;Variation in nuclear size and shape is an important criterion of malignancy
for many tumor types; however, categorical estimates by pathologists have poor
reproducibility. Measurements of nuclear characteristics (morphometry) can
improve reproducibility, but manual methods are time consuming. In this study,
we evaluated fully automated morphometry using a deep learning-based algorithm
in 96 canine cutaneous mast cell tumors with information on patient survival.
Algorithmic morphometry was compared with karyomegaly estimates by 11
pathologists, manual nuclear morphometry of 12 cells by 9 pathologists, and the
mitotic count as a benchmark. The prognostic value of automated morphometry was
high with an area under the ROC curve regarding the tumor-specific survival of
0.943 (95% CI: 0.889 - 0.996) for the standard deviation (SD) of nuclear area,
which was higher than manual morphometry of all pathologists combined (0.868,
95% CI: 0.737 - 0.991) and the mitotic count (0.885, 95% CI: 0.765 - 1.00). At
the proposed thresholds, the hazard ratio for algorithmic morphometry (SD of
nuclear area $\geq 9.0 \mu m^2$) was 18.3 (95% CI: 5.0 - 67.1), for manual
morphometry (SD of nuclear area $\geq 10.9 \mu m^2$) 9.0 (95% CI: 6.0 - 13.4),
for karyomegaly estimates 7.6 (95% CI: 5.7 - 10.1), and for the mitotic count
30.5 (95% CI: 7.8 - 118.0). Inter-rater reproducibility for karyomegaly
estimates was fair ($\kappa$ = 0.226) with highly variable
sensitivity/specificity values for the individual pathologists. Reproducibility
for manual morphometry (SD of nuclear area) was good (ICC = 0.654). This study
supports the use of algorithmic morphometry as a prognostic test to overcome
the limitations of estimates and manual measurements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haghofer_A/0/1/0/all/0/1&quot;&gt;Andreas Haghofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parlak_E/0/1/0/all/0/1&quot;&gt;Eda Parlak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartel_A/0/1/0/all/0/1&quot;&gt;Alexander Bartel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donovan_T/0/1/0/all/0/1&quot;&gt;Taryn A. Donovan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assenmacher_C/0/1/0/all/0/1&quot;&gt;Charles-Antoine Assenmacher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bolfa_P/0/1/0/all/0/1&quot;&gt;Pompei Bolfa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dark_M/0/1/0/all/0/1&quot;&gt;Michael J. Dark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuchs_Baumgartinger_A/0/1/0/all/0/1&quot;&gt;Andrea Fuchs-Baumgartinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klang_A/0/1/0/all/0/1&quot;&gt;Andrea Klang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jager_K/0/1/0/all/0/1&quot;&gt;Kathrin J&amp;#xe4;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klopfleisch_R/0/1/0/all/0/1&quot;&gt;Robert Klopfleisch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merz_S/0/1/0/all/0/1&quot;&gt;Sophie Merz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richter_B/0/1/0/all/0/1&quot;&gt;Barbara Richter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulman_F/0/1/0/all/0/1&quot;&gt;F. Yvonne Schulman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganz_J/0/1/0/all/0/1&quot;&gt;Jonathan Ganz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scharinger_J/0/1/0/all/0/1&quot;&gt;Josef Scharinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aubreville_M/0/1/0/all/0/1&quot;&gt;Marc Aubreville&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winkler_S/0/1/0/all/0/1&quot;&gt;Stephan M. Winkler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiupel_M/0/1/0/all/0/1&quot;&gt;Matti Kiupel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertram_C/0/1/0/all/0/1&quot;&gt;Christof A. Bertram&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03940">
<title>Hard View Selection for Self-Supervised Learning. (arXiv:2310.03940v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03940</link>
<description rdf:parseType="Literal">&lt;p&gt;Many Self-Supervised Learning (SSL) methods train their models to be
invariant to different &quot;views&quot; of an image input for which a good data
augmentation pipeline is crucial. While considerable efforts were directed
towards improving pre-text tasks, architectures, or robustness (e.g., Siamese
networks or teacher-softmax centering), the majority of these methods remain
strongly reliant on the random sampling of operations within the image
augmentation pipeline, such as the random resized crop or color distortion
operation. In this paper, we argue that the role of the view generation and its
effect on performance has so far received insufficient attention. To address
this, we propose an easy, learning-free, yet powerful Hard View Selection (HVS)
strategy designed to extend the random view generation to expose the pretrained
model to harder samples during SSL training. It encompasses the following
iterative steps: 1) randomly sample multiple views and create pairs of two
views, 2) run forward passes for each view pair on the currently trained model,
3) adversarially select the pair yielding the worst loss, and 4) run the
backward pass with the selected pair. In our empirical analysis we show that
under the hood, HVS increases task difficulty by controlling the Intersection
over Union of views during pretraining. With only 300-epoch pretraining, HVS is
able to closely rival the 800-epoch DINO baseline which remains very favorable
even when factoring in the slowdown induced by the additional forwards of HVS.
Additionally, HVS consistently achieves accuracy improvements on ImageNet
between 0.4% and 1.9% on linear evaluation and similar improvements on transfer
tasks across multiple SSL methods, such as DINO, SimSiam, iBOT, and SimCLR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1&quot;&gt;Fabio Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rapant_I/0/1/0/all/0/1&quot;&gt;Ivo Rapant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06594">
<title>On the Evaluation and Refinement of Vision-Language Instruction Tuning Datasets. (arXiv:2310.06594v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06594</link>
<description rdf:parseType="Literal">&lt;p&gt;There is an emerging line of research on multimodal instruction tuning, and a
line of benchmarks has been proposed for evaluating these models recently.
Instead of evaluating the models directly, in this paper, we try to evaluate
the Vision-Language Instruction-Tuning (VLIT) datasets. Also, we seek the way
of building a dataset for developing an all-powerful VLIT model, which we
believe could also be of utility for establishing a grounded protocol for
benchmarking VLIT models. For effective evaluation of VLIT datasets that
remains an open question, we propose a tune-cross-evaluation paradigm: tuning
on one dataset and evaluating on the others in turn. For each single
tune-evaluation experiment set, we define the Meta Quality (MQ) as the mean
score obtained by a set of caption metrics including BLEU, METEOR, and ROUGE-L
to quantify the quality of a certain dataset or a sample. On this basis, to
evaluate the comprehensiveness of a dataset, we develop the Dataset Quality
(DQ) covering all tune-evaluation sets. To lay the foundation for building a
comprehensive dataset and developing an all-powerful model for practical
applications, we define the Sample Quality (SQ) to quantify the all-sided
quality of each sample. Extensive experiments validate the rationality of the
proposed evaluation paradigm. Based on the holistic evaluation, we build a new
dataset, REVO-LION (REfining VisiOn-Language InstructiOn tuNing), by collecting
samples with higher SQ from each dataset. Remarkably, even with only half of
the complete data, the model trained on REVO-LION can achieve the performance
comparable to simply adding all VLIT datasets up. Furthermore, REVO-LION not
only facilitates the development of a powerful model but also incorporates an
evaluation set, which is designed to serve as a convenient benchmark for future
research in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_N/0/1/0/all/0/1&quot;&gt;Ning Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaofeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_R/0/1/0/all/0/1&quot;&gt;Renqiu Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1&quot;&gt;Min Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10835">
<title>Provable Probabilistic Imaging using Score-Based Generative Priors. (arXiv:2310.10835v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10835</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating high-quality images while also quantifying their uncertainty are
two desired features in an image reconstruction algorithm for solving ill-posed
inverse problems. In this paper, we propose plug-and-play Monte Carlo (PMC) as
a principled framework for characterizing the space of possible solutions to a
general inverse problem. PMC is able to incorporate expressive score-based
generative priors for high-quality image reconstruction while also performing
uncertainty quantification via posterior sampling. In particular, we introduce
two PMC algorithms which can be viewed as the sampling analogues of the
traditional plug-and-play priors (PnP) and regularization by denoising (RED)
algorithms. We also establish a theoretical analysis for characterizing the
convergence of the PMC algorithms. Our analysis provides non-asymptotic
stationarity guarantees for both algorithms, even in the presence of
non-log-concave likelihoods and imperfect score networks. We demonstrate the
performance of the PMC algorithms on multiple representative inverse problems
with both linear and nonlinear forward models. Experimental results show that
PMC significantly improves reconstruction quality and enables high-fidelity
uncertainty quantification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zihui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_B/0/1/0/all/0/1&quot;&gt;Berthy T. Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bouman_K/0/1/0/all/0/1&quot;&gt;Katherine L. Bouman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16542">
<title>ParisLuco3D: A high-quality target dataset for domain generalization of LiDAR perception. (arXiv:2310.16542v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16542</link>
<description rdf:parseType="Literal">&lt;p&gt;LiDAR is an essential sensor for autonomous driving by collecting precise
geometric information regarding a scene. As the performance of various LiDAR
perception tasks has improved, generalizations to new environments and sensors
has emerged to test these optimized models in real-world conditions.
Unfortunately, the various annotation strategies of data providers complicate
the computation of cross-domain performances.
&lt;/p&gt;
&lt;p&gt;This paper provides a novel dataset, ParisLuco3D, specifically designed for
cross-domain evaluation to make it easier to evaluate the performance utilizing
various source datasets. Alongside the dataset, online benchmarks for LiDAR
semantic segmentation, LiDAR object detection, and LiDAR tracking are provided
to ensure a fair comparison across methods.
&lt;/p&gt;
&lt;p&gt;The ParisLuco3D dataset, evaluation scripts, and links to benchmarks can be
found at the following website: https://npm3d.fr/parisluco3d
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_J/0/1/0/all/0/1&quot;&gt;Jules Sanchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soum_Fontez_L/0/1/0/all/0/1&quot;&gt;Louis Soum-Fontez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deschaud_J/0/1/0/all/0/1&quot;&gt;Jean-Emmanuel Deschaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goulette_F/0/1/0/all/0/1&quot;&gt;Francois Goulette&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01004">
<title>Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning for Medical Image Captioning. (arXiv:2311.01004v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01004</link>
<description rdf:parseType="Literal">&lt;p&gt;With the development of multimodality and large language models, the deep
learning-based technique for medical image captioning holds the potential to
offer valuable diagnostic recommendations. However, current generic text and
image pre-trained models do not yield satisfactory results when it comes to
describing intricate details within medical images. In this paper, we present a
novel medical image captioning method guided by the segment anything model
(SAM) to enable enhanced encoding with both general and detailed feature
extraction. In addition, our approach employs a distinctive pre-training
strategy with mixed semantic learning to simultaneously capture both the
overall information and finer details within medical images. We demonstrate the
effectiveness of this approach, as it outperforms the pre-trained BLIP2 model
on various evaluation metrics for generating descriptions of medical images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Benlu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Weijie Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yizhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xuechen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guanhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shiyan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Gaoang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03198">
<title>LCPR: A Multi-Scale Attention-Based LiDAR-Camera Fusion Network for Place Recognition. (arXiv:2311.03198v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03198</link>
<description rdf:parseType="Literal">&lt;p&gt;Place recognition is one of the most crucial modules for autonomous vehicles
to identify places that were previously visited in GPS-invalid environments.
Sensor fusion is considered an effective method to overcome the weaknesses of
individual sensors. In recent years, multimodal place recognition fusing
information from multiple sensors has gathered increasing attention. However,
most existing multimodal place recognition methods only use limited
field-of-view camera images, which leads to an imbalance between features from
different modalities and limits the effectiveness of sensor fusion. In this
paper, we present a novel neural network named LCPR for robust multimodal place
recognition, which fuses LiDAR point clouds with multi-view RGB images to
generate discriminative and yaw-rotation invariant representations of the
environment. A multi-scale attention-based fusion module is proposed to fully
exploit the panoramic views from different modalities of the environment and
their correlations. We evaluate our method on the nuScenes dataset, and the
experimental results show that our method can effectively utilize multi-view
camera and LiDAR data to improve the place recognition performance while
maintaining strong robustness to viewpoint changes. Our open-source code and
pre-trained models are available at https://github.com/ZhouZijie77/LCPR .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zijie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jingyi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_G/0/1/0/all/0/1&quot;&gt;Guangming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Junyi Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03356">
<title>GLaMM: Pixel Grounding Large Multimodal Model. (arXiv:2311.03356v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03356</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Multimodal Models (LMMs) extend Large Language Models to the vision
domain. Initial LMMs used holistic images and text prompts to generate
ungrounded textual responses. Recently, region-level LMMs have been used to
generate visually grounded responses. However, they are limited to only
referring to a single object category at a time, require users to specify the
regions, or cannot offer dense pixel-wise object grounding. In this work, we
present Grounding LMM (GLaMM), the first model that can generate natural
language responses seamlessly intertwined with corresponding object
segmentation masks. GLaMM not only grounds objects appearing in the
conversations but is flexible enough to accept both textual and optional visual
prompts (region of interest) as input. This empowers users to interact with the
model at various levels of granularity, both in textual and visual domains. Due
to the lack of standard benchmarks for the novel setting of visually Grounded
Conversation Generation (GCG), we introduce a comprehensive evaluation protocol
with our curated grounded conversations. Our proposed GCG task requires densely
grounded concepts in natural scenes at a large-scale. To this end, we propose a
densely annotated Grounding-anything Dataset (GranD) using our proposed
automated annotation pipeline that encompasses 7.5M unique concepts grounded in
a total of 810M regions available with segmentation masks. Besides GCG, GLaMM
also performs effectively on several downstream tasks, e.g., referring
expression segmentation, image and region-level captioning and vision-language
conversations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasheed_H/0/1/0/all/0/1&quot;&gt;Hanoona Rasheed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maaz_M/0/1/0/all/0/1&quot;&gt;Muhammad Maaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mullappilly_S/0/1/0/all/0/1&quot;&gt;Sahal Shaji Mullappilly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1&quot;&gt;Abdelrahman Shaker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cholakkal_H/0/1/0/all/0/1&quot;&gt;Hisham Cholakkal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1&quot;&gt;Rao M. Anwer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Erix Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming-Hsuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad S. Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03380">
<title>An attempt to generate new bridge types from latent space of variational autoencoder. (arXiv:2311.03380v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03380</link>
<description rdf:parseType="Literal">&lt;p&gt;Try to generate new bridge types using generative artificial intelligence
technology. The grayscale images of the bridge facade with the change of
component width was rendered by 3dsMax animation software, and then the OpenCV
module performed an appropriate amount of geometric transformation (rotation,
horizontal scale, vertical scale) to obtain the image dataset of three-span
beam bridge, arch bridge, cable-stayed bridge and suspension bridge. Based on
Python programming language, TensorFlow and Keras deep learning platform
framework, variational autoencoder was constructed and trained, and
low-dimensional bridge-type latent space that is convenient for vector
operations was obtained. Variational autoencoder can combine two bridge types
on the basis of the original of human into one that is a new bridge type.
Generative artificial intelligence technology can assist bridge designers in
bridge-type innovation, and can be used as copilot.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongjun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06542">
<title>Generation Of Colors using Bidirectional Long Short Term Memory Networks. (arXiv:2311.06542v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06542</link>
<description rdf:parseType="Literal">&lt;p&gt;Human vision can distinguish between a vast spectrum of colours, estimated to
be between 2 to 7 million discernible shades. However, this impressive range
does not inherently imply that all these colours have been precisely named and
described within our lexicon. We often associate colours with familiar objects
and concepts in our daily lives. This research endeavors to bridge the gap
between our visual perception of countless shades and our ability to articulate
and name them accurately. A novel model has been developed to achieve this
goal, leveraging Bidirectional Long Short-Term Memory (BiLSTM) networks with
Active learning. This model operates on a proprietary dataset meticulously
curated for this study. The primary objective of this research is to create a
versatile tool for categorizing and naming previously unnamed colours or
identifying intermediate shades that elude traditional colour terminology. The
findings underscore the potential of this innovative approach in
revolutionizing our understanding of colour perception and language. Through
rigorous experimentation and analysis, this study illuminates a promising
avenue for Natural Language Processing (NLP) applications in diverse
industries. By facilitating the exploration of the vast colour spectrum the
potential applications of NLP are extended beyond conventional boundaries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1&quot;&gt;A. Sinha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10116">
<title>Wildfire Smoke Detection with Cross Contrast Patch Embedding. (arXiv:2311.10116v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10116</link>
<description rdf:parseType="Literal">&lt;p&gt;The Transformer-based deep networks have increasingly shown significant
advantages over CNNs. Some existing work has applied it in the field of
wildfire recognition or detection. However, we observed that the vanilla
Transformer is not friendly for extracting smoke features. Because low-level
information such as color, transparency and texture is very important for smoke
recognition, and transformer pays more attention to the semantic relevance
between middle- or high-level features, and is not sensitive to the subtle
changes of low-level features along the space. To solve this problem, we
propose the Cross Contrast Patch Embedding(CCPE) module based on the Swin
Transformer, which uses the multi-scales spatial frequency contrast information
in both vertical and horizontal directions to improve the discrimination of the
network on the underlying details. The fuzzy boundary of smoke makes the
positive and negative label assignment for instances in a dilemma, which is
another challenge for wildfires detection. To solve this problem, a Separable
Negative Sampling Mechanism(SNSM) is proposed. By using two different negative
instance sampling strategies on positive images and negative images
respectively, the problem of supervision signal confusion caused by label
diversity in the process of network training is alleviated. This paper also
releases the RealFire Test, the largest real wildfire test set so far, to
evaluate the proposed method and promote future research. It contains 50,535
images from 3,649 video clips. The proposed method has been extensively tested
and evaluated on RealFire Test dataset, and has a significant performance
improvement compared with the baseline detection models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Cheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akram_A/0/1/0/all/0/1&quot;&gt;Adeel Akram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Z/0/1/0/all/0/1&quot;&gt;Zhilin Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qixing Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13091">
<title>Stable Unlearnable Example: Enhancing the Robustness of Unlearnable Examples via Stable Error-Minimizing Noise. (arXiv:2311.13091v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13091</link>
<description rdf:parseType="Literal">&lt;p&gt;The open source of large amounts of image data promotes the development of
deep learning techniques. Along with this comes the privacy risk of these
open-source image datasets being exploited by unauthorized third parties to
train deep learning models for commercial or illegal purposes. To avoid the
abuse of public data, a poisoning-based technique, the unlearnable example, is
proposed to significantly degrade the generalization performance of models by
adding a kind of imperceptible noise to the data. To further enhance its
robustness against adversarial training, existing works leverage iterative
adversarial training on both the defensive noise and the surrogate model.
However, it still remains unknown whether the robustness of unlearnable
examples primarily comes from the effect of enhancement in the surrogate model
or the defensive noise. Observing that simply removing the adversarial noise on
the training process of the defensive noise can improve the performance of
robust unlearnable examples, we identify that solely the surrogate model&apos;s
robustness contributes to the performance. Furthermore, we found a negative
correlation exists between the robustness of defensive noise and the protection
performance, indicating defensive noise&apos;s instability issue. Motivated by this,
to further boost the robust unlearnable example, we introduce stable
error-minimizing noise (SEM), which trains the defensive noise against random
perturbation instead of the time-consuming adversarial perturbation to improve
the stability of defensive noise. Through extensive experiments, we demonstrate
that SEM achieves a new state-of-the-art performance on CIFAR-10, CIFAR-100,
and ImageNet Subset in terms of both effectiveness and efficiency. The code is
available at https://github.com/liuyixin-louis/Stable-Unlearnable-Example.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yixin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kaidi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lichao Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16754">
<title>Towards Full-scene Domain Generalization in Multi-agent Collaborative Bird&apos;s Eye View Segmentation for Connected and Autonomous Driving. (arXiv:2311.16754v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16754</link>
<description rdf:parseType="Literal">&lt;p&gt;Collaborative perception has recently gained significant attention in
autonomous driving, improving perception quality by enabling the exchange of
additional information among vehicles. However, deploying collaborative
perception systems can lead to domain shifts due to diverse environmental
conditions and data heterogeneity among connected and autonomous vehicles
(CAVs). To address these challenges, we propose a unified domain generalization
framework applicable in both training and inference stages of collaborative
perception. In the training phase, we introduce an Amplitude Augmentation
(AmpAug) method to augment low-frequency image variations, broadening the
model&apos;s ability to learn across various domains. We also employ a
meta-consistency training scheme to simulate domain shifts, optimizing the
model with a carefully designed consistency loss to encourage domain-invariant
representations. In the inference phase, we introduce an intra-system domain
alignment mechanism to reduce or potentially eliminate the domain discrepancy
among CAVs prior to inference. Comprehensive experiments substantiate the
effectiveness of our method in comparison with the existing state-of-the-art
works. Code will be released at https://github.com/DG-CAVs/DG-CoPerception.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Senkang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zhengru Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xianhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yuguang Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1&quot;&gt;Sam Kwong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01324">
<title>MABViT -- Modified Attention Block Enhances Vision Transformers. (arXiv:2312.01324v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01324</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies have demonstrated the effectiveness of Gated Linear Units
(GLU) in enhancing transformer models, particularly in Large Language Models
(LLMs). Additionally, utilizing a parallel configuration within each
Transformer block rather than the conventional serialized method has been
revealed to accelerate the training of LLMs without significantly impacting
performance. However, when the MLP and attention block were run in parallel for
the image classification task, we observed a noticeable decline in performance.
We propose a novel transformer variant that integrates non-linearity within the
attention block to tackle this problem. We implemented the GLU-based activation
function on the Value tensor, and this new technique surpasses the current
state-of-the-art S/16 variant of Vision Transformers by 0.6% on the ImageNet-1K
dataset while utilizing fewer parameters. It also supersedes the B/16 variant
while using only half the parameters. Furthermore, we provide results with the
GELU activation function variant to confirm our assertions. Lastly, we showcase
that the MABViT variants exhibit greater potential when utilized in deep
transformers compared to the standard architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramesh_M/0/1/0/all/0/1&quot;&gt;Mahesh Ramesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramkumar_A/0/1/0/all/0/1&quot;&gt;Aswinkumar Ramkumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05634">
<title>PGS: Pose-Guided Supervision for Mitigating Clothes-Changing in Person Re-Identification. (arXiv:2312.05634v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05634</link>
<description rdf:parseType="Literal">&lt;p&gt;Person Re-Identification (Re-ID) task seeks to enhance the tracking of
multiple individuals by surveillance cameras. It provides additional support
for multimodal tasks, including text-based person retrieval and human matching.
Among the significant challenges faced in Re-ID, one of the most prominent is
dealing with clothes-changing, where the same person may appear in different
outfits. While previous methods have made notable progress in maintaining
clothing data consistency and handling clothing change data, they still tend to
rely excessively on clothing information, which can limit performance due to
the dynamic nature of human appearances. To mitigate this challenge, we propose
the Pose-Guided Supervision (PGS), an effective framework for learning pose
guidance within the Re-ID task. Our PGS consists of three modules: a human
encoder, a pose encoder, and a Pose-to-Human Projection module (PHP). The pose
encoder module utilizes a frozen pre-trained model while we fine-tune a
pre-trained human-centric model for the human encoder module. Our PHP transfers
pose knowledge from the pose encoder module to the human encoder module through
multiple projectors. Our framework, following extensive experimentation on five
benchmark datasets, consistently surpasses the performance of current
state-of-the-art methods. Our code is available at
https://github.com/huyquoctrinh/PGS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trinh_Q/0/1/0/all/0/1&quot;&gt;Quoc-Huy Trinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bui_N/0/1/0/all/0/1&quot;&gt;Nhat-Tan Bui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_D/0/1/0/all/0/1&quot;&gt;Dinh-Hieu Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thi_P/0/1/0/all/0/1&quot;&gt;Phuoc-Thao Vo Thi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hai-Dang Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1&quot;&gt;Debesh Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagci_U/0/1/0/all/0/1&quot;&gt;Ulas Bagci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngan Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1&quot;&gt;Minh-Triet Tran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07586">
<title>Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale. (arXiv:2312.07586v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07586</link>
<description rdf:parseType="Literal">&lt;p&gt;Popular guidance for denoising diffusion probabilistic model (DDPM) linearly
combines distinct conditional models together to provide enhanced control over
samples. However, this approach overlooks nonlinear effects that become
significant when guidance scale is large. To address this issue, we propose
characteristic guidance, a sampling method that provides first-principle
non-linear correction for classifier-free guided DDPMs. Such correction forces
the guided DDPMs to respect the Fokker-Planck equation of their underlying
diffusion process, in a way that is training-free, derivative-free, and
compatible with existing sampling methods. Experiments show that characteristic
guidance enhances control and reduces color and exposure issues in image
generation, proving effective in diverse applications ranging from latent space
sampling to solving physics problems like magnet phase transitions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Candi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yuan Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09168">
<title>DiffusionLight: Light Probes for Free by Painting a Chrome Ball. (arXiv:2312.09168v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09168</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a simple yet effective technique to estimate lighting in a single
input image. Current techniques rely heavily on HDR panorama datasets to train
neural networks to regress an input with limited field-of-view to a full
environment map. However, these approaches often struggle with real-world,
uncontrolled settings due to the limited diversity and size of their datasets.
To address this problem, we leverage diffusion models trained on billions of
standard images to render a chrome ball into the input image. Despite its
simplicity, this task remains challenging: the diffusion models often insert
incorrect or inconsistent objects and cannot readily generate images in HDR
format. Our research uncovers a surprising relationship between the appearance
of chrome balls and the initial diffusion noise map, which we utilize to
consistently generate high-quality chrome balls. We further fine-tune an LDR
difusion model (Stable Diffusion XL) with LoRA, enabling it to perform exposure
bracketing for HDR light estimation. Our method produces convincing light
estimates across diverse settings and demonstrates superior generalization to
in-the-wild scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phongthawee_P/0/1/0/all/0/1&quot;&gt;Pakkapon Phongthawee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chinchuthakun_W/0/1/0/all/0/1&quot;&gt;Worameth Chinchuthakun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinsunthithet_N/0/1/0/all/0/1&quot;&gt;Nontaphat Sinsunthithet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raj_A/0/1/0/all/0/1&quot;&gt;Amit Raj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1&quot;&gt;Varun Jampani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khungurn_P/0/1/0/all/0/1&quot;&gt;Pramook Khungurn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suwajanakorn_S/0/1/0/all/0/1&quot;&gt;Supasorn Suwajanakorn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10324">
<title>Federated Learning with Instance-Dependent Noisy Labels. (arXiv:2312.10324v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10324</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) with noisy labels poses a significant challenge.
Existing methods designed for handling noisy labels in centralized learning
tend to lose their effectiveness in the FL setting, mainly due to the small
dataset size and the heterogeneity of client data. While some attempts have
been made to tackle FL with noisy labels, they primarily focused on scenarios
involving class-conditional noise. In this paper, we study the more challenging
and practical issue of instance-dependent noise (IDN) in FL. We introduce a
novel algorithm called FedBeat (Federated Learning with Bayesian
Ensemble-Assisted Transition Matrix Estimation). FedBeat aims to build a global
statistically consistent classifier using the IDN transition matrix (IDNTM),
which encompasses three synergistic steps: (1) A federated data extraction step
that constructs a weak global model and extracts high-confidence data using a
Bayesian model ensemble method. (2) A federated transition matrix estimation
step in which clients collaboratively train an IDNTM estimation network based
on the extracted data. (3) A federated classifier correction step that enhances
the global model&apos;s performance by training it using a loss function tailored
for noisy labels, leveraging the IDNTM. Experiments conducted on CIFAR-10 and
SVHN verify that the proposed method significantly outperforms state-of-the-art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1&quot;&gt;Jieming Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jie Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11451">
<title>Language-Assisted 3D Scene Understanding. (arXiv:2312.11451v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11451</link>
<description rdf:parseType="Literal">&lt;p&gt;The scale and quality of point cloud datasets constrain the advancement of
point cloud learning. Recently, with the development of multi-modal learning,
the incorporation of domain-agnostic prior knowledge from other modalities,
such as images and text, to assist in point cloud feature learning has been
considered a promising avenue. Existing methods have demonstrated the
effectiveness of multi-modal contrastive training and feature distillation on
point clouds. However, challenges remain, including the requirement for paired
triplet data, redundancy and ambiguity in supervised features, and the
disruption of the original priors. In this paper, we propose a
language-assisted approach to point cloud feature learning (LAST-PCL),
enriching semantic concepts through LLMs-based text enrichment. We achieve
de-redundancy and feature dimensionality reduction without compromising textual
priors by statistical-based and training-free significant feature selection.
Furthermore, we also delve into an in-depth analysis of the impact of text
contrastive training on the point cloud. Extensive experiments validate that
the proposed method learns semantically meaningful point cloud features and
achieves state-of-the-art or comparable performance in 3D semantic
segmentation, 3D object detection, and 3D scene classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yanmin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1&quot;&gt;Qiankun Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Renrui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jian Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13108">
<title>ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation. (arXiv:2312.13108v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13108</link>
<description rdf:parseType="Literal">&lt;p&gt;Graphical User Interface (GUI) automation holds significant promise for
assisting users with complex tasks, thereby boosting human productivity.
Existing works leveraging Large Language Model (LLM) or LLM-based AI agents
have shown capabilities in automating tasks on Android and Web platforms.
However, these tasks are primarily aimed at simple device usage and
entertainment operations. This paper presents a novel benchmark, AssistGUI, to
evaluate whether models are capable of manipulating the mouse and keyboard on
the Windows platform in response to user-requested tasks. We carefully
collected a set of 100 tasks from nine widely-used software applications, such
as, After Effects and MS Word, each accompanied by the necessary project files
for better evaluation. Moreover, we propose an advanced Actor-Critic Embodied
Agent framework, which incorporates a sophisticated GUI parser driven by an
LLM-agent and an enhanced reasoning mechanism adept at handling lengthy
procedural tasks. Our experimental results reveal that our GUI Parser and
Reasoning mechanism outshine existing methods in performance. Nevertheless, the
potential remains substantial, with the best model attaining only a 46% success
rate on our benchmark. We conclude with a thorough analysis of the current
methods&apos; limitations, setting the stage for future breakthroughs in this
domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1&quot;&gt;Difei Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1&quot;&gt;Lei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1&quot;&gt;Zechen Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_M/0/1/0/all/0/1&quot;&gt;Mingyu Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peiran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_D/0/1/0/all/0/1&quot;&gt;Dongxing Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qinchen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weichen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peiyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xiangwu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hengxu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Luowei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16256">
<title>DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision. (arXiv:2312.16256v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16256</link>
<description rdf:parseType="Literal">&lt;p&gt;We have witnessed significant progress in deep learning-based 3D vision,
ranging from neural radiance field (NeRF) based 3D representation learning to
applications in novel view synthesis (NVS). However, existing scene-level
datasets for deep learning-based 3D vision, limited to either synthetic
environments or a narrow selection of real-world scenes, are quite
insufficient. This insufficiency not only hinders a comprehensive benchmark of
existing methods but also caps what could be explored in deep learning-based 3D
analysis. To address this critical gap, we present DL3DV-10K, a large-scale
scene dataset, featuring 51.2 million frames from 10,510 videos captured from
65 types of point-of-interest (POI) locations, covering both bounded and
unbounded scenes, with different levels of reflection, transparency, and
lighting. We conducted a comprehensive benchmark of recent NVS methods on
DL3DV-10K, which revealed valuable insights for future research in NVS. In
addition, we have obtained encouraging results in a pilot study to learn
generalizable NeRF from DL3DV-10K, which manifests the necessity of a
large-scale scene-level dataset to forge a path toward a foundation model for
learning 3D representation. Our DL3DV-10K dataset, benchmark results, and
models will be publicly accessible at https://dl3dv-10k.github.io/DL3DV-10K/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_L/0/1/0/all/0/1&quot;&gt;Lu Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1&quot;&gt;Yichen Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhi Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wentian Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_C/0/1/0/all/0/1&quot;&gt;Cheng Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_K/0/1/0/all/0/1&quot;&gt;Kun Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lantao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qianyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zixun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yawen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuanmao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xingpeng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashok_R/0/1/0/all/0/1&quot;&gt;Rohan Ashok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1&quot;&gt;Aniruddha Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1&quot;&gt;Hao Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1&quot;&gt;Xiangrui Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1&quot;&gt;Gang Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benes_B/0/1/0/all/0/1&quot;&gt;Bedrich Benes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1&quot;&gt;Aniket Bera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16471">
<title>A Survey on Super Resolution for video Enhancement Using GAN. (arXiv:2312.16471v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16471</link>
<description rdf:parseType="Literal">&lt;p&gt;This compilation of various research paper highlights provides a
comprehensive overview of recent developments in super-resolution image and
video using deep learning algorithms such as Generative Adversarial Networks.
The studies covered in these summaries provide fresh techniques to addressing
the issues of improving image and video quality, such as recursive learning for
video super-resolution, novel loss functions, frame-rate enhancement, and
attention model integration. These approaches are frequently evaluated using
criteria such as PSNR, SSIM, and perceptual indices. These advancements, which
aim to increase the visual clarity and quality of low-resolution video, have
tremendous potential in a variety of sectors ranging from surveillance
technology to medical imaging. In addition, this collection delves into the
wider field of Generative Adversarial Networks, exploring their principles,
training approaches, and applications across a broad range of domains, while
also emphasizing the challenges and opportunities for future research in this
rapidly advancing and changing field of artificial intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maity_A/0/1/0/all/0/1&quot;&gt;Ankush Maity&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pious_R/0/1/0/all/0/1&quot;&gt;Roshan Pious&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lenka_S/0/1/0/all/0/1&quot;&gt;Sourabh Kumar Lenka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Choudhary_V/0/1/0/all/0/1&quot;&gt;Vishal Choudhary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lokhande_P/0/1/0/all/0/1&quot;&gt;Prof. Sharayu Lokhande&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16477">
<title>Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding. (arXiv:2312.16477v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16477</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the results of view-based 3D shape recognition methods have
saturated, and models with excellent performance cannot be deployed on
memory-limited devices due to their huge size of parameters. To address this
problem, we introduce a compression method based on knowledge distillation for
this field, which largely reduces the number of parameters while preserving
model performance as much as possible. Specifically, to enhance the
capabilities of smaller models, we design a high-performing large model called
Group Multi-view Vision Transformer (GMViT). In GMViT, the view-level ViT first
establishes relationships between view-level features. Additionally, to capture
deeper features, we employ the grouping module to enhance view-level features
into group-level features. Finally, the group-level ViT aggregates group-level
features into complete, well-formed 3D shape descriptors. Notably, in both
ViTs, we introduce spatial encoding of camera coordinates as innovative
position embeddings. Furthermore, we propose two compressed versions based on
GMViT, namely GMViT-simple and GMViT-mini. To enhance the training
effectiveness of the small models, we introduce a knowledge distillation method
throughout the GMViT process, where the key outputs of each GMViT component
serve as distillation targets. Extensive experiments demonstrate the efficacy
of the proposed method. The large model GMViT achieves excellent 3D
classification and retrieval results on the benchmark datasets ModelNet,
ShapeNetCore55, and MCB. The smaller models, GMViT-simple and GMViT-mini,
reduce the parameter size by 8 and 17.6 times, respectively, and improve shape
recognition speed by 1.5 times on average, while preserving at least 90% of the
classification and retrieval performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lixiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1&quot;&gt;Qingzhe Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1&quot;&gt;Richang Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1&quot;&gt;Enhong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenglong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yuanyan Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16580">
<title>VLCounter: Text-aware Visual Representation for Zero-Shot Object Counting. (arXiv:2312.16580v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16580</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-Shot Object Counting (ZSOC) aims to count referred instances of
arbitrary classes in a query image without human-annotated exemplars. To deal
with ZSOC, preceding studies proposed a two-stage pipeline: discovering
exemplars and counting. However, there remains a challenge of vulnerability to
error propagation of the sequentially designed two-stage process. In this work,
an one-stage baseline, Visual-Language Baseline (VLBase), exploring the
implicit association of the semantic-patch embeddings of CLIP is proposed.
Subsequently, the extension of VLBase to Visual-language Counter (VLCounter) is
achieved by incorporating three modules devised to tailor VLBase for object
counting. First, Semantic-conditioned Prompt Tuning (SPT) is introduced within
the image encoder to acquire target-highlighted representations. Second,
Learnable Affine Transformation (LAT) is employed to translate the
semantic-patch similarity map to be appropriate for the counting task. Lastly,
the layer-wisely encoded features are transferred to the decoder through
Segment-aware Skip Connection (SaSC) to keep the generalization capability for
unseen classes. Through extensive experiments on FSC147, CARPK, and PUCPR+, the
benefits of the end-to-end framework, VLCounter, are demonstrated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1&quot;&gt;Seunggu Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_W/0/1/0/all/0/1&quot;&gt;WonJun Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1&quot;&gt;Euiyeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1&quot;&gt;Jae-Pil Heo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16886">
<title>MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices. (arXiv:2312.16886v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16886</link>
<description rdf:parseType="Literal">&lt;p&gt;We present MobileVLM, a competent multimodal vision language model (MMVLM)
targeted to run on mobile devices. It is an amalgamation of a myriad of
architectural designs and techniques that are mobile-oriented, which comprises
a set of language models at the scale of 1.4B and 2.7B parameters, trained from
scratch, a multimodal vision model that is pre-trained in the CLIP fashion,
cross-modality interaction via an efficient projector. We evaluate MobileVLM on
several typical VLM benchmarks. Our models demonstrate on par performance
compared with a few much larger models. More importantly, we measure the
inference speed on both a Qualcomm Snapdragon 888 CPU and an NVIDIA Jeston Orin
GPU, and we obtain state-of-the-art performance of 21.5 tokens and 65.3 tokens
per second, respectively. Our code will be made available at:
https://github.com/Meituan-AutoML/MobileVLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1&quot;&gt;Xiangxiang Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1&quot;&gt;Limeng Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xinyang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yiming Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1&quot;&gt;Fei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiaolin Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chunhua Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.17163">
<title>FENet: Focusing Enhanced Network for Lane Detection. (arXiv:2312.17163v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.17163</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by human driving focus, this research pioneers networks augmented
with Focusing Sampling, Partial Field of View Evaluation, Enhanced FPN
architecture and Directional IoU Loss - targeted innovations addressing
obstacles to precise lane detection for autonomous driving. Experiments
demonstrate our Focusing Sampling strategy, emphasizing vital distant details
unlike uniform approaches, significantly boosts both benchmark and practical
curved/distant lane recognition accuracy essential for safety. While FENetV1
achieves state-of-the-art conventional metric performance via enhancements
isolating perspective-aware contexts mimicking driver vision, FENetV2 proves
most reliable on the proposed Partial Field analysis. Hence we specifically
recommend V2 for practical lane navigation despite fractional degradation on
standard entire-image measures. Future directions include collecting on-road
data and integrating complementary dual frameworks to further breakthroughs
guided by human perception principles. Code will be made available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liman Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1&quot;&gt;Hanyang Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.17205">
<title>EFHQ: Multi-purpose ExtremePose-Face-HQ dataset. (arXiv:2312.17205v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.17205</link>
<description rdf:parseType="Literal">&lt;p&gt;The existing facial datasets, while having plentiful images at near frontal
views, lack images with extreme head poses, leading to the downgraded
performance of deep learning models when dealing with profile or pitched faces.
This work aims to address this gap by introducing a novel dataset named Extreme
Pose Face High-Quality Dataset (EFHQ), which includes a maximum of 450k
high-quality images of faces at extreme poses. To produce such a massive
dataset, we utilize a novel and meticulous dataset processing pipeline to
curate two publicly available datasets, VFHQ and CelebV-HQ, which contain many
high-resolution face videos captured in various settings. Our dataset can
complement existing datasets on various facial-related tasks, such as facial
synthesis with 2D/3D-aware GAN, diffusion-based text-to-image face generation,
and face reenactment. Specifically, training with EFHQ helps models generalize
well across diverse poses, significantly improving performance in scenarios
involving extreme views, confirmed by extensive experiments. Additionally, we
utilize EFHQ to define a challenging cross-view face verification benchmark, in
which the performance of SOTA face recognition models drops 5-37% compared to
frontal-to-frontal scenarios, aiming to stimulate studies on face recognition
under severe pose conditions in the wild.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dao_T/0/1/0/all/0/1&quot;&gt;Trung Tuan Dao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_D/0/1/0/all/0/1&quot;&gt;Duc Hong Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1&quot;&gt;Cuong Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1&quot;&gt;Anh Tran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04430">
<title>Breaking Through the Haze: An Advanced Non-Homogeneous Dehazing Method based on Fast Fourier Convolution and ConvNeXt. (arXiv:2305.04430v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2305.04430</link>
<description rdf:parseType="Literal">&lt;p&gt;Haze usually leads to deteriorated images with low contrast, color shift and
structural distortion. We observe that many deep learning based models exhibit
exceptional performance on removing homogeneous haze, but they usually fail to
address the challenge of non-homogeneous dehazing. Two main factors account for
this situation. Firstly, due to the intricate and non uniform distribution of
dense haze, the recovery of structural and chromatic features with high
fidelity is challenging, particularly in regions with heavy haze. Secondly, the
existing small scale datasets for non-homogeneous dehazing are inadequate to
support reliable learning of feature mappings between hazy images and their
corresponding haze-free counterparts by convolutional neural network
(CNN)-based models. To tackle these two challenges, we propose a novel two
branch network that leverages 2D discrete wavelete transform (DWT), fast
Fourier convolution (FFC) residual block and a pretrained ConvNeXt model.
Specifically, in the DWT-FFC frequency branch, our model exploits DWT to
capture more high-frequency features. Moreover, by taking advantage of the
large receptive field provided by FFC residual blocks, our model is able to
effectively explore global contextual information and produce images with
better perceptual quality. In the prior knowledge branch, an ImageNet
pretrained ConvNeXt as opposed to Res2Net is adopted. This enables our model to
learn more supplementary information and acquire a stronger generalization
ability. The feasibility and effectiveness of the proposed method is
demonstrated via extensive experiments and ablation studies. The code is
available at https://github.com/zhouh115/DWT-FFC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Han Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1&quot;&gt;Wei Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yangyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jun Chen&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>